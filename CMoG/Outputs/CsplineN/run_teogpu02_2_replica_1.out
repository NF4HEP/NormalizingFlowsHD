2023-10-25 16:07:04.516036: Importing os...
2023-10-25 16:07:04.516099: Importing sys...
2023-10-25 16:07:04.516114: Importing and initializing argparse...
Visible devices: [2]
2023-10-25 16:07:04.532612: Importing timer from timeit...
2023-10-25 16:07:04.533187: Setting env variables for tf import (only device [2] will be available)...
2023-10-25 16:07:04.533230: Importing numpy...
2023-10-25 16:07:04.699613: Importing pandas...
2023-10-25 16:07:04.888147: Importing shutil...
2023-10-25 16:07:04.888172: Importing subprocess...
2023-10-25 16:07:04.888180: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-25 16:07:07.294748: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-25 16:07:07.762668: Importing textwrap...
2023-10-25 16:07:07.762710: Importing timeit...
2023-10-25 16:07:07.762722: Importing traceback...
2023-10-25 16:07:07.762730: Importing typing...
2023-10-25 16:07:07.762746: Setting tf configs...
2023-10-25 16:07:08.209340: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-25 16:07:09.371900: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

===========
Generating train data for run 359.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1817280   
 r)                                                              
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fef8848ff10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef886d09a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef886d09a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff010af9e70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef884c7310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef884c7880>, <keras.callbacks.ModelCheckpoint object at 0x7fef884c79d0>, <keras.callbacks.EarlyStopping object at 0x7fef884c7be0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef884c7c10>, <keras.callbacks.TerminateOnNaN object at 0x7fef884c7940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:07:19.670650
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:10:02.038 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 162s/epoch - 827ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 359.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7ff399b52bf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff391394ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff391394ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4c2974c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff39128dd20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff39128e290>, <keras.callbacks.ModelCheckpoint object at 0x7ff39128e350>, <keras.callbacks.EarlyStopping object at 0x7ff39128e5c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff39128e5f0>, <keras.callbacks.TerminateOnNaN object at 0x7ff39128e230>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:10:11.807555
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:12:52.617 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 161s/epoch - 820ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 359.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7fec7421ac20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec4c591600>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec4c591600>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feccc25e470>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee043e1900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee043e1e70>, <keras.callbacks.ModelCheckpoint object at 0x7fee043e1f30>, <keras.callbacks.EarlyStopping object at 0x7fee043e21a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee043e21d0>, <keras.callbacks.TerminateOnNaN object at 0x7fee043e1e10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:13:03.453422
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:15:56.907 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 173s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 173s/epoch - 884ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 359.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fed684bd690>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed6823bfa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed6823bfa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef6825e650>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecf49e90f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecf49e9660>, <keras.callbacks.ModelCheckpoint object at 0x7fecf49e9720>, <keras.callbacks.EarlyStopping object at 0x7fecf49e9990>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecf49e99c0>, <keras.callbacks.TerminateOnNaN object at 0x7fecf49e9600>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:16:07.246269
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:18:53.981 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 167s/epoch - 850ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 359.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7feccd123070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed8464f8b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed8464f8b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feccea637f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff388b80880>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff388b80df0>, <keras.callbacks.ModelCheckpoint object at 0x7ff388b80eb0>, <keras.callbacks.EarlyStopping object at 0x7ff388b81120>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff388b81150>, <keras.callbacks.TerminateOnNaN object at 0x7ff388b80d90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:19:02.900387
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7fecd1c3f250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:21:50.787 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 168s/epoch - 856ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 359.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fecf4babd30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecf4c63730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecf4c63730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff3aaaa54b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef284d2ec0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef284d3430>, <keras.callbacks.ModelCheckpoint object at 0x7fef284d34f0>, <keras.callbacks.EarlyStopping object at 0x7fef284d3760>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef284d3790>, <keras.callbacks.TerminateOnNaN object at 0x7fef284d33d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:22:02.611890
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7fecce526320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:25:08.146 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 185s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 185s/epoch - 945ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 359.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7feec85a57e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed847ee170>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed847ee170>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fee58588c70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef48213370>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef482138e0>, <keras.callbacks.ModelCheckpoint object at 0x7fef482139a0>, <keras.callbacks.EarlyStopping object at 0x7fef48213c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef48213c40>, <keras.callbacks.TerminateOnNaN object at 0x7fef48213880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:25:20.869705
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:28:56.703 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 215s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 215s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 359.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7fecafe7f550>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecaf567e20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecaf567e20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fecd1b955d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecafe6e920>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecafe6ee90>, <keras.callbacks.ModelCheckpoint object at 0x7fecafe6ef50>, <keras.callbacks.EarlyStopping object at 0x7fecafe6f1c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecafe6f1f0>, <keras.callbacks.TerminateOnNaN object at 0x7fecafe6ee30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:29:06.806463
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:32:20.870 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 194s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 194s/epoch - 989ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 359.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7feccc9fa110>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecce0dc880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecce0dc880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef487e7b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecce11e380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecce11e8f0>, <keras.callbacks.ModelCheckpoint object at 0x7fecce11e9b0>, <keras.callbacks.EarlyStopping object at 0x7fecce11ec20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecce11ec50>, <keras.callbacks.TerminateOnNaN object at 0x7fecce11e890>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:32:33.433563
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:35:34.767 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 181s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 181s/epoch - 923ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 359.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7ff3803f3a90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff380343ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff380343ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feccf456dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff37fcc2350>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff37fcc28c0>, <keras.callbacks.ModelCheckpoint object at 0x7ff37fcc2980>, <keras.callbacks.EarlyStopping object at 0x7ff37fcc2bf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff37fcc2c20>, <keras.callbacks.TerminateOnNaN object at 0x7ff37fcc2860>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:35:45.763309
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:38:42.343 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 176s/epoch - 900ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 359.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_359/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_359
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fede8597af0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed245e5b10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed245e5b10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fee245c6980>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fede85d2b00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_359/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fede85d3070>, <keras.callbacks.ModelCheckpoint object at 0x7fede85d3130>, <keras.callbacks.EarlyStopping object at 0x7fede85d33a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fede85d33d0>, <keras.callbacks.TerminateOnNaN object at 0x7fede85d3010>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_359/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 359/720 with hyperparameters:
timestamp = 2023-10-25 16:38:54.608038
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:42:05.926 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.6328, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 191s - loss: nan - MinusLogProbMetric: 7812.6328 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 191s/epoch - 975ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 359/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

===========
Generating train data for run 363.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_363/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_363/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_363/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_363
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7fee8030f8e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec874cbc70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec874cbc70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef48386e90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecf475e080>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecf475dae0>, <keras.callbacks.ModelCheckpoint object at 0x7fecf475da50>, <keras.callbacks.EarlyStopping object at 0x7fecf475d810>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecf475d7e0>, <keras.callbacks.TerminateOnNaN object at 0x7fecf475db70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_363/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 363/720 with hyperparameters:
timestamp = 2023-10-25 16:42:12.130659
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 26: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:43:33.758 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4031.4771, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 81s - loss: nan - MinusLogProbMetric: 4031.4771 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 81s/epoch - 416ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 363.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_363/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_363/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_363/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_363
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7ff36f263e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3777a1a80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3777a1a80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4c903cd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff36f0c25c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff36f0c2b30>, <keras.callbacks.ModelCheckpoint object at 0x7ff36f0c2bf0>, <keras.callbacks.EarlyStopping object at 0x7ff36f0c2e60>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff36f0c2e90>, <keras.callbacks.TerminateOnNaN object at 0x7ff36f0c2ad0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_363/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 363/720 with hyperparameters:
timestamp = 2023-10-25 16:43:41.026256
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 16:46:07.936 
Epoch 1/1000 
	 loss: 1208.1312, MinusLogProbMetric: 1208.1312, val_loss: 348.2860, val_MinusLogProbMetric: 348.2860

Epoch 1: val_loss improved from inf to 348.28601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 147s - loss: 1208.1312 - MinusLogProbMetric: 1208.1312 - val_loss: 348.2860 - val_MinusLogProbMetric: 348.2860 - lr: 3.3333e-04 - 147s/epoch - 751ms/step
Epoch 2/1000
2023-10-25 16:46:52.437 
Epoch 2/1000 
	 loss: 265.8022, MinusLogProbMetric: 265.8022, val_loss: 223.0381, val_MinusLogProbMetric: 223.0381

Epoch 2: val_loss improved from 348.28601 to 223.03809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 265.8022 - MinusLogProbMetric: 265.8022 - val_loss: 223.0381 - val_MinusLogProbMetric: 223.0381 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 3/1000
2023-10-25 16:47:36.464 
Epoch 3/1000 
	 loss: 194.9817, MinusLogProbMetric: 194.9817, val_loss: 174.0552, val_MinusLogProbMetric: 174.0552

Epoch 3: val_loss improved from 223.03809 to 174.05522, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 194.9817 - MinusLogProbMetric: 194.9817 - val_loss: 174.0552 - val_MinusLogProbMetric: 174.0552 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 4/1000
2023-10-25 16:48:20.146 
Epoch 4/1000 
	 loss: 158.8368, MinusLogProbMetric: 158.8368, val_loss: 145.3067, val_MinusLogProbMetric: 145.3067

Epoch 4: val_loss improved from 174.05522 to 145.30672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 158.8368 - MinusLogProbMetric: 158.8368 - val_loss: 145.3067 - val_MinusLogProbMetric: 145.3067 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 5/1000
2023-10-25 16:49:03.574 
Epoch 5/1000 
	 loss: 140.5972, MinusLogProbMetric: 140.5972, val_loss: 130.1574, val_MinusLogProbMetric: 130.1574

Epoch 5: val_loss improved from 145.30672 to 130.15744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 140.5972 - MinusLogProbMetric: 140.5972 - val_loss: 130.1574 - val_MinusLogProbMetric: 130.1574 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 6/1000
2023-10-25 16:49:47.171 
Epoch 6/1000 
	 loss: 125.2424, MinusLogProbMetric: 125.2424, val_loss: 118.7049, val_MinusLogProbMetric: 118.7049

Epoch 6: val_loss improved from 130.15744 to 118.70486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 125.2424 - MinusLogProbMetric: 125.2424 - val_loss: 118.7049 - val_MinusLogProbMetric: 118.7049 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 7/1000
2023-10-25 16:50:28.277 
Epoch 7/1000 
	 loss: 114.2112, MinusLogProbMetric: 114.2112, val_loss: 107.8808, val_MinusLogProbMetric: 107.8808

Epoch 7: val_loss improved from 118.70486 to 107.88082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 114.2112 - MinusLogProbMetric: 114.2112 - val_loss: 107.8808 - val_MinusLogProbMetric: 107.8808 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 8/1000
2023-10-25 16:51:08.144 
Epoch 8/1000 
	 loss: 105.1395, MinusLogProbMetric: 105.1395, val_loss: 101.5936, val_MinusLogProbMetric: 101.5936

Epoch 8: val_loss improved from 107.88082 to 101.59359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 40s - loss: 105.1395 - MinusLogProbMetric: 105.1395 - val_loss: 101.5936 - val_MinusLogProbMetric: 101.5936 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 9/1000
2023-10-25 16:51:45.333 
Epoch 9/1000 
	 loss: 96.9044, MinusLogProbMetric: 96.9044, val_loss: 95.6685, val_MinusLogProbMetric: 95.6685

Epoch 9: val_loss improved from 101.59359 to 95.66855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 37s - loss: 96.9044 - MinusLogProbMetric: 96.9044 - val_loss: 95.6685 - val_MinusLogProbMetric: 95.6685 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 10/1000
2023-10-25 16:52:22.738 
Epoch 10/1000 
	 loss: 91.1917, MinusLogProbMetric: 91.1917, val_loss: 88.6719, val_MinusLogProbMetric: 88.6719

Epoch 10: val_loss improved from 95.66855 to 88.67194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 37s - loss: 91.1917 - MinusLogProbMetric: 91.1917 - val_loss: 88.6719 - val_MinusLogProbMetric: 88.6719 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 11/1000
2023-10-25 16:53:03.900 
Epoch 11/1000 
	 loss: 85.3110, MinusLogProbMetric: 85.3110, val_loss: 82.9752, val_MinusLogProbMetric: 82.9752

Epoch 11: val_loss improved from 88.67194 to 82.97520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 85.3110 - MinusLogProbMetric: 85.3110 - val_loss: 82.9752 - val_MinusLogProbMetric: 82.9752 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 12/1000
2023-10-25 16:53:46.931 
Epoch 12/1000 
	 loss: 80.6234, MinusLogProbMetric: 80.6234, val_loss: 78.0284, val_MinusLogProbMetric: 78.0284

Epoch 12: val_loss improved from 82.97520 to 78.02843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 80.6234 - MinusLogProbMetric: 80.6234 - val_loss: 78.0284 - val_MinusLogProbMetric: 78.0284 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 13/1000
2023-10-25 16:54:30.095 
Epoch 13/1000 
	 loss: 76.2621, MinusLogProbMetric: 76.2621, val_loss: 74.5640, val_MinusLogProbMetric: 74.5640

Epoch 13: val_loss improved from 78.02843 to 74.56404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 76.2621 - MinusLogProbMetric: 76.2621 - val_loss: 74.5640 - val_MinusLogProbMetric: 74.5640 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 14/1000
2023-10-25 16:55:12.285 
Epoch 14/1000 
	 loss: 72.8897, MinusLogProbMetric: 72.8897, val_loss: 71.3206, val_MinusLogProbMetric: 71.3206

Epoch 14: val_loss improved from 74.56404 to 71.32058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 72.8897 - MinusLogProbMetric: 72.8897 - val_loss: 71.3206 - val_MinusLogProbMetric: 71.3206 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 15/1000
2023-10-25 16:55:54.316 
Epoch 15/1000 
	 loss: 69.4164, MinusLogProbMetric: 69.4164, val_loss: 67.6318, val_MinusLogProbMetric: 67.6318

Epoch 15: val_loss improved from 71.32058 to 67.63177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 69.4164 - MinusLogProbMetric: 69.4164 - val_loss: 67.6318 - val_MinusLogProbMetric: 67.6318 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 16/1000
2023-10-25 16:56:31.340 
Epoch 16/1000 
	 loss: 66.8517, MinusLogProbMetric: 66.8517, val_loss: 66.9029, val_MinusLogProbMetric: 66.9029

Epoch 16: val_loss improved from 67.63177 to 66.90285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 37s - loss: 66.8517 - MinusLogProbMetric: 66.8517 - val_loss: 66.9029 - val_MinusLogProbMetric: 66.9029 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 17/1000
2023-10-25 16:57:12.498 
Epoch 17/1000 
	 loss: 64.0762, MinusLogProbMetric: 64.0762, val_loss: 63.2986, val_MinusLogProbMetric: 63.2986

Epoch 17: val_loss improved from 66.90285 to 63.29858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 64.0762 - MinusLogProbMetric: 64.0762 - val_loss: 63.2986 - val_MinusLogProbMetric: 63.2986 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 18/1000
2023-10-25 16:57:52.348 
Epoch 18/1000 
	 loss: 61.8797, MinusLogProbMetric: 61.8797, val_loss: 60.9075, val_MinusLogProbMetric: 60.9075

Epoch 18: val_loss improved from 63.29858 to 60.90746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 40s - loss: 61.8797 - MinusLogProbMetric: 61.8797 - val_loss: 60.9075 - val_MinusLogProbMetric: 60.9075 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 19/1000
2023-10-25 16:58:35.080 
Epoch 19/1000 
	 loss: 59.9224, MinusLogProbMetric: 59.9224, val_loss: 61.6553, val_MinusLogProbMetric: 61.6553

Epoch 19: val_loss did not improve from 60.90746
196/196 - 42s - loss: 59.9224 - MinusLogProbMetric: 59.9224 - val_loss: 61.6553 - val_MinusLogProbMetric: 61.6553 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 20/1000
2023-10-25 16:59:17.615 
Epoch 20/1000 
	 loss: 58.2723, MinusLogProbMetric: 58.2723, val_loss: 57.6661, val_MinusLogProbMetric: 57.6661

Epoch 20: val_loss improved from 60.90746 to 57.66611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 58.2723 - MinusLogProbMetric: 58.2723 - val_loss: 57.6661 - val_MinusLogProbMetric: 57.6661 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 21/1000
2023-10-25 17:00:01.038 
Epoch 21/1000 
	 loss: 56.5243, MinusLogProbMetric: 56.5243, val_loss: 56.0093, val_MinusLogProbMetric: 56.0093

Epoch 21: val_loss improved from 57.66611 to 56.00930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 56.5243 - MinusLogProbMetric: 56.5243 - val_loss: 56.0093 - val_MinusLogProbMetric: 56.0093 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 22/1000
2023-10-25 17:00:44.078 
Epoch 22/1000 
	 loss: 55.0237, MinusLogProbMetric: 55.0237, val_loss: 54.9254, val_MinusLogProbMetric: 54.9254

Epoch 22: val_loss improved from 56.00930 to 54.92536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 55.0237 - MinusLogProbMetric: 55.0237 - val_loss: 54.9254 - val_MinusLogProbMetric: 54.9254 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 23/1000
2023-10-25 17:01:27.097 
Epoch 23/1000 
	 loss: 53.6136, MinusLogProbMetric: 53.6136, val_loss: 52.8370, val_MinusLogProbMetric: 52.8370

Epoch 23: val_loss improved from 54.92536 to 52.83696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 53.6136 - MinusLogProbMetric: 53.6136 - val_loss: 52.8370 - val_MinusLogProbMetric: 52.8370 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 24/1000
2023-10-25 17:02:09.672 
Epoch 24/1000 
	 loss: 52.3750, MinusLogProbMetric: 52.3750, val_loss: 52.0793, val_MinusLogProbMetric: 52.0793

Epoch 24: val_loss improved from 52.83696 to 52.07931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 52.3750 - MinusLogProbMetric: 52.3750 - val_loss: 52.0793 - val_MinusLogProbMetric: 52.0793 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 25/1000
2023-10-25 17:02:52.240 
Epoch 25/1000 
	 loss: 51.3148, MinusLogProbMetric: 51.3148, val_loss: 51.2580, val_MinusLogProbMetric: 51.2580

Epoch 25: val_loss improved from 52.07931 to 51.25798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 51.3148 - MinusLogProbMetric: 51.3148 - val_loss: 51.2580 - val_MinusLogProbMetric: 51.2580 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 26/1000
2023-10-25 17:03:34.972 
Epoch 26/1000 
	 loss: 50.4083, MinusLogProbMetric: 50.4083, val_loss: 50.4886, val_MinusLogProbMetric: 50.4886

Epoch 26: val_loss improved from 51.25798 to 50.48856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 50.4083 - MinusLogProbMetric: 50.4083 - val_loss: 50.4886 - val_MinusLogProbMetric: 50.4886 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 27/1000
2023-10-25 17:04:14.854 
Epoch 27/1000 
	 loss: 49.4922, MinusLogProbMetric: 49.4922, val_loss: 49.5737, val_MinusLogProbMetric: 49.5737

Epoch 27: val_loss improved from 50.48856 to 49.57375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 40s - loss: 49.4922 - MinusLogProbMetric: 49.4922 - val_loss: 49.5737 - val_MinusLogProbMetric: 49.5737 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 28/1000
2023-10-25 17:04:55.182 
Epoch 28/1000 
	 loss: 48.7150, MinusLogProbMetric: 48.7150, val_loss: 48.6198, val_MinusLogProbMetric: 48.6198

Epoch 28: val_loss improved from 49.57375 to 48.61982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 40s - loss: 48.7150 - MinusLogProbMetric: 48.7150 - val_loss: 48.6198 - val_MinusLogProbMetric: 48.6198 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 29/1000
2023-10-25 17:05:37.684 
Epoch 29/1000 
	 loss: 47.9001, MinusLogProbMetric: 47.9001, val_loss: 47.5623, val_MinusLogProbMetric: 47.5623

Epoch 29: val_loss improved from 48.61982 to 47.56229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 47.9001 - MinusLogProbMetric: 47.9001 - val_loss: 47.5623 - val_MinusLogProbMetric: 47.5623 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 30/1000
2023-10-25 17:06:20.329 
Epoch 30/1000 
	 loss: 47.2480, MinusLogProbMetric: 47.2480, val_loss: 47.0870, val_MinusLogProbMetric: 47.0870

Epoch 30: val_loss improved from 47.56229 to 47.08705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 47.2480 - MinusLogProbMetric: 47.2480 - val_loss: 47.0870 - val_MinusLogProbMetric: 47.0870 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 31/1000
2023-10-25 17:07:03.625 
Epoch 31/1000 
	 loss: 46.5812, MinusLogProbMetric: 46.5812, val_loss: 46.8730, val_MinusLogProbMetric: 46.8730

Epoch 31: val_loss improved from 47.08705 to 46.87304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 46.5812 - MinusLogProbMetric: 46.5812 - val_loss: 46.8730 - val_MinusLogProbMetric: 46.8730 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 32/1000
2023-10-25 17:07:46.090 
Epoch 32/1000 
	 loss: 46.4202, MinusLogProbMetric: 46.4202, val_loss: 45.5054, val_MinusLogProbMetric: 45.5054

Epoch 32: val_loss improved from 46.87304 to 45.50540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 46.4202 - MinusLogProbMetric: 46.4202 - val_loss: 45.5054 - val_MinusLogProbMetric: 45.5054 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 33/1000
2023-10-25 17:08:28.821 
Epoch 33/1000 
	 loss: 45.3618, MinusLogProbMetric: 45.3618, val_loss: 45.5996, val_MinusLogProbMetric: 45.5996

Epoch 33: val_loss did not improve from 45.50540
196/196 - 42s - loss: 45.3618 - MinusLogProbMetric: 45.3618 - val_loss: 45.5996 - val_MinusLogProbMetric: 45.5996 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 34/1000
2023-10-25 17:09:08.472 
Epoch 34/1000 
	 loss: 44.8633, MinusLogProbMetric: 44.8633, val_loss: 44.6928, val_MinusLogProbMetric: 44.6928

Epoch 34: val_loss improved from 45.50540 to 44.69279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 40s - loss: 44.8633 - MinusLogProbMetric: 44.8633 - val_loss: 44.6928 - val_MinusLogProbMetric: 44.6928 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 35/1000
2023-10-25 17:09:49.732 
Epoch 35/1000 
	 loss: 44.2188, MinusLogProbMetric: 44.2188, val_loss: 44.2625, val_MinusLogProbMetric: 44.2625

Epoch 35: val_loss improved from 44.69279 to 44.26247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 44.2188 - MinusLogProbMetric: 44.2188 - val_loss: 44.2625 - val_MinusLogProbMetric: 44.2625 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 36/1000
2023-10-25 17:10:30.826 
Epoch 36/1000 
	 loss: 44.0098, MinusLogProbMetric: 44.0098, val_loss: 44.4345, val_MinusLogProbMetric: 44.4345

Epoch 36: val_loss did not improve from 44.26247
196/196 - 40s - loss: 44.0098 - MinusLogProbMetric: 44.0098 - val_loss: 44.4345 - val_MinusLogProbMetric: 44.4345 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 37/1000
2023-10-25 17:11:06.079 
Epoch 37/1000 
	 loss: 43.5791, MinusLogProbMetric: 43.5791, val_loss: 44.0937, val_MinusLogProbMetric: 44.0937

Epoch 37: val_loss improved from 44.26247 to 44.09370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 36s - loss: 43.5791 - MinusLogProbMetric: 43.5791 - val_loss: 44.0937 - val_MinusLogProbMetric: 44.0937 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 38/1000
2023-10-25 17:11:48.918 
Epoch 38/1000 
	 loss: 42.9175, MinusLogProbMetric: 42.9175, val_loss: 42.9590, val_MinusLogProbMetric: 42.9590

Epoch 38: val_loss improved from 44.09370 to 42.95900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 42.9175 - MinusLogProbMetric: 42.9175 - val_loss: 42.9590 - val_MinusLogProbMetric: 42.9590 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 39/1000
2023-10-25 17:12:32.003 
Epoch 39/1000 
	 loss: 42.7448, MinusLogProbMetric: 42.7448, val_loss: 42.7183, val_MinusLogProbMetric: 42.7183

Epoch 39: val_loss improved from 42.95900 to 42.71834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 42.7448 - MinusLogProbMetric: 42.7448 - val_loss: 42.7183 - val_MinusLogProbMetric: 42.7183 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 40/1000
2023-10-25 17:13:15.154 
Epoch 40/1000 
	 loss: 42.4256, MinusLogProbMetric: 42.4256, val_loss: 41.8878, val_MinusLogProbMetric: 41.8878

Epoch 40: val_loss improved from 42.71834 to 41.88781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 42.4256 - MinusLogProbMetric: 42.4256 - val_loss: 41.8878 - val_MinusLogProbMetric: 41.8878 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 41/1000
2023-10-25 17:13:57.893 
Epoch 41/1000 
	 loss: 41.9016, MinusLogProbMetric: 41.9016, val_loss: 41.8859, val_MinusLogProbMetric: 41.8859

Epoch 41: val_loss improved from 41.88781 to 41.88591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 41.9016 - MinusLogProbMetric: 41.9016 - val_loss: 41.8859 - val_MinusLogProbMetric: 41.8859 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 42/1000
2023-10-25 17:14:41.145 
Epoch 42/1000 
	 loss: 41.5399, MinusLogProbMetric: 41.5399, val_loss: 42.3210, val_MinusLogProbMetric: 42.3210

Epoch 42: val_loss did not improve from 41.88591
196/196 - 43s - loss: 41.5399 - MinusLogProbMetric: 41.5399 - val_loss: 42.3210 - val_MinusLogProbMetric: 42.3210 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 43/1000
2023-10-25 17:15:23.796 
Epoch 43/1000 
	 loss: 41.4833, MinusLogProbMetric: 41.4833, val_loss: 42.1858, val_MinusLogProbMetric: 42.1858

Epoch 43: val_loss did not improve from 41.88591
196/196 - 43s - loss: 41.4833 - MinusLogProbMetric: 41.4833 - val_loss: 42.1858 - val_MinusLogProbMetric: 42.1858 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 44/1000
2023-10-25 17:16:06.410 
Epoch 44/1000 
	 loss: 40.8906, MinusLogProbMetric: 40.8906, val_loss: 41.1671, val_MinusLogProbMetric: 41.1671

Epoch 44: val_loss improved from 41.88591 to 41.16710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 40.8906 - MinusLogProbMetric: 40.8906 - val_loss: 41.1671 - val_MinusLogProbMetric: 41.1671 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 45/1000
2023-10-25 17:16:49.893 
Epoch 45/1000 
	 loss: 40.6302, MinusLogProbMetric: 40.6302, val_loss: 40.2419, val_MinusLogProbMetric: 40.2419

Epoch 45: val_loss improved from 41.16710 to 40.24191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 40.6302 - MinusLogProbMetric: 40.6302 - val_loss: 40.2419 - val_MinusLogProbMetric: 40.2419 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 46/1000
2023-10-25 17:17:32.798 
Epoch 46/1000 
	 loss: 40.6213, MinusLogProbMetric: 40.6213, val_loss: 43.8518, val_MinusLogProbMetric: 43.8518

Epoch 46: val_loss did not improve from 40.24191
196/196 - 42s - loss: 40.6213 - MinusLogProbMetric: 40.6213 - val_loss: 43.8518 - val_MinusLogProbMetric: 43.8518 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 47/1000
2023-10-25 17:18:15.622 
Epoch 47/1000 
	 loss: 41.0267, MinusLogProbMetric: 41.0267, val_loss: 41.3112, val_MinusLogProbMetric: 41.3112

Epoch 47: val_loss did not improve from 40.24191
196/196 - 43s - loss: 41.0267 - MinusLogProbMetric: 41.0267 - val_loss: 41.3112 - val_MinusLogProbMetric: 41.3112 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 48/1000
2023-10-25 17:18:58.077 
Epoch 48/1000 
	 loss: 40.1233, MinusLogProbMetric: 40.1233, val_loss: 39.9413, val_MinusLogProbMetric: 39.9413

Epoch 48: val_loss improved from 40.24191 to 39.94127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 40.1233 - MinusLogProbMetric: 40.1233 - val_loss: 39.9413 - val_MinusLogProbMetric: 39.9413 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 49/1000
2023-10-25 17:19:41.521 
Epoch 49/1000 
	 loss: 40.4045, MinusLogProbMetric: 40.4045, val_loss: 41.2181, val_MinusLogProbMetric: 41.2181

Epoch 49: val_loss did not improve from 39.94127
196/196 - 43s - loss: 40.4045 - MinusLogProbMetric: 40.4045 - val_loss: 41.2181 - val_MinusLogProbMetric: 41.2181 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 50/1000
2023-10-25 17:20:23.832 
Epoch 50/1000 
	 loss: 39.6220, MinusLogProbMetric: 39.6220, val_loss: 39.8804, val_MinusLogProbMetric: 39.8804

Epoch 50: val_loss improved from 39.94127 to 39.88042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 39.6220 - MinusLogProbMetric: 39.6220 - val_loss: 39.8804 - val_MinusLogProbMetric: 39.8804 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 51/1000
2023-10-25 17:21:06.957 
Epoch 51/1000 
	 loss: 39.2344, MinusLogProbMetric: 39.2344, val_loss: 39.3285, val_MinusLogProbMetric: 39.3285

Epoch 51: val_loss improved from 39.88042 to 39.32853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 39.2344 - MinusLogProbMetric: 39.2344 - val_loss: 39.3285 - val_MinusLogProbMetric: 39.3285 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 52/1000
2023-10-25 17:21:49.953 
Epoch 52/1000 
	 loss: 39.1917, MinusLogProbMetric: 39.1917, val_loss: 39.0447, val_MinusLogProbMetric: 39.0447

Epoch 52: val_loss improved from 39.32853 to 39.04466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 39.1917 - MinusLogProbMetric: 39.1917 - val_loss: 39.0447 - val_MinusLogProbMetric: 39.0447 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 53/1000
2023-10-25 17:22:33.118 
Epoch 53/1000 
	 loss: 38.9333, MinusLogProbMetric: 38.9333, val_loss: 39.3996, val_MinusLogProbMetric: 39.3996

Epoch 53: val_loss did not improve from 39.04466
196/196 - 42s - loss: 38.9333 - MinusLogProbMetric: 38.9333 - val_loss: 39.3996 - val_MinusLogProbMetric: 39.3996 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 54/1000
2023-10-25 17:23:15.437 
Epoch 54/1000 
	 loss: 38.9004, MinusLogProbMetric: 38.9004, val_loss: 38.9707, val_MinusLogProbMetric: 38.9707

Epoch 54: val_loss improved from 39.04466 to 38.97074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 38.9004 - MinusLogProbMetric: 38.9004 - val_loss: 38.9707 - val_MinusLogProbMetric: 38.9707 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 55/1000
2023-10-25 17:23:58.454 
Epoch 55/1000 
	 loss: 38.5092, MinusLogProbMetric: 38.5092, val_loss: 38.8877, val_MinusLogProbMetric: 38.8877

Epoch 55: val_loss improved from 38.97074 to 38.88769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 38.5092 - MinusLogProbMetric: 38.5092 - val_loss: 38.8877 - val_MinusLogProbMetric: 38.8877 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 56/1000
2023-10-25 17:24:41.416 
Epoch 56/1000 
	 loss: 38.5311, MinusLogProbMetric: 38.5311, val_loss: 38.3112, val_MinusLogProbMetric: 38.3112

Epoch 56: val_loss improved from 38.88769 to 38.31119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 38.5311 - MinusLogProbMetric: 38.5311 - val_loss: 38.3112 - val_MinusLogProbMetric: 38.3112 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 57/1000
2023-10-25 17:25:24.437 
Epoch 57/1000 
	 loss: 38.4958, MinusLogProbMetric: 38.4958, val_loss: 38.3579, val_MinusLogProbMetric: 38.3579

Epoch 57: val_loss did not improve from 38.31119
196/196 - 42s - loss: 38.4958 - MinusLogProbMetric: 38.4958 - val_loss: 38.3579 - val_MinusLogProbMetric: 38.3579 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 58/1000
2023-10-25 17:26:06.828 
Epoch 58/1000 
	 loss: 38.0404, MinusLogProbMetric: 38.0404, val_loss: 40.4897, val_MinusLogProbMetric: 40.4897

Epoch 58: val_loss did not improve from 38.31119
196/196 - 42s - loss: 38.0404 - MinusLogProbMetric: 38.0404 - val_loss: 40.4897 - val_MinusLogProbMetric: 40.4897 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 59/1000
2023-10-25 17:26:49.249 
Epoch 59/1000 
	 loss: 37.8928, MinusLogProbMetric: 37.8928, val_loss: 38.4831, val_MinusLogProbMetric: 38.4831

Epoch 59: val_loss did not improve from 38.31119
196/196 - 42s - loss: 37.8928 - MinusLogProbMetric: 37.8928 - val_loss: 38.4831 - val_MinusLogProbMetric: 38.4831 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 60/1000
2023-10-25 17:27:31.511 
Epoch 60/1000 
	 loss: 39.7096, MinusLogProbMetric: 39.7096, val_loss: 38.8670, val_MinusLogProbMetric: 38.8670

Epoch 60: val_loss did not improve from 38.31119
196/196 - 42s - loss: 39.7096 - MinusLogProbMetric: 39.7096 - val_loss: 38.8670 - val_MinusLogProbMetric: 38.8670 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 61/1000
2023-10-25 17:28:13.893 
Epoch 61/1000 
	 loss: 37.6158, MinusLogProbMetric: 37.6158, val_loss: 37.8377, val_MinusLogProbMetric: 37.8377

Epoch 61: val_loss improved from 38.31119 to 37.83773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 37.6158 - MinusLogProbMetric: 37.6158 - val_loss: 37.8377 - val_MinusLogProbMetric: 37.8377 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 62/1000
2023-10-25 17:28:57.191 
Epoch 62/1000 
	 loss: 37.3997, MinusLogProbMetric: 37.3997, val_loss: 36.9518, val_MinusLogProbMetric: 36.9518

Epoch 62: val_loss improved from 37.83773 to 36.95177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 37.3997 - MinusLogProbMetric: 37.3997 - val_loss: 36.9518 - val_MinusLogProbMetric: 36.9518 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 63/1000
2023-10-25 17:29:40.645 
Epoch 63/1000 
	 loss: 37.3426, MinusLogProbMetric: 37.3426, val_loss: 37.0199, val_MinusLogProbMetric: 37.0199

Epoch 63: val_loss did not improve from 36.95177
196/196 - 43s - loss: 37.3426 - MinusLogProbMetric: 37.3426 - val_loss: 37.0199 - val_MinusLogProbMetric: 37.0199 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 64/1000
2023-10-25 17:30:23.014 
Epoch 64/1000 
	 loss: 37.2329, MinusLogProbMetric: 37.2329, val_loss: 38.4593, val_MinusLogProbMetric: 38.4593

Epoch 64: val_loss did not improve from 36.95177
196/196 - 42s - loss: 37.2329 - MinusLogProbMetric: 37.2329 - val_loss: 38.4593 - val_MinusLogProbMetric: 38.4593 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 65/1000
2023-10-25 17:31:05.393 
Epoch 65/1000 
	 loss: 36.9152, MinusLogProbMetric: 36.9152, val_loss: 38.7470, val_MinusLogProbMetric: 38.7470

Epoch 65: val_loss did not improve from 36.95177
196/196 - 42s - loss: 36.9152 - MinusLogProbMetric: 36.9152 - val_loss: 38.7470 - val_MinusLogProbMetric: 38.7470 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 66/1000
2023-10-25 17:31:48.442 
Epoch 66/1000 
	 loss: 37.1062, MinusLogProbMetric: 37.1062, val_loss: 38.7666, val_MinusLogProbMetric: 38.7666

Epoch 66: val_loss did not improve from 36.95177
196/196 - 43s - loss: 37.1062 - MinusLogProbMetric: 37.1062 - val_loss: 38.7666 - val_MinusLogProbMetric: 38.7666 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 67/1000
2023-10-25 17:32:31.510 
Epoch 67/1000 
	 loss: 37.1084, MinusLogProbMetric: 37.1084, val_loss: 43.6900, val_MinusLogProbMetric: 43.6900

Epoch 67: val_loss did not improve from 36.95177
196/196 - 43s - loss: 37.1084 - MinusLogProbMetric: 37.1084 - val_loss: 43.6900 - val_MinusLogProbMetric: 43.6900 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 68/1000
2023-10-25 17:33:14.349 
Epoch 68/1000 
	 loss: 37.4861, MinusLogProbMetric: 37.4861, val_loss: 36.3808, val_MinusLogProbMetric: 36.3808

Epoch 68: val_loss improved from 36.95177 to 36.38077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 37.4861 - MinusLogProbMetric: 37.4861 - val_loss: 36.3808 - val_MinusLogProbMetric: 36.3808 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 69/1000
2023-10-25 17:33:57.584 
Epoch 69/1000 
	 loss: 36.6257, MinusLogProbMetric: 36.6257, val_loss: 38.6877, val_MinusLogProbMetric: 38.6877

Epoch 69: val_loss did not improve from 36.38077
196/196 - 43s - loss: 36.6257 - MinusLogProbMetric: 36.6257 - val_loss: 38.6877 - val_MinusLogProbMetric: 38.6877 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 70/1000
2023-10-25 17:34:39.886 
Epoch 70/1000 
	 loss: 36.7929, MinusLogProbMetric: 36.7929, val_loss: 36.6838, val_MinusLogProbMetric: 36.6838

Epoch 70: val_loss did not improve from 36.38077
196/196 - 42s - loss: 36.7929 - MinusLogProbMetric: 36.7929 - val_loss: 36.6838 - val_MinusLogProbMetric: 36.6838 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 71/1000
2023-10-25 17:35:22.275 
Epoch 71/1000 
	 loss: 36.8437, MinusLogProbMetric: 36.8437, val_loss: 36.4103, val_MinusLogProbMetric: 36.4103

Epoch 71: val_loss did not improve from 36.38077
196/196 - 42s - loss: 36.8437 - MinusLogProbMetric: 36.8437 - val_loss: 36.4103 - val_MinusLogProbMetric: 36.4103 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 72/1000
2023-10-25 17:36:04.615 
Epoch 72/1000 
	 loss: 36.3827, MinusLogProbMetric: 36.3827, val_loss: 37.3249, val_MinusLogProbMetric: 37.3249

Epoch 72: val_loss did not improve from 36.38077
196/196 - 42s - loss: 36.3827 - MinusLogProbMetric: 36.3827 - val_loss: 37.3249 - val_MinusLogProbMetric: 37.3249 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 73/1000
2023-10-25 17:36:47.420 
Epoch 73/1000 
	 loss: 36.6351, MinusLogProbMetric: 36.6351, val_loss: 35.9525, val_MinusLogProbMetric: 35.9525

Epoch 73: val_loss improved from 36.38077 to 35.95250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 36.6351 - MinusLogProbMetric: 36.6351 - val_loss: 35.9525 - val_MinusLogProbMetric: 35.9525 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 74/1000
2023-10-25 17:37:30.866 
Epoch 74/1000 
	 loss: 36.8386, MinusLogProbMetric: 36.8386, val_loss: 36.5694, val_MinusLogProbMetric: 36.5694

Epoch 74: val_loss did not improve from 35.95250
196/196 - 43s - loss: 36.8386 - MinusLogProbMetric: 36.8386 - val_loss: 36.5694 - val_MinusLogProbMetric: 36.5694 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 75/1000
2023-10-25 17:38:13.623 
Epoch 75/1000 
	 loss: 36.4423, MinusLogProbMetric: 36.4423, val_loss: 37.1349, val_MinusLogProbMetric: 37.1349

Epoch 75: val_loss did not improve from 35.95250
196/196 - 43s - loss: 36.4423 - MinusLogProbMetric: 36.4423 - val_loss: 37.1349 - val_MinusLogProbMetric: 37.1349 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 76/1000
2023-10-25 17:38:55.595 
Epoch 76/1000 
	 loss: 36.6282, MinusLogProbMetric: 36.6282, val_loss: 36.3512, val_MinusLogProbMetric: 36.3512

Epoch 76: val_loss did not improve from 35.95250
196/196 - 42s - loss: 36.6282 - MinusLogProbMetric: 36.6282 - val_loss: 36.3512 - val_MinusLogProbMetric: 36.3512 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 77/1000
2023-10-25 17:39:37.928 
Epoch 77/1000 
	 loss: 36.2183, MinusLogProbMetric: 36.2183, val_loss: 36.3247, val_MinusLogProbMetric: 36.3247

Epoch 77: val_loss did not improve from 35.95250
196/196 - 42s - loss: 36.2183 - MinusLogProbMetric: 36.2183 - val_loss: 36.3247 - val_MinusLogProbMetric: 36.3247 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 78/1000
2023-10-25 17:40:20.364 
Epoch 78/1000 
	 loss: 35.6846, MinusLogProbMetric: 35.6846, val_loss: 36.0330, val_MinusLogProbMetric: 36.0330

Epoch 78: val_loss did not improve from 35.95250
196/196 - 42s - loss: 35.6846 - MinusLogProbMetric: 35.6846 - val_loss: 36.0330 - val_MinusLogProbMetric: 36.0330 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 79/1000
2023-10-25 17:41:03.073 
Epoch 79/1000 
	 loss: 36.5043, MinusLogProbMetric: 36.5043, val_loss: 36.1227, val_MinusLogProbMetric: 36.1227

Epoch 79: val_loss did not improve from 35.95250
196/196 - 43s - loss: 36.5043 - MinusLogProbMetric: 36.5043 - val_loss: 36.1227 - val_MinusLogProbMetric: 36.1227 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 80/1000
2023-10-25 17:41:45.697 
Epoch 80/1000 
	 loss: 35.6412, MinusLogProbMetric: 35.6412, val_loss: 35.3387, val_MinusLogProbMetric: 35.3387

Epoch 80: val_loss improved from 35.95250 to 35.33867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 35.6412 - MinusLogProbMetric: 35.6412 - val_loss: 35.3387 - val_MinusLogProbMetric: 35.3387 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 81/1000
2023-10-25 17:42:29.146 
Epoch 81/1000 
	 loss: 37.7673, MinusLogProbMetric: 37.7673, val_loss: 35.7313, val_MinusLogProbMetric: 35.7313

Epoch 81: val_loss did not improve from 35.33867
196/196 - 43s - loss: 37.7673 - MinusLogProbMetric: 37.7673 - val_loss: 35.7313 - val_MinusLogProbMetric: 35.7313 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 82/1000
2023-10-25 17:43:11.727 
Epoch 82/1000 
	 loss: 35.5870, MinusLogProbMetric: 35.5870, val_loss: 35.5145, val_MinusLogProbMetric: 35.5145

Epoch 82: val_loss did not improve from 35.33867
196/196 - 43s - loss: 35.5870 - MinusLogProbMetric: 35.5870 - val_loss: 35.5145 - val_MinusLogProbMetric: 35.5145 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 83/1000
2023-10-25 17:43:54.415 
Epoch 83/1000 
	 loss: 35.4303, MinusLogProbMetric: 35.4303, val_loss: 35.3600, val_MinusLogProbMetric: 35.3600

Epoch 83: val_loss did not improve from 35.33867
196/196 - 43s - loss: 35.4303 - MinusLogProbMetric: 35.4303 - val_loss: 35.3600 - val_MinusLogProbMetric: 35.3600 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 84/1000
2023-10-25 17:44:37.252 
Epoch 84/1000 
	 loss: 35.3907, MinusLogProbMetric: 35.3907, val_loss: 35.8824, val_MinusLogProbMetric: 35.8824

Epoch 84: val_loss did not improve from 35.33867
196/196 - 43s - loss: 35.3907 - MinusLogProbMetric: 35.3907 - val_loss: 35.8824 - val_MinusLogProbMetric: 35.8824 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 85/1000
2023-10-25 17:45:19.758 
Epoch 85/1000 
	 loss: 35.9047, MinusLogProbMetric: 35.9047, val_loss: 35.0248, val_MinusLogProbMetric: 35.0248

Epoch 85: val_loss improved from 35.33867 to 35.02478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 35.9047 - MinusLogProbMetric: 35.9047 - val_loss: 35.0248 - val_MinusLogProbMetric: 35.0248 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 86/1000
2023-10-25 17:46:02.911 
Epoch 86/1000 
	 loss: 35.7413, MinusLogProbMetric: 35.7413, val_loss: 35.4556, val_MinusLogProbMetric: 35.4556

Epoch 86: val_loss did not improve from 35.02478
196/196 - 42s - loss: 35.7413 - MinusLogProbMetric: 35.7413 - val_loss: 35.4556 - val_MinusLogProbMetric: 35.4556 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 87/1000
2023-10-25 17:46:45.573 
Epoch 87/1000 
	 loss: 35.8966, MinusLogProbMetric: 35.8966, val_loss: 36.8288, val_MinusLogProbMetric: 36.8288

Epoch 87: val_loss did not improve from 35.02478
196/196 - 43s - loss: 35.8966 - MinusLogProbMetric: 35.8966 - val_loss: 36.8288 - val_MinusLogProbMetric: 36.8288 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 88/1000
2023-10-25 17:47:28.260 
Epoch 88/1000 
	 loss: 35.3332, MinusLogProbMetric: 35.3332, val_loss: 37.1566, val_MinusLogProbMetric: 37.1566

Epoch 88: val_loss did not improve from 35.02478
196/196 - 43s - loss: 35.3332 - MinusLogProbMetric: 35.3332 - val_loss: 37.1566 - val_MinusLogProbMetric: 37.1566 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 89/1000
2023-10-25 17:48:11.020 
Epoch 89/1000 
	 loss: 35.5460, MinusLogProbMetric: 35.5460, val_loss: 35.5946, val_MinusLogProbMetric: 35.5946

Epoch 89: val_loss did not improve from 35.02478
196/196 - 43s - loss: 35.5460 - MinusLogProbMetric: 35.5460 - val_loss: 35.5946 - val_MinusLogProbMetric: 35.5946 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 90/1000
2023-10-25 17:48:53.311 
Epoch 90/1000 
	 loss: 34.9908, MinusLogProbMetric: 34.9908, val_loss: 34.7201, val_MinusLogProbMetric: 34.7201

Epoch 90: val_loss improved from 35.02478 to 34.72007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 34.9908 - MinusLogProbMetric: 34.9908 - val_loss: 34.7201 - val_MinusLogProbMetric: 34.7201 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 91/1000
2023-10-25 17:49:36.331 
Epoch 91/1000 
	 loss: 35.1917, MinusLogProbMetric: 35.1917, val_loss: 34.9487, val_MinusLogProbMetric: 34.9487

Epoch 91: val_loss did not improve from 34.72007
196/196 - 42s - loss: 35.1917 - MinusLogProbMetric: 35.1917 - val_loss: 34.9487 - val_MinusLogProbMetric: 34.9487 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 92/1000
2023-10-25 17:50:18.884 
Epoch 92/1000 
	 loss: 34.7290, MinusLogProbMetric: 34.7290, val_loss: 34.5446, val_MinusLogProbMetric: 34.5446

Epoch 92: val_loss improved from 34.72007 to 34.54465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 34.7290 - MinusLogProbMetric: 34.7290 - val_loss: 34.5446 - val_MinusLogProbMetric: 34.5446 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 93/1000
2023-10-25 17:51:02.127 
Epoch 93/1000 
	 loss: 34.9722, MinusLogProbMetric: 34.9722, val_loss: 34.7673, val_MinusLogProbMetric: 34.7673

Epoch 93: val_loss did not improve from 34.54465
196/196 - 43s - loss: 34.9722 - MinusLogProbMetric: 34.9722 - val_loss: 34.7673 - val_MinusLogProbMetric: 34.7673 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 94/1000
2023-10-25 17:51:44.601 
Epoch 94/1000 
	 loss: 34.7920, MinusLogProbMetric: 34.7920, val_loss: 35.5905, val_MinusLogProbMetric: 35.5905

Epoch 94: val_loss did not improve from 34.54465
196/196 - 42s - loss: 34.7920 - MinusLogProbMetric: 34.7920 - val_loss: 35.5905 - val_MinusLogProbMetric: 35.5905 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 95/1000
2023-10-25 17:52:26.926 
Epoch 95/1000 
	 loss: 34.8495, MinusLogProbMetric: 34.8495, val_loss: 35.2107, val_MinusLogProbMetric: 35.2107

Epoch 95: val_loss did not improve from 34.54465
196/196 - 42s - loss: 34.8495 - MinusLogProbMetric: 34.8495 - val_loss: 35.2107 - val_MinusLogProbMetric: 35.2107 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 96/1000
2023-10-25 17:53:09.686 
Epoch 96/1000 
	 loss: 35.0614, MinusLogProbMetric: 35.0614, val_loss: 35.9232, val_MinusLogProbMetric: 35.9232

Epoch 96: val_loss did not improve from 34.54465
196/196 - 43s - loss: 35.0614 - MinusLogProbMetric: 35.0614 - val_loss: 35.9232 - val_MinusLogProbMetric: 35.9232 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 97/1000
2023-10-25 17:53:52.842 
Epoch 97/1000 
	 loss: 34.5524, MinusLogProbMetric: 34.5524, val_loss: 34.9773, val_MinusLogProbMetric: 34.9773

Epoch 97: val_loss did not improve from 34.54465
196/196 - 43s - loss: 34.5524 - MinusLogProbMetric: 34.5524 - val_loss: 34.9773 - val_MinusLogProbMetric: 34.9773 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 98/1000
2023-10-25 17:54:35.647 
Epoch 98/1000 
	 loss: 35.8444, MinusLogProbMetric: 35.8444, val_loss: 34.7168, val_MinusLogProbMetric: 34.7168

Epoch 98: val_loss did not improve from 34.54465
196/196 - 43s - loss: 35.8444 - MinusLogProbMetric: 35.8444 - val_loss: 34.7168 - val_MinusLogProbMetric: 34.7168 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 99/1000
2023-10-25 17:55:17.436 
Epoch 99/1000 
	 loss: 34.5387, MinusLogProbMetric: 34.5387, val_loss: 34.6922, val_MinusLogProbMetric: 34.6922

Epoch 99: val_loss did not improve from 34.54465
196/196 - 42s - loss: 34.5387 - MinusLogProbMetric: 34.5387 - val_loss: 34.6922 - val_MinusLogProbMetric: 34.6922 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 100/1000
2023-10-25 17:55:51.919 
Epoch 100/1000 
	 loss: 34.8434, MinusLogProbMetric: 34.8434, val_loss: 34.5922, val_MinusLogProbMetric: 34.5922

Epoch 100: val_loss did not improve from 34.54465
196/196 - 34s - loss: 34.8434 - MinusLogProbMetric: 34.8434 - val_loss: 34.5922 - val_MinusLogProbMetric: 34.5922 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 101/1000
2023-10-25 17:56:27.533 
Epoch 101/1000 
	 loss: 34.6153, MinusLogProbMetric: 34.6153, val_loss: 35.2891, val_MinusLogProbMetric: 35.2891

Epoch 101: val_loss did not improve from 34.54465
196/196 - 36s - loss: 34.6153 - MinusLogProbMetric: 34.6153 - val_loss: 35.2891 - val_MinusLogProbMetric: 35.2891 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 102/1000
2023-10-25 17:57:07.305 
Epoch 102/1000 
	 loss: 38.4872, MinusLogProbMetric: 38.4872, val_loss: 49.1841, val_MinusLogProbMetric: 49.1841

Epoch 102: val_loss did not improve from 34.54465
196/196 - 40s - loss: 38.4872 - MinusLogProbMetric: 38.4872 - val_loss: 49.1841 - val_MinusLogProbMetric: 49.1841 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 103/1000
2023-10-25 17:57:48.249 
Epoch 103/1000 
	 loss: 36.6509, MinusLogProbMetric: 36.6509, val_loss: 35.0951, val_MinusLogProbMetric: 35.0951

Epoch 103: val_loss did not improve from 34.54465
196/196 - 41s - loss: 36.6509 - MinusLogProbMetric: 36.6509 - val_loss: 35.0951 - val_MinusLogProbMetric: 35.0951 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 104/1000
2023-10-25 17:58:25.884 
Epoch 104/1000 
	 loss: 34.3828, MinusLogProbMetric: 34.3828, val_loss: 35.1733, val_MinusLogProbMetric: 35.1733

Epoch 104: val_loss did not improve from 34.54465
196/196 - 38s - loss: 34.3828 - MinusLogProbMetric: 34.3828 - val_loss: 35.1733 - val_MinusLogProbMetric: 35.1733 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 105/1000
2023-10-25 17:59:06.714 
Epoch 105/1000 
	 loss: 34.2635, MinusLogProbMetric: 34.2635, val_loss: 34.5831, val_MinusLogProbMetric: 34.5831

Epoch 105: val_loss did not improve from 34.54465
196/196 - 41s - loss: 34.2635 - MinusLogProbMetric: 34.2635 - val_loss: 34.5831 - val_MinusLogProbMetric: 34.5831 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 106/1000
2023-10-25 17:59:46.906 
Epoch 106/1000 
	 loss: 34.1321, MinusLogProbMetric: 34.1321, val_loss: 34.4778, val_MinusLogProbMetric: 34.4778

Epoch 106: val_loss improved from 34.54465 to 34.47780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 34.1321 - MinusLogProbMetric: 34.1321 - val_loss: 34.4778 - val_MinusLogProbMetric: 34.4778 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 107/1000
2023-10-25 18:00:27.553 
Epoch 107/1000 
	 loss: 34.1290, MinusLogProbMetric: 34.1290, val_loss: 35.3426, val_MinusLogProbMetric: 35.3426

Epoch 107: val_loss did not improve from 34.47780
196/196 - 40s - loss: 34.1290 - MinusLogProbMetric: 34.1290 - val_loss: 35.3426 - val_MinusLogProbMetric: 35.3426 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 108/1000
2023-10-25 18:01:07.370 
Epoch 108/1000 
	 loss: 33.9712, MinusLogProbMetric: 33.9712, val_loss: 33.9518, val_MinusLogProbMetric: 33.9518

Epoch 108: val_loss improved from 34.47780 to 33.95182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 33.9712 - MinusLogProbMetric: 33.9712 - val_loss: 33.9518 - val_MinusLogProbMetric: 33.9518 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 109/1000
2023-10-25 18:01:44.964 
Epoch 109/1000 
	 loss: 34.3048, MinusLogProbMetric: 34.3048, val_loss: 35.0535, val_MinusLogProbMetric: 35.0535

Epoch 109: val_loss did not improve from 33.95182
196/196 - 37s - loss: 34.3048 - MinusLogProbMetric: 34.3048 - val_loss: 35.0535 - val_MinusLogProbMetric: 35.0535 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 110/1000
2023-10-25 18:02:20.025 
Epoch 110/1000 
	 loss: 34.0348, MinusLogProbMetric: 34.0348, val_loss: 34.6119, val_MinusLogProbMetric: 34.6119

Epoch 110: val_loss did not improve from 33.95182
196/196 - 35s - loss: 34.0348 - MinusLogProbMetric: 34.0348 - val_loss: 34.6119 - val_MinusLogProbMetric: 34.6119 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 111/1000
2023-10-25 18:02:59.327 
Epoch 111/1000 
	 loss: 35.3367, MinusLogProbMetric: 35.3367, val_loss: 34.9139, val_MinusLogProbMetric: 34.9139

Epoch 111: val_loss did not improve from 33.95182
196/196 - 39s - loss: 35.3367 - MinusLogProbMetric: 35.3367 - val_loss: 34.9139 - val_MinusLogProbMetric: 34.9139 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 112/1000
2023-10-25 18:03:39.446 
Epoch 112/1000 
	 loss: 33.9905, MinusLogProbMetric: 33.9905, val_loss: 33.9668, val_MinusLogProbMetric: 33.9668

Epoch 112: val_loss did not improve from 33.95182
196/196 - 40s - loss: 33.9905 - MinusLogProbMetric: 33.9905 - val_loss: 33.9668 - val_MinusLogProbMetric: 33.9668 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 113/1000
2023-10-25 18:04:16.696 
Epoch 113/1000 
	 loss: 33.9894, MinusLogProbMetric: 33.9894, val_loss: 33.7980, val_MinusLogProbMetric: 33.7980

Epoch 113: val_loss improved from 33.95182 to 33.79799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 38s - loss: 33.9894 - MinusLogProbMetric: 33.9894 - val_loss: 33.7980 - val_MinusLogProbMetric: 33.7980 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 114/1000
2023-10-25 18:04:55.097 
Epoch 114/1000 
	 loss: 33.9292, MinusLogProbMetric: 33.9292, val_loss: 34.3430, val_MinusLogProbMetric: 34.3430

Epoch 114: val_loss did not improve from 33.79799
196/196 - 38s - loss: 33.9292 - MinusLogProbMetric: 33.9292 - val_loss: 34.3430 - val_MinusLogProbMetric: 34.3430 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 115/1000
2023-10-25 18:05:34.754 
Epoch 115/1000 
	 loss: 33.9642, MinusLogProbMetric: 33.9642, val_loss: 35.6682, val_MinusLogProbMetric: 35.6682

Epoch 115: val_loss did not improve from 33.79799
196/196 - 40s - loss: 33.9642 - MinusLogProbMetric: 33.9642 - val_loss: 35.6682 - val_MinusLogProbMetric: 35.6682 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 116/1000
2023-10-25 18:06:16.116 
Epoch 116/1000 
	 loss: 33.7524, MinusLogProbMetric: 33.7524, val_loss: 35.2801, val_MinusLogProbMetric: 35.2801

Epoch 116: val_loss did not improve from 33.79799
196/196 - 41s - loss: 33.7524 - MinusLogProbMetric: 33.7524 - val_loss: 35.2801 - val_MinusLogProbMetric: 35.2801 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 117/1000
2023-10-25 18:06:56.431 
Epoch 117/1000 
	 loss: 33.7502, MinusLogProbMetric: 33.7502, val_loss: 34.2526, val_MinusLogProbMetric: 34.2526

Epoch 117: val_loss did not improve from 33.79799
196/196 - 40s - loss: 33.7502 - MinusLogProbMetric: 33.7502 - val_loss: 34.2526 - val_MinusLogProbMetric: 34.2526 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 118/1000
2023-10-25 18:07:37.618 
Epoch 118/1000 
	 loss: 33.6264, MinusLogProbMetric: 33.6264, val_loss: 33.4756, val_MinusLogProbMetric: 33.4756

Epoch 118: val_loss improved from 33.79799 to 33.47564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 33.6264 - MinusLogProbMetric: 33.6264 - val_loss: 33.4756 - val_MinusLogProbMetric: 33.4756 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 119/1000
2023-10-25 18:08:18.297 
Epoch 119/1000 
	 loss: 33.8361, MinusLogProbMetric: 33.8361, val_loss: 33.9217, val_MinusLogProbMetric: 33.9217

Epoch 119: val_loss did not improve from 33.47564
196/196 - 40s - loss: 33.8361 - MinusLogProbMetric: 33.8361 - val_loss: 33.9217 - val_MinusLogProbMetric: 33.9217 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 120/1000
2023-10-25 18:08:56.961 
Epoch 120/1000 
	 loss: 33.9686, MinusLogProbMetric: 33.9686, val_loss: 33.6858, val_MinusLogProbMetric: 33.6858

Epoch 120: val_loss did not improve from 33.47564
196/196 - 39s - loss: 33.9686 - MinusLogProbMetric: 33.9686 - val_loss: 33.6858 - val_MinusLogProbMetric: 33.6858 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 121/1000
2023-10-25 18:09:37.071 
Epoch 121/1000 
	 loss: 33.7861, MinusLogProbMetric: 33.7861, val_loss: 33.6217, val_MinusLogProbMetric: 33.6217

Epoch 121: val_loss did not improve from 33.47564
196/196 - 40s - loss: 33.7861 - MinusLogProbMetric: 33.7861 - val_loss: 33.6217 - val_MinusLogProbMetric: 33.6217 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 122/1000
2023-10-25 18:10:18.015 
Epoch 122/1000 
	 loss: 33.6994, MinusLogProbMetric: 33.6994, val_loss: 33.1807, val_MinusLogProbMetric: 33.1807

Epoch 122: val_loss improved from 33.47564 to 33.18075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 33.6994 - MinusLogProbMetric: 33.6994 - val_loss: 33.1807 - val_MinusLogProbMetric: 33.1807 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 123/1000
2023-10-25 18:10:55.182 
Epoch 123/1000 
	 loss: 33.8514, MinusLogProbMetric: 33.8514, val_loss: 33.3563, val_MinusLogProbMetric: 33.3563

Epoch 123: val_loss did not improve from 33.18075
196/196 - 36s - loss: 33.8514 - MinusLogProbMetric: 33.8514 - val_loss: 33.3563 - val_MinusLogProbMetric: 33.3563 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 124/1000
2023-10-25 18:11:32.149 
Epoch 124/1000 
	 loss: 33.5959, MinusLogProbMetric: 33.5959, val_loss: 35.5434, val_MinusLogProbMetric: 35.5434

Epoch 124: val_loss did not improve from 33.18075
196/196 - 37s - loss: 33.5959 - MinusLogProbMetric: 33.5959 - val_loss: 35.5434 - val_MinusLogProbMetric: 35.5434 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 125/1000
2023-10-25 18:12:13.667 
Epoch 125/1000 
	 loss: 34.0738, MinusLogProbMetric: 34.0738, val_loss: 33.2724, val_MinusLogProbMetric: 33.2724

Epoch 125: val_loss did not improve from 33.18075
196/196 - 42s - loss: 34.0738 - MinusLogProbMetric: 34.0738 - val_loss: 33.2724 - val_MinusLogProbMetric: 33.2724 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 126/1000
2023-10-25 18:12:50.338 
Epoch 126/1000 
	 loss: 33.8375, MinusLogProbMetric: 33.8375, val_loss: 33.3809, val_MinusLogProbMetric: 33.3809

Epoch 126: val_loss did not improve from 33.18075
196/196 - 37s - loss: 33.8375 - MinusLogProbMetric: 33.8375 - val_loss: 33.3809 - val_MinusLogProbMetric: 33.3809 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 127/1000
2023-10-25 18:13:26.164 
Epoch 127/1000 
	 loss: 33.5124, MinusLogProbMetric: 33.5124, val_loss: 32.9431, val_MinusLogProbMetric: 32.9431

Epoch 127: val_loss improved from 33.18075 to 32.94312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 37s - loss: 33.5124 - MinusLogProbMetric: 33.5124 - val_loss: 32.9431 - val_MinusLogProbMetric: 32.9431 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 128/1000
2023-10-25 18:14:04.506 
Epoch 128/1000 
	 loss: 33.4385, MinusLogProbMetric: 33.4385, val_loss: 33.7235, val_MinusLogProbMetric: 33.7235

Epoch 128: val_loss did not improve from 32.94312
196/196 - 38s - loss: 33.4385 - MinusLogProbMetric: 33.4385 - val_loss: 33.7235 - val_MinusLogProbMetric: 33.7235 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 129/1000
2023-10-25 18:14:45.173 
Epoch 129/1000 
	 loss: 33.2943, MinusLogProbMetric: 33.2943, val_loss: 33.3419, val_MinusLogProbMetric: 33.3419

Epoch 129: val_loss did not improve from 32.94312
196/196 - 41s - loss: 33.2943 - MinusLogProbMetric: 33.2943 - val_loss: 33.3419 - val_MinusLogProbMetric: 33.3419 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 130/1000
2023-10-25 18:15:27.173 
Epoch 130/1000 
	 loss: 33.5086, MinusLogProbMetric: 33.5086, val_loss: 33.3495, val_MinusLogProbMetric: 33.3495

Epoch 130: val_loss did not improve from 32.94312
196/196 - 42s - loss: 33.5086 - MinusLogProbMetric: 33.5086 - val_loss: 33.3495 - val_MinusLogProbMetric: 33.3495 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 131/1000
2023-10-25 18:16:04.145 
Epoch 131/1000 
	 loss: 33.6466, MinusLogProbMetric: 33.6466, val_loss: 33.3781, val_MinusLogProbMetric: 33.3781

Epoch 131: val_loss did not improve from 32.94312
196/196 - 37s - loss: 33.6466 - MinusLogProbMetric: 33.6466 - val_loss: 33.3781 - val_MinusLogProbMetric: 33.3781 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 132/1000
2023-10-25 18:16:42.411 
Epoch 132/1000 
	 loss: 33.1426, MinusLogProbMetric: 33.1426, val_loss: 34.9708, val_MinusLogProbMetric: 34.9708

Epoch 132: val_loss did not improve from 32.94312
196/196 - 38s - loss: 33.1426 - MinusLogProbMetric: 33.1426 - val_loss: 34.9708 - val_MinusLogProbMetric: 34.9708 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 133/1000
2023-10-25 18:17:20.134 
Epoch 133/1000 
	 loss: 33.5374, MinusLogProbMetric: 33.5374, val_loss: 33.2394, val_MinusLogProbMetric: 33.2394

Epoch 133: val_loss did not improve from 32.94312
196/196 - 38s - loss: 33.5374 - MinusLogProbMetric: 33.5374 - val_loss: 33.2394 - val_MinusLogProbMetric: 33.2394 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 134/1000
2023-10-25 18:18:01.819 
Epoch 134/1000 
	 loss: 33.3655, MinusLogProbMetric: 33.3655, val_loss: 35.9114, val_MinusLogProbMetric: 35.9114

Epoch 134: val_loss did not improve from 32.94312
196/196 - 42s - loss: 33.3655 - MinusLogProbMetric: 33.3655 - val_loss: 35.9114 - val_MinusLogProbMetric: 35.9114 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 135/1000
2023-10-25 18:18:40.607 
Epoch 135/1000 
	 loss: 33.2887, MinusLogProbMetric: 33.2887, val_loss: 33.3448, val_MinusLogProbMetric: 33.3448

Epoch 135: val_loss did not improve from 32.94312
196/196 - 39s - loss: 33.2887 - MinusLogProbMetric: 33.2887 - val_loss: 33.3448 - val_MinusLogProbMetric: 33.3448 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 136/1000
2023-10-25 18:19:20.087 
Epoch 136/1000 
	 loss: 33.4703, MinusLogProbMetric: 33.4703, val_loss: 33.7444, val_MinusLogProbMetric: 33.7444

Epoch 136: val_loss did not improve from 32.94312
196/196 - 39s - loss: 33.4703 - MinusLogProbMetric: 33.4703 - val_loss: 33.7444 - val_MinusLogProbMetric: 33.7444 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 137/1000
2023-10-25 18:19:59.273 
Epoch 137/1000 
	 loss: 33.2765, MinusLogProbMetric: 33.2765, val_loss: 33.1282, val_MinusLogProbMetric: 33.1282

Epoch 137: val_loss did not improve from 32.94312
196/196 - 39s - loss: 33.2765 - MinusLogProbMetric: 33.2765 - val_loss: 33.1282 - val_MinusLogProbMetric: 33.1282 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 138/1000
2023-10-25 18:20:40.218 
Epoch 138/1000 
	 loss: 33.4435, MinusLogProbMetric: 33.4435, val_loss: 33.4356, val_MinusLogProbMetric: 33.4356

Epoch 138: val_loss did not improve from 32.94312
196/196 - 41s - loss: 33.4435 - MinusLogProbMetric: 33.4435 - val_loss: 33.4356 - val_MinusLogProbMetric: 33.4356 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 139/1000
2023-10-25 18:21:20.690 
Epoch 139/1000 
	 loss: 33.1844, MinusLogProbMetric: 33.1844, val_loss: 34.0556, val_MinusLogProbMetric: 34.0556

Epoch 139: val_loss did not improve from 32.94312
196/196 - 40s - loss: 33.1844 - MinusLogProbMetric: 33.1844 - val_loss: 34.0556 - val_MinusLogProbMetric: 34.0556 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 140/1000
2023-10-25 18:22:01.061 
Epoch 140/1000 
	 loss: 33.2060, MinusLogProbMetric: 33.2060, val_loss: 34.3589, val_MinusLogProbMetric: 34.3589

Epoch 140: val_loss did not improve from 32.94312
196/196 - 40s - loss: 33.2060 - MinusLogProbMetric: 33.2060 - val_loss: 34.3589 - val_MinusLogProbMetric: 34.3589 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 141/1000
2023-10-25 18:22:38.069 
Epoch 141/1000 
	 loss: 33.0981, MinusLogProbMetric: 33.0981, val_loss: 33.0375, val_MinusLogProbMetric: 33.0375

Epoch 141: val_loss did not improve from 32.94312
196/196 - 37s - loss: 33.0981 - MinusLogProbMetric: 33.0981 - val_loss: 33.0375 - val_MinusLogProbMetric: 33.0375 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 142/1000
2023-10-25 18:23:14.811 
Epoch 142/1000 
	 loss: 33.4128, MinusLogProbMetric: 33.4128, val_loss: 33.3211, val_MinusLogProbMetric: 33.3211

Epoch 142: val_loss did not improve from 32.94312
196/196 - 37s - loss: 33.4128 - MinusLogProbMetric: 33.4128 - val_loss: 33.3211 - val_MinusLogProbMetric: 33.3211 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 143/1000
2023-10-25 18:23:56.496 
Epoch 143/1000 
	 loss: 32.9359, MinusLogProbMetric: 32.9359, val_loss: 33.5549, val_MinusLogProbMetric: 33.5549

Epoch 143: val_loss did not improve from 32.94312
196/196 - 42s - loss: 32.9359 - MinusLogProbMetric: 32.9359 - val_loss: 33.5549 - val_MinusLogProbMetric: 33.5549 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 144/1000
2023-10-25 18:24:37.643 
Epoch 144/1000 
	 loss: 33.0860, MinusLogProbMetric: 33.0860, val_loss: 32.7512, val_MinusLogProbMetric: 32.7512

Epoch 144: val_loss improved from 32.94312 to 32.75115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 33.0860 - MinusLogProbMetric: 33.0860 - val_loss: 32.7512 - val_MinusLogProbMetric: 32.7512 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 145/1000
2023-10-25 18:25:15.145 
Epoch 145/1000 
	 loss: 33.1092, MinusLogProbMetric: 33.1092, val_loss: 32.9148, val_MinusLogProbMetric: 32.9148

Epoch 145: val_loss did not improve from 32.75115
196/196 - 37s - loss: 33.1092 - MinusLogProbMetric: 33.1092 - val_loss: 32.9148 - val_MinusLogProbMetric: 32.9148 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 146/1000
2023-10-25 18:25:51.608 
Epoch 146/1000 
	 loss: 33.3822, MinusLogProbMetric: 33.3822, val_loss: 33.2563, val_MinusLogProbMetric: 33.2563

Epoch 146: val_loss did not improve from 32.75115
196/196 - 36s - loss: 33.3822 - MinusLogProbMetric: 33.3822 - val_loss: 33.2563 - val_MinusLogProbMetric: 33.2563 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 147/1000
2023-10-25 18:26:33.715 
Epoch 147/1000 
	 loss: 32.9573, MinusLogProbMetric: 32.9573, val_loss: 34.4466, val_MinusLogProbMetric: 34.4466

Epoch 147: val_loss did not improve from 32.75115
196/196 - 42s - loss: 32.9573 - MinusLogProbMetric: 32.9573 - val_loss: 34.4466 - val_MinusLogProbMetric: 34.4466 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 148/1000
2023-10-25 18:27:13.752 
Epoch 148/1000 
	 loss: 33.8108, MinusLogProbMetric: 33.8108, val_loss: 33.0685, val_MinusLogProbMetric: 33.0685

Epoch 148: val_loss did not improve from 32.75115
196/196 - 40s - loss: 33.8108 - MinusLogProbMetric: 33.8108 - val_loss: 33.0685 - val_MinusLogProbMetric: 33.0685 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 149/1000
2023-10-25 18:27:50.271 
Epoch 149/1000 
	 loss: 32.8364, MinusLogProbMetric: 32.8364, val_loss: 33.8292, val_MinusLogProbMetric: 33.8292

Epoch 149: val_loss did not improve from 32.75115
196/196 - 37s - loss: 32.8364 - MinusLogProbMetric: 32.8364 - val_loss: 33.8292 - val_MinusLogProbMetric: 33.8292 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 150/1000
2023-10-25 18:28:28.248 
Epoch 150/1000 
	 loss: 32.7830, MinusLogProbMetric: 32.7830, val_loss: 37.7787, val_MinusLogProbMetric: 37.7787

Epoch 150: val_loss did not improve from 32.75115
196/196 - 38s - loss: 32.7830 - MinusLogProbMetric: 32.7830 - val_loss: 37.7787 - val_MinusLogProbMetric: 37.7787 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 151/1000
2023-10-25 18:29:08.610 
Epoch 151/1000 
	 loss: 32.9868, MinusLogProbMetric: 32.9868, val_loss: 32.8572, val_MinusLogProbMetric: 32.8572

Epoch 151: val_loss did not improve from 32.75115
196/196 - 40s - loss: 32.9868 - MinusLogProbMetric: 32.9868 - val_loss: 32.8572 - val_MinusLogProbMetric: 32.8572 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 152/1000
2023-10-25 18:29:50.492 
Epoch 152/1000 
	 loss: 32.9356, MinusLogProbMetric: 32.9356, val_loss: 32.7431, val_MinusLogProbMetric: 32.7431

Epoch 152: val_loss improved from 32.75115 to 32.74311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 32.9356 - MinusLogProbMetric: 32.9356 - val_loss: 32.7431 - val_MinusLogProbMetric: 32.7431 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 153/1000
2023-10-25 18:30:27.918 
Epoch 153/1000 
	 loss: 33.1233, MinusLogProbMetric: 33.1233, val_loss: 32.9656, val_MinusLogProbMetric: 32.9656

Epoch 153: val_loss did not improve from 32.74311
196/196 - 37s - loss: 33.1233 - MinusLogProbMetric: 33.1233 - val_loss: 32.9656 - val_MinusLogProbMetric: 32.9656 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 154/1000
2023-10-25 18:31:05.616 
Epoch 154/1000 
	 loss: 32.7636, MinusLogProbMetric: 32.7636, val_loss: 32.9723, val_MinusLogProbMetric: 32.9723

Epoch 154: val_loss did not improve from 32.74311
196/196 - 38s - loss: 32.7636 - MinusLogProbMetric: 32.7636 - val_loss: 32.9723 - val_MinusLogProbMetric: 32.9723 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 155/1000
2023-10-25 18:31:41.357 
Epoch 155/1000 
	 loss: 32.9421, MinusLogProbMetric: 32.9421, val_loss: 35.6148, val_MinusLogProbMetric: 35.6148

Epoch 155: val_loss did not improve from 32.74311
196/196 - 36s - loss: 32.9421 - MinusLogProbMetric: 32.9421 - val_loss: 35.6148 - val_MinusLogProbMetric: 35.6148 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 156/1000
2023-10-25 18:32:23.137 
Epoch 156/1000 
	 loss: 32.6422, MinusLogProbMetric: 32.6422, val_loss: 32.9065, val_MinusLogProbMetric: 32.9065

Epoch 156: val_loss did not improve from 32.74311
196/196 - 42s - loss: 32.6422 - MinusLogProbMetric: 32.6422 - val_loss: 32.9065 - val_MinusLogProbMetric: 32.9065 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 157/1000
2023-10-25 18:33:04.088 
Epoch 157/1000 
	 loss: 32.7163, MinusLogProbMetric: 32.7163, val_loss: 33.2964, val_MinusLogProbMetric: 33.2964

Epoch 157: val_loss did not improve from 32.74311
196/196 - 41s - loss: 32.7163 - MinusLogProbMetric: 32.7163 - val_loss: 33.2964 - val_MinusLogProbMetric: 33.2964 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 158/1000
2023-10-25 18:33:45.096 
Epoch 158/1000 
	 loss: 32.8447, MinusLogProbMetric: 32.8447, val_loss: 32.5786, val_MinusLogProbMetric: 32.5786

Epoch 158: val_loss improved from 32.74311 to 32.57857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 32.8447 - MinusLogProbMetric: 32.8447 - val_loss: 32.5786 - val_MinusLogProbMetric: 32.5786 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 159/1000
2023-10-25 18:34:24.472 
Epoch 159/1000 
	 loss: 32.4664, MinusLogProbMetric: 32.4664, val_loss: 32.3558, val_MinusLogProbMetric: 32.3558

Epoch 159: val_loss improved from 32.57857 to 32.35582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 39s - loss: 32.4664 - MinusLogProbMetric: 32.4664 - val_loss: 32.3558 - val_MinusLogProbMetric: 32.3558 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 160/1000
2023-10-25 18:35:04.660 
Epoch 160/1000 
	 loss: 32.6806, MinusLogProbMetric: 32.6806, val_loss: 34.4147, val_MinusLogProbMetric: 34.4147

Epoch 160: val_loss did not improve from 32.35582
196/196 - 40s - loss: 32.6806 - MinusLogProbMetric: 32.6806 - val_loss: 34.4147 - val_MinusLogProbMetric: 34.4147 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 161/1000
2023-10-25 18:35:46.503 
Epoch 161/1000 
	 loss: 32.5151, MinusLogProbMetric: 32.5151, val_loss: 32.8689, val_MinusLogProbMetric: 32.8689

Epoch 161: val_loss did not improve from 32.35582
196/196 - 42s - loss: 32.5151 - MinusLogProbMetric: 32.5151 - val_loss: 32.8689 - val_MinusLogProbMetric: 32.8689 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 162/1000
2023-10-25 18:36:27.725 
Epoch 162/1000 
	 loss: 32.7387, MinusLogProbMetric: 32.7387, val_loss: 33.7993, val_MinusLogProbMetric: 33.7993

Epoch 162: val_loss did not improve from 32.35582
196/196 - 41s - loss: 32.7387 - MinusLogProbMetric: 32.7387 - val_loss: 33.7993 - val_MinusLogProbMetric: 33.7993 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 163/1000
2023-10-25 18:37:04.327 
Epoch 163/1000 
	 loss: 32.9121, MinusLogProbMetric: 32.9121, val_loss: 32.5172, val_MinusLogProbMetric: 32.5172

Epoch 163: val_loss did not improve from 32.35582
196/196 - 37s - loss: 32.9121 - MinusLogProbMetric: 32.9121 - val_loss: 32.5172 - val_MinusLogProbMetric: 32.5172 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 164/1000
2023-10-25 18:37:41.470 
Epoch 164/1000 
	 loss: 32.4525, MinusLogProbMetric: 32.4525, val_loss: 35.8523, val_MinusLogProbMetric: 35.8523

Epoch 164: val_loss did not improve from 32.35582
196/196 - 37s - loss: 32.4525 - MinusLogProbMetric: 32.4525 - val_loss: 35.8523 - val_MinusLogProbMetric: 35.8523 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 165/1000
2023-10-25 18:38:22.343 
Epoch 165/1000 
	 loss: 32.4489, MinusLogProbMetric: 32.4489, val_loss: 33.0032, val_MinusLogProbMetric: 33.0032

Epoch 165: val_loss did not improve from 32.35582
196/196 - 41s - loss: 32.4489 - MinusLogProbMetric: 32.4489 - val_loss: 33.0032 - val_MinusLogProbMetric: 33.0032 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 166/1000
2023-10-25 18:39:02.690 
Epoch 166/1000 
	 loss: 32.6516, MinusLogProbMetric: 32.6516, val_loss: 32.8859, val_MinusLogProbMetric: 32.8859

Epoch 166: val_loss did not improve from 32.35582
196/196 - 40s - loss: 32.6516 - MinusLogProbMetric: 32.6516 - val_loss: 32.8859 - val_MinusLogProbMetric: 32.8859 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 167/1000
2023-10-25 18:39:41.317 
Epoch 167/1000 
	 loss: 32.6164, MinusLogProbMetric: 32.6164, val_loss: 33.8757, val_MinusLogProbMetric: 33.8757

Epoch 167: val_loss did not improve from 32.35582
196/196 - 39s - loss: 32.6164 - MinusLogProbMetric: 32.6164 - val_loss: 33.8757 - val_MinusLogProbMetric: 33.8757 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 168/1000
2023-10-25 18:40:20.925 
Epoch 168/1000 
	 loss: 32.5442, MinusLogProbMetric: 32.5442, val_loss: 32.8941, val_MinusLogProbMetric: 32.8941

Epoch 168: val_loss did not improve from 32.35582
196/196 - 40s - loss: 32.5442 - MinusLogProbMetric: 32.5442 - val_loss: 32.8941 - val_MinusLogProbMetric: 32.8941 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 169/1000
2023-10-25 18:41:01.332 
Epoch 169/1000 
	 loss: 32.4584, MinusLogProbMetric: 32.4584, val_loss: 32.8135, val_MinusLogProbMetric: 32.8135

Epoch 169: val_loss did not improve from 32.35582
196/196 - 40s - loss: 32.4584 - MinusLogProbMetric: 32.4584 - val_loss: 32.8135 - val_MinusLogProbMetric: 32.8135 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 170/1000
2023-10-25 18:41:42.934 
Epoch 170/1000 
	 loss: 32.4743, MinusLogProbMetric: 32.4743, val_loss: 33.5192, val_MinusLogProbMetric: 33.5192

Epoch 170: val_loss did not improve from 32.35582
196/196 - 42s - loss: 32.4743 - MinusLogProbMetric: 32.4743 - val_loss: 33.5192 - val_MinusLogProbMetric: 33.5192 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 171/1000
2023-10-25 18:42:20.632 
Epoch 171/1000 
	 loss: 32.4888, MinusLogProbMetric: 32.4888, val_loss: 32.5495, val_MinusLogProbMetric: 32.5495

Epoch 171: val_loss did not improve from 32.35582
196/196 - 38s - loss: 32.4888 - MinusLogProbMetric: 32.4888 - val_loss: 32.5495 - val_MinusLogProbMetric: 32.5495 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 172/1000
2023-10-25 18:42:59.622 
Epoch 172/1000 
	 loss: 32.4444, MinusLogProbMetric: 32.4444, val_loss: 32.4585, val_MinusLogProbMetric: 32.4585

Epoch 172: val_loss did not improve from 32.35582
196/196 - 39s - loss: 32.4444 - MinusLogProbMetric: 32.4444 - val_loss: 32.4585 - val_MinusLogProbMetric: 32.4585 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 173/1000
2023-10-25 18:43:38.529 
Epoch 173/1000 
	 loss: 32.4502, MinusLogProbMetric: 32.4502, val_loss: 33.9016, val_MinusLogProbMetric: 33.9016

Epoch 173: val_loss did not improve from 32.35582
196/196 - 39s - loss: 32.4502 - MinusLogProbMetric: 32.4502 - val_loss: 33.9016 - val_MinusLogProbMetric: 33.9016 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 174/1000
2023-10-25 18:44:15.970 
Epoch 174/1000 
	 loss: 32.2327, MinusLogProbMetric: 32.2327, val_loss: 32.2262, val_MinusLogProbMetric: 32.2262

Epoch 174: val_loss improved from 32.35582 to 32.22619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 38s - loss: 32.2327 - MinusLogProbMetric: 32.2327 - val_loss: 32.2262 - val_MinusLogProbMetric: 32.2262 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 175/1000
2023-10-25 18:44:58.674 
Epoch 175/1000 
	 loss: 32.3328, MinusLogProbMetric: 32.3328, val_loss: 33.5104, val_MinusLogProbMetric: 33.5104

Epoch 175: val_loss did not improve from 32.22619
196/196 - 42s - loss: 32.3328 - MinusLogProbMetric: 32.3328 - val_loss: 33.5104 - val_MinusLogProbMetric: 33.5104 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 176/1000
2023-10-25 18:45:40.517 
Epoch 176/1000 
	 loss: 32.3374, MinusLogProbMetric: 32.3374, val_loss: 32.4266, val_MinusLogProbMetric: 32.4266

Epoch 176: val_loss did not improve from 32.22619
196/196 - 42s - loss: 32.3374 - MinusLogProbMetric: 32.3374 - val_loss: 32.4266 - val_MinusLogProbMetric: 32.4266 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 177/1000
2023-10-25 18:46:20.122 
Epoch 177/1000 
	 loss: 32.3977, MinusLogProbMetric: 32.3977, val_loss: 32.4374, val_MinusLogProbMetric: 32.4374

Epoch 177: val_loss did not improve from 32.22619
196/196 - 40s - loss: 32.3977 - MinusLogProbMetric: 32.3977 - val_loss: 32.4374 - val_MinusLogProbMetric: 32.4374 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 178/1000
2023-10-25 18:46:55.690 
Epoch 178/1000 
	 loss: 32.5645, MinusLogProbMetric: 32.5645, val_loss: 33.2914, val_MinusLogProbMetric: 33.2914

Epoch 178: val_loss did not improve from 32.22619
196/196 - 36s - loss: 32.5645 - MinusLogProbMetric: 32.5645 - val_loss: 33.2914 - val_MinusLogProbMetric: 33.2914 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 179/1000
2023-10-25 18:47:29.420 
Epoch 179/1000 
	 loss: 32.2031, MinusLogProbMetric: 32.2031, val_loss: 32.7316, val_MinusLogProbMetric: 32.7316

Epoch 179: val_loss did not improve from 32.22619
196/196 - 34s - loss: 32.2031 - MinusLogProbMetric: 32.2031 - val_loss: 32.7316 - val_MinusLogProbMetric: 32.7316 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 180/1000
2023-10-25 18:48:05.292 
Epoch 180/1000 
	 loss: 33.7241, MinusLogProbMetric: 33.7241, val_loss: 32.6196, val_MinusLogProbMetric: 32.6196

Epoch 180: val_loss did not improve from 32.22619
196/196 - 36s - loss: 33.7241 - MinusLogProbMetric: 33.7241 - val_loss: 32.6196 - val_MinusLogProbMetric: 32.6196 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 181/1000
2023-10-25 18:48:44.681 
Epoch 181/1000 
	 loss: 32.3467, MinusLogProbMetric: 32.3467, val_loss: 32.4339, val_MinusLogProbMetric: 32.4339

Epoch 181: val_loss did not improve from 32.22619
196/196 - 39s - loss: 32.3467 - MinusLogProbMetric: 32.3467 - val_loss: 32.4339 - val_MinusLogProbMetric: 32.4339 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 182/1000
2023-10-25 18:49:26.317 
Epoch 182/1000 
	 loss: 32.3595, MinusLogProbMetric: 32.3595, val_loss: 32.5916, val_MinusLogProbMetric: 32.5916

Epoch 182: val_loss did not improve from 32.22619
196/196 - 42s - loss: 32.3595 - MinusLogProbMetric: 32.3595 - val_loss: 32.5916 - val_MinusLogProbMetric: 32.5916 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 183/1000
2023-10-25 18:50:03.844 
Epoch 183/1000 
	 loss: 32.1232, MinusLogProbMetric: 32.1232, val_loss: 32.1787, val_MinusLogProbMetric: 32.1787

Epoch 183: val_loss improved from 32.22619 to 32.17866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 38s - loss: 32.1232 - MinusLogProbMetric: 32.1232 - val_loss: 32.1787 - val_MinusLogProbMetric: 32.1787 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 184/1000
2023-10-25 18:50:43.529 
Epoch 184/1000 
	 loss: 32.3776, MinusLogProbMetric: 32.3776, val_loss: 32.3318, val_MinusLogProbMetric: 32.3318

Epoch 184: val_loss did not improve from 32.17866
196/196 - 39s - loss: 32.3776 - MinusLogProbMetric: 32.3776 - val_loss: 32.3318 - val_MinusLogProbMetric: 32.3318 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 185/1000
2023-10-25 18:51:21.917 
Epoch 185/1000 
	 loss: 32.1126, MinusLogProbMetric: 32.1126, val_loss: 32.0176, val_MinusLogProbMetric: 32.0176

Epoch 185: val_loss improved from 32.17866 to 32.01758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 39s - loss: 32.1126 - MinusLogProbMetric: 32.1126 - val_loss: 32.0176 - val_MinusLogProbMetric: 32.0176 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 186/1000
2023-10-25 18:52:01.446 
Epoch 186/1000 
	 loss: 32.0542, MinusLogProbMetric: 32.0542, val_loss: 32.3943, val_MinusLogProbMetric: 32.3943

Epoch 186: val_loss did not improve from 32.01758
196/196 - 39s - loss: 32.0542 - MinusLogProbMetric: 32.0542 - val_loss: 32.3943 - val_MinusLogProbMetric: 32.3943 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 187/1000
2023-10-25 18:52:41.849 
Epoch 187/1000 
	 loss: 32.0889, MinusLogProbMetric: 32.0889, val_loss: 32.6213, val_MinusLogProbMetric: 32.6213

Epoch 187: val_loss did not improve from 32.01758
196/196 - 40s - loss: 32.0889 - MinusLogProbMetric: 32.0889 - val_loss: 32.6213 - val_MinusLogProbMetric: 32.6213 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 188/1000
2023-10-25 18:53:23.429 
Epoch 188/1000 
	 loss: 32.2973, MinusLogProbMetric: 32.2973, val_loss: 31.9967, val_MinusLogProbMetric: 31.9967

Epoch 188: val_loss improved from 32.01758 to 31.99673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 32.2973 - MinusLogProbMetric: 32.2973 - val_loss: 31.9967 - val_MinusLogProbMetric: 31.9967 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 189/1000
2023-10-25 18:54:00.673 
Epoch 189/1000 
	 loss: 32.0685, MinusLogProbMetric: 32.0685, val_loss: 32.4465, val_MinusLogProbMetric: 32.4465

Epoch 189: val_loss did not improve from 31.99673
196/196 - 37s - loss: 32.0685 - MinusLogProbMetric: 32.0685 - val_loss: 32.4465 - val_MinusLogProbMetric: 32.4465 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 190/1000
2023-10-25 18:54:37.272 
Epoch 190/1000 
	 loss: 32.0676, MinusLogProbMetric: 32.0676, val_loss: 32.7263, val_MinusLogProbMetric: 32.7263

Epoch 190: val_loss did not improve from 31.99673
196/196 - 37s - loss: 32.0676 - MinusLogProbMetric: 32.0676 - val_loss: 32.7263 - val_MinusLogProbMetric: 32.7263 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 191/1000
2023-10-25 18:55:15.839 
Epoch 191/1000 
	 loss: 31.9295, MinusLogProbMetric: 31.9295, val_loss: 32.1722, val_MinusLogProbMetric: 32.1722

Epoch 191: val_loss did not improve from 31.99673
196/196 - 39s - loss: 31.9295 - MinusLogProbMetric: 31.9295 - val_loss: 32.1722 - val_MinusLogProbMetric: 32.1722 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 192/1000
2023-10-25 18:55:51.715 
Epoch 192/1000 
	 loss: 32.1865, MinusLogProbMetric: 32.1865, val_loss: 32.0029, val_MinusLogProbMetric: 32.0029

Epoch 192: val_loss did not improve from 31.99673
196/196 - 36s - loss: 32.1865 - MinusLogProbMetric: 32.1865 - val_loss: 32.0029 - val_MinusLogProbMetric: 32.0029 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 193/1000
2023-10-25 18:56:32.359 
Epoch 193/1000 
	 loss: 32.0585, MinusLogProbMetric: 32.0585, val_loss: 31.9126, val_MinusLogProbMetric: 31.9126

Epoch 193: val_loss improved from 31.99673 to 31.91262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 32.0585 - MinusLogProbMetric: 32.0585 - val_loss: 31.9126 - val_MinusLogProbMetric: 31.9126 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 194/1000
2023-10-25 18:57:13.274 
Epoch 194/1000 
	 loss: 32.0108, MinusLogProbMetric: 32.0108, val_loss: 32.1487, val_MinusLogProbMetric: 32.1487

Epoch 194: val_loss did not improve from 31.91262
196/196 - 40s - loss: 32.0108 - MinusLogProbMetric: 32.0108 - val_loss: 32.1487 - val_MinusLogProbMetric: 32.1487 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 195/1000
2023-10-25 18:57:53.181 
Epoch 195/1000 
	 loss: 32.0488, MinusLogProbMetric: 32.0488, val_loss: 32.2661, val_MinusLogProbMetric: 32.2661

Epoch 195: val_loss did not improve from 31.91262
196/196 - 40s - loss: 32.0488 - MinusLogProbMetric: 32.0488 - val_loss: 32.2661 - val_MinusLogProbMetric: 32.2661 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 196/1000
2023-10-25 18:58:27.920 
Epoch 196/1000 
	 loss: 32.1449, MinusLogProbMetric: 32.1449, val_loss: 32.5086, val_MinusLogProbMetric: 32.5086

Epoch 196: val_loss did not improve from 31.91262
196/196 - 35s - loss: 32.1449 - MinusLogProbMetric: 32.1449 - val_loss: 32.5086 - val_MinusLogProbMetric: 32.5086 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 197/1000
2023-10-25 18:59:01.968 
Epoch 197/1000 
	 loss: 32.1345, MinusLogProbMetric: 32.1345, val_loss: 32.2066, val_MinusLogProbMetric: 32.2066

Epoch 197: val_loss did not improve from 31.91262
196/196 - 34s - loss: 32.1345 - MinusLogProbMetric: 32.1345 - val_loss: 32.2066 - val_MinusLogProbMetric: 32.2066 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 198/1000
2023-10-25 18:59:39.980 
Epoch 198/1000 
	 loss: 32.0421, MinusLogProbMetric: 32.0421, val_loss: 32.1442, val_MinusLogProbMetric: 32.1442

Epoch 198: val_loss did not improve from 31.91262
196/196 - 38s - loss: 32.0421 - MinusLogProbMetric: 32.0421 - val_loss: 32.1442 - val_MinusLogProbMetric: 32.1442 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 199/1000
2023-10-25 19:00:21.100 
Epoch 199/1000 
	 loss: 31.9363, MinusLogProbMetric: 31.9363, val_loss: 32.5187, val_MinusLogProbMetric: 32.5187

Epoch 199: val_loss did not improve from 31.91262
196/196 - 41s - loss: 31.9363 - MinusLogProbMetric: 31.9363 - val_loss: 32.5187 - val_MinusLogProbMetric: 32.5187 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 200/1000
2023-10-25 19:01:00.403 
Epoch 200/1000 
	 loss: 32.1983, MinusLogProbMetric: 32.1983, val_loss: 31.9472, val_MinusLogProbMetric: 31.9472

Epoch 200: val_loss did not improve from 31.91262
196/196 - 39s - loss: 32.1983 - MinusLogProbMetric: 32.1983 - val_loss: 31.9472 - val_MinusLogProbMetric: 31.9472 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 201/1000
2023-10-25 19:01:37.957 
Epoch 201/1000 
	 loss: 32.2281, MinusLogProbMetric: 32.2281, val_loss: 32.2334, val_MinusLogProbMetric: 32.2334

Epoch 201: val_loss did not improve from 31.91262
196/196 - 38s - loss: 32.2281 - MinusLogProbMetric: 32.2281 - val_loss: 32.2334 - val_MinusLogProbMetric: 32.2334 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 202/1000
2023-10-25 19:02:12.446 
Epoch 202/1000 
	 loss: 31.8556, MinusLogProbMetric: 31.8556, val_loss: 32.6882, val_MinusLogProbMetric: 32.6882

Epoch 202: val_loss did not improve from 31.91262
196/196 - 34s - loss: 31.8556 - MinusLogProbMetric: 31.8556 - val_loss: 32.6882 - val_MinusLogProbMetric: 32.6882 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 203/1000
2023-10-25 19:02:52.516 
Epoch 203/1000 
	 loss: 31.7882, MinusLogProbMetric: 31.7882, val_loss: 32.0737, val_MinusLogProbMetric: 32.0737

Epoch 203: val_loss did not improve from 31.91262
196/196 - 40s - loss: 31.7882 - MinusLogProbMetric: 31.7882 - val_loss: 32.0737 - val_MinusLogProbMetric: 32.0737 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 204/1000
2023-10-25 19:03:34.608 
Epoch 204/1000 
	 loss: 31.8278, MinusLogProbMetric: 31.8278, val_loss: 32.9509, val_MinusLogProbMetric: 32.9509

Epoch 204: val_loss did not improve from 31.91262
196/196 - 42s - loss: 31.8278 - MinusLogProbMetric: 31.8278 - val_loss: 32.9509 - val_MinusLogProbMetric: 32.9509 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 205/1000
2023-10-25 19:04:13.413 
Epoch 205/1000 
	 loss: 31.6884, MinusLogProbMetric: 31.6884, val_loss: 34.1256, val_MinusLogProbMetric: 34.1256

Epoch 205: val_loss did not improve from 31.91262
196/196 - 39s - loss: 31.6884 - MinusLogProbMetric: 31.6884 - val_loss: 34.1256 - val_MinusLogProbMetric: 34.1256 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 206/1000
2023-10-25 19:04:50.743 
Epoch 206/1000 
	 loss: 31.9979, MinusLogProbMetric: 31.9979, val_loss: 33.6502, val_MinusLogProbMetric: 33.6502

Epoch 206: val_loss did not improve from 31.91262
196/196 - 37s - loss: 31.9979 - MinusLogProbMetric: 31.9979 - val_loss: 33.6502 - val_MinusLogProbMetric: 33.6502 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 207/1000
2023-10-25 19:05:27.004 
Epoch 207/1000 
	 loss: 32.1745, MinusLogProbMetric: 32.1745, val_loss: 32.2557, val_MinusLogProbMetric: 32.2557

Epoch 207: val_loss did not improve from 31.91262
196/196 - 36s - loss: 32.1745 - MinusLogProbMetric: 32.1745 - val_loss: 32.2557 - val_MinusLogProbMetric: 32.2557 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 208/1000
2023-10-25 19:06:08.605 
Epoch 208/1000 
	 loss: 31.7536, MinusLogProbMetric: 31.7536, val_loss: 31.9386, val_MinusLogProbMetric: 31.9386

Epoch 208: val_loss did not improve from 31.91262
196/196 - 42s - loss: 31.7536 - MinusLogProbMetric: 31.7536 - val_loss: 31.9386 - val_MinusLogProbMetric: 31.9386 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 209/1000
2023-10-25 19:06:50.046 
Epoch 209/1000 
	 loss: 31.7553, MinusLogProbMetric: 31.7553, val_loss: 31.9624, val_MinusLogProbMetric: 31.9624

Epoch 209: val_loss did not improve from 31.91262
196/196 - 41s - loss: 31.7553 - MinusLogProbMetric: 31.7553 - val_loss: 31.9624 - val_MinusLogProbMetric: 31.9624 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 210/1000
2023-10-25 19:07:23.936 
Epoch 210/1000 
	 loss: 44.8655, MinusLogProbMetric: 44.8655, val_loss: 46.2071, val_MinusLogProbMetric: 46.2071

Epoch 210: val_loss did not improve from 31.91262
196/196 - 34s - loss: 44.8655 - MinusLogProbMetric: 44.8655 - val_loss: 46.2071 - val_MinusLogProbMetric: 46.2071 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 211/1000
2023-10-25 19:08:03.182 
Epoch 211/1000 
	 loss: 37.5970, MinusLogProbMetric: 37.5970, val_loss: 34.9026, val_MinusLogProbMetric: 34.9026

Epoch 211: val_loss did not improve from 31.91262
196/196 - 39s - loss: 37.5970 - MinusLogProbMetric: 37.5970 - val_loss: 34.9026 - val_MinusLogProbMetric: 34.9026 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 212/1000
2023-10-25 19:08:43.307 
Epoch 212/1000 
	 loss: 33.4375, MinusLogProbMetric: 33.4375, val_loss: 32.8523, val_MinusLogProbMetric: 32.8523

Epoch 212: val_loss did not improve from 31.91262
196/196 - 40s - loss: 33.4375 - MinusLogProbMetric: 33.4375 - val_loss: 32.8523 - val_MinusLogProbMetric: 32.8523 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 213/1000
2023-10-25 19:09:22.302 
Epoch 213/1000 
	 loss: 32.5884, MinusLogProbMetric: 32.5884, val_loss: 32.6772, val_MinusLogProbMetric: 32.6772

Epoch 213: val_loss did not improve from 31.91262
196/196 - 39s - loss: 32.5884 - MinusLogProbMetric: 32.5884 - val_loss: 32.6772 - val_MinusLogProbMetric: 32.6772 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 214/1000
2023-10-25 19:09:58.938 
Epoch 214/1000 
	 loss: 32.1307, MinusLogProbMetric: 32.1307, val_loss: 32.1533, val_MinusLogProbMetric: 32.1533

Epoch 214: val_loss did not improve from 31.91262
196/196 - 37s - loss: 32.1307 - MinusLogProbMetric: 32.1307 - val_loss: 32.1533 - val_MinusLogProbMetric: 32.1533 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 215/1000
2023-10-25 19:10:39.564 
Epoch 215/1000 
	 loss: 32.1722, MinusLogProbMetric: 32.1722, val_loss: 32.0825, val_MinusLogProbMetric: 32.0825

Epoch 215: val_loss did not improve from 31.91262
196/196 - 41s - loss: 32.1722 - MinusLogProbMetric: 32.1722 - val_loss: 32.0825 - val_MinusLogProbMetric: 32.0825 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 216/1000
2023-10-25 19:11:16.184 
Epoch 216/1000 
	 loss: 31.9398, MinusLogProbMetric: 31.9398, val_loss: 32.2294, val_MinusLogProbMetric: 32.2294

Epoch 216: val_loss did not improve from 31.91262
196/196 - 37s - loss: 31.9398 - MinusLogProbMetric: 31.9398 - val_loss: 32.2294 - val_MinusLogProbMetric: 32.2294 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 217/1000
2023-10-25 19:11:56.581 
Epoch 217/1000 
	 loss: 31.8169, MinusLogProbMetric: 31.8169, val_loss: 31.8072, val_MinusLogProbMetric: 31.8072

Epoch 217: val_loss improved from 31.91262 to 31.80720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 31.8169 - MinusLogProbMetric: 31.8169 - val_loss: 31.8072 - val_MinusLogProbMetric: 31.8072 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 218/1000
2023-10-25 19:12:33.330 
Epoch 218/1000 
	 loss: 31.8480, MinusLogProbMetric: 31.8480, val_loss: 31.8651, val_MinusLogProbMetric: 31.8651

Epoch 218: val_loss did not improve from 31.80720
196/196 - 36s - loss: 31.8480 - MinusLogProbMetric: 31.8480 - val_loss: 31.8651 - val_MinusLogProbMetric: 31.8651 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 219/1000
2023-10-25 19:13:12.137 
Epoch 219/1000 
	 loss: 31.6927, MinusLogProbMetric: 31.6927, val_loss: 31.9154, val_MinusLogProbMetric: 31.9154

Epoch 219: val_loss did not improve from 31.80720
196/196 - 39s - loss: 31.6927 - MinusLogProbMetric: 31.6927 - val_loss: 31.9154 - val_MinusLogProbMetric: 31.9154 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 220/1000
2023-10-25 19:13:54.925 
Epoch 220/1000 
	 loss: 31.6228, MinusLogProbMetric: 31.6228, val_loss: 31.7570, val_MinusLogProbMetric: 31.7570

Epoch 220: val_loss improved from 31.80720 to 31.75698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.6228 - MinusLogProbMetric: 31.6228 - val_loss: 31.7570 - val_MinusLogProbMetric: 31.7570 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 221/1000
2023-10-25 19:14:35.113 
Epoch 221/1000 
	 loss: 31.7382, MinusLogProbMetric: 31.7382, val_loss: 31.8397, val_MinusLogProbMetric: 31.8397

Epoch 221: val_loss did not improve from 31.75698
196/196 - 39s - loss: 31.7382 - MinusLogProbMetric: 31.7382 - val_loss: 31.8397 - val_MinusLogProbMetric: 31.8397 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 222/1000
2023-10-25 19:15:17.306 
Epoch 222/1000 
	 loss: 31.5628, MinusLogProbMetric: 31.5628, val_loss: 31.6548, val_MinusLogProbMetric: 31.6548

Epoch 222: val_loss improved from 31.75698 to 31.65476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.5628 - MinusLogProbMetric: 31.5628 - val_loss: 31.6548 - val_MinusLogProbMetric: 31.6548 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 223/1000
2023-10-25 19:15:57.384 
Epoch 223/1000 
	 loss: 31.6313, MinusLogProbMetric: 31.6313, val_loss: 31.8601, val_MinusLogProbMetric: 31.8601

Epoch 223: val_loss did not improve from 31.65476
196/196 - 39s - loss: 31.6313 - MinusLogProbMetric: 31.6313 - val_loss: 31.8601 - val_MinusLogProbMetric: 31.8601 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 224/1000
2023-10-25 19:16:38.504 
Epoch 224/1000 
	 loss: 31.5674, MinusLogProbMetric: 31.5674, val_loss: 31.5250, val_MinusLogProbMetric: 31.5250

Epoch 224: val_loss improved from 31.65476 to 31.52503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 31.5674 - MinusLogProbMetric: 31.5674 - val_loss: 31.5250 - val_MinusLogProbMetric: 31.5250 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 225/1000
2023-10-25 19:17:20.505 
Epoch 225/1000 
	 loss: 31.4315, MinusLogProbMetric: 31.4315, val_loss: 31.6170, val_MinusLogProbMetric: 31.6170

Epoch 225: val_loss did not improve from 31.52503
196/196 - 41s - loss: 31.4315 - MinusLogProbMetric: 31.4315 - val_loss: 31.6170 - val_MinusLogProbMetric: 31.6170 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 226/1000
2023-10-25 19:18:02.564 
Epoch 226/1000 
	 loss: 31.4643, MinusLogProbMetric: 31.4643, val_loss: 31.5576, val_MinusLogProbMetric: 31.5576

Epoch 226: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.4643 - MinusLogProbMetric: 31.4643 - val_loss: 31.5576 - val_MinusLogProbMetric: 31.5576 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 227/1000
2023-10-25 19:18:44.809 
Epoch 227/1000 
	 loss: 31.4288, MinusLogProbMetric: 31.4288, val_loss: 32.0865, val_MinusLogProbMetric: 32.0865

Epoch 227: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.4288 - MinusLogProbMetric: 31.4288 - val_loss: 32.0865 - val_MinusLogProbMetric: 32.0865 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 228/1000
2023-10-25 19:19:26.093 
Epoch 228/1000 
	 loss: 31.4672, MinusLogProbMetric: 31.4672, val_loss: 31.5398, val_MinusLogProbMetric: 31.5398

Epoch 228: val_loss did not improve from 31.52503
196/196 - 41s - loss: 31.4672 - MinusLogProbMetric: 31.4672 - val_loss: 31.5398 - val_MinusLogProbMetric: 31.5398 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 229/1000
2023-10-25 19:20:08.096 
Epoch 229/1000 
	 loss: 31.4116, MinusLogProbMetric: 31.4116, val_loss: 31.6796, val_MinusLogProbMetric: 31.6796

Epoch 229: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.4116 - MinusLogProbMetric: 31.4116 - val_loss: 31.6796 - val_MinusLogProbMetric: 31.6796 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 230/1000
2023-10-25 19:20:50.480 
Epoch 230/1000 
	 loss: 31.4561, MinusLogProbMetric: 31.4561, val_loss: 32.7631, val_MinusLogProbMetric: 32.7631

Epoch 230: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.4561 - MinusLogProbMetric: 31.4561 - val_loss: 32.7631 - val_MinusLogProbMetric: 32.7631 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 231/1000
2023-10-25 19:21:32.415 
Epoch 231/1000 
	 loss: 31.4775, MinusLogProbMetric: 31.4775, val_loss: 33.0154, val_MinusLogProbMetric: 33.0154

Epoch 231: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.4775 - MinusLogProbMetric: 31.4775 - val_loss: 33.0154 - val_MinusLogProbMetric: 33.0154 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 232/1000
2023-10-25 19:22:14.740 
Epoch 232/1000 
	 loss: 31.3511, MinusLogProbMetric: 31.3511, val_loss: 31.7666, val_MinusLogProbMetric: 31.7666

Epoch 232: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.3511 - MinusLogProbMetric: 31.3511 - val_loss: 31.7666 - val_MinusLogProbMetric: 31.7666 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 233/1000
2023-10-25 19:22:57.002 
Epoch 233/1000 
	 loss: 31.3503, MinusLogProbMetric: 31.3503, val_loss: 32.2717, val_MinusLogProbMetric: 32.2717

Epoch 233: val_loss did not improve from 31.52503
196/196 - 42s - loss: 31.3503 - MinusLogProbMetric: 31.3503 - val_loss: 32.2717 - val_MinusLogProbMetric: 32.2717 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 234/1000
2023-10-25 19:23:39.355 
Epoch 234/1000 
	 loss: 31.4005, MinusLogProbMetric: 31.4005, val_loss: 31.4114, val_MinusLogProbMetric: 31.4114

Epoch 234: val_loss improved from 31.52503 to 31.41144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.4005 - MinusLogProbMetric: 31.4005 - val_loss: 31.4114 - val_MinusLogProbMetric: 31.4114 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 235/1000
2023-10-25 19:24:22.631 
Epoch 235/1000 
	 loss: 31.3716, MinusLogProbMetric: 31.3716, val_loss: 31.3632, val_MinusLogProbMetric: 31.3632

Epoch 235: val_loss improved from 31.41144 to 31.36317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.3716 - MinusLogProbMetric: 31.3716 - val_loss: 31.3632 - val_MinusLogProbMetric: 31.3632 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 236/1000
2023-10-25 19:25:05.385 
Epoch 236/1000 
	 loss: 31.3234, MinusLogProbMetric: 31.3234, val_loss: 31.3121, val_MinusLogProbMetric: 31.3121

Epoch 236: val_loss improved from 31.36317 to 31.31209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.3234 - MinusLogProbMetric: 31.3234 - val_loss: 31.3121 - val_MinusLogProbMetric: 31.3121 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 237/1000
2023-10-25 19:25:48.155 
Epoch 237/1000 
	 loss: 31.2737, MinusLogProbMetric: 31.2737, val_loss: 32.2706, val_MinusLogProbMetric: 32.2706

Epoch 237: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.2737 - MinusLogProbMetric: 31.2737 - val_loss: 32.2706 - val_MinusLogProbMetric: 32.2706 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 238/1000
2023-10-25 19:26:30.271 
Epoch 238/1000 
	 loss: 31.3453, MinusLogProbMetric: 31.3453, val_loss: 31.6688, val_MinusLogProbMetric: 31.6688

Epoch 238: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.3453 - MinusLogProbMetric: 31.3453 - val_loss: 31.6688 - val_MinusLogProbMetric: 31.6688 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 239/1000
2023-10-25 19:27:12.321 
Epoch 239/1000 
	 loss: 31.3005, MinusLogProbMetric: 31.3005, val_loss: 33.1440, val_MinusLogProbMetric: 33.1440

Epoch 239: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.3005 - MinusLogProbMetric: 31.3005 - val_loss: 33.1440 - val_MinusLogProbMetric: 33.1440 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 240/1000
2023-10-25 19:27:54.058 
Epoch 240/1000 
	 loss: 31.1606, MinusLogProbMetric: 31.1606, val_loss: 31.9429, val_MinusLogProbMetric: 31.9429

Epoch 240: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.1606 - MinusLogProbMetric: 31.1606 - val_loss: 31.9429 - val_MinusLogProbMetric: 31.9429 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 241/1000
2023-10-25 19:28:36.365 
Epoch 241/1000 
	 loss: 31.2288, MinusLogProbMetric: 31.2288, val_loss: 31.7967, val_MinusLogProbMetric: 31.7967

Epoch 241: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.2288 - MinusLogProbMetric: 31.2288 - val_loss: 31.7967 - val_MinusLogProbMetric: 31.7967 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 242/1000
2023-10-25 19:29:18.412 
Epoch 242/1000 
	 loss: 31.2260, MinusLogProbMetric: 31.2260, val_loss: 33.2763, val_MinusLogProbMetric: 33.2763

Epoch 242: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.2260 - MinusLogProbMetric: 31.2260 - val_loss: 33.2763 - val_MinusLogProbMetric: 33.2763 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 243/1000
2023-10-25 19:30:00.445 
Epoch 243/1000 
	 loss: 31.2162, MinusLogProbMetric: 31.2162, val_loss: 33.3876, val_MinusLogProbMetric: 33.3876

Epoch 243: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.2162 - MinusLogProbMetric: 31.2162 - val_loss: 33.3876 - val_MinusLogProbMetric: 33.3876 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 244/1000
2023-10-25 19:30:42.587 
Epoch 244/1000 
	 loss: 31.1767, MinusLogProbMetric: 31.1767, val_loss: 31.5980, val_MinusLogProbMetric: 31.5980

Epoch 244: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.1767 - MinusLogProbMetric: 31.1767 - val_loss: 31.5980 - val_MinusLogProbMetric: 31.5980 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 245/1000
2023-10-25 19:31:24.600 
Epoch 245/1000 
	 loss: 31.3016, MinusLogProbMetric: 31.3016, val_loss: 31.4633, val_MinusLogProbMetric: 31.4633

Epoch 245: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.3016 - MinusLogProbMetric: 31.3016 - val_loss: 31.4633 - val_MinusLogProbMetric: 31.4633 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 246/1000
2023-10-25 19:32:06.361 
Epoch 246/1000 
	 loss: 31.0828, MinusLogProbMetric: 31.0828, val_loss: 31.6852, val_MinusLogProbMetric: 31.6852

Epoch 246: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.0828 - MinusLogProbMetric: 31.0828 - val_loss: 31.6852 - val_MinusLogProbMetric: 31.6852 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 247/1000
2023-10-25 19:32:48.192 
Epoch 247/1000 
	 loss: 31.3056, MinusLogProbMetric: 31.3056, val_loss: 32.1829, val_MinusLogProbMetric: 32.1829

Epoch 247: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.3056 - MinusLogProbMetric: 31.3056 - val_loss: 32.1829 - val_MinusLogProbMetric: 32.1829 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 248/1000
2023-10-25 19:33:30.135 
Epoch 248/1000 
	 loss: 31.1578, MinusLogProbMetric: 31.1578, val_loss: 31.9466, val_MinusLogProbMetric: 31.9466

Epoch 248: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.1578 - MinusLogProbMetric: 31.1578 - val_loss: 31.9466 - val_MinusLogProbMetric: 31.9466 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 249/1000
2023-10-25 19:34:12.555 
Epoch 249/1000 
	 loss: 31.0805, MinusLogProbMetric: 31.0805, val_loss: 33.0420, val_MinusLogProbMetric: 33.0420

Epoch 249: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.0805 - MinusLogProbMetric: 31.0805 - val_loss: 33.0420 - val_MinusLogProbMetric: 33.0420 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 250/1000
2023-10-25 19:34:54.623 
Epoch 250/1000 
	 loss: 31.3421, MinusLogProbMetric: 31.3421, val_loss: 31.4795, val_MinusLogProbMetric: 31.4795

Epoch 250: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.3421 - MinusLogProbMetric: 31.3421 - val_loss: 31.4795 - val_MinusLogProbMetric: 31.4795 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 251/1000
2023-10-25 19:35:36.979 
Epoch 251/1000 
	 loss: 31.1582, MinusLogProbMetric: 31.1582, val_loss: 32.9984, val_MinusLogProbMetric: 32.9984

Epoch 251: val_loss did not improve from 31.31209
196/196 - 42s - loss: 31.1582 - MinusLogProbMetric: 31.1582 - val_loss: 32.9984 - val_MinusLogProbMetric: 32.9984 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 252/1000
2023-10-25 19:36:19.388 
Epoch 252/1000 
	 loss: 31.1808, MinusLogProbMetric: 31.1808, val_loss: 31.3101, val_MinusLogProbMetric: 31.3101

Epoch 252: val_loss improved from 31.31209 to 31.31011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.1808 - MinusLogProbMetric: 31.1808 - val_loss: 31.3101 - val_MinusLogProbMetric: 31.3101 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 253/1000
2023-10-25 19:37:02.496 
Epoch 253/1000 
	 loss: 31.0418, MinusLogProbMetric: 31.0418, val_loss: 32.2850, val_MinusLogProbMetric: 32.2850

Epoch 253: val_loss did not improve from 31.31011
196/196 - 42s - loss: 31.0418 - MinusLogProbMetric: 31.0418 - val_loss: 32.2850 - val_MinusLogProbMetric: 32.2850 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 254/1000
2023-10-25 19:37:44.536 
Epoch 254/1000 
	 loss: 31.0115, MinusLogProbMetric: 31.0115, val_loss: 31.0421, val_MinusLogProbMetric: 31.0421

Epoch 254: val_loss improved from 31.31011 to 31.04208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 31.0115 - MinusLogProbMetric: 31.0115 - val_loss: 31.0421 - val_MinusLogProbMetric: 31.0421 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 255/1000
2023-10-25 19:38:27.330 
Epoch 255/1000 
	 loss: 31.0759, MinusLogProbMetric: 31.0759, val_loss: 31.2037, val_MinusLogProbMetric: 31.2037

Epoch 255: val_loss did not improve from 31.04208
196/196 - 42s - loss: 31.0759 - MinusLogProbMetric: 31.0759 - val_loss: 31.2037 - val_MinusLogProbMetric: 31.2037 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 256/1000
2023-10-25 19:39:09.196 
Epoch 256/1000 
	 loss: 31.0746, MinusLogProbMetric: 31.0746, val_loss: 32.7967, val_MinusLogProbMetric: 32.7967

Epoch 256: val_loss did not improve from 31.04208
196/196 - 42s - loss: 31.0746 - MinusLogProbMetric: 31.0746 - val_loss: 32.7967 - val_MinusLogProbMetric: 32.7967 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 257/1000
2023-10-25 19:39:51.351 
Epoch 257/1000 
	 loss: 30.9754, MinusLogProbMetric: 30.9754, val_loss: 31.8393, val_MinusLogProbMetric: 31.8393

Epoch 257: val_loss did not improve from 31.04208
196/196 - 42s - loss: 30.9754 - MinusLogProbMetric: 30.9754 - val_loss: 31.8393 - val_MinusLogProbMetric: 31.8393 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 258/1000
2023-10-25 19:40:33.230 
Epoch 258/1000 
	 loss: 31.1570, MinusLogProbMetric: 31.1570, val_loss: 31.6342, val_MinusLogProbMetric: 31.6342

Epoch 258: val_loss did not improve from 31.04208
196/196 - 42s - loss: 31.1570 - MinusLogProbMetric: 31.1570 - val_loss: 31.6342 - val_MinusLogProbMetric: 31.6342 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 259/1000
2023-10-25 19:41:12.230 
Epoch 259/1000 
	 loss: 31.0632, MinusLogProbMetric: 31.0632, val_loss: 31.6552, val_MinusLogProbMetric: 31.6552

Epoch 259: val_loss did not improve from 31.04208
196/196 - 39s - loss: 31.0632 - MinusLogProbMetric: 31.0632 - val_loss: 31.6552 - val_MinusLogProbMetric: 31.6552 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 260/1000
2023-10-25 19:41:53.747 
Epoch 260/1000 
	 loss: 31.0699, MinusLogProbMetric: 31.0699, val_loss: 31.0570, val_MinusLogProbMetric: 31.0570

Epoch 260: val_loss did not improve from 31.04208
196/196 - 42s - loss: 31.0699 - MinusLogProbMetric: 31.0699 - val_loss: 31.0570 - val_MinusLogProbMetric: 31.0570 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 261/1000
2023-10-25 19:42:34.586 
Epoch 261/1000 
	 loss: 30.9335, MinusLogProbMetric: 30.9335, val_loss: 32.5418, val_MinusLogProbMetric: 32.5418

Epoch 261: val_loss did not improve from 31.04208
196/196 - 41s - loss: 30.9335 - MinusLogProbMetric: 30.9335 - val_loss: 32.5418 - val_MinusLogProbMetric: 32.5418 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 262/1000
2023-10-25 19:43:16.466 
Epoch 262/1000 
	 loss: 30.9487, MinusLogProbMetric: 30.9487, val_loss: 31.2384, val_MinusLogProbMetric: 31.2384

Epoch 262: val_loss did not improve from 31.04208
196/196 - 42s - loss: 30.9487 - MinusLogProbMetric: 30.9487 - val_loss: 31.2384 - val_MinusLogProbMetric: 31.2384 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 263/1000
2023-10-25 19:43:58.477 
Epoch 263/1000 
	 loss: 30.9687, MinusLogProbMetric: 30.9687, val_loss: 32.0035, val_MinusLogProbMetric: 32.0035

Epoch 263: val_loss did not improve from 31.04208
196/196 - 42s - loss: 30.9687 - MinusLogProbMetric: 30.9687 - val_loss: 32.0035 - val_MinusLogProbMetric: 32.0035 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 264/1000
2023-10-25 19:44:40.822 
Epoch 264/1000 
	 loss: 30.9162, MinusLogProbMetric: 30.9162, val_loss: 31.5409, val_MinusLogProbMetric: 31.5409

Epoch 264: val_loss did not improve from 31.04208
196/196 - 42s - loss: 30.9162 - MinusLogProbMetric: 30.9162 - val_loss: 31.5409 - val_MinusLogProbMetric: 31.5409 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 265/1000
2023-10-25 19:45:22.017 
Epoch 265/1000 
	 loss: 30.9538, MinusLogProbMetric: 30.9538, val_loss: 31.0246, val_MinusLogProbMetric: 31.0246

Epoch 265: val_loss improved from 31.04208 to 31.02459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 30.9538 - MinusLogProbMetric: 30.9538 - val_loss: 31.0246 - val_MinusLogProbMetric: 31.0246 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 266/1000
2023-10-25 19:46:04.810 
Epoch 266/1000 
	 loss: 31.0333, MinusLogProbMetric: 31.0333, val_loss: 31.2913, val_MinusLogProbMetric: 31.2913

Epoch 266: val_loss did not improve from 31.02459
196/196 - 42s - loss: 31.0333 - MinusLogProbMetric: 31.0333 - val_loss: 31.2913 - val_MinusLogProbMetric: 31.2913 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 267/1000
2023-10-25 19:46:47.074 
Epoch 267/1000 
	 loss: 30.9007, MinusLogProbMetric: 30.9007, val_loss: 30.8617, val_MinusLogProbMetric: 30.8617

Epoch 267: val_loss improved from 31.02459 to 30.86168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.9007 - MinusLogProbMetric: 30.9007 - val_loss: 30.8617 - val_MinusLogProbMetric: 30.8617 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 268/1000
2023-10-25 19:47:29.758 
Epoch 268/1000 
	 loss: 30.9019, MinusLogProbMetric: 30.9019, val_loss: 31.2953, val_MinusLogProbMetric: 31.2953

Epoch 268: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.9019 - MinusLogProbMetric: 30.9019 - val_loss: 31.2953 - val_MinusLogProbMetric: 31.2953 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 269/1000
2023-10-25 19:48:12.293 
Epoch 269/1000 
	 loss: 30.7951, MinusLogProbMetric: 30.7951, val_loss: 30.9299, val_MinusLogProbMetric: 30.9299

Epoch 269: val_loss did not improve from 30.86168
196/196 - 43s - loss: 30.7951 - MinusLogProbMetric: 30.7951 - val_loss: 30.9299 - val_MinusLogProbMetric: 30.9299 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 270/1000
2023-10-25 19:48:54.422 
Epoch 270/1000 
	 loss: 30.9420, MinusLogProbMetric: 30.9420, val_loss: 31.0500, val_MinusLogProbMetric: 31.0500

Epoch 270: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.9420 - MinusLogProbMetric: 30.9420 - val_loss: 31.0500 - val_MinusLogProbMetric: 31.0500 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 271/1000
2023-10-25 19:49:36.734 
Epoch 271/1000 
	 loss: 30.8996, MinusLogProbMetric: 30.8996, val_loss: 31.1068, val_MinusLogProbMetric: 31.1068

Epoch 271: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8996 - MinusLogProbMetric: 30.8996 - val_loss: 31.1068 - val_MinusLogProbMetric: 31.1068 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 272/1000
2023-10-25 19:50:18.609 
Epoch 272/1000 
	 loss: 30.8689, MinusLogProbMetric: 30.8689, val_loss: 30.9415, val_MinusLogProbMetric: 30.9415

Epoch 272: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8689 - MinusLogProbMetric: 30.8689 - val_loss: 30.9415 - val_MinusLogProbMetric: 30.9415 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 273/1000
2023-10-25 19:51:00.762 
Epoch 273/1000 
	 loss: 30.9028, MinusLogProbMetric: 30.9028, val_loss: 30.9560, val_MinusLogProbMetric: 30.9560

Epoch 273: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.9028 - MinusLogProbMetric: 30.9028 - val_loss: 30.9560 - val_MinusLogProbMetric: 30.9560 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 274/1000
2023-10-25 19:51:42.777 
Epoch 274/1000 
	 loss: 30.8983, MinusLogProbMetric: 30.8983, val_loss: 30.9582, val_MinusLogProbMetric: 30.9582

Epoch 274: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8983 - MinusLogProbMetric: 30.8983 - val_loss: 30.9582 - val_MinusLogProbMetric: 30.9582 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 275/1000
2023-10-25 19:52:24.835 
Epoch 275/1000 
	 loss: 30.7428, MinusLogProbMetric: 30.7428, val_loss: 32.1083, val_MinusLogProbMetric: 32.1083

Epoch 275: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.7428 - MinusLogProbMetric: 30.7428 - val_loss: 32.1083 - val_MinusLogProbMetric: 32.1083 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 276/1000
2023-10-25 19:53:06.878 
Epoch 276/1000 
	 loss: 30.8996, MinusLogProbMetric: 30.8996, val_loss: 32.0077, val_MinusLogProbMetric: 32.0077

Epoch 276: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8996 - MinusLogProbMetric: 30.8996 - val_loss: 32.0077 - val_MinusLogProbMetric: 32.0077 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 277/1000
2023-10-25 19:53:48.822 
Epoch 277/1000 
	 loss: 30.8930, MinusLogProbMetric: 30.8930, val_loss: 31.5352, val_MinusLogProbMetric: 31.5352

Epoch 277: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8930 - MinusLogProbMetric: 30.8930 - val_loss: 31.5352 - val_MinusLogProbMetric: 31.5352 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 278/1000
2023-10-25 19:54:30.958 
Epoch 278/1000 
	 loss: 30.9717, MinusLogProbMetric: 30.9717, val_loss: 31.0094, val_MinusLogProbMetric: 31.0094

Epoch 278: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.9717 - MinusLogProbMetric: 30.9717 - val_loss: 31.0094 - val_MinusLogProbMetric: 31.0094 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 279/1000
2023-10-25 19:55:12.916 
Epoch 279/1000 
	 loss: 30.8054, MinusLogProbMetric: 30.8054, val_loss: 31.3472, val_MinusLogProbMetric: 31.3472

Epoch 279: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8054 - MinusLogProbMetric: 30.8054 - val_loss: 31.3472 - val_MinusLogProbMetric: 31.3472 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 280/1000
2023-10-25 19:55:54.716 
Epoch 280/1000 
	 loss: 30.7899, MinusLogProbMetric: 30.7899, val_loss: 31.2930, val_MinusLogProbMetric: 31.2930

Epoch 280: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.7899 - MinusLogProbMetric: 30.7899 - val_loss: 31.2930 - val_MinusLogProbMetric: 31.2930 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 281/1000
2023-10-25 19:56:37.226 
Epoch 281/1000 
	 loss: 30.8865, MinusLogProbMetric: 30.8865, val_loss: 31.3838, val_MinusLogProbMetric: 31.3838

Epoch 281: val_loss did not improve from 30.86168
196/196 - 43s - loss: 30.8865 - MinusLogProbMetric: 30.8865 - val_loss: 31.3838 - val_MinusLogProbMetric: 31.3838 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 282/1000
2023-10-25 19:57:19.260 
Epoch 282/1000 
	 loss: 30.8147, MinusLogProbMetric: 30.8147, val_loss: 30.8830, val_MinusLogProbMetric: 30.8830

Epoch 282: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.8147 - MinusLogProbMetric: 30.8147 - val_loss: 30.8830 - val_MinusLogProbMetric: 30.8830 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 283/1000
2023-10-25 19:58:01.363 
Epoch 283/1000 
	 loss: 30.6872, MinusLogProbMetric: 30.6872, val_loss: 31.7838, val_MinusLogProbMetric: 31.7838

Epoch 283: val_loss did not improve from 30.86168
196/196 - 42s - loss: 30.6872 - MinusLogProbMetric: 30.6872 - val_loss: 31.7838 - val_MinusLogProbMetric: 31.7838 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 284/1000
2023-10-25 19:58:43.246 
Epoch 284/1000 
	 loss: 30.7848, MinusLogProbMetric: 30.7848, val_loss: 30.7262, val_MinusLogProbMetric: 30.7262

Epoch 284: val_loss improved from 30.86168 to 30.72624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.7848 - MinusLogProbMetric: 30.7848 - val_loss: 30.7262 - val_MinusLogProbMetric: 30.7262 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 285/1000
2023-10-25 19:59:25.863 
Epoch 285/1000 
	 loss: 30.9530, MinusLogProbMetric: 30.9530, val_loss: 30.9578, val_MinusLogProbMetric: 30.9578

Epoch 285: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.9530 - MinusLogProbMetric: 30.9530 - val_loss: 30.9578 - val_MinusLogProbMetric: 30.9578 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 286/1000
2023-10-25 20:00:07.714 
Epoch 286/1000 
	 loss: 30.8174, MinusLogProbMetric: 30.8174, val_loss: 30.9165, val_MinusLogProbMetric: 30.9165

Epoch 286: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.8174 - MinusLogProbMetric: 30.8174 - val_loss: 30.9165 - val_MinusLogProbMetric: 30.9165 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 287/1000
2023-10-25 20:00:49.455 
Epoch 287/1000 
	 loss: 30.6823, MinusLogProbMetric: 30.6823, val_loss: 31.3753, val_MinusLogProbMetric: 31.3753

Epoch 287: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.6823 - MinusLogProbMetric: 30.6823 - val_loss: 31.3753 - val_MinusLogProbMetric: 31.3753 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 288/1000
2023-10-25 20:01:31.978 
Epoch 288/1000 
	 loss: 30.8526, MinusLogProbMetric: 30.8526, val_loss: 31.1850, val_MinusLogProbMetric: 31.1850

Epoch 288: val_loss did not improve from 30.72624
196/196 - 43s - loss: 30.8526 - MinusLogProbMetric: 30.8526 - val_loss: 31.1850 - val_MinusLogProbMetric: 31.1850 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 289/1000
2023-10-25 20:02:14.141 
Epoch 289/1000 
	 loss: 30.7199, MinusLogProbMetric: 30.7199, val_loss: 31.3474, val_MinusLogProbMetric: 31.3474

Epoch 289: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.7199 - MinusLogProbMetric: 30.7199 - val_loss: 31.3474 - val_MinusLogProbMetric: 31.3474 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 290/1000
2023-10-25 20:02:56.089 
Epoch 290/1000 
	 loss: 30.6918, MinusLogProbMetric: 30.6918, val_loss: 31.4245, val_MinusLogProbMetric: 31.4245

Epoch 290: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.6918 - MinusLogProbMetric: 30.6918 - val_loss: 31.4245 - val_MinusLogProbMetric: 31.4245 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 291/1000
2023-10-25 20:03:38.347 
Epoch 291/1000 
	 loss: 30.7471, MinusLogProbMetric: 30.7471, val_loss: 30.9174, val_MinusLogProbMetric: 30.9174

Epoch 291: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.7471 - MinusLogProbMetric: 30.7471 - val_loss: 30.9174 - val_MinusLogProbMetric: 30.9174 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 292/1000
2023-10-25 20:04:20.111 
Epoch 292/1000 
	 loss: 30.7083, MinusLogProbMetric: 30.7083, val_loss: 31.1217, val_MinusLogProbMetric: 31.1217

Epoch 292: val_loss did not improve from 30.72624
196/196 - 42s - loss: 30.7083 - MinusLogProbMetric: 30.7083 - val_loss: 31.1217 - val_MinusLogProbMetric: 31.1217 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 293/1000
2023-10-25 20:05:02.108 
Epoch 293/1000 
	 loss: 30.6902, MinusLogProbMetric: 30.6902, val_loss: 30.6098, val_MinusLogProbMetric: 30.6098

Epoch 293: val_loss improved from 30.72624 to 30.60983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.6902 - MinusLogProbMetric: 30.6902 - val_loss: 30.6098 - val_MinusLogProbMetric: 30.6098 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 294/1000
2023-10-25 20:05:45.229 
Epoch 294/1000 
	 loss: 30.7626, MinusLogProbMetric: 30.7626, val_loss: 30.8463, val_MinusLogProbMetric: 30.8463

Epoch 294: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.7626 - MinusLogProbMetric: 30.7626 - val_loss: 30.8463 - val_MinusLogProbMetric: 30.8463 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 295/1000
2023-10-25 20:06:27.325 
Epoch 295/1000 
	 loss: 30.6830, MinusLogProbMetric: 30.6830, val_loss: 31.2010, val_MinusLogProbMetric: 31.2010

Epoch 295: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6830 - MinusLogProbMetric: 30.6830 - val_loss: 31.2010 - val_MinusLogProbMetric: 31.2010 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 296/1000
2023-10-25 20:07:09.655 
Epoch 296/1000 
	 loss: 30.7513, MinusLogProbMetric: 30.7513, val_loss: 31.6672, val_MinusLogProbMetric: 31.6672

Epoch 296: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.7513 - MinusLogProbMetric: 30.7513 - val_loss: 31.6672 - val_MinusLogProbMetric: 31.6672 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 297/1000
2023-10-25 20:07:52.007 
Epoch 297/1000 
	 loss: 30.6004, MinusLogProbMetric: 30.6004, val_loss: 31.5934, val_MinusLogProbMetric: 31.5934

Epoch 297: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6004 - MinusLogProbMetric: 30.6004 - val_loss: 31.5934 - val_MinusLogProbMetric: 31.5934 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 298/1000
2023-10-25 20:08:34.339 
Epoch 298/1000 
	 loss: 30.6988, MinusLogProbMetric: 30.6988, val_loss: 30.8345, val_MinusLogProbMetric: 30.8345

Epoch 298: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6988 - MinusLogProbMetric: 30.6988 - val_loss: 30.8345 - val_MinusLogProbMetric: 30.8345 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 299/1000
2023-10-25 20:09:16.648 
Epoch 299/1000 
	 loss: 30.6704, MinusLogProbMetric: 30.6704, val_loss: 31.1549, val_MinusLogProbMetric: 31.1549

Epoch 299: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6704 - MinusLogProbMetric: 30.6704 - val_loss: 31.1549 - val_MinusLogProbMetric: 31.1549 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 300/1000
2023-10-25 20:09:58.717 
Epoch 300/1000 
	 loss: 30.6186, MinusLogProbMetric: 30.6186, val_loss: 31.3067, val_MinusLogProbMetric: 31.3067

Epoch 300: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6186 - MinusLogProbMetric: 30.6186 - val_loss: 31.3067 - val_MinusLogProbMetric: 31.3067 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 301/1000
2023-10-25 20:10:41.012 
Epoch 301/1000 
	 loss: 30.6083, MinusLogProbMetric: 30.6083, val_loss: 31.7428, val_MinusLogProbMetric: 31.7428

Epoch 301: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6083 - MinusLogProbMetric: 30.6083 - val_loss: 31.7428 - val_MinusLogProbMetric: 31.7428 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 302/1000
2023-10-25 20:11:23.273 
Epoch 302/1000 
	 loss: 30.7029, MinusLogProbMetric: 30.7029, val_loss: 31.6201, val_MinusLogProbMetric: 31.6201

Epoch 302: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.7029 - MinusLogProbMetric: 30.7029 - val_loss: 31.6201 - val_MinusLogProbMetric: 31.6201 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 303/1000
2023-10-25 20:12:05.474 
Epoch 303/1000 
	 loss: 30.6944, MinusLogProbMetric: 30.6944, val_loss: 31.3097, val_MinusLogProbMetric: 31.3097

Epoch 303: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6944 - MinusLogProbMetric: 30.6944 - val_loss: 31.3097 - val_MinusLogProbMetric: 31.3097 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 304/1000
2023-10-25 20:12:47.290 
Epoch 304/1000 
	 loss: 30.6765, MinusLogProbMetric: 30.6765, val_loss: 30.7433, val_MinusLogProbMetric: 30.7433

Epoch 304: val_loss did not improve from 30.60983
196/196 - 42s - loss: 30.6765 - MinusLogProbMetric: 30.6765 - val_loss: 30.7433 - val_MinusLogProbMetric: 30.7433 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 305/1000
2023-10-25 20:13:28.346 
Epoch 305/1000 
	 loss: 30.5328, MinusLogProbMetric: 30.5328, val_loss: 30.8472, val_MinusLogProbMetric: 30.8472

Epoch 305: val_loss did not improve from 30.60983
196/196 - 41s - loss: 30.5328 - MinusLogProbMetric: 30.5328 - val_loss: 30.8472 - val_MinusLogProbMetric: 30.8472 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 306/1000
2023-10-25 20:14:09.096 
Epoch 306/1000 
	 loss: 30.4669, MinusLogProbMetric: 30.4669, val_loss: 30.6040, val_MinusLogProbMetric: 30.6040

Epoch 306: val_loss improved from 30.60983 to 30.60395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 41s - loss: 30.4669 - MinusLogProbMetric: 30.4669 - val_loss: 30.6040 - val_MinusLogProbMetric: 30.6040 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 307/1000
2023-10-25 20:14:51.462 
Epoch 307/1000 
	 loss: 30.6018, MinusLogProbMetric: 30.6018, val_loss: 30.7808, val_MinusLogProbMetric: 30.7808

Epoch 307: val_loss did not improve from 30.60395
196/196 - 42s - loss: 30.6018 - MinusLogProbMetric: 30.6018 - val_loss: 30.7808 - val_MinusLogProbMetric: 30.7808 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 308/1000
2023-10-25 20:15:33.804 
Epoch 308/1000 
	 loss: 30.6148, MinusLogProbMetric: 30.6148, val_loss: 32.3792, val_MinusLogProbMetric: 32.3792

Epoch 308: val_loss did not improve from 30.60395
196/196 - 42s - loss: 30.6148 - MinusLogProbMetric: 30.6148 - val_loss: 32.3792 - val_MinusLogProbMetric: 32.3792 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 309/1000
2023-10-25 20:16:15.589 
Epoch 309/1000 
	 loss: 30.5695, MinusLogProbMetric: 30.5695, val_loss: 30.8782, val_MinusLogProbMetric: 30.8782

Epoch 309: val_loss did not improve from 30.60395
196/196 - 42s - loss: 30.5695 - MinusLogProbMetric: 30.5695 - val_loss: 30.8782 - val_MinusLogProbMetric: 30.8782 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 310/1000
2023-10-25 20:16:57.949 
Epoch 310/1000 
	 loss: 30.5003, MinusLogProbMetric: 30.5003, val_loss: 30.6278, val_MinusLogProbMetric: 30.6278

Epoch 310: val_loss did not improve from 30.60395
196/196 - 42s - loss: 30.5003 - MinusLogProbMetric: 30.5003 - val_loss: 30.6278 - val_MinusLogProbMetric: 30.6278 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 311/1000
2023-10-25 20:17:40.210 
Epoch 311/1000 
	 loss: 30.5390, MinusLogProbMetric: 30.5390, val_loss: 30.5300, val_MinusLogProbMetric: 30.5300

Epoch 311: val_loss improved from 30.60395 to 30.52997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.5390 - MinusLogProbMetric: 30.5390 - val_loss: 30.5300 - val_MinusLogProbMetric: 30.5300 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 312/1000
2023-10-25 20:18:22.651 
Epoch 312/1000 
	 loss: 30.4991, MinusLogProbMetric: 30.4991, val_loss: 30.6901, val_MinusLogProbMetric: 30.6901

Epoch 312: val_loss did not improve from 30.52997
196/196 - 42s - loss: 30.4991 - MinusLogProbMetric: 30.4991 - val_loss: 30.6901 - val_MinusLogProbMetric: 30.6901 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 313/1000
2023-10-25 20:19:04.866 
Epoch 313/1000 
	 loss: 30.6639, MinusLogProbMetric: 30.6639, val_loss: 30.6239, val_MinusLogProbMetric: 30.6239

Epoch 313: val_loss did not improve from 30.52997
196/196 - 42s - loss: 30.6639 - MinusLogProbMetric: 30.6639 - val_loss: 30.6239 - val_MinusLogProbMetric: 30.6239 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 314/1000
2023-10-25 20:19:47.384 
Epoch 314/1000 
	 loss: 30.5515, MinusLogProbMetric: 30.5515, val_loss: 31.1646, val_MinusLogProbMetric: 31.1646

Epoch 314: val_loss did not improve from 30.52997
196/196 - 43s - loss: 30.5515 - MinusLogProbMetric: 30.5515 - val_loss: 31.1646 - val_MinusLogProbMetric: 31.1646 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 315/1000
2023-10-25 20:20:29.724 
Epoch 315/1000 
	 loss: 30.4786, MinusLogProbMetric: 30.4786, val_loss: 30.4050, val_MinusLogProbMetric: 30.4050

Epoch 315: val_loss improved from 30.52997 to 30.40503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.4786 - MinusLogProbMetric: 30.4786 - val_loss: 30.4050 - val_MinusLogProbMetric: 30.4050 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 316/1000
2023-10-25 20:21:13.093 
Epoch 316/1000 
	 loss: 30.4343, MinusLogProbMetric: 30.4343, val_loss: 30.4953, val_MinusLogProbMetric: 30.4953

Epoch 316: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.4343 - MinusLogProbMetric: 30.4343 - val_loss: 30.4953 - val_MinusLogProbMetric: 30.4953 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 317/1000
2023-10-25 20:21:55.876 
Epoch 317/1000 
	 loss: 30.4800, MinusLogProbMetric: 30.4800, val_loss: 31.7508, val_MinusLogProbMetric: 31.7508

Epoch 317: val_loss did not improve from 30.40503
196/196 - 43s - loss: 30.4800 - MinusLogProbMetric: 30.4800 - val_loss: 31.7508 - val_MinusLogProbMetric: 31.7508 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 318/1000
2023-10-25 20:22:38.369 
Epoch 318/1000 
	 loss: 30.5309, MinusLogProbMetric: 30.5309, val_loss: 30.7082, val_MinusLogProbMetric: 30.7082

Epoch 318: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.5309 - MinusLogProbMetric: 30.5309 - val_loss: 30.7082 - val_MinusLogProbMetric: 30.7082 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 319/1000
2023-10-25 20:23:20.780 
Epoch 319/1000 
	 loss: 30.5151, MinusLogProbMetric: 30.5151, val_loss: 30.8240, val_MinusLogProbMetric: 30.8240

Epoch 319: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.5151 - MinusLogProbMetric: 30.5151 - val_loss: 30.8240 - val_MinusLogProbMetric: 30.8240 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 320/1000
2023-10-25 20:24:03.546 
Epoch 320/1000 
	 loss: 30.4956, MinusLogProbMetric: 30.4956, val_loss: 30.8186, val_MinusLogProbMetric: 30.8186

Epoch 320: val_loss did not improve from 30.40503
196/196 - 43s - loss: 30.4956 - MinusLogProbMetric: 30.4956 - val_loss: 30.8186 - val_MinusLogProbMetric: 30.8186 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 321/1000
2023-10-25 20:24:45.866 
Epoch 321/1000 
	 loss: 30.5185, MinusLogProbMetric: 30.5185, val_loss: 31.1320, val_MinusLogProbMetric: 31.1320

Epoch 321: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.5185 - MinusLogProbMetric: 30.5185 - val_loss: 31.1320 - val_MinusLogProbMetric: 31.1320 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 322/1000
2023-10-25 20:25:28.403 
Epoch 322/1000 
	 loss: 30.5109, MinusLogProbMetric: 30.5109, val_loss: 30.5585, val_MinusLogProbMetric: 30.5585

Epoch 322: val_loss did not improve from 30.40503
196/196 - 43s - loss: 30.5109 - MinusLogProbMetric: 30.5109 - val_loss: 30.5585 - val_MinusLogProbMetric: 30.5585 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 323/1000
2023-10-25 20:26:11.096 
Epoch 323/1000 
	 loss: 30.4593, MinusLogProbMetric: 30.4593, val_loss: 31.1031, val_MinusLogProbMetric: 31.1031

Epoch 323: val_loss did not improve from 30.40503
196/196 - 43s - loss: 30.4593 - MinusLogProbMetric: 30.4593 - val_loss: 31.1031 - val_MinusLogProbMetric: 31.1031 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 324/1000
2023-10-25 20:26:53.424 
Epoch 324/1000 
	 loss: 30.3854, MinusLogProbMetric: 30.3854, val_loss: 30.6325, val_MinusLogProbMetric: 30.6325

Epoch 324: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.3854 - MinusLogProbMetric: 30.3854 - val_loss: 30.6325 - val_MinusLogProbMetric: 30.6325 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 325/1000
2023-10-25 20:27:36.158 
Epoch 325/1000 
	 loss: 30.3864, MinusLogProbMetric: 30.3864, val_loss: 30.6780, val_MinusLogProbMetric: 30.6780

Epoch 325: val_loss did not improve from 30.40503
196/196 - 43s - loss: 30.3864 - MinusLogProbMetric: 30.3864 - val_loss: 30.6780 - val_MinusLogProbMetric: 30.6780 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 326/1000
2023-10-25 20:28:18.358 
Epoch 326/1000 
	 loss: 30.4047, MinusLogProbMetric: 30.4047, val_loss: 30.9210, val_MinusLogProbMetric: 30.9210

Epoch 326: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.4047 - MinusLogProbMetric: 30.4047 - val_loss: 30.9210 - val_MinusLogProbMetric: 30.9210 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 327/1000
2023-10-25 20:29:00.335 
Epoch 327/1000 
	 loss: 30.4074, MinusLogProbMetric: 30.4074, val_loss: 30.5190, val_MinusLogProbMetric: 30.5190

Epoch 327: val_loss did not improve from 30.40503
196/196 - 42s - loss: 30.4074 - MinusLogProbMetric: 30.4074 - val_loss: 30.5190 - val_MinusLogProbMetric: 30.5190 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 328/1000
2023-10-25 20:29:42.846 
Epoch 328/1000 
	 loss: 30.3935, MinusLogProbMetric: 30.3935, val_loss: 30.6925, val_MinusLogProbMetric: 30.6925

Epoch 328: val_loss did not improve from 30.40503
196/196 - 43s - loss: 30.3935 - MinusLogProbMetric: 30.3935 - val_loss: 30.6925 - val_MinusLogProbMetric: 30.6925 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 329/1000
2023-10-25 20:30:25.032 
Epoch 329/1000 
	 loss: 30.3494, MinusLogProbMetric: 30.3494, val_loss: 30.4005, val_MinusLogProbMetric: 30.4005

Epoch 329: val_loss improved from 30.40503 to 30.40052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.3494 - MinusLogProbMetric: 30.3494 - val_loss: 30.4005 - val_MinusLogProbMetric: 30.4005 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 330/1000
2023-10-25 20:31:07.924 
Epoch 330/1000 
	 loss: 30.3597, MinusLogProbMetric: 30.3597, val_loss: 31.2772, val_MinusLogProbMetric: 31.2772

Epoch 330: val_loss did not improve from 30.40052
196/196 - 42s - loss: 30.3597 - MinusLogProbMetric: 30.3597 - val_loss: 31.2772 - val_MinusLogProbMetric: 31.2772 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 331/1000
2023-10-25 20:31:50.621 
Epoch 331/1000 
	 loss: 30.3753, MinusLogProbMetric: 30.3753, val_loss: 30.2581, val_MinusLogProbMetric: 30.2581

Epoch 331: val_loss improved from 30.40052 to 30.25811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.3753 - MinusLogProbMetric: 30.3753 - val_loss: 30.2581 - val_MinusLogProbMetric: 30.2581 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 332/1000
2023-10-25 20:32:33.312 
Epoch 332/1000 
	 loss: 30.3388, MinusLogProbMetric: 30.3388, val_loss: 30.8547, val_MinusLogProbMetric: 30.8547

Epoch 332: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.3388 - MinusLogProbMetric: 30.3388 - val_loss: 30.8547 - val_MinusLogProbMetric: 30.8547 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 333/1000
2023-10-25 20:33:15.866 
Epoch 333/1000 
	 loss: 30.4628, MinusLogProbMetric: 30.4628, val_loss: 30.9369, val_MinusLogProbMetric: 30.9369

Epoch 333: val_loss did not improve from 30.25811
196/196 - 43s - loss: 30.4628 - MinusLogProbMetric: 30.4628 - val_loss: 30.9369 - val_MinusLogProbMetric: 30.9369 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 334/1000
2023-10-25 20:33:58.438 
Epoch 334/1000 
	 loss: 30.4133, MinusLogProbMetric: 30.4133, val_loss: 30.5967, val_MinusLogProbMetric: 30.5967

Epoch 334: val_loss did not improve from 30.25811
196/196 - 43s - loss: 30.4133 - MinusLogProbMetric: 30.4133 - val_loss: 30.5967 - val_MinusLogProbMetric: 30.5967 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 335/1000
2023-10-25 20:34:40.862 
Epoch 335/1000 
	 loss: 30.2805, MinusLogProbMetric: 30.2805, val_loss: 30.6497, val_MinusLogProbMetric: 30.6497

Epoch 335: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2805 - MinusLogProbMetric: 30.2805 - val_loss: 30.6497 - val_MinusLogProbMetric: 30.6497 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 336/1000
2023-10-25 20:35:22.737 
Epoch 336/1000 
	 loss: 30.4083, MinusLogProbMetric: 30.4083, val_loss: 30.6115, val_MinusLogProbMetric: 30.6115

Epoch 336: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.4083 - MinusLogProbMetric: 30.4083 - val_loss: 30.6115 - val_MinusLogProbMetric: 30.6115 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 337/1000
2023-10-25 20:36:05.220 
Epoch 337/1000 
	 loss: 30.4316, MinusLogProbMetric: 30.4316, val_loss: 30.4794, val_MinusLogProbMetric: 30.4794

Epoch 337: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.4316 - MinusLogProbMetric: 30.4316 - val_loss: 30.4794 - val_MinusLogProbMetric: 30.4794 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 338/1000
2023-10-25 20:36:47.319 
Epoch 338/1000 
	 loss: 30.2980, MinusLogProbMetric: 30.2980, val_loss: 30.7421, val_MinusLogProbMetric: 30.7421

Epoch 338: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2980 - MinusLogProbMetric: 30.2980 - val_loss: 30.7421 - val_MinusLogProbMetric: 30.7421 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 339/1000
2023-10-25 20:37:29.773 
Epoch 339/1000 
	 loss: 30.2712, MinusLogProbMetric: 30.2712, val_loss: 30.5765, val_MinusLogProbMetric: 30.5765

Epoch 339: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2712 - MinusLogProbMetric: 30.2712 - val_loss: 30.5765 - val_MinusLogProbMetric: 30.5765 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 340/1000
2023-10-25 20:38:12.295 
Epoch 340/1000 
	 loss: 30.3996, MinusLogProbMetric: 30.3996, val_loss: 30.3878, val_MinusLogProbMetric: 30.3878

Epoch 340: val_loss did not improve from 30.25811
196/196 - 43s - loss: 30.3996 - MinusLogProbMetric: 30.3996 - val_loss: 30.3878 - val_MinusLogProbMetric: 30.3878 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 341/1000
2023-10-25 20:38:54.946 
Epoch 341/1000 
	 loss: 30.3834, MinusLogProbMetric: 30.3834, val_loss: 30.9179, val_MinusLogProbMetric: 30.9179

Epoch 341: val_loss did not improve from 30.25811
196/196 - 43s - loss: 30.3834 - MinusLogProbMetric: 30.3834 - val_loss: 30.9179 - val_MinusLogProbMetric: 30.9179 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 342/1000
2023-10-25 20:39:37.404 
Epoch 342/1000 
	 loss: 30.2782, MinusLogProbMetric: 30.2782, val_loss: 30.8527, val_MinusLogProbMetric: 30.8527

Epoch 342: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2782 - MinusLogProbMetric: 30.2782 - val_loss: 30.8527 - val_MinusLogProbMetric: 30.8527 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 343/1000
2023-10-25 20:40:19.410 
Epoch 343/1000 
	 loss: 30.3272, MinusLogProbMetric: 30.3272, val_loss: 30.5175, val_MinusLogProbMetric: 30.5175

Epoch 343: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.3272 - MinusLogProbMetric: 30.3272 - val_loss: 30.5175 - val_MinusLogProbMetric: 30.5175 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 344/1000
2023-10-25 20:41:01.485 
Epoch 344/1000 
	 loss: 30.2955, MinusLogProbMetric: 30.2955, val_loss: 30.6422, val_MinusLogProbMetric: 30.6422

Epoch 344: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2955 - MinusLogProbMetric: 30.2955 - val_loss: 30.6422 - val_MinusLogProbMetric: 30.6422 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 345/1000
2023-10-25 20:41:43.418 
Epoch 345/1000 
	 loss: 30.4270, MinusLogProbMetric: 30.4270, val_loss: 30.8850, val_MinusLogProbMetric: 30.8850

Epoch 345: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.4270 - MinusLogProbMetric: 30.4270 - val_loss: 30.8850 - val_MinusLogProbMetric: 30.8850 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 346/1000
2023-10-25 20:42:24.512 
Epoch 346/1000 
	 loss: 30.2914, MinusLogProbMetric: 30.2914, val_loss: 31.7113, val_MinusLogProbMetric: 31.7113

Epoch 346: val_loss did not improve from 30.25811
196/196 - 41s - loss: 30.2914 - MinusLogProbMetric: 30.2914 - val_loss: 31.7113 - val_MinusLogProbMetric: 31.7113 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 347/1000
2023-10-25 20:43:03.572 
Epoch 347/1000 
	 loss: 30.2631, MinusLogProbMetric: 30.2631, val_loss: 31.0979, val_MinusLogProbMetric: 31.0979

Epoch 347: val_loss did not improve from 30.25811
196/196 - 39s - loss: 30.2631 - MinusLogProbMetric: 30.2631 - val_loss: 31.0979 - val_MinusLogProbMetric: 31.0979 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 348/1000
2023-10-25 20:43:44.930 
Epoch 348/1000 
	 loss: 30.2979, MinusLogProbMetric: 30.2979, val_loss: 30.3413, val_MinusLogProbMetric: 30.3413

Epoch 348: val_loss did not improve from 30.25811
196/196 - 41s - loss: 30.2979 - MinusLogProbMetric: 30.2979 - val_loss: 30.3413 - val_MinusLogProbMetric: 30.3413 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 349/1000
2023-10-25 20:44:24.505 
Epoch 349/1000 
	 loss: 30.2526, MinusLogProbMetric: 30.2526, val_loss: 30.4176, val_MinusLogProbMetric: 30.4176

Epoch 349: val_loss did not improve from 30.25811
196/196 - 40s - loss: 30.2526 - MinusLogProbMetric: 30.2526 - val_loss: 30.4176 - val_MinusLogProbMetric: 30.4176 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 350/1000
2023-10-25 20:45:06.984 
Epoch 350/1000 
	 loss: 30.3768, MinusLogProbMetric: 30.3768, val_loss: 30.5401, val_MinusLogProbMetric: 30.5401

Epoch 350: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.3768 - MinusLogProbMetric: 30.3768 - val_loss: 30.5401 - val_MinusLogProbMetric: 30.5401 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 351/1000
2023-10-25 20:45:49.631 
Epoch 351/1000 
	 loss: 30.2729, MinusLogProbMetric: 30.2729, val_loss: 30.3759, val_MinusLogProbMetric: 30.3759

Epoch 351: val_loss did not improve from 30.25811
196/196 - 43s - loss: 30.2729 - MinusLogProbMetric: 30.2729 - val_loss: 30.3759 - val_MinusLogProbMetric: 30.3759 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 352/1000
2023-10-25 20:46:30.112 
Epoch 352/1000 
	 loss: 30.2231, MinusLogProbMetric: 30.2231, val_loss: 30.6386, val_MinusLogProbMetric: 30.6386

Epoch 352: val_loss did not improve from 30.25811
196/196 - 40s - loss: 30.2231 - MinusLogProbMetric: 30.2231 - val_loss: 30.6386 - val_MinusLogProbMetric: 30.6386 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 353/1000
2023-10-25 20:47:12.349 
Epoch 353/1000 
	 loss: 30.2223, MinusLogProbMetric: 30.2223, val_loss: 30.3287, val_MinusLogProbMetric: 30.3287

Epoch 353: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2223 - MinusLogProbMetric: 30.2223 - val_loss: 30.3287 - val_MinusLogProbMetric: 30.3287 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 354/1000
2023-10-25 20:47:55.012 
Epoch 354/1000 
	 loss: 30.1319, MinusLogProbMetric: 30.1319, val_loss: 30.5218, val_MinusLogProbMetric: 30.5218

Epoch 354: val_loss did not improve from 30.25811
196/196 - 43s - loss: 30.1319 - MinusLogProbMetric: 30.1319 - val_loss: 30.5218 - val_MinusLogProbMetric: 30.5218 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 355/1000
2023-10-25 20:48:36.993 
Epoch 355/1000 
	 loss: 30.1879, MinusLogProbMetric: 30.1879, val_loss: 30.6808, val_MinusLogProbMetric: 30.6808

Epoch 355: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1879 - MinusLogProbMetric: 30.1879 - val_loss: 30.6808 - val_MinusLogProbMetric: 30.6808 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 356/1000
2023-10-25 20:49:19.200 
Epoch 356/1000 
	 loss: 30.2334, MinusLogProbMetric: 30.2334, val_loss: 30.3544, val_MinusLogProbMetric: 30.3544

Epoch 356: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2334 - MinusLogProbMetric: 30.2334 - val_loss: 30.3544 - val_MinusLogProbMetric: 30.3544 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 357/1000
2023-10-25 20:50:01.478 
Epoch 357/1000 
	 loss: 30.2400, MinusLogProbMetric: 30.2400, val_loss: 30.5923, val_MinusLogProbMetric: 30.5923

Epoch 357: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2400 - MinusLogProbMetric: 30.2400 - val_loss: 30.5923 - val_MinusLogProbMetric: 30.5923 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 358/1000
2023-10-25 20:50:43.799 
Epoch 358/1000 
	 loss: 30.2464, MinusLogProbMetric: 30.2464, val_loss: 30.3236, val_MinusLogProbMetric: 30.3236

Epoch 358: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2464 - MinusLogProbMetric: 30.2464 - val_loss: 30.3236 - val_MinusLogProbMetric: 30.3236 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 359/1000
2023-10-25 20:51:25.964 
Epoch 359/1000 
	 loss: 30.1927, MinusLogProbMetric: 30.1927, val_loss: 30.3737, val_MinusLogProbMetric: 30.3737

Epoch 359: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1927 - MinusLogProbMetric: 30.1927 - val_loss: 30.3737 - val_MinusLogProbMetric: 30.3737 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 360/1000
2023-10-25 20:52:08.191 
Epoch 360/1000 
	 loss: 30.2370, MinusLogProbMetric: 30.2370, val_loss: 30.8823, val_MinusLogProbMetric: 30.8823

Epoch 360: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2370 - MinusLogProbMetric: 30.2370 - val_loss: 30.8823 - val_MinusLogProbMetric: 30.8823 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 361/1000
2023-10-25 20:52:50.454 
Epoch 361/1000 
	 loss: 30.1205, MinusLogProbMetric: 30.1205, val_loss: 30.4440, val_MinusLogProbMetric: 30.4440

Epoch 361: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1205 - MinusLogProbMetric: 30.1205 - val_loss: 30.4440 - val_MinusLogProbMetric: 30.4440 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 362/1000
2023-10-25 20:53:32.819 
Epoch 362/1000 
	 loss: 30.2088, MinusLogProbMetric: 30.2088, val_loss: 30.3055, val_MinusLogProbMetric: 30.3055

Epoch 362: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2088 - MinusLogProbMetric: 30.2088 - val_loss: 30.3055 - val_MinusLogProbMetric: 30.3055 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 363/1000
2023-10-25 20:54:14.718 
Epoch 363/1000 
	 loss: 30.1642, MinusLogProbMetric: 30.1642, val_loss: 30.4283, val_MinusLogProbMetric: 30.4283

Epoch 363: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1642 - MinusLogProbMetric: 30.1642 - val_loss: 30.4283 - val_MinusLogProbMetric: 30.4283 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 364/1000
2023-10-25 20:54:57.020 
Epoch 364/1000 
	 loss: 30.1460, MinusLogProbMetric: 30.1460, val_loss: 30.4561, val_MinusLogProbMetric: 30.4561

Epoch 364: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1460 - MinusLogProbMetric: 30.1460 - val_loss: 30.4561 - val_MinusLogProbMetric: 30.4561 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 365/1000
2023-10-25 20:55:39.377 
Epoch 365/1000 
	 loss: 30.1928, MinusLogProbMetric: 30.1928, val_loss: 30.4624, val_MinusLogProbMetric: 30.4624

Epoch 365: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1928 - MinusLogProbMetric: 30.1928 - val_loss: 30.4624 - val_MinusLogProbMetric: 30.4624 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 366/1000
2023-10-25 20:56:21.643 
Epoch 366/1000 
	 loss: 30.1194, MinusLogProbMetric: 30.1194, val_loss: 30.3289, val_MinusLogProbMetric: 30.3289

Epoch 366: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1194 - MinusLogProbMetric: 30.1194 - val_loss: 30.3289 - val_MinusLogProbMetric: 30.3289 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 367/1000
2023-10-25 20:57:03.356 
Epoch 367/1000 
	 loss: 30.1771, MinusLogProbMetric: 30.1771, val_loss: 30.2704, val_MinusLogProbMetric: 30.2704

Epoch 367: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1771 - MinusLogProbMetric: 30.1771 - val_loss: 30.2704 - val_MinusLogProbMetric: 30.2704 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 368/1000
2023-10-25 20:57:45.512 
Epoch 368/1000 
	 loss: 30.1380, MinusLogProbMetric: 30.1380, val_loss: 31.7105, val_MinusLogProbMetric: 31.7105

Epoch 368: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.1380 - MinusLogProbMetric: 30.1380 - val_loss: 31.7105 - val_MinusLogProbMetric: 31.7105 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 369/1000
2023-10-25 20:58:27.405 
Epoch 369/1000 
	 loss: 30.2259, MinusLogProbMetric: 30.2259, val_loss: 30.3360, val_MinusLogProbMetric: 30.3360

Epoch 369: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.2259 - MinusLogProbMetric: 30.2259 - val_loss: 30.3360 - val_MinusLogProbMetric: 30.3360 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 370/1000
2023-10-25 20:59:09.262 
Epoch 370/1000 
	 loss: 30.0731, MinusLogProbMetric: 30.0731, val_loss: 30.3540, val_MinusLogProbMetric: 30.3540

Epoch 370: val_loss did not improve from 30.25811
196/196 - 42s - loss: 30.0731 - MinusLogProbMetric: 30.0731 - val_loss: 30.3540 - val_MinusLogProbMetric: 30.3540 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 371/1000
2023-10-25 20:59:50.710 
Epoch 371/1000 
	 loss: 30.1970, MinusLogProbMetric: 30.1970, val_loss: 30.3279, val_MinusLogProbMetric: 30.3279

Epoch 371: val_loss did not improve from 30.25811
196/196 - 41s - loss: 30.1970 - MinusLogProbMetric: 30.1970 - val_loss: 30.3279 - val_MinusLogProbMetric: 30.3279 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 372/1000
2023-10-25 21:00:33.061 
Epoch 372/1000 
	 loss: 30.0690, MinusLogProbMetric: 30.0690, val_loss: 30.2255, val_MinusLogProbMetric: 30.2255

Epoch 372: val_loss improved from 30.25811 to 30.22548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.0690 - MinusLogProbMetric: 30.0690 - val_loss: 30.2255 - val_MinusLogProbMetric: 30.2255 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 373/1000
2023-10-25 21:01:16.076 
Epoch 373/1000 
	 loss: 30.1590, MinusLogProbMetric: 30.1590, val_loss: 30.7423, val_MinusLogProbMetric: 30.7423

Epoch 373: val_loss did not improve from 30.22548
196/196 - 42s - loss: 30.1590 - MinusLogProbMetric: 30.1590 - val_loss: 30.7423 - val_MinusLogProbMetric: 30.7423 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 374/1000
2023-10-25 21:01:58.556 
Epoch 374/1000 
	 loss: 30.1923, MinusLogProbMetric: 30.1923, val_loss: 30.2235, val_MinusLogProbMetric: 30.2235

Epoch 374: val_loss improved from 30.22548 to 30.22346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.1923 - MinusLogProbMetric: 30.1923 - val_loss: 30.2235 - val_MinusLogProbMetric: 30.2235 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 375/1000
2023-10-25 21:02:41.877 
Epoch 375/1000 
	 loss: 30.1562, MinusLogProbMetric: 30.1562, val_loss: 30.8581, val_MinusLogProbMetric: 30.8581

Epoch 375: val_loss did not improve from 30.22346
196/196 - 43s - loss: 30.1562 - MinusLogProbMetric: 30.1562 - val_loss: 30.8581 - val_MinusLogProbMetric: 30.8581 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 376/1000
2023-10-25 21:03:24.057 
Epoch 376/1000 
	 loss: 30.0570, MinusLogProbMetric: 30.0570, val_loss: 30.2984, val_MinusLogProbMetric: 30.2984

Epoch 376: val_loss did not improve from 30.22346
196/196 - 42s - loss: 30.0570 - MinusLogProbMetric: 30.0570 - val_loss: 30.2984 - val_MinusLogProbMetric: 30.2984 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 377/1000
2023-10-25 21:04:06.474 
Epoch 377/1000 
	 loss: 30.1338, MinusLogProbMetric: 30.1338, val_loss: 30.4500, val_MinusLogProbMetric: 30.4500

Epoch 377: val_loss did not improve from 30.22346
196/196 - 42s - loss: 30.1338 - MinusLogProbMetric: 30.1338 - val_loss: 30.4500 - val_MinusLogProbMetric: 30.4500 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 378/1000
2023-10-25 21:04:47.713 
Epoch 378/1000 
	 loss: 30.1130, MinusLogProbMetric: 30.1130, val_loss: 30.5335, val_MinusLogProbMetric: 30.5335

Epoch 378: val_loss did not improve from 30.22346
196/196 - 41s - loss: 30.1130 - MinusLogProbMetric: 30.1130 - val_loss: 30.5335 - val_MinusLogProbMetric: 30.5335 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 379/1000
2023-10-25 21:05:30.391 
Epoch 379/1000 
	 loss: 30.1360, MinusLogProbMetric: 30.1360, val_loss: 30.1144, val_MinusLogProbMetric: 30.1144

Epoch 379: val_loss improved from 30.22346 to 30.11442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.1360 - MinusLogProbMetric: 30.1360 - val_loss: 30.1144 - val_MinusLogProbMetric: 30.1144 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 380/1000
2023-10-25 21:06:13.500 
Epoch 380/1000 
	 loss: 29.9718, MinusLogProbMetric: 29.9718, val_loss: 30.6406, val_MinusLogProbMetric: 30.6406

Epoch 380: val_loss did not improve from 30.11442
196/196 - 42s - loss: 29.9718 - MinusLogProbMetric: 29.9718 - val_loss: 30.6406 - val_MinusLogProbMetric: 30.6406 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 381/1000
2023-10-25 21:06:55.717 
Epoch 381/1000 
	 loss: 30.0932, MinusLogProbMetric: 30.0932, val_loss: 30.5481, val_MinusLogProbMetric: 30.5481

Epoch 381: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.0932 - MinusLogProbMetric: 30.0932 - val_loss: 30.5481 - val_MinusLogProbMetric: 30.5481 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 382/1000
2023-10-25 21:07:37.404 
Epoch 382/1000 
	 loss: 30.1272, MinusLogProbMetric: 30.1272, val_loss: 30.2831, val_MinusLogProbMetric: 30.2831

Epoch 382: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.1272 - MinusLogProbMetric: 30.1272 - val_loss: 30.2831 - val_MinusLogProbMetric: 30.2831 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 383/1000
2023-10-25 21:08:19.106 
Epoch 383/1000 
	 loss: 30.1222, MinusLogProbMetric: 30.1222, val_loss: 30.3065, val_MinusLogProbMetric: 30.3065

Epoch 383: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.1222 - MinusLogProbMetric: 30.1222 - val_loss: 30.3065 - val_MinusLogProbMetric: 30.3065 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 384/1000
2023-10-25 21:09:00.792 
Epoch 384/1000 
	 loss: 30.0483, MinusLogProbMetric: 30.0483, val_loss: 30.6039, val_MinusLogProbMetric: 30.6039

Epoch 384: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.0483 - MinusLogProbMetric: 30.0483 - val_loss: 30.6039 - val_MinusLogProbMetric: 30.6039 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 385/1000
2023-10-25 21:09:43.237 
Epoch 385/1000 
	 loss: 30.1068, MinusLogProbMetric: 30.1068, val_loss: 30.4282, val_MinusLogProbMetric: 30.4282

Epoch 385: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.1068 - MinusLogProbMetric: 30.1068 - val_loss: 30.4282 - val_MinusLogProbMetric: 30.4282 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 386/1000
2023-10-25 21:10:25.286 
Epoch 386/1000 
	 loss: 30.1191, MinusLogProbMetric: 30.1191, val_loss: 30.3258, val_MinusLogProbMetric: 30.3258

Epoch 386: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.1191 - MinusLogProbMetric: 30.1191 - val_loss: 30.3258 - val_MinusLogProbMetric: 30.3258 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 387/1000
2023-10-25 21:11:07.594 
Epoch 387/1000 
	 loss: 30.0159, MinusLogProbMetric: 30.0159, val_loss: 30.1480, val_MinusLogProbMetric: 30.1480

Epoch 387: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.0159 - MinusLogProbMetric: 30.0159 - val_loss: 30.1480 - val_MinusLogProbMetric: 30.1480 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 388/1000
2023-10-25 21:11:50.032 
Epoch 388/1000 
	 loss: 30.0358, MinusLogProbMetric: 30.0358, val_loss: 30.3753, val_MinusLogProbMetric: 30.3753

Epoch 388: val_loss did not improve from 30.11442
196/196 - 42s - loss: 30.0358 - MinusLogProbMetric: 30.0358 - val_loss: 30.3753 - val_MinusLogProbMetric: 30.3753 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 389/1000
2023-10-25 21:12:32.184 
Epoch 389/1000 
	 loss: 30.1154, MinusLogProbMetric: 30.1154, val_loss: 30.0431, val_MinusLogProbMetric: 30.0431

Epoch 389: val_loss improved from 30.11442 to 30.04310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 30.1154 - MinusLogProbMetric: 30.1154 - val_loss: 30.0431 - val_MinusLogProbMetric: 30.0431 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 390/1000
2023-10-25 21:13:15.134 
Epoch 390/1000 
	 loss: 30.0084, MinusLogProbMetric: 30.0084, val_loss: 30.6511, val_MinusLogProbMetric: 30.6511

Epoch 390: val_loss did not improve from 30.04310
196/196 - 42s - loss: 30.0084 - MinusLogProbMetric: 30.0084 - val_loss: 30.6511 - val_MinusLogProbMetric: 30.6511 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 391/1000
2023-10-25 21:13:57.494 
Epoch 391/1000 
	 loss: 29.9555, MinusLogProbMetric: 29.9555, val_loss: 30.3289, val_MinusLogProbMetric: 30.3289

Epoch 391: val_loss did not improve from 30.04310
196/196 - 42s - loss: 29.9555 - MinusLogProbMetric: 29.9555 - val_loss: 30.3289 - val_MinusLogProbMetric: 30.3289 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 392/1000
2023-10-25 21:14:39.551 
Epoch 392/1000 
	 loss: 30.0110, MinusLogProbMetric: 30.0110, val_loss: 30.7003, val_MinusLogProbMetric: 30.7003

Epoch 392: val_loss did not improve from 30.04310
196/196 - 42s - loss: 30.0110 - MinusLogProbMetric: 30.0110 - val_loss: 30.7003 - val_MinusLogProbMetric: 30.7003 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 393/1000
2023-10-25 21:15:21.800 
Epoch 393/1000 
	 loss: 29.9996, MinusLogProbMetric: 29.9996, val_loss: 30.6550, val_MinusLogProbMetric: 30.6550

Epoch 393: val_loss did not improve from 30.04310
196/196 - 42s - loss: 29.9996 - MinusLogProbMetric: 29.9996 - val_loss: 30.6550 - val_MinusLogProbMetric: 30.6550 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 394/1000
2023-10-25 21:16:03.975 
Epoch 394/1000 
	 loss: 29.9669, MinusLogProbMetric: 29.9669, val_loss: 30.2546, val_MinusLogProbMetric: 30.2546

Epoch 394: val_loss did not improve from 30.04310
196/196 - 42s - loss: 29.9669 - MinusLogProbMetric: 29.9669 - val_loss: 30.2546 - val_MinusLogProbMetric: 30.2546 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 395/1000
2023-10-25 21:16:46.049 
Epoch 395/1000 
	 loss: 30.0144, MinusLogProbMetric: 30.0144, val_loss: 30.2172, val_MinusLogProbMetric: 30.2172

Epoch 395: val_loss did not improve from 30.04310
196/196 - 42s - loss: 30.0144 - MinusLogProbMetric: 30.0144 - val_loss: 30.2172 - val_MinusLogProbMetric: 30.2172 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 396/1000
2023-10-25 21:17:28.766 
Epoch 396/1000 
	 loss: 29.9546, MinusLogProbMetric: 29.9546, val_loss: 30.2642, val_MinusLogProbMetric: 30.2642

Epoch 396: val_loss did not improve from 30.04310
196/196 - 43s - loss: 29.9546 - MinusLogProbMetric: 29.9546 - val_loss: 30.2642 - val_MinusLogProbMetric: 30.2642 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 397/1000
2023-10-25 21:18:10.945 
Epoch 397/1000 
	 loss: 30.1067, MinusLogProbMetric: 30.1067, val_loss: 30.5832, val_MinusLogProbMetric: 30.5832

Epoch 397: val_loss did not improve from 30.04310
196/196 - 42s - loss: 30.1067 - MinusLogProbMetric: 30.1067 - val_loss: 30.5832 - val_MinusLogProbMetric: 30.5832 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 398/1000
2023-10-25 21:18:53.736 
Epoch 398/1000 
	 loss: 30.0289, MinusLogProbMetric: 30.0289, val_loss: 30.3557, val_MinusLogProbMetric: 30.3557

Epoch 398: val_loss did not improve from 30.04310
196/196 - 43s - loss: 30.0289 - MinusLogProbMetric: 30.0289 - val_loss: 30.3557 - val_MinusLogProbMetric: 30.3557 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 399/1000
2023-10-25 21:19:36.128 
Epoch 399/1000 
	 loss: 29.9459, MinusLogProbMetric: 29.9459, val_loss: 30.3107, val_MinusLogProbMetric: 30.3107

Epoch 399: val_loss did not improve from 30.04310
196/196 - 42s - loss: 29.9459 - MinusLogProbMetric: 29.9459 - val_loss: 30.3107 - val_MinusLogProbMetric: 30.3107 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 400/1000
2023-10-25 21:20:17.952 
Epoch 400/1000 
	 loss: 29.9495, MinusLogProbMetric: 29.9495, val_loss: 29.9607, val_MinusLogProbMetric: 29.9607

Epoch 400: val_loss improved from 30.04310 to 29.96072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 29.9495 - MinusLogProbMetric: 29.9495 - val_loss: 29.9607 - val_MinusLogProbMetric: 29.9607 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 401/1000
2023-10-25 21:21:01.141 
Epoch 401/1000 
	 loss: 30.0138, MinusLogProbMetric: 30.0138, val_loss: 30.2561, val_MinusLogProbMetric: 30.2561

Epoch 401: val_loss did not improve from 29.96072
196/196 - 43s - loss: 30.0138 - MinusLogProbMetric: 30.0138 - val_loss: 30.2561 - val_MinusLogProbMetric: 30.2561 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 402/1000
2023-10-25 21:21:43.370 
Epoch 402/1000 
	 loss: 29.9497, MinusLogProbMetric: 29.9497, val_loss: 30.2225, val_MinusLogProbMetric: 30.2225

Epoch 402: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9497 - MinusLogProbMetric: 29.9497 - val_loss: 30.2225 - val_MinusLogProbMetric: 30.2225 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 403/1000
2023-10-25 21:22:25.692 
Epoch 403/1000 
	 loss: 30.0215, MinusLogProbMetric: 30.0215, val_loss: 30.8123, val_MinusLogProbMetric: 30.8123

Epoch 403: val_loss did not improve from 29.96072
196/196 - 42s - loss: 30.0215 - MinusLogProbMetric: 30.0215 - val_loss: 30.8123 - val_MinusLogProbMetric: 30.8123 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 404/1000
2023-10-25 21:23:07.832 
Epoch 404/1000 
	 loss: 29.8424, MinusLogProbMetric: 29.8424, val_loss: 30.2743, val_MinusLogProbMetric: 30.2743

Epoch 404: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.8424 - MinusLogProbMetric: 29.8424 - val_loss: 30.2743 - val_MinusLogProbMetric: 30.2743 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 405/1000
2023-10-25 21:23:49.976 
Epoch 405/1000 
	 loss: 30.1057, MinusLogProbMetric: 30.1057, val_loss: 30.0578, val_MinusLogProbMetric: 30.0578

Epoch 405: val_loss did not improve from 29.96072
196/196 - 42s - loss: 30.1057 - MinusLogProbMetric: 30.1057 - val_loss: 30.0578 - val_MinusLogProbMetric: 30.0578 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 406/1000
2023-10-25 21:24:31.967 
Epoch 406/1000 
	 loss: 29.9151, MinusLogProbMetric: 29.9151, val_loss: 30.0529, val_MinusLogProbMetric: 30.0529

Epoch 406: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9151 - MinusLogProbMetric: 29.9151 - val_loss: 30.0529 - val_MinusLogProbMetric: 30.0529 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 407/1000
2023-10-25 21:25:14.160 
Epoch 407/1000 
	 loss: 29.9414, MinusLogProbMetric: 29.9414, val_loss: 30.0990, val_MinusLogProbMetric: 30.0990

Epoch 407: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9414 - MinusLogProbMetric: 29.9414 - val_loss: 30.0990 - val_MinusLogProbMetric: 30.0990 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 408/1000
2023-10-25 21:25:56.432 
Epoch 408/1000 
	 loss: 29.9858, MinusLogProbMetric: 29.9858, val_loss: 30.4561, val_MinusLogProbMetric: 30.4561

Epoch 408: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9858 - MinusLogProbMetric: 29.9858 - val_loss: 30.4561 - val_MinusLogProbMetric: 30.4561 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 409/1000
2023-10-25 21:26:38.790 
Epoch 409/1000 
	 loss: 29.9626, MinusLogProbMetric: 29.9626, val_loss: 30.3965, val_MinusLogProbMetric: 30.3965

Epoch 409: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9626 - MinusLogProbMetric: 29.9626 - val_loss: 30.3965 - val_MinusLogProbMetric: 30.3965 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 410/1000
2023-10-25 21:27:21.812 
Epoch 410/1000 
	 loss: 29.8942, MinusLogProbMetric: 29.8942, val_loss: 30.2276, val_MinusLogProbMetric: 30.2276

Epoch 410: val_loss did not improve from 29.96072
196/196 - 43s - loss: 29.8942 - MinusLogProbMetric: 29.8942 - val_loss: 30.2276 - val_MinusLogProbMetric: 30.2276 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 411/1000
2023-10-25 21:28:04.276 
Epoch 411/1000 
	 loss: 29.9327, MinusLogProbMetric: 29.9327, val_loss: 30.2498, val_MinusLogProbMetric: 30.2498

Epoch 411: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9327 - MinusLogProbMetric: 29.9327 - val_loss: 30.2498 - val_MinusLogProbMetric: 30.2498 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 412/1000
2023-10-25 21:28:46.704 
Epoch 412/1000 
	 loss: 30.0560, MinusLogProbMetric: 30.0560, val_loss: 30.0979, val_MinusLogProbMetric: 30.0979

Epoch 412: val_loss did not improve from 29.96072
196/196 - 42s - loss: 30.0560 - MinusLogProbMetric: 30.0560 - val_loss: 30.0979 - val_MinusLogProbMetric: 30.0979 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 413/1000
2023-10-25 21:29:28.865 
Epoch 413/1000 
	 loss: 29.9108, MinusLogProbMetric: 29.9108, val_loss: 30.0894, val_MinusLogProbMetric: 30.0894

Epoch 413: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9108 - MinusLogProbMetric: 29.9108 - val_loss: 30.0894 - val_MinusLogProbMetric: 30.0894 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 414/1000
2023-10-25 21:30:11.188 
Epoch 414/1000 
	 loss: 29.9953, MinusLogProbMetric: 29.9953, val_loss: 30.0823, val_MinusLogProbMetric: 30.0823

Epoch 414: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9953 - MinusLogProbMetric: 29.9953 - val_loss: 30.0823 - val_MinusLogProbMetric: 30.0823 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 415/1000
2023-10-25 21:30:53.545 
Epoch 415/1000 
	 loss: 29.9107, MinusLogProbMetric: 29.9107, val_loss: 30.2464, val_MinusLogProbMetric: 30.2464

Epoch 415: val_loss did not improve from 29.96072
196/196 - 42s - loss: 29.9107 - MinusLogProbMetric: 29.9107 - val_loss: 30.2464 - val_MinusLogProbMetric: 30.2464 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 416/1000
2023-10-25 21:31:35.676 
Epoch 416/1000 
	 loss: 29.9017, MinusLogProbMetric: 29.9017, val_loss: 29.8916, val_MinusLogProbMetric: 29.8916

Epoch 416: val_loss improved from 29.96072 to 29.89157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.9017 - MinusLogProbMetric: 29.9017 - val_loss: 29.8916 - val_MinusLogProbMetric: 29.8916 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 417/1000
2023-10-25 21:32:18.654 
Epoch 417/1000 
	 loss: 29.8450, MinusLogProbMetric: 29.8450, val_loss: 30.0442, val_MinusLogProbMetric: 30.0442

Epoch 417: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8450 - MinusLogProbMetric: 29.8450 - val_loss: 30.0442 - val_MinusLogProbMetric: 30.0442 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 418/1000
2023-10-25 21:33:00.977 
Epoch 418/1000 
	 loss: 29.9580, MinusLogProbMetric: 29.9580, val_loss: 30.0480, val_MinusLogProbMetric: 30.0480

Epoch 418: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.9580 - MinusLogProbMetric: 29.9580 - val_loss: 30.0480 - val_MinusLogProbMetric: 30.0480 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 419/1000
2023-10-25 21:33:43.421 
Epoch 419/1000 
	 loss: 29.8714, MinusLogProbMetric: 29.8714, val_loss: 30.8527, val_MinusLogProbMetric: 30.8527

Epoch 419: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8714 - MinusLogProbMetric: 29.8714 - val_loss: 30.8527 - val_MinusLogProbMetric: 30.8527 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 420/1000
2023-10-25 21:34:25.064 
Epoch 420/1000 
	 loss: 29.9110, MinusLogProbMetric: 29.9110, val_loss: 30.0753, val_MinusLogProbMetric: 30.0753

Epoch 420: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.9110 - MinusLogProbMetric: 29.9110 - val_loss: 30.0753 - val_MinusLogProbMetric: 30.0753 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 421/1000
2023-10-25 21:35:07.526 
Epoch 421/1000 
	 loss: 29.9531, MinusLogProbMetric: 29.9531, val_loss: 30.0592, val_MinusLogProbMetric: 30.0592

Epoch 421: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.9531 - MinusLogProbMetric: 29.9531 - val_loss: 30.0592 - val_MinusLogProbMetric: 30.0592 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 422/1000
2023-10-25 21:35:49.847 
Epoch 422/1000 
	 loss: 29.8442, MinusLogProbMetric: 29.8442, val_loss: 30.2668, val_MinusLogProbMetric: 30.2668

Epoch 422: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8442 - MinusLogProbMetric: 29.8442 - val_loss: 30.2668 - val_MinusLogProbMetric: 30.2668 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 423/1000
2023-10-25 21:36:31.691 
Epoch 423/1000 
	 loss: 29.8971, MinusLogProbMetric: 29.8971, val_loss: 30.8429, val_MinusLogProbMetric: 30.8429

Epoch 423: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8971 - MinusLogProbMetric: 29.8971 - val_loss: 30.8429 - val_MinusLogProbMetric: 30.8429 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 424/1000
2023-10-25 21:37:14.203 
Epoch 424/1000 
	 loss: 29.9222, MinusLogProbMetric: 29.9222, val_loss: 30.1105, val_MinusLogProbMetric: 30.1105

Epoch 424: val_loss did not improve from 29.89157
196/196 - 43s - loss: 29.9222 - MinusLogProbMetric: 29.9222 - val_loss: 30.1105 - val_MinusLogProbMetric: 30.1105 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 425/1000
2023-10-25 21:37:56.427 
Epoch 425/1000 
	 loss: 29.8158, MinusLogProbMetric: 29.8158, val_loss: 30.1355, val_MinusLogProbMetric: 30.1355

Epoch 425: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8158 - MinusLogProbMetric: 29.8158 - val_loss: 30.1355 - val_MinusLogProbMetric: 30.1355 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 426/1000
2023-10-25 21:38:38.665 
Epoch 426/1000 
	 loss: 29.9191, MinusLogProbMetric: 29.9191, val_loss: 29.9250, val_MinusLogProbMetric: 29.9250

Epoch 426: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.9191 - MinusLogProbMetric: 29.9191 - val_loss: 29.9250 - val_MinusLogProbMetric: 29.9250 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 427/1000
2023-10-25 21:39:20.810 
Epoch 427/1000 
	 loss: 29.8970, MinusLogProbMetric: 29.8970, val_loss: 29.9675, val_MinusLogProbMetric: 29.9675

Epoch 427: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8970 - MinusLogProbMetric: 29.8970 - val_loss: 29.9675 - val_MinusLogProbMetric: 29.9675 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 428/1000
2023-10-25 21:40:02.992 
Epoch 428/1000 
	 loss: 29.8850, MinusLogProbMetric: 29.8850, val_loss: 30.8569, val_MinusLogProbMetric: 30.8569

Epoch 428: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8850 - MinusLogProbMetric: 29.8850 - val_loss: 30.8569 - val_MinusLogProbMetric: 30.8569 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 429/1000
2023-10-25 21:40:45.339 
Epoch 429/1000 
	 loss: 29.8521, MinusLogProbMetric: 29.8521, val_loss: 30.3523, val_MinusLogProbMetric: 30.3523

Epoch 429: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8521 - MinusLogProbMetric: 29.8521 - val_loss: 30.3523 - val_MinusLogProbMetric: 30.3523 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 430/1000
2023-10-25 21:41:27.015 
Epoch 430/1000 
	 loss: 29.8841, MinusLogProbMetric: 29.8841, val_loss: 30.8268, val_MinusLogProbMetric: 30.8268

Epoch 430: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8841 - MinusLogProbMetric: 29.8841 - val_loss: 30.8268 - val_MinusLogProbMetric: 30.8268 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 431/1000
2023-10-25 21:42:09.278 
Epoch 431/1000 
	 loss: 30.1136, MinusLogProbMetric: 30.1136, val_loss: 30.1526, val_MinusLogProbMetric: 30.1526

Epoch 431: val_loss did not improve from 29.89157
196/196 - 42s - loss: 30.1136 - MinusLogProbMetric: 30.1136 - val_loss: 30.1526 - val_MinusLogProbMetric: 30.1526 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 432/1000
2023-10-25 21:42:51.800 
Epoch 432/1000 
	 loss: 29.7023, MinusLogProbMetric: 29.7023, val_loss: 30.4943, val_MinusLogProbMetric: 30.4943

Epoch 432: val_loss did not improve from 29.89157
196/196 - 43s - loss: 29.7023 - MinusLogProbMetric: 29.7023 - val_loss: 30.4943 - val_MinusLogProbMetric: 30.4943 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 433/1000
2023-10-25 21:43:34.148 
Epoch 433/1000 
	 loss: 29.8768, MinusLogProbMetric: 29.8768, val_loss: 30.9251, val_MinusLogProbMetric: 30.9251

Epoch 433: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8768 - MinusLogProbMetric: 29.8768 - val_loss: 30.9251 - val_MinusLogProbMetric: 30.9251 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 434/1000
2023-10-25 21:44:15.989 
Epoch 434/1000 
	 loss: 29.7936, MinusLogProbMetric: 29.7936, val_loss: 29.9646, val_MinusLogProbMetric: 29.9646

Epoch 434: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.7936 - MinusLogProbMetric: 29.7936 - val_loss: 29.9646 - val_MinusLogProbMetric: 29.9646 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 435/1000
2023-10-25 21:44:58.041 
Epoch 435/1000 
	 loss: 29.8791, MinusLogProbMetric: 29.8791, val_loss: 30.0931, val_MinusLogProbMetric: 30.0931

Epoch 435: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8791 - MinusLogProbMetric: 29.8791 - val_loss: 30.0931 - val_MinusLogProbMetric: 30.0931 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 436/1000
2023-10-25 21:45:40.278 
Epoch 436/1000 
	 loss: 29.8985, MinusLogProbMetric: 29.8985, val_loss: 30.1610, val_MinusLogProbMetric: 30.1610

Epoch 436: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8985 - MinusLogProbMetric: 29.8985 - val_loss: 30.1610 - val_MinusLogProbMetric: 30.1610 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 437/1000
2023-10-25 21:46:22.183 
Epoch 437/1000 
	 loss: 29.8372, MinusLogProbMetric: 29.8372, val_loss: 30.6264, val_MinusLogProbMetric: 30.6264

Epoch 437: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8372 - MinusLogProbMetric: 29.8372 - val_loss: 30.6264 - val_MinusLogProbMetric: 30.6264 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 438/1000
2023-10-25 21:47:04.394 
Epoch 438/1000 
	 loss: 29.8825, MinusLogProbMetric: 29.8825, val_loss: 30.0434, val_MinusLogProbMetric: 30.0434

Epoch 438: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8825 - MinusLogProbMetric: 29.8825 - val_loss: 30.0434 - val_MinusLogProbMetric: 30.0434 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 439/1000
2023-10-25 21:47:46.597 
Epoch 439/1000 
	 loss: 29.8039, MinusLogProbMetric: 29.8039, val_loss: 31.1892, val_MinusLogProbMetric: 31.1892

Epoch 439: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8039 - MinusLogProbMetric: 29.8039 - val_loss: 31.1892 - val_MinusLogProbMetric: 31.1892 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 440/1000
2023-10-25 21:48:28.739 
Epoch 440/1000 
	 loss: 29.8937, MinusLogProbMetric: 29.8937, val_loss: 30.4927, val_MinusLogProbMetric: 30.4927

Epoch 440: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.8937 - MinusLogProbMetric: 29.8937 - val_loss: 30.4927 - val_MinusLogProbMetric: 30.4927 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 441/1000
2023-10-25 21:49:10.964 
Epoch 441/1000 
	 loss: 29.7943, MinusLogProbMetric: 29.7943, val_loss: 30.1593, val_MinusLogProbMetric: 30.1593

Epoch 441: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.7943 - MinusLogProbMetric: 29.7943 - val_loss: 30.1593 - val_MinusLogProbMetric: 30.1593 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 442/1000
2023-10-25 21:49:53.331 
Epoch 442/1000 
	 loss: 29.7261, MinusLogProbMetric: 29.7261, val_loss: 30.2154, val_MinusLogProbMetric: 30.2154

Epoch 442: val_loss did not improve from 29.89157
196/196 - 42s - loss: 29.7261 - MinusLogProbMetric: 29.7261 - val_loss: 30.2154 - val_MinusLogProbMetric: 30.2154 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 443/1000
2023-10-25 21:50:35.228 
Epoch 443/1000 
	 loss: 29.8243, MinusLogProbMetric: 29.8243, val_loss: 29.7735, val_MinusLogProbMetric: 29.7735

Epoch 443: val_loss improved from 29.89157 to 29.77353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 29.8243 - MinusLogProbMetric: 29.8243 - val_loss: 29.7735 - val_MinusLogProbMetric: 29.7735 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 444/1000
2023-10-25 21:51:17.748 
Epoch 444/1000 
	 loss: 29.7221, MinusLogProbMetric: 29.7221, val_loss: 29.9614, val_MinusLogProbMetric: 29.9614

Epoch 444: val_loss did not improve from 29.77353
196/196 - 42s - loss: 29.7221 - MinusLogProbMetric: 29.7221 - val_loss: 29.9614 - val_MinusLogProbMetric: 29.9614 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 445/1000
2023-10-25 21:51:59.937 
Epoch 445/1000 
	 loss: 29.7988, MinusLogProbMetric: 29.7988, val_loss: 29.7527, val_MinusLogProbMetric: 29.7527

Epoch 445: val_loss improved from 29.77353 to 29.75269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.7988 - MinusLogProbMetric: 29.7988 - val_loss: 29.7527 - val_MinusLogProbMetric: 29.7527 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 446/1000
2023-10-25 21:52:42.821 
Epoch 446/1000 
	 loss: 29.7630, MinusLogProbMetric: 29.7630, val_loss: 30.3036, val_MinusLogProbMetric: 30.3036

Epoch 446: val_loss did not improve from 29.75269
196/196 - 42s - loss: 29.7630 - MinusLogProbMetric: 29.7630 - val_loss: 30.3036 - val_MinusLogProbMetric: 30.3036 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 447/1000
2023-10-25 21:53:25.364 
Epoch 447/1000 
	 loss: 29.7379, MinusLogProbMetric: 29.7379, val_loss: 30.2764, val_MinusLogProbMetric: 30.2764

Epoch 447: val_loss did not improve from 29.75269
196/196 - 43s - loss: 29.7379 - MinusLogProbMetric: 29.7379 - val_loss: 30.2764 - val_MinusLogProbMetric: 30.2764 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 448/1000
2023-10-25 21:54:07.188 
Epoch 448/1000 
	 loss: 29.8622, MinusLogProbMetric: 29.8622, val_loss: 30.2057, val_MinusLogProbMetric: 30.2057

Epoch 448: val_loss did not improve from 29.75269
196/196 - 42s - loss: 29.8622 - MinusLogProbMetric: 29.8622 - val_loss: 30.2057 - val_MinusLogProbMetric: 30.2057 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 449/1000
2023-10-25 21:54:49.086 
Epoch 449/1000 
	 loss: 29.7040, MinusLogProbMetric: 29.7040, val_loss: 30.7759, val_MinusLogProbMetric: 30.7759

Epoch 449: val_loss did not improve from 29.75269
196/196 - 42s - loss: 29.7040 - MinusLogProbMetric: 29.7040 - val_loss: 30.7759 - val_MinusLogProbMetric: 30.7759 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 450/1000
2023-10-25 21:55:30.929 
Epoch 450/1000 
	 loss: 29.7323, MinusLogProbMetric: 29.7323, val_loss: 32.2512, val_MinusLogProbMetric: 32.2512

Epoch 450: val_loss did not improve from 29.75269
196/196 - 42s - loss: 29.7323 - MinusLogProbMetric: 29.7323 - val_loss: 32.2512 - val_MinusLogProbMetric: 32.2512 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 451/1000
2023-10-25 21:56:13.111 
Epoch 451/1000 
	 loss: 29.6853, MinusLogProbMetric: 29.6853, val_loss: 29.9411, val_MinusLogProbMetric: 29.9411

Epoch 451: val_loss did not improve from 29.75269
196/196 - 42s - loss: 29.6853 - MinusLogProbMetric: 29.6853 - val_loss: 29.9411 - val_MinusLogProbMetric: 29.9411 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 452/1000
2023-10-25 21:56:55.344 
Epoch 452/1000 
	 loss: 29.8046, MinusLogProbMetric: 29.8046, val_loss: 30.6038, val_MinusLogProbMetric: 30.6038

Epoch 452: val_loss did not improve from 29.75269
196/196 - 42s - loss: 29.8046 - MinusLogProbMetric: 29.8046 - val_loss: 30.6038 - val_MinusLogProbMetric: 30.6038 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 453/1000
2023-10-25 21:57:37.322 
Epoch 453/1000 
	 loss: 29.8390, MinusLogProbMetric: 29.8390, val_loss: 29.7452, val_MinusLogProbMetric: 29.7452

Epoch 453: val_loss improved from 29.75269 to 29.74517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.8390 - MinusLogProbMetric: 29.8390 - val_loss: 29.7452 - val_MinusLogProbMetric: 29.7452 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 454/1000
2023-10-25 21:58:19.804 
Epoch 454/1000 
	 loss: 29.7125, MinusLogProbMetric: 29.7125, val_loss: 30.1541, val_MinusLogProbMetric: 30.1541

Epoch 454: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7125 - MinusLogProbMetric: 29.7125 - val_loss: 30.1541 - val_MinusLogProbMetric: 30.1541 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 455/1000
2023-10-25 21:59:02.174 
Epoch 455/1000 
	 loss: 29.6834, MinusLogProbMetric: 29.6834, val_loss: 31.0217, val_MinusLogProbMetric: 31.0217

Epoch 455: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.6834 - MinusLogProbMetric: 29.6834 - val_loss: 31.0217 - val_MinusLogProbMetric: 31.0217 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 456/1000
2023-10-25 21:59:44.130 
Epoch 456/1000 
	 loss: 29.8095, MinusLogProbMetric: 29.8095, val_loss: 30.0555, val_MinusLogProbMetric: 30.0555

Epoch 456: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.8095 - MinusLogProbMetric: 29.8095 - val_loss: 30.0555 - val_MinusLogProbMetric: 30.0555 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 457/1000
2023-10-25 22:00:26.025 
Epoch 457/1000 
	 loss: 29.7043, MinusLogProbMetric: 29.7043, val_loss: 29.8370, val_MinusLogProbMetric: 29.8370

Epoch 457: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7043 - MinusLogProbMetric: 29.7043 - val_loss: 29.8370 - val_MinusLogProbMetric: 29.8370 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 458/1000
2023-10-25 22:01:07.084 
Epoch 458/1000 
	 loss: 29.7687, MinusLogProbMetric: 29.7687, val_loss: 29.9296, val_MinusLogProbMetric: 29.9296

Epoch 458: val_loss did not improve from 29.74517
196/196 - 41s - loss: 29.7687 - MinusLogProbMetric: 29.7687 - val_loss: 29.9296 - val_MinusLogProbMetric: 29.9296 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 459/1000
2023-10-25 22:01:49.425 
Epoch 459/1000 
	 loss: 29.7802, MinusLogProbMetric: 29.7802, val_loss: 30.2834, val_MinusLogProbMetric: 30.2834

Epoch 459: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7802 - MinusLogProbMetric: 29.7802 - val_loss: 30.2834 - val_MinusLogProbMetric: 30.2834 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 460/1000
2023-10-25 22:02:31.112 
Epoch 460/1000 
	 loss: 29.7765, MinusLogProbMetric: 29.7765, val_loss: 30.3711, val_MinusLogProbMetric: 30.3711

Epoch 460: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7765 - MinusLogProbMetric: 29.7765 - val_loss: 30.3711 - val_MinusLogProbMetric: 30.3711 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 461/1000
2023-10-25 22:03:13.548 
Epoch 461/1000 
	 loss: 29.7659, MinusLogProbMetric: 29.7659, val_loss: 30.0071, val_MinusLogProbMetric: 30.0071

Epoch 461: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7659 - MinusLogProbMetric: 29.7659 - val_loss: 30.0071 - val_MinusLogProbMetric: 30.0071 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 462/1000
2023-10-25 22:03:55.693 
Epoch 462/1000 
	 loss: 29.7392, MinusLogProbMetric: 29.7392, val_loss: 29.9988, val_MinusLogProbMetric: 29.9988

Epoch 462: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7392 - MinusLogProbMetric: 29.7392 - val_loss: 29.9988 - val_MinusLogProbMetric: 29.9988 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 463/1000
2023-10-25 22:04:37.692 
Epoch 463/1000 
	 loss: 29.7640, MinusLogProbMetric: 29.7640, val_loss: 30.1599, val_MinusLogProbMetric: 30.1599

Epoch 463: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7640 - MinusLogProbMetric: 29.7640 - val_loss: 30.1599 - val_MinusLogProbMetric: 30.1599 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 464/1000
2023-10-25 22:05:19.768 
Epoch 464/1000 
	 loss: 29.7644, MinusLogProbMetric: 29.7644, val_loss: 30.3119, val_MinusLogProbMetric: 30.3119

Epoch 464: val_loss did not improve from 29.74517
196/196 - 42s - loss: 29.7644 - MinusLogProbMetric: 29.7644 - val_loss: 30.3119 - val_MinusLogProbMetric: 30.3119 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 465/1000
2023-10-25 22:06:01.755 
Epoch 465/1000 
	 loss: 29.6849, MinusLogProbMetric: 29.6849, val_loss: 29.7094, val_MinusLogProbMetric: 29.7094

Epoch 465: val_loss improved from 29.74517 to 29.70936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.6849 - MinusLogProbMetric: 29.6849 - val_loss: 29.7094 - val_MinusLogProbMetric: 29.7094 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 466/1000
2023-10-25 22:06:44.164 
Epoch 466/1000 
	 loss: 29.6165, MinusLogProbMetric: 29.6165, val_loss: 29.7902, val_MinusLogProbMetric: 29.7902

Epoch 466: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6165 - MinusLogProbMetric: 29.6165 - val_loss: 29.7902 - val_MinusLogProbMetric: 29.7902 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 467/1000
2023-10-25 22:07:26.009 
Epoch 467/1000 
	 loss: 29.7593, MinusLogProbMetric: 29.7593, val_loss: 29.7682, val_MinusLogProbMetric: 29.7682

Epoch 467: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.7593 - MinusLogProbMetric: 29.7593 - val_loss: 29.7682 - val_MinusLogProbMetric: 29.7682 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 468/1000
2023-10-25 22:08:07.232 
Epoch 468/1000 
	 loss: 29.6672, MinusLogProbMetric: 29.6672, val_loss: 29.9434, val_MinusLogProbMetric: 29.9434

Epoch 468: val_loss did not improve from 29.70936
196/196 - 41s - loss: 29.6672 - MinusLogProbMetric: 29.6672 - val_loss: 29.9434 - val_MinusLogProbMetric: 29.9434 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 469/1000
2023-10-25 22:08:48.620 
Epoch 469/1000 
	 loss: 29.6916, MinusLogProbMetric: 29.6916, val_loss: 31.1667, val_MinusLogProbMetric: 31.1667

Epoch 469: val_loss did not improve from 29.70936
196/196 - 41s - loss: 29.6916 - MinusLogProbMetric: 29.6916 - val_loss: 31.1667 - val_MinusLogProbMetric: 31.1667 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 470/1000
2023-10-25 22:09:30.578 
Epoch 470/1000 
	 loss: 29.6386, MinusLogProbMetric: 29.6386, val_loss: 30.5846, val_MinusLogProbMetric: 30.5846

Epoch 470: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6386 - MinusLogProbMetric: 29.6386 - val_loss: 30.5846 - val_MinusLogProbMetric: 30.5846 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 471/1000
2023-10-25 22:10:12.604 
Epoch 471/1000 
	 loss: 29.7097, MinusLogProbMetric: 29.7097, val_loss: 30.2666, val_MinusLogProbMetric: 30.2666

Epoch 471: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.7097 - MinusLogProbMetric: 29.7097 - val_loss: 30.2666 - val_MinusLogProbMetric: 30.2666 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 472/1000
2023-10-25 22:10:54.329 
Epoch 472/1000 
	 loss: 29.6016, MinusLogProbMetric: 29.6016, val_loss: 29.9233, val_MinusLogProbMetric: 29.9233

Epoch 472: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6016 - MinusLogProbMetric: 29.6016 - val_loss: 29.9233 - val_MinusLogProbMetric: 29.9233 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 473/1000
2023-10-25 22:11:36.417 
Epoch 473/1000 
	 loss: 29.7085, MinusLogProbMetric: 29.7085, val_loss: 32.0835, val_MinusLogProbMetric: 32.0835

Epoch 473: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.7085 - MinusLogProbMetric: 29.7085 - val_loss: 32.0835 - val_MinusLogProbMetric: 32.0835 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 474/1000
2023-10-25 22:12:18.123 
Epoch 474/1000 
	 loss: 29.7327, MinusLogProbMetric: 29.7327, val_loss: 30.2386, val_MinusLogProbMetric: 30.2386

Epoch 474: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.7327 - MinusLogProbMetric: 29.7327 - val_loss: 30.2386 - val_MinusLogProbMetric: 30.2386 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 475/1000
2023-10-25 22:12:59.860 
Epoch 475/1000 
	 loss: 29.6357, MinusLogProbMetric: 29.6357, val_loss: 29.9956, val_MinusLogProbMetric: 29.9956

Epoch 475: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6357 - MinusLogProbMetric: 29.6357 - val_loss: 29.9956 - val_MinusLogProbMetric: 29.9956 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 476/1000
2023-10-25 22:13:41.866 
Epoch 476/1000 
	 loss: 29.6378, MinusLogProbMetric: 29.6378, val_loss: 30.1939, val_MinusLogProbMetric: 30.1939

Epoch 476: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6378 - MinusLogProbMetric: 29.6378 - val_loss: 30.1939 - val_MinusLogProbMetric: 30.1939 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 477/1000
2023-10-25 22:14:23.443 
Epoch 477/1000 
	 loss: 29.6401, MinusLogProbMetric: 29.6401, val_loss: 29.9536, val_MinusLogProbMetric: 29.9536

Epoch 477: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6401 - MinusLogProbMetric: 29.6401 - val_loss: 29.9536 - val_MinusLogProbMetric: 29.9536 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 478/1000
2023-10-25 22:15:04.802 
Epoch 478/1000 
	 loss: 29.6628, MinusLogProbMetric: 29.6628, val_loss: 29.9344, val_MinusLogProbMetric: 29.9344

Epoch 478: val_loss did not improve from 29.70936
196/196 - 41s - loss: 29.6628 - MinusLogProbMetric: 29.6628 - val_loss: 29.9344 - val_MinusLogProbMetric: 29.9344 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 479/1000
2023-10-25 22:15:46.369 
Epoch 479/1000 
	 loss: 29.6234, MinusLogProbMetric: 29.6234, val_loss: 31.0982, val_MinusLogProbMetric: 31.0982

Epoch 479: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6234 - MinusLogProbMetric: 29.6234 - val_loss: 31.0982 - val_MinusLogProbMetric: 31.0982 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 480/1000
2023-10-25 22:16:28.463 
Epoch 480/1000 
	 loss: 29.7006, MinusLogProbMetric: 29.7006, val_loss: 29.8890, val_MinusLogProbMetric: 29.8890

Epoch 480: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.7006 - MinusLogProbMetric: 29.7006 - val_loss: 29.8890 - val_MinusLogProbMetric: 29.8890 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 481/1000
2023-10-25 22:17:10.122 
Epoch 481/1000 
	 loss: 29.6851, MinusLogProbMetric: 29.6851, val_loss: 30.0395, val_MinusLogProbMetric: 30.0395

Epoch 481: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6851 - MinusLogProbMetric: 29.6851 - val_loss: 30.0395 - val_MinusLogProbMetric: 30.0395 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 482/1000
2023-10-25 22:17:51.896 
Epoch 482/1000 
	 loss: 29.7144, MinusLogProbMetric: 29.7144, val_loss: 29.7884, val_MinusLogProbMetric: 29.7884

Epoch 482: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.7144 - MinusLogProbMetric: 29.7144 - val_loss: 29.7884 - val_MinusLogProbMetric: 29.7884 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 483/1000
2023-10-25 22:18:33.651 
Epoch 483/1000 
	 loss: 29.6289, MinusLogProbMetric: 29.6289, val_loss: 29.8448, val_MinusLogProbMetric: 29.8448

Epoch 483: val_loss did not improve from 29.70936
196/196 - 42s - loss: 29.6289 - MinusLogProbMetric: 29.6289 - val_loss: 29.8448 - val_MinusLogProbMetric: 29.8448 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 484/1000
2023-10-25 22:19:15.458 
Epoch 484/1000 
	 loss: 29.6154, MinusLogProbMetric: 29.6154, val_loss: 29.6758, val_MinusLogProbMetric: 29.6758

Epoch 484: val_loss improved from 29.70936 to 29.67578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.6154 - MinusLogProbMetric: 29.6154 - val_loss: 29.6758 - val_MinusLogProbMetric: 29.6758 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 485/1000
2023-10-25 22:19:57.892 
Epoch 485/1000 
	 loss: 29.6592, MinusLogProbMetric: 29.6592, val_loss: 29.9981, val_MinusLogProbMetric: 29.9981

Epoch 485: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6592 - MinusLogProbMetric: 29.6592 - val_loss: 29.9981 - val_MinusLogProbMetric: 29.9981 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 486/1000
2023-10-25 22:20:40.128 
Epoch 486/1000 
	 loss: 29.6622, MinusLogProbMetric: 29.6622, val_loss: 29.9319, val_MinusLogProbMetric: 29.9319

Epoch 486: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6622 - MinusLogProbMetric: 29.6622 - val_loss: 29.9319 - val_MinusLogProbMetric: 29.9319 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 487/1000
2023-10-25 22:21:22.136 
Epoch 487/1000 
	 loss: 29.6048, MinusLogProbMetric: 29.6048, val_loss: 29.9436, val_MinusLogProbMetric: 29.9436

Epoch 487: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6048 - MinusLogProbMetric: 29.6048 - val_loss: 29.9436 - val_MinusLogProbMetric: 29.9436 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 488/1000
2023-10-25 22:22:04.318 
Epoch 488/1000 
	 loss: 29.6743, MinusLogProbMetric: 29.6743, val_loss: 30.1890, val_MinusLogProbMetric: 30.1890

Epoch 488: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6743 - MinusLogProbMetric: 29.6743 - val_loss: 30.1890 - val_MinusLogProbMetric: 30.1890 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 489/1000
2023-10-25 22:22:46.494 
Epoch 489/1000 
	 loss: 29.6361, MinusLogProbMetric: 29.6361, val_loss: 30.0529, val_MinusLogProbMetric: 30.0529

Epoch 489: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6361 - MinusLogProbMetric: 29.6361 - val_loss: 30.0529 - val_MinusLogProbMetric: 30.0529 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 490/1000
2023-10-25 22:23:28.762 
Epoch 490/1000 
	 loss: 29.6440, MinusLogProbMetric: 29.6440, val_loss: 30.1757, val_MinusLogProbMetric: 30.1757

Epoch 490: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6440 - MinusLogProbMetric: 29.6440 - val_loss: 30.1757 - val_MinusLogProbMetric: 30.1757 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 491/1000
2023-10-25 22:24:11.002 
Epoch 491/1000 
	 loss: 29.6067, MinusLogProbMetric: 29.6067, val_loss: 30.2183, val_MinusLogProbMetric: 30.2183

Epoch 491: val_loss did not improve from 29.67578
196/196 - 42s - loss: 29.6067 - MinusLogProbMetric: 29.6067 - val_loss: 30.2183 - val_MinusLogProbMetric: 30.2183 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 492/1000
2023-10-25 22:24:52.706 
Epoch 492/1000 
	 loss: 29.6319, MinusLogProbMetric: 29.6319, val_loss: 29.5519, val_MinusLogProbMetric: 29.5519

Epoch 492: val_loss improved from 29.67578 to 29.55188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 42s - loss: 29.6319 - MinusLogProbMetric: 29.6319 - val_loss: 29.5519 - val_MinusLogProbMetric: 29.5519 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 493/1000
2023-10-25 22:25:35.425 
Epoch 493/1000 
	 loss: 29.5728, MinusLogProbMetric: 29.5728, val_loss: 30.3854, val_MinusLogProbMetric: 30.3854

Epoch 493: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5728 - MinusLogProbMetric: 29.5728 - val_loss: 30.3854 - val_MinusLogProbMetric: 30.3854 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 494/1000
2023-10-25 22:26:17.748 
Epoch 494/1000 
	 loss: 29.5458, MinusLogProbMetric: 29.5458, val_loss: 29.9660, val_MinusLogProbMetric: 29.9660

Epoch 494: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5458 - MinusLogProbMetric: 29.5458 - val_loss: 29.9660 - val_MinusLogProbMetric: 29.9660 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 495/1000
2023-10-25 22:26:59.905 
Epoch 495/1000 
	 loss: 29.6737, MinusLogProbMetric: 29.6737, val_loss: 29.9059, val_MinusLogProbMetric: 29.9059

Epoch 495: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.6737 - MinusLogProbMetric: 29.6737 - val_loss: 29.9059 - val_MinusLogProbMetric: 29.9059 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 496/1000
2023-10-25 22:27:42.048 
Epoch 496/1000 
	 loss: 29.5503, MinusLogProbMetric: 29.5503, val_loss: 29.8297, val_MinusLogProbMetric: 29.8297

Epoch 496: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5503 - MinusLogProbMetric: 29.5503 - val_loss: 29.8297 - val_MinusLogProbMetric: 29.8297 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 497/1000
2023-10-25 22:28:24.332 
Epoch 497/1000 
	 loss: 29.6052, MinusLogProbMetric: 29.6052, val_loss: 29.8890, val_MinusLogProbMetric: 29.8890

Epoch 497: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.6052 - MinusLogProbMetric: 29.6052 - val_loss: 29.8890 - val_MinusLogProbMetric: 29.8890 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 498/1000
2023-10-25 22:29:06.233 
Epoch 498/1000 
	 loss: 29.6337, MinusLogProbMetric: 29.6337, val_loss: 29.6164, val_MinusLogProbMetric: 29.6164

Epoch 498: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.6337 - MinusLogProbMetric: 29.6337 - val_loss: 29.6164 - val_MinusLogProbMetric: 29.6164 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 499/1000
2023-10-25 22:29:43.546 
Epoch 499/1000 
	 loss: 29.5591, MinusLogProbMetric: 29.5591, val_loss: 29.8717, val_MinusLogProbMetric: 29.8717

Epoch 499: val_loss did not improve from 29.55188
196/196 - 37s - loss: 29.5591 - MinusLogProbMetric: 29.5591 - val_loss: 29.8717 - val_MinusLogProbMetric: 29.8717 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 500/1000
2023-10-25 22:30:24.662 
Epoch 500/1000 
	 loss: 29.7313, MinusLogProbMetric: 29.7313, val_loss: 30.3253, val_MinusLogProbMetric: 30.3253

Epoch 500: val_loss did not improve from 29.55188
196/196 - 41s - loss: 29.7313 - MinusLogProbMetric: 29.7313 - val_loss: 30.3253 - val_MinusLogProbMetric: 30.3253 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 501/1000
2023-10-25 22:31:07.221 
Epoch 501/1000 
	 loss: 29.6251, MinusLogProbMetric: 29.6251, val_loss: 29.8391, val_MinusLogProbMetric: 29.8391

Epoch 501: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.6251 - MinusLogProbMetric: 29.6251 - val_loss: 29.8391 - val_MinusLogProbMetric: 29.8391 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 502/1000
2023-10-25 22:31:49.435 
Epoch 502/1000 
	 loss: 29.5417, MinusLogProbMetric: 29.5417, val_loss: 29.8078, val_MinusLogProbMetric: 29.8078

Epoch 502: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5417 - MinusLogProbMetric: 29.5417 - val_loss: 29.8078 - val_MinusLogProbMetric: 29.8078 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 503/1000
2023-10-25 22:32:31.574 
Epoch 503/1000 
	 loss: 29.5680, MinusLogProbMetric: 29.5680, val_loss: 30.3903, val_MinusLogProbMetric: 30.3903

Epoch 503: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5680 - MinusLogProbMetric: 29.5680 - val_loss: 30.3903 - val_MinusLogProbMetric: 30.3903 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 504/1000
2023-10-25 22:33:13.652 
Epoch 504/1000 
	 loss: 29.6115, MinusLogProbMetric: 29.6115, val_loss: 29.8104, val_MinusLogProbMetric: 29.8104

Epoch 504: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.6115 - MinusLogProbMetric: 29.6115 - val_loss: 29.8104 - val_MinusLogProbMetric: 29.8104 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 505/1000
2023-10-25 22:33:56.394 
Epoch 505/1000 
	 loss: 29.6230, MinusLogProbMetric: 29.6230, val_loss: 29.8910, val_MinusLogProbMetric: 29.8910

Epoch 505: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.6230 - MinusLogProbMetric: 29.6230 - val_loss: 29.8910 - val_MinusLogProbMetric: 29.8910 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 506/1000
2023-10-25 22:34:38.611 
Epoch 506/1000 
	 loss: 29.5869, MinusLogProbMetric: 29.5869, val_loss: 29.7785, val_MinusLogProbMetric: 29.7785

Epoch 506: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5869 - MinusLogProbMetric: 29.5869 - val_loss: 29.7785 - val_MinusLogProbMetric: 29.7785 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 507/1000
2023-10-25 22:35:20.922 
Epoch 507/1000 
	 loss: 29.5637, MinusLogProbMetric: 29.5637, val_loss: 29.7955, val_MinusLogProbMetric: 29.7955

Epoch 507: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5637 - MinusLogProbMetric: 29.5637 - val_loss: 29.7955 - val_MinusLogProbMetric: 29.7955 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 508/1000
2023-10-25 22:36:03.897 
Epoch 508/1000 
	 loss: 29.5630, MinusLogProbMetric: 29.5630, val_loss: 29.6687, val_MinusLogProbMetric: 29.6687

Epoch 508: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5630 - MinusLogProbMetric: 29.5630 - val_loss: 29.6687 - val_MinusLogProbMetric: 29.6687 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 509/1000
2023-10-25 22:36:46.369 
Epoch 509/1000 
	 loss: 29.5976, MinusLogProbMetric: 29.5976, val_loss: 30.3684, val_MinusLogProbMetric: 30.3684

Epoch 509: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5976 - MinusLogProbMetric: 29.5976 - val_loss: 30.3684 - val_MinusLogProbMetric: 30.3684 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 510/1000
2023-10-25 22:37:29.150 
Epoch 510/1000 
	 loss: 29.5802, MinusLogProbMetric: 29.5802, val_loss: 29.7086, val_MinusLogProbMetric: 29.7086

Epoch 510: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5802 - MinusLogProbMetric: 29.5802 - val_loss: 29.7086 - val_MinusLogProbMetric: 29.7086 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 511/1000
2023-10-25 22:38:11.918 
Epoch 511/1000 
	 loss: 29.5494, MinusLogProbMetric: 29.5494, val_loss: 29.8336, val_MinusLogProbMetric: 29.8336

Epoch 511: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5494 - MinusLogProbMetric: 29.5494 - val_loss: 29.8336 - val_MinusLogProbMetric: 29.8336 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 512/1000
2023-10-25 22:38:54.056 
Epoch 512/1000 
	 loss: 29.5245, MinusLogProbMetric: 29.5245, val_loss: 29.9392, val_MinusLogProbMetric: 29.9392

Epoch 512: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5245 - MinusLogProbMetric: 29.5245 - val_loss: 29.9392 - val_MinusLogProbMetric: 29.9392 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 513/1000
2023-10-25 22:39:36.915 
Epoch 513/1000 
	 loss: 29.5634, MinusLogProbMetric: 29.5634, val_loss: 29.8319, val_MinusLogProbMetric: 29.8319

Epoch 513: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5634 - MinusLogProbMetric: 29.5634 - val_loss: 29.8319 - val_MinusLogProbMetric: 29.8319 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 514/1000
2023-10-25 22:40:19.410 
Epoch 514/1000 
	 loss: 29.5346, MinusLogProbMetric: 29.5346, val_loss: 29.8401, val_MinusLogProbMetric: 29.8401

Epoch 514: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5346 - MinusLogProbMetric: 29.5346 - val_loss: 29.8401 - val_MinusLogProbMetric: 29.8401 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 515/1000
2023-10-25 22:41:02.096 
Epoch 515/1000 
	 loss: 29.5154, MinusLogProbMetric: 29.5154, val_loss: 30.2238, val_MinusLogProbMetric: 30.2238

Epoch 515: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5154 - MinusLogProbMetric: 29.5154 - val_loss: 30.2238 - val_MinusLogProbMetric: 30.2238 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 516/1000
2023-10-25 22:41:44.736 
Epoch 516/1000 
	 loss: 29.5123, MinusLogProbMetric: 29.5123, val_loss: 30.3186, val_MinusLogProbMetric: 30.3186

Epoch 516: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5123 - MinusLogProbMetric: 29.5123 - val_loss: 30.3186 - val_MinusLogProbMetric: 30.3186 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 517/1000
2023-10-25 22:42:27.548 
Epoch 517/1000 
	 loss: 29.4831, MinusLogProbMetric: 29.4831, val_loss: 30.2388, val_MinusLogProbMetric: 30.2388

Epoch 517: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.4831 - MinusLogProbMetric: 29.4831 - val_loss: 30.2388 - val_MinusLogProbMetric: 30.2388 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 518/1000
2023-10-25 22:43:10.099 
Epoch 518/1000 
	 loss: 29.5444, MinusLogProbMetric: 29.5444, val_loss: 30.7191, val_MinusLogProbMetric: 30.7191

Epoch 518: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5444 - MinusLogProbMetric: 29.5444 - val_loss: 30.7191 - val_MinusLogProbMetric: 30.7191 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 519/1000
2023-10-25 22:43:52.364 
Epoch 519/1000 
	 loss: 29.5434, MinusLogProbMetric: 29.5434, val_loss: 30.0759, val_MinusLogProbMetric: 30.0759

Epoch 519: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5434 - MinusLogProbMetric: 29.5434 - val_loss: 30.0759 - val_MinusLogProbMetric: 30.0759 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 520/1000
2023-10-25 22:44:34.912 
Epoch 520/1000 
	 loss: 29.5909, MinusLogProbMetric: 29.5909, val_loss: 29.7030, val_MinusLogProbMetric: 29.7030

Epoch 520: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5909 - MinusLogProbMetric: 29.5909 - val_loss: 29.7030 - val_MinusLogProbMetric: 29.7030 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 521/1000
2023-10-25 22:45:17.212 
Epoch 521/1000 
	 loss: 29.5730, MinusLogProbMetric: 29.5730, val_loss: 30.1057, val_MinusLogProbMetric: 30.1057

Epoch 521: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5730 - MinusLogProbMetric: 29.5730 - val_loss: 30.1057 - val_MinusLogProbMetric: 30.1057 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 522/1000
2023-10-25 22:45:59.641 
Epoch 522/1000 
	 loss: 29.5239, MinusLogProbMetric: 29.5239, val_loss: 29.6849, val_MinusLogProbMetric: 29.6849

Epoch 522: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.5239 - MinusLogProbMetric: 29.5239 - val_loss: 29.6849 - val_MinusLogProbMetric: 29.6849 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 523/1000
2023-10-25 22:46:42.252 
Epoch 523/1000 
	 loss: 29.5092, MinusLogProbMetric: 29.5092, val_loss: 29.7170, val_MinusLogProbMetric: 29.7170

Epoch 523: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5092 - MinusLogProbMetric: 29.5092 - val_loss: 29.7170 - val_MinusLogProbMetric: 29.7170 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 524/1000
2023-10-25 22:47:24.529 
Epoch 524/1000 
	 loss: 29.4596, MinusLogProbMetric: 29.4596, val_loss: 29.5910, val_MinusLogProbMetric: 29.5910

Epoch 524: val_loss did not improve from 29.55188
196/196 - 42s - loss: 29.4596 - MinusLogProbMetric: 29.4596 - val_loss: 29.5910 - val_MinusLogProbMetric: 29.5910 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 525/1000
2023-10-25 22:48:07.059 
Epoch 525/1000 
	 loss: 29.5517, MinusLogProbMetric: 29.5517, val_loss: 29.6187, val_MinusLogProbMetric: 29.6187

Epoch 525: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5517 - MinusLogProbMetric: 29.5517 - val_loss: 29.6187 - val_MinusLogProbMetric: 29.6187 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 526/1000
2023-10-25 22:48:49.749 
Epoch 526/1000 
	 loss: 29.5049, MinusLogProbMetric: 29.5049, val_loss: 29.8195, val_MinusLogProbMetric: 29.8195

Epoch 526: val_loss did not improve from 29.55188
196/196 - 43s - loss: 29.5049 - MinusLogProbMetric: 29.5049 - val_loss: 29.8195 - val_MinusLogProbMetric: 29.8195 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 527/1000
2023-10-25 22:49:31.963 
Epoch 527/1000 
	 loss: 29.4526, MinusLogProbMetric: 29.4526, val_loss: 29.4768, val_MinusLogProbMetric: 29.4768

Epoch 527: val_loss improved from 29.55188 to 29.47679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.4526 - MinusLogProbMetric: 29.4526 - val_loss: 29.4768 - val_MinusLogProbMetric: 29.4768 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 528/1000
2023-10-25 22:50:15.442 
Epoch 528/1000 
	 loss: 29.4862, MinusLogProbMetric: 29.4862, val_loss: 29.8297, val_MinusLogProbMetric: 29.8297

Epoch 528: val_loss did not improve from 29.47679
196/196 - 43s - loss: 29.4862 - MinusLogProbMetric: 29.4862 - val_loss: 29.8297 - val_MinusLogProbMetric: 29.8297 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 529/1000
2023-10-25 22:50:58.071 
Epoch 529/1000 
	 loss: 29.4351, MinusLogProbMetric: 29.4351, val_loss: 29.9683, val_MinusLogProbMetric: 29.9683

Epoch 529: val_loss did not improve from 29.47679
196/196 - 43s - loss: 29.4351 - MinusLogProbMetric: 29.4351 - val_loss: 29.9683 - val_MinusLogProbMetric: 29.9683 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 530/1000
2023-10-25 22:51:40.311 
Epoch 530/1000 
	 loss: 29.3766, MinusLogProbMetric: 29.3766, val_loss: 30.1859, val_MinusLogProbMetric: 30.1859

Epoch 530: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.3766 - MinusLogProbMetric: 29.3766 - val_loss: 30.1859 - val_MinusLogProbMetric: 30.1859 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 531/1000
2023-10-25 22:52:22.779 
Epoch 531/1000 
	 loss: 29.4428, MinusLogProbMetric: 29.4428, val_loss: 30.3345, val_MinusLogProbMetric: 30.3345

Epoch 531: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.4428 - MinusLogProbMetric: 29.4428 - val_loss: 30.3345 - val_MinusLogProbMetric: 30.3345 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 532/1000
2023-10-25 22:53:06.016 
Epoch 532/1000 
	 loss: 29.4169, MinusLogProbMetric: 29.4169, val_loss: 29.7624, val_MinusLogProbMetric: 29.7624

Epoch 532: val_loss did not improve from 29.47679
196/196 - 43s - loss: 29.4169 - MinusLogProbMetric: 29.4169 - val_loss: 29.7624 - val_MinusLogProbMetric: 29.7624 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 533/1000
2023-10-25 22:53:48.642 
Epoch 533/1000 
	 loss: 29.5835, MinusLogProbMetric: 29.5835, val_loss: 29.5015, val_MinusLogProbMetric: 29.5015

Epoch 533: val_loss did not improve from 29.47679
196/196 - 43s - loss: 29.5835 - MinusLogProbMetric: 29.5835 - val_loss: 29.5015 - val_MinusLogProbMetric: 29.5015 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 534/1000
2023-10-25 22:54:31.044 
Epoch 534/1000 
	 loss: 29.5033, MinusLogProbMetric: 29.5033, val_loss: 29.7374, val_MinusLogProbMetric: 29.7374

Epoch 534: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.5033 - MinusLogProbMetric: 29.5033 - val_loss: 29.7374 - val_MinusLogProbMetric: 29.7374 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 535/1000
2023-10-25 22:55:13.342 
Epoch 535/1000 
	 loss: 29.6112, MinusLogProbMetric: 29.6112, val_loss: 29.7441, val_MinusLogProbMetric: 29.7441

Epoch 535: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.6112 - MinusLogProbMetric: 29.6112 - val_loss: 29.7441 - val_MinusLogProbMetric: 29.7441 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 536/1000
2023-10-25 22:55:55.466 
Epoch 536/1000 
	 loss: 29.3971, MinusLogProbMetric: 29.3971, val_loss: 29.5741, val_MinusLogProbMetric: 29.5741

Epoch 536: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.3971 - MinusLogProbMetric: 29.3971 - val_loss: 29.5741 - val_MinusLogProbMetric: 29.5741 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 537/1000
2023-10-25 22:56:39.865 
Epoch 537/1000 
	 loss: 29.5320, MinusLogProbMetric: 29.5320, val_loss: 30.0486, val_MinusLogProbMetric: 30.0486

Epoch 537: val_loss did not improve from 29.47679
196/196 - 44s - loss: 29.5320 - MinusLogProbMetric: 29.5320 - val_loss: 30.0486 - val_MinusLogProbMetric: 30.0486 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 538/1000
2023-10-25 22:57:21.636 
Epoch 538/1000 
	 loss: 29.4665, MinusLogProbMetric: 29.4665, val_loss: 30.0823, val_MinusLogProbMetric: 30.0823

Epoch 538: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.4665 - MinusLogProbMetric: 29.4665 - val_loss: 30.0823 - val_MinusLogProbMetric: 30.0823 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 539/1000
2023-10-25 22:58:03.893 
Epoch 539/1000 
	 loss: 29.4109, MinusLogProbMetric: 29.4109, val_loss: 29.8540, val_MinusLogProbMetric: 29.8540

Epoch 539: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.4109 - MinusLogProbMetric: 29.4109 - val_loss: 29.8540 - val_MinusLogProbMetric: 29.8540 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 540/1000
2023-10-25 22:58:46.161 
Epoch 540/1000 
	 loss: 29.4218, MinusLogProbMetric: 29.4218, val_loss: 29.7464, val_MinusLogProbMetric: 29.7464

Epoch 540: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.4218 - MinusLogProbMetric: 29.4218 - val_loss: 29.7464 - val_MinusLogProbMetric: 29.7464 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 541/1000
2023-10-25 22:59:28.287 
Epoch 541/1000 
	 loss: 29.4623, MinusLogProbMetric: 29.4623, val_loss: 30.2137, val_MinusLogProbMetric: 30.2137

Epoch 541: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.4623 - MinusLogProbMetric: 29.4623 - val_loss: 30.2137 - val_MinusLogProbMetric: 30.2137 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 542/1000
2023-10-25 23:00:10.426 
Epoch 542/1000 
	 loss: 29.3883, MinusLogProbMetric: 29.3883, val_loss: 29.9950, val_MinusLogProbMetric: 29.9950

Epoch 542: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.3883 - MinusLogProbMetric: 29.3883 - val_loss: 29.9950 - val_MinusLogProbMetric: 29.9950 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 543/1000
2023-10-25 23:00:52.694 
Epoch 543/1000 
	 loss: 29.4225, MinusLogProbMetric: 29.4225, val_loss: 30.0886, val_MinusLogProbMetric: 30.0886

Epoch 543: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.4225 - MinusLogProbMetric: 29.4225 - val_loss: 30.0886 - val_MinusLogProbMetric: 30.0886 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 544/1000
2023-10-25 23:01:34.887 
Epoch 544/1000 
	 loss: 29.3829, MinusLogProbMetric: 29.3829, val_loss: 29.6523, val_MinusLogProbMetric: 29.6523

Epoch 544: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.3829 - MinusLogProbMetric: 29.3829 - val_loss: 29.6523 - val_MinusLogProbMetric: 29.6523 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 545/1000
2023-10-25 23:02:16.885 
Epoch 545/1000 
	 loss: 29.5599, MinusLogProbMetric: 29.5599, val_loss: 29.7719, val_MinusLogProbMetric: 29.7719

Epoch 545: val_loss did not improve from 29.47679
196/196 - 42s - loss: 29.5599 - MinusLogProbMetric: 29.5599 - val_loss: 29.7719 - val_MinusLogProbMetric: 29.7719 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 546/1000
2023-10-25 23:02:58.958 
Epoch 546/1000 
	 loss: 29.4387, MinusLogProbMetric: 29.4387, val_loss: 29.4118, val_MinusLogProbMetric: 29.4118

Epoch 546: val_loss improved from 29.47679 to 29.41178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 29.4387 - MinusLogProbMetric: 29.4387 - val_loss: 29.4118 - val_MinusLogProbMetric: 29.4118 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 547/1000
2023-10-25 23:03:41.670 
Epoch 547/1000 
	 loss: 29.3622, MinusLogProbMetric: 29.3622, val_loss: 30.1178, val_MinusLogProbMetric: 30.1178

Epoch 547: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3622 - MinusLogProbMetric: 29.3622 - val_loss: 30.1178 - val_MinusLogProbMetric: 30.1178 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 548/1000
2023-10-25 23:04:24.116 
Epoch 548/1000 
	 loss: 29.3967, MinusLogProbMetric: 29.3967, val_loss: 29.5980, val_MinusLogProbMetric: 29.5980

Epoch 548: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3967 - MinusLogProbMetric: 29.3967 - val_loss: 29.5980 - val_MinusLogProbMetric: 29.5980 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 549/1000
2023-10-25 23:05:06.135 
Epoch 549/1000 
	 loss: 29.4878, MinusLogProbMetric: 29.4878, val_loss: 29.8175, val_MinusLogProbMetric: 29.8175

Epoch 549: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4878 - MinusLogProbMetric: 29.4878 - val_loss: 29.8175 - val_MinusLogProbMetric: 29.8175 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 550/1000
2023-10-25 23:05:48.357 
Epoch 550/1000 
	 loss: 29.4166, MinusLogProbMetric: 29.4166, val_loss: 30.2557, val_MinusLogProbMetric: 30.2557

Epoch 550: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4166 - MinusLogProbMetric: 29.4166 - val_loss: 30.2557 - val_MinusLogProbMetric: 30.2557 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 551/1000
2023-10-25 23:06:30.557 
Epoch 551/1000 
	 loss: 29.4803, MinusLogProbMetric: 29.4803, val_loss: 30.0008, val_MinusLogProbMetric: 30.0008

Epoch 551: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4803 - MinusLogProbMetric: 29.4803 - val_loss: 30.0008 - val_MinusLogProbMetric: 30.0008 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 552/1000
2023-10-25 23:07:12.924 
Epoch 552/1000 
	 loss: 29.4446, MinusLogProbMetric: 29.4446, val_loss: 29.6764, val_MinusLogProbMetric: 29.6764

Epoch 552: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4446 - MinusLogProbMetric: 29.4446 - val_loss: 29.6764 - val_MinusLogProbMetric: 29.6764 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 553/1000
2023-10-25 23:07:55.379 
Epoch 553/1000 
	 loss: 29.4340, MinusLogProbMetric: 29.4340, val_loss: 29.9398, val_MinusLogProbMetric: 29.9398

Epoch 553: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4340 - MinusLogProbMetric: 29.4340 - val_loss: 29.9398 - val_MinusLogProbMetric: 29.9398 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 554/1000
2023-10-25 23:08:37.349 
Epoch 554/1000 
	 loss: 29.4456, MinusLogProbMetric: 29.4456, val_loss: 29.8841, val_MinusLogProbMetric: 29.8841

Epoch 554: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4456 - MinusLogProbMetric: 29.4456 - val_loss: 29.8841 - val_MinusLogProbMetric: 29.8841 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 555/1000
2023-10-25 23:09:19.588 
Epoch 555/1000 
	 loss: 29.3739, MinusLogProbMetric: 29.3739, val_loss: 29.7282, val_MinusLogProbMetric: 29.7282

Epoch 555: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3739 - MinusLogProbMetric: 29.3739 - val_loss: 29.7282 - val_MinusLogProbMetric: 29.7282 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 556/1000
2023-10-25 23:10:01.703 
Epoch 556/1000 
	 loss: 29.3734, MinusLogProbMetric: 29.3734, val_loss: 30.2545, val_MinusLogProbMetric: 30.2545

Epoch 556: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3734 - MinusLogProbMetric: 29.3734 - val_loss: 30.2545 - val_MinusLogProbMetric: 30.2545 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 557/1000
2023-10-25 23:10:43.835 
Epoch 557/1000 
	 loss: 29.4290, MinusLogProbMetric: 29.4290, val_loss: 29.6761, val_MinusLogProbMetric: 29.6761

Epoch 557: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4290 - MinusLogProbMetric: 29.4290 - val_loss: 29.6761 - val_MinusLogProbMetric: 29.6761 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 558/1000
2023-10-25 23:11:26.294 
Epoch 558/1000 
	 loss: 29.3915, MinusLogProbMetric: 29.3915, val_loss: 29.5383, val_MinusLogProbMetric: 29.5383

Epoch 558: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3915 - MinusLogProbMetric: 29.3915 - val_loss: 29.5383 - val_MinusLogProbMetric: 29.5383 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 559/1000
2023-10-25 23:12:08.854 
Epoch 559/1000 
	 loss: 29.3626, MinusLogProbMetric: 29.3626, val_loss: 31.0648, val_MinusLogProbMetric: 31.0648

Epoch 559: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.3626 - MinusLogProbMetric: 29.3626 - val_loss: 31.0648 - val_MinusLogProbMetric: 31.0648 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 560/1000
2023-10-25 23:12:51.265 
Epoch 560/1000 
	 loss: 29.4250, MinusLogProbMetric: 29.4250, val_loss: 30.7433, val_MinusLogProbMetric: 30.7433

Epoch 560: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4250 - MinusLogProbMetric: 29.4250 - val_loss: 30.7433 - val_MinusLogProbMetric: 30.7433 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 561/1000
2023-10-25 23:13:33.437 
Epoch 561/1000 
	 loss: 29.3870, MinusLogProbMetric: 29.3870, val_loss: 29.4535, val_MinusLogProbMetric: 29.4535

Epoch 561: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3870 - MinusLogProbMetric: 29.3870 - val_loss: 29.4535 - val_MinusLogProbMetric: 29.4535 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 562/1000
2023-10-25 23:14:15.497 
Epoch 562/1000 
	 loss: 29.4537, MinusLogProbMetric: 29.4537, val_loss: 29.6387, val_MinusLogProbMetric: 29.6387

Epoch 562: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4537 - MinusLogProbMetric: 29.4537 - val_loss: 29.6387 - val_MinusLogProbMetric: 29.6387 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 563/1000
2023-10-25 23:14:57.808 
Epoch 563/1000 
	 loss: 29.3623, MinusLogProbMetric: 29.3623, val_loss: 29.7664, val_MinusLogProbMetric: 29.7664

Epoch 563: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3623 - MinusLogProbMetric: 29.3623 - val_loss: 29.7664 - val_MinusLogProbMetric: 29.7664 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 564/1000
2023-10-25 23:15:39.948 
Epoch 564/1000 
	 loss: 29.3883, MinusLogProbMetric: 29.3883, val_loss: 30.0383, val_MinusLogProbMetric: 30.0383

Epoch 564: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3883 - MinusLogProbMetric: 29.3883 - val_loss: 30.0383 - val_MinusLogProbMetric: 30.0383 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 565/1000
2023-10-25 23:16:22.121 
Epoch 565/1000 
	 loss: 29.5224, MinusLogProbMetric: 29.5224, val_loss: 29.4682, val_MinusLogProbMetric: 29.4682

Epoch 565: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.5224 - MinusLogProbMetric: 29.5224 - val_loss: 29.4682 - val_MinusLogProbMetric: 29.4682 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 566/1000
2023-10-25 23:17:04.737 
Epoch 566/1000 
	 loss: 29.3625, MinusLogProbMetric: 29.3625, val_loss: 29.6490, val_MinusLogProbMetric: 29.6490

Epoch 566: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.3625 - MinusLogProbMetric: 29.3625 - val_loss: 29.6490 - val_MinusLogProbMetric: 29.6490 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 567/1000
2023-10-25 23:17:47.070 
Epoch 567/1000 
	 loss: 29.3314, MinusLogProbMetric: 29.3314, val_loss: 29.5404, val_MinusLogProbMetric: 29.5404

Epoch 567: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3314 - MinusLogProbMetric: 29.3314 - val_loss: 29.5404 - val_MinusLogProbMetric: 29.5404 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 568/1000
2023-10-25 23:18:29.490 
Epoch 568/1000 
	 loss: 29.4509, MinusLogProbMetric: 29.4509, val_loss: 29.4225, val_MinusLogProbMetric: 29.4225

Epoch 568: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4509 - MinusLogProbMetric: 29.4509 - val_loss: 29.4225 - val_MinusLogProbMetric: 29.4225 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 569/1000
2023-10-25 23:19:12.142 
Epoch 569/1000 
	 loss: 29.4567, MinusLogProbMetric: 29.4567, val_loss: 29.6814, val_MinusLogProbMetric: 29.6814

Epoch 569: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.4567 - MinusLogProbMetric: 29.4567 - val_loss: 29.6814 - val_MinusLogProbMetric: 29.6814 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 570/1000
2023-10-25 23:19:54.647 
Epoch 570/1000 
	 loss: 29.4215, MinusLogProbMetric: 29.4215, val_loss: 29.7581, val_MinusLogProbMetric: 29.7581

Epoch 570: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.4215 - MinusLogProbMetric: 29.4215 - val_loss: 29.7581 - val_MinusLogProbMetric: 29.7581 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 571/1000
2023-10-25 23:20:37.021 
Epoch 571/1000 
	 loss: 29.4245, MinusLogProbMetric: 29.4245, val_loss: 29.8305, val_MinusLogProbMetric: 29.8305

Epoch 571: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4245 - MinusLogProbMetric: 29.4245 - val_loss: 29.8305 - val_MinusLogProbMetric: 29.8305 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 572/1000
2023-10-25 23:21:19.311 
Epoch 572/1000 
	 loss: 29.3757, MinusLogProbMetric: 29.3757, val_loss: 30.0531, val_MinusLogProbMetric: 30.0531

Epoch 572: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3757 - MinusLogProbMetric: 29.3757 - val_loss: 30.0531 - val_MinusLogProbMetric: 30.0531 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 573/1000
2023-10-25 23:22:01.874 
Epoch 573/1000 
	 loss: 29.2573, MinusLogProbMetric: 29.2573, val_loss: 29.7012, val_MinusLogProbMetric: 29.7012

Epoch 573: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.2573 - MinusLogProbMetric: 29.2573 - val_loss: 29.7012 - val_MinusLogProbMetric: 29.7012 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 574/1000
2023-10-25 23:22:43.899 
Epoch 574/1000 
	 loss: 29.2819, MinusLogProbMetric: 29.2819, val_loss: 31.1763, val_MinusLogProbMetric: 31.1763

Epoch 574: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.2819 - MinusLogProbMetric: 29.2819 - val_loss: 31.1763 - val_MinusLogProbMetric: 31.1763 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 575/1000
2023-10-25 23:23:26.095 
Epoch 575/1000 
	 loss: 29.4852, MinusLogProbMetric: 29.4852, val_loss: 29.7869, val_MinusLogProbMetric: 29.7869

Epoch 575: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4852 - MinusLogProbMetric: 29.4852 - val_loss: 29.7869 - val_MinusLogProbMetric: 29.7869 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 576/1000
2023-10-25 23:24:08.567 
Epoch 576/1000 
	 loss: 29.5470, MinusLogProbMetric: 29.5470, val_loss: 30.3719, val_MinusLogProbMetric: 30.3719

Epoch 576: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.5470 - MinusLogProbMetric: 29.5470 - val_loss: 30.3719 - val_MinusLogProbMetric: 30.3719 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 577/1000
2023-10-25 23:24:50.722 
Epoch 577/1000 
	 loss: 29.3552, MinusLogProbMetric: 29.3552, val_loss: 29.7426, val_MinusLogProbMetric: 29.7426

Epoch 577: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3552 - MinusLogProbMetric: 29.3552 - val_loss: 29.7426 - val_MinusLogProbMetric: 29.7426 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 578/1000
2023-10-25 23:25:33.304 
Epoch 578/1000 
	 loss: 29.4226, MinusLogProbMetric: 29.4226, val_loss: 29.4546, val_MinusLogProbMetric: 29.4546

Epoch 578: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.4226 - MinusLogProbMetric: 29.4226 - val_loss: 29.4546 - val_MinusLogProbMetric: 29.4546 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 579/1000
2023-10-25 23:26:15.502 
Epoch 579/1000 
	 loss: 29.3556, MinusLogProbMetric: 29.3556, val_loss: 30.0850, val_MinusLogProbMetric: 30.0850

Epoch 579: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3556 - MinusLogProbMetric: 29.3556 - val_loss: 30.0850 - val_MinusLogProbMetric: 30.0850 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 580/1000
2023-10-25 23:26:57.720 
Epoch 580/1000 
	 loss: 29.3144, MinusLogProbMetric: 29.3144, val_loss: 29.8163, val_MinusLogProbMetric: 29.8163

Epoch 580: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3144 - MinusLogProbMetric: 29.3144 - val_loss: 29.8163 - val_MinusLogProbMetric: 29.8163 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 581/1000
2023-10-25 23:27:40.304 
Epoch 581/1000 
	 loss: 29.3374, MinusLogProbMetric: 29.3374, val_loss: 29.9475, val_MinusLogProbMetric: 29.9475

Epoch 581: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.3374 - MinusLogProbMetric: 29.3374 - val_loss: 29.9475 - val_MinusLogProbMetric: 29.9475 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 582/1000
2023-10-25 23:28:23.177 
Epoch 582/1000 
	 loss: 29.3519, MinusLogProbMetric: 29.3519, val_loss: 29.6523, val_MinusLogProbMetric: 29.6523

Epoch 582: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.3519 - MinusLogProbMetric: 29.3519 - val_loss: 29.6523 - val_MinusLogProbMetric: 29.6523 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 583/1000
2023-10-25 23:29:05.176 
Epoch 583/1000 
	 loss: 29.3432, MinusLogProbMetric: 29.3432, val_loss: 29.7554, val_MinusLogProbMetric: 29.7554

Epoch 583: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3432 - MinusLogProbMetric: 29.3432 - val_loss: 29.7554 - val_MinusLogProbMetric: 29.7554 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 584/1000
2023-10-25 23:29:47.393 
Epoch 584/1000 
	 loss: 29.3080, MinusLogProbMetric: 29.3080, val_loss: 29.8858, val_MinusLogProbMetric: 29.8858

Epoch 584: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3080 - MinusLogProbMetric: 29.3080 - val_loss: 29.8858 - val_MinusLogProbMetric: 29.8858 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 585/1000
2023-10-25 23:30:29.701 
Epoch 585/1000 
	 loss: 29.3150, MinusLogProbMetric: 29.3150, val_loss: 29.5391, val_MinusLogProbMetric: 29.5391

Epoch 585: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3150 - MinusLogProbMetric: 29.3150 - val_loss: 29.5391 - val_MinusLogProbMetric: 29.5391 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 586/1000
2023-10-25 23:31:11.733 
Epoch 586/1000 
	 loss: 29.4151, MinusLogProbMetric: 29.4151, val_loss: 29.7278, val_MinusLogProbMetric: 29.7278

Epoch 586: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.4151 - MinusLogProbMetric: 29.4151 - val_loss: 29.7278 - val_MinusLogProbMetric: 29.7278 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 587/1000
2023-10-25 23:31:54.191 
Epoch 587/1000 
	 loss: 29.2955, MinusLogProbMetric: 29.2955, val_loss: 29.5891, val_MinusLogProbMetric: 29.5891

Epoch 587: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.2955 - MinusLogProbMetric: 29.2955 - val_loss: 29.5891 - val_MinusLogProbMetric: 29.5891 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 588/1000
2023-10-25 23:32:36.346 
Epoch 588/1000 
	 loss: 29.3708, MinusLogProbMetric: 29.3708, val_loss: 29.6863, val_MinusLogProbMetric: 29.6863

Epoch 588: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3708 - MinusLogProbMetric: 29.3708 - val_loss: 29.6863 - val_MinusLogProbMetric: 29.6863 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 589/1000
2023-10-25 23:33:18.778 
Epoch 589/1000 
	 loss: 29.3498, MinusLogProbMetric: 29.3498, val_loss: 29.8301, val_MinusLogProbMetric: 29.8301

Epoch 589: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3498 - MinusLogProbMetric: 29.3498 - val_loss: 29.8301 - val_MinusLogProbMetric: 29.8301 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 590/1000
2023-10-25 23:34:01.076 
Epoch 590/1000 
	 loss: 29.3962, MinusLogProbMetric: 29.3962, val_loss: 29.4582, val_MinusLogProbMetric: 29.4582

Epoch 590: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3962 - MinusLogProbMetric: 29.3962 - val_loss: 29.4582 - val_MinusLogProbMetric: 29.4582 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 591/1000
2023-10-25 23:34:43.830 
Epoch 591/1000 
	 loss: 29.3160, MinusLogProbMetric: 29.3160, val_loss: 29.7219, val_MinusLogProbMetric: 29.7219

Epoch 591: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.3160 - MinusLogProbMetric: 29.3160 - val_loss: 29.7219 - val_MinusLogProbMetric: 29.7219 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 592/1000
2023-10-25 23:35:25.765 
Epoch 592/1000 
	 loss: 29.3242, MinusLogProbMetric: 29.3242, val_loss: 29.7941, val_MinusLogProbMetric: 29.7941

Epoch 592: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3242 - MinusLogProbMetric: 29.3242 - val_loss: 29.7941 - val_MinusLogProbMetric: 29.7941 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 593/1000
2023-10-25 23:36:07.950 
Epoch 593/1000 
	 loss: 29.2795, MinusLogProbMetric: 29.2795, val_loss: 30.2546, val_MinusLogProbMetric: 30.2546

Epoch 593: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.2795 - MinusLogProbMetric: 29.2795 - val_loss: 30.2546 - val_MinusLogProbMetric: 30.2546 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 594/1000
2023-10-25 23:36:50.298 
Epoch 594/1000 
	 loss: 29.3058, MinusLogProbMetric: 29.3058, val_loss: 29.4843, val_MinusLogProbMetric: 29.4843

Epoch 594: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3058 - MinusLogProbMetric: 29.3058 - val_loss: 29.4843 - val_MinusLogProbMetric: 29.4843 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 595/1000
2023-10-25 23:37:32.912 
Epoch 595/1000 
	 loss: 29.4127, MinusLogProbMetric: 29.4127, val_loss: 30.3020, val_MinusLogProbMetric: 30.3020

Epoch 595: val_loss did not improve from 29.41178
196/196 - 43s - loss: 29.4127 - MinusLogProbMetric: 29.4127 - val_loss: 30.3020 - val_MinusLogProbMetric: 30.3020 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 596/1000
2023-10-25 23:38:15.113 
Epoch 596/1000 
	 loss: 29.3862, MinusLogProbMetric: 29.3862, val_loss: 29.5670, val_MinusLogProbMetric: 29.5670

Epoch 596: val_loss did not improve from 29.41178
196/196 - 42s - loss: 29.3862 - MinusLogProbMetric: 29.3862 - val_loss: 29.5670 - val_MinusLogProbMetric: 29.5670 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 597/1000
2023-10-25 23:38:57.355 
Epoch 597/1000 
	 loss: 28.8365, MinusLogProbMetric: 28.8365, val_loss: 29.2800, val_MinusLogProbMetric: 29.2800

Epoch 597: val_loss improved from 29.41178 to 29.28003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.8365 - MinusLogProbMetric: 28.8365 - val_loss: 29.2800 - val_MinusLogProbMetric: 29.2800 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 598/1000
2023-10-25 23:39:40.181 
Epoch 598/1000 
	 loss: 28.8306, MinusLogProbMetric: 28.8306, val_loss: 29.3041, val_MinusLogProbMetric: 29.3041

Epoch 598: val_loss did not improve from 29.28003
196/196 - 42s - loss: 28.8306 - MinusLogProbMetric: 28.8306 - val_loss: 29.3041 - val_MinusLogProbMetric: 29.3041 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 599/1000
2023-10-25 23:40:22.479 
Epoch 599/1000 
	 loss: 28.8270, MinusLogProbMetric: 28.8270, val_loss: 29.0983, val_MinusLogProbMetric: 29.0983

Epoch 599: val_loss improved from 29.28003 to 29.09833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.8270 - MinusLogProbMetric: 28.8270 - val_loss: 29.0983 - val_MinusLogProbMetric: 29.0983 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 600/1000
2023-10-25 23:41:05.355 
Epoch 600/1000 
	 loss: 28.8057, MinusLogProbMetric: 28.8057, val_loss: 29.1274, val_MinusLogProbMetric: 29.1274

Epoch 600: val_loss did not improve from 29.09833
196/196 - 42s - loss: 28.8057 - MinusLogProbMetric: 28.8057 - val_loss: 29.1274 - val_MinusLogProbMetric: 29.1274 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 601/1000
2023-10-25 23:41:47.885 
Epoch 601/1000 
	 loss: 28.8108, MinusLogProbMetric: 28.8108, val_loss: 29.5287, val_MinusLogProbMetric: 29.5287

Epoch 601: val_loss did not improve from 29.09833
196/196 - 43s - loss: 28.8108 - MinusLogProbMetric: 28.8108 - val_loss: 29.5287 - val_MinusLogProbMetric: 29.5287 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 602/1000
2023-10-25 23:42:30.278 
Epoch 602/1000 
	 loss: 28.8316, MinusLogProbMetric: 28.8316, val_loss: 29.1401, val_MinusLogProbMetric: 29.1401

Epoch 602: val_loss did not improve from 29.09833
196/196 - 42s - loss: 28.8316 - MinusLogProbMetric: 28.8316 - val_loss: 29.1401 - val_MinusLogProbMetric: 29.1401 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 603/1000
2023-10-25 23:43:12.877 
Epoch 603/1000 
	 loss: 28.8368, MinusLogProbMetric: 28.8368, val_loss: 29.2808, val_MinusLogProbMetric: 29.2808

Epoch 603: val_loss did not improve from 29.09833
196/196 - 43s - loss: 28.8368 - MinusLogProbMetric: 28.8368 - val_loss: 29.2808 - val_MinusLogProbMetric: 29.2808 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 604/1000
2023-10-25 23:43:55.132 
Epoch 604/1000 
	 loss: 28.8425, MinusLogProbMetric: 28.8425, val_loss: 29.1545, val_MinusLogProbMetric: 29.1545

Epoch 604: val_loss did not improve from 29.09833
196/196 - 42s - loss: 28.8425 - MinusLogProbMetric: 28.8425 - val_loss: 29.1545 - val_MinusLogProbMetric: 29.1545 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 605/1000
2023-10-25 23:44:37.639 
Epoch 605/1000 
	 loss: 28.8193, MinusLogProbMetric: 28.8193, val_loss: 29.3251, val_MinusLogProbMetric: 29.3251

Epoch 605: val_loss did not improve from 29.09833
196/196 - 43s - loss: 28.8193 - MinusLogProbMetric: 28.8193 - val_loss: 29.3251 - val_MinusLogProbMetric: 29.3251 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 606/1000
2023-10-25 23:45:20.245 
Epoch 606/1000 
	 loss: 28.8242, MinusLogProbMetric: 28.8242, val_loss: 29.1848, val_MinusLogProbMetric: 29.1848

Epoch 606: val_loss did not improve from 29.09833
196/196 - 43s - loss: 28.8242 - MinusLogProbMetric: 28.8242 - val_loss: 29.1848 - val_MinusLogProbMetric: 29.1848 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 607/1000
2023-10-25 23:46:02.339 
Epoch 607/1000 
	 loss: 28.8497, MinusLogProbMetric: 28.8497, val_loss: 29.2047, val_MinusLogProbMetric: 29.2047

Epoch 607: val_loss did not improve from 29.09833
196/196 - 42s - loss: 28.8497 - MinusLogProbMetric: 28.8497 - val_loss: 29.2047 - val_MinusLogProbMetric: 29.2047 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 608/1000
2023-10-25 23:46:44.909 
Epoch 608/1000 
	 loss: 28.8647, MinusLogProbMetric: 28.8647, val_loss: 29.0817, val_MinusLogProbMetric: 29.0817

Epoch 608: val_loss improved from 29.09833 to 29.08172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.8647 - MinusLogProbMetric: 28.8647 - val_loss: 29.0817 - val_MinusLogProbMetric: 29.0817 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 609/1000
2023-10-25 23:47:28.179 
Epoch 609/1000 
	 loss: 28.8366, MinusLogProbMetric: 28.8366, val_loss: 29.2706, val_MinusLogProbMetric: 29.2706

Epoch 609: val_loss did not improve from 29.08172
196/196 - 43s - loss: 28.8366 - MinusLogProbMetric: 28.8366 - val_loss: 29.2706 - val_MinusLogProbMetric: 29.2706 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 610/1000
2023-10-25 23:48:10.567 
Epoch 610/1000 
	 loss: 28.8350, MinusLogProbMetric: 28.8350, val_loss: 29.0856, val_MinusLogProbMetric: 29.0856

Epoch 610: val_loss did not improve from 29.08172
196/196 - 42s - loss: 28.8350 - MinusLogProbMetric: 28.8350 - val_loss: 29.0856 - val_MinusLogProbMetric: 29.0856 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 611/1000
2023-10-25 23:48:53.055 
Epoch 611/1000 
	 loss: 28.8032, MinusLogProbMetric: 28.8032, val_loss: 29.5156, val_MinusLogProbMetric: 29.5156

Epoch 611: val_loss did not improve from 29.08172
196/196 - 42s - loss: 28.8032 - MinusLogProbMetric: 28.8032 - val_loss: 29.5156 - val_MinusLogProbMetric: 29.5156 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 612/1000
2023-10-25 23:49:35.271 
Epoch 612/1000 
	 loss: 28.8089, MinusLogProbMetric: 28.8089, val_loss: 29.1592, val_MinusLogProbMetric: 29.1592

Epoch 612: val_loss did not improve from 29.08172
196/196 - 42s - loss: 28.8089 - MinusLogProbMetric: 28.8089 - val_loss: 29.1592 - val_MinusLogProbMetric: 29.1592 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 613/1000
2023-10-25 23:50:17.744 
Epoch 613/1000 
	 loss: 28.8713, MinusLogProbMetric: 28.8713, val_loss: 29.0944, val_MinusLogProbMetric: 29.0944

Epoch 613: val_loss did not improve from 29.08172
196/196 - 42s - loss: 28.8713 - MinusLogProbMetric: 28.8713 - val_loss: 29.0944 - val_MinusLogProbMetric: 29.0944 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 614/1000
2023-10-25 23:50:59.682 
Epoch 614/1000 
	 loss: 28.8018, MinusLogProbMetric: 28.8018, val_loss: 29.0565, val_MinusLogProbMetric: 29.0565

Epoch 614: val_loss improved from 29.08172 to 29.05647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.8018 - MinusLogProbMetric: 28.8018 - val_loss: 29.0565 - val_MinusLogProbMetric: 29.0565 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 615/1000
2023-10-25 23:51:43.055 
Epoch 615/1000 
	 loss: 28.7733, MinusLogProbMetric: 28.7733, val_loss: 29.0262, val_MinusLogProbMetric: 29.0262

Epoch 615: val_loss improved from 29.05647 to 29.02624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.7733 - MinusLogProbMetric: 28.7733 - val_loss: 29.0262 - val_MinusLogProbMetric: 29.0262 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 616/1000
2023-10-25 23:52:26.158 
Epoch 616/1000 
	 loss: 28.8325, MinusLogProbMetric: 28.8325, val_loss: 29.1659, val_MinusLogProbMetric: 29.1659

Epoch 616: val_loss did not improve from 29.02624
196/196 - 42s - loss: 28.8325 - MinusLogProbMetric: 28.8325 - val_loss: 29.1659 - val_MinusLogProbMetric: 29.1659 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 617/1000
2023-10-25 23:53:08.814 
Epoch 617/1000 
	 loss: 28.8241, MinusLogProbMetric: 28.8241, val_loss: 29.1143, val_MinusLogProbMetric: 29.1143

Epoch 617: val_loss did not improve from 29.02624
196/196 - 43s - loss: 28.8241 - MinusLogProbMetric: 28.8241 - val_loss: 29.1143 - val_MinusLogProbMetric: 29.1143 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 618/1000
2023-10-25 23:53:50.940 
Epoch 618/1000 
	 loss: 28.7960, MinusLogProbMetric: 28.7960, val_loss: 29.1927, val_MinusLogProbMetric: 29.1927

Epoch 618: val_loss did not improve from 29.02624
196/196 - 42s - loss: 28.7960 - MinusLogProbMetric: 28.7960 - val_loss: 29.1927 - val_MinusLogProbMetric: 29.1927 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 619/1000
2023-10-25 23:54:33.618 
Epoch 619/1000 
	 loss: 28.8251, MinusLogProbMetric: 28.8251, val_loss: 29.1296, val_MinusLogProbMetric: 29.1296

Epoch 619: val_loss did not improve from 29.02624
196/196 - 43s - loss: 28.8251 - MinusLogProbMetric: 28.8251 - val_loss: 29.1296 - val_MinusLogProbMetric: 29.1296 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 620/1000
2023-10-25 23:55:15.878 
Epoch 620/1000 
	 loss: 28.7957, MinusLogProbMetric: 28.7957, val_loss: 29.2211, val_MinusLogProbMetric: 29.2211

Epoch 620: val_loss did not improve from 29.02624
196/196 - 42s - loss: 28.7957 - MinusLogProbMetric: 28.7957 - val_loss: 29.2211 - val_MinusLogProbMetric: 29.2211 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 621/1000
2023-10-25 23:55:57.978 
Epoch 621/1000 
	 loss: 28.8396, MinusLogProbMetric: 28.8396, val_loss: 29.0855, val_MinusLogProbMetric: 29.0855

Epoch 621: val_loss did not improve from 29.02624
196/196 - 42s - loss: 28.8396 - MinusLogProbMetric: 28.8396 - val_loss: 29.0855 - val_MinusLogProbMetric: 29.0855 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 622/1000
2023-10-25 23:56:40.086 
Epoch 622/1000 
	 loss: 28.8013, MinusLogProbMetric: 28.8013, val_loss: 29.1270, val_MinusLogProbMetric: 29.1270

Epoch 622: val_loss did not improve from 29.02624
196/196 - 42s - loss: 28.8013 - MinusLogProbMetric: 28.8013 - val_loss: 29.1270 - val_MinusLogProbMetric: 29.1270 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 623/1000
2023-10-25 23:57:22.072 
Epoch 623/1000 
	 loss: 28.8146, MinusLogProbMetric: 28.8146, val_loss: 29.1155, val_MinusLogProbMetric: 29.1155

Epoch 623: val_loss did not improve from 29.02624
196/196 - 42s - loss: 28.8146 - MinusLogProbMetric: 28.8146 - val_loss: 29.1155 - val_MinusLogProbMetric: 29.1155 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 624/1000
2023-10-25 23:58:04.516 
Epoch 624/1000 
	 loss: 28.8061, MinusLogProbMetric: 28.8061, val_loss: 28.9949, val_MinusLogProbMetric: 28.9949

Epoch 624: val_loss improved from 29.02624 to 28.99492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.8061 - MinusLogProbMetric: 28.8061 - val_loss: 28.9949 - val_MinusLogProbMetric: 28.9949 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 625/1000
2023-10-25 23:58:47.493 
Epoch 625/1000 
	 loss: 28.8378, MinusLogProbMetric: 28.8378, val_loss: 29.1462, val_MinusLogProbMetric: 29.1462

Epoch 625: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8378 - MinusLogProbMetric: 28.8378 - val_loss: 29.1462 - val_MinusLogProbMetric: 29.1462 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 626/1000
2023-10-25 23:59:29.694 
Epoch 626/1000 
	 loss: 28.8323, MinusLogProbMetric: 28.8323, val_loss: 29.0086, val_MinusLogProbMetric: 29.0086

Epoch 626: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8323 - MinusLogProbMetric: 28.8323 - val_loss: 29.0086 - val_MinusLogProbMetric: 29.0086 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 627/1000
2023-10-26 00:00:12.016 
Epoch 627/1000 
	 loss: 28.8139, MinusLogProbMetric: 28.8139, val_loss: 29.2065, val_MinusLogProbMetric: 29.2065

Epoch 627: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8139 - MinusLogProbMetric: 28.8139 - val_loss: 29.2065 - val_MinusLogProbMetric: 29.2065 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 628/1000
2023-10-26 00:00:54.160 
Epoch 628/1000 
	 loss: 28.8266, MinusLogProbMetric: 28.8266, val_loss: 29.0513, val_MinusLogProbMetric: 29.0513

Epoch 628: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8266 - MinusLogProbMetric: 28.8266 - val_loss: 29.0513 - val_MinusLogProbMetric: 29.0513 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 629/1000
2023-10-26 00:01:31.019 
Epoch 629/1000 
	 loss: 28.7854, MinusLogProbMetric: 28.7854, val_loss: 29.2240, val_MinusLogProbMetric: 29.2240

Epoch 629: val_loss did not improve from 28.99492
196/196 - 37s - loss: 28.7854 - MinusLogProbMetric: 28.7854 - val_loss: 29.2240 - val_MinusLogProbMetric: 29.2240 - lr: 1.6667e-04 - 37s/epoch - 188ms/step
Epoch 630/1000
2023-10-26 00:02:13.569 
Epoch 630/1000 
	 loss: 28.8157, MinusLogProbMetric: 28.8157, val_loss: 29.0123, val_MinusLogProbMetric: 29.0123

Epoch 630: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.8157 - MinusLogProbMetric: 28.8157 - val_loss: 29.0123 - val_MinusLogProbMetric: 29.0123 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 631/1000
2023-10-26 00:02:56.299 
Epoch 631/1000 
	 loss: 28.8220, MinusLogProbMetric: 28.8220, val_loss: 29.1112, val_MinusLogProbMetric: 29.1112

Epoch 631: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.8220 - MinusLogProbMetric: 28.8220 - val_loss: 29.1112 - val_MinusLogProbMetric: 29.1112 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 632/1000
2023-10-26 00:03:38.715 
Epoch 632/1000 
	 loss: 28.7841, MinusLogProbMetric: 28.7841, val_loss: 29.1704, val_MinusLogProbMetric: 29.1704

Epoch 632: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7841 - MinusLogProbMetric: 28.7841 - val_loss: 29.1704 - val_MinusLogProbMetric: 29.1704 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 633/1000
2023-10-26 00:04:20.973 
Epoch 633/1000 
	 loss: 28.7992, MinusLogProbMetric: 28.7992, val_loss: 29.1913, val_MinusLogProbMetric: 29.1913

Epoch 633: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7992 - MinusLogProbMetric: 28.7992 - val_loss: 29.1913 - val_MinusLogProbMetric: 29.1913 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 634/1000
2023-10-26 00:04:58.756 
Epoch 634/1000 
	 loss: 28.7946, MinusLogProbMetric: 28.7946, val_loss: 29.0274, val_MinusLogProbMetric: 29.0274

Epoch 634: val_loss did not improve from 28.99492
196/196 - 38s - loss: 28.7946 - MinusLogProbMetric: 28.7946 - val_loss: 29.0274 - val_MinusLogProbMetric: 29.0274 - lr: 1.6667e-04 - 38s/epoch - 193ms/step
Epoch 635/1000
2023-10-26 00:05:40.019 
Epoch 635/1000 
	 loss: 28.8533, MinusLogProbMetric: 28.8533, val_loss: 29.4751, val_MinusLogProbMetric: 29.4751

Epoch 635: val_loss did not improve from 28.99492
196/196 - 41s - loss: 28.8533 - MinusLogProbMetric: 28.8533 - val_loss: 29.4751 - val_MinusLogProbMetric: 29.4751 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 636/1000
2023-10-26 00:06:21.598 
Epoch 636/1000 
	 loss: 28.7909, MinusLogProbMetric: 28.7909, val_loss: 29.2567, val_MinusLogProbMetric: 29.2567

Epoch 636: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7909 - MinusLogProbMetric: 28.7909 - val_loss: 29.2567 - val_MinusLogProbMetric: 29.2567 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 637/1000
2023-10-26 00:07:04.405 
Epoch 637/1000 
	 loss: 28.8370, MinusLogProbMetric: 28.8370, val_loss: 29.1360, val_MinusLogProbMetric: 29.1360

Epoch 637: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.8370 - MinusLogProbMetric: 28.8370 - val_loss: 29.1360 - val_MinusLogProbMetric: 29.1360 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 638/1000
2023-10-26 00:07:46.683 
Epoch 638/1000 
	 loss: 28.7720, MinusLogProbMetric: 28.7720, val_loss: 29.4211, val_MinusLogProbMetric: 29.4211

Epoch 638: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7720 - MinusLogProbMetric: 28.7720 - val_loss: 29.4211 - val_MinusLogProbMetric: 29.4211 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 639/1000
2023-10-26 00:08:29.142 
Epoch 639/1000 
	 loss: 28.7816, MinusLogProbMetric: 28.7816, val_loss: 29.0975, val_MinusLogProbMetric: 29.0975

Epoch 639: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7816 - MinusLogProbMetric: 28.7816 - val_loss: 29.0975 - val_MinusLogProbMetric: 29.0975 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 640/1000
2023-10-26 00:09:11.645 
Epoch 640/1000 
	 loss: 28.7907, MinusLogProbMetric: 28.7907, val_loss: 29.0167, val_MinusLogProbMetric: 29.0167

Epoch 640: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7907 - MinusLogProbMetric: 28.7907 - val_loss: 29.0167 - val_MinusLogProbMetric: 29.0167 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 641/1000
2023-10-26 00:09:54.188 
Epoch 641/1000 
	 loss: 28.7849, MinusLogProbMetric: 28.7849, val_loss: 29.0970, val_MinusLogProbMetric: 29.0970

Epoch 641: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7849 - MinusLogProbMetric: 28.7849 - val_loss: 29.0970 - val_MinusLogProbMetric: 29.0970 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 642/1000
2023-10-26 00:10:37.015 
Epoch 642/1000 
	 loss: 28.7822, MinusLogProbMetric: 28.7822, val_loss: 29.0400, val_MinusLogProbMetric: 29.0400

Epoch 642: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7822 - MinusLogProbMetric: 28.7822 - val_loss: 29.0400 - val_MinusLogProbMetric: 29.0400 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 643/1000
2023-10-26 00:11:19.365 
Epoch 643/1000 
	 loss: 28.7520, MinusLogProbMetric: 28.7520, val_loss: 29.1117, val_MinusLogProbMetric: 29.1117

Epoch 643: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7520 - MinusLogProbMetric: 28.7520 - val_loss: 29.1117 - val_MinusLogProbMetric: 29.1117 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 644/1000
2023-10-26 00:12:01.650 
Epoch 644/1000 
	 loss: 28.8101, MinusLogProbMetric: 28.8101, val_loss: 29.1345, val_MinusLogProbMetric: 29.1345

Epoch 644: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8101 - MinusLogProbMetric: 28.8101 - val_loss: 29.1345 - val_MinusLogProbMetric: 29.1345 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 645/1000
2023-10-26 00:12:44.325 
Epoch 645/1000 
	 loss: 28.7773, MinusLogProbMetric: 28.7773, val_loss: 29.2485, val_MinusLogProbMetric: 29.2485

Epoch 645: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7773 - MinusLogProbMetric: 28.7773 - val_loss: 29.2485 - val_MinusLogProbMetric: 29.2485 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 646/1000
2023-10-26 00:13:26.678 
Epoch 646/1000 
	 loss: 28.7892, MinusLogProbMetric: 28.7892, val_loss: 29.3256, val_MinusLogProbMetric: 29.3256

Epoch 646: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7892 - MinusLogProbMetric: 28.7892 - val_loss: 29.3256 - val_MinusLogProbMetric: 29.3256 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 647/1000
2023-10-26 00:14:08.863 
Epoch 647/1000 
	 loss: 28.8268, MinusLogProbMetric: 28.8268, val_loss: 29.0485, val_MinusLogProbMetric: 29.0485

Epoch 647: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8268 - MinusLogProbMetric: 28.8268 - val_loss: 29.0485 - val_MinusLogProbMetric: 29.0485 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 648/1000
2023-10-26 00:14:51.069 
Epoch 648/1000 
	 loss: 28.7802, MinusLogProbMetric: 28.7802, val_loss: 29.3192, val_MinusLogProbMetric: 29.3192

Epoch 648: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7802 - MinusLogProbMetric: 28.7802 - val_loss: 29.3192 - val_MinusLogProbMetric: 29.3192 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 649/1000
2023-10-26 00:15:33.412 
Epoch 649/1000 
	 loss: 28.7994, MinusLogProbMetric: 28.7994, val_loss: 29.3191, val_MinusLogProbMetric: 29.3191

Epoch 649: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7994 - MinusLogProbMetric: 28.7994 - val_loss: 29.3191 - val_MinusLogProbMetric: 29.3191 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 650/1000
2023-10-26 00:16:15.736 
Epoch 650/1000 
	 loss: 28.7888, MinusLogProbMetric: 28.7888, val_loss: 29.0924, val_MinusLogProbMetric: 29.0924

Epoch 650: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7888 - MinusLogProbMetric: 28.7888 - val_loss: 29.0924 - val_MinusLogProbMetric: 29.0924 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 651/1000
2023-10-26 00:16:57.892 
Epoch 651/1000 
	 loss: 28.7824, MinusLogProbMetric: 28.7824, val_loss: 29.1252, val_MinusLogProbMetric: 29.1252

Epoch 651: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7824 - MinusLogProbMetric: 28.7824 - val_loss: 29.1252 - val_MinusLogProbMetric: 29.1252 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 652/1000
2023-10-26 00:17:40.325 
Epoch 652/1000 
	 loss: 28.7503, MinusLogProbMetric: 28.7503, val_loss: 29.3132, val_MinusLogProbMetric: 29.3132

Epoch 652: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7503 - MinusLogProbMetric: 28.7503 - val_loss: 29.3132 - val_MinusLogProbMetric: 29.3132 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 653/1000
2023-10-26 00:18:22.553 
Epoch 653/1000 
	 loss: 28.7900, MinusLogProbMetric: 28.7900, val_loss: 29.1111, val_MinusLogProbMetric: 29.1111

Epoch 653: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7900 - MinusLogProbMetric: 28.7900 - val_loss: 29.1111 - val_MinusLogProbMetric: 29.1111 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 654/1000
2023-10-26 00:19:05.346 
Epoch 654/1000 
	 loss: 28.7805, MinusLogProbMetric: 28.7805, val_loss: 29.1524, val_MinusLogProbMetric: 29.1524

Epoch 654: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7805 - MinusLogProbMetric: 28.7805 - val_loss: 29.1524 - val_MinusLogProbMetric: 29.1524 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 655/1000
2023-10-26 00:19:47.763 
Epoch 655/1000 
	 loss: 28.7342, MinusLogProbMetric: 28.7342, val_loss: 29.1232, val_MinusLogProbMetric: 29.1232

Epoch 655: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7342 - MinusLogProbMetric: 28.7342 - val_loss: 29.1232 - val_MinusLogProbMetric: 29.1232 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 656/1000
2023-10-26 00:20:29.855 
Epoch 656/1000 
	 loss: 28.7790, MinusLogProbMetric: 28.7790, val_loss: 29.0662, val_MinusLogProbMetric: 29.0662

Epoch 656: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7790 - MinusLogProbMetric: 28.7790 - val_loss: 29.0662 - val_MinusLogProbMetric: 29.0662 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 657/1000
2023-10-26 00:21:12.043 
Epoch 657/1000 
	 loss: 28.8151, MinusLogProbMetric: 28.8151, val_loss: 29.0486, val_MinusLogProbMetric: 29.0486

Epoch 657: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8151 - MinusLogProbMetric: 28.8151 - val_loss: 29.0486 - val_MinusLogProbMetric: 29.0486 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 658/1000
2023-10-26 00:21:54.290 
Epoch 658/1000 
	 loss: 28.7518, MinusLogProbMetric: 28.7518, val_loss: 29.2757, val_MinusLogProbMetric: 29.2757

Epoch 658: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7518 - MinusLogProbMetric: 28.7518 - val_loss: 29.2757 - val_MinusLogProbMetric: 29.2757 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 659/1000
2023-10-26 00:22:36.942 
Epoch 659/1000 
	 loss: 28.8104, MinusLogProbMetric: 28.8104, val_loss: 29.2230, val_MinusLogProbMetric: 29.2230

Epoch 659: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.8104 - MinusLogProbMetric: 28.8104 - val_loss: 29.2230 - val_MinusLogProbMetric: 29.2230 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 660/1000
2023-10-26 00:23:19.349 
Epoch 660/1000 
	 loss: 28.7995, MinusLogProbMetric: 28.7995, val_loss: 29.7448, val_MinusLogProbMetric: 29.7448

Epoch 660: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7995 - MinusLogProbMetric: 28.7995 - val_loss: 29.7448 - val_MinusLogProbMetric: 29.7448 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 661/1000
2023-10-26 00:24:01.889 
Epoch 661/1000 
	 loss: 28.7785, MinusLogProbMetric: 28.7785, val_loss: 29.1084, val_MinusLogProbMetric: 29.1084

Epoch 661: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7785 - MinusLogProbMetric: 28.7785 - val_loss: 29.1084 - val_MinusLogProbMetric: 29.1084 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 662/1000
2023-10-26 00:24:44.379 
Epoch 662/1000 
	 loss: 28.7714, MinusLogProbMetric: 28.7714, val_loss: 29.1697, val_MinusLogProbMetric: 29.1697

Epoch 662: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7714 - MinusLogProbMetric: 28.7714 - val_loss: 29.1697 - val_MinusLogProbMetric: 29.1697 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 663/1000
2023-10-26 00:25:26.615 
Epoch 663/1000 
	 loss: 28.7464, MinusLogProbMetric: 28.7464, val_loss: 29.0284, val_MinusLogProbMetric: 29.0284

Epoch 663: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7464 - MinusLogProbMetric: 28.7464 - val_loss: 29.0284 - val_MinusLogProbMetric: 29.0284 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 664/1000
2023-10-26 00:26:09.154 
Epoch 664/1000 
	 loss: 28.7795, MinusLogProbMetric: 28.7795, val_loss: 29.3291, val_MinusLogProbMetric: 29.3291

Epoch 664: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7795 - MinusLogProbMetric: 28.7795 - val_loss: 29.3291 - val_MinusLogProbMetric: 29.3291 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 665/1000
2023-10-26 00:26:51.191 
Epoch 665/1000 
	 loss: 28.8015, MinusLogProbMetric: 28.8015, val_loss: 29.2515, val_MinusLogProbMetric: 29.2515

Epoch 665: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8015 - MinusLogProbMetric: 28.8015 - val_loss: 29.2515 - val_MinusLogProbMetric: 29.2515 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 666/1000
2023-10-26 00:27:33.584 
Epoch 666/1000 
	 loss: 28.7865, MinusLogProbMetric: 28.7865, val_loss: 29.0652, val_MinusLogProbMetric: 29.0652

Epoch 666: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7865 - MinusLogProbMetric: 28.7865 - val_loss: 29.0652 - val_MinusLogProbMetric: 29.0652 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 667/1000
2023-10-26 00:28:16.037 
Epoch 667/1000 
	 loss: 28.7312, MinusLogProbMetric: 28.7312, val_loss: 29.2553, val_MinusLogProbMetric: 29.2553

Epoch 667: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7312 - MinusLogProbMetric: 28.7312 - val_loss: 29.2553 - val_MinusLogProbMetric: 29.2553 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 668/1000
2023-10-26 00:28:58.580 
Epoch 668/1000 
	 loss: 28.7606, MinusLogProbMetric: 28.7606, val_loss: 29.1673, val_MinusLogProbMetric: 29.1673

Epoch 668: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7606 - MinusLogProbMetric: 28.7606 - val_loss: 29.1673 - val_MinusLogProbMetric: 29.1673 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 669/1000
2023-10-26 00:29:41.122 
Epoch 669/1000 
	 loss: 28.7762, MinusLogProbMetric: 28.7762, val_loss: 29.5584, val_MinusLogProbMetric: 29.5584

Epoch 669: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7762 - MinusLogProbMetric: 28.7762 - val_loss: 29.5584 - val_MinusLogProbMetric: 29.5584 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 670/1000
2023-10-26 00:30:23.440 
Epoch 670/1000 
	 loss: 28.8348, MinusLogProbMetric: 28.8348, val_loss: 29.0134, val_MinusLogProbMetric: 29.0134

Epoch 670: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8348 - MinusLogProbMetric: 28.8348 - val_loss: 29.0134 - val_MinusLogProbMetric: 29.0134 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 671/1000
2023-10-26 00:31:05.881 
Epoch 671/1000 
	 loss: 28.7752, MinusLogProbMetric: 28.7752, val_loss: 29.1306, val_MinusLogProbMetric: 29.1306

Epoch 671: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.7752 - MinusLogProbMetric: 28.7752 - val_loss: 29.1306 - val_MinusLogProbMetric: 29.1306 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 672/1000
2023-10-26 00:31:48.499 
Epoch 672/1000 
	 loss: 28.7665, MinusLogProbMetric: 28.7665, val_loss: 29.1718, val_MinusLogProbMetric: 29.1718

Epoch 672: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7665 - MinusLogProbMetric: 28.7665 - val_loss: 29.1718 - val_MinusLogProbMetric: 29.1718 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 673/1000
2023-10-26 00:32:30.572 
Epoch 673/1000 
	 loss: 28.8123, MinusLogProbMetric: 28.8123, val_loss: 29.0508, val_MinusLogProbMetric: 29.0508

Epoch 673: val_loss did not improve from 28.99492
196/196 - 42s - loss: 28.8123 - MinusLogProbMetric: 28.8123 - val_loss: 29.0508 - val_MinusLogProbMetric: 29.0508 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 674/1000
2023-10-26 00:33:13.355 
Epoch 674/1000 
	 loss: 28.7805, MinusLogProbMetric: 28.7805, val_loss: 29.1615, val_MinusLogProbMetric: 29.1615

Epoch 674: val_loss did not improve from 28.99492
196/196 - 43s - loss: 28.7805 - MinusLogProbMetric: 28.7805 - val_loss: 29.1615 - val_MinusLogProbMetric: 29.1615 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 675/1000
2023-10-26 00:33:55.153 
Epoch 675/1000 
	 loss: 28.5884, MinusLogProbMetric: 28.5884, val_loss: 28.9243, val_MinusLogProbMetric: 28.9243

Epoch 675: val_loss improved from 28.99492 to 28.92428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.5884 - MinusLogProbMetric: 28.5884 - val_loss: 28.9243 - val_MinusLogProbMetric: 28.9243 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 676/1000
2023-10-26 00:34:38.297 
Epoch 676/1000 
	 loss: 28.5707, MinusLogProbMetric: 28.5707, val_loss: 28.9363, val_MinusLogProbMetric: 28.9363

Epoch 676: val_loss did not improve from 28.92428
196/196 - 42s - loss: 28.5707 - MinusLogProbMetric: 28.5707 - val_loss: 28.9363 - val_MinusLogProbMetric: 28.9363 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 677/1000
2023-10-26 00:35:20.803 
Epoch 677/1000 
	 loss: 28.5771, MinusLogProbMetric: 28.5771, val_loss: 28.8823, val_MinusLogProbMetric: 28.8823

Epoch 677: val_loss improved from 28.92428 to 28.88228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.5771 - MinusLogProbMetric: 28.5771 - val_loss: 28.8823 - val_MinusLogProbMetric: 28.8823 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 678/1000
2023-10-26 00:36:03.922 
Epoch 678/1000 
	 loss: 28.5619, MinusLogProbMetric: 28.5619, val_loss: 28.8943, val_MinusLogProbMetric: 28.8943

Epoch 678: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5619 - MinusLogProbMetric: 28.5619 - val_loss: 28.8943 - val_MinusLogProbMetric: 28.8943 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 679/1000
2023-10-26 00:36:46.360 
Epoch 679/1000 
	 loss: 28.5705, MinusLogProbMetric: 28.5705, val_loss: 29.0392, val_MinusLogProbMetric: 29.0392

Epoch 679: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5705 - MinusLogProbMetric: 28.5705 - val_loss: 29.0392 - val_MinusLogProbMetric: 29.0392 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 680/1000
2023-10-26 00:37:29.114 
Epoch 680/1000 
	 loss: 28.5698, MinusLogProbMetric: 28.5698, val_loss: 29.0112, val_MinusLogProbMetric: 29.0112

Epoch 680: val_loss did not improve from 28.88228
196/196 - 43s - loss: 28.5698 - MinusLogProbMetric: 28.5698 - val_loss: 29.0112 - val_MinusLogProbMetric: 29.0112 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 681/1000
2023-10-26 00:38:11.824 
Epoch 681/1000 
	 loss: 28.5819, MinusLogProbMetric: 28.5819, val_loss: 28.9633, val_MinusLogProbMetric: 28.9633

Epoch 681: val_loss did not improve from 28.88228
196/196 - 43s - loss: 28.5819 - MinusLogProbMetric: 28.5819 - val_loss: 28.9633 - val_MinusLogProbMetric: 28.9633 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 682/1000
2023-10-26 00:38:54.464 
Epoch 682/1000 
	 loss: 28.5836, MinusLogProbMetric: 28.5836, val_loss: 28.9669, val_MinusLogProbMetric: 28.9669

Epoch 682: val_loss did not improve from 28.88228
196/196 - 43s - loss: 28.5836 - MinusLogProbMetric: 28.5836 - val_loss: 28.9669 - val_MinusLogProbMetric: 28.9669 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 683/1000
2023-10-26 00:39:37.077 
Epoch 683/1000 
	 loss: 28.5676, MinusLogProbMetric: 28.5676, val_loss: 28.9002, val_MinusLogProbMetric: 28.9002

Epoch 683: val_loss did not improve from 28.88228
196/196 - 43s - loss: 28.5676 - MinusLogProbMetric: 28.5676 - val_loss: 28.9002 - val_MinusLogProbMetric: 28.9002 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 684/1000
2023-10-26 00:40:19.087 
Epoch 684/1000 
	 loss: 28.5556, MinusLogProbMetric: 28.5556, val_loss: 29.0707, val_MinusLogProbMetric: 29.0707

Epoch 684: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5556 - MinusLogProbMetric: 28.5556 - val_loss: 29.0707 - val_MinusLogProbMetric: 29.0707 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 685/1000
2023-10-26 00:41:01.491 
Epoch 685/1000 
	 loss: 28.5688, MinusLogProbMetric: 28.5688, val_loss: 28.8898, val_MinusLogProbMetric: 28.8898

Epoch 685: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5688 - MinusLogProbMetric: 28.5688 - val_loss: 28.8898 - val_MinusLogProbMetric: 28.8898 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 686/1000
2023-10-26 00:41:44.073 
Epoch 686/1000 
	 loss: 28.5542, MinusLogProbMetric: 28.5542, val_loss: 28.8995, val_MinusLogProbMetric: 28.8995

Epoch 686: val_loss did not improve from 28.88228
196/196 - 43s - loss: 28.5542 - MinusLogProbMetric: 28.5542 - val_loss: 28.8995 - val_MinusLogProbMetric: 28.8995 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 687/1000
2023-10-26 00:42:26.954 
Epoch 687/1000 
	 loss: 28.5437, MinusLogProbMetric: 28.5437, val_loss: 28.8838, val_MinusLogProbMetric: 28.8838

Epoch 687: val_loss did not improve from 28.88228
196/196 - 43s - loss: 28.5437 - MinusLogProbMetric: 28.5437 - val_loss: 28.8838 - val_MinusLogProbMetric: 28.8838 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 688/1000
2023-10-26 00:43:09.436 
Epoch 688/1000 
	 loss: 28.5533, MinusLogProbMetric: 28.5533, val_loss: 28.9211, val_MinusLogProbMetric: 28.9211

Epoch 688: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5533 - MinusLogProbMetric: 28.5533 - val_loss: 28.9211 - val_MinusLogProbMetric: 28.9211 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 689/1000
2023-10-26 00:43:51.637 
Epoch 689/1000 
	 loss: 28.5510, MinusLogProbMetric: 28.5510, val_loss: 28.9105, val_MinusLogProbMetric: 28.9105

Epoch 689: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5510 - MinusLogProbMetric: 28.5510 - val_loss: 28.9105 - val_MinusLogProbMetric: 28.9105 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 690/1000
2023-10-26 00:44:33.710 
Epoch 690/1000 
	 loss: 28.5541, MinusLogProbMetric: 28.5541, val_loss: 28.8966, val_MinusLogProbMetric: 28.8966

Epoch 690: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5541 - MinusLogProbMetric: 28.5541 - val_loss: 28.8966 - val_MinusLogProbMetric: 28.8966 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 691/1000
2023-10-26 00:45:16.026 
Epoch 691/1000 
	 loss: 28.5368, MinusLogProbMetric: 28.5368, val_loss: 28.9422, val_MinusLogProbMetric: 28.9422

Epoch 691: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5368 - MinusLogProbMetric: 28.5368 - val_loss: 28.9422 - val_MinusLogProbMetric: 28.9422 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 692/1000
2023-10-26 00:45:58.371 
Epoch 692/1000 
	 loss: 28.5614, MinusLogProbMetric: 28.5614, val_loss: 28.8937, val_MinusLogProbMetric: 28.8937

Epoch 692: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5614 - MinusLogProbMetric: 28.5614 - val_loss: 28.8937 - val_MinusLogProbMetric: 28.8937 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 693/1000
2023-10-26 00:46:40.818 
Epoch 693/1000 
	 loss: 28.5521, MinusLogProbMetric: 28.5521, val_loss: 28.8980, val_MinusLogProbMetric: 28.8980

Epoch 693: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5521 - MinusLogProbMetric: 28.5521 - val_loss: 28.8980 - val_MinusLogProbMetric: 28.8980 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 694/1000
2023-10-26 00:47:22.746 
Epoch 694/1000 
	 loss: 28.5603, MinusLogProbMetric: 28.5603, val_loss: 28.8827, val_MinusLogProbMetric: 28.8827

Epoch 694: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5603 - MinusLogProbMetric: 28.5603 - val_loss: 28.8827 - val_MinusLogProbMetric: 28.8827 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 695/1000
2023-10-26 00:48:04.735 
Epoch 695/1000 
	 loss: 28.5675, MinusLogProbMetric: 28.5675, val_loss: 28.8901, val_MinusLogProbMetric: 28.8901

Epoch 695: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5675 - MinusLogProbMetric: 28.5675 - val_loss: 28.8901 - val_MinusLogProbMetric: 28.8901 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 696/1000
2023-10-26 00:48:47.072 
Epoch 696/1000 
	 loss: 28.5594, MinusLogProbMetric: 28.5594, val_loss: 28.9265, val_MinusLogProbMetric: 28.9265

Epoch 696: val_loss did not improve from 28.88228
196/196 - 42s - loss: 28.5594 - MinusLogProbMetric: 28.5594 - val_loss: 28.9265 - val_MinusLogProbMetric: 28.9265 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 697/1000
2023-10-26 00:49:28.918 
Epoch 697/1000 
	 loss: 28.5741, MinusLogProbMetric: 28.5741, val_loss: 28.8661, val_MinusLogProbMetric: 28.8661

Epoch 697: val_loss improved from 28.88228 to 28.86612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.5741 - MinusLogProbMetric: 28.5741 - val_loss: 28.8661 - val_MinusLogProbMetric: 28.8661 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 698/1000
2023-10-26 00:50:12.167 
Epoch 698/1000 
	 loss: 28.5605, MinusLogProbMetric: 28.5605, val_loss: 28.9099, val_MinusLogProbMetric: 28.9099

Epoch 698: val_loss did not improve from 28.86612
196/196 - 42s - loss: 28.5605 - MinusLogProbMetric: 28.5605 - val_loss: 28.9099 - val_MinusLogProbMetric: 28.9099 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 699/1000
2023-10-26 00:50:54.028 
Epoch 699/1000 
	 loss: 28.5674, MinusLogProbMetric: 28.5674, val_loss: 28.9195, val_MinusLogProbMetric: 28.9195

Epoch 699: val_loss did not improve from 28.86612
196/196 - 42s - loss: 28.5674 - MinusLogProbMetric: 28.5674 - val_loss: 28.9195 - val_MinusLogProbMetric: 28.9195 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 700/1000
2023-10-26 00:51:36.629 
Epoch 700/1000 
	 loss: 28.5670, MinusLogProbMetric: 28.5670, val_loss: 28.9185, val_MinusLogProbMetric: 28.9185

Epoch 700: val_loss did not improve from 28.86612
196/196 - 43s - loss: 28.5670 - MinusLogProbMetric: 28.5670 - val_loss: 28.9185 - val_MinusLogProbMetric: 28.9185 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 701/1000
2023-10-26 00:52:13.401 
Epoch 701/1000 
	 loss: 28.5504, MinusLogProbMetric: 28.5504, val_loss: 28.9008, val_MinusLogProbMetric: 28.9008

Epoch 701: val_loss did not improve from 28.86612
196/196 - 37s - loss: 28.5504 - MinusLogProbMetric: 28.5504 - val_loss: 28.9008 - val_MinusLogProbMetric: 28.9008 - lr: 8.3333e-05 - 37s/epoch - 188ms/step
Epoch 702/1000
2023-10-26 00:52:49.651 
Epoch 702/1000 
	 loss: 28.5634, MinusLogProbMetric: 28.5634, val_loss: 28.8591, val_MinusLogProbMetric: 28.8591

Epoch 702: val_loss improved from 28.86612 to 28.85905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 37s - loss: 28.5634 - MinusLogProbMetric: 28.5634 - val_loss: 28.8591 - val_MinusLogProbMetric: 28.8591 - lr: 8.3333e-05 - 37s/epoch - 189ms/step
Epoch 703/1000
2023-10-26 00:53:32.452 
Epoch 703/1000 
	 loss: 28.5704, MinusLogProbMetric: 28.5704, val_loss: 28.8767, val_MinusLogProbMetric: 28.8767

Epoch 703: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5704 - MinusLogProbMetric: 28.5704 - val_loss: 28.8767 - val_MinusLogProbMetric: 28.8767 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 704/1000
2023-10-26 00:54:15.128 
Epoch 704/1000 
	 loss: 28.5619, MinusLogProbMetric: 28.5619, val_loss: 28.9400, val_MinusLogProbMetric: 28.9400

Epoch 704: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5619 - MinusLogProbMetric: 28.5619 - val_loss: 28.9400 - val_MinusLogProbMetric: 28.9400 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 705/1000
2023-10-26 00:54:57.448 
Epoch 705/1000 
	 loss: 28.5531, MinusLogProbMetric: 28.5531, val_loss: 28.9036, val_MinusLogProbMetric: 28.9036

Epoch 705: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5531 - MinusLogProbMetric: 28.5531 - val_loss: 28.9036 - val_MinusLogProbMetric: 28.9036 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 706/1000
2023-10-26 00:55:39.770 
Epoch 706/1000 
	 loss: 28.5794, MinusLogProbMetric: 28.5794, val_loss: 28.9501, val_MinusLogProbMetric: 28.9501

Epoch 706: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5794 - MinusLogProbMetric: 28.5794 - val_loss: 28.9501 - val_MinusLogProbMetric: 28.9501 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 707/1000
2023-10-26 00:56:22.302 
Epoch 707/1000 
	 loss: 28.5590, MinusLogProbMetric: 28.5590, val_loss: 28.9620, val_MinusLogProbMetric: 28.9620

Epoch 707: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5590 - MinusLogProbMetric: 28.5590 - val_loss: 28.9620 - val_MinusLogProbMetric: 28.9620 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 708/1000
2023-10-26 00:57:05.058 
Epoch 708/1000 
	 loss: 28.5387, MinusLogProbMetric: 28.5387, val_loss: 29.0441, val_MinusLogProbMetric: 29.0441

Epoch 708: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5387 - MinusLogProbMetric: 28.5387 - val_loss: 29.0441 - val_MinusLogProbMetric: 29.0441 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 709/1000
2023-10-26 00:57:47.321 
Epoch 709/1000 
	 loss: 28.5625, MinusLogProbMetric: 28.5625, val_loss: 28.8985, val_MinusLogProbMetric: 28.8985

Epoch 709: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5625 - MinusLogProbMetric: 28.5625 - val_loss: 28.8985 - val_MinusLogProbMetric: 28.8985 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 710/1000
2023-10-26 00:58:30.161 
Epoch 710/1000 
	 loss: 28.5853, MinusLogProbMetric: 28.5853, val_loss: 28.9165, val_MinusLogProbMetric: 28.9165

Epoch 710: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5853 - MinusLogProbMetric: 28.5853 - val_loss: 28.9165 - val_MinusLogProbMetric: 28.9165 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 711/1000
2023-10-26 00:59:12.271 
Epoch 711/1000 
	 loss: 28.5536, MinusLogProbMetric: 28.5536, val_loss: 28.9719, val_MinusLogProbMetric: 28.9719

Epoch 711: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5536 - MinusLogProbMetric: 28.5536 - val_loss: 28.9719 - val_MinusLogProbMetric: 28.9719 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 712/1000
2023-10-26 00:59:54.921 
Epoch 712/1000 
	 loss: 28.5544, MinusLogProbMetric: 28.5544, val_loss: 28.9390, val_MinusLogProbMetric: 28.9390

Epoch 712: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5544 - MinusLogProbMetric: 28.5544 - val_loss: 28.9390 - val_MinusLogProbMetric: 28.9390 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 713/1000
2023-10-26 01:00:37.115 
Epoch 713/1000 
	 loss: 28.5698, MinusLogProbMetric: 28.5698, val_loss: 28.9439, val_MinusLogProbMetric: 28.9439

Epoch 713: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5698 - MinusLogProbMetric: 28.5698 - val_loss: 28.9439 - val_MinusLogProbMetric: 28.9439 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 714/1000
2023-10-26 01:01:19.589 
Epoch 714/1000 
	 loss: 28.5629, MinusLogProbMetric: 28.5629, val_loss: 28.8782, val_MinusLogProbMetric: 28.8782

Epoch 714: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5629 - MinusLogProbMetric: 28.5629 - val_loss: 28.8782 - val_MinusLogProbMetric: 28.8782 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 715/1000
2023-10-26 01:02:02.412 
Epoch 715/1000 
	 loss: 28.5505, MinusLogProbMetric: 28.5505, val_loss: 28.8616, val_MinusLogProbMetric: 28.8616

Epoch 715: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5505 - MinusLogProbMetric: 28.5505 - val_loss: 28.8616 - val_MinusLogProbMetric: 28.8616 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 716/1000
2023-10-26 01:02:44.930 
Epoch 716/1000 
	 loss: 28.5475, MinusLogProbMetric: 28.5475, val_loss: 28.9670, val_MinusLogProbMetric: 28.9670

Epoch 716: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5475 - MinusLogProbMetric: 28.5475 - val_loss: 28.9670 - val_MinusLogProbMetric: 28.9670 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 717/1000
2023-10-26 01:03:27.945 
Epoch 717/1000 
	 loss: 28.5534, MinusLogProbMetric: 28.5534, val_loss: 29.1036, val_MinusLogProbMetric: 29.1036

Epoch 717: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5534 - MinusLogProbMetric: 28.5534 - val_loss: 29.1036 - val_MinusLogProbMetric: 29.1036 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 718/1000
2023-10-26 01:04:10.540 
Epoch 718/1000 
	 loss: 28.5541, MinusLogProbMetric: 28.5541, val_loss: 28.9880, val_MinusLogProbMetric: 28.9880

Epoch 718: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5541 - MinusLogProbMetric: 28.5541 - val_loss: 28.9880 - val_MinusLogProbMetric: 28.9880 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 719/1000
2023-10-26 01:04:52.678 
Epoch 719/1000 
	 loss: 28.5691, MinusLogProbMetric: 28.5691, val_loss: 28.8738, val_MinusLogProbMetric: 28.8738

Epoch 719: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5691 - MinusLogProbMetric: 28.5691 - val_loss: 28.8738 - val_MinusLogProbMetric: 28.8738 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 720/1000
2023-10-26 01:05:34.912 
Epoch 720/1000 
	 loss: 28.5366, MinusLogProbMetric: 28.5366, val_loss: 28.8688, val_MinusLogProbMetric: 28.8688

Epoch 720: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5366 - MinusLogProbMetric: 28.5366 - val_loss: 28.8688 - val_MinusLogProbMetric: 28.8688 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 721/1000
2023-10-26 01:06:17.630 
Epoch 721/1000 
	 loss: 28.5673, MinusLogProbMetric: 28.5673, val_loss: 28.8996, val_MinusLogProbMetric: 28.8996

Epoch 721: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5673 - MinusLogProbMetric: 28.5673 - val_loss: 28.8996 - val_MinusLogProbMetric: 28.8996 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 722/1000
2023-10-26 01:07:00.457 
Epoch 722/1000 
	 loss: 28.5463, MinusLogProbMetric: 28.5463, val_loss: 28.8884, val_MinusLogProbMetric: 28.8884

Epoch 722: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5463 - MinusLogProbMetric: 28.5463 - val_loss: 28.8884 - val_MinusLogProbMetric: 28.8884 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 723/1000
2023-10-26 01:07:42.991 
Epoch 723/1000 
	 loss: 28.5662, MinusLogProbMetric: 28.5662, val_loss: 28.9192, val_MinusLogProbMetric: 28.9192

Epoch 723: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5662 - MinusLogProbMetric: 28.5662 - val_loss: 28.9192 - val_MinusLogProbMetric: 28.9192 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 724/1000
2023-10-26 01:08:25.800 
Epoch 724/1000 
	 loss: 28.5462, MinusLogProbMetric: 28.5462, val_loss: 28.9315, val_MinusLogProbMetric: 28.9315

Epoch 724: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5462 - MinusLogProbMetric: 28.5462 - val_loss: 28.9315 - val_MinusLogProbMetric: 28.9315 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 725/1000
2023-10-26 01:09:08.000 
Epoch 725/1000 
	 loss: 28.5441, MinusLogProbMetric: 28.5441, val_loss: 28.9049, val_MinusLogProbMetric: 28.9049

Epoch 725: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5441 - MinusLogProbMetric: 28.5441 - val_loss: 28.9049 - val_MinusLogProbMetric: 28.9049 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 726/1000
2023-10-26 01:09:50.525 
Epoch 726/1000 
	 loss: 28.5695, MinusLogProbMetric: 28.5695, val_loss: 28.8707, val_MinusLogProbMetric: 28.8707

Epoch 726: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5695 - MinusLogProbMetric: 28.5695 - val_loss: 28.8707 - val_MinusLogProbMetric: 28.8707 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 727/1000
2023-10-26 01:10:33.151 
Epoch 727/1000 
	 loss: 28.5360, MinusLogProbMetric: 28.5360, val_loss: 28.8916, val_MinusLogProbMetric: 28.8916

Epoch 727: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5360 - MinusLogProbMetric: 28.5360 - val_loss: 28.8916 - val_MinusLogProbMetric: 28.8916 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 728/1000
2023-10-26 01:11:15.496 
Epoch 728/1000 
	 loss: 28.5357, MinusLogProbMetric: 28.5357, val_loss: 28.9560, val_MinusLogProbMetric: 28.9560

Epoch 728: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5357 - MinusLogProbMetric: 28.5357 - val_loss: 28.9560 - val_MinusLogProbMetric: 28.9560 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 729/1000
2023-10-26 01:11:58.276 
Epoch 729/1000 
	 loss: 28.5407, MinusLogProbMetric: 28.5407, val_loss: 28.9985, val_MinusLogProbMetric: 28.9985

Epoch 729: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5407 - MinusLogProbMetric: 28.5407 - val_loss: 28.9985 - val_MinusLogProbMetric: 28.9985 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 730/1000
2023-10-26 01:12:40.387 
Epoch 730/1000 
	 loss: 28.5404, MinusLogProbMetric: 28.5404, val_loss: 28.8768, val_MinusLogProbMetric: 28.8768

Epoch 730: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5404 - MinusLogProbMetric: 28.5404 - val_loss: 28.8768 - val_MinusLogProbMetric: 28.8768 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 731/1000
2023-10-26 01:13:22.803 
Epoch 731/1000 
	 loss: 28.5303, MinusLogProbMetric: 28.5303, val_loss: 28.9032, val_MinusLogProbMetric: 28.9032

Epoch 731: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5303 - MinusLogProbMetric: 28.5303 - val_loss: 28.9032 - val_MinusLogProbMetric: 28.9032 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 732/1000
2023-10-26 01:14:05.163 
Epoch 732/1000 
	 loss: 28.5413, MinusLogProbMetric: 28.5413, val_loss: 28.8838, val_MinusLogProbMetric: 28.8838

Epoch 732: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5413 - MinusLogProbMetric: 28.5413 - val_loss: 28.8838 - val_MinusLogProbMetric: 28.8838 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 733/1000
2023-10-26 01:14:47.487 
Epoch 733/1000 
	 loss: 28.5345, MinusLogProbMetric: 28.5345, val_loss: 28.8913, val_MinusLogProbMetric: 28.8913

Epoch 733: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5345 - MinusLogProbMetric: 28.5345 - val_loss: 28.8913 - val_MinusLogProbMetric: 28.8913 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 734/1000
2023-10-26 01:15:30.077 
Epoch 734/1000 
	 loss: 28.5476, MinusLogProbMetric: 28.5476, val_loss: 28.8940, val_MinusLogProbMetric: 28.8940

Epoch 734: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5476 - MinusLogProbMetric: 28.5476 - val_loss: 28.8940 - val_MinusLogProbMetric: 28.8940 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 735/1000
2023-10-26 01:16:12.119 
Epoch 735/1000 
	 loss: 28.5336, MinusLogProbMetric: 28.5336, val_loss: 28.8995, val_MinusLogProbMetric: 28.8995

Epoch 735: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5336 - MinusLogProbMetric: 28.5336 - val_loss: 28.8995 - val_MinusLogProbMetric: 28.8995 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 736/1000
2023-10-26 01:16:55.117 
Epoch 736/1000 
	 loss: 28.5377, MinusLogProbMetric: 28.5377, val_loss: 28.9299, val_MinusLogProbMetric: 28.9299

Epoch 736: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5377 - MinusLogProbMetric: 28.5377 - val_loss: 28.9299 - val_MinusLogProbMetric: 28.9299 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 737/1000
2023-10-26 01:17:37.412 
Epoch 737/1000 
	 loss: 28.5337, MinusLogProbMetric: 28.5337, val_loss: 28.8837, val_MinusLogProbMetric: 28.8837

Epoch 737: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5337 - MinusLogProbMetric: 28.5337 - val_loss: 28.8837 - val_MinusLogProbMetric: 28.8837 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 738/1000
2023-10-26 01:18:19.842 
Epoch 738/1000 
	 loss: 28.5450, MinusLogProbMetric: 28.5450, val_loss: 28.8988, val_MinusLogProbMetric: 28.8988

Epoch 738: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5450 - MinusLogProbMetric: 28.5450 - val_loss: 28.8988 - val_MinusLogProbMetric: 28.8988 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 739/1000
2023-10-26 01:19:01.947 
Epoch 739/1000 
	 loss: 28.5243, MinusLogProbMetric: 28.5243, val_loss: 28.8975, val_MinusLogProbMetric: 28.8975

Epoch 739: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5243 - MinusLogProbMetric: 28.5243 - val_loss: 28.8975 - val_MinusLogProbMetric: 28.8975 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 740/1000
2023-10-26 01:19:44.348 
Epoch 740/1000 
	 loss: 28.5297, MinusLogProbMetric: 28.5297, val_loss: 28.8901, val_MinusLogProbMetric: 28.8901

Epoch 740: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5297 - MinusLogProbMetric: 28.5297 - val_loss: 28.8901 - val_MinusLogProbMetric: 28.8901 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 741/1000
2023-10-26 01:20:26.733 
Epoch 741/1000 
	 loss: 28.5248, MinusLogProbMetric: 28.5248, val_loss: 28.9036, val_MinusLogProbMetric: 28.9036

Epoch 741: val_loss did not improve from 28.85905
196/196 - 42s - loss: 28.5248 - MinusLogProbMetric: 28.5248 - val_loss: 28.9036 - val_MinusLogProbMetric: 28.9036 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 742/1000
2023-10-26 01:21:09.453 
Epoch 742/1000 
	 loss: 28.5385, MinusLogProbMetric: 28.5385, val_loss: 28.8966, val_MinusLogProbMetric: 28.8966

Epoch 742: val_loss did not improve from 28.85905
196/196 - 43s - loss: 28.5385 - MinusLogProbMetric: 28.5385 - val_loss: 28.8966 - val_MinusLogProbMetric: 28.8966 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 743/1000
2023-10-26 01:21:52.224 
Epoch 743/1000 
	 loss: 28.5354, MinusLogProbMetric: 28.5354, val_loss: 28.8242, val_MinusLogProbMetric: 28.8242

Epoch 743: val_loss improved from 28.85905 to 28.82415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 28.5354 - MinusLogProbMetric: 28.5354 - val_loss: 28.8242 - val_MinusLogProbMetric: 28.8242 - lr: 8.3333e-05 - 44s/epoch - 222ms/step
Epoch 744/1000
2023-10-26 01:22:35.220 
Epoch 744/1000 
	 loss: 28.5347, MinusLogProbMetric: 28.5347, val_loss: 28.9509, val_MinusLogProbMetric: 28.9509

Epoch 744: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5347 - MinusLogProbMetric: 28.5347 - val_loss: 28.9509 - val_MinusLogProbMetric: 28.9509 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 745/1000
2023-10-26 01:23:17.594 
Epoch 745/1000 
	 loss: 28.5258, MinusLogProbMetric: 28.5258, val_loss: 28.8505, val_MinusLogProbMetric: 28.8505

Epoch 745: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5258 - MinusLogProbMetric: 28.5258 - val_loss: 28.8505 - val_MinusLogProbMetric: 28.8505 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 746/1000
2023-10-26 01:24:00.217 
Epoch 746/1000 
	 loss: 28.5275, MinusLogProbMetric: 28.5275, val_loss: 28.8689, val_MinusLogProbMetric: 28.8689

Epoch 746: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5275 - MinusLogProbMetric: 28.5275 - val_loss: 28.8689 - val_MinusLogProbMetric: 28.8689 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 747/1000
2023-10-26 01:24:42.959 
Epoch 747/1000 
	 loss: 28.5170, MinusLogProbMetric: 28.5170, val_loss: 28.8409, val_MinusLogProbMetric: 28.8409

Epoch 747: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5170 - MinusLogProbMetric: 28.5170 - val_loss: 28.8409 - val_MinusLogProbMetric: 28.8409 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 748/1000
2023-10-26 01:25:25.688 
Epoch 748/1000 
	 loss: 28.5252, MinusLogProbMetric: 28.5252, val_loss: 28.8326, val_MinusLogProbMetric: 28.8326

Epoch 748: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5252 - MinusLogProbMetric: 28.5252 - val_loss: 28.8326 - val_MinusLogProbMetric: 28.8326 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 749/1000
2023-10-26 01:26:08.231 
Epoch 749/1000 
	 loss: 28.5263, MinusLogProbMetric: 28.5263, val_loss: 28.8580, val_MinusLogProbMetric: 28.8580

Epoch 749: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5263 - MinusLogProbMetric: 28.5263 - val_loss: 28.8580 - val_MinusLogProbMetric: 28.8580 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 750/1000
2023-10-26 01:26:50.606 
Epoch 750/1000 
	 loss: 28.5101, MinusLogProbMetric: 28.5101, val_loss: 28.8597, val_MinusLogProbMetric: 28.8597

Epoch 750: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5101 - MinusLogProbMetric: 28.5101 - val_loss: 28.8597 - val_MinusLogProbMetric: 28.8597 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 751/1000
2023-10-26 01:27:33.388 
Epoch 751/1000 
	 loss: 28.5220, MinusLogProbMetric: 28.5220, val_loss: 28.9446, val_MinusLogProbMetric: 28.9446

Epoch 751: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5220 - MinusLogProbMetric: 28.5220 - val_loss: 28.9446 - val_MinusLogProbMetric: 28.9446 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 752/1000
2023-10-26 01:28:15.824 
Epoch 752/1000 
	 loss: 28.5149, MinusLogProbMetric: 28.5149, val_loss: 28.9662, val_MinusLogProbMetric: 28.9662

Epoch 752: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5149 - MinusLogProbMetric: 28.5149 - val_loss: 28.9662 - val_MinusLogProbMetric: 28.9662 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 753/1000
2023-10-26 01:28:58.265 
Epoch 753/1000 
	 loss: 28.5174, MinusLogProbMetric: 28.5174, val_loss: 28.9774, val_MinusLogProbMetric: 28.9774

Epoch 753: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5174 - MinusLogProbMetric: 28.5174 - val_loss: 28.9774 - val_MinusLogProbMetric: 28.9774 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 754/1000
2023-10-26 01:29:40.238 
Epoch 754/1000 
	 loss: 28.5303, MinusLogProbMetric: 28.5303, val_loss: 29.0053, val_MinusLogProbMetric: 29.0053

Epoch 754: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5303 - MinusLogProbMetric: 28.5303 - val_loss: 29.0053 - val_MinusLogProbMetric: 29.0053 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 755/1000
2023-10-26 01:30:22.188 
Epoch 755/1000 
	 loss: 28.5163, MinusLogProbMetric: 28.5163, val_loss: 28.8809, val_MinusLogProbMetric: 28.8809

Epoch 755: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5163 - MinusLogProbMetric: 28.5163 - val_loss: 28.8809 - val_MinusLogProbMetric: 28.8809 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 756/1000
2023-10-26 01:31:04.641 
Epoch 756/1000 
	 loss: 28.5267, MinusLogProbMetric: 28.5267, val_loss: 28.8698, val_MinusLogProbMetric: 28.8698

Epoch 756: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5267 - MinusLogProbMetric: 28.5267 - val_loss: 28.8698 - val_MinusLogProbMetric: 28.8698 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 757/1000
2023-10-26 01:31:47.535 
Epoch 757/1000 
	 loss: 28.5133, MinusLogProbMetric: 28.5133, val_loss: 28.9043, val_MinusLogProbMetric: 28.9043

Epoch 757: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5133 - MinusLogProbMetric: 28.5133 - val_loss: 28.9043 - val_MinusLogProbMetric: 28.9043 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 758/1000
2023-10-26 01:32:29.966 
Epoch 758/1000 
	 loss: 28.5175, MinusLogProbMetric: 28.5175, val_loss: 28.8485, val_MinusLogProbMetric: 28.8485

Epoch 758: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5175 - MinusLogProbMetric: 28.5175 - val_loss: 28.8485 - val_MinusLogProbMetric: 28.8485 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 759/1000
2023-10-26 01:33:12.331 
Epoch 759/1000 
	 loss: 28.5205, MinusLogProbMetric: 28.5205, val_loss: 28.8695, val_MinusLogProbMetric: 28.8695

Epoch 759: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5205 - MinusLogProbMetric: 28.5205 - val_loss: 28.8695 - val_MinusLogProbMetric: 28.8695 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 760/1000
2023-10-26 01:33:54.694 
Epoch 760/1000 
	 loss: 28.5051, MinusLogProbMetric: 28.5051, val_loss: 28.8660, val_MinusLogProbMetric: 28.8660

Epoch 760: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5051 - MinusLogProbMetric: 28.5051 - val_loss: 28.8660 - val_MinusLogProbMetric: 28.8660 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 761/1000
2023-10-26 01:34:37.319 
Epoch 761/1000 
	 loss: 28.5089, MinusLogProbMetric: 28.5089, val_loss: 28.9045, val_MinusLogProbMetric: 28.9045

Epoch 761: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5089 - MinusLogProbMetric: 28.5089 - val_loss: 28.9045 - val_MinusLogProbMetric: 28.9045 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 762/1000
2023-10-26 01:35:19.803 
Epoch 762/1000 
	 loss: 28.5210, MinusLogProbMetric: 28.5210, val_loss: 28.9270, val_MinusLogProbMetric: 28.9270

Epoch 762: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5210 - MinusLogProbMetric: 28.5210 - val_loss: 28.9270 - val_MinusLogProbMetric: 28.9270 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 763/1000
2023-10-26 01:36:02.081 
Epoch 763/1000 
	 loss: 28.5197, MinusLogProbMetric: 28.5197, val_loss: 28.9297, val_MinusLogProbMetric: 28.9297

Epoch 763: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5197 - MinusLogProbMetric: 28.5197 - val_loss: 28.9297 - val_MinusLogProbMetric: 28.9297 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 764/1000
2023-10-26 01:36:44.150 
Epoch 764/1000 
	 loss: 28.5253, MinusLogProbMetric: 28.5253, val_loss: 28.8527, val_MinusLogProbMetric: 28.8527

Epoch 764: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5253 - MinusLogProbMetric: 28.5253 - val_loss: 28.8527 - val_MinusLogProbMetric: 28.8527 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 765/1000
2023-10-26 01:37:25.912 
Epoch 765/1000 
	 loss: 28.5154, MinusLogProbMetric: 28.5154, val_loss: 28.9215, val_MinusLogProbMetric: 28.9215

Epoch 765: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5154 - MinusLogProbMetric: 28.5154 - val_loss: 28.9215 - val_MinusLogProbMetric: 28.9215 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 766/1000
2023-10-26 01:38:01.941 
Epoch 766/1000 
	 loss: 28.5091, MinusLogProbMetric: 28.5091, val_loss: 28.9321, val_MinusLogProbMetric: 28.9321

Epoch 766: val_loss did not improve from 28.82415
196/196 - 36s - loss: 28.5091 - MinusLogProbMetric: 28.5091 - val_loss: 28.9321 - val_MinusLogProbMetric: 28.9321 - lr: 8.3333e-05 - 36s/epoch - 184ms/step
Epoch 767/1000
2023-10-26 01:38:39.812 
Epoch 767/1000 
	 loss: 28.5161, MinusLogProbMetric: 28.5161, val_loss: 28.8679, val_MinusLogProbMetric: 28.8679

Epoch 767: val_loss did not improve from 28.82415
196/196 - 38s - loss: 28.5161 - MinusLogProbMetric: 28.5161 - val_loss: 28.8679 - val_MinusLogProbMetric: 28.8679 - lr: 8.3333e-05 - 38s/epoch - 193ms/step
Epoch 768/1000
2023-10-26 01:39:19.264 
Epoch 768/1000 
	 loss: 28.5345, MinusLogProbMetric: 28.5345, val_loss: 28.8780, val_MinusLogProbMetric: 28.8780

Epoch 768: val_loss did not improve from 28.82415
196/196 - 39s - loss: 28.5345 - MinusLogProbMetric: 28.5345 - val_loss: 28.8780 - val_MinusLogProbMetric: 28.8780 - lr: 8.3333e-05 - 39s/epoch - 201ms/step
Epoch 769/1000
2023-10-26 01:39:57.110 
Epoch 769/1000 
	 loss: 28.5266, MinusLogProbMetric: 28.5266, val_loss: 28.8339, val_MinusLogProbMetric: 28.8339

Epoch 769: val_loss did not improve from 28.82415
196/196 - 38s - loss: 28.5266 - MinusLogProbMetric: 28.5266 - val_loss: 28.8339 - val_MinusLogProbMetric: 28.8339 - lr: 8.3333e-05 - 38s/epoch - 193ms/step
Epoch 770/1000
2023-10-26 01:40:39.117 
Epoch 770/1000 
	 loss: 28.5139, MinusLogProbMetric: 28.5139, val_loss: 28.8394, val_MinusLogProbMetric: 28.8394

Epoch 770: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5139 - MinusLogProbMetric: 28.5139 - val_loss: 28.8394 - val_MinusLogProbMetric: 28.8394 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 771/1000
2023-10-26 01:41:21.043 
Epoch 771/1000 
	 loss: 28.5233, MinusLogProbMetric: 28.5233, val_loss: 28.9962, val_MinusLogProbMetric: 28.9962

Epoch 771: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5233 - MinusLogProbMetric: 28.5233 - val_loss: 28.9962 - val_MinusLogProbMetric: 28.9962 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 772/1000
2023-10-26 01:42:03.445 
Epoch 772/1000 
	 loss: 28.5185, MinusLogProbMetric: 28.5185, val_loss: 29.0660, val_MinusLogProbMetric: 29.0660

Epoch 772: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5185 - MinusLogProbMetric: 28.5185 - val_loss: 29.0660 - val_MinusLogProbMetric: 29.0660 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 773/1000
2023-10-26 01:42:45.692 
Epoch 773/1000 
	 loss: 28.5205, MinusLogProbMetric: 28.5205, val_loss: 29.0942, val_MinusLogProbMetric: 29.0942

Epoch 773: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5205 - MinusLogProbMetric: 28.5205 - val_loss: 29.0942 - val_MinusLogProbMetric: 29.0942 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 774/1000
2023-10-26 01:43:28.205 
Epoch 774/1000 
	 loss: 28.5345, MinusLogProbMetric: 28.5345, val_loss: 28.9081, val_MinusLogProbMetric: 28.9081

Epoch 774: val_loss did not improve from 28.82415
196/196 - 43s - loss: 28.5345 - MinusLogProbMetric: 28.5345 - val_loss: 28.9081 - val_MinusLogProbMetric: 28.9081 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 775/1000
2023-10-26 01:44:10.690 
Epoch 775/1000 
	 loss: 28.5252, MinusLogProbMetric: 28.5252, val_loss: 28.8528, val_MinusLogProbMetric: 28.8528

Epoch 775: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5252 - MinusLogProbMetric: 28.5252 - val_loss: 28.8528 - val_MinusLogProbMetric: 28.8528 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 776/1000
2023-10-26 01:44:52.865 
Epoch 776/1000 
	 loss: 28.5152, MinusLogProbMetric: 28.5152, val_loss: 28.9282, val_MinusLogProbMetric: 28.9282

Epoch 776: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5152 - MinusLogProbMetric: 28.5152 - val_loss: 28.9282 - val_MinusLogProbMetric: 28.9282 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 777/1000
2023-10-26 01:45:35.195 
Epoch 777/1000 
	 loss: 28.5465, MinusLogProbMetric: 28.5465, val_loss: 28.9339, val_MinusLogProbMetric: 28.9339

Epoch 777: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5465 - MinusLogProbMetric: 28.5465 - val_loss: 28.9339 - val_MinusLogProbMetric: 28.9339 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 778/1000
2023-10-26 01:46:17.293 
Epoch 778/1000 
	 loss: 28.5346, MinusLogProbMetric: 28.5346, val_loss: 28.9643, val_MinusLogProbMetric: 28.9643

Epoch 778: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5346 - MinusLogProbMetric: 28.5346 - val_loss: 28.9643 - val_MinusLogProbMetric: 28.9643 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 779/1000
2023-10-26 01:46:59.627 
Epoch 779/1000 
	 loss: 28.5330, MinusLogProbMetric: 28.5330, val_loss: 28.8449, val_MinusLogProbMetric: 28.8449

Epoch 779: val_loss did not improve from 28.82415
196/196 - 42s - loss: 28.5330 - MinusLogProbMetric: 28.5330 - val_loss: 28.8449 - val_MinusLogProbMetric: 28.8449 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 780/1000
2023-10-26 01:47:42.029 
Epoch 780/1000 
	 loss: 28.5307, MinusLogProbMetric: 28.5307, val_loss: 28.8152, val_MinusLogProbMetric: 28.8152

Epoch 780: val_loss improved from 28.82415 to 28.81525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.5307 - MinusLogProbMetric: 28.5307 - val_loss: 28.8152 - val_MinusLogProbMetric: 28.8152 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 781/1000
2023-10-26 01:48:25.379 
Epoch 781/1000 
	 loss: 28.5059, MinusLogProbMetric: 28.5059, val_loss: 28.9034, val_MinusLogProbMetric: 28.9034

Epoch 781: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5059 - MinusLogProbMetric: 28.5059 - val_loss: 28.9034 - val_MinusLogProbMetric: 28.9034 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 782/1000
2023-10-26 01:49:08.164 
Epoch 782/1000 
	 loss: 28.5149, MinusLogProbMetric: 28.5149, val_loss: 28.9718, val_MinusLogProbMetric: 28.9718

Epoch 782: val_loss did not improve from 28.81525
196/196 - 43s - loss: 28.5149 - MinusLogProbMetric: 28.5149 - val_loss: 28.9718 - val_MinusLogProbMetric: 28.9718 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 783/1000
2023-10-26 01:49:50.424 
Epoch 783/1000 
	 loss: 28.4984, MinusLogProbMetric: 28.4984, val_loss: 28.8441, val_MinusLogProbMetric: 28.8441

Epoch 783: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4984 - MinusLogProbMetric: 28.4984 - val_loss: 28.8441 - val_MinusLogProbMetric: 28.8441 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 784/1000
2023-10-26 01:50:32.507 
Epoch 784/1000 
	 loss: 28.5064, MinusLogProbMetric: 28.5064, val_loss: 28.8552, val_MinusLogProbMetric: 28.8552

Epoch 784: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5064 - MinusLogProbMetric: 28.5064 - val_loss: 28.8552 - val_MinusLogProbMetric: 28.8552 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 785/1000
2023-10-26 01:51:14.549 
Epoch 785/1000 
	 loss: 28.5181, MinusLogProbMetric: 28.5181, val_loss: 28.8727, val_MinusLogProbMetric: 28.8727

Epoch 785: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5181 - MinusLogProbMetric: 28.5181 - val_loss: 28.8727 - val_MinusLogProbMetric: 28.8727 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 786/1000
2023-10-26 01:51:56.588 
Epoch 786/1000 
	 loss: 28.5066, MinusLogProbMetric: 28.5066, val_loss: 28.9022, val_MinusLogProbMetric: 28.9022

Epoch 786: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5066 - MinusLogProbMetric: 28.5066 - val_loss: 28.9022 - val_MinusLogProbMetric: 28.9022 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 787/1000
2023-10-26 01:52:38.609 
Epoch 787/1000 
	 loss: 28.4977, MinusLogProbMetric: 28.4977, val_loss: 28.9214, val_MinusLogProbMetric: 28.9214

Epoch 787: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4977 - MinusLogProbMetric: 28.4977 - val_loss: 28.9214 - val_MinusLogProbMetric: 28.9214 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 788/1000
2023-10-26 01:53:20.776 
Epoch 788/1000 
	 loss: 28.5047, MinusLogProbMetric: 28.5047, val_loss: 28.8778, val_MinusLogProbMetric: 28.8778

Epoch 788: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5047 - MinusLogProbMetric: 28.5047 - val_loss: 28.8778 - val_MinusLogProbMetric: 28.8778 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 789/1000
2023-10-26 01:54:03.064 
Epoch 789/1000 
	 loss: 28.5037, MinusLogProbMetric: 28.5037, val_loss: 28.8876, val_MinusLogProbMetric: 28.8876

Epoch 789: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5037 - MinusLogProbMetric: 28.5037 - val_loss: 28.8876 - val_MinusLogProbMetric: 28.8876 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 790/1000
2023-10-26 01:54:45.044 
Epoch 790/1000 
	 loss: 28.5021, MinusLogProbMetric: 28.5021, val_loss: 28.8807, val_MinusLogProbMetric: 28.8807

Epoch 790: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5021 - MinusLogProbMetric: 28.5021 - val_loss: 28.8807 - val_MinusLogProbMetric: 28.8807 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 791/1000
2023-10-26 01:55:27.027 
Epoch 791/1000 
	 loss: 28.4974, MinusLogProbMetric: 28.4974, val_loss: 28.8580, val_MinusLogProbMetric: 28.8580

Epoch 791: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4974 - MinusLogProbMetric: 28.4974 - val_loss: 28.8580 - val_MinusLogProbMetric: 28.8580 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 792/1000
2023-10-26 01:56:08.922 
Epoch 792/1000 
	 loss: 28.5151, MinusLogProbMetric: 28.5151, val_loss: 28.8484, val_MinusLogProbMetric: 28.8484

Epoch 792: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5151 - MinusLogProbMetric: 28.5151 - val_loss: 28.8484 - val_MinusLogProbMetric: 28.8484 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 793/1000
2023-10-26 01:56:51.312 
Epoch 793/1000 
	 loss: 28.5154, MinusLogProbMetric: 28.5154, val_loss: 28.8843, val_MinusLogProbMetric: 28.8843

Epoch 793: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5154 - MinusLogProbMetric: 28.5154 - val_loss: 28.8843 - val_MinusLogProbMetric: 28.8843 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 794/1000
2023-10-26 01:57:33.489 
Epoch 794/1000 
	 loss: 28.5041, MinusLogProbMetric: 28.5041, val_loss: 28.8363, val_MinusLogProbMetric: 28.8363

Epoch 794: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5041 - MinusLogProbMetric: 28.5041 - val_loss: 28.8363 - val_MinusLogProbMetric: 28.8363 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 795/1000
2023-10-26 01:58:15.479 
Epoch 795/1000 
	 loss: 28.5142, MinusLogProbMetric: 28.5142, val_loss: 29.0174, val_MinusLogProbMetric: 29.0174

Epoch 795: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5142 - MinusLogProbMetric: 28.5142 - val_loss: 29.0174 - val_MinusLogProbMetric: 29.0174 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 796/1000
2023-10-26 01:58:57.476 
Epoch 796/1000 
	 loss: 28.4971, MinusLogProbMetric: 28.4971, val_loss: 28.8487, val_MinusLogProbMetric: 28.8487

Epoch 796: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4971 - MinusLogProbMetric: 28.4971 - val_loss: 28.8487 - val_MinusLogProbMetric: 28.8487 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 797/1000
2023-10-26 01:59:38.934 
Epoch 797/1000 
	 loss: 28.4985, MinusLogProbMetric: 28.4985, val_loss: 28.8746, val_MinusLogProbMetric: 28.8746

Epoch 797: val_loss did not improve from 28.81525
196/196 - 41s - loss: 28.4985 - MinusLogProbMetric: 28.4985 - val_loss: 28.8746 - val_MinusLogProbMetric: 28.8746 - lr: 8.3333e-05 - 41s/epoch - 212ms/step
Epoch 798/1000
2023-10-26 02:00:20.897 
Epoch 798/1000 
	 loss: 28.5148, MinusLogProbMetric: 28.5148, val_loss: 28.8803, val_MinusLogProbMetric: 28.8803

Epoch 798: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5148 - MinusLogProbMetric: 28.5148 - val_loss: 28.8803 - val_MinusLogProbMetric: 28.8803 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 799/1000
2023-10-26 02:01:03.067 
Epoch 799/1000 
	 loss: 28.5025, MinusLogProbMetric: 28.5025, val_loss: 28.8798, val_MinusLogProbMetric: 28.8798

Epoch 799: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5025 - MinusLogProbMetric: 28.5025 - val_loss: 28.8798 - val_MinusLogProbMetric: 28.8798 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 800/1000
2023-10-26 02:01:45.195 
Epoch 800/1000 
	 loss: 28.4910, MinusLogProbMetric: 28.4910, val_loss: 28.8588, val_MinusLogProbMetric: 28.8588

Epoch 800: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4910 - MinusLogProbMetric: 28.4910 - val_loss: 28.8588 - val_MinusLogProbMetric: 28.8588 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 801/1000
2023-10-26 02:02:27.251 
Epoch 801/1000 
	 loss: 28.5055, MinusLogProbMetric: 28.5055, val_loss: 28.8717, val_MinusLogProbMetric: 28.8717

Epoch 801: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5055 - MinusLogProbMetric: 28.5055 - val_loss: 28.8717 - val_MinusLogProbMetric: 28.8717 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 802/1000
2023-10-26 02:03:09.393 
Epoch 802/1000 
	 loss: 28.4942, MinusLogProbMetric: 28.4942, val_loss: 28.8851, val_MinusLogProbMetric: 28.8851

Epoch 802: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4942 - MinusLogProbMetric: 28.4942 - val_loss: 28.8851 - val_MinusLogProbMetric: 28.8851 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 803/1000
2023-10-26 02:03:51.396 
Epoch 803/1000 
	 loss: 28.4981, MinusLogProbMetric: 28.4981, val_loss: 28.8493, val_MinusLogProbMetric: 28.8493

Epoch 803: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4981 - MinusLogProbMetric: 28.4981 - val_loss: 28.8493 - val_MinusLogProbMetric: 28.8493 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 804/1000
2023-10-26 02:04:33.674 
Epoch 804/1000 
	 loss: 28.5071, MinusLogProbMetric: 28.5071, val_loss: 28.9635, val_MinusLogProbMetric: 28.9635

Epoch 804: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5071 - MinusLogProbMetric: 28.5071 - val_loss: 28.9635 - val_MinusLogProbMetric: 28.9635 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 805/1000
2023-10-26 02:05:15.958 
Epoch 805/1000 
	 loss: 28.4883, MinusLogProbMetric: 28.4883, val_loss: 28.8200, val_MinusLogProbMetric: 28.8200

Epoch 805: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4883 - MinusLogProbMetric: 28.4883 - val_loss: 28.8200 - val_MinusLogProbMetric: 28.8200 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 806/1000
2023-10-26 02:05:57.762 
Epoch 806/1000 
	 loss: 28.4919, MinusLogProbMetric: 28.4919, val_loss: 28.8610, val_MinusLogProbMetric: 28.8610

Epoch 806: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4919 - MinusLogProbMetric: 28.4919 - val_loss: 28.8610 - val_MinusLogProbMetric: 28.8610 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 807/1000
2023-10-26 02:06:40.117 
Epoch 807/1000 
	 loss: 28.5183, MinusLogProbMetric: 28.5183, val_loss: 28.9282, val_MinusLogProbMetric: 28.9282

Epoch 807: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5183 - MinusLogProbMetric: 28.5183 - val_loss: 28.9282 - val_MinusLogProbMetric: 28.9282 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 808/1000
2023-10-26 02:07:21.879 
Epoch 808/1000 
	 loss: 28.4960, MinusLogProbMetric: 28.4960, val_loss: 28.8551, val_MinusLogProbMetric: 28.8551

Epoch 808: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4960 - MinusLogProbMetric: 28.4960 - val_loss: 28.8551 - val_MinusLogProbMetric: 28.8551 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 809/1000
2023-10-26 02:08:04.588 
Epoch 809/1000 
	 loss: 28.5005, MinusLogProbMetric: 28.5005, val_loss: 28.8640, val_MinusLogProbMetric: 28.8640

Epoch 809: val_loss did not improve from 28.81525
196/196 - 43s - loss: 28.5005 - MinusLogProbMetric: 28.5005 - val_loss: 28.8640 - val_MinusLogProbMetric: 28.8640 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 810/1000
2023-10-26 02:08:47.041 
Epoch 810/1000 
	 loss: 28.5022, MinusLogProbMetric: 28.5022, val_loss: 28.8857, val_MinusLogProbMetric: 28.8857

Epoch 810: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5022 - MinusLogProbMetric: 28.5022 - val_loss: 28.8857 - val_MinusLogProbMetric: 28.8857 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 811/1000
2023-10-26 02:09:29.781 
Epoch 811/1000 
	 loss: 28.4923, MinusLogProbMetric: 28.4923, val_loss: 28.8235, val_MinusLogProbMetric: 28.8235

Epoch 811: val_loss did not improve from 28.81525
196/196 - 43s - loss: 28.4923 - MinusLogProbMetric: 28.4923 - val_loss: 28.8235 - val_MinusLogProbMetric: 28.8235 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 812/1000
2023-10-26 02:10:11.926 
Epoch 812/1000 
	 loss: 28.4851, MinusLogProbMetric: 28.4851, val_loss: 28.8182, val_MinusLogProbMetric: 28.8182

Epoch 812: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4851 - MinusLogProbMetric: 28.4851 - val_loss: 28.8182 - val_MinusLogProbMetric: 28.8182 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 813/1000
2023-10-26 02:10:54.081 
Epoch 813/1000 
	 loss: 28.5086, MinusLogProbMetric: 28.5086, val_loss: 28.9108, val_MinusLogProbMetric: 28.9108

Epoch 813: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5086 - MinusLogProbMetric: 28.5086 - val_loss: 28.9108 - val_MinusLogProbMetric: 28.9108 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 814/1000
2023-10-26 02:11:36.536 
Epoch 814/1000 
	 loss: 28.4926, MinusLogProbMetric: 28.4926, val_loss: 28.8190, val_MinusLogProbMetric: 28.8190

Epoch 814: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4926 - MinusLogProbMetric: 28.4926 - val_loss: 28.8190 - val_MinusLogProbMetric: 28.8190 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 815/1000
2023-10-26 02:12:18.397 
Epoch 815/1000 
	 loss: 28.4953, MinusLogProbMetric: 28.4953, val_loss: 28.8544, val_MinusLogProbMetric: 28.8544

Epoch 815: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4953 - MinusLogProbMetric: 28.4953 - val_loss: 28.8544 - val_MinusLogProbMetric: 28.8544 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 816/1000
2023-10-26 02:13:00.792 
Epoch 816/1000 
	 loss: 28.4787, MinusLogProbMetric: 28.4787, val_loss: 28.9119, val_MinusLogProbMetric: 28.9119

Epoch 816: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4787 - MinusLogProbMetric: 28.4787 - val_loss: 28.9119 - val_MinusLogProbMetric: 28.9119 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 817/1000
2023-10-26 02:13:43.114 
Epoch 817/1000 
	 loss: 28.5026, MinusLogProbMetric: 28.5026, val_loss: 28.8434, val_MinusLogProbMetric: 28.8434

Epoch 817: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5026 - MinusLogProbMetric: 28.5026 - val_loss: 28.8434 - val_MinusLogProbMetric: 28.8434 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 818/1000
2023-10-26 02:14:24.865 
Epoch 818/1000 
	 loss: 28.5087, MinusLogProbMetric: 28.5087, val_loss: 28.8589, val_MinusLogProbMetric: 28.8589

Epoch 818: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.5087 - MinusLogProbMetric: 28.5087 - val_loss: 28.8589 - val_MinusLogProbMetric: 28.8589 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 819/1000
2023-10-26 02:15:07.295 
Epoch 819/1000 
	 loss: 28.4870, MinusLogProbMetric: 28.4870, val_loss: 28.8163, val_MinusLogProbMetric: 28.8163

Epoch 819: val_loss did not improve from 28.81525
196/196 - 42s - loss: 28.4870 - MinusLogProbMetric: 28.4870 - val_loss: 28.8163 - val_MinusLogProbMetric: 28.8163 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 820/1000
2023-10-26 02:15:49.539 
Epoch 820/1000 
	 loss: 28.5006, MinusLogProbMetric: 28.5006, val_loss: 28.8031, val_MinusLogProbMetric: 28.8031

Epoch 820: val_loss improved from 28.81525 to 28.80310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.5006 - MinusLogProbMetric: 28.5006 - val_loss: 28.8031 - val_MinusLogProbMetric: 28.8031 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 821/1000
2023-10-26 02:16:32.716 
Epoch 821/1000 
	 loss: 28.4986, MinusLogProbMetric: 28.4986, val_loss: 28.8629, val_MinusLogProbMetric: 28.8629

Epoch 821: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.4986 - MinusLogProbMetric: 28.4986 - val_loss: 28.8629 - val_MinusLogProbMetric: 28.8629 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 822/1000
2023-10-26 02:17:15.408 
Epoch 822/1000 
	 loss: 28.5000, MinusLogProbMetric: 28.5000, val_loss: 28.8795, val_MinusLogProbMetric: 28.8795

Epoch 822: val_loss did not improve from 28.80310
196/196 - 43s - loss: 28.5000 - MinusLogProbMetric: 28.5000 - val_loss: 28.8795 - val_MinusLogProbMetric: 28.8795 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 823/1000
2023-10-26 02:17:57.877 
Epoch 823/1000 
	 loss: 28.4842, MinusLogProbMetric: 28.4842, val_loss: 28.8337, val_MinusLogProbMetric: 28.8337

Epoch 823: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.4842 - MinusLogProbMetric: 28.4842 - val_loss: 28.8337 - val_MinusLogProbMetric: 28.8337 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 824/1000
2023-10-26 02:18:40.279 
Epoch 824/1000 
	 loss: 28.5038, MinusLogProbMetric: 28.5038, val_loss: 28.8567, val_MinusLogProbMetric: 28.8567

Epoch 824: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.5038 - MinusLogProbMetric: 28.5038 - val_loss: 28.8567 - val_MinusLogProbMetric: 28.8567 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 825/1000
2023-10-26 02:19:22.204 
Epoch 825/1000 
	 loss: 28.4915, MinusLogProbMetric: 28.4915, val_loss: 28.8452, val_MinusLogProbMetric: 28.8452

Epoch 825: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.4915 - MinusLogProbMetric: 28.4915 - val_loss: 28.8452 - val_MinusLogProbMetric: 28.8452 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 826/1000
2023-10-26 02:20:04.780 
Epoch 826/1000 
	 loss: 28.4866, MinusLogProbMetric: 28.4866, val_loss: 29.0177, val_MinusLogProbMetric: 29.0177

Epoch 826: val_loss did not improve from 28.80310
196/196 - 43s - loss: 28.4866 - MinusLogProbMetric: 28.4866 - val_loss: 29.0177 - val_MinusLogProbMetric: 29.0177 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 827/1000
2023-10-26 02:20:46.818 
Epoch 827/1000 
	 loss: 28.5228, MinusLogProbMetric: 28.5228, val_loss: 28.8183, val_MinusLogProbMetric: 28.8183

Epoch 827: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.5228 - MinusLogProbMetric: 28.5228 - val_loss: 28.8183 - val_MinusLogProbMetric: 28.8183 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 828/1000
2023-10-26 02:21:29.260 
Epoch 828/1000 
	 loss: 28.4862, MinusLogProbMetric: 28.4862, val_loss: 28.8768, val_MinusLogProbMetric: 28.8768

Epoch 828: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.4862 - MinusLogProbMetric: 28.4862 - val_loss: 28.8768 - val_MinusLogProbMetric: 28.8768 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 829/1000
2023-10-26 02:22:11.716 
Epoch 829/1000 
	 loss: 28.4856, MinusLogProbMetric: 28.4856, val_loss: 28.8603, val_MinusLogProbMetric: 28.8603

Epoch 829: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.4856 - MinusLogProbMetric: 28.4856 - val_loss: 28.8603 - val_MinusLogProbMetric: 28.8603 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 830/1000
2023-10-26 02:22:54.249 
Epoch 830/1000 
	 loss: 28.4981, MinusLogProbMetric: 28.4981, val_loss: 28.8944, val_MinusLogProbMetric: 28.8944

Epoch 830: val_loss did not improve from 28.80310
196/196 - 43s - loss: 28.4981 - MinusLogProbMetric: 28.4981 - val_loss: 28.8944 - val_MinusLogProbMetric: 28.8944 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 831/1000
2023-10-26 02:23:36.341 
Epoch 831/1000 
	 loss: 28.5127, MinusLogProbMetric: 28.5127, val_loss: 29.1761, val_MinusLogProbMetric: 29.1761

Epoch 831: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.5127 - MinusLogProbMetric: 28.5127 - val_loss: 29.1761 - val_MinusLogProbMetric: 29.1761 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 832/1000
2023-10-26 02:24:18.678 
Epoch 832/1000 
	 loss: 28.4932, MinusLogProbMetric: 28.4932, val_loss: 29.0140, val_MinusLogProbMetric: 29.0140

Epoch 832: val_loss did not improve from 28.80310
196/196 - 42s - loss: 28.4932 - MinusLogProbMetric: 28.4932 - val_loss: 29.0140 - val_MinusLogProbMetric: 29.0140 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 833/1000
2023-10-26 02:25:00.933 
Epoch 833/1000 
	 loss: 28.4768, MinusLogProbMetric: 28.4768, val_loss: 28.7857, val_MinusLogProbMetric: 28.7857

Epoch 833: val_loss improved from 28.80310 to 28.78567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.4768 - MinusLogProbMetric: 28.4768 - val_loss: 28.7857 - val_MinusLogProbMetric: 28.7857 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 834/1000
2023-10-26 02:25:44.391 
Epoch 834/1000 
	 loss: 28.4853, MinusLogProbMetric: 28.4853, val_loss: 28.9212, val_MinusLogProbMetric: 28.9212

Epoch 834: val_loss did not improve from 28.78567
196/196 - 43s - loss: 28.4853 - MinusLogProbMetric: 28.4853 - val_loss: 28.9212 - val_MinusLogProbMetric: 28.9212 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 835/1000
2023-10-26 02:26:26.429 
Epoch 835/1000 
	 loss: 28.4930, MinusLogProbMetric: 28.4930, val_loss: 28.9132, val_MinusLogProbMetric: 28.9132

Epoch 835: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4930 - MinusLogProbMetric: 28.4930 - val_loss: 28.9132 - val_MinusLogProbMetric: 28.9132 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 836/1000
2023-10-26 02:27:08.778 
Epoch 836/1000 
	 loss: 28.4812, MinusLogProbMetric: 28.4812, val_loss: 28.8704, val_MinusLogProbMetric: 28.8704

Epoch 836: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4812 - MinusLogProbMetric: 28.4812 - val_loss: 28.8704 - val_MinusLogProbMetric: 28.8704 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 837/1000
2023-10-26 02:27:50.684 
Epoch 837/1000 
	 loss: 28.5031, MinusLogProbMetric: 28.5031, val_loss: 28.8446, val_MinusLogProbMetric: 28.8446

Epoch 837: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5031 - MinusLogProbMetric: 28.5031 - val_loss: 28.8446 - val_MinusLogProbMetric: 28.8446 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 838/1000
2023-10-26 02:28:32.695 
Epoch 838/1000 
	 loss: 28.4830, MinusLogProbMetric: 28.4830, val_loss: 28.8689, val_MinusLogProbMetric: 28.8689

Epoch 838: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4830 - MinusLogProbMetric: 28.4830 - val_loss: 28.8689 - val_MinusLogProbMetric: 28.8689 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 839/1000
2023-10-26 02:29:15.132 
Epoch 839/1000 
	 loss: 28.4792, MinusLogProbMetric: 28.4792, val_loss: 28.8463, val_MinusLogProbMetric: 28.8463

Epoch 839: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4792 - MinusLogProbMetric: 28.4792 - val_loss: 28.8463 - val_MinusLogProbMetric: 28.8463 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 840/1000
2023-10-26 02:29:57.456 
Epoch 840/1000 
	 loss: 28.4824, MinusLogProbMetric: 28.4824, val_loss: 28.8392, val_MinusLogProbMetric: 28.8392

Epoch 840: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4824 - MinusLogProbMetric: 28.4824 - val_loss: 28.8392 - val_MinusLogProbMetric: 28.8392 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 841/1000
2023-10-26 02:30:39.899 
Epoch 841/1000 
	 loss: 28.4972, MinusLogProbMetric: 28.4972, val_loss: 28.8247, val_MinusLogProbMetric: 28.8247

Epoch 841: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4972 - MinusLogProbMetric: 28.4972 - val_loss: 28.8247 - val_MinusLogProbMetric: 28.8247 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 842/1000
2023-10-26 02:31:21.739 
Epoch 842/1000 
	 loss: 28.5021, MinusLogProbMetric: 28.5021, val_loss: 28.8917, val_MinusLogProbMetric: 28.8917

Epoch 842: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5021 - MinusLogProbMetric: 28.5021 - val_loss: 28.8917 - val_MinusLogProbMetric: 28.8917 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 843/1000
2023-10-26 02:32:04.068 
Epoch 843/1000 
	 loss: 28.4871, MinusLogProbMetric: 28.4871, val_loss: 28.9263, val_MinusLogProbMetric: 28.9263

Epoch 843: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4871 - MinusLogProbMetric: 28.4871 - val_loss: 28.9263 - val_MinusLogProbMetric: 28.9263 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 844/1000
2023-10-26 02:32:46.313 
Epoch 844/1000 
	 loss: 28.4888, MinusLogProbMetric: 28.4888, val_loss: 28.9258, val_MinusLogProbMetric: 28.9258

Epoch 844: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4888 - MinusLogProbMetric: 28.4888 - val_loss: 28.9258 - val_MinusLogProbMetric: 28.9258 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 845/1000
2023-10-26 02:33:28.593 
Epoch 845/1000 
	 loss: 28.4908, MinusLogProbMetric: 28.4908, val_loss: 28.8589, val_MinusLogProbMetric: 28.8589

Epoch 845: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4908 - MinusLogProbMetric: 28.4908 - val_loss: 28.8589 - val_MinusLogProbMetric: 28.8589 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 846/1000
2023-10-26 02:34:10.539 
Epoch 846/1000 
	 loss: 28.4744, MinusLogProbMetric: 28.4744, val_loss: 28.9583, val_MinusLogProbMetric: 28.9583

Epoch 846: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4744 - MinusLogProbMetric: 28.4744 - val_loss: 28.9583 - val_MinusLogProbMetric: 28.9583 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 847/1000
2023-10-26 02:34:52.771 
Epoch 847/1000 
	 loss: 28.4746, MinusLogProbMetric: 28.4746, val_loss: 28.8467, val_MinusLogProbMetric: 28.8467

Epoch 847: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4746 - MinusLogProbMetric: 28.4746 - val_loss: 28.8467 - val_MinusLogProbMetric: 28.8467 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 848/1000
2023-10-26 02:35:34.601 
Epoch 848/1000 
	 loss: 28.5046, MinusLogProbMetric: 28.5046, val_loss: 28.8866, val_MinusLogProbMetric: 28.8866

Epoch 848: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5046 - MinusLogProbMetric: 28.5046 - val_loss: 28.8866 - val_MinusLogProbMetric: 28.8866 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 849/1000
2023-10-26 02:36:16.604 
Epoch 849/1000 
	 loss: 28.4863, MinusLogProbMetric: 28.4863, val_loss: 28.8344, val_MinusLogProbMetric: 28.8344

Epoch 849: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4863 - MinusLogProbMetric: 28.4863 - val_loss: 28.8344 - val_MinusLogProbMetric: 28.8344 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 850/1000
2023-10-26 02:36:58.618 
Epoch 850/1000 
	 loss: 28.5134, MinusLogProbMetric: 28.5134, val_loss: 28.9756, val_MinusLogProbMetric: 28.9756

Epoch 850: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5134 - MinusLogProbMetric: 28.5134 - val_loss: 28.9756 - val_MinusLogProbMetric: 28.9756 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 851/1000
2023-10-26 02:37:40.632 
Epoch 851/1000 
	 loss: 28.5052, MinusLogProbMetric: 28.5052, val_loss: 28.9758, val_MinusLogProbMetric: 28.9758

Epoch 851: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5052 - MinusLogProbMetric: 28.5052 - val_loss: 28.9758 - val_MinusLogProbMetric: 28.9758 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 852/1000
2023-10-26 02:38:22.500 
Epoch 852/1000 
	 loss: 28.5029, MinusLogProbMetric: 28.5029, val_loss: 28.8555, val_MinusLogProbMetric: 28.8555

Epoch 852: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5029 - MinusLogProbMetric: 28.5029 - val_loss: 28.8555 - val_MinusLogProbMetric: 28.8555 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 853/1000
2023-10-26 02:39:04.469 
Epoch 853/1000 
	 loss: 28.5021, MinusLogProbMetric: 28.5021, val_loss: 28.9028, val_MinusLogProbMetric: 28.9028

Epoch 853: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5021 - MinusLogProbMetric: 28.5021 - val_loss: 28.9028 - val_MinusLogProbMetric: 28.9028 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 854/1000
2023-10-26 02:39:46.185 
Epoch 854/1000 
	 loss: 28.4797, MinusLogProbMetric: 28.4797, val_loss: 28.8160, val_MinusLogProbMetric: 28.8160

Epoch 854: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4797 - MinusLogProbMetric: 28.4797 - val_loss: 28.8160 - val_MinusLogProbMetric: 28.8160 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 855/1000
2023-10-26 02:40:28.283 
Epoch 855/1000 
	 loss: 28.4910, MinusLogProbMetric: 28.4910, val_loss: 28.8759, val_MinusLogProbMetric: 28.8759

Epoch 855: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4910 - MinusLogProbMetric: 28.4910 - val_loss: 28.8759 - val_MinusLogProbMetric: 28.8759 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 856/1000
2023-10-26 02:41:10.365 
Epoch 856/1000 
	 loss: 28.4750, MinusLogProbMetric: 28.4750, val_loss: 28.9017, val_MinusLogProbMetric: 28.9017

Epoch 856: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4750 - MinusLogProbMetric: 28.4750 - val_loss: 28.9017 - val_MinusLogProbMetric: 28.9017 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 857/1000
2023-10-26 02:41:52.520 
Epoch 857/1000 
	 loss: 28.4766, MinusLogProbMetric: 28.4766, val_loss: 28.9500, val_MinusLogProbMetric: 28.9500

Epoch 857: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4766 - MinusLogProbMetric: 28.4766 - val_loss: 28.9500 - val_MinusLogProbMetric: 28.9500 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 858/1000
2023-10-26 02:42:34.398 
Epoch 858/1000 
	 loss: 28.5113, MinusLogProbMetric: 28.5113, val_loss: 28.8731, val_MinusLogProbMetric: 28.8731

Epoch 858: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5113 - MinusLogProbMetric: 28.5113 - val_loss: 28.8731 - val_MinusLogProbMetric: 28.8731 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 859/1000
2023-10-26 02:43:16.431 
Epoch 859/1000 
	 loss: 28.4788, MinusLogProbMetric: 28.4788, val_loss: 28.8722, val_MinusLogProbMetric: 28.8722

Epoch 859: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4788 - MinusLogProbMetric: 28.4788 - val_loss: 28.8722 - val_MinusLogProbMetric: 28.8722 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 860/1000
2023-10-26 02:43:58.701 
Epoch 860/1000 
	 loss: 28.4786, MinusLogProbMetric: 28.4786, val_loss: 28.7866, val_MinusLogProbMetric: 28.7866

Epoch 860: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4786 - MinusLogProbMetric: 28.4786 - val_loss: 28.7866 - val_MinusLogProbMetric: 28.7866 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 861/1000
2023-10-26 02:44:40.190 
Epoch 861/1000 
	 loss: 28.4779, MinusLogProbMetric: 28.4779, val_loss: 28.8393, val_MinusLogProbMetric: 28.8393

Epoch 861: val_loss did not improve from 28.78567
196/196 - 41s - loss: 28.4779 - MinusLogProbMetric: 28.4779 - val_loss: 28.8393 - val_MinusLogProbMetric: 28.8393 - lr: 8.3333e-05 - 41s/epoch - 212ms/step
Epoch 862/1000
2023-10-26 02:45:22.246 
Epoch 862/1000 
	 loss: 28.4799, MinusLogProbMetric: 28.4799, val_loss: 28.8600, val_MinusLogProbMetric: 28.8600

Epoch 862: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4799 - MinusLogProbMetric: 28.4799 - val_loss: 28.8600 - val_MinusLogProbMetric: 28.8600 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 863/1000
2023-10-26 02:46:04.070 
Epoch 863/1000 
	 loss: 28.4926, MinusLogProbMetric: 28.4926, val_loss: 28.8491, val_MinusLogProbMetric: 28.8491

Epoch 863: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4926 - MinusLogProbMetric: 28.4926 - val_loss: 28.8491 - val_MinusLogProbMetric: 28.8491 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 864/1000
2023-10-26 02:46:46.276 
Epoch 864/1000 
	 loss: 28.4720, MinusLogProbMetric: 28.4720, val_loss: 28.9855, val_MinusLogProbMetric: 28.9855

Epoch 864: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4720 - MinusLogProbMetric: 28.4720 - val_loss: 28.9855 - val_MinusLogProbMetric: 28.9855 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 865/1000
2023-10-26 02:47:28.395 
Epoch 865/1000 
	 loss: 28.4918, MinusLogProbMetric: 28.4918, val_loss: 28.8430, val_MinusLogProbMetric: 28.8430

Epoch 865: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4918 - MinusLogProbMetric: 28.4918 - val_loss: 28.8430 - val_MinusLogProbMetric: 28.8430 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 866/1000
2023-10-26 02:48:10.502 
Epoch 866/1000 
	 loss: 28.4818, MinusLogProbMetric: 28.4818, val_loss: 28.8182, val_MinusLogProbMetric: 28.8182

Epoch 866: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4818 - MinusLogProbMetric: 28.4818 - val_loss: 28.8182 - val_MinusLogProbMetric: 28.8182 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 867/1000
2023-10-26 02:48:52.770 
Epoch 867/1000 
	 loss: 28.4796, MinusLogProbMetric: 28.4796, val_loss: 28.8929, val_MinusLogProbMetric: 28.8929

Epoch 867: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4796 - MinusLogProbMetric: 28.4796 - val_loss: 28.8929 - val_MinusLogProbMetric: 28.8929 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 868/1000
2023-10-26 02:49:35.038 
Epoch 868/1000 
	 loss: 28.5006, MinusLogProbMetric: 28.5006, val_loss: 28.9131, val_MinusLogProbMetric: 28.9131

Epoch 868: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5006 - MinusLogProbMetric: 28.5006 - val_loss: 28.9131 - val_MinusLogProbMetric: 28.9131 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 869/1000
2023-10-26 02:50:17.403 
Epoch 869/1000 
	 loss: 28.4871, MinusLogProbMetric: 28.4871, val_loss: 28.8500, val_MinusLogProbMetric: 28.8500

Epoch 869: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.4871 - MinusLogProbMetric: 28.4871 - val_loss: 28.8500 - val_MinusLogProbMetric: 28.8500 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 870/1000
2023-10-26 02:50:59.967 
Epoch 870/1000 
	 loss: 28.4876, MinusLogProbMetric: 28.4876, val_loss: 28.9347, val_MinusLogProbMetric: 28.9347

Epoch 870: val_loss did not improve from 28.78567
196/196 - 43s - loss: 28.4876 - MinusLogProbMetric: 28.4876 - val_loss: 28.9347 - val_MinusLogProbMetric: 28.9347 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 871/1000
2023-10-26 02:51:42.332 
Epoch 871/1000 
	 loss: 28.5005, MinusLogProbMetric: 28.5005, val_loss: 28.8734, val_MinusLogProbMetric: 28.8734

Epoch 871: val_loss did not improve from 28.78567
196/196 - 42s - loss: 28.5005 - MinusLogProbMetric: 28.5005 - val_loss: 28.8734 - val_MinusLogProbMetric: 28.8734 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 872/1000
2023-10-26 02:52:24.304 
Epoch 872/1000 
	 loss: 28.4923, MinusLogProbMetric: 28.4923, val_loss: 28.7622, val_MinusLogProbMetric: 28.7622

Epoch 872: val_loss improved from 28.78567 to 28.76219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.4923 - MinusLogProbMetric: 28.4923 - val_loss: 28.7622 - val_MinusLogProbMetric: 28.7622 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 873/1000
2023-10-26 02:53:07.937 
Epoch 873/1000 
	 loss: 28.4563, MinusLogProbMetric: 28.4563, val_loss: 28.8389, val_MinusLogProbMetric: 28.8389

Epoch 873: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4563 - MinusLogProbMetric: 28.4563 - val_loss: 28.8389 - val_MinusLogProbMetric: 28.8389 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 874/1000
2023-10-26 02:53:50.614 
Epoch 874/1000 
	 loss: 28.4765, MinusLogProbMetric: 28.4765, val_loss: 28.9176, val_MinusLogProbMetric: 28.9176

Epoch 874: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4765 - MinusLogProbMetric: 28.4765 - val_loss: 28.9176 - val_MinusLogProbMetric: 28.9176 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 875/1000
2023-10-26 02:54:33.085 
Epoch 875/1000 
	 loss: 28.5021, MinusLogProbMetric: 28.5021, val_loss: 28.9276, val_MinusLogProbMetric: 28.9276

Epoch 875: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.5021 - MinusLogProbMetric: 28.5021 - val_loss: 28.9276 - val_MinusLogProbMetric: 28.9276 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 876/1000
2023-10-26 02:55:15.635 
Epoch 876/1000 
	 loss: 28.4810, MinusLogProbMetric: 28.4810, val_loss: 28.8467, val_MinusLogProbMetric: 28.8467

Epoch 876: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4810 - MinusLogProbMetric: 28.4810 - val_loss: 28.8467 - val_MinusLogProbMetric: 28.8467 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 877/1000
2023-10-26 02:55:58.188 
Epoch 877/1000 
	 loss: 28.4694, MinusLogProbMetric: 28.4694, val_loss: 28.8819, val_MinusLogProbMetric: 28.8819

Epoch 877: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4694 - MinusLogProbMetric: 28.4694 - val_loss: 28.8819 - val_MinusLogProbMetric: 28.8819 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 878/1000
2023-10-26 02:56:41.116 
Epoch 878/1000 
	 loss: 28.4742, MinusLogProbMetric: 28.4742, val_loss: 28.8443, val_MinusLogProbMetric: 28.8443

Epoch 878: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4742 - MinusLogProbMetric: 28.4742 - val_loss: 28.8443 - val_MinusLogProbMetric: 28.8443 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 879/1000
2023-10-26 02:57:23.408 
Epoch 879/1000 
	 loss: 28.4712, MinusLogProbMetric: 28.4712, val_loss: 28.8134, val_MinusLogProbMetric: 28.8134

Epoch 879: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4712 - MinusLogProbMetric: 28.4712 - val_loss: 28.8134 - val_MinusLogProbMetric: 28.8134 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 880/1000
2023-10-26 02:58:06.096 
Epoch 880/1000 
	 loss: 28.4670, MinusLogProbMetric: 28.4670, val_loss: 28.8152, val_MinusLogProbMetric: 28.8152

Epoch 880: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4670 - MinusLogProbMetric: 28.4670 - val_loss: 28.8152 - val_MinusLogProbMetric: 28.8152 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 881/1000
2023-10-26 02:58:48.427 
Epoch 881/1000 
	 loss: 28.4669, MinusLogProbMetric: 28.4669, val_loss: 28.8357, val_MinusLogProbMetric: 28.8357

Epoch 881: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4669 - MinusLogProbMetric: 28.4669 - val_loss: 28.8357 - val_MinusLogProbMetric: 28.8357 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 882/1000
2023-10-26 02:59:31.030 
Epoch 882/1000 
	 loss: 28.4792, MinusLogProbMetric: 28.4792, val_loss: 28.8490, val_MinusLogProbMetric: 28.8490

Epoch 882: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4792 - MinusLogProbMetric: 28.4792 - val_loss: 28.8490 - val_MinusLogProbMetric: 28.8490 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 883/1000
2023-10-26 03:00:13.996 
Epoch 883/1000 
	 loss: 28.4577, MinusLogProbMetric: 28.4577, val_loss: 28.8247, val_MinusLogProbMetric: 28.8247

Epoch 883: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4577 - MinusLogProbMetric: 28.4577 - val_loss: 28.8247 - val_MinusLogProbMetric: 28.8247 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 884/1000
2023-10-26 03:00:56.404 
Epoch 884/1000 
	 loss: 28.4592, MinusLogProbMetric: 28.4592, val_loss: 28.8827, val_MinusLogProbMetric: 28.8827

Epoch 884: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4592 - MinusLogProbMetric: 28.4592 - val_loss: 28.8827 - val_MinusLogProbMetric: 28.8827 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 885/1000
2023-10-26 03:01:38.720 
Epoch 885/1000 
	 loss: 28.4796, MinusLogProbMetric: 28.4796, val_loss: 28.9489, val_MinusLogProbMetric: 28.9489

Epoch 885: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4796 - MinusLogProbMetric: 28.4796 - val_loss: 28.9489 - val_MinusLogProbMetric: 28.9489 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 886/1000
2023-10-26 03:02:20.829 
Epoch 886/1000 
	 loss: 28.4648, MinusLogProbMetric: 28.4648, val_loss: 28.8030, val_MinusLogProbMetric: 28.8030

Epoch 886: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4648 - MinusLogProbMetric: 28.4648 - val_loss: 28.8030 - val_MinusLogProbMetric: 28.8030 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 887/1000
2023-10-26 03:03:03.335 
Epoch 887/1000 
	 loss: 28.4813, MinusLogProbMetric: 28.4813, val_loss: 28.8068, val_MinusLogProbMetric: 28.8068

Epoch 887: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4813 - MinusLogProbMetric: 28.4813 - val_loss: 28.8068 - val_MinusLogProbMetric: 28.8068 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 888/1000
2023-10-26 03:03:45.590 
Epoch 888/1000 
	 loss: 28.4847, MinusLogProbMetric: 28.4847, val_loss: 28.8779, val_MinusLogProbMetric: 28.8779

Epoch 888: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4847 - MinusLogProbMetric: 28.4847 - val_loss: 28.8779 - val_MinusLogProbMetric: 28.8779 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 889/1000
2023-10-26 03:04:28.021 
Epoch 889/1000 
	 loss: 28.4707, MinusLogProbMetric: 28.4707, val_loss: 28.9081, val_MinusLogProbMetric: 28.9081

Epoch 889: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4707 - MinusLogProbMetric: 28.4707 - val_loss: 28.9081 - val_MinusLogProbMetric: 28.9081 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 890/1000
2023-10-26 03:05:10.347 
Epoch 890/1000 
	 loss: 28.4583, MinusLogProbMetric: 28.4583, val_loss: 28.9991, val_MinusLogProbMetric: 28.9991

Epoch 890: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4583 - MinusLogProbMetric: 28.4583 - val_loss: 28.9991 - val_MinusLogProbMetric: 28.9991 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 891/1000
2023-10-26 03:05:52.532 
Epoch 891/1000 
	 loss: 28.4829, MinusLogProbMetric: 28.4829, val_loss: 28.8145, val_MinusLogProbMetric: 28.8145

Epoch 891: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4829 - MinusLogProbMetric: 28.4829 - val_loss: 28.8145 - val_MinusLogProbMetric: 28.8145 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 892/1000
2023-10-26 03:06:35.197 
Epoch 892/1000 
	 loss: 28.4507, MinusLogProbMetric: 28.4507, val_loss: 28.7883, val_MinusLogProbMetric: 28.7883

Epoch 892: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4507 - MinusLogProbMetric: 28.4507 - val_loss: 28.7883 - val_MinusLogProbMetric: 28.7883 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 893/1000
2023-10-26 03:07:17.802 
Epoch 893/1000 
	 loss: 28.4718, MinusLogProbMetric: 28.4718, val_loss: 28.8565, val_MinusLogProbMetric: 28.8565

Epoch 893: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4718 - MinusLogProbMetric: 28.4718 - val_loss: 28.8565 - val_MinusLogProbMetric: 28.8565 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 894/1000
2023-10-26 03:07:59.695 
Epoch 894/1000 
	 loss: 28.4578, MinusLogProbMetric: 28.4578, val_loss: 28.7966, val_MinusLogProbMetric: 28.7966

Epoch 894: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4578 - MinusLogProbMetric: 28.4578 - val_loss: 28.7966 - val_MinusLogProbMetric: 28.7966 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 895/1000
2023-10-26 03:08:42.000 
Epoch 895/1000 
	 loss: 28.4708, MinusLogProbMetric: 28.4708, val_loss: 28.9153, val_MinusLogProbMetric: 28.9153

Epoch 895: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4708 - MinusLogProbMetric: 28.4708 - val_loss: 28.9153 - val_MinusLogProbMetric: 28.9153 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 896/1000
2023-10-26 03:09:24.893 
Epoch 896/1000 
	 loss: 28.4644, MinusLogProbMetric: 28.4644, val_loss: 28.8892, val_MinusLogProbMetric: 28.8892

Epoch 896: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4644 - MinusLogProbMetric: 28.4644 - val_loss: 28.8892 - val_MinusLogProbMetric: 28.8892 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 897/1000
2023-10-26 03:10:05.818 
Epoch 897/1000 
	 loss: 28.4576, MinusLogProbMetric: 28.4576, val_loss: 28.8507, val_MinusLogProbMetric: 28.8507

Epoch 897: val_loss did not improve from 28.76219
196/196 - 41s - loss: 28.4576 - MinusLogProbMetric: 28.4576 - val_loss: 28.8507 - val_MinusLogProbMetric: 28.8507 - lr: 8.3333e-05 - 41s/epoch - 209ms/step
Epoch 898/1000
2023-10-26 03:10:44.010 
Epoch 898/1000 
	 loss: 28.4568, MinusLogProbMetric: 28.4568, val_loss: 28.8155, val_MinusLogProbMetric: 28.8155

Epoch 898: val_loss did not improve from 28.76219
196/196 - 38s - loss: 28.4568 - MinusLogProbMetric: 28.4568 - val_loss: 28.8155 - val_MinusLogProbMetric: 28.8155 - lr: 8.3333e-05 - 38s/epoch - 195ms/step
Epoch 899/1000
2023-10-26 03:11:23.987 
Epoch 899/1000 
	 loss: 28.4572, MinusLogProbMetric: 28.4572, val_loss: 28.9756, val_MinusLogProbMetric: 28.9756

Epoch 899: val_loss did not improve from 28.76219
196/196 - 40s - loss: 28.4572 - MinusLogProbMetric: 28.4572 - val_loss: 28.9756 - val_MinusLogProbMetric: 28.9756 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 900/1000
2023-10-26 03:12:05.624 
Epoch 900/1000 
	 loss: 28.4628, MinusLogProbMetric: 28.4628, val_loss: 28.8503, val_MinusLogProbMetric: 28.8503

Epoch 900: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4628 - MinusLogProbMetric: 28.4628 - val_loss: 28.8503 - val_MinusLogProbMetric: 28.8503 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 901/1000
2023-10-26 03:12:48.252 
Epoch 901/1000 
	 loss: 28.4619, MinusLogProbMetric: 28.4619, val_loss: 28.9790, val_MinusLogProbMetric: 28.9790

Epoch 901: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4619 - MinusLogProbMetric: 28.4619 - val_loss: 28.9790 - val_MinusLogProbMetric: 28.9790 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 902/1000
2023-10-26 03:13:30.788 
Epoch 902/1000 
	 loss: 28.4622, MinusLogProbMetric: 28.4622, val_loss: 29.6874, val_MinusLogProbMetric: 29.6874

Epoch 902: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4622 - MinusLogProbMetric: 28.4622 - val_loss: 29.6874 - val_MinusLogProbMetric: 29.6874 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 903/1000
2023-10-26 03:14:13.032 
Epoch 903/1000 
	 loss: 28.5145, MinusLogProbMetric: 28.5145, val_loss: 28.8664, val_MinusLogProbMetric: 28.8664

Epoch 903: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.5145 - MinusLogProbMetric: 28.5145 - val_loss: 28.8664 - val_MinusLogProbMetric: 28.8664 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 904/1000
2023-10-26 03:14:55.442 
Epoch 904/1000 
	 loss: 28.4571, MinusLogProbMetric: 28.4571, val_loss: 28.9255, val_MinusLogProbMetric: 28.9255

Epoch 904: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4571 - MinusLogProbMetric: 28.4571 - val_loss: 28.9255 - val_MinusLogProbMetric: 28.9255 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 905/1000
2023-10-26 03:15:37.630 
Epoch 905/1000 
	 loss: 28.4775, MinusLogProbMetric: 28.4775, val_loss: 28.8001, val_MinusLogProbMetric: 28.8001

Epoch 905: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4775 - MinusLogProbMetric: 28.4775 - val_loss: 28.8001 - val_MinusLogProbMetric: 28.8001 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 906/1000
2023-10-26 03:16:19.915 
Epoch 906/1000 
	 loss: 28.4797, MinusLogProbMetric: 28.4797, val_loss: 28.8991, val_MinusLogProbMetric: 28.8991

Epoch 906: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4797 - MinusLogProbMetric: 28.4797 - val_loss: 28.8991 - val_MinusLogProbMetric: 28.8991 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 907/1000
2023-10-26 03:17:01.796 
Epoch 907/1000 
	 loss: 28.4814, MinusLogProbMetric: 28.4814, val_loss: 28.8742, val_MinusLogProbMetric: 28.8742

Epoch 907: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4814 - MinusLogProbMetric: 28.4814 - val_loss: 28.8742 - val_MinusLogProbMetric: 28.8742 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 908/1000
2023-10-26 03:17:43.914 
Epoch 908/1000 
	 loss: 28.4559, MinusLogProbMetric: 28.4559, val_loss: 28.8106, val_MinusLogProbMetric: 28.8106

Epoch 908: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4559 - MinusLogProbMetric: 28.4559 - val_loss: 28.8106 - val_MinusLogProbMetric: 28.8106 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 909/1000
2023-10-26 03:18:26.235 
Epoch 909/1000 
	 loss: 28.4541, MinusLogProbMetric: 28.4541, val_loss: 28.8539, val_MinusLogProbMetric: 28.8539

Epoch 909: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4541 - MinusLogProbMetric: 28.4541 - val_loss: 28.8539 - val_MinusLogProbMetric: 28.8539 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 910/1000
2023-10-26 03:19:08.689 
Epoch 910/1000 
	 loss: 28.4770, MinusLogProbMetric: 28.4770, val_loss: 28.8414, val_MinusLogProbMetric: 28.8414

Epoch 910: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4770 - MinusLogProbMetric: 28.4770 - val_loss: 28.8414 - val_MinusLogProbMetric: 28.8414 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 911/1000
2023-10-26 03:19:50.805 
Epoch 911/1000 
	 loss: 28.4597, MinusLogProbMetric: 28.4597, val_loss: 28.7948, val_MinusLogProbMetric: 28.7948

Epoch 911: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4597 - MinusLogProbMetric: 28.4597 - val_loss: 28.7948 - val_MinusLogProbMetric: 28.7948 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 912/1000
2023-10-26 03:20:32.895 
Epoch 912/1000 
	 loss: 28.4636, MinusLogProbMetric: 28.4636, val_loss: 28.8132, val_MinusLogProbMetric: 28.8132

Epoch 912: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4636 - MinusLogProbMetric: 28.4636 - val_loss: 28.8132 - val_MinusLogProbMetric: 28.8132 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 913/1000
2023-10-26 03:21:15.325 
Epoch 913/1000 
	 loss: 28.4367, MinusLogProbMetric: 28.4367, val_loss: 28.8287, val_MinusLogProbMetric: 28.8287

Epoch 913: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4367 - MinusLogProbMetric: 28.4367 - val_loss: 28.8287 - val_MinusLogProbMetric: 28.8287 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 914/1000
2023-10-26 03:21:57.484 
Epoch 914/1000 
	 loss: 28.4566, MinusLogProbMetric: 28.4566, val_loss: 28.8647, val_MinusLogProbMetric: 28.8647

Epoch 914: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4566 - MinusLogProbMetric: 28.4566 - val_loss: 28.8647 - val_MinusLogProbMetric: 28.8647 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 915/1000
2023-10-26 03:22:39.623 
Epoch 915/1000 
	 loss: 28.4483, MinusLogProbMetric: 28.4483, val_loss: 28.7697, val_MinusLogProbMetric: 28.7697

Epoch 915: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4483 - MinusLogProbMetric: 28.4483 - val_loss: 28.7697 - val_MinusLogProbMetric: 28.7697 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 916/1000
2023-10-26 03:23:21.550 
Epoch 916/1000 
	 loss: 28.4603, MinusLogProbMetric: 28.4603, val_loss: 28.7812, val_MinusLogProbMetric: 28.7812

Epoch 916: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4603 - MinusLogProbMetric: 28.4603 - val_loss: 28.7812 - val_MinusLogProbMetric: 28.7812 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 917/1000
2023-10-26 03:24:04.098 
Epoch 917/1000 
	 loss: 28.4378, MinusLogProbMetric: 28.4378, val_loss: 28.8690, val_MinusLogProbMetric: 28.8690

Epoch 917: val_loss did not improve from 28.76219
196/196 - 43s - loss: 28.4378 - MinusLogProbMetric: 28.4378 - val_loss: 28.8690 - val_MinusLogProbMetric: 28.8690 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 918/1000
2023-10-26 03:24:46.316 
Epoch 918/1000 
	 loss: 28.4640, MinusLogProbMetric: 28.4640, val_loss: 28.8299, val_MinusLogProbMetric: 28.8299

Epoch 918: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4640 - MinusLogProbMetric: 28.4640 - val_loss: 28.8299 - val_MinusLogProbMetric: 28.8299 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 919/1000
2023-10-26 03:25:28.522 
Epoch 919/1000 
	 loss: 28.4493, MinusLogProbMetric: 28.4493, val_loss: 28.8292, val_MinusLogProbMetric: 28.8292

Epoch 919: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4493 - MinusLogProbMetric: 28.4493 - val_loss: 28.8292 - val_MinusLogProbMetric: 28.8292 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 920/1000
2023-10-26 03:26:10.895 
Epoch 920/1000 
	 loss: 28.4493, MinusLogProbMetric: 28.4493, val_loss: 28.7990, val_MinusLogProbMetric: 28.7990

Epoch 920: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4493 - MinusLogProbMetric: 28.4493 - val_loss: 28.7990 - val_MinusLogProbMetric: 28.7990 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 921/1000
2023-10-26 03:26:53.086 
Epoch 921/1000 
	 loss: 28.4507, MinusLogProbMetric: 28.4507, val_loss: 28.7664, val_MinusLogProbMetric: 28.7664

Epoch 921: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4507 - MinusLogProbMetric: 28.4507 - val_loss: 28.7664 - val_MinusLogProbMetric: 28.7664 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 922/1000
2023-10-26 03:27:35.110 
Epoch 922/1000 
	 loss: 28.4462, MinusLogProbMetric: 28.4462, val_loss: 28.8958, val_MinusLogProbMetric: 28.8958

Epoch 922: val_loss did not improve from 28.76219
196/196 - 42s - loss: 28.4462 - MinusLogProbMetric: 28.4462 - val_loss: 28.8958 - val_MinusLogProbMetric: 28.8958 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 923/1000
2023-10-26 03:28:17.800 
Epoch 923/1000 
	 loss: 28.3724, MinusLogProbMetric: 28.3724, val_loss: 28.7493, val_MinusLogProbMetric: 28.7493

Epoch 923: val_loss improved from 28.76219 to 28.74934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 28.3724 - MinusLogProbMetric: 28.3724 - val_loss: 28.7493 - val_MinusLogProbMetric: 28.7493 - lr: 4.1667e-05 - 44s/epoch - 223ms/step
Epoch 924/1000
2023-10-26 03:29:01.197 
Epoch 924/1000 
	 loss: 28.3692, MinusLogProbMetric: 28.3692, val_loss: 28.7607, val_MinusLogProbMetric: 28.7607

Epoch 924: val_loss did not improve from 28.74934
196/196 - 42s - loss: 28.3692 - MinusLogProbMetric: 28.3692 - val_loss: 28.7607 - val_MinusLogProbMetric: 28.7607 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 925/1000
2023-10-26 03:29:43.838 
Epoch 925/1000 
	 loss: 28.3710, MinusLogProbMetric: 28.3710, val_loss: 28.7777, val_MinusLogProbMetric: 28.7777

Epoch 925: val_loss did not improve from 28.74934
196/196 - 43s - loss: 28.3710 - MinusLogProbMetric: 28.3710 - val_loss: 28.7777 - val_MinusLogProbMetric: 28.7777 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 926/1000
2023-10-26 03:30:26.387 
Epoch 926/1000 
	 loss: 28.3701, MinusLogProbMetric: 28.3701, val_loss: 28.7623, val_MinusLogProbMetric: 28.7623

Epoch 926: val_loss did not improve from 28.74934
196/196 - 43s - loss: 28.3701 - MinusLogProbMetric: 28.3701 - val_loss: 28.7623 - val_MinusLogProbMetric: 28.7623 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 927/1000
2023-10-26 03:31:09.212 
Epoch 927/1000 
	 loss: 28.3726, MinusLogProbMetric: 28.3726, val_loss: 28.7459, val_MinusLogProbMetric: 28.7459

Epoch 927: val_loss improved from 28.74934 to 28.74593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 28.3726 - MinusLogProbMetric: 28.3726 - val_loss: 28.7459 - val_MinusLogProbMetric: 28.7459 - lr: 4.1667e-05 - 44s/epoch - 222ms/step
Epoch 928/1000
2023-10-26 03:31:52.393 
Epoch 928/1000 
	 loss: 28.3835, MinusLogProbMetric: 28.3835, val_loss: 28.7660, val_MinusLogProbMetric: 28.7660

Epoch 928: val_loss did not improve from 28.74593
196/196 - 42s - loss: 28.3835 - MinusLogProbMetric: 28.3835 - val_loss: 28.7660 - val_MinusLogProbMetric: 28.7660 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 929/1000
2023-10-26 03:32:34.658 
Epoch 929/1000 
	 loss: 28.3723, MinusLogProbMetric: 28.3723, val_loss: 28.8236, val_MinusLogProbMetric: 28.8236

Epoch 929: val_loss did not improve from 28.74593
196/196 - 42s - loss: 28.3723 - MinusLogProbMetric: 28.3723 - val_loss: 28.8236 - val_MinusLogProbMetric: 28.8236 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 930/1000
2023-10-26 03:33:17.072 
Epoch 930/1000 
	 loss: 28.3738, MinusLogProbMetric: 28.3738, val_loss: 28.7536, val_MinusLogProbMetric: 28.7536

Epoch 930: val_loss did not improve from 28.74593
196/196 - 42s - loss: 28.3738 - MinusLogProbMetric: 28.3738 - val_loss: 28.7536 - val_MinusLogProbMetric: 28.7536 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 931/1000
2023-10-26 03:33:59.469 
Epoch 931/1000 
	 loss: 28.3682, MinusLogProbMetric: 28.3682, val_loss: 28.7418, val_MinusLogProbMetric: 28.7418

Epoch 931: val_loss improved from 28.74593 to 28.74177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.3682 - MinusLogProbMetric: 28.3682 - val_loss: 28.7418 - val_MinusLogProbMetric: 28.7418 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 932/1000
2023-10-26 03:34:42.570 
Epoch 932/1000 
	 loss: 28.3694, MinusLogProbMetric: 28.3694, val_loss: 28.7261, val_MinusLogProbMetric: 28.7261

Epoch 932: val_loss improved from 28.74177 to 28.72614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.3694 - MinusLogProbMetric: 28.3694 - val_loss: 28.7261 - val_MinusLogProbMetric: 28.7261 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 933/1000
2023-10-26 03:35:25.950 
Epoch 933/1000 
	 loss: 28.3764, MinusLogProbMetric: 28.3764, val_loss: 28.7417, val_MinusLogProbMetric: 28.7417

Epoch 933: val_loss did not improve from 28.72614
196/196 - 43s - loss: 28.3764 - MinusLogProbMetric: 28.3764 - val_loss: 28.7417 - val_MinusLogProbMetric: 28.7417 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 934/1000
2023-10-26 03:36:08.315 
Epoch 934/1000 
	 loss: 28.3730, MinusLogProbMetric: 28.3730, val_loss: 28.7487, val_MinusLogProbMetric: 28.7487

Epoch 934: val_loss did not improve from 28.72614
196/196 - 42s - loss: 28.3730 - MinusLogProbMetric: 28.3730 - val_loss: 28.7487 - val_MinusLogProbMetric: 28.7487 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 935/1000
2023-10-26 03:36:50.588 
Epoch 935/1000 
	 loss: 28.3720, MinusLogProbMetric: 28.3720, val_loss: 28.7629, val_MinusLogProbMetric: 28.7629

Epoch 935: val_loss did not improve from 28.72614
196/196 - 42s - loss: 28.3720 - MinusLogProbMetric: 28.3720 - val_loss: 28.7629 - val_MinusLogProbMetric: 28.7629 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 936/1000
2023-10-26 03:37:33.125 
Epoch 936/1000 
	 loss: 28.3734, MinusLogProbMetric: 28.3734, val_loss: 28.7387, val_MinusLogProbMetric: 28.7387

Epoch 936: val_loss did not improve from 28.72614
196/196 - 43s - loss: 28.3734 - MinusLogProbMetric: 28.3734 - val_loss: 28.7387 - val_MinusLogProbMetric: 28.7387 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 937/1000
2023-10-26 03:38:15.667 
Epoch 937/1000 
	 loss: 28.3784, MinusLogProbMetric: 28.3784, val_loss: 28.7826, val_MinusLogProbMetric: 28.7826

Epoch 937: val_loss did not improve from 28.72614
196/196 - 43s - loss: 28.3784 - MinusLogProbMetric: 28.3784 - val_loss: 28.7826 - val_MinusLogProbMetric: 28.7826 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 938/1000
2023-10-26 03:38:58.464 
Epoch 938/1000 
	 loss: 28.3782, MinusLogProbMetric: 28.3782, val_loss: 28.7239, val_MinusLogProbMetric: 28.7239

Epoch 938: val_loss improved from 28.72614 to 28.72390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 28.3782 - MinusLogProbMetric: 28.3782 - val_loss: 28.7239 - val_MinusLogProbMetric: 28.7239 - lr: 4.1667e-05 - 44s/epoch - 224ms/step
Epoch 939/1000
2023-10-26 03:39:42.245 
Epoch 939/1000 
	 loss: 28.3701, MinusLogProbMetric: 28.3701, val_loss: 28.8019, val_MinusLogProbMetric: 28.8019

Epoch 939: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3701 - MinusLogProbMetric: 28.3701 - val_loss: 28.8019 - val_MinusLogProbMetric: 28.8019 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 940/1000
2023-10-26 03:40:25.187 
Epoch 940/1000 
	 loss: 28.3642, MinusLogProbMetric: 28.3642, val_loss: 28.7831, val_MinusLogProbMetric: 28.7831

Epoch 940: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3642 - MinusLogProbMetric: 28.3642 - val_loss: 28.7831 - val_MinusLogProbMetric: 28.7831 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 941/1000
2023-10-26 03:41:07.768 
Epoch 941/1000 
	 loss: 28.3675, MinusLogProbMetric: 28.3675, val_loss: 28.7467, val_MinusLogProbMetric: 28.7467

Epoch 941: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3675 - MinusLogProbMetric: 28.3675 - val_loss: 28.7467 - val_MinusLogProbMetric: 28.7467 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 942/1000
2023-10-26 03:41:50.356 
Epoch 942/1000 
	 loss: 28.3703, MinusLogProbMetric: 28.3703, val_loss: 28.7590, val_MinusLogProbMetric: 28.7590

Epoch 942: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3703 - MinusLogProbMetric: 28.3703 - val_loss: 28.7590 - val_MinusLogProbMetric: 28.7590 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 943/1000
2023-10-26 03:42:32.496 
Epoch 943/1000 
	 loss: 28.3651, MinusLogProbMetric: 28.3651, val_loss: 28.7422, val_MinusLogProbMetric: 28.7422

Epoch 943: val_loss did not improve from 28.72390
196/196 - 42s - loss: 28.3651 - MinusLogProbMetric: 28.3651 - val_loss: 28.7422 - val_MinusLogProbMetric: 28.7422 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 944/1000
2023-10-26 03:43:14.672 
Epoch 944/1000 
	 loss: 28.3687, MinusLogProbMetric: 28.3687, val_loss: 28.7908, val_MinusLogProbMetric: 28.7908

Epoch 944: val_loss did not improve from 28.72390
196/196 - 42s - loss: 28.3687 - MinusLogProbMetric: 28.3687 - val_loss: 28.7908 - val_MinusLogProbMetric: 28.7908 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 945/1000
2023-10-26 03:43:57.222 
Epoch 945/1000 
	 loss: 28.3705, MinusLogProbMetric: 28.3705, val_loss: 28.7947, val_MinusLogProbMetric: 28.7947

Epoch 945: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3705 - MinusLogProbMetric: 28.3705 - val_loss: 28.7947 - val_MinusLogProbMetric: 28.7947 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 946/1000
2023-10-26 03:44:39.596 
Epoch 946/1000 
	 loss: 28.3724, MinusLogProbMetric: 28.3724, val_loss: 28.7298, val_MinusLogProbMetric: 28.7298

Epoch 946: val_loss did not improve from 28.72390
196/196 - 42s - loss: 28.3724 - MinusLogProbMetric: 28.3724 - val_loss: 28.7298 - val_MinusLogProbMetric: 28.7298 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 947/1000
2023-10-26 03:45:22.371 
Epoch 947/1000 
	 loss: 28.3697, MinusLogProbMetric: 28.3697, val_loss: 28.7317, val_MinusLogProbMetric: 28.7317

Epoch 947: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3697 - MinusLogProbMetric: 28.3697 - val_loss: 28.7317 - val_MinusLogProbMetric: 28.7317 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 948/1000
2023-10-26 03:46:05.050 
Epoch 948/1000 
	 loss: 28.3740, MinusLogProbMetric: 28.3740, val_loss: 28.7395, val_MinusLogProbMetric: 28.7395

Epoch 948: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3740 - MinusLogProbMetric: 28.3740 - val_loss: 28.7395 - val_MinusLogProbMetric: 28.7395 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 949/1000
2023-10-26 03:46:47.761 
Epoch 949/1000 
	 loss: 28.3743, MinusLogProbMetric: 28.3743, val_loss: 28.8070, val_MinusLogProbMetric: 28.8070

Epoch 949: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3743 - MinusLogProbMetric: 28.3743 - val_loss: 28.8070 - val_MinusLogProbMetric: 28.8070 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 950/1000
2023-10-26 03:47:30.575 
Epoch 950/1000 
	 loss: 28.3653, MinusLogProbMetric: 28.3653, val_loss: 28.7322, val_MinusLogProbMetric: 28.7322

Epoch 950: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3653 - MinusLogProbMetric: 28.3653 - val_loss: 28.7322 - val_MinusLogProbMetric: 28.7322 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 951/1000
2023-10-26 03:48:12.978 
Epoch 951/1000 
	 loss: 28.3682, MinusLogProbMetric: 28.3682, val_loss: 28.7425, val_MinusLogProbMetric: 28.7425

Epoch 951: val_loss did not improve from 28.72390
196/196 - 42s - loss: 28.3682 - MinusLogProbMetric: 28.3682 - val_loss: 28.7425 - val_MinusLogProbMetric: 28.7425 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 952/1000
2023-10-26 03:48:55.588 
Epoch 952/1000 
	 loss: 28.3737, MinusLogProbMetric: 28.3737, val_loss: 28.7522, val_MinusLogProbMetric: 28.7522

Epoch 952: val_loss did not improve from 28.72390
196/196 - 43s - loss: 28.3737 - MinusLogProbMetric: 28.3737 - val_loss: 28.7522 - val_MinusLogProbMetric: 28.7522 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 953/1000
2023-10-26 03:49:38.328 
Epoch 953/1000 
	 loss: 28.3697, MinusLogProbMetric: 28.3697, val_loss: 28.7223, val_MinusLogProbMetric: 28.7223

Epoch 953: val_loss improved from 28.72390 to 28.72226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 44s - loss: 28.3697 - MinusLogProbMetric: 28.3697 - val_loss: 28.7223 - val_MinusLogProbMetric: 28.7223 - lr: 4.1667e-05 - 44s/epoch - 222ms/step
Epoch 954/1000
2023-10-26 03:50:21.588 
Epoch 954/1000 
	 loss: 28.3679, MinusLogProbMetric: 28.3679, val_loss: 28.7436, val_MinusLogProbMetric: 28.7436

Epoch 954: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3679 - MinusLogProbMetric: 28.3679 - val_loss: 28.7436 - val_MinusLogProbMetric: 28.7436 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 955/1000
2023-10-26 03:51:03.937 
Epoch 955/1000 
	 loss: 28.3686, MinusLogProbMetric: 28.3686, val_loss: 28.7759, val_MinusLogProbMetric: 28.7759

Epoch 955: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3686 - MinusLogProbMetric: 28.3686 - val_loss: 28.7759 - val_MinusLogProbMetric: 28.7759 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 956/1000
2023-10-26 03:51:46.187 
Epoch 956/1000 
	 loss: 28.3746, MinusLogProbMetric: 28.3746, val_loss: 28.7502, val_MinusLogProbMetric: 28.7502

Epoch 956: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3746 - MinusLogProbMetric: 28.3746 - val_loss: 28.7502 - val_MinusLogProbMetric: 28.7502 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 957/1000
2023-10-26 03:52:28.305 
Epoch 957/1000 
	 loss: 28.3661, MinusLogProbMetric: 28.3661, val_loss: 28.7612, val_MinusLogProbMetric: 28.7612

Epoch 957: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3661 - MinusLogProbMetric: 28.3661 - val_loss: 28.7612 - val_MinusLogProbMetric: 28.7612 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 958/1000
2023-10-26 03:53:10.483 
Epoch 958/1000 
	 loss: 28.3734, MinusLogProbMetric: 28.3734, val_loss: 28.7739, val_MinusLogProbMetric: 28.7739

Epoch 958: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3734 - MinusLogProbMetric: 28.3734 - val_loss: 28.7739 - val_MinusLogProbMetric: 28.7739 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 959/1000
2023-10-26 03:53:52.446 
Epoch 959/1000 
	 loss: 28.3701, MinusLogProbMetric: 28.3701, val_loss: 28.7420, val_MinusLogProbMetric: 28.7420

Epoch 959: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3701 - MinusLogProbMetric: 28.3701 - val_loss: 28.7420 - val_MinusLogProbMetric: 28.7420 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 960/1000
2023-10-26 03:54:34.315 
Epoch 960/1000 
	 loss: 28.3677, MinusLogProbMetric: 28.3677, val_loss: 28.7706, val_MinusLogProbMetric: 28.7706

Epoch 960: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3677 - MinusLogProbMetric: 28.3677 - val_loss: 28.7706 - val_MinusLogProbMetric: 28.7706 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 961/1000
2023-10-26 03:55:16.681 
Epoch 961/1000 
	 loss: 28.3608, MinusLogProbMetric: 28.3608, val_loss: 28.7785, val_MinusLogProbMetric: 28.7785

Epoch 961: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3608 - MinusLogProbMetric: 28.3608 - val_loss: 28.7785 - val_MinusLogProbMetric: 28.7785 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 962/1000
2023-10-26 03:55:58.834 
Epoch 962/1000 
	 loss: 28.3604, MinusLogProbMetric: 28.3604, val_loss: 28.7436, val_MinusLogProbMetric: 28.7436

Epoch 962: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3604 - MinusLogProbMetric: 28.3604 - val_loss: 28.7436 - val_MinusLogProbMetric: 28.7436 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 963/1000
2023-10-26 03:56:41.396 
Epoch 963/1000 
	 loss: 28.3650, MinusLogProbMetric: 28.3650, val_loss: 28.7354, val_MinusLogProbMetric: 28.7354

Epoch 963: val_loss did not improve from 28.72226
196/196 - 43s - loss: 28.3650 - MinusLogProbMetric: 28.3650 - val_loss: 28.7354 - val_MinusLogProbMetric: 28.7354 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 964/1000
2023-10-26 03:57:23.555 
Epoch 964/1000 
	 loss: 28.3577, MinusLogProbMetric: 28.3577, val_loss: 28.7280, val_MinusLogProbMetric: 28.7280

Epoch 964: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3577 - MinusLogProbMetric: 28.3577 - val_loss: 28.7280 - val_MinusLogProbMetric: 28.7280 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 965/1000
2023-10-26 03:58:06.146 
Epoch 965/1000 
	 loss: 28.3641, MinusLogProbMetric: 28.3641, val_loss: 28.7815, val_MinusLogProbMetric: 28.7815

Epoch 965: val_loss did not improve from 28.72226
196/196 - 43s - loss: 28.3641 - MinusLogProbMetric: 28.3641 - val_loss: 28.7815 - val_MinusLogProbMetric: 28.7815 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 966/1000
2023-10-26 03:58:48.337 
Epoch 966/1000 
	 loss: 28.3599, MinusLogProbMetric: 28.3599, val_loss: 28.7534, val_MinusLogProbMetric: 28.7534

Epoch 966: val_loss did not improve from 28.72226
196/196 - 42s - loss: 28.3599 - MinusLogProbMetric: 28.3599 - val_loss: 28.7534 - val_MinusLogProbMetric: 28.7534 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 967/1000
2023-10-26 03:59:30.766 
Epoch 967/1000 
	 loss: 28.3614, MinusLogProbMetric: 28.3614, val_loss: 28.7153, val_MinusLogProbMetric: 28.7153

Epoch 967: val_loss improved from 28.72226 to 28.71534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_363/weights/best_weights.h5
196/196 - 43s - loss: 28.3614 - MinusLogProbMetric: 28.3614 - val_loss: 28.7153 - val_MinusLogProbMetric: 28.7153 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 968/1000
2023-10-26 04:00:14.061 
Epoch 968/1000 
	 loss: 28.3645, MinusLogProbMetric: 28.3645, val_loss: 28.7476, val_MinusLogProbMetric: 28.7476

Epoch 968: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3645 - MinusLogProbMetric: 28.3645 - val_loss: 28.7476 - val_MinusLogProbMetric: 28.7476 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 969/1000
2023-10-26 04:00:56.608 
Epoch 969/1000 
	 loss: 28.3621, MinusLogProbMetric: 28.3621, val_loss: 28.7268, val_MinusLogProbMetric: 28.7268

Epoch 969: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3621 - MinusLogProbMetric: 28.3621 - val_loss: 28.7268 - val_MinusLogProbMetric: 28.7268 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 970/1000
2023-10-26 04:01:38.841 
Epoch 970/1000 
	 loss: 28.3581, MinusLogProbMetric: 28.3581, val_loss: 28.7353, val_MinusLogProbMetric: 28.7353

Epoch 970: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3581 - MinusLogProbMetric: 28.3581 - val_loss: 28.7353 - val_MinusLogProbMetric: 28.7353 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 971/1000
2023-10-26 04:02:21.117 
Epoch 971/1000 
	 loss: 28.3605, MinusLogProbMetric: 28.3605, val_loss: 28.7441, val_MinusLogProbMetric: 28.7441

Epoch 971: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3605 - MinusLogProbMetric: 28.3605 - val_loss: 28.7441 - val_MinusLogProbMetric: 28.7441 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 972/1000
2023-10-26 04:03:03.291 
Epoch 972/1000 
	 loss: 28.3629, MinusLogProbMetric: 28.3629, val_loss: 28.7431, val_MinusLogProbMetric: 28.7431

Epoch 972: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3629 - MinusLogProbMetric: 28.3629 - val_loss: 28.7431 - val_MinusLogProbMetric: 28.7431 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 973/1000
2023-10-26 04:03:45.523 
Epoch 973/1000 
	 loss: 28.3653, MinusLogProbMetric: 28.3653, val_loss: 28.7204, val_MinusLogProbMetric: 28.7204

Epoch 973: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3653 - MinusLogProbMetric: 28.3653 - val_loss: 28.7204 - val_MinusLogProbMetric: 28.7204 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 974/1000
2023-10-26 04:04:27.922 
Epoch 974/1000 
	 loss: 28.3705, MinusLogProbMetric: 28.3705, val_loss: 28.7584, val_MinusLogProbMetric: 28.7584

Epoch 974: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3705 - MinusLogProbMetric: 28.3705 - val_loss: 28.7584 - val_MinusLogProbMetric: 28.7584 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 975/1000
2023-10-26 04:05:09.800 
Epoch 975/1000 
	 loss: 28.3621, MinusLogProbMetric: 28.3621, val_loss: 28.7340, val_MinusLogProbMetric: 28.7340

Epoch 975: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3621 - MinusLogProbMetric: 28.3621 - val_loss: 28.7340 - val_MinusLogProbMetric: 28.7340 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 976/1000
2023-10-26 04:05:52.249 
Epoch 976/1000 
	 loss: 28.3616, MinusLogProbMetric: 28.3616, val_loss: 28.7750, val_MinusLogProbMetric: 28.7750

Epoch 976: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3616 - MinusLogProbMetric: 28.3616 - val_loss: 28.7750 - val_MinusLogProbMetric: 28.7750 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 977/1000
2023-10-26 04:06:34.453 
Epoch 977/1000 
	 loss: 28.3597, MinusLogProbMetric: 28.3597, val_loss: 28.7578, val_MinusLogProbMetric: 28.7578

Epoch 977: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3597 - MinusLogProbMetric: 28.3597 - val_loss: 28.7578 - val_MinusLogProbMetric: 28.7578 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 978/1000
2023-10-26 04:07:17.092 
Epoch 978/1000 
	 loss: 28.3631, MinusLogProbMetric: 28.3631, val_loss: 28.7614, val_MinusLogProbMetric: 28.7614

Epoch 978: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3631 - MinusLogProbMetric: 28.3631 - val_loss: 28.7614 - val_MinusLogProbMetric: 28.7614 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 979/1000
2023-10-26 04:07:59.470 
Epoch 979/1000 
	 loss: 28.3610, MinusLogProbMetric: 28.3610, val_loss: 28.7871, val_MinusLogProbMetric: 28.7871

Epoch 979: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3610 - MinusLogProbMetric: 28.3610 - val_loss: 28.7871 - val_MinusLogProbMetric: 28.7871 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 980/1000
2023-10-26 04:08:41.835 
Epoch 980/1000 
	 loss: 28.3611, MinusLogProbMetric: 28.3611, val_loss: 28.7390, val_MinusLogProbMetric: 28.7390

Epoch 980: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3611 - MinusLogProbMetric: 28.3611 - val_loss: 28.7390 - val_MinusLogProbMetric: 28.7390 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 981/1000
2023-10-26 04:09:24.231 
Epoch 981/1000 
	 loss: 28.3630, MinusLogProbMetric: 28.3630, val_loss: 28.7617, val_MinusLogProbMetric: 28.7617

Epoch 981: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3630 - MinusLogProbMetric: 28.3630 - val_loss: 28.7617 - val_MinusLogProbMetric: 28.7617 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 982/1000
2023-10-26 04:10:06.974 
Epoch 982/1000 
	 loss: 28.3554, MinusLogProbMetric: 28.3554, val_loss: 28.7735, val_MinusLogProbMetric: 28.7735

Epoch 982: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3554 - MinusLogProbMetric: 28.3554 - val_loss: 28.7735 - val_MinusLogProbMetric: 28.7735 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 983/1000
2023-10-26 04:10:48.891 
Epoch 983/1000 
	 loss: 28.3697, MinusLogProbMetric: 28.3697, val_loss: 28.7696, val_MinusLogProbMetric: 28.7696

Epoch 983: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3697 - MinusLogProbMetric: 28.3697 - val_loss: 28.7696 - val_MinusLogProbMetric: 28.7696 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 984/1000
2023-10-26 04:11:30.183 
Epoch 984/1000 
	 loss: 28.3617, MinusLogProbMetric: 28.3617, val_loss: 28.7686, val_MinusLogProbMetric: 28.7686

Epoch 984: val_loss did not improve from 28.71534
196/196 - 41s - loss: 28.3617 - MinusLogProbMetric: 28.3617 - val_loss: 28.7686 - val_MinusLogProbMetric: 28.7686 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 985/1000
2023-10-26 04:12:12.825 
Epoch 985/1000 
	 loss: 28.3683, MinusLogProbMetric: 28.3683, val_loss: 28.7611, val_MinusLogProbMetric: 28.7611

Epoch 985: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3683 - MinusLogProbMetric: 28.3683 - val_loss: 28.7611 - val_MinusLogProbMetric: 28.7611 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 986/1000
2023-10-26 04:12:53.070 
Epoch 986/1000 
	 loss: 28.3616, MinusLogProbMetric: 28.3616, val_loss: 28.7684, val_MinusLogProbMetric: 28.7684

Epoch 986: val_loss did not improve from 28.71534
196/196 - 40s - loss: 28.3616 - MinusLogProbMetric: 28.3616 - val_loss: 28.7684 - val_MinusLogProbMetric: 28.7684 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 987/1000
2023-10-26 04:13:33.144 
Epoch 987/1000 
	 loss: 28.3681, MinusLogProbMetric: 28.3681, val_loss: 28.7450, val_MinusLogProbMetric: 28.7450

Epoch 987: val_loss did not improve from 28.71534
196/196 - 40s - loss: 28.3681 - MinusLogProbMetric: 28.3681 - val_loss: 28.7450 - val_MinusLogProbMetric: 28.7450 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 988/1000
2023-10-26 04:14:12.699 
Epoch 988/1000 
	 loss: 28.3608, MinusLogProbMetric: 28.3608, val_loss: 28.7698, val_MinusLogProbMetric: 28.7698

Epoch 988: val_loss did not improve from 28.71534
196/196 - 40s - loss: 28.3608 - MinusLogProbMetric: 28.3608 - val_loss: 28.7698 - val_MinusLogProbMetric: 28.7698 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 989/1000
2023-10-26 04:14:55.962 
Epoch 989/1000 
	 loss: 28.3581, MinusLogProbMetric: 28.3581, val_loss: 28.8769, val_MinusLogProbMetric: 28.8769

Epoch 989: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3581 - MinusLogProbMetric: 28.3581 - val_loss: 28.8769 - val_MinusLogProbMetric: 28.8769 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 990/1000
2023-10-26 04:15:37.952 
Epoch 990/1000 
	 loss: 28.3647, MinusLogProbMetric: 28.3647, val_loss: 28.7550, val_MinusLogProbMetric: 28.7550

Epoch 990: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3647 - MinusLogProbMetric: 28.3647 - val_loss: 28.7550 - val_MinusLogProbMetric: 28.7550 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 991/1000
2023-10-26 04:16:20.090 
Epoch 991/1000 
	 loss: 28.3675, MinusLogProbMetric: 28.3675, val_loss: 28.7486, val_MinusLogProbMetric: 28.7486

Epoch 991: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3675 - MinusLogProbMetric: 28.3675 - val_loss: 28.7486 - val_MinusLogProbMetric: 28.7486 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 992/1000
2023-10-26 04:17:01.334 
Epoch 992/1000 
	 loss: 28.3628, MinusLogProbMetric: 28.3628, val_loss: 28.7603, val_MinusLogProbMetric: 28.7603

Epoch 992: val_loss did not improve from 28.71534
196/196 - 41s - loss: 28.3628 - MinusLogProbMetric: 28.3628 - val_loss: 28.7603 - val_MinusLogProbMetric: 28.7603 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 993/1000
2023-10-26 04:17:43.180 
Epoch 993/1000 
	 loss: 28.3585, MinusLogProbMetric: 28.3585, val_loss: 28.7394, val_MinusLogProbMetric: 28.7394

Epoch 993: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3585 - MinusLogProbMetric: 28.3585 - val_loss: 28.7394 - val_MinusLogProbMetric: 28.7394 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 994/1000
2023-10-26 04:18:25.619 
Epoch 994/1000 
	 loss: 28.3676, MinusLogProbMetric: 28.3676, val_loss: 28.7555, val_MinusLogProbMetric: 28.7555

Epoch 994: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3676 - MinusLogProbMetric: 28.3676 - val_loss: 28.7555 - val_MinusLogProbMetric: 28.7555 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 995/1000
2023-10-26 04:19:08.148 
Epoch 995/1000 
	 loss: 28.3712, MinusLogProbMetric: 28.3712, val_loss: 28.7622, val_MinusLogProbMetric: 28.7622

Epoch 995: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3712 - MinusLogProbMetric: 28.3712 - val_loss: 28.7622 - val_MinusLogProbMetric: 28.7622 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 996/1000
2023-10-26 04:19:50.395 
Epoch 996/1000 
	 loss: 28.3651, MinusLogProbMetric: 28.3651, val_loss: 28.7725, val_MinusLogProbMetric: 28.7725

Epoch 996: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3651 - MinusLogProbMetric: 28.3651 - val_loss: 28.7725 - val_MinusLogProbMetric: 28.7725 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 997/1000
2023-10-26 04:20:32.658 
Epoch 997/1000 
	 loss: 28.3628, MinusLogProbMetric: 28.3628, val_loss: 28.7542, val_MinusLogProbMetric: 28.7542

Epoch 997: val_loss did not improve from 28.71534
196/196 - 42s - loss: 28.3628 - MinusLogProbMetric: 28.3628 - val_loss: 28.7542 - val_MinusLogProbMetric: 28.7542 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 998/1000
2023-10-26 04:21:15.379 
Epoch 998/1000 
	 loss: 28.3640, MinusLogProbMetric: 28.3640, val_loss: 28.7660, val_MinusLogProbMetric: 28.7660

Epoch 998: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3640 - MinusLogProbMetric: 28.3640 - val_loss: 28.7660 - val_MinusLogProbMetric: 28.7660 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 999/1000
2023-10-26 04:21:58.097 
Epoch 999/1000 
	 loss: 28.3661, MinusLogProbMetric: 28.3661, val_loss: 28.7536, val_MinusLogProbMetric: 28.7536

Epoch 999: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3661 - MinusLogProbMetric: 28.3661 - val_loss: 28.7536 - val_MinusLogProbMetric: 28.7536 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 1000/1000
2023-10-26 04:22:40.881 
Epoch 1000/1000 
	 loss: 28.3597, MinusLogProbMetric: 28.3597, val_loss: 28.7997, val_MinusLogProbMetric: 28.7997

Epoch 1000: val_loss did not improve from 28.71534
196/196 - 43s - loss: 28.3597 - MinusLogProbMetric: 28.3597 - val_loss: 28.7997 - val_MinusLogProbMetric: 28.7997 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 41939.99 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.00 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.54 s.
===========
Run 363/720 done in 42036.15 s.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

===========
Generating train data for run 373.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_143"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_144 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fec75f7aef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec758c45b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec758c45b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4dadefe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec755a2950>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec755a2ec0>, <keras.callbacks.ModelCheckpoint object at 0x7fec755a2f80>, <keras.callbacks.EarlyStopping object at 0x7fec755a31f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec755a3220>, <keras.callbacks.TerminateOnNaN object at 0x7fec755a2e60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:22:52.197168
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:25:07.575 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 135s/epoch - 689ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 373.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_154"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_155 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fec27c99540>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fead82c0dc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fead82c0dc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec2780e980>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec26b48460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec26b489d0>, <keras.callbacks.ModelCheckpoint object at 0x7fec26b48a90>, <keras.callbacks.EarlyStopping object at 0x7fec26b48d00>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec26b48d30>, <keras.callbacks.TerminateOnNaN object at 0x7fec26b48970>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:25:17.786178
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:27:25.696 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 128s/epoch - 651ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 373.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_165"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_166 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7feb58237c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec26741840>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec26741840>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febec166e60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec26695270>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec266957e0>, <keras.callbacks.ModelCheckpoint object at 0x7fec266958a0>, <keras.callbacks.EarlyStopping object at 0x7fec26695b10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec26695b40>, <keras.callbacks.TerminateOnNaN object at 0x7fec26695780>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:27:35.825728
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:29:45.488 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 129s/epoch - 660ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 3.703703703703703e-05.
===========
Generating train data for run 373.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_176"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_177 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7feae061c580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb506d61d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb506d61d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fead86432e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec75dc7fd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec76500580>, <keras.callbacks.ModelCheckpoint object at 0x7fec76500640>, <keras.callbacks.EarlyStopping object at 0x7fec765008b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec765008e0>, <keras.callbacks.TerminateOnNaN object at 0x7fec76500520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:29:52.675815
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:32:04.267 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 131s/epoch - 671ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.2345679012345677e-05.
===========
Generating train data for run 373.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_187"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_188 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7feb84187400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb003940d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb003940d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea30f87b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb841fe440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb841fe9b0>, <keras.callbacks.ModelCheckpoint object at 0x7feb841fea70>, <keras.callbacks.EarlyStopping object at 0x7feb841fece0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb841fed10>, <keras.callbacks.TerminateOnNaN object at 0x7feb841fe950>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:32:13.559760
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:34:29.954 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 136s/epoch - 695ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.115226337448558e-06.
===========
Generating train data for run 373.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_199 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7febd8321de0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feaf424b910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feaf424b910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea404478b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fead858c280>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fead858c7f0>, <keras.callbacks.ModelCheckpoint object at 0x7fead858c8b0>, <keras.callbacks.EarlyStopping object at 0x7fead858cb20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fead858cb50>, <keras.callbacks.TerminateOnNaN object at 0x7fead858c790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:34:40.222105
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:36:53.128 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 133s/epoch - 678ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.3717421124828526e-06.
===========
Generating train data for run 373.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_209"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_210 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7feae04ddf60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb206c3700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb206c3700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4fc96ce0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec75bc4b80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec75bc7dc0>, <keras.callbacks.ModelCheckpoint object at 0x7fec75bc6c50>, <keras.callbacks.EarlyStopping object at 0x7fec75bc7b20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec75bc4e80>, <keras.callbacks.TerminateOnNaN object at 0x7fec75bc78e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:37:03.581262
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:39:13.465 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 130s/epoch - 662ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.572473708276175e-07.
===========
Generating train data for run 373.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_220"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_221 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7fec25ebec20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea18194d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea18194d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febed3c80a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7febed230430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7febed2309a0>, <keras.callbacks.ModelCheckpoint object at 0x7febed230a60>, <keras.callbacks.EarlyStopping object at 0x7febed230cd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7febed230d00>, <keras.callbacks.TerminateOnNaN object at 0x7febed230940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:39:23.439171
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:41:38.323 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 135s/epoch - 687ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.524157902758725e-07.
===========
Generating train data for run 373.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_231"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_232 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7fea184d8940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feae07751e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feae07751e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea2a65a8f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb185ba020>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb185ba590>, <keras.callbacks.ModelCheckpoint object at 0x7feb185ba650>, <keras.callbacks.EarlyStopping object at 0x7feb185ba8c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb185ba8f0>, <keras.callbacks.TerminateOnNaN object at 0x7feb185ba530>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:41:48.751108
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:44:12.159 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 143s/epoch - 730ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 5.0805263425290834e-08.
===========
Generating train data for run 373.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_242"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_243 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7feb0857fdf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec755f3f70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec755f3f70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feae018bf10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb0853f5e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb0853fb50>, <keras.callbacks.ModelCheckpoint object at 0x7feb0853fc10>, <keras.callbacks.EarlyStopping object at 0x7feb0853fe80>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb0853feb0>, <keras.callbacks.TerminateOnNaN object at 0x7feb0853faf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:44:22.424814
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:46:30.639 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 128s/epoch - 653ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.6935087808430278e-08.
===========
Generating train data for run 373.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_253"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_254 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7febdb6cbe20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb085783a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb085783a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea2b1eac50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7febdb502140>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7febdb5026b0>, <keras.callbacks.ModelCheckpoint object at 0x7febdb502770>, <keras.callbacks.EarlyStopping object at 0x7febdb5029e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7febdb502a10>, <keras.callbacks.TerminateOnNaN object at 0x7febdb502650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-26 04:46:40.677952
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:48:53.578 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 133s/epoch - 677ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 5.645029269476759e-09.
===========
Run 373/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 374.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_264"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_265 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7fea397608e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7febec7605e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7febec7605e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb3822f0a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea29ec74c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea29ec6cb0>, <keras.callbacks.ModelCheckpoint object at 0x7fea29ec6350>, <keras.callbacks.EarlyStopping object at 0x7fea29ec6470>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea29ec6d70>, <keras.callbacks.TerminateOnNaN object at 0x7fea29ec7100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_374/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 04:49:02.952647
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 11: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:51:26.642 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5585.6450, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 5585.6450 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 144s/epoch - 732ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 374.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_275"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_276 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7fedc471efb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed841ce590>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed841ce590>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fed08703370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fedc47c3190>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fedc47c3700>, <keras.callbacks.ModelCheckpoint object at 0x7fedc47c37c0>, <keras.callbacks.EarlyStopping object at 0x7fedc47c3a30>, <keras.callbacks.ReduceLROnPlateau object at 0x7fedc47c3a60>, <keras.callbacks.TerminateOnNaN object at 0x7fedc47c36a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_374/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 04:51:36.755567
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 90: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:54:19.889 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3152.5938, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 3152.5938 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 163s/epoch - 831ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 374.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_286"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_287 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7fede833eb30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feda84c4eb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feda84c4eb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feccc791780>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef08188af0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef081894b0>, <keras.callbacks.ModelCheckpoint object at 0x7fef08189930>, <keras.callbacks.EarlyStopping object at 0x7fef08188f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef08189450>, <keras.callbacks.TerminateOnNaN object at 0x7fef08189360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_374/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 04:54:30.461513
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 04:57:41.539 
Epoch 1/1000 
	 loss: 2998.2434, MinusLogProbMetric: 2998.2434, val_loss: 1443.7521, val_MinusLogProbMetric: 1443.7521

Epoch 1: val_loss improved from inf to 1443.75208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 192s - loss: 2998.2434 - MinusLogProbMetric: 2998.2434 - val_loss: 1443.7521 - val_MinusLogProbMetric: 1443.7521 - lr: 1.1111e-04 - 192s/epoch - 977ms/step
Epoch 2/1000
2023-10-26 04:58:47.113 
Epoch 2/1000 
	 loss: 1032.5695, MinusLogProbMetric: 1032.5695, val_loss: 909.4217, val_MinusLogProbMetric: 909.4217

Epoch 2: val_loss improved from 1443.75208 to 909.42169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 1032.5695 - MinusLogProbMetric: 1032.5695 - val_loss: 909.4217 - val_MinusLogProbMetric: 909.4217 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 3/1000
2023-10-26 04:59:46.088 
Epoch 3/1000 
	 loss: 785.0858, MinusLogProbMetric: 785.0858, val_loss: 921.5274, val_MinusLogProbMetric: 921.5274

Epoch 3: val_loss did not improve from 909.42169
196/196 - 58s - loss: 785.0858 - MinusLogProbMetric: 785.0858 - val_loss: 921.5274 - val_MinusLogProbMetric: 921.5274 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 4/1000
2023-10-26 05:00:41.610 
Epoch 4/1000 
	 loss: 747.0627, MinusLogProbMetric: 747.0627, val_loss: 657.2921, val_MinusLogProbMetric: 657.2921

Epoch 4: val_loss improved from 909.42169 to 657.29211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 747.0627 - MinusLogProbMetric: 747.0627 - val_loss: 657.2921 - val_MinusLogProbMetric: 657.2921 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 5/1000
2023-10-26 05:01:47.803 
Epoch 5/1000 
	 loss: 640.3487, MinusLogProbMetric: 640.3487, val_loss: 671.5236, val_MinusLogProbMetric: 671.5236

Epoch 5: val_loss did not improve from 657.29211
196/196 - 65s - loss: 640.3487 - MinusLogProbMetric: 640.3487 - val_loss: 671.5236 - val_MinusLogProbMetric: 671.5236 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 6/1000
2023-10-26 05:02:42.853 
Epoch 6/1000 
	 loss: 628.2202, MinusLogProbMetric: 628.2202, val_loss: 655.0147, val_MinusLogProbMetric: 655.0147

Epoch 6: val_loss improved from 657.29211 to 655.01471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 628.2202 - MinusLogProbMetric: 628.2202 - val_loss: 655.0147 - val_MinusLogProbMetric: 655.0147 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 7/1000
2023-10-26 05:03:39.657 
Epoch 7/1000 
	 loss: 600.4102, MinusLogProbMetric: 600.4102, val_loss: 604.0745, val_MinusLogProbMetric: 604.0745

Epoch 7: val_loss improved from 655.01471 to 604.07446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 600.4102 - MinusLogProbMetric: 600.4102 - val_loss: 604.0745 - val_MinusLogProbMetric: 604.0745 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 8/1000
2023-10-26 05:04:45.403 
Epoch 8/1000 
	 loss: 644.2828, MinusLogProbMetric: 644.2828, val_loss: 614.9293, val_MinusLogProbMetric: 614.9293

Epoch 8: val_loss did not improve from 604.07446
196/196 - 65s - loss: 644.2828 - MinusLogProbMetric: 644.2828 - val_loss: 614.9293 - val_MinusLogProbMetric: 614.9293 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 9/1000
2023-10-26 05:05:46.902 
Epoch 9/1000 
	 loss: 532.8833, MinusLogProbMetric: 532.8833, val_loss: 501.3674, val_MinusLogProbMetric: 501.3674

Epoch 9: val_loss improved from 604.07446 to 501.36743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 532.8833 - MinusLogProbMetric: 532.8833 - val_loss: 501.3674 - val_MinusLogProbMetric: 501.3674 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 10/1000
2023-10-26 05:06:43.410 
Epoch 10/1000 
	 loss: 441.8712, MinusLogProbMetric: 441.8712, val_loss: 404.3595, val_MinusLogProbMetric: 404.3595

Epoch 10: val_loss improved from 501.36743 to 404.35950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 441.8712 - MinusLogProbMetric: 441.8712 - val_loss: 404.3595 - val_MinusLogProbMetric: 404.3595 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 11/1000
2023-10-26 05:07:50.973 
Epoch 11/1000 
	 loss: 415.5456, MinusLogProbMetric: 415.5456, val_loss: 389.3137, val_MinusLogProbMetric: 389.3137

Epoch 11: val_loss improved from 404.35950 to 389.31366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 415.5456 - MinusLogProbMetric: 415.5456 - val_loss: 389.3137 - val_MinusLogProbMetric: 389.3137 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 12/1000
2023-10-26 05:08:52.072 
Epoch 12/1000 
	 loss: 375.2783, MinusLogProbMetric: 375.2783, val_loss: 362.4154, val_MinusLogProbMetric: 362.4154

Epoch 12: val_loss improved from 389.31366 to 362.41541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 375.2783 - MinusLogProbMetric: 375.2783 - val_loss: 362.4154 - val_MinusLogProbMetric: 362.4154 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 13/1000
2023-10-26 05:09:50.234 
Epoch 13/1000 
	 loss: 336.6747, MinusLogProbMetric: 336.6747, val_loss: 317.4085, val_MinusLogProbMetric: 317.4085

Epoch 13: val_loss improved from 362.41541 to 317.40854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 336.6747 - MinusLogProbMetric: 336.6747 - val_loss: 317.4085 - val_MinusLogProbMetric: 317.4085 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 14/1000
2023-10-26 05:10:54.777 
Epoch 14/1000 
	 loss: 320.7304, MinusLogProbMetric: 320.7304, val_loss: 315.7443, val_MinusLogProbMetric: 315.7443

Epoch 14: val_loss improved from 317.40854 to 315.74432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 320.7304 - MinusLogProbMetric: 320.7304 - val_loss: 315.7443 - val_MinusLogProbMetric: 315.7443 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 15/1000
2023-10-26 05:11:56.477 
Epoch 15/1000 
	 loss: 343.1611, MinusLogProbMetric: 343.1611, val_loss: 333.5157, val_MinusLogProbMetric: 333.5157

Epoch 15: val_loss did not improve from 315.74432
196/196 - 61s - loss: 343.1611 - MinusLogProbMetric: 343.1611 - val_loss: 333.5157 - val_MinusLogProbMetric: 333.5157 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 16/1000
2023-10-26 05:12:53.491 
Epoch 16/1000 
	 loss: 307.9246, MinusLogProbMetric: 307.9246, val_loss: 299.0861, val_MinusLogProbMetric: 299.0861

Epoch 16: val_loss improved from 315.74432 to 299.08606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 307.9246 - MinusLogProbMetric: 307.9246 - val_loss: 299.0861 - val_MinusLogProbMetric: 299.0861 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 17/1000
2023-10-26 05:13:56.492 
Epoch 17/1000 
	 loss: 289.7158, MinusLogProbMetric: 289.7158, val_loss: 284.6081, val_MinusLogProbMetric: 284.6081

Epoch 17: val_loss improved from 299.08606 to 284.60812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 289.7158 - MinusLogProbMetric: 289.7158 - val_loss: 284.6081 - val_MinusLogProbMetric: 284.6081 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 18/1000
2023-10-26 05:15:04.288 
Epoch 18/1000 
	 loss: 271.3366, MinusLogProbMetric: 271.3366, val_loss: 262.9891, val_MinusLogProbMetric: 262.9891

Epoch 18: val_loss improved from 284.60812 to 262.98914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 271.3366 - MinusLogProbMetric: 271.3366 - val_loss: 262.9891 - val_MinusLogProbMetric: 262.9891 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 19/1000
2023-10-26 05:16:01.466 
Epoch 19/1000 
	 loss: 262.9884, MinusLogProbMetric: 262.9884, val_loss: 255.9290, val_MinusLogProbMetric: 255.9290

Epoch 19: val_loss improved from 262.98914 to 255.92902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 262.9884 - MinusLogProbMetric: 262.9884 - val_loss: 255.9290 - val_MinusLogProbMetric: 255.9290 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 20/1000
2023-10-26 05:17:00.056 
Epoch 20/1000 
	 loss: 252.3436, MinusLogProbMetric: 252.3436, val_loss: 248.0096, val_MinusLogProbMetric: 248.0096

Epoch 20: val_loss improved from 255.92902 to 248.00955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 252.3436 - MinusLogProbMetric: 252.3436 - val_loss: 248.0096 - val_MinusLogProbMetric: 248.0096 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 21/1000
2023-10-26 05:18:08.893 
Epoch 21/1000 
	 loss: 248.8401, MinusLogProbMetric: 248.8401, val_loss: 255.3265, val_MinusLogProbMetric: 255.3265

Epoch 21: val_loss did not improve from 248.00955
196/196 - 68s - loss: 248.8401 - MinusLogProbMetric: 248.8401 - val_loss: 255.3265 - val_MinusLogProbMetric: 255.3265 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 22/1000
2023-10-26 05:19:04.706 
Epoch 22/1000 
	 loss: 239.0347, MinusLogProbMetric: 239.0347, val_loss: 233.6978, val_MinusLogProbMetric: 233.6978

Epoch 22: val_loss improved from 248.00955 to 233.69783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 239.0347 - MinusLogProbMetric: 239.0347 - val_loss: 233.6978 - val_MinusLogProbMetric: 233.6978 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 23/1000
2023-10-26 05:20:02.313 
Epoch 23/1000 
	 loss: 232.3632, MinusLogProbMetric: 232.3632, val_loss: 225.7288, val_MinusLogProbMetric: 225.7288

Epoch 23: val_loss improved from 233.69783 to 225.72876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 232.3632 - MinusLogProbMetric: 232.3632 - val_loss: 225.7288 - val_MinusLogProbMetric: 225.7288 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 24/1000
2023-10-26 05:21:09.511 
Epoch 24/1000 
	 loss: 223.4549, MinusLogProbMetric: 223.4549, val_loss: 225.3275, val_MinusLogProbMetric: 225.3275

Epoch 24: val_loss improved from 225.72876 to 225.32745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 223.4549 - MinusLogProbMetric: 223.4549 - val_loss: 225.3275 - val_MinusLogProbMetric: 225.3275 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 25/1000
2023-10-26 05:22:11.376 
Epoch 25/1000 
	 loss: 221.0998, MinusLogProbMetric: 221.0998, val_loss: 215.2590, val_MinusLogProbMetric: 215.2590

Epoch 25: val_loss improved from 225.32745 to 215.25897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 221.0998 - MinusLogProbMetric: 221.0998 - val_loss: 215.2590 - val_MinusLogProbMetric: 215.2590 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 26/1000
2023-10-26 05:23:09.133 
Epoch 26/1000 
	 loss: 212.0423, MinusLogProbMetric: 212.0423, val_loss: 208.9086, val_MinusLogProbMetric: 208.9086

Epoch 26: val_loss improved from 215.25897 to 208.90857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 212.0423 - MinusLogProbMetric: 212.0423 - val_loss: 208.9086 - val_MinusLogProbMetric: 208.9086 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 27/1000
2023-10-26 05:24:13.678 
Epoch 27/1000 
	 loss: 206.7944, MinusLogProbMetric: 206.7944, val_loss: 206.2365, val_MinusLogProbMetric: 206.2365

Epoch 27: val_loss improved from 208.90857 to 206.23651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 206.7944 - MinusLogProbMetric: 206.7944 - val_loss: 206.2365 - val_MinusLogProbMetric: 206.2365 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 28/1000
2023-10-26 05:25:17.576 
Epoch 28/1000 
	 loss: 201.6647, MinusLogProbMetric: 201.6647, val_loss: 198.9738, val_MinusLogProbMetric: 198.9738

Epoch 28: val_loss improved from 206.23651 to 198.97377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 201.6647 - MinusLogProbMetric: 201.6647 - val_loss: 198.9738 - val_MinusLogProbMetric: 198.9738 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 29/1000
2023-10-26 05:26:14.388 
Epoch 29/1000 
	 loss: 214.8407, MinusLogProbMetric: 214.8407, val_loss: 214.6147, val_MinusLogProbMetric: 214.6147

Epoch 29: val_loss did not improve from 198.97377
196/196 - 56s - loss: 214.8407 - MinusLogProbMetric: 214.8407 - val_loss: 214.6147 - val_MinusLogProbMetric: 214.6147 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 30/1000
2023-10-26 05:27:09.692 
Epoch 30/1000 
	 loss: 205.3766, MinusLogProbMetric: 205.3766, val_loss: 198.6197, val_MinusLogProbMetric: 198.6197

Epoch 30: val_loss improved from 198.97377 to 198.61966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 205.3766 - MinusLogProbMetric: 205.3766 - val_loss: 198.6197 - val_MinusLogProbMetric: 198.6197 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 31/1000
2023-10-26 05:28:05.425 
Epoch 31/1000 
	 loss: 195.5973, MinusLogProbMetric: 195.5973, val_loss: 189.7178, val_MinusLogProbMetric: 189.7178

Epoch 31: val_loss improved from 198.61966 to 189.71779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 195.5973 - MinusLogProbMetric: 195.5973 - val_loss: 189.7178 - val_MinusLogProbMetric: 189.7178 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 32/1000
2023-10-26 05:29:08.508 
Epoch 32/1000 
	 loss: 184.6939, MinusLogProbMetric: 184.6939, val_loss: 180.2820, val_MinusLogProbMetric: 180.2820

Epoch 32: val_loss improved from 189.71779 to 180.28197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 184.6939 - MinusLogProbMetric: 184.6939 - val_loss: 180.2820 - val_MinusLogProbMetric: 180.2820 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 33/1000
2023-10-26 05:30:05.295 
Epoch 33/1000 
	 loss: 178.0389, MinusLogProbMetric: 178.0389, val_loss: 176.8492, val_MinusLogProbMetric: 176.8492

Epoch 33: val_loss improved from 180.28197 to 176.84924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 178.0389 - MinusLogProbMetric: 178.0389 - val_loss: 176.8492 - val_MinusLogProbMetric: 176.8492 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 34/1000
2023-10-26 05:31:01.827 
Epoch 34/1000 
	 loss: 174.8006, MinusLogProbMetric: 174.8006, val_loss: 172.7966, val_MinusLogProbMetric: 172.7966

Epoch 34: val_loss improved from 176.84924 to 172.79663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 174.8006 - MinusLogProbMetric: 174.8006 - val_loss: 172.7966 - val_MinusLogProbMetric: 172.7966 - lr: 1.1111e-04 - 57s/epoch - 288ms/step
Epoch 35/1000
2023-10-26 05:32:03.031 
Epoch 35/1000 
	 loss: 168.9995, MinusLogProbMetric: 168.9995, val_loss: 167.3945, val_MinusLogProbMetric: 167.3945

Epoch 35: val_loss improved from 172.79663 to 167.39447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 168.9995 - MinusLogProbMetric: 168.9995 - val_loss: 167.3945 - val_MinusLogProbMetric: 167.3945 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 36/1000
2023-10-26 05:33:07.406 
Epoch 36/1000 
	 loss: 169.7629, MinusLogProbMetric: 169.7629, val_loss: 165.3622, val_MinusLogProbMetric: 165.3622

Epoch 36: val_loss improved from 167.39447 to 165.36221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 169.7629 - MinusLogProbMetric: 169.7629 - val_loss: 165.3622 - val_MinusLogProbMetric: 165.3622 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 37/1000
2023-10-26 05:34:03.469 
Epoch 37/1000 
	 loss: 224.6297, MinusLogProbMetric: 224.6297, val_loss: 213.5558, val_MinusLogProbMetric: 213.5558

Epoch 37: val_loss did not improve from 165.36221
196/196 - 55s - loss: 224.6297 - MinusLogProbMetric: 224.6297 - val_loss: 213.5558 - val_MinusLogProbMetric: 213.5558 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 38/1000
2023-10-26 05:34:58.948 
Epoch 38/1000 
	 loss: 198.9108, MinusLogProbMetric: 198.9108, val_loss: 190.1956, val_MinusLogProbMetric: 190.1956

Epoch 38: val_loss did not improve from 165.36221
196/196 - 55s - loss: 198.9108 - MinusLogProbMetric: 198.9108 - val_loss: 190.1956 - val_MinusLogProbMetric: 190.1956 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 39/1000
2023-10-26 05:35:59.621 
Epoch 39/1000 
	 loss: 186.1986, MinusLogProbMetric: 186.1986, val_loss: 178.0915, val_MinusLogProbMetric: 178.0915

Epoch 39: val_loss did not improve from 165.36221
196/196 - 61s - loss: 186.1986 - MinusLogProbMetric: 186.1986 - val_loss: 178.0915 - val_MinusLogProbMetric: 178.0915 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 40/1000
2023-10-26 05:37:00.725 
Epoch 40/1000 
	 loss: 175.5251, MinusLogProbMetric: 175.5251, val_loss: 172.5039, val_MinusLogProbMetric: 172.5039

Epoch 40: val_loss did not improve from 165.36221
196/196 - 61s - loss: 175.5251 - MinusLogProbMetric: 175.5251 - val_loss: 172.5039 - val_MinusLogProbMetric: 172.5039 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 41/1000
2023-10-26 05:37:59.632 
Epoch 41/1000 
	 loss: 164.8147, MinusLogProbMetric: 164.8147, val_loss: 159.5614, val_MinusLogProbMetric: 159.5614

Epoch 41: val_loss improved from 165.36221 to 159.56139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 60s - loss: 164.8147 - MinusLogProbMetric: 164.8147 - val_loss: 159.5614 - val_MinusLogProbMetric: 159.5614 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 42/1000
2023-10-26 05:38:54.405 
Epoch 42/1000 
	 loss: 156.6001, MinusLogProbMetric: 156.6001, val_loss: 154.7522, val_MinusLogProbMetric: 154.7522

Epoch 42: val_loss improved from 159.56139 to 154.75217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 55s - loss: 156.6001 - MinusLogProbMetric: 156.6001 - val_loss: 154.7522 - val_MinusLogProbMetric: 154.7522 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 43/1000
2023-10-26 05:39:50.557 
Epoch 43/1000 
	 loss: 152.8712, MinusLogProbMetric: 152.8712, val_loss: 150.8058, val_MinusLogProbMetric: 150.8058

Epoch 43: val_loss improved from 154.75217 to 150.80580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 152.8712 - MinusLogProbMetric: 152.8712 - val_loss: 150.8058 - val_MinusLogProbMetric: 150.8058 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 44/1000
2023-10-26 05:40:49.719 
Epoch 44/1000 
	 loss: 152.7494, MinusLogProbMetric: 152.7494, val_loss: 152.4190, val_MinusLogProbMetric: 152.4190

Epoch 44: val_loss did not improve from 150.80580
196/196 - 58s - loss: 152.7494 - MinusLogProbMetric: 152.7494 - val_loss: 152.4190 - val_MinusLogProbMetric: 152.4190 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 45/1000
2023-10-26 05:41:43.920 
Epoch 45/1000 
	 loss: 151.6179, MinusLogProbMetric: 151.6179, val_loss: 153.1379, val_MinusLogProbMetric: 153.1379

Epoch 45: val_loss did not improve from 150.80580
196/196 - 54s - loss: 151.6179 - MinusLogProbMetric: 151.6179 - val_loss: 153.1379 - val_MinusLogProbMetric: 153.1379 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 46/1000
2023-10-26 05:42:38.293 
Epoch 46/1000 
	 loss: 211.8744, MinusLogProbMetric: 211.8744, val_loss: 201.9940, val_MinusLogProbMetric: 201.9940

Epoch 46: val_loss did not improve from 150.80580
196/196 - 54s - loss: 211.8744 - MinusLogProbMetric: 211.8744 - val_loss: 201.9940 - val_MinusLogProbMetric: 201.9940 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 47/1000
2023-10-26 05:43:43.024 
Epoch 47/1000 
	 loss: 183.9441, MinusLogProbMetric: 183.9441, val_loss: 170.4476, val_MinusLogProbMetric: 170.4476

Epoch 47: val_loss did not improve from 150.80580
196/196 - 65s - loss: 183.9441 - MinusLogProbMetric: 183.9441 - val_loss: 170.4476 - val_MinusLogProbMetric: 170.4476 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 48/1000
2023-10-26 05:44:50.188 
Epoch 48/1000 
	 loss: 166.1110, MinusLogProbMetric: 166.1110, val_loss: 161.0519, val_MinusLogProbMetric: 161.0519

Epoch 48: val_loss did not improve from 150.80580
196/196 - 67s - loss: 166.1110 - MinusLogProbMetric: 166.1110 - val_loss: 161.0519 - val_MinusLogProbMetric: 161.0519 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 49/1000
2023-10-26 05:45:59.628 
Epoch 49/1000 
	 loss: 158.6825, MinusLogProbMetric: 158.6825, val_loss: 159.3542, val_MinusLogProbMetric: 159.3542

Epoch 49: val_loss did not improve from 150.80580
196/196 - 69s - loss: 158.6825 - MinusLogProbMetric: 158.6825 - val_loss: 159.3542 - val_MinusLogProbMetric: 159.3542 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 50/1000
2023-10-26 05:47:08.169 
Epoch 50/1000 
	 loss: 177.5753, MinusLogProbMetric: 177.5753, val_loss: 154.4548, val_MinusLogProbMetric: 154.4548

Epoch 50: val_loss did not improve from 150.80580
196/196 - 69s - loss: 177.5753 - MinusLogProbMetric: 177.5753 - val_loss: 154.4548 - val_MinusLogProbMetric: 154.4548 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 51/1000
2023-10-26 05:48:16.723 
Epoch 51/1000 
	 loss: 152.3639, MinusLogProbMetric: 152.3639, val_loss: 148.9142, val_MinusLogProbMetric: 148.9142

Epoch 51: val_loss improved from 150.80580 to 148.91415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 152.3639 - MinusLogProbMetric: 152.3639 - val_loss: 148.9142 - val_MinusLogProbMetric: 148.9142 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 52/1000
2023-10-26 05:49:26.651 
Epoch 52/1000 
	 loss: 146.9453, MinusLogProbMetric: 146.9453, val_loss: 145.9024, val_MinusLogProbMetric: 145.9024

Epoch 52: val_loss improved from 148.91415 to 145.90240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 146.9453 - MinusLogProbMetric: 146.9453 - val_loss: 145.9024 - val_MinusLogProbMetric: 145.9024 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 53/1000
2023-10-26 05:50:36.673 
Epoch 53/1000 
	 loss: 144.2123, MinusLogProbMetric: 144.2123, val_loss: 144.7576, val_MinusLogProbMetric: 144.7576

Epoch 53: val_loss improved from 145.90240 to 144.75763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 144.2123 - MinusLogProbMetric: 144.2123 - val_loss: 144.7576 - val_MinusLogProbMetric: 144.7576 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 54/1000
2023-10-26 05:51:47.508 
Epoch 54/1000 
	 loss: 141.9293, MinusLogProbMetric: 141.9293, val_loss: 140.4678, val_MinusLogProbMetric: 140.4678

Epoch 54: val_loss improved from 144.75763 to 140.46785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 141.9293 - MinusLogProbMetric: 141.9293 - val_loss: 140.4678 - val_MinusLogProbMetric: 140.4678 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 55/1000
2023-10-26 05:52:57.899 
Epoch 55/1000 
	 loss: 138.7291, MinusLogProbMetric: 138.7291, val_loss: 137.2933, val_MinusLogProbMetric: 137.2933

Epoch 55: val_loss improved from 140.46785 to 137.29327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 138.7291 - MinusLogProbMetric: 138.7291 - val_loss: 137.2933 - val_MinusLogProbMetric: 137.2933 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 56/1000
2023-10-26 05:54:08.063 
Epoch 56/1000 
	 loss: 136.3546, MinusLogProbMetric: 136.3546, val_loss: 134.9423, val_MinusLogProbMetric: 134.9423

Epoch 56: val_loss improved from 137.29327 to 134.94229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 136.3546 - MinusLogProbMetric: 136.3546 - val_loss: 134.9423 - val_MinusLogProbMetric: 134.9423 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 57/1000
2023-10-26 05:55:18.873 
Epoch 57/1000 
	 loss: 296.7122, MinusLogProbMetric: 296.7122, val_loss: 313.5127, val_MinusLogProbMetric: 313.5127

Epoch 57: val_loss did not improve from 134.94229
196/196 - 70s - loss: 296.7122 - MinusLogProbMetric: 296.7122 - val_loss: 313.5127 - val_MinusLogProbMetric: 313.5127 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 58/1000
2023-10-26 05:56:28.333 
Epoch 58/1000 
	 loss: 269.1850, MinusLogProbMetric: 269.1850, val_loss: 242.0478, val_MinusLogProbMetric: 242.0478

Epoch 58: val_loss did not improve from 134.94229
196/196 - 69s - loss: 269.1850 - MinusLogProbMetric: 269.1850 - val_loss: 242.0478 - val_MinusLogProbMetric: 242.0478 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 59/1000
2023-10-26 05:57:38.004 
Epoch 59/1000 
	 loss: 236.0255, MinusLogProbMetric: 236.0255, val_loss: 227.4739, val_MinusLogProbMetric: 227.4739

Epoch 59: val_loss did not improve from 134.94229
196/196 - 70s - loss: 236.0255 - MinusLogProbMetric: 236.0255 - val_loss: 227.4739 - val_MinusLogProbMetric: 227.4739 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 60/1000
2023-10-26 05:58:47.948 
Epoch 60/1000 
	 loss: 268.7471, MinusLogProbMetric: 268.7471, val_loss: 396.2299, val_MinusLogProbMetric: 396.2299

Epoch 60: val_loss did not improve from 134.94229
196/196 - 70s - loss: 268.7471 - MinusLogProbMetric: 268.7471 - val_loss: 396.2299 - val_MinusLogProbMetric: 396.2299 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 61/1000
2023-10-26 05:59:58.241 
Epoch 61/1000 
	 loss: 248.5689, MinusLogProbMetric: 248.5689, val_loss: 230.4493, val_MinusLogProbMetric: 230.4493

Epoch 61: val_loss did not improve from 134.94229
196/196 - 70s - loss: 248.5689 - MinusLogProbMetric: 248.5689 - val_loss: 230.4493 - val_MinusLogProbMetric: 230.4493 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 62/1000
2023-10-26 06:01:07.854 
Epoch 62/1000 
	 loss: 223.3962, MinusLogProbMetric: 223.3962, val_loss: 222.5576, val_MinusLogProbMetric: 222.5576

Epoch 62: val_loss did not improve from 134.94229
196/196 - 70s - loss: 223.3962 - MinusLogProbMetric: 223.3962 - val_loss: 222.5576 - val_MinusLogProbMetric: 222.5576 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 63/1000
2023-10-26 06:02:16.396 
Epoch 63/1000 
	 loss: 213.1680, MinusLogProbMetric: 213.1680, val_loss: 207.3481, val_MinusLogProbMetric: 207.3481

Epoch 63: val_loss did not improve from 134.94229
196/196 - 69s - loss: 213.1680 - MinusLogProbMetric: 213.1680 - val_loss: 207.3481 - val_MinusLogProbMetric: 207.3481 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 64/1000
2023-10-26 06:03:25.920 
Epoch 64/1000 
	 loss: 204.9140, MinusLogProbMetric: 204.9140, val_loss: 201.7953, val_MinusLogProbMetric: 201.7953

Epoch 64: val_loss did not improve from 134.94229
196/196 - 70s - loss: 204.9140 - MinusLogProbMetric: 204.9140 - val_loss: 201.7953 - val_MinusLogProbMetric: 201.7953 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 65/1000
2023-10-26 06:04:35.321 
Epoch 65/1000 
	 loss: 198.5099, MinusLogProbMetric: 198.5099, val_loss: 196.0152, val_MinusLogProbMetric: 196.0152

Epoch 65: val_loss did not improve from 134.94229
196/196 - 69s - loss: 198.5099 - MinusLogProbMetric: 198.5099 - val_loss: 196.0152 - val_MinusLogProbMetric: 196.0152 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 66/1000
2023-10-26 06:05:45.059 
Epoch 66/1000 
	 loss: 192.6751, MinusLogProbMetric: 192.6751, val_loss: 190.7652, val_MinusLogProbMetric: 190.7652

Epoch 66: val_loss did not improve from 134.94229
196/196 - 70s - loss: 192.6751 - MinusLogProbMetric: 192.6751 - val_loss: 190.7652 - val_MinusLogProbMetric: 190.7652 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 67/1000
2023-10-26 06:06:54.957 
Epoch 67/1000 
	 loss: 179.8702, MinusLogProbMetric: 179.8702, val_loss: 173.1709, val_MinusLogProbMetric: 173.1709

Epoch 67: val_loss did not improve from 134.94229
196/196 - 70s - loss: 179.8702 - MinusLogProbMetric: 179.8702 - val_loss: 173.1709 - val_MinusLogProbMetric: 173.1709 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 68/1000
2023-10-26 06:08:03.758 
Epoch 68/1000 
	 loss: 171.9998, MinusLogProbMetric: 171.9998, val_loss: 169.6256, val_MinusLogProbMetric: 169.6256

Epoch 68: val_loss did not improve from 134.94229
196/196 - 69s - loss: 171.9998 - MinusLogProbMetric: 171.9998 - val_loss: 169.6256 - val_MinusLogProbMetric: 169.6256 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 69/1000
2023-10-26 06:09:13.066 
Epoch 69/1000 
	 loss: 168.7413, MinusLogProbMetric: 168.7413, val_loss: 164.5347, val_MinusLogProbMetric: 164.5347

Epoch 69: val_loss did not improve from 134.94229
196/196 - 69s - loss: 168.7413 - MinusLogProbMetric: 168.7413 - val_loss: 164.5347 - val_MinusLogProbMetric: 164.5347 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 70/1000
2023-10-26 06:10:22.496 
Epoch 70/1000 
	 loss: 163.0455, MinusLogProbMetric: 163.0455, val_loss: 159.4416, val_MinusLogProbMetric: 159.4416

Epoch 70: val_loss did not improve from 134.94229
196/196 - 69s - loss: 163.0455 - MinusLogProbMetric: 163.0455 - val_loss: 159.4416 - val_MinusLogProbMetric: 159.4416 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 71/1000
2023-10-26 06:11:32.440 
Epoch 71/1000 
	 loss: 164.2465, MinusLogProbMetric: 164.2465, val_loss: 158.0347, val_MinusLogProbMetric: 158.0347

Epoch 71: val_loss did not improve from 134.94229
196/196 - 70s - loss: 164.2465 - MinusLogProbMetric: 164.2465 - val_loss: 158.0347 - val_MinusLogProbMetric: 158.0347 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 72/1000
2023-10-26 06:12:42.761 
Epoch 72/1000 
	 loss: 148.3513, MinusLogProbMetric: 148.3513, val_loss: 144.7569, val_MinusLogProbMetric: 144.7569

Epoch 72: val_loss did not improve from 134.94229
196/196 - 70s - loss: 148.3513 - MinusLogProbMetric: 148.3513 - val_loss: 144.7569 - val_MinusLogProbMetric: 144.7569 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 73/1000
2023-10-26 06:13:52.239 
Epoch 73/1000 
	 loss: 151.2611, MinusLogProbMetric: 151.2611, val_loss: 139.0307, val_MinusLogProbMetric: 139.0307

Epoch 73: val_loss did not improve from 134.94229
196/196 - 69s - loss: 151.2611 - MinusLogProbMetric: 151.2611 - val_loss: 139.0307 - val_MinusLogProbMetric: 139.0307 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 74/1000
2023-10-26 06:15:02.061 
Epoch 74/1000 
	 loss: 137.7796, MinusLogProbMetric: 137.7796, val_loss: 132.5085, val_MinusLogProbMetric: 132.5085

Epoch 74: val_loss improved from 134.94229 to 132.50847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 137.7796 - MinusLogProbMetric: 137.7796 - val_loss: 132.5085 - val_MinusLogProbMetric: 132.5085 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 75/1000
2023-10-26 06:16:13.257 
Epoch 75/1000 
	 loss: 132.2096, MinusLogProbMetric: 132.2096, val_loss: 129.1903, val_MinusLogProbMetric: 129.1903

Epoch 75: val_loss improved from 132.50847 to 129.19026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 132.2096 - MinusLogProbMetric: 132.2096 - val_loss: 129.1903 - val_MinusLogProbMetric: 129.1903 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 76/1000
2023-10-26 06:17:24.692 
Epoch 76/1000 
	 loss: 129.5926, MinusLogProbMetric: 129.5926, val_loss: 128.1461, val_MinusLogProbMetric: 128.1461

Epoch 76: val_loss improved from 129.19026 to 128.14607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 129.5926 - MinusLogProbMetric: 129.5926 - val_loss: 128.1461 - val_MinusLogProbMetric: 128.1461 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 77/1000
2023-10-26 06:18:35.847 
Epoch 77/1000 
	 loss: 125.5171, MinusLogProbMetric: 125.5171, val_loss: 122.9781, val_MinusLogProbMetric: 122.9781

Epoch 77: val_loss improved from 128.14607 to 122.97807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 125.5171 - MinusLogProbMetric: 125.5171 - val_loss: 122.9781 - val_MinusLogProbMetric: 122.9781 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 78/1000
2023-10-26 06:19:45.929 
Epoch 78/1000 
	 loss: 122.9469, MinusLogProbMetric: 122.9469, val_loss: 121.4443, val_MinusLogProbMetric: 121.4443

Epoch 78: val_loss improved from 122.97807 to 121.44427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 122.9469 - MinusLogProbMetric: 122.9469 - val_loss: 121.4443 - val_MinusLogProbMetric: 121.4443 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 79/1000
2023-10-26 06:20:56.946 
Epoch 79/1000 
	 loss: 120.8871, MinusLogProbMetric: 120.8871, val_loss: 119.1961, val_MinusLogProbMetric: 119.1961

Epoch 79: val_loss improved from 121.44427 to 119.19608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 120.8871 - MinusLogProbMetric: 120.8871 - val_loss: 119.1961 - val_MinusLogProbMetric: 119.1961 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 80/1000
2023-10-26 06:22:07.733 
Epoch 80/1000 
	 loss: 118.4760, MinusLogProbMetric: 118.4760, val_loss: 117.1441, val_MinusLogProbMetric: 117.1441

Epoch 80: val_loss improved from 119.19608 to 117.14413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 118.4760 - MinusLogProbMetric: 118.4760 - val_loss: 117.1441 - val_MinusLogProbMetric: 117.1441 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 81/1000
2023-10-26 06:23:18.552 
Epoch 81/1000 
	 loss: 115.9380, MinusLogProbMetric: 115.9380, val_loss: 115.0821, val_MinusLogProbMetric: 115.0821

Epoch 81: val_loss improved from 117.14413 to 115.08207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 115.9380 - MinusLogProbMetric: 115.9380 - val_loss: 115.0821 - val_MinusLogProbMetric: 115.0821 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 82/1000
2023-10-26 06:24:29.808 
Epoch 82/1000 
	 loss: 116.1076, MinusLogProbMetric: 116.1076, val_loss: 114.0324, val_MinusLogProbMetric: 114.0324

Epoch 82: val_loss improved from 115.08207 to 114.03237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 116.1076 - MinusLogProbMetric: 116.1076 - val_loss: 114.0324 - val_MinusLogProbMetric: 114.0324 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 83/1000
2023-10-26 06:25:40.771 
Epoch 83/1000 
	 loss: 112.4832, MinusLogProbMetric: 112.4832, val_loss: 112.4973, val_MinusLogProbMetric: 112.4973

Epoch 83: val_loss improved from 114.03237 to 112.49728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 112.4832 - MinusLogProbMetric: 112.4832 - val_loss: 112.4973 - val_MinusLogProbMetric: 112.4973 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 84/1000
2023-10-26 06:26:51.605 
Epoch 84/1000 
	 loss: 110.5203, MinusLogProbMetric: 110.5203, val_loss: 111.1128, val_MinusLogProbMetric: 111.1128

Epoch 84: val_loss improved from 112.49728 to 111.11280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 110.5203 - MinusLogProbMetric: 110.5203 - val_loss: 111.1128 - val_MinusLogProbMetric: 111.1128 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 85/1000
2023-10-26 06:28:02.821 
Epoch 85/1000 
	 loss: 108.8920, MinusLogProbMetric: 108.8920, val_loss: 107.8918, val_MinusLogProbMetric: 107.8918

Epoch 85: val_loss improved from 111.11280 to 107.89182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 108.8920 - MinusLogProbMetric: 108.8920 - val_loss: 107.8918 - val_MinusLogProbMetric: 107.8918 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 86/1000
2023-10-26 06:29:13.414 
Epoch 86/1000 
	 loss: 108.3827, MinusLogProbMetric: 108.3827, val_loss: 107.7623, val_MinusLogProbMetric: 107.7623

Epoch 86: val_loss improved from 107.89182 to 107.76232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 108.3827 - MinusLogProbMetric: 108.3827 - val_loss: 107.7623 - val_MinusLogProbMetric: 107.7623 - lr: 1.1111e-04 - 70s/epoch - 360ms/step
Epoch 87/1000
2023-10-26 06:30:24.545 
Epoch 87/1000 
	 loss: 105.8833, MinusLogProbMetric: 105.8833, val_loss: 105.4763, val_MinusLogProbMetric: 105.4763

Epoch 87: val_loss improved from 107.76232 to 105.47627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 105.8833 - MinusLogProbMetric: 105.8833 - val_loss: 105.4763 - val_MinusLogProbMetric: 105.4763 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 88/1000
2023-10-26 06:31:35.865 
Epoch 88/1000 
	 loss: 106.8063, MinusLogProbMetric: 106.8063, val_loss: 105.4546, val_MinusLogProbMetric: 105.4546

Epoch 88: val_loss improved from 105.47627 to 105.45464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 106.8063 - MinusLogProbMetric: 106.8063 - val_loss: 105.4546 - val_MinusLogProbMetric: 105.4546 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 89/1000
2023-10-26 06:32:47.436 
Epoch 89/1000 
	 loss: 103.2783, MinusLogProbMetric: 103.2783, val_loss: 101.9555, val_MinusLogProbMetric: 101.9555

Epoch 89: val_loss improved from 105.45464 to 101.95554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 103.2783 - MinusLogProbMetric: 103.2783 - val_loss: 101.9555 - val_MinusLogProbMetric: 101.9555 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 90/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 06:33:59.378 
Epoch 90/1000 
	 loss: 101.6646, MinusLogProbMetric: 101.6646, val_loss: 101.3067, val_MinusLogProbMetric: 101.3067

Epoch 90: val_loss improved from 101.95554 to 101.30675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 101.6646 - MinusLogProbMetric: 101.6646 - val_loss: 101.3067 - val_MinusLogProbMetric: 101.3067 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 91/1000
2023-10-26 06:35:10.689 
Epoch 91/1000 
	 loss: 103.4704, MinusLogProbMetric: 103.4704, val_loss: 102.5438, val_MinusLogProbMetric: 102.5438

Epoch 91: val_loss did not improve from 101.30675
196/196 - 70s - loss: 103.4704 - MinusLogProbMetric: 103.4704 - val_loss: 102.5438 - val_MinusLogProbMetric: 102.5438 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 92/1000
2023-10-26 06:36:20.724 
Epoch 92/1000 
	 loss: 100.4515, MinusLogProbMetric: 100.4515, val_loss: 115.2305, val_MinusLogProbMetric: 115.2305

Epoch 92: val_loss did not improve from 101.30675
196/196 - 70s - loss: 100.4515 - MinusLogProbMetric: 100.4515 - val_loss: 115.2305 - val_MinusLogProbMetric: 115.2305 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 93/1000
2023-10-26 06:37:30.384 
Epoch 93/1000 
	 loss: 143.2894, MinusLogProbMetric: 143.2894, val_loss: 112.4112, val_MinusLogProbMetric: 112.4112

Epoch 93: val_loss did not improve from 101.30675
196/196 - 70s - loss: 143.2894 - MinusLogProbMetric: 143.2894 - val_loss: 112.4112 - val_MinusLogProbMetric: 112.4112 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 94/1000
2023-10-26 06:38:40.854 
Epoch 94/1000 
	 loss: 106.7925, MinusLogProbMetric: 106.7925, val_loss: 104.4115, val_MinusLogProbMetric: 104.4115

Epoch 94: val_loss did not improve from 101.30675
196/196 - 70s - loss: 106.7925 - MinusLogProbMetric: 106.7925 - val_loss: 104.4115 - val_MinusLogProbMetric: 104.4115 - lr: 1.1111e-04 - 70s/epoch - 360ms/step
Epoch 95/1000
2023-10-26 06:39:50.306 
Epoch 95/1000 
	 loss: 102.1442, MinusLogProbMetric: 102.1442, val_loss: 102.6082, val_MinusLogProbMetric: 102.6082

Epoch 95: val_loss did not improve from 101.30675
196/196 - 69s - loss: 102.1442 - MinusLogProbMetric: 102.1442 - val_loss: 102.6082 - val_MinusLogProbMetric: 102.6082 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 96/1000
2023-10-26 06:41:00.092 
Epoch 96/1000 
	 loss: 100.2218, MinusLogProbMetric: 100.2218, val_loss: 99.2843, val_MinusLogProbMetric: 99.2843

Epoch 96: val_loss improved from 101.30675 to 99.28426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 100.2218 - MinusLogProbMetric: 100.2218 - val_loss: 99.2843 - val_MinusLogProbMetric: 99.2843 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 97/1000
2023-10-26 06:42:11.789 
Epoch 97/1000 
	 loss: 98.0547, MinusLogProbMetric: 98.0547, val_loss: 97.6236, val_MinusLogProbMetric: 97.6236

Epoch 97: val_loss improved from 99.28426 to 97.62357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 98.0547 - MinusLogProbMetric: 98.0547 - val_loss: 97.6236 - val_MinusLogProbMetric: 97.6236 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 98/1000
2023-10-26 06:43:22.285 
Epoch 98/1000 
	 loss: 98.9768, MinusLogProbMetric: 98.9768, val_loss: 101.8726, val_MinusLogProbMetric: 101.8726

Epoch 98: val_loss did not improve from 97.62357
196/196 - 69s - loss: 98.9768 - MinusLogProbMetric: 98.9768 - val_loss: 101.8726 - val_MinusLogProbMetric: 101.8726 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 99/1000
2023-10-26 06:44:31.138 
Epoch 99/1000 
	 loss: 96.8153, MinusLogProbMetric: 96.8153, val_loss: 95.0668, val_MinusLogProbMetric: 95.0668

Epoch 99: val_loss improved from 97.62357 to 95.06680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 70s - loss: 96.8153 - MinusLogProbMetric: 96.8153 - val_loss: 95.0668 - val_MinusLogProbMetric: 95.0668 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 100/1000
2023-10-26 06:45:41.857 
Epoch 100/1000 
	 loss: 94.8605, MinusLogProbMetric: 94.8605, val_loss: 93.2676, val_MinusLogProbMetric: 93.2676

Epoch 100: val_loss improved from 95.06680 to 93.26762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 94.8605 - MinusLogProbMetric: 94.8605 - val_loss: 93.2676 - val_MinusLogProbMetric: 93.2676 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 101/1000
2023-10-26 06:46:52.922 
Epoch 101/1000 
	 loss: 97.8283, MinusLogProbMetric: 97.8283, val_loss: 93.6866, val_MinusLogProbMetric: 93.6866

Epoch 101: val_loss did not improve from 93.26762
196/196 - 70s - loss: 97.8283 - MinusLogProbMetric: 97.8283 - val_loss: 93.6866 - val_MinusLogProbMetric: 93.6866 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 102/1000
2023-10-26 06:48:02.488 
Epoch 102/1000 
	 loss: 91.8147, MinusLogProbMetric: 91.8147, val_loss: 91.2535, val_MinusLogProbMetric: 91.2535

Epoch 102: val_loss improved from 93.26762 to 91.25347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 91.8147 - MinusLogProbMetric: 91.8147 - val_loss: 91.2535 - val_MinusLogProbMetric: 91.2535 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 103/1000
2023-10-26 06:49:13.209 
Epoch 103/1000 
	 loss: 93.1773, MinusLogProbMetric: 93.1773, val_loss: 98.9598, val_MinusLogProbMetric: 98.9598

Epoch 103: val_loss did not improve from 91.25347
196/196 - 70s - loss: 93.1773 - MinusLogProbMetric: 93.1773 - val_loss: 98.9598 - val_MinusLogProbMetric: 98.9598 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 104/1000
2023-10-26 06:50:22.934 
Epoch 104/1000 
	 loss: 90.6820, MinusLogProbMetric: 90.6820, val_loss: 90.0428, val_MinusLogProbMetric: 90.0428

Epoch 104: val_loss improved from 91.25347 to 90.04276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 90.6820 - MinusLogProbMetric: 90.6820 - val_loss: 90.0428 - val_MinusLogProbMetric: 90.0428 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 105/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 111: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 06:51:05.969 
Epoch 105/1000 
	 loss: nan, MinusLogProbMetric: 93.9183, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 105: val_loss did not improve from 90.04276
196/196 - 42s - loss: nan - MinusLogProbMetric: 93.9183 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 42s/epoch - 214ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 3.703703703703703e-05.
===========
Generating train data for run 374.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_297"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_298 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7ff34d147a00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff34d0bc8b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff34d0bc8b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fecae10aa40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff34cf86a10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff34cf86f80>, <keras.callbacks.ModelCheckpoint object at 0x7ff34cf87040>, <keras.callbacks.EarlyStopping object at 0x7ff34cf872b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff34cf872e0>, <keras.callbacks.TerminateOnNaN object at 0x7ff34cf86f20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 06:51:17.524524
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 108: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 06:55:17.179 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 238.0800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 239s - loss: nan - MinusLogProbMetric: 238.0800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 239s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.2345679012345677e-05.
===========
Generating train data for run 374.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_308"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_309 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7fea32151ea0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecae943580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecae943580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fecce10d330>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecafda26e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecafda2c50>, <keras.callbacks.ModelCheckpoint object at 0x7fecafda2d10>, <keras.callbacks.EarlyStopping object at 0x7fecafda2f80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecafda2fb0>, <keras.callbacks.TerminateOnNaN object at 0x7fecafda2bf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 06:55:27.116103
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 06:59:51.450 
Epoch 1/1000 
	 loss: 96.7368, MinusLogProbMetric: 96.7368, val_loss: 87.0610, val_MinusLogProbMetric: 87.0610

Epoch 1: val_loss improved from inf to 87.06095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 266s - loss: 96.7368 - MinusLogProbMetric: 96.7368 - val_loss: 87.0610 - val_MinusLogProbMetric: 87.0610 - lr: 1.2346e-05 - 266s/epoch - 1s/step
Epoch 2/1000
2023-10-26 07:01:06.153 
Epoch 2/1000 
	 loss: 87.5502, MinusLogProbMetric: 87.5502, val_loss: 88.5714, val_MinusLogProbMetric: 88.5714

Epoch 2: val_loss did not improve from 87.06095
196/196 - 73s - loss: 87.5502 - MinusLogProbMetric: 87.5502 - val_loss: 88.5714 - val_MinusLogProbMetric: 88.5714 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 3/1000
2023-10-26 07:02:18.736 
Epoch 3/1000 
	 loss: 84.8351, MinusLogProbMetric: 84.8351, val_loss: 83.7869, val_MinusLogProbMetric: 83.7869

Epoch 3: val_loss improved from 87.06095 to 83.78688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 84.8351 - MinusLogProbMetric: 84.8351 - val_loss: 83.7869 - val_MinusLogProbMetric: 83.7869 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 4/1000
2023-10-26 07:03:32.557 
Epoch 4/1000 
	 loss: 88.7564, MinusLogProbMetric: 88.7564, val_loss: 82.0008, val_MinusLogProbMetric: 82.0008

Epoch 4: val_loss improved from 83.78688 to 82.00079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 88.7564 - MinusLogProbMetric: 88.7564 - val_loss: 82.0008 - val_MinusLogProbMetric: 82.0008 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 5/1000
2023-10-26 07:04:46.411 
Epoch 5/1000 
	 loss: 83.1828, MinusLogProbMetric: 83.1828, val_loss: 81.6428, val_MinusLogProbMetric: 81.6428

Epoch 5: val_loss improved from 82.00079 to 81.64285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 83.1828 - MinusLogProbMetric: 83.1828 - val_loss: 81.6428 - val_MinusLogProbMetric: 81.6428 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 6/1000
2023-10-26 07:06:00.256 
Epoch 6/1000 
	 loss: 79.5001, MinusLogProbMetric: 79.5001, val_loss: 81.6912, val_MinusLogProbMetric: 81.6912

Epoch 6: val_loss did not improve from 81.64285
196/196 - 73s - loss: 79.5001 - MinusLogProbMetric: 79.5001 - val_loss: 81.6912 - val_MinusLogProbMetric: 81.6912 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 7/1000
2023-10-26 07:07:12.761 
Epoch 7/1000 
	 loss: 98.9578, MinusLogProbMetric: 98.9578, val_loss: 129.9275, val_MinusLogProbMetric: 129.9275

Epoch 7: val_loss did not improve from 81.64285
196/196 - 73s - loss: 98.9578 - MinusLogProbMetric: 98.9578 - val_loss: 129.9275 - val_MinusLogProbMetric: 129.9275 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 8/1000
2023-10-26 07:08:25.675 
Epoch 8/1000 
	 loss: 94.3730, MinusLogProbMetric: 94.3730, val_loss: 82.8764, val_MinusLogProbMetric: 82.8764

Epoch 8: val_loss did not improve from 81.64285
196/196 - 73s - loss: 94.3730 - MinusLogProbMetric: 94.3730 - val_loss: 82.8764 - val_MinusLogProbMetric: 82.8764 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 9/1000
2023-10-26 07:09:38.794 
Epoch 9/1000 
	 loss: 83.2482, MinusLogProbMetric: 83.2482, val_loss: 79.5742, val_MinusLogProbMetric: 79.5742

Epoch 9: val_loss improved from 81.64285 to 79.57419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 83.2482 - MinusLogProbMetric: 83.2482 - val_loss: 79.5742 - val_MinusLogProbMetric: 79.5742 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 10/1000
2023-10-26 07:10:52.776 
Epoch 10/1000 
	 loss: 78.4218, MinusLogProbMetric: 78.4218, val_loss: 79.0245, val_MinusLogProbMetric: 79.0245

Epoch 10: val_loss improved from 79.57419 to 79.02453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 78.4218 - MinusLogProbMetric: 78.4218 - val_loss: 79.0245 - val_MinusLogProbMetric: 79.0245 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 11/1000
2023-10-26 07:12:06.638 
Epoch 11/1000 
	 loss: 76.0066, MinusLogProbMetric: 76.0066, val_loss: 76.2574, val_MinusLogProbMetric: 76.2574

Epoch 11: val_loss improved from 79.02453 to 76.25745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 76.0066 - MinusLogProbMetric: 76.0066 - val_loss: 76.2574 - val_MinusLogProbMetric: 76.2574 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 12/1000
2023-10-26 07:13:20.464 
Epoch 12/1000 
	 loss: 74.2032, MinusLogProbMetric: 74.2032, val_loss: 73.6125, val_MinusLogProbMetric: 73.6125

Epoch 12: val_loss improved from 76.25745 to 73.61250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 74.2032 - MinusLogProbMetric: 74.2032 - val_loss: 73.6125 - val_MinusLogProbMetric: 73.6125 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 13/1000
2023-10-26 07:14:33.974 
Epoch 13/1000 
	 loss: 129.0067, MinusLogProbMetric: 129.0067, val_loss: 104.2653, val_MinusLogProbMetric: 104.2653

Epoch 13: val_loss did not improve from 73.61250
196/196 - 72s - loss: 129.0067 - MinusLogProbMetric: 129.0067 - val_loss: 104.2653 - val_MinusLogProbMetric: 104.2653 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 14/1000
2023-10-26 07:15:46.851 
Epoch 14/1000 
	 loss: 95.4020, MinusLogProbMetric: 95.4020, val_loss: 89.9046, val_MinusLogProbMetric: 89.9046

Epoch 14: val_loss did not improve from 73.61250
196/196 - 73s - loss: 95.4020 - MinusLogProbMetric: 95.4020 - val_loss: 89.9046 - val_MinusLogProbMetric: 89.9046 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 15/1000
2023-10-26 07:16:59.824 
Epoch 15/1000 
	 loss: 87.9233, MinusLogProbMetric: 87.9233, val_loss: 87.6027, val_MinusLogProbMetric: 87.6027

Epoch 15: val_loss did not improve from 73.61250
196/196 - 73s - loss: 87.9233 - MinusLogProbMetric: 87.9233 - val_loss: 87.6027 - val_MinusLogProbMetric: 87.6027 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 16/1000
2023-10-26 07:18:12.964 
Epoch 16/1000 
	 loss: 82.6045, MinusLogProbMetric: 82.6045, val_loss: 80.0288, val_MinusLogProbMetric: 80.0288

Epoch 16: val_loss did not improve from 73.61250
196/196 - 73s - loss: 82.6045 - MinusLogProbMetric: 82.6045 - val_loss: 80.0288 - val_MinusLogProbMetric: 80.0288 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 17/1000
2023-10-26 07:19:25.663 
Epoch 17/1000 
	 loss: 77.5702, MinusLogProbMetric: 77.5702, val_loss: 76.2567, val_MinusLogProbMetric: 76.2567

Epoch 17: val_loss did not improve from 73.61250
196/196 - 73s - loss: 77.5702 - MinusLogProbMetric: 77.5702 - val_loss: 76.2567 - val_MinusLogProbMetric: 76.2567 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 18/1000
2023-10-26 07:20:38.123 
Epoch 18/1000 
	 loss: 75.3390, MinusLogProbMetric: 75.3390, val_loss: 75.4243, val_MinusLogProbMetric: 75.4243

Epoch 18: val_loss did not improve from 73.61250
196/196 - 72s - loss: 75.3390 - MinusLogProbMetric: 75.3390 - val_loss: 75.4243 - val_MinusLogProbMetric: 75.4243 - lr: 1.2346e-05 - 72s/epoch - 370ms/step
Epoch 19/1000
2023-10-26 07:21:51.233 
Epoch 19/1000 
	 loss: 73.4740, MinusLogProbMetric: 73.4740, val_loss: 72.8816, val_MinusLogProbMetric: 72.8816

Epoch 19: val_loss improved from 73.61250 to 72.88163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 73.4740 - MinusLogProbMetric: 73.4740 - val_loss: 72.8816 - val_MinusLogProbMetric: 72.8816 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 20/1000
2023-10-26 07:23:05.756 
Epoch 20/1000 
	 loss: 72.1236, MinusLogProbMetric: 72.1236, val_loss: 72.5307, val_MinusLogProbMetric: 72.5307

Epoch 20: val_loss improved from 72.88163 to 72.53073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 75s - loss: 72.1236 - MinusLogProbMetric: 72.1236 - val_loss: 72.5307 - val_MinusLogProbMetric: 72.5307 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 21/1000
2023-10-26 07:24:19.761 
Epoch 21/1000 
	 loss: 71.0804, MinusLogProbMetric: 71.0804, val_loss: 70.7614, val_MinusLogProbMetric: 70.7614

Epoch 21: val_loss improved from 72.53073 to 70.76142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 71.0804 - MinusLogProbMetric: 71.0804 - val_loss: 70.7614 - val_MinusLogProbMetric: 70.7614 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 22/1000
2023-10-26 07:25:33.278 
Epoch 22/1000 
	 loss: 71.9542, MinusLogProbMetric: 71.9542, val_loss: 134.1534, val_MinusLogProbMetric: 134.1534

Epoch 22: val_loss did not improve from 70.76142
196/196 - 73s - loss: 71.9542 - MinusLogProbMetric: 71.9542 - val_loss: 134.1534 - val_MinusLogProbMetric: 134.1534 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 23/1000
2023-10-26 07:26:46.071 
Epoch 23/1000 
	 loss: 84.4170, MinusLogProbMetric: 84.4170, val_loss: 72.9107, val_MinusLogProbMetric: 72.9107

Epoch 23: val_loss did not improve from 70.76142
196/196 - 73s - loss: 84.4170 - MinusLogProbMetric: 84.4170 - val_loss: 72.9107 - val_MinusLogProbMetric: 72.9107 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 24/1000
2023-10-26 07:27:58.637 
Epoch 24/1000 
	 loss: 71.2455, MinusLogProbMetric: 71.2455, val_loss: 70.6132, val_MinusLogProbMetric: 70.6132

Epoch 24: val_loss improved from 70.76142 to 70.61323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 71.2455 - MinusLogProbMetric: 71.2455 - val_loss: 70.6132 - val_MinusLogProbMetric: 70.6132 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 25/1000
2023-10-26 07:29:12.828 
Epoch 25/1000 
	 loss: 69.3652, MinusLogProbMetric: 69.3652, val_loss: 68.4285, val_MinusLogProbMetric: 68.4285

Epoch 25: val_loss improved from 70.61323 to 68.42852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 69.3652 - MinusLogProbMetric: 69.3652 - val_loss: 68.4285 - val_MinusLogProbMetric: 68.4285 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 26/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 51: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 07:29:36.682 
Epoch 26/1000 
	 loss: nan, MinusLogProbMetric: 72.2747, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 26: val_loss did not improve from 68.42852
196/196 - 23s - loss: nan - MinusLogProbMetric: 72.2747 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 23s/epoch - 116ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.115226337448558e-06.
===========
Generating train data for run 374.
===========
Train data generated in 0.44 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_319"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_320 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7fea12e9ebf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fedc434a170>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fedc434a170>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fecac297640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef486ba680>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef486bad40>, <keras.callbacks.ModelCheckpoint object at 0x7fef486baaa0>, <keras.callbacks.EarlyStopping object at 0x7fef486b9ba0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef486b8c70>, <keras.callbacks.TerminateOnNaN object at 0x7fef486bbd30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 07:29:47.965334
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 07:33:50.925 
Epoch 1/1000 
	 loss: 67.9828, MinusLogProbMetric: 67.9828, val_loss: 66.8114, val_MinusLogProbMetric: 66.8114

Epoch 1: val_loss improved from inf to 66.81136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 244s - loss: 67.9828 - MinusLogProbMetric: 67.9828 - val_loss: 66.8114 - val_MinusLogProbMetric: 66.8114 - lr: 4.1152e-06 - 244s/epoch - 1s/step
Epoch 2/1000
2023-10-26 07:35:03.403 
Epoch 2/1000 
	 loss: 66.1664, MinusLogProbMetric: 66.1664, val_loss: 65.7927, val_MinusLogProbMetric: 65.7927

Epoch 2: val_loss improved from 66.81136 to 65.79266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 66.1664 - MinusLogProbMetric: 66.1664 - val_loss: 65.7927 - val_MinusLogProbMetric: 65.7927 - lr: 4.1152e-06 - 72s/epoch - 369ms/step
Epoch 3/1000
2023-10-26 07:36:15.934 
Epoch 3/1000 
	 loss: 65.0468, MinusLogProbMetric: 65.0468, val_loss: 64.6041, val_MinusLogProbMetric: 64.6041

Epoch 3: val_loss improved from 65.79266 to 64.60409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 65.0468 - MinusLogProbMetric: 65.0468 - val_loss: 64.6041 - val_MinusLogProbMetric: 64.6041 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 4/1000
2023-10-26 07:37:28.484 
Epoch 4/1000 
	 loss: 65.2667, MinusLogProbMetric: 65.2667, val_loss: 63.5893, val_MinusLogProbMetric: 63.5893

Epoch 4: val_loss improved from 64.60409 to 63.58930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 73s - loss: 65.2667 - MinusLogProbMetric: 65.2667 - val_loss: 63.5893 - val_MinusLogProbMetric: 63.5893 - lr: 4.1152e-06 - 73s/epoch - 371ms/step
Epoch 5/1000
2023-10-26 07:38:40.253 
Epoch 5/1000 
	 loss: 63.4164, MinusLogProbMetric: 63.4164, val_loss: 62.4129, val_MinusLogProbMetric: 62.4129

Epoch 5: val_loss improved from 63.58930 to 62.41292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 63.4164 - MinusLogProbMetric: 63.4164 - val_loss: 62.4129 - val_MinusLogProbMetric: 62.4129 - lr: 4.1152e-06 - 72s/epoch - 366ms/step
Epoch 6/1000
2023-10-26 07:39:52.234 
Epoch 6/1000 
	 loss: 62.5330, MinusLogProbMetric: 62.5330, val_loss: 62.4599, val_MinusLogProbMetric: 62.4599

Epoch 6: val_loss did not improve from 62.41292
196/196 - 71s - loss: 62.5330 - MinusLogProbMetric: 62.5330 - val_loss: 62.4599 - val_MinusLogProbMetric: 62.4599 - lr: 4.1152e-06 - 71s/epoch - 361ms/step
Epoch 7/1000
2023-10-26 07:41:02.152 
Epoch 7/1000 
	 loss: 61.4841, MinusLogProbMetric: 61.4841, val_loss: 66.8949, val_MinusLogProbMetric: 66.8949

Epoch 7: val_loss did not improve from 62.41292
196/196 - 70s - loss: 61.4841 - MinusLogProbMetric: 61.4841 - val_loss: 66.8949 - val_MinusLogProbMetric: 66.8949 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 8/1000
2023-10-26 07:42:12.262 
Epoch 8/1000 
	 loss: 60.9695, MinusLogProbMetric: 60.9695, val_loss: 60.9231, val_MinusLogProbMetric: 60.9231

Epoch 8: val_loss improved from 62.41292 to 60.92308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 60.9695 - MinusLogProbMetric: 60.9695 - val_loss: 60.9231 - val_MinusLogProbMetric: 60.9231 - lr: 4.1152e-06 - 71s/epoch - 364ms/step
Epoch 9/1000
2023-10-26 07:43:24.056 
Epoch 9/1000 
	 loss: 60.5364, MinusLogProbMetric: 60.5364, val_loss: 60.6213, val_MinusLogProbMetric: 60.6213

Epoch 9: val_loss improved from 60.92308 to 60.62128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 60.5364 - MinusLogProbMetric: 60.5364 - val_loss: 60.6213 - val_MinusLogProbMetric: 60.6213 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 10/1000
2023-10-26 07:44:35.883 
Epoch 10/1000 
	 loss: 59.4028, MinusLogProbMetric: 59.4028, val_loss: 59.2210, val_MinusLogProbMetric: 59.2210

Epoch 10: val_loss improved from 60.62128 to 59.22095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 59.4028 - MinusLogProbMetric: 59.4028 - val_loss: 59.2210 - val_MinusLogProbMetric: 59.2210 - lr: 4.1152e-06 - 71s/epoch - 365ms/step
Epoch 11/1000
2023-10-26 07:45:46.520 
Epoch 11/1000 
	 loss: 58.7986, MinusLogProbMetric: 58.7986, val_loss: 58.4420, val_MinusLogProbMetric: 58.4420

Epoch 11: val_loss improved from 59.22095 to 58.44199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 71s - loss: 58.7986 - MinusLogProbMetric: 58.7986 - val_loss: 58.4420 - val_MinusLogProbMetric: 58.4420 - lr: 4.1152e-06 - 71s/epoch - 360ms/step
Epoch 12/1000
2023-10-26 07:46:58.192 
Epoch 12/1000 
	 loss: 58.0599, MinusLogProbMetric: 58.0599, val_loss: 57.9258, val_MinusLogProbMetric: 57.9258

Epoch 12: val_loss improved from 58.44199 to 57.92578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 58.0599 - MinusLogProbMetric: 58.0599 - val_loss: 57.9258 - val_MinusLogProbMetric: 57.9258 - lr: 4.1152e-06 - 72s/epoch - 366ms/step
Epoch 13/1000
2023-10-26 07:48:10.298 
Epoch 13/1000 
	 loss: 58.4537, MinusLogProbMetric: 58.4537, val_loss: 58.6113, val_MinusLogProbMetric: 58.6113

Epoch 13: val_loss did not improve from 57.92578
196/196 - 71s - loss: 58.4537 - MinusLogProbMetric: 58.4537 - val_loss: 58.6113 - val_MinusLogProbMetric: 58.6113 - lr: 4.1152e-06 - 71s/epoch - 362ms/step
Epoch 14/1000
2023-10-26 07:49:21.428 
Epoch 14/1000 
	 loss: 57.1706, MinusLogProbMetric: 57.1706, val_loss: 57.0438, val_MinusLogProbMetric: 57.0438

Epoch 14: val_loss improved from 57.92578 to 57.04375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 57.1706 - MinusLogProbMetric: 57.1706 - val_loss: 57.0438 - val_MinusLogProbMetric: 57.0438 - lr: 4.1152e-06 - 72s/epoch - 369ms/step
Epoch 15/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 76: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 07:49:53.483 
Epoch 15/1000 
	 loss: nan, MinusLogProbMetric: 56.8680, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 15: val_loss did not improve from 57.04375
196/196 - 31s - loss: nan - MinusLogProbMetric: 56.8680 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 31s/epoch - 158ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.3717421124828526e-06.
===========
Generating train data for run 374.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_330"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_331 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7ff34cedba60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea11eb9240>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea11eb9240>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feba77a3fa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff34c8a9ba0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff34c8aa110>, <keras.callbacks.ModelCheckpoint object at 0x7ff34c8aa1d0>, <keras.callbacks.EarlyStopping object at 0x7ff34c8aa440>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff34c8aa470>, <keras.callbacks.TerminateOnNaN object at 0x7ff34c8aa0b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 07:50:05.124175
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 07:54:40.027 
Epoch 1/1000 
	 loss: 56.2407, MinusLogProbMetric: 56.2407, val_loss: 56.1457, val_MinusLogProbMetric: 56.1457

Epoch 1: val_loss improved from inf to 56.14573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 276s - loss: 56.2407 - MinusLogProbMetric: 56.2407 - val_loss: 56.1457 - val_MinusLogProbMetric: 56.1457 - lr: 1.3717e-06 - 276s/epoch - 1s/step
Epoch 2/1000
2023-10-26 07:55:55.177 
Epoch 2/1000 
	 loss: 55.9229, MinusLogProbMetric: 55.9229, val_loss: 55.9243, val_MinusLogProbMetric: 55.9243

Epoch 2: val_loss improved from 56.14573 to 55.92428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 75s - loss: 55.9229 - MinusLogProbMetric: 55.9229 - val_loss: 55.9243 - val_MinusLogProbMetric: 55.9243 - lr: 1.3717e-06 - 75s/epoch - 381ms/step
Epoch 3/1000
2023-10-26 07:57:09.423 
Epoch 3/1000 
	 loss: 55.8765, MinusLogProbMetric: 55.8765, val_loss: 55.8145, val_MinusLogProbMetric: 55.8145

Epoch 3: val_loss improved from 55.92428 to 55.81449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 55.8765 - MinusLogProbMetric: 55.8765 - val_loss: 55.8145 - val_MinusLogProbMetric: 55.8145 - lr: 1.3717e-06 - 74s/epoch - 378ms/step
Epoch 4/1000
2023-10-26 07:58:17.456 
Epoch 4/1000 
	 loss: 55.6065, MinusLogProbMetric: 55.6065, val_loss: 55.5829, val_MinusLogProbMetric: 55.5829

Epoch 4: val_loss improved from 55.81449 to 55.58290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 55.6065 - MinusLogProbMetric: 55.6065 - val_loss: 55.5829 - val_MinusLogProbMetric: 55.5829 - lr: 1.3717e-06 - 68s/epoch - 346ms/step
Epoch 5/1000
2023-10-26 07:59:15.032 
Epoch 5/1000 
	 loss: 55.4370, MinusLogProbMetric: 55.4370, val_loss: 55.3533, val_MinusLogProbMetric: 55.3533

Epoch 5: val_loss improved from 55.58290 to 55.35334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 55.4370 - MinusLogProbMetric: 55.4370 - val_loss: 55.3533 - val_MinusLogProbMetric: 55.3533 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 6/1000
2023-10-26 08:00:13.505 
Epoch 6/1000 
	 loss: 56.2535, MinusLogProbMetric: 56.2535, val_loss: 55.4393, val_MinusLogProbMetric: 55.4393

Epoch 6: val_loss did not improve from 55.35334
196/196 - 58s - loss: 56.2535 - MinusLogProbMetric: 56.2535 - val_loss: 55.4393 - val_MinusLogProbMetric: 55.4393 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 7/1000
2023-10-26 08:01:24.783 
Epoch 7/1000 
	 loss: 55.0500, MinusLogProbMetric: 55.0500, val_loss: 55.0763, val_MinusLogProbMetric: 55.0763

Epoch 7: val_loss improved from 55.35334 to 55.07629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 72s - loss: 55.0500 - MinusLogProbMetric: 55.0500 - val_loss: 55.0763 - val_MinusLogProbMetric: 55.0763 - lr: 1.3717e-06 - 72s/epoch - 370ms/step
Epoch 8/1000
2023-10-26 08:02:35.112 
Epoch 8/1000 
	 loss: 54.7523, MinusLogProbMetric: 54.7523, val_loss: 58.8111, val_MinusLogProbMetric: 58.8111

Epoch 8: val_loss did not improve from 55.07629
196/196 - 69s - loss: 54.7523 - MinusLogProbMetric: 54.7523 - val_loss: 58.8111 - val_MinusLogProbMetric: 58.8111 - lr: 1.3717e-06 - 69s/epoch - 353ms/step
Epoch 9/1000
2023-10-26 08:03:47.960 
Epoch 9/1000 
	 loss: 58.1220, MinusLogProbMetric: 58.1220, val_loss: 55.6915, val_MinusLogProbMetric: 55.6915

Epoch 9: val_loss did not improve from 55.07629
196/196 - 73s - loss: 58.1220 - MinusLogProbMetric: 58.1220 - val_loss: 55.6915 - val_MinusLogProbMetric: 55.6915 - lr: 1.3717e-06 - 73s/epoch - 372ms/step
Epoch 10/1000
2023-10-26 08:05:00.066 
Epoch 10/1000 
	 loss: 54.9993, MinusLogProbMetric: 54.9993, val_loss: 54.7543, val_MinusLogProbMetric: 54.7543

Epoch 10: val_loss improved from 55.07629 to 54.75435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 73s - loss: 54.9993 - MinusLogProbMetric: 54.9993 - val_loss: 54.7543 - val_MinusLogProbMetric: 54.7543 - lr: 1.3717e-06 - 73s/epoch - 374ms/step
Epoch 11/1000
2023-10-26 08:06:13.961 
Epoch 11/1000 
	 loss: 54.4827, MinusLogProbMetric: 54.4827, val_loss: 54.4721, val_MinusLogProbMetric: 54.4721

Epoch 11: val_loss improved from 54.75435 to 54.47208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 54.4827 - MinusLogProbMetric: 54.4827 - val_loss: 54.4721 - val_MinusLogProbMetric: 54.4721 - lr: 1.3717e-06 - 74s/epoch - 377ms/step
Epoch 12/1000
2023-10-26 08:07:27.103 
Epoch 12/1000 
	 loss: 54.2150, MinusLogProbMetric: 54.2150, val_loss: 54.3476, val_MinusLogProbMetric: 54.3476

Epoch 12: val_loss improved from 54.47208 to 54.34764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 73s - loss: 54.2150 - MinusLogProbMetric: 54.2150 - val_loss: 54.3476 - val_MinusLogProbMetric: 54.3476 - lr: 1.3717e-06 - 73s/epoch - 373ms/step
Epoch 13/1000
2023-10-26 08:08:41.517 
Epoch 13/1000 
	 loss: 54.0336, MinusLogProbMetric: 54.0336, val_loss: 54.1930, val_MinusLogProbMetric: 54.1930

Epoch 13: val_loss improved from 54.34764 to 54.19303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 74s - loss: 54.0336 - MinusLogProbMetric: 54.0336 - val_loss: 54.1930 - val_MinusLogProbMetric: 54.1930 - lr: 1.3717e-06 - 74s/epoch - 380ms/step
Epoch 14/1000
2023-10-26 08:09:55.233 
Epoch 14/1000 
	 loss: 54.2732, MinusLogProbMetric: 54.2732, val_loss: 54.9852, val_MinusLogProbMetric: 54.9852

Epoch 14: val_loss did not improve from 54.19303
196/196 - 73s - loss: 54.2732 - MinusLogProbMetric: 54.2732 - val_loss: 54.9852 - val_MinusLogProbMetric: 54.9852 - lr: 1.3717e-06 - 73s/epoch - 370ms/step
Epoch 15/1000
2023-10-26 08:10:52.236 
Epoch 15/1000 
	 loss: 53.7856, MinusLogProbMetric: 53.7856, val_loss: 53.7265, val_MinusLogProbMetric: 53.7265

Epoch 15: val_loss improved from 54.19303 to 53.72646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 53.7856 - MinusLogProbMetric: 53.7856 - val_loss: 53.7265 - val_MinusLogProbMetric: 53.7265 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 16/1000
2023-10-26 08:11:54.170 
Epoch 16/1000 
	 loss: 53.5458, MinusLogProbMetric: 53.5458, val_loss: 53.7000, val_MinusLogProbMetric: 53.7000

Epoch 16: val_loss improved from 53.72646 to 53.70002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 53.5458 - MinusLogProbMetric: 53.5458 - val_loss: 53.7000 - val_MinusLogProbMetric: 53.7000 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 17/1000
2023-10-26 08:12:55.044 
Epoch 17/1000 
	 loss: 53.5466, MinusLogProbMetric: 53.5466, val_loss: 54.2365, val_MinusLogProbMetric: 54.2365

Epoch 17: val_loss did not improve from 53.70002
196/196 - 60s - loss: 53.5466 - MinusLogProbMetric: 53.5466 - val_loss: 54.2365 - val_MinusLogProbMetric: 54.2365 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 18/1000
2023-10-26 08:13:54.180 
Epoch 18/1000 
	 loss: 53.3682, MinusLogProbMetric: 53.3682, val_loss: 53.7805, val_MinusLogProbMetric: 53.7805

Epoch 18: val_loss did not improve from 53.70002
196/196 - 59s - loss: 53.3682 - MinusLogProbMetric: 53.3682 - val_loss: 53.7805 - val_MinusLogProbMetric: 53.7805 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 19/1000
2023-10-26 08:14:57.063 
Epoch 19/1000 
	 loss: 53.2005, MinusLogProbMetric: 53.2005, val_loss: 53.2514, val_MinusLogProbMetric: 53.2514

Epoch 19: val_loss improved from 53.70002 to 53.25137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 53.2005 - MinusLogProbMetric: 53.2005 - val_loss: 53.2514 - val_MinusLogProbMetric: 53.2514 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 20/1000
2023-10-26 08:15:57.592 
Epoch 20/1000 
	 loss: 53.1101, MinusLogProbMetric: 53.1101, val_loss: 53.0606, val_MinusLogProbMetric: 53.0606

Epoch 20: val_loss improved from 53.25137 to 53.06061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 53.1101 - MinusLogProbMetric: 53.1101 - val_loss: 53.0606 - val_MinusLogProbMetric: 53.0606 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 21/1000
2023-10-26 08:16:56.042 
Epoch 21/1000 
	 loss: 52.9469, MinusLogProbMetric: 52.9469, val_loss: 52.9730, val_MinusLogProbMetric: 52.9730

Epoch 21: val_loss improved from 53.06061 to 52.97301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 52.9469 - MinusLogProbMetric: 52.9469 - val_loss: 52.9730 - val_MinusLogProbMetric: 52.9730 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 22/1000
2023-10-26 08:17:59.214 
Epoch 22/1000 
	 loss: 52.8624, MinusLogProbMetric: 52.8624, val_loss: 53.0479, val_MinusLogProbMetric: 53.0479

Epoch 22: val_loss did not improve from 52.97301
196/196 - 62s - loss: 52.8624 - MinusLogProbMetric: 52.8624 - val_loss: 53.0479 - val_MinusLogProbMetric: 53.0479 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 23/1000
2023-10-26 08:18:56.941 
Epoch 23/1000 
	 loss: 52.7182, MinusLogProbMetric: 52.7182, val_loss: 52.7166, val_MinusLogProbMetric: 52.7166

Epoch 23: val_loss improved from 52.97301 to 52.71665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 52.7182 - MinusLogProbMetric: 52.7182 - val_loss: 52.7166 - val_MinusLogProbMetric: 52.7166 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 24/1000
2023-10-26 08:19:58.038 
Epoch 24/1000 
	 loss: 53.6355, MinusLogProbMetric: 53.6355, val_loss: 52.6327, val_MinusLogProbMetric: 52.6327

Epoch 24: val_loss improved from 52.71665 to 52.63275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 53.6355 - MinusLogProbMetric: 53.6355 - val_loss: 52.6327 - val_MinusLogProbMetric: 52.6327 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 25/1000
2023-10-26 08:21:00.634 
Epoch 25/1000 
	 loss: 52.5087, MinusLogProbMetric: 52.5087, val_loss: 52.5227, val_MinusLogProbMetric: 52.5227

Epoch 25: val_loss improved from 52.63275 to 52.52275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 52.5087 - MinusLogProbMetric: 52.5087 - val_loss: 52.5227 - val_MinusLogProbMetric: 52.5227 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 26/1000
2023-10-26 08:21:56.096 
Epoch 26/1000 
	 loss: 52.4975, MinusLogProbMetric: 52.4975, val_loss: 52.3995, val_MinusLogProbMetric: 52.3995

Epoch 26: val_loss improved from 52.52275 to 52.39955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 55s - loss: 52.4975 - MinusLogProbMetric: 52.4975 - val_loss: 52.3995 - val_MinusLogProbMetric: 52.3995 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 27/1000
2023-10-26 08:22:56.655 
Epoch 27/1000 
	 loss: 52.4127, MinusLogProbMetric: 52.4127, val_loss: 52.2919, val_MinusLogProbMetric: 52.2919

Epoch 27: val_loss improved from 52.39955 to 52.29188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 52.4127 - MinusLogProbMetric: 52.4127 - val_loss: 52.2919 - val_MinusLogProbMetric: 52.2919 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 28/1000
2023-10-26 08:23:55.226 
Epoch 28/1000 
	 loss: 52.1028, MinusLogProbMetric: 52.1028, val_loss: 52.2150, val_MinusLogProbMetric: 52.2150

Epoch 28: val_loss improved from 52.29188 to 52.21500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 52.1028 - MinusLogProbMetric: 52.1028 - val_loss: 52.2150 - val_MinusLogProbMetric: 52.2150 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 29/1000
2023-10-26 08:24:52.712 
Epoch 29/1000 
	 loss: 52.1713, MinusLogProbMetric: 52.1713, val_loss: 52.1501, val_MinusLogProbMetric: 52.1501

Epoch 29: val_loss improved from 52.21500 to 52.15014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 52.1713 - MinusLogProbMetric: 52.1713 - val_loss: 52.1501 - val_MinusLogProbMetric: 52.1501 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 30/1000
2023-10-26 08:25:56.153 
Epoch 30/1000 
	 loss: 51.9543, MinusLogProbMetric: 51.9543, val_loss: 52.1574, val_MinusLogProbMetric: 52.1574

Epoch 30: val_loss did not improve from 52.15014
196/196 - 63s - loss: 51.9543 - MinusLogProbMetric: 51.9543 - val_loss: 52.1574 - val_MinusLogProbMetric: 52.1574 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 31/1000
2023-10-26 08:26:53.981 
Epoch 31/1000 
	 loss: 51.8341, MinusLogProbMetric: 51.8341, val_loss: 51.9999, val_MinusLogProbMetric: 51.9999

Epoch 31: val_loss improved from 52.15014 to 51.99987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 51.8341 - MinusLogProbMetric: 51.8341 - val_loss: 51.9999 - val_MinusLogProbMetric: 51.9999 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 32/1000
2023-10-26 08:27:54.951 
Epoch 32/1000 
	 loss: 51.6601, MinusLogProbMetric: 51.6601, val_loss: 51.7500, val_MinusLogProbMetric: 51.7500

Epoch 32: val_loss improved from 51.99987 to 51.74997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 51.6601 - MinusLogProbMetric: 51.6601 - val_loss: 51.7500 - val_MinusLogProbMetric: 51.7500 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 33/1000
2023-10-26 08:28:55.743 
Epoch 33/1000 
	 loss: 51.6376, MinusLogProbMetric: 51.6376, val_loss: 51.7299, val_MinusLogProbMetric: 51.7299

Epoch 33: val_loss improved from 51.74997 to 51.72987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 51.6376 - MinusLogProbMetric: 51.6376 - val_loss: 51.7299 - val_MinusLogProbMetric: 51.7299 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 34/1000
2023-10-26 08:30:01.742 
Epoch 34/1000 
	 loss: 51.4832, MinusLogProbMetric: 51.4832, val_loss: 51.6359, val_MinusLogProbMetric: 51.6359

Epoch 34: val_loss improved from 51.72987 to 51.63595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 51.4832 - MinusLogProbMetric: 51.4832 - val_loss: 51.6359 - val_MinusLogProbMetric: 51.6359 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 35/1000
2023-10-26 08:31:08.715 
Epoch 35/1000 
	 loss: 51.3841, MinusLogProbMetric: 51.3841, val_loss: 51.5724, val_MinusLogProbMetric: 51.5724

Epoch 35: val_loss improved from 51.63595 to 51.57242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 51.3841 - MinusLogProbMetric: 51.3841 - val_loss: 51.5724 - val_MinusLogProbMetric: 51.5724 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 36/1000
2023-10-26 08:32:14.548 
Epoch 36/1000 
	 loss: 51.2449, MinusLogProbMetric: 51.2449, val_loss: 51.4382, val_MinusLogProbMetric: 51.4382

Epoch 36: val_loss improved from 51.57242 to 51.43816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 51.2449 - MinusLogProbMetric: 51.2449 - val_loss: 51.4382 - val_MinusLogProbMetric: 51.4382 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 37/1000
2023-10-26 08:33:20.470 
Epoch 37/1000 
	 loss: 51.3068, MinusLogProbMetric: 51.3068, val_loss: 51.1783, val_MinusLogProbMetric: 51.1783

Epoch 37: val_loss improved from 51.43816 to 51.17834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 51.3068 - MinusLogProbMetric: 51.3068 - val_loss: 51.1783 - val_MinusLogProbMetric: 51.1783 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 38/1000
2023-10-26 08:34:27.709 
Epoch 38/1000 
	 loss: 51.0475, MinusLogProbMetric: 51.0475, val_loss: 51.3096, val_MinusLogProbMetric: 51.3096

Epoch 38: val_loss did not improve from 51.17834
196/196 - 66s - loss: 51.0475 - MinusLogProbMetric: 51.0475 - val_loss: 51.3096 - val_MinusLogProbMetric: 51.3096 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 39/1000
2023-10-26 08:35:32.606 
Epoch 39/1000 
	 loss: 50.9858, MinusLogProbMetric: 50.9858, val_loss: 51.0253, val_MinusLogProbMetric: 51.0253

Epoch 39: val_loss improved from 51.17834 to 51.02531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 50.9858 - MinusLogProbMetric: 50.9858 - val_loss: 51.0253 - val_MinusLogProbMetric: 51.0253 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 40/1000
2023-10-26 08:36:39.197 
Epoch 40/1000 
	 loss: 50.9233, MinusLogProbMetric: 50.9233, val_loss: 51.0205, val_MinusLogProbMetric: 51.0205

Epoch 40: val_loss improved from 51.02531 to 51.02051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 50.9233 - MinusLogProbMetric: 50.9233 - val_loss: 51.0205 - val_MinusLogProbMetric: 51.0205 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 41/1000
2023-10-26 08:37:45.299 
Epoch 41/1000 
	 loss: 50.8717, MinusLogProbMetric: 50.8717, val_loss: 50.8259, val_MinusLogProbMetric: 50.8259

Epoch 41: val_loss improved from 51.02051 to 50.82586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 50.8717 - MinusLogProbMetric: 50.8717 - val_loss: 50.8259 - val_MinusLogProbMetric: 50.8259 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 42/1000
2023-10-26 08:38:50.678 
Epoch 42/1000 
	 loss: 50.6678, MinusLogProbMetric: 50.6678, val_loss: 50.8005, val_MinusLogProbMetric: 50.8005

Epoch 42: val_loss improved from 50.82586 to 50.80050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 50.6678 - MinusLogProbMetric: 50.6678 - val_loss: 50.8005 - val_MinusLogProbMetric: 50.8005 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 43/1000
2023-10-26 08:39:57.039 
Epoch 43/1000 
	 loss: 51.3719, MinusLogProbMetric: 51.3719, val_loss: 51.8294, val_MinusLogProbMetric: 51.8294

Epoch 43: val_loss did not improve from 50.80050
196/196 - 65s - loss: 51.3719 - MinusLogProbMetric: 51.3719 - val_loss: 51.8294 - val_MinusLogProbMetric: 51.8294 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 44/1000
2023-10-26 08:41:00.891 
Epoch 44/1000 
	 loss: 50.8735, MinusLogProbMetric: 50.8735, val_loss: 50.9712, val_MinusLogProbMetric: 50.9712

Epoch 44: val_loss did not improve from 50.80050
196/196 - 64s - loss: 50.8735 - MinusLogProbMetric: 50.8735 - val_loss: 50.9712 - val_MinusLogProbMetric: 50.9712 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 45/1000
2023-10-26 08:42:06.445 
Epoch 45/1000 
	 loss: 52.3019, MinusLogProbMetric: 52.3019, val_loss: 52.4923, val_MinusLogProbMetric: 52.4923

Epoch 45: val_loss did not improve from 50.80050
196/196 - 66s - loss: 52.3019 - MinusLogProbMetric: 52.3019 - val_loss: 52.4923 - val_MinusLogProbMetric: 52.4923 - lr: 1.3717e-06 - 66s/epoch - 334ms/step
Epoch 46/1000
2023-10-26 08:43:11.961 
Epoch 46/1000 
	 loss: 50.6587, MinusLogProbMetric: 50.6587, val_loss: 50.5808, val_MinusLogProbMetric: 50.5808

Epoch 46: val_loss improved from 50.80050 to 50.58079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 50.6587 - MinusLogProbMetric: 50.6587 - val_loss: 50.5808 - val_MinusLogProbMetric: 50.5808 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 47/1000
2023-10-26 08:44:18.531 
Epoch 47/1000 
	 loss: 50.3388, MinusLogProbMetric: 50.3388, val_loss: 50.3449, val_MinusLogProbMetric: 50.3449

Epoch 47: val_loss improved from 50.58079 to 50.34493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 50.3388 - MinusLogProbMetric: 50.3388 - val_loss: 50.3449 - val_MinusLogProbMetric: 50.3449 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 48/1000
2023-10-26 08:45:23.261 
Epoch 48/1000 
	 loss: 50.1248, MinusLogProbMetric: 50.1248, val_loss: 50.3902, val_MinusLogProbMetric: 50.3902

Epoch 48: val_loss did not improve from 50.34493
196/196 - 64s - loss: 50.1248 - MinusLogProbMetric: 50.1248 - val_loss: 50.3902 - val_MinusLogProbMetric: 50.3902 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 49/1000
2023-10-26 08:46:28.565 
Epoch 49/1000 
	 loss: 50.0443, MinusLogProbMetric: 50.0443, val_loss: 50.2533, val_MinusLogProbMetric: 50.2533

Epoch 49: val_loss improved from 50.34493 to 50.25328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 50.0443 - MinusLogProbMetric: 50.0443 - val_loss: 50.2533 - val_MinusLogProbMetric: 50.2533 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 50/1000
2023-10-26 08:47:35.532 
Epoch 50/1000 
	 loss: 50.3793, MinusLogProbMetric: 50.3793, val_loss: 50.2489, val_MinusLogProbMetric: 50.2489

Epoch 50: val_loss improved from 50.25328 to 50.24886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 50.3793 - MinusLogProbMetric: 50.3793 - val_loss: 50.2489 - val_MinusLogProbMetric: 50.2489 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 51/1000
2023-10-26 08:48:42.278 
Epoch 51/1000 
	 loss: 49.9250, MinusLogProbMetric: 49.9250, val_loss: 50.2435, val_MinusLogProbMetric: 50.2435

Epoch 51: val_loss improved from 50.24886 to 50.24350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 49.9250 - MinusLogProbMetric: 49.9250 - val_loss: 50.2435 - val_MinusLogProbMetric: 50.2435 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 52/1000
2023-10-26 08:49:48.528 
Epoch 52/1000 
	 loss: 49.8823, MinusLogProbMetric: 49.8823, val_loss: 51.1402, val_MinusLogProbMetric: 51.1402

Epoch 52: val_loss did not improve from 50.24350
196/196 - 65s - loss: 49.8823 - MinusLogProbMetric: 49.8823 - val_loss: 51.1402 - val_MinusLogProbMetric: 51.1402 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 53/1000
2023-10-26 08:50:51.930 
Epoch 53/1000 
	 loss: 49.7599, MinusLogProbMetric: 49.7599, val_loss: 50.1475, val_MinusLogProbMetric: 50.1475

Epoch 53: val_loss improved from 50.24350 to 50.14749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 49.7599 - MinusLogProbMetric: 49.7599 - val_loss: 50.1475 - val_MinusLogProbMetric: 50.1475 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 54/1000
2023-10-26 08:51:57.582 
Epoch 54/1000 
	 loss: 49.6698, MinusLogProbMetric: 49.6698, val_loss: 50.0934, val_MinusLogProbMetric: 50.0934

Epoch 54: val_loss improved from 50.14749 to 50.09338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 49.6698 - MinusLogProbMetric: 49.6698 - val_loss: 50.0934 - val_MinusLogProbMetric: 50.0934 - lr: 1.3717e-06 - 66s/epoch - 334ms/step
Epoch 55/1000
2023-10-26 08:53:03.979 
Epoch 55/1000 
	 loss: 49.6581, MinusLogProbMetric: 49.6581, val_loss: 49.7139, val_MinusLogProbMetric: 49.7139

Epoch 55: val_loss improved from 50.09338 to 49.71389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 49.6581 - MinusLogProbMetric: 49.6581 - val_loss: 49.7139 - val_MinusLogProbMetric: 49.7139 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 56/1000
2023-10-26 08:54:09.473 
Epoch 56/1000 
	 loss: 49.4792, MinusLogProbMetric: 49.4792, val_loss: 49.7187, val_MinusLogProbMetric: 49.7187

Epoch 56: val_loss did not improve from 49.71389
196/196 - 64s - loss: 49.4792 - MinusLogProbMetric: 49.4792 - val_loss: 49.7187 - val_MinusLogProbMetric: 49.7187 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 57/1000
2023-10-26 08:55:16.531 
Epoch 57/1000 
	 loss: 49.2859, MinusLogProbMetric: 49.2859, val_loss: 49.5236, val_MinusLogProbMetric: 49.5236

Epoch 57: val_loss improved from 49.71389 to 49.52361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 49.2859 - MinusLogProbMetric: 49.2859 - val_loss: 49.5236 - val_MinusLogProbMetric: 49.5236 - lr: 1.3717e-06 - 68s/epoch - 348ms/step
Epoch 58/1000
2023-10-26 08:56:22.585 
Epoch 58/1000 
	 loss: 49.3367, MinusLogProbMetric: 49.3367, val_loss: 49.4964, val_MinusLogProbMetric: 49.4964

Epoch 58: val_loss improved from 49.52361 to 49.49636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 49.3367 - MinusLogProbMetric: 49.3367 - val_loss: 49.4964 - val_MinusLogProbMetric: 49.4964 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 59/1000
2023-10-26 08:57:29.861 
Epoch 59/1000 
	 loss: 49.1692, MinusLogProbMetric: 49.1692, val_loss: 49.3577, val_MinusLogProbMetric: 49.3577

Epoch 59: val_loss improved from 49.49636 to 49.35767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 49.1692 - MinusLogProbMetric: 49.1692 - val_loss: 49.3577 - val_MinusLogProbMetric: 49.3577 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 60/1000
2023-10-26 08:58:35.359 
Epoch 60/1000 
	 loss: 49.1629, MinusLogProbMetric: 49.1629, val_loss: 49.4057, val_MinusLogProbMetric: 49.4057

Epoch 60: val_loss did not improve from 49.35767
196/196 - 64s - loss: 49.1629 - MinusLogProbMetric: 49.1629 - val_loss: 49.4057 - val_MinusLogProbMetric: 49.4057 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 61/1000
2023-10-26 08:59:39.564 
Epoch 61/1000 
	 loss: 49.8663, MinusLogProbMetric: 49.8663, val_loss: 49.3535, val_MinusLogProbMetric: 49.3535

Epoch 61: val_loss improved from 49.35767 to 49.35353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 49.8663 - MinusLogProbMetric: 49.8663 - val_loss: 49.3535 - val_MinusLogProbMetric: 49.3535 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 62/1000
2023-10-26 09:00:46.119 
Epoch 62/1000 
	 loss: 49.0672, MinusLogProbMetric: 49.0672, val_loss: 49.9918, val_MinusLogProbMetric: 49.9918

Epoch 62: val_loss did not improve from 49.35353
196/196 - 65s - loss: 49.0672 - MinusLogProbMetric: 49.0672 - val_loss: 49.9918 - val_MinusLogProbMetric: 49.9918 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 63/1000
2023-10-26 09:01:50.396 
Epoch 63/1000 
	 loss: 49.3584, MinusLogProbMetric: 49.3584, val_loss: 49.1381, val_MinusLogProbMetric: 49.1381

Epoch 63: val_loss improved from 49.35353 to 49.13808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 49.3584 - MinusLogProbMetric: 49.3584 - val_loss: 49.1381 - val_MinusLogProbMetric: 49.1381 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 64/1000
2023-10-26 09:02:57.306 
Epoch 64/1000 
	 loss: 48.7948, MinusLogProbMetric: 48.7948, val_loss: 49.1040, val_MinusLogProbMetric: 49.1040

Epoch 64: val_loss improved from 49.13808 to 49.10403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 48.7948 - MinusLogProbMetric: 48.7948 - val_loss: 49.1040 - val_MinusLogProbMetric: 49.1040 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 65/1000
2023-10-26 09:04:03.513 
Epoch 65/1000 
	 loss: 48.7069, MinusLogProbMetric: 48.7069, val_loss: 49.1008, val_MinusLogProbMetric: 49.1008

Epoch 65: val_loss improved from 49.10403 to 49.10079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.7069 - MinusLogProbMetric: 48.7069 - val_loss: 49.1008 - val_MinusLogProbMetric: 49.1008 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 66/1000
2023-10-26 09:05:09.736 
Epoch 66/1000 
	 loss: 48.6114, MinusLogProbMetric: 48.6114, val_loss: 48.9106, val_MinusLogProbMetric: 48.9106

Epoch 66: val_loss improved from 49.10079 to 48.91057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.6114 - MinusLogProbMetric: 48.6114 - val_loss: 48.9106 - val_MinusLogProbMetric: 48.9106 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 67/1000
2023-10-26 09:06:15.658 
Epoch 67/1000 
	 loss: 48.5775, MinusLogProbMetric: 48.5775, val_loss: 48.8826, val_MinusLogProbMetric: 48.8826

Epoch 67: val_loss improved from 48.91057 to 48.88263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.5775 - MinusLogProbMetric: 48.5775 - val_loss: 48.8826 - val_MinusLogProbMetric: 48.8826 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 68/1000
2023-10-26 09:07:22.486 
Epoch 68/1000 
	 loss: 48.4604, MinusLogProbMetric: 48.4604, val_loss: 48.9072, val_MinusLogProbMetric: 48.9072

Epoch 68: val_loss did not improve from 48.88263
196/196 - 66s - loss: 48.4604 - MinusLogProbMetric: 48.4604 - val_loss: 48.9072 - val_MinusLogProbMetric: 48.9072 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 69/1000
2023-10-26 09:08:28.016 
Epoch 69/1000 
	 loss: 49.3347, MinusLogProbMetric: 49.3347, val_loss: 48.7231, val_MinusLogProbMetric: 48.7231

Epoch 69: val_loss improved from 48.88263 to 48.72310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 49.3347 - MinusLogProbMetric: 49.3347 - val_loss: 48.7231 - val_MinusLogProbMetric: 48.7231 - lr: 1.3717e-06 - 67s/epoch - 339ms/step
Epoch 70/1000
2023-10-26 09:09:33.880 
Epoch 70/1000 
	 loss: 48.4509, MinusLogProbMetric: 48.4509, val_loss: 48.6270, val_MinusLogProbMetric: 48.6270

Epoch 70: val_loss improved from 48.72310 to 48.62704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.4509 - MinusLogProbMetric: 48.4509 - val_loss: 48.6270 - val_MinusLogProbMetric: 48.6270 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 71/1000
2023-10-26 09:10:40.206 
Epoch 71/1000 
	 loss: 48.3565, MinusLogProbMetric: 48.3565, val_loss: 49.1272, val_MinusLogProbMetric: 49.1272

Epoch 71: val_loss did not improve from 48.62704
196/196 - 65s - loss: 48.3565 - MinusLogProbMetric: 48.3565 - val_loss: 49.1272 - val_MinusLogProbMetric: 49.1272 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 72/1000
2023-10-26 09:11:45.041 
Epoch 72/1000 
	 loss: 48.2518, MinusLogProbMetric: 48.2518, val_loss: 48.5416, val_MinusLogProbMetric: 48.5416

Epoch 72: val_loss improved from 48.62704 to 48.54163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.2518 - MinusLogProbMetric: 48.2518 - val_loss: 48.5416 - val_MinusLogProbMetric: 48.5416 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 73/1000
2023-10-26 09:12:50.907 
Epoch 73/1000 
	 loss: 48.9817, MinusLogProbMetric: 48.9817, val_loss: 48.4390, val_MinusLogProbMetric: 48.4390

Epoch 73: val_loss improved from 48.54163 to 48.43895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.9817 - MinusLogProbMetric: 48.9817 - val_loss: 48.4390 - val_MinusLogProbMetric: 48.4390 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 74/1000
2023-10-26 09:13:58.750 
Epoch 74/1000 
	 loss: 48.1418, MinusLogProbMetric: 48.1418, val_loss: 48.5802, val_MinusLogProbMetric: 48.5802

Epoch 74: val_loss did not improve from 48.43895
196/196 - 67s - loss: 48.1418 - MinusLogProbMetric: 48.1418 - val_loss: 48.5802 - val_MinusLogProbMetric: 48.5802 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 75/1000
2023-10-26 09:15:03.990 
Epoch 75/1000 
	 loss: 47.9936, MinusLogProbMetric: 47.9936, val_loss: 48.3422, val_MinusLogProbMetric: 48.3422

Epoch 75: val_loss improved from 48.43895 to 48.34222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 47.9936 - MinusLogProbMetric: 47.9936 - val_loss: 48.3422 - val_MinusLogProbMetric: 48.3422 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 76/1000
2023-10-26 09:16:10.732 
Epoch 76/1000 
	 loss: 47.9579, MinusLogProbMetric: 47.9579, val_loss: 48.5998, val_MinusLogProbMetric: 48.5998

Epoch 76: val_loss did not improve from 48.34222
196/196 - 66s - loss: 47.9579 - MinusLogProbMetric: 47.9579 - val_loss: 48.5998 - val_MinusLogProbMetric: 48.5998 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 77/1000
2023-10-26 09:17:16.730 
Epoch 77/1000 
	 loss: 47.8611, MinusLogProbMetric: 47.8611, val_loss: 48.4778, val_MinusLogProbMetric: 48.4778

Epoch 77: val_loss did not improve from 48.34222
196/196 - 66s - loss: 47.8611 - MinusLogProbMetric: 47.8611 - val_loss: 48.4778 - val_MinusLogProbMetric: 48.4778 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 78/1000
2023-10-26 09:18:22.374 
Epoch 78/1000 
	 loss: 47.8022, MinusLogProbMetric: 47.8022, val_loss: 48.1814, val_MinusLogProbMetric: 48.1814

Epoch 78: val_loss improved from 48.34222 to 48.18144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 47.8022 - MinusLogProbMetric: 47.8022 - val_loss: 48.1814 - val_MinusLogProbMetric: 48.1814 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 79/1000
2023-10-26 09:19:28.997 
Epoch 79/1000 
	 loss: 48.1451, MinusLogProbMetric: 48.1451, val_loss: 48.7563, val_MinusLogProbMetric: 48.7563

Epoch 79: val_loss did not improve from 48.18144
196/196 - 66s - loss: 48.1451 - MinusLogProbMetric: 48.1451 - val_loss: 48.7563 - val_MinusLogProbMetric: 48.7563 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 80/1000
2023-10-26 09:20:34.101 
Epoch 80/1000 
	 loss: 48.1552, MinusLogProbMetric: 48.1552, val_loss: 47.9225, val_MinusLogProbMetric: 47.9225

Epoch 80: val_loss improved from 48.18144 to 47.92252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 48.1552 - MinusLogProbMetric: 48.1552 - val_loss: 47.9225 - val_MinusLogProbMetric: 47.9225 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 81/1000
2023-10-26 09:21:38.710 
Epoch 81/1000 
	 loss: 47.6041, MinusLogProbMetric: 47.6041, val_loss: 47.8596, val_MinusLogProbMetric: 47.8596

Epoch 81: val_loss improved from 47.92252 to 47.85961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 47.6041 - MinusLogProbMetric: 47.6041 - val_loss: 47.8596 - val_MinusLogProbMetric: 47.8596 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 82/1000
2023-10-26 09:22:45.261 
Epoch 82/1000 
	 loss: 47.6153, MinusLogProbMetric: 47.6153, val_loss: 48.4906, val_MinusLogProbMetric: 48.4906

Epoch 82: val_loss did not improve from 47.85961
196/196 - 65s - loss: 47.6153 - MinusLogProbMetric: 47.6153 - val_loss: 48.4906 - val_MinusLogProbMetric: 48.4906 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 83/1000
2023-10-26 09:23:49.732 
Epoch 83/1000 
	 loss: 48.4650, MinusLogProbMetric: 48.4650, val_loss: 47.7548, val_MinusLogProbMetric: 47.7548

Epoch 83: val_loss improved from 47.85961 to 47.75483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 48.4650 - MinusLogProbMetric: 48.4650 - val_loss: 47.7548 - val_MinusLogProbMetric: 47.7548 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 84/1000
2023-10-26 09:24:56.701 
Epoch 84/1000 
	 loss: 47.4617, MinusLogProbMetric: 47.4617, val_loss: 47.8469, val_MinusLogProbMetric: 47.8469

Epoch 84: val_loss did not improve from 47.75483
196/196 - 66s - loss: 47.4617 - MinusLogProbMetric: 47.4617 - val_loss: 47.8469 - val_MinusLogProbMetric: 47.8469 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 85/1000
2023-10-26 09:26:00.877 
Epoch 85/1000 
	 loss: 47.3334, MinusLogProbMetric: 47.3334, val_loss: 47.5713, val_MinusLogProbMetric: 47.5713

Epoch 85: val_loss improved from 47.75483 to 47.57128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 47.3334 - MinusLogProbMetric: 47.3334 - val_loss: 47.5713 - val_MinusLogProbMetric: 47.5713 - lr: 1.3717e-06 - 65s/epoch - 332ms/step
Epoch 86/1000
2023-10-26 09:27:07.845 
Epoch 86/1000 
	 loss: 47.3239, MinusLogProbMetric: 47.3239, val_loss: 47.7180, val_MinusLogProbMetric: 47.7180

Epoch 86: val_loss did not improve from 47.57128
196/196 - 66s - loss: 47.3239 - MinusLogProbMetric: 47.3239 - val_loss: 47.7180 - val_MinusLogProbMetric: 47.7180 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 87/1000
2023-10-26 09:28:12.554 
Epoch 87/1000 
	 loss: 47.4243, MinusLogProbMetric: 47.4243, val_loss: 47.4393, val_MinusLogProbMetric: 47.4393

Epoch 87: val_loss improved from 47.57128 to 47.43932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 47.4243 - MinusLogProbMetric: 47.4243 - val_loss: 47.4393 - val_MinusLogProbMetric: 47.4393 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 88/1000
2023-10-26 09:29:19.302 
Epoch 88/1000 
	 loss: 47.2504, MinusLogProbMetric: 47.2504, val_loss: 47.3894, val_MinusLogProbMetric: 47.3894

Epoch 88: val_loss improved from 47.43932 to 47.38937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 47.2504 - MinusLogProbMetric: 47.2504 - val_loss: 47.3894 - val_MinusLogProbMetric: 47.3894 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 89/1000
2023-10-26 09:30:23.103 
Epoch 89/1000 
	 loss: 47.1018, MinusLogProbMetric: 47.1018, val_loss: 47.3965, val_MinusLogProbMetric: 47.3965

Epoch 89: val_loss did not improve from 47.38937
196/196 - 63s - loss: 47.1018 - MinusLogProbMetric: 47.1018 - val_loss: 47.3965 - val_MinusLogProbMetric: 47.3965 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 90/1000
2023-10-26 09:31:26.888 
Epoch 90/1000 
	 loss: 47.2802, MinusLogProbMetric: 47.2802, val_loss: 47.1758, val_MinusLogProbMetric: 47.1758

Epoch 90: val_loss improved from 47.38937 to 47.17575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 47.2802 - MinusLogProbMetric: 47.2802 - val_loss: 47.1758 - val_MinusLogProbMetric: 47.1758 - lr: 1.3717e-06 - 65s/epoch - 331ms/step
Epoch 91/1000
2023-10-26 09:32:32.504 
Epoch 91/1000 
	 loss: 47.3470, MinusLogProbMetric: 47.3470, val_loss: 47.3272, val_MinusLogProbMetric: 47.3272

Epoch 91: val_loss did not improve from 47.17575
196/196 - 64s - loss: 47.3470 - MinusLogProbMetric: 47.3470 - val_loss: 47.3272 - val_MinusLogProbMetric: 47.3272 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 92/1000
2023-10-26 09:33:39.437 
Epoch 92/1000 
	 loss: 46.9084, MinusLogProbMetric: 46.9084, val_loss: 47.1556, val_MinusLogProbMetric: 47.1556

Epoch 92: val_loss improved from 47.17575 to 47.15556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 46.9084 - MinusLogProbMetric: 46.9084 - val_loss: 47.1556 - val_MinusLogProbMetric: 47.1556 - lr: 1.3717e-06 - 68s/epoch - 347ms/step
Epoch 93/1000
2023-10-26 09:34:45.489 
Epoch 93/1000 
	 loss: 46.8616, MinusLogProbMetric: 46.8616, val_loss: 47.2104, val_MinusLogProbMetric: 47.2104

Epoch 93: val_loss did not improve from 47.15556
196/196 - 65s - loss: 46.8616 - MinusLogProbMetric: 46.8616 - val_loss: 47.2104 - val_MinusLogProbMetric: 47.2104 - lr: 1.3717e-06 - 65s/epoch - 332ms/step
Epoch 94/1000
2023-10-26 09:35:51.871 
Epoch 94/1000 
	 loss: 46.8542, MinusLogProbMetric: 46.8542, val_loss: 47.0297, val_MinusLogProbMetric: 47.0297

Epoch 94: val_loss improved from 47.15556 to 47.02966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 46.8542 - MinusLogProbMetric: 46.8542 - val_loss: 47.0297 - val_MinusLogProbMetric: 47.0297 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 95/1000
2023-10-26 09:36:58.802 
Epoch 95/1000 
	 loss: 47.5665, MinusLogProbMetric: 47.5665, val_loss: 47.1398, val_MinusLogProbMetric: 47.1398

Epoch 95: val_loss did not improve from 47.02966
196/196 - 66s - loss: 47.5665 - MinusLogProbMetric: 47.5665 - val_loss: 47.1398 - val_MinusLogProbMetric: 47.1398 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 96/1000
2023-10-26 09:38:04.951 
Epoch 96/1000 
	 loss: 50.4192, MinusLogProbMetric: 50.4192, val_loss: 49.6022, val_MinusLogProbMetric: 49.6022

Epoch 96: val_loss did not improve from 47.02966
196/196 - 66s - loss: 50.4192 - MinusLogProbMetric: 50.4192 - val_loss: 49.6022 - val_MinusLogProbMetric: 49.6022 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 97/1000
2023-10-26 09:39:10.396 
Epoch 97/1000 
	 loss: 47.8212, MinusLogProbMetric: 47.8212, val_loss: 47.4203, val_MinusLogProbMetric: 47.4203

Epoch 97: val_loss did not improve from 47.02966
196/196 - 65s - loss: 47.8212 - MinusLogProbMetric: 47.8212 - val_loss: 47.4203 - val_MinusLogProbMetric: 47.4203 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 98/1000
2023-10-26 09:40:15.999 
Epoch 98/1000 
	 loss: 46.9834, MinusLogProbMetric: 46.9834, val_loss: 47.1427, val_MinusLogProbMetric: 47.1427

Epoch 98: val_loss did not improve from 47.02966
196/196 - 66s - loss: 46.9834 - MinusLogProbMetric: 46.9834 - val_loss: 47.1427 - val_MinusLogProbMetric: 47.1427 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 99/1000
2023-10-26 09:41:20.816 
Epoch 99/1000 
	 loss: 46.6645, MinusLogProbMetric: 46.6645, val_loss: 46.9293, val_MinusLogProbMetric: 46.9293

Epoch 99: val_loss improved from 47.02966 to 46.92929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 46.6645 - MinusLogProbMetric: 46.6645 - val_loss: 46.9293 - val_MinusLogProbMetric: 46.9293 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 100/1000
2023-10-26 09:42:27.338 
Epoch 100/1000 
	 loss: 46.9608, MinusLogProbMetric: 46.9608, val_loss: 47.1390, val_MinusLogProbMetric: 47.1390

Epoch 100: val_loss did not improve from 46.92929
196/196 - 66s - loss: 46.9608 - MinusLogProbMetric: 46.9608 - val_loss: 47.1390 - val_MinusLogProbMetric: 47.1390 - lr: 1.3717e-06 - 66s/epoch - 334ms/step
Epoch 101/1000
2023-10-26 09:43:32.498 
Epoch 101/1000 
	 loss: 47.6625, MinusLogProbMetric: 47.6625, val_loss: 46.8779, val_MinusLogProbMetric: 46.8779

Epoch 101: val_loss improved from 46.92929 to 46.87791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 47.6625 - MinusLogProbMetric: 47.6625 - val_loss: 46.8779 - val_MinusLogProbMetric: 46.8779 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 102/1000
2023-10-26 09:44:38.886 
Epoch 102/1000 
	 loss: 46.4991, MinusLogProbMetric: 46.4991, val_loss: 46.9197, val_MinusLogProbMetric: 46.9197

Epoch 102: val_loss did not improve from 46.87791
196/196 - 65s - loss: 46.4991 - MinusLogProbMetric: 46.4991 - val_loss: 46.9197 - val_MinusLogProbMetric: 46.9197 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 103/1000
2023-10-26 09:45:43.328 
Epoch 103/1000 
	 loss: 46.4289, MinusLogProbMetric: 46.4289, val_loss: 46.5762, val_MinusLogProbMetric: 46.5762

Epoch 103: val_loss improved from 46.87791 to 46.57623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 46.4289 - MinusLogProbMetric: 46.4289 - val_loss: 46.5762 - val_MinusLogProbMetric: 46.5762 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 104/1000
2023-10-26 09:46:50.632 
Epoch 104/1000 
	 loss: 47.4275, MinusLogProbMetric: 47.4275, val_loss: 46.7156, val_MinusLogProbMetric: 46.7156

Epoch 104: val_loss did not improve from 46.57623
196/196 - 66s - loss: 47.4275 - MinusLogProbMetric: 47.4275 - val_loss: 46.7156 - val_MinusLogProbMetric: 46.7156 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 105/1000
2023-10-26 09:47:55.300 
Epoch 105/1000 
	 loss: 46.3715, MinusLogProbMetric: 46.3715, val_loss: 46.5616, val_MinusLogProbMetric: 46.5616

Epoch 105: val_loss improved from 46.57623 to 46.56161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 46.3715 - MinusLogProbMetric: 46.3715 - val_loss: 46.5616 - val_MinusLogProbMetric: 46.5616 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 106/1000
2023-10-26 09:49:02.363 
Epoch 106/1000 
	 loss: 46.2449, MinusLogProbMetric: 46.2449, val_loss: 46.4918, val_MinusLogProbMetric: 46.4918

Epoch 106: val_loss improved from 46.56161 to 46.49183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 46.2449 - MinusLogProbMetric: 46.2449 - val_loss: 46.4918 - val_MinusLogProbMetric: 46.4918 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 107/1000
2023-10-26 09:50:08.685 
Epoch 107/1000 
	 loss: 46.1602, MinusLogProbMetric: 46.1602, val_loss: 46.2961, val_MinusLogProbMetric: 46.2961

Epoch 107: val_loss improved from 46.49183 to 46.29606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 46.1602 - MinusLogProbMetric: 46.1602 - val_loss: 46.2961 - val_MinusLogProbMetric: 46.2961 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 108/1000
2023-10-26 09:51:14.329 
Epoch 108/1000 
	 loss: 46.0805, MinusLogProbMetric: 46.0805, val_loss: 46.5199, val_MinusLogProbMetric: 46.5199

Epoch 108: val_loss did not improve from 46.29606
196/196 - 65s - loss: 46.0805 - MinusLogProbMetric: 46.0805 - val_loss: 46.5199 - val_MinusLogProbMetric: 46.5199 - lr: 1.3717e-06 - 65s/epoch - 329ms/step
Epoch 109/1000
2023-10-26 09:52:19.984 
Epoch 109/1000 
	 loss: 46.0098, MinusLogProbMetric: 46.0098, val_loss: 46.5141, val_MinusLogProbMetric: 46.5141

Epoch 109: val_loss did not improve from 46.29606
196/196 - 66s - loss: 46.0098 - MinusLogProbMetric: 46.0098 - val_loss: 46.5141 - val_MinusLogProbMetric: 46.5141 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 110/1000
2023-10-26 09:53:24.201 
Epoch 110/1000 
	 loss: 46.0763, MinusLogProbMetric: 46.0763, val_loss: 46.2369, val_MinusLogProbMetric: 46.2369

Epoch 110: val_loss improved from 46.29606 to 46.23690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 46.0763 - MinusLogProbMetric: 46.0763 - val_loss: 46.2369 - val_MinusLogProbMetric: 46.2369 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 111/1000
2023-10-26 09:54:30.805 
Epoch 111/1000 
	 loss: 45.9364, MinusLogProbMetric: 45.9364, val_loss: 46.2529, val_MinusLogProbMetric: 46.2529

Epoch 111: val_loss did not improve from 46.23690
196/196 - 65s - loss: 45.9364 - MinusLogProbMetric: 45.9364 - val_loss: 46.2529 - val_MinusLogProbMetric: 46.2529 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 112/1000
2023-10-26 09:55:35.197 
Epoch 112/1000 
	 loss: 45.9485, MinusLogProbMetric: 45.9485, val_loss: 46.1450, val_MinusLogProbMetric: 46.1450

Epoch 112: val_loss improved from 46.23690 to 46.14504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 45.9485 - MinusLogProbMetric: 45.9485 - val_loss: 46.1450 - val_MinusLogProbMetric: 46.1450 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 113/1000
2023-10-26 09:56:41.594 
Epoch 113/1000 
	 loss: 45.8430, MinusLogProbMetric: 45.8430, val_loss: 46.0647, val_MinusLogProbMetric: 46.0647

Epoch 113: val_loss improved from 46.14504 to 46.06473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 45.8430 - MinusLogProbMetric: 45.8430 - val_loss: 46.0647 - val_MinusLogProbMetric: 46.0647 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 114/1000
2023-10-26 09:57:48.742 
Epoch 114/1000 
	 loss: 45.7934, MinusLogProbMetric: 45.7934, val_loss: 46.0540, val_MinusLogProbMetric: 46.0540

Epoch 114: val_loss improved from 46.06473 to 46.05404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.7934 - MinusLogProbMetric: 45.7934 - val_loss: 46.0540 - val_MinusLogProbMetric: 46.0540 - lr: 1.3717e-06 - 67s/epoch - 343ms/step
Epoch 115/1000
2023-10-26 09:58:55.166 
Epoch 115/1000 
	 loss: 45.7287, MinusLogProbMetric: 45.7287, val_loss: 46.0378, val_MinusLogProbMetric: 46.0378

Epoch 115: val_loss improved from 46.05404 to 46.03783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 45.7287 - MinusLogProbMetric: 45.7287 - val_loss: 46.0378 - val_MinusLogProbMetric: 46.0378 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 116/1000
2023-10-26 10:00:00.834 
Epoch 116/1000 
	 loss: 45.7476, MinusLogProbMetric: 45.7476, val_loss: 46.0061, val_MinusLogProbMetric: 46.0061

Epoch 116: val_loss improved from 46.03783 to 46.00613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 45.7476 - MinusLogProbMetric: 45.7476 - val_loss: 46.0061 - val_MinusLogProbMetric: 46.0061 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 117/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 10:00:08.186 
Epoch 117/1000 
	 loss: nan, MinusLogProbMetric: 48.9301, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 117: val_loss did not improve from 46.00613
196/196 - 6s - loss: nan - MinusLogProbMetric: 48.9301 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 6s/epoch - 32ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.572473708276175e-07.
===========
Generating train data for run 374.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_374/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_374
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_341"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_342 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7feccd828970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecac48ff10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecac48ff10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb50509e70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec8789ec80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec8789f1f0>, <keras.callbacks.ModelCheckpoint object at 0x7fec8789f2b0>, <keras.callbacks.EarlyStopping object at 0x7fec8789f520>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec8789f550>, <keras.callbacks.TerminateOnNaN object at 0x7fec8789f190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 374/720 with hyperparameters:
timestamp = 2023-10-26 10:00:19.575930
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 10:03:55.133 
Epoch 1/1000 
	 loss: 45.5587, MinusLogProbMetric: 45.5587, val_loss: 45.9730, val_MinusLogProbMetric: 45.9730

Epoch 1: val_loss improved from inf to 45.97298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 216s - loss: 45.5587 - MinusLogProbMetric: 45.5587 - val_loss: 45.9730 - val_MinusLogProbMetric: 45.9730 - lr: 4.5725e-07 - 216s/epoch - 1s/step
Epoch 2/1000
2023-10-26 10:05:01.209 
Epoch 2/1000 
	 loss: 45.5808, MinusLogProbMetric: 45.5808, val_loss: 45.8173, val_MinusLogProbMetric: 45.8173

Epoch 2: val_loss improved from 45.97298 to 45.81730, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 45.5808 - MinusLogProbMetric: 45.5808 - val_loss: 45.8173 - val_MinusLogProbMetric: 45.8173 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 3/1000
2023-10-26 10:06:07.971 
Epoch 3/1000 
	 loss: 45.5096, MinusLogProbMetric: 45.5096, val_loss: 45.9268, val_MinusLogProbMetric: 45.9268

Epoch 3: val_loss did not improve from 45.81730
196/196 - 66s - loss: 45.5096 - MinusLogProbMetric: 45.5096 - val_loss: 45.9268 - val_MinusLogProbMetric: 45.9268 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 4/1000
2023-10-26 10:07:13.981 
Epoch 4/1000 
	 loss: 45.4413, MinusLogProbMetric: 45.4413, val_loss: 45.6907, val_MinusLogProbMetric: 45.6907

Epoch 4: val_loss improved from 45.81730 to 45.69071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.4413 - MinusLogProbMetric: 45.4413 - val_loss: 45.6907 - val_MinusLogProbMetric: 45.6907 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 5/1000
2023-10-26 10:08:21.200 
Epoch 5/1000 
	 loss: 45.4816, MinusLogProbMetric: 45.4816, val_loss: 45.8406, val_MinusLogProbMetric: 45.8406

Epoch 5: val_loss did not improve from 45.69071
196/196 - 66s - loss: 45.4816 - MinusLogProbMetric: 45.4816 - val_loss: 45.8406 - val_MinusLogProbMetric: 45.8406 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 6/1000
2023-10-26 10:09:26.706 
Epoch 6/1000 
	 loss: 45.3991, MinusLogProbMetric: 45.3991, val_loss: 45.6909, val_MinusLogProbMetric: 45.6909

Epoch 6: val_loss did not improve from 45.69071
196/196 - 66s - loss: 45.3991 - MinusLogProbMetric: 45.3991 - val_loss: 45.6909 - val_MinusLogProbMetric: 45.6909 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 7/1000
2023-10-26 10:10:32.118 
Epoch 7/1000 
	 loss: 45.3601, MinusLogProbMetric: 45.3601, val_loss: 45.6020, val_MinusLogProbMetric: 45.6020

Epoch 7: val_loss improved from 45.69071 to 45.60202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.3601 - MinusLogProbMetric: 45.3601 - val_loss: 45.6020 - val_MinusLogProbMetric: 45.6020 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 8/1000
2023-10-26 10:11:41.168 
Epoch 8/1000 
	 loss: 45.3142, MinusLogProbMetric: 45.3142, val_loss: 45.7307, val_MinusLogProbMetric: 45.7307

Epoch 8: val_loss did not improve from 45.60202
196/196 - 68s - loss: 45.3142 - MinusLogProbMetric: 45.3142 - val_loss: 45.7307 - val_MinusLogProbMetric: 45.7307 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 9/1000
2023-10-26 10:12:47.278 
Epoch 9/1000 
	 loss: 45.2729, MinusLogProbMetric: 45.2729, val_loss: 45.9903, val_MinusLogProbMetric: 45.9903

Epoch 9: val_loss did not improve from 45.60202
196/196 - 66s - loss: 45.2729 - MinusLogProbMetric: 45.2729 - val_loss: 45.9903 - val_MinusLogProbMetric: 45.9903 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 10/1000
2023-10-26 10:13:53.141 
Epoch 10/1000 
	 loss: 45.4252, MinusLogProbMetric: 45.4252, val_loss: 45.5492, val_MinusLogProbMetric: 45.5492

Epoch 10: val_loss improved from 45.60202 to 45.54918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.4252 - MinusLogProbMetric: 45.4252 - val_loss: 45.5492 - val_MinusLogProbMetric: 45.5492 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 11/1000
2023-10-26 10:14:59.949 
Epoch 11/1000 
	 loss: 45.2106, MinusLogProbMetric: 45.2106, val_loss: 45.5091, val_MinusLogProbMetric: 45.5091

Epoch 11: val_loss improved from 45.54918 to 45.50906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.2106 - MinusLogProbMetric: 45.2106 - val_loss: 45.5091 - val_MinusLogProbMetric: 45.5091 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 12/1000
2023-10-26 10:16:07.930 
Epoch 12/1000 
	 loss: 45.2328, MinusLogProbMetric: 45.2328, val_loss: 45.7170, val_MinusLogProbMetric: 45.7170

Epoch 12: val_loss did not improve from 45.50906
196/196 - 67s - loss: 45.2328 - MinusLogProbMetric: 45.2328 - val_loss: 45.7170 - val_MinusLogProbMetric: 45.7170 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 13/1000
2023-10-26 10:17:14.733 
Epoch 13/1000 
	 loss: 45.5458, MinusLogProbMetric: 45.5458, val_loss: 45.5381, val_MinusLogProbMetric: 45.5381

Epoch 13: val_loss did not improve from 45.50906
196/196 - 67s - loss: 45.5458 - MinusLogProbMetric: 45.5458 - val_loss: 45.5381 - val_MinusLogProbMetric: 45.5381 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 14/1000
2023-10-26 10:18:20.333 
Epoch 14/1000 
	 loss: 45.1313, MinusLogProbMetric: 45.1313, val_loss: 45.4712, val_MinusLogProbMetric: 45.4712

Epoch 14: val_loss improved from 45.50906 to 45.47121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.1313 - MinusLogProbMetric: 45.1313 - val_loss: 45.4712 - val_MinusLogProbMetric: 45.4712 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 15/1000
2023-10-26 10:19:24.242 
Epoch 15/1000 
	 loss: 45.2137, MinusLogProbMetric: 45.2137, val_loss: 45.4200, val_MinusLogProbMetric: 45.4200

Epoch 15: val_loss improved from 45.47121 to 45.42000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 45.2137 - MinusLogProbMetric: 45.2137 - val_loss: 45.4200 - val_MinusLogProbMetric: 45.4200 - lr: 4.5725e-07 - 64s/epoch - 325ms/step
Epoch 16/1000
2023-10-26 10:20:30.900 
Epoch 16/1000 
	 loss: 45.5331, MinusLogProbMetric: 45.5331, val_loss: 45.4281, val_MinusLogProbMetric: 45.4281

Epoch 16: val_loss did not improve from 45.42000
196/196 - 66s - loss: 45.5331 - MinusLogProbMetric: 45.5331 - val_loss: 45.4281 - val_MinusLogProbMetric: 45.4281 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 17/1000
2023-10-26 10:21:37.183 
Epoch 17/1000 
	 loss: 45.0737, MinusLogProbMetric: 45.0737, val_loss: 45.3924, val_MinusLogProbMetric: 45.3924

Epoch 17: val_loss improved from 45.42000 to 45.39242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.0737 - MinusLogProbMetric: 45.0737 - val_loss: 45.3924 - val_MinusLogProbMetric: 45.3924 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 18/1000
2023-10-26 10:22:43.753 
Epoch 18/1000 
	 loss: 45.0605, MinusLogProbMetric: 45.0605, val_loss: 45.3986, val_MinusLogProbMetric: 45.3986

Epoch 18: val_loss did not improve from 45.39242
196/196 - 66s - loss: 45.0605 - MinusLogProbMetric: 45.0605 - val_loss: 45.3986 - val_MinusLogProbMetric: 45.3986 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 19/1000
2023-10-26 10:23:49.133 
Epoch 19/1000 
	 loss: 45.0971, MinusLogProbMetric: 45.0971, val_loss: 46.2831, val_MinusLogProbMetric: 46.2831

Epoch 19: val_loss did not improve from 45.39242
196/196 - 65s - loss: 45.0971 - MinusLogProbMetric: 45.0971 - val_loss: 46.2831 - val_MinusLogProbMetric: 46.2831 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 20/1000
2023-10-26 10:24:55.270 
Epoch 20/1000 
	 loss: 45.0811, MinusLogProbMetric: 45.0811, val_loss: 45.2832, val_MinusLogProbMetric: 45.2832

Epoch 20: val_loss improved from 45.39242 to 45.28321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.0811 - MinusLogProbMetric: 45.0811 - val_loss: 45.2832 - val_MinusLogProbMetric: 45.2832 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 21/1000
2023-10-26 10:26:02.514 
Epoch 21/1000 
	 loss: 45.0593, MinusLogProbMetric: 45.0593, val_loss: 46.3139, val_MinusLogProbMetric: 46.3139

Epoch 21: val_loss did not improve from 45.28321
196/196 - 66s - loss: 45.0593 - MinusLogProbMetric: 45.0593 - val_loss: 46.3139 - val_MinusLogProbMetric: 46.3139 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 22/1000
2023-10-26 10:27:08.802 
Epoch 22/1000 
	 loss: 45.0047, MinusLogProbMetric: 45.0047, val_loss: 45.3073, val_MinusLogProbMetric: 45.3073

Epoch 22: val_loss did not improve from 45.28321
196/196 - 66s - loss: 45.0047 - MinusLogProbMetric: 45.0047 - val_loss: 45.3073 - val_MinusLogProbMetric: 45.3073 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 23/1000
2023-10-26 10:28:14.165 
Epoch 23/1000 
	 loss: 44.9348, MinusLogProbMetric: 44.9348, val_loss: 45.2040, val_MinusLogProbMetric: 45.2040

Epoch 23: val_loss improved from 45.28321 to 45.20397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.9348 - MinusLogProbMetric: 44.9348 - val_loss: 45.2040 - val_MinusLogProbMetric: 45.2040 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 24/1000
2023-10-26 10:29:21.454 
Epoch 24/1000 
	 loss: 45.1741, MinusLogProbMetric: 45.1741, val_loss: 45.1947, val_MinusLogProbMetric: 45.1947

Epoch 24: val_loss improved from 45.20397 to 45.19468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 45.1741 - MinusLogProbMetric: 45.1741 - val_loss: 45.1947 - val_MinusLogProbMetric: 45.1947 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 25/1000
2023-10-26 10:30:28.085 
Epoch 25/1000 
	 loss: 44.9278, MinusLogProbMetric: 44.9278, val_loss: 45.2165, val_MinusLogProbMetric: 45.2165

Epoch 25: val_loss did not improve from 45.19468
196/196 - 66s - loss: 44.9278 - MinusLogProbMetric: 44.9278 - val_loss: 45.2165 - val_MinusLogProbMetric: 45.2165 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 26/1000
2023-10-26 10:31:35.491 
Epoch 26/1000 
	 loss: 44.8570, MinusLogProbMetric: 44.8570, val_loss: 45.1002, val_MinusLogProbMetric: 45.1002

Epoch 26: val_loss improved from 45.19468 to 45.10020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 69s - loss: 44.8570 - MinusLogProbMetric: 44.8570 - val_loss: 45.1002 - val_MinusLogProbMetric: 45.1002 - lr: 4.5725e-07 - 69s/epoch - 351ms/step
Epoch 27/1000
2023-10-26 10:32:43.118 
Epoch 27/1000 
	 loss: 44.9461, MinusLogProbMetric: 44.9461, val_loss: 45.1873, val_MinusLogProbMetric: 45.1873

Epoch 27: val_loss did not improve from 45.10020
196/196 - 66s - loss: 44.9461 - MinusLogProbMetric: 44.9461 - val_loss: 45.1873 - val_MinusLogProbMetric: 45.1873 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 28/1000
2023-10-26 10:33:50.077 
Epoch 28/1000 
	 loss: 44.8133, MinusLogProbMetric: 44.8133, val_loss: 45.1557, val_MinusLogProbMetric: 45.1557

Epoch 28: val_loss did not improve from 45.10020
196/196 - 67s - loss: 44.8133 - MinusLogProbMetric: 44.8133 - val_loss: 45.1557 - val_MinusLogProbMetric: 45.1557 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 29/1000
2023-10-26 10:34:54.896 
Epoch 29/1000 
	 loss: 44.7975, MinusLogProbMetric: 44.7975, val_loss: 45.0840, val_MinusLogProbMetric: 45.0840

Epoch 29: val_loss improved from 45.10020 to 45.08399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.7975 - MinusLogProbMetric: 44.7975 - val_loss: 45.0840 - val_MinusLogProbMetric: 45.0840 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 30/1000
2023-10-26 10:36:00.321 
Epoch 30/1000 
	 loss: 45.0039, MinusLogProbMetric: 45.0039, val_loss: 45.2865, val_MinusLogProbMetric: 45.2865

Epoch 30: val_loss did not improve from 45.08399
196/196 - 64s - loss: 45.0039 - MinusLogProbMetric: 45.0039 - val_loss: 45.2865 - val_MinusLogProbMetric: 45.2865 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 31/1000
2023-10-26 10:37:06.358 
Epoch 31/1000 
	 loss: 44.8049, MinusLogProbMetric: 44.8049, val_loss: 45.0745, val_MinusLogProbMetric: 45.0745

Epoch 31: val_loss improved from 45.08399 to 45.07454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.8049 - MinusLogProbMetric: 44.8049 - val_loss: 45.0745 - val_MinusLogProbMetric: 45.0745 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 32/1000
2023-10-26 10:38:13.857 
Epoch 32/1000 
	 loss: 44.8039, MinusLogProbMetric: 44.8039, val_loss: 45.0022, val_MinusLogProbMetric: 45.0022

Epoch 32: val_loss improved from 45.07454 to 45.00217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.8039 - MinusLogProbMetric: 44.8039 - val_loss: 45.0022 - val_MinusLogProbMetric: 45.0022 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 33/1000
2023-10-26 10:39:19.147 
Epoch 33/1000 
	 loss: 44.6777, MinusLogProbMetric: 44.6777, val_loss: 45.0012, val_MinusLogProbMetric: 45.0012

Epoch 33: val_loss improved from 45.00217 to 45.00119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.6777 - MinusLogProbMetric: 44.6777 - val_loss: 45.0012 - val_MinusLogProbMetric: 45.0012 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 34/1000
2023-10-26 10:40:25.044 
Epoch 34/1000 
	 loss: 44.7067, MinusLogProbMetric: 44.7067, val_loss: 44.9335, val_MinusLogProbMetric: 44.9335

Epoch 34: val_loss improved from 45.00119 to 44.93353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.7067 - MinusLogProbMetric: 44.7067 - val_loss: 44.9335 - val_MinusLogProbMetric: 44.9335 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 35/1000
2023-10-26 10:41:30.934 
Epoch 35/1000 
	 loss: 44.6586, MinusLogProbMetric: 44.6586, val_loss: 45.0305, val_MinusLogProbMetric: 45.0305

Epoch 35: val_loss did not improve from 44.93353
196/196 - 65s - loss: 44.6586 - MinusLogProbMetric: 44.6586 - val_loss: 45.0305 - val_MinusLogProbMetric: 45.0305 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 36/1000
2023-10-26 10:42:35.559 
Epoch 36/1000 
	 loss: 44.6332, MinusLogProbMetric: 44.6332, val_loss: 44.9865, val_MinusLogProbMetric: 44.9865

Epoch 36: val_loss did not improve from 44.93353
196/196 - 65s - loss: 44.6332 - MinusLogProbMetric: 44.6332 - val_loss: 44.9865 - val_MinusLogProbMetric: 44.9865 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 37/1000
2023-10-26 10:43:39.774 
Epoch 37/1000 
	 loss: 44.8156, MinusLogProbMetric: 44.8156, val_loss: 44.9204, val_MinusLogProbMetric: 44.9204

Epoch 37: val_loss improved from 44.93353 to 44.92042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 44.8156 - MinusLogProbMetric: 44.8156 - val_loss: 44.9204 - val_MinusLogProbMetric: 44.9204 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 38/1000
2023-10-26 10:44:43.030 
Epoch 38/1000 
	 loss: 44.5889, MinusLogProbMetric: 44.5889, val_loss: 44.9408, val_MinusLogProbMetric: 44.9408

Epoch 38: val_loss did not improve from 44.92042
196/196 - 62s - loss: 44.5889 - MinusLogProbMetric: 44.5889 - val_loss: 44.9408 - val_MinusLogProbMetric: 44.9408 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 39/1000
2023-10-26 10:45:49.450 
Epoch 39/1000 
	 loss: 44.5803, MinusLogProbMetric: 44.5803, val_loss: 44.8891, val_MinusLogProbMetric: 44.8891

Epoch 39: val_loss improved from 44.92042 to 44.88912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.5803 - MinusLogProbMetric: 44.5803 - val_loss: 44.8891 - val_MinusLogProbMetric: 44.8891 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 40/1000
2023-10-26 10:46:56.825 
Epoch 40/1000 
	 loss: 44.5555, MinusLogProbMetric: 44.5555, val_loss: 44.8979, val_MinusLogProbMetric: 44.8979

Epoch 40: val_loss did not improve from 44.88912
196/196 - 67s - loss: 44.5555 - MinusLogProbMetric: 44.5555 - val_loss: 44.8979 - val_MinusLogProbMetric: 44.8979 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 41/1000
2023-10-26 10:48:02.696 
Epoch 41/1000 
	 loss: 44.6007, MinusLogProbMetric: 44.6007, val_loss: 44.9408, val_MinusLogProbMetric: 44.9408

Epoch 41: val_loss did not improve from 44.88912
196/196 - 66s - loss: 44.6007 - MinusLogProbMetric: 44.6007 - val_loss: 44.9408 - val_MinusLogProbMetric: 44.9408 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 42/1000
2023-10-26 10:49:07.564 
Epoch 42/1000 
	 loss: 44.5499, MinusLogProbMetric: 44.5499, val_loss: 44.8393, val_MinusLogProbMetric: 44.8393

Epoch 42: val_loss improved from 44.88912 to 44.83933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.5499 - MinusLogProbMetric: 44.5499 - val_loss: 44.8393 - val_MinusLogProbMetric: 44.8393 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 43/1000
2023-10-26 10:50:13.816 
Epoch 43/1000 
	 loss: 44.6612, MinusLogProbMetric: 44.6612, val_loss: 44.8456, val_MinusLogProbMetric: 44.8456

Epoch 43: val_loss did not improve from 44.83933
196/196 - 65s - loss: 44.6612 - MinusLogProbMetric: 44.6612 - val_loss: 44.8456 - val_MinusLogProbMetric: 44.8456 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 44/1000
2023-10-26 10:51:19.160 
Epoch 44/1000 
	 loss: 44.4793, MinusLogProbMetric: 44.4793, val_loss: 44.7655, val_MinusLogProbMetric: 44.7655

Epoch 44: val_loss improved from 44.83933 to 44.76551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.4793 - MinusLogProbMetric: 44.4793 - val_loss: 44.7655 - val_MinusLogProbMetric: 44.7655 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 45/1000
2023-10-26 10:52:25.914 
Epoch 45/1000 
	 loss: 44.4402, MinusLogProbMetric: 44.4402, val_loss: 44.8032, val_MinusLogProbMetric: 44.8032

Epoch 45: val_loss did not improve from 44.76551
196/196 - 66s - loss: 44.4402 - MinusLogProbMetric: 44.4402 - val_loss: 44.8032 - val_MinusLogProbMetric: 44.8032 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 46/1000
2023-10-26 10:53:31.658 
Epoch 46/1000 
	 loss: 44.4249, MinusLogProbMetric: 44.4249, val_loss: 44.7714, val_MinusLogProbMetric: 44.7714

Epoch 46: val_loss did not improve from 44.76551
196/196 - 66s - loss: 44.4249 - MinusLogProbMetric: 44.4249 - val_loss: 44.7714 - val_MinusLogProbMetric: 44.7714 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 47/1000
2023-10-26 10:54:38.543 
Epoch 47/1000 
	 loss: 44.5649, MinusLogProbMetric: 44.5649, val_loss: 44.7691, val_MinusLogProbMetric: 44.7691

Epoch 47: val_loss did not improve from 44.76551
196/196 - 67s - loss: 44.5649 - MinusLogProbMetric: 44.5649 - val_loss: 44.7691 - val_MinusLogProbMetric: 44.7691 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 48/1000
2023-10-26 10:55:44.436 
Epoch 48/1000 
	 loss: 44.4295, MinusLogProbMetric: 44.4295, val_loss: 44.7264, val_MinusLogProbMetric: 44.7264

Epoch 48: val_loss improved from 44.76551 to 44.72635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.4295 - MinusLogProbMetric: 44.4295 - val_loss: 44.7264 - val_MinusLogProbMetric: 44.7264 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 49/1000
2023-10-26 10:56:50.816 
Epoch 49/1000 
	 loss: 44.3967, MinusLogProbMetric: 44.3967, val_loss: 44.6499, val_MinusLogProbMetric: 44.6499

Epoch 49: val_loss improved from 44.72635 to 44.64995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.3967 - MinusLogProbMetric: 44.3967 - val_loss: 44.6499 - val_MinusLogProbMetric: 44.6499 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 50/1000
2023-10-26 10:57:58.003 
Epoch 50/1000 
	 loss: 44.4103, MinusLogProbMetric: 44.4103, val_loss: 44.8646, val_MinusLogProbMetric: 44.8646

Epoch 50: val_loss did not improve from 44.64995
196/196 - 66s - loss: 44.4103 - MinusLogProbMetric: 44.4103 - val_loss: 44.8646 - val_MinusLogProbMetric: 44.8646 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 51/1000
2023-10-26 10:59:04.010 
Epoch 51/1000 
	 loss: 44.3485, MinusLogProbMetric: 44.3485, val_loss: 44.6882, val_MinusLogProbMetric: 44.6882

Epoch 51: val_loss did not improve from 44.64995
196/196 - 66s - loss: 44.3485 - MinusLogProbMetric: 44.3485 - val_loss: 44.6882 - val_MinusLogProbMetric: 44.6882 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 52/1000
2023-10-26 11:00:09.543 
Epoch 52/1000 
	 loss: 44.3210, MinusLogProbMetric: 44.3210, val_loss: 44.5545, val_MinusLogProbMetric: 44.5545

Epoch 52: val_loss improved from 44.64995 to 44.55447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.3210 - MinusLogProbMetric: 44.3210 - val_loss: 44.5545 - val_MinusLogProbMetric: 44.5545 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 53/1000
2023-10-26 11:01:15.966 
Epoch 53/1000 
	 loss: 44.2930, MinusLogProbMetric: 44.2930, val_loss: 44.6105, val_MinusLogProbMetric: 44.6105

Epoch 53: val_loss did not improve from 44.55447
196/196 - 65s - loss: 44.2930 - MinusLogProbMetric: 44.2930 - val_loss: 44.6105 - val_MinusLogProbMetric: 44.6105 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 54/1000
2023-10-26 11:02:22.002 
Epoch 54/1000 
	 loss: 44.2916, MinusLogProbMetric: 44.2916, val_loss: 44.5820, val_MinusLogProbMetric: 44.5820

Epoch 54: val_loss did not improve from 44.55447
196/196 - 66s - loss: 44.2916 - MinusLogProbMetric: 44.2916 - val_loss: 44.5820 - val_MinusLogProbMetric: 44.5820 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 55/1000
2023-10-26 11:03:27.118 
Epoch 55/1000 
	 loss: 44.4193, MinusLogProbMetric: 44.4193, val_loss: 44.5896, val_MinusLogProbMetric: 44.5896

Epoch 55: val_loss did not improve from 44.55447
196/196 - 65s - loss: 44.4193 - MinusLogProbMetric: 44.4193 - val_loss: 44.5896 - val_MinusLogProbMetric: 44.5896 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 56/1000
2023-10-26 11:04:32.810 
Epoch 56/1000 
	 loss: 44.2415, MinusLogProbMetric: 44.2415, val_loss: 44.5798, val_MinusLogProbMetric: 44.5798

Epoch 56: val_loss did not improve from 44.55447
196/196 - 66s - loss: 44.2415 - MinusLogProbMetric: 44.2415 - val_loss: 44.5798 - val_MinusLogProbMetric: 44.5798 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 57/1000
2023-10-26 11:05:40.221 
Epoch 57/1000 
	 loss: 44.2368, MinusLogProbMetric: 44.2368, val_loss: 44.5705, val_MinusLogProbMetric: 44.5705

Epoch 57: val_loss did not improve from 44.55447
196/196 - 67s - loss: 44.2368 - MinusLogProbMetric: 44.2368 - val_loss: 44.5705 - val_MinusLogProbMetric: 44.5705 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 58/1000
2023-10-26 11:06:46.959 
Epoch 58/1000 
	 loss: 44.2957, MinusLogProbMetric: 44.2957, val_loss: 44.5792, val_MinusLogProbMetric: 44.5792

Epoch 58: val_loss did not improve from 44.55447
196/196 - 67s - loss: 44.2957 - MinusLogProbMetric: 44.2957 - val_loss: 44.5792 - val_MinusLogProbMetric: 44.5792 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 59/1000
2023-10-26 11:07:51.323 
Epoch 59/1000 
	 loss: 44.1987, MinusLogProbMetric: 44.1987, val_loss: 44.6530, val_MinusLogProbMetric: 44.6530

Epoch 59: val_loss did not improve from 44.55447
196/196 - 64s - loss: 44.1987 - MinusLogProbMetric: 44.1987 - val_loss: 44.6530 - val_MinusLogProbMetric: 44.6530 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 60/1000
2023-10-26 11:08:58.048 
Epoch 60/1000 
	 loss: 44.1597, MinusLogProbMetric: 44.1597, val_loss: 44.5122, val_MinusLogProbMetric: 44.5122

Epoch 60: val_loss improved from 44.55447 to 44.51224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 44.1597 - MinusLogProbMetric: 44.1597 - val_loss: 44.5122 - val_MinusLogProbMetric: 44.5122 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 61/1000
2023-10-26 11:10:05.677 
Epoch 61/1000 
	 loss: 44.4678, MinusLogProbMetric: 44.4678, val_loss: 44.5233, val_MinusLogProbMetric: 44.5233

Epoch 61: val_loss did not improve from 44.51224
196/196 - 67s - loss: 44.4678 - MinusLogProbMetric: 44.4678 - val_loss: 44.5233 - val_MinusLogProbMetric: 44.5233 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 62/1000
2023-10-26 11:11:12.460 
Epoch 62/1000 
	 loss: 44.1323, MinusLogProbMetric: 44.1323, val_loss: 44.9921, val_MinusLogProbMetric: 44.9921

Epoch 62: val_loss did not improve from 44.51224
196/196 - 67s - loss: 44.1323 - MinusLogProbMetric: 44.1323 - val_loss: 44.9921 - val_MinusLogProbMetric: 44.9921 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 63/1000
2023-10-26 11:12:18.542 
Epoch 63/1000 
	 loss: 44.1601, MinusLogProbMetric: 44.1601, val_loss: 44.4556, val_MinusLogProbMetric: 44.4556

Epoch 63: val_loss improved from 44.51224 to 44.45559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.1601 - MinusLogProbMetric: 44.1601 - val_loss: 44.4556 - val_MinusLogProbMetric: 44.4556 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 64/1000
2023-10-26 11:13:25.195 
Epoch 64/1000 
	 loss: 44.1243, MinusLogProbMetric: 44.1243, val_loss: 44.4243, val_MinusLogProbMetric: 44.4243

Epoch 64: val_loss improved from 44.45559 to 44.42430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.1243 - MinusLogProbMetric: 44.1243 - val_loss: 44.4243 - val_MinusLogProbMetric: 44.4243 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 65/1000
2023-10-26 11:14:32.059 
Epoch 65/1000 
	 loss: 44.0738, MinusLogProbMetric: 44.0738, val_loss: 44.4293, val_MinusLogProbMetric: 44.4293

Epoch 65: val_loss did not improve from 44.42430
196/196 - 66s - loss: 44.0738 - MinusLogProbMetric: 44.0738 - val_loss: 44.4293 - val_MinusLogProbMetric: 44.4293 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 66/1000
2023-10-26 11:15:37.159 
Epoch 66/1000 
	 loss: 44.2387, MinusLogProbMetric: 44.2387, val_loss: 44.4253, val_MinusLogProbMetric: 44.4253

Epoch 66: val_loss did not improve from 44.42430
196/196 - 65s - loss: 44.2387 - MinusLogProbMetric: 44.2387 - val_loss: 44.4253 - val_MinusLogProbMetric: 44.4253 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 67/1000
2023-10-26 11:16:45.066 
Epoch 67/1000 
	 loss: 44.0374, MinusLogProbMetric: 44.0374, val_loss: 44.3600, val_MinusLogProbMetric: 44.3600

Epoch 67: val_loss improved from 44.42430 to 44.36002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 69s - loss: 44.0374 - MinusLogProbMetric: 44.0374 - val_loss: 44.3600 - val_MinusLogProbMetric: 44.3600 - lr: 4.5725e-07 - 69s/epoch - 353ms/step
Epoch 68/1000
2023-10-26 11:17:52.264 
Epoch 68/1000 
	 loss: 44.0245, MinusLogProbMetric: 44.0245, val_loss: 44.3255, val_MinusLogProbMetric: 44.3255

Epoch 68: val_loss improved from 44.36002 to 44.32549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 44.0245 - MinusLogProbMetric: 44.0245 - val_loss: 44.3255 - val_MinusLogProbMetric: 44.3255 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 69/1000
2023-10-26 11:18:57.917 
Epoch 69/1000 
	 loss: 44.1091, MinusLogProbMetric: 44.1091, val_loss: 44.2863, val_MinusLogProbMetric: 44.2863

Epoch 69: val_loss improved from 44.32549 to 44.28627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.1091 - MinusLogProbMetric: 44.1091 - val_loss: 44.2863 - val_MinusLogProbMetric: 44.2863 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 70/1000
2023-10-26 11:20:04.344 
Epoch 70/1000 
	 loss: 43.9975, MinusLogProbMetric: 43.9975, val_loss: 44.2951, val_MinusLogProbMetric: 44.2951

Epoch 70: val_loss did not improve from 44.28627
196/196 - 65s - loss: 43.9975 - MinusLogProbMetric: 43.9975 - val_loss: 44.2951 - val_MinusLogProbMetric: 44.2951 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 71/1000
2023-10-26 11:21:10.656 
Epoch 71/1000 
	 loss: 44.4227, MinusLogProbMetric: 44.4227, val_loss: 44.4914, val_MinusLogProbMetric: 44.4914

Epoch 71: val_loss did not improve from 44.28627
196/196 - 66s - loss: 44.4227 - MinusLogProbMetric: 44.4227 - val_loss: 44.4914 - val_MinusLogProbMetric: 44.4914 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 72/1000
2023-10-26 11:22:16.374 
Epoch 72/1000 
	 loss: 44.1557, MinusLogProbMetric: 44.1557, val_loss: 44.7252, val_MinusLogProbMetric: 44.7252

Epoch 72: val_loss did not improve from 44.28627
196/196 - 66s - loss: 44.1557 - MinusLogProbMetric: 44.1557 - val_loss: 44.7252 - val_MinusLogProbMetric: 44.7252 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 73/1000
2023-10-26 11:23:23.086 
Epoch 73/1000 
	 loss: 43.9962, MinusLogProbMetric: 43.9962, val_loss: 44.2506, val_MinusLogProbMetric: 44.2506

Epoch 73: val_loss improved from 44.28627 to 44.25056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 43.9962 - MinusLogProbMetric: 43.9962 - val_loss: 44.2506 - val_MinusLogProbMetric: 44.2506 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 74/1000
2023-10-26 11:24:29.497 
Epoch 74/1000 
	 loss: 43.9010, MinusLogProbMetric: 43.9010, val_loss: 44.3088, val_MinusLogProbMetric: 44.3088

Epoch 74: val_loss did not improve from 44.25056
196/196 - 65s - loss: 43.9010 - MinusLogProbMetric: 43.9010 - val_loss: 44.3088 - val_MinusLogProbMetric: 44.3088 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 75/1000
2023-10-26 11:25:35.273 
Epoch 75/1000 
	 loss: 44.3509, MinusLogProbMetric: 44.3509, val_loss: 44.3162, val_MinusLogProbMetric: 44.3162

Epoch 75: val_loss did not improve from 44.25056
196/196 - 66s - loss: 44.3509 - MinusLogProbMetric: 44.3509 - val_loss: 44.3162 - val_MinusLogProbMetric: 44.3162 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 76/1000
2023-10-26 11:26:41.345 
Epoch 76/1000 
	 loss: 43.9307, MinusLogProbMetric: 43.9307, val_loss: 44.2876, val_MinusLogProbMetric: 44.2876

Epoch 76: val_loss did not improve from 44.25056
196/196 - 66s - loss: 43.9307 - MinusLogProbMetric: 43.9307 - val_loss: 44.2876 - val_MinusLogProbMetric: 44.2876 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 77/1000
2023-10-26 11:27:48.097 
Epoch 77/1000 
	 loss: 43.8419, MinusLogProbMetric: 43.8419, val_loss: 44.2167, val_MinusLogProbMetric: 44.2167

Epoch 77: val_loss improved from 44.25056 to 44.21669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 43.8419 - MinusLogProbMetric: 43.8419 - val_loss: 44.2167 - val_MinusLogProbMetric: 44.2167 - lr: 4.5725e-07 - 68s/epoch - 347ms/step
Epoch 78/1000
2023-10-26 11:28:54.557 
Epoch 78/1000 
	 loss: 43.8044, MinusLogProbMetric: 43.8044, val_loss: 44.1376, val_MinusLogProbMetric: 44.1376

Epoch 78: val_loss improved from 44.21669 to 44.13763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.8044 - MinusLogProbMetric: 43.8044 - val_loss: 44.1376 - val_MinusLogProbMetric: 44.1376 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 79/1000
2023-10-26 11:29:59.428 
Epoch 79/1000 
	 loss: 43.8260, MinusLogProbMetric: 43.8260, val_loss: 44.0802, val_MinusLogProbMetric: 44.0802

Epoch 79: val_loss improved from 44.13763 to 44.08023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 43.8260 - MinusLogProbMetric: 43.8260 - val_loss: 44.0802 - val_MinusLogProbMetric: 44.0802 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 80/1000
2023-10-26 11:31:04.342 
Epoch 80/1000 
	 loss: 43.8650, MinusLogProbMetric: 43.8650, val_loss: 44.1044, val_MinusLogProbMetric: 44.1044

Epoch 80: val_loss did not improve from 44.08023
196/196 - 64s - loss: 43.8650 - MinusLogProbMetric: 43.8650 - val_loss: 44.1044 - val_MinusLogProbMetric: 44.1044 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 81/1000
2023-10-26 11:32:10.022 
Epoch 81/1000 
	 loss: 43.7556, MinusLogProbMetric: 43.7556, val_loss: 44.0878, val_MinusLogProbMetric: 44.0878

Epoch 81: val_loss did not improve from 44.08023
196/196 - 66s - loss: 43.7556 - MinusLogProbMetric: 43.7556 - val_loss: 44.0878 - val_MinusLogProbMetric: 44.0878 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 82/1000
2023-10-26 11:33:15.021 
Epoch 82/1000 
	 loss: 44.0819, MinusLogProbMetric: 44.0819, val_loss: 44.0527, val_MinusLogProbMetric: 44.0527

Epoch 82: val_loss improved from 44.08023 to 44.05270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 44.0819 - MinusLogProbMetric: 44.0819 - val_loss: 44.0527 - val_MinusLogProbMetric: 44.0527 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 83/1000
2023-10-26 11:34:21.768 
Epoch 83/1000 
	 loss: 43.7124, MinusLogProbMetric: 43.7124, val_loss: 44.0696, val_MinusLogProbMetric: 44.0696

Epoch 83: val_loss did not improve from 44.05270
196/196 - 66s - loss: 43.7124 - MinusLogProbMetric: 43.7124 - val_loss: 44.0696 - val_MinusLogProbMetric: 44.0696 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 84/1000
2023-10-26 11:35:26.453 
Epoch 84/1000 
	 loss: 43.7004, MinusLogProbMetric: 43.7004, val_loss: 44.0312, val_MinusLogProbMetric: 44.0312

Epoch 84: val_loss improved from 44.05270 to 44.03121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.7004 - MinusLogProbMetric: 43.7004 - val_loss: 44.0312 - val_MinusLogProbMetric: 44.0312 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 85/1000
2023-10-26 11:36:32.882 
Epoch 85/1000 
	 loss: 43.7151, MinusLogProbMetric: 43.7151, val_loss: 44.0134, val_MinusLogProbMetric: 44.0134

Epoch 85: val_loss improved from 44.03121 to 44.01342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.7151 - MinusLogProbMetric: 43.7151 - val_loss: 44.0134 - val_MinusLogProbMetric: 44.0134 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 86/1000
2023-10-26 11:37:39.397 
Epoch 86/1000 
	 loss: 43.7685, MinusLogProbMetric: 43.7685, val_loss: 43.9931, val_MinusLogProbMetric: 43.9931

Epoch 86: val_loss improved from 44.01342 to 43.99314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.7685 - MinusLogProbMetric: 43.7685 - val_loss: 43.9931 - val_MinusLogProbMetric: 43.9931 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 87/1000
2023-10-26 11:38:44.030 
Epoch 87/1000 
	 loss: 43.8064, MinusLogProbMetric: 43.8064, val_loss: 44.1288, val_MinusLogProbMetric: 44.1288

Epoch 87: val_loss did not improve from 43.99314
196/196 - 63s - loss: 43.8064 - MinusLogProbMetric: 43.8064 - val_loss: 44.1288 - val_MinusLogProbMetric: 44.1288 - lr: 4.5725e-07 - 63s/epoch - 324ms/step
Epoch 88/1000
2023-10-26 11:39:48.485 
Epoch 88/1000 
	 loss: 43.6101, MinusLogProbMetric: 43.6101, val_loss: 43.9220, val_MinusLogProbMetric: 43.9220

Epoch 88: val_loss improved from 43.99314 to 43.92201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.6101 - MinusLogProbMetric: 43.6101 - val_loss: 43.9220 - val_MinusLogProbMetric: 43.9220 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 89/1000
2023-10-26 11:40:55.286 
Epoch 89/1000 
	 loss: 43.6060, MinusLogProbMetric: 43.6060, val_loss: 43.9109, val_MinusLogProbMetric: 43.9109

Epoch 89: val_loss improved from 43.92201 to 43.91088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.6060 - MinusLogProbMetric: 43.6060 - val_loss: 43.9109 - val_MinusLogProbMetric: 43.9109 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 90/1000
2023-10-26 11:42:01.928 
Epoch 90/1000 
	 loss: 43.5968, MinusLogProbMetric: 43.5968, val_loss: 43.9493, val_MinusLogProbMetric: 43.9493

Epoch 90: val_loss did not improve from 43.91088
196/196 - 66s - loss: 43.5968 - MinusLogProbMetric: 43.5968 - val_loss: 43.9493 - val_MinusLogProbMetric: 43.9493 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 91/1000
2023-10-26 11:43:07.485 
Epoch 91/1000 
	 loss: 43.5697, MinusLogProbMetric: 43.5697, val_loss: 43.8919, val_MinusLogProbMetric: 43.8919

Epoch 91: val_loss improved from 43.91088 to 43.89188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.5697 - MinusLogProbMetric: 43.5697 - val_loss: 43.8919 - val_MinusLogProbMetric: 43.8919 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 92/1000
2023-10-26 11:44:14.300 
Epoch 92/1000 
	 loss: 43.7209, MinusLogProbMetric: 43.7209, val_loss: 44.0605, val_MinusLogProbMetric: 44.0605

Epoch 92: val_loss did not improve from 43.89188
196/196 - 66s - loss: 43.7209 - MinusLogProbMetric: 43.7209 - val_loss: 44.0605 - val_MinusLogProbMetric: 44.0605 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 93/1000
2023-10-26 11:45:19.500 
Epoch 93/1000 
	 loss: 43.5194, MinusLogProbMetric: 43.5194, val_loss: 43.8446, val_MinusLogProbMetric: 43.8446

Epoch 93: val_loss improved from 43.89188 to 43.84459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.5194 - MinusLogProbMetric: 43.5194 - val_loss: 43.8446 - val_MinusLogProbMetric: 43.8446 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 94/1000
2023-10-26 11:46:28.096 
Epoch 94/1000 
	 loss: 43.6783, MinusLogProbMetric: 43.6783, val_loss: 43.8349, val_MinusLogProbMetric: 43.8349

Epoch 94: val_loss improved from 43.84459 to 43.83488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 43.6783 - MinusLogProbMetric: 43.6783 - val_loss: 43.8349 - val_MinusLogProbMetric: 43.8349 - lr: 4.5725e-07 - 68s/epoch - 349ms/step
Epoch 95/1000
2023-10-26 11:47:34.465 
Epoch 95/1000 
	 loss: 43.4831, MinusLogProbMetric: 43.4831, val_loss: 43.7770, val_MinusLogProbMetric: 43.7770

Epoch 95: val_loss improved from 43.83488 to 43.77701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.4831 - MinusLogProbMetric: 43.4831 - val_loss: 43.7770 - val_MinusLogProbMetric: 43.7770 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 96/1000
2023-10-26 11:48:41.542 
Epoch 96/1000 
	 loss: 43.4828, MinusLogProbMetric: 43.4828, val_loss: 44.0064, val_MinusLogProbMetric: 44.0064

Epoch 96: val_loss did not improve from 43.77701
196/196 - 66s - loss: 43.4828 - MinusLogProbMetric: 43.4828 - val_loss: 44.0064 - val_MinusLogProbMetric: 44.0064 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 97/1000
2023-10-26 11:49:47.014 
Epoch 97/1000 
	 loss: 43.4818, MinusLogProbMetric: 43.4818, val_loss: 43.7947, val_MinusLogProbMetric: 43.7947

Epoch 97: val_loss did not improve from 43.77701
196/196 - 65s - loss: 43.4818 - MinusLogProbMetric: 43.4818 - val_loss: 43.7947 - val_MinusLogProbMetric: 43.7947 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 98/1000
2023-10-26 11:50:52.921 
Epoch 98/1000 
	 loss: 43.4314, MinusLogProbMetric: 43.4314, val_loss: 43.7346, val_MinusLogProbMetric: 43.7346

Epoch 98: val_loss improved from 43.77701 to 43.73455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.4314 - MinusLogProbMetric: 43.4314 - val_loss: 43.7346 - val_MinusLogProbMetric: 43.7346 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 99/1000
2023-10-26 11:51:59.073 
Epoch 99/1000 
	 loss: 43.4030, MinusLogProbMetric: 43.4030, val_loss: 43.8086, val_MinusLogProbMetric: 43.8086

Epoch 99: val_loss did not improve from 43.73455
196/196 - 65s - loss: 43.4030 - MinusLogProbMetric: 43.4030 - val_loss: 43.8086 - val_MinusLogProbMetric: 43.8086 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 100/1000
2023-10-26 11:53:05.104 
Epoch 100/1000 
	 loss: 43.5373, MinusLogProbMetric: 43.5373, val_loss: 43.8067, val_MinusLogProbMetric: 43.8067

Epoch 100: val_loss did not improve from 43.73455
196/196 - 66s - loss: 43.5373 - MinusLogProbMetric: 43.5373 - val_loss: 43.8067 - val_MinusLogProbMetric: 43.8067 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 101/1000
2023-10-26 11:54:10.329 
Epoch 101/1000 
	 loss: 43.3961, MinusLogProbMetric: 43.3961, val_loss: 43.7306, val_MinusLogProbMetric: 43.7306

Epoch 101: val_loss improved from 43.73455 to 43.73060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.3961 - MinusLogProbMetric: 43.3961 - val_loss: 43.7306 - val_MinusLogProbMetric: 43.7306 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 102/1000
2023-10-26 11:55:17.445 
Epoch 102/1000 
	 loss: 43.3530, MinusLogProbMetric: 43.3530, val_loss: 43.6865, val_MinusLogProbMetric: 43.6865

Epoch 102: val_loss improved from 43.73060 to 43.68651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.3530 - MinusLogProbMetric: 43.3530 - val_loss: 43.6865 - val_MinusLogProbMetric: 43.6865 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 103/1000
2023-10-26 11:56:25.731 
Epoch 103/1000 
	 loss: 43.3985, MinusLogProbMetric: 43.3985, val_loss: 43.7656, val_MinusLogProbMetric: 43.7656

Epoch 103: val_loss did not improve from 43.68651
196/196 - 67s - loss: 43.3985 - MinusLogProbMetric: 43.3985 - val_loss: 43.7656 - val_MinusLogProbMetric: 43.7656 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 104/1000
2023-10-26 11:57:31.324 
Epoch 104/1000 
	 loss: 43.3863, MinusLogProbMetric: 43.3863, val_loss: 43.6864, val_MinusLogProbMetric: 43.6864

Epoch 104: val_loss improved from 43.68651 to 43.68638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.3863 - MinusLogProbMetric: 43.3863 - val_loss: 43.6864 - val_MinusLogProbMetric: 43.6864 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 105/1000
2023-10-26 11:58:37.497 
Epoch 105/1000 
	 loss: 43.3741, MinusLogProbMetric: 43.3741, val_loss: 43.8137, val_MinusLogProbMetric: 43.8137

Epoch 105: val_loss did not improve from 43.68638
196/196 - 65s - loss: 43.3741 - MinusLogProbMetric: 43.3741 - val_loss: 43.8137 - val_MinusLogProbMetric: 43.8137 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 106/1000
2023-10-26 11:59:42.296 
Epoch 106/1000 
	 loss: 43.3185, MinusLogProbMetric: 43.3185, val_loss: 43.6139, val_MinusLogProbMetric: 43.6139

Epoch 106: val_loss improved from 43.68638 to 43.61395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.3185 - MinusLogProbMetric: 43.3185 - val_loss: 43.6139 - val_MinusLogProbMetric: 43.6139 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 107/1000
2023-10-26 12:00:48.314 
Epoch 107/1000 
	 loss: 43.2696, MinusLogProbMetric: 43.2696, val_loss: 43.5797, val_MinusLogProbMetric: 43.5797

Epoch 107: val_loss improved from 43.61395 to 43.57965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.2696 - MinusLogProbMetric: 43.2696 - val_loss: 43.5797 - val_MinusLogProbMetric: 43.5797 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 108/1000
2023-10-26 12:01:55.757 
Epoch 108/1000 
	 loss: 43.2861, MinusLogProbMetric: 43.2861, val_loss: 43.6056, val_MinusLogProbMetric: 43.6056

Epoch 108: val_loss did not improve from 43.57965
196/196 - 66s - loss: 43.2861 - MinusLogProbMetric: 43.2861 - val_loss: 43.6056 - val_MinusLogProbMetric: 43.6056 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 109/1000
2023-10-26 12:03:00.930 
Epoch 109/1000 
	 loss: 43.6964, MinusLogProbMetric: 43.6964, val_loss: 43.5564, val_MinusLogProbMetric: 43.5564

Epoch 109: val_loss improved from 43.57965 to 43.55638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.6964 - MinusLogProbMetric: 43.6964 - val_loss: 43.5564 - val_MinusLogProbMetric: 43.5564 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 110/1000
2023-10-26 12:04:07.774 
Epoch 110/1000 
	 loss: 43.7395, MinusLogProbMetric: 43.7395, val_loss: 43.7413, val_MinusLogProbMetric: 43.7413

Epoch 110: val_loss did not improve from 43.55638
196/196 - 66s - loss: 43.7395 - MinusLogProbMetric: 43.7395 - val_loss: 43.7413 - val_MinusLogProbMetric: 43.7413 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 111/1000
2023-10-26 12:05:12.636 
Epoch 111/1000 
	 loss: 43.2266, MinusLogProbMetric: 43.2266, val_loss: 43.5609, val_MinusLogProbMetric: 43.5609

Epoch 111: val_loss did not improve from 43.55638
196/196 - 65s - loss: 43.2266 - MinusLogProbMetric: 43.2266 - val_loss: 43.5609 - val_MinusLogProbMetric: 43.5609 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 112/1000
2023-10-26 12:06:17.935 
Epoch 112/1000 
	 loss: 43.1711, MinusLogProbMetric: 43.1711, val_loss: 43.5188, val_MinusLogProbMetric: 43.5188

Epoch 112: val_loss improved from 43.55638 to 43.51877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.1711 - MinusLogProbMetric: 43.1711 - val_loss: 43.5188 - val_MinusLogProbMetric: 43.5188 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 113/1000
2023-10-26 12:07:24.360 
Epoch 113/1000 
	 loss: 43.1806, MinusLogProbMetric: 43.1806, val_loss: 43.4768, val_MinusLogProbMetric: 43.4768

Epoch 113: val_loss improved from 43.51877 to 43.47675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.1806 - MinusLogProbMetric: 43.1806 - val_loss: 43.4768 - val_MinusLogProbMetric: 43.4768 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 114/1000
2023-10-26 12:08:31.030 
Epoch 114/1000 
	 loss: 43.1333, MinusLogProbMetric: 43.1333, val_loss: 43.4332, val_MinusLogProbMetric: 43.4332

Epoch 114: val_loss improved from 43.47675 to 43.43324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.1333 - MinusLogProbMetric: 43.1333 - val_loss: 43.4332 - val_MinusLogProbMetric: 43.4332 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 115/1000
2023-10-26 12:09:37.169 
Epoch 115/1000 
	 loss: 43.1182, MinusLogProbMetric: 43.1182, val_loss: 43.4857, val_MinusLogProbMetric: 43.4857

Epoch 115: val_loss did not improve from 43.43324
196/196 - 65s - loss: 43.1182 - MinusLogProbMetric: 43.1182 - val_loss: 43.4857 - val_MinusLogProbMetric: 43.4857 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 116/1000
2023-10-26 12:10:44.008 
Epoch 116/1000 
	 loss: 43.1017, MinusLogProbMetric: 43.1017, val_loss: 43.4918, val_MinusLogProbMetric: 43.4918

Epoch 116: val_loss did not improve from 43.43324
196/196 - 67s - loss: 43.1017 - MinusLogProbMetric: 43.1017 - val_loss: 43.4918 - val_MinusLogProbMetric: 43.4918 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 117/1000
2023-10-26 12:11:50.204 
Epoch 117/1000 
	 loss: 43.1026, MinusLogProbMetric: 43.1026, val_loss: 43.4280, val_MinusLogProbMetric: 43.4280

Epoch 117: val_loss improved from 43.43324 to 43.42797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.1026 - MinusLogProbMetric: 43.1026 - val_loss: 43.4280 - val_MinusLogProbMetric: 43.4280 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 118/1000
2023-10-26 12:12:56.166 
Epoch 118/1000 
	 loss: 43.0633, MinusLogProbMetric: 43.0633, val_loss: 43.4504, val_MinusLogProbMetric: 43.4504

Epoch 118: val_loss did not improve from 43.42797
196/196 - 65s - loss: 43.0633 - MinusLogProbMetric: 43.0633 - val_loss: 43.4504 - val_MinusLogProbMetric: 43.4504 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 119/1000
2023-10-26 12:14:00.929 
Epoch 119/1000 
	 loss: 43.0904, MinusLogProbMetric: 43.0904, val_loss: 43.3930, val_MinusLogProbMetric: 43.3930

Epoch 119: val_loss improved from 43.42797 to 43.39303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 43.0904 - MinusLogProbMetric: 43.0904 - val_loss: 43.3930 - val_MinusLogProbMetric: 43.3930 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 120/1000
2023-10-26 12:15:07.971 
Epoch 120/1000 
	 loss: 43.1417, MinusLogProbMetric: 43.1417, val_loss: 43.3312, val_MinusLogProbMetric: 43.3312

Epoch 120: val_loss improved from 43.39303 to 43.33120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.1417 - MinusLogProbMetric: 43.1417 - val_loss: 43.3312 - val_MinusLogProbMetric: 43.3312 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 121/1000
2023-10-26 12:16:13.492 
Epoch 121/1000 
	 loss: 43.0149, MinusLogProbMetric: 43.0149, val_loss: 43.3770, val_MinusLogProbMetric: 43.3770

Epoch 121: val_loss did not improve from 43.33120
196/196 - 64s - loss: 43.0149 - MinusLogProbMetric: 43.0149 - val_loss: 43.3770 - val_MinusLogProbMetric: 43.3770 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 122/1000
2023-10-26 12:17:18.997 
Epoch 122/1000 
	 loss: 42.9965, MinusLogProbMetric: 42.9965, val_loss: 43.3442, val_MinusLogProbMetric: 43.3442

Epoch 122: val_loss did not improve from 43.33120
196/196 - 66s - loss: 42.9965 - MinusLogProbMetric: 42.9965 - val_loss: 43.3442 - val_MinusLogProbMetric: 43.3442 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 123/1000
2023-10-26 12:18:24.582 
Epoch 123/1000 
	 loss: 43.1359, MinusLogProbMetric: 43.1359, val_loss: 43.2982, val_MinusLogProbMetric: 43.2982

Epoch 123: val_loss improved from 43.33120 to 43.29823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 43.1359 - MinusLogProbMetric: 43.1359 - val_loss: 43.2982 - val_MinusLogProbMetric: 43.2982 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 124/1000
2023-10-26 12:19:30.774 
Epoch 124/1000 
	 loss: 42.9754, MinusLogProbMetric: 42.9754, val_loss: 43.3044, val_MinusLogProbMetric: 43.3044

Epoch 124: val_loss did not improve from 43.29823
196/196 - 65s - loss: 42.9754 - MinusLogProbMetric: 42.9754 - val_loss: 43.3044 - val_MinusLogProbMetric: 43.3044 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 125/1000
2023-10-26 12:20:37.039 
Epoch 125/1000 
	 loss: 42.9364, MinusLogProbMetric: 42.9364, val_loss: 43.3683, val_MinusLogProbMetric: 43.3683

Epoch 125: val_loss did not improve from 43.29823
196/196 - 66s - loss: 42.9364 - MinusLogProbMetric: 42.9364 - val_loss: 43.3683 - val_MinusLogProbMetric: 43.3683 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 126/1000
2023-10-26 12:21:43.073 
Epoch 126/1000 
	 loss: 42.9339, MinusLogProbMetric: 42.9339, val_loss: 43.3133, val_MinusLogProbMetric: 43.3133

Epoch 126: val_loss did not improve from 43.29823
196/196 - 66s - loss: 42.9339 - MinusLogProbMetric: 42.9339 - val_loss: 43.3133 - val_MinusLogProbMetric: 43.3133 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 127/1000
2023-10-26 12:22:50.121 
Epoch 127/1000 
	 loss: 42.9432, MinusLogProbMetric: 42.9432, val_loss: 43.2292, val_MinusLogProbMetric: 43.2292

Epoch 127: val_loss improved from 43.29823 to 43.22921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 42.9432 - MinusLogProbMetric: 42.9432 - val_loss: 43.2292 - val_MinusLogProbMetric: 43.2292 - lr: 4.5725e-07 - 68s/epoch - 347ms/step
Epoch 128/1000
2023-10-26 12:23:56.800 
Epoch 128/1000 
	 loss: 42.9589, MinusLogProbMetric: 42.9589, val_loss: 43.3131, val_MinusLogProbMetric: 43.3131

Epoch 128: val_loss did not improve from 43.22921
196/196 - 66s - loss: 42.9589 - MinusLogProbMetric: 42.9589 - val_loss: 43.3131 - val_MinusLogProbMetric: 43.3131 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 129/1000
2023-10-26 12:25:01.740 
Epoch 129/1000 
	 loss: 42.9079, MinusLogProbMetric: 42.9079, val_loss: 43.2633, val_MinusLogProbMetric: 43.2633

Epoch 129: val_loss did not improve from 43.22921
196/196 - 65s - loss: 42.9079 - MinusLogProbMetric: 42.9079 - val_loss: 43.2633 - val_MinusLogProbMetric: 43.2633 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 130/1000
2023-10-26 12:26:04.607 
Epoch 130/1000 
	 loss: 42.8855, MinusLogProbMetric: 42.8855, val_loss: 43.3253, val_MinusLogProbMetric: 43.3253

Epoch 130: val_loss did not improve from 43.22921
196/196 - 63s - loss: 42.8855 - MinusLogProbMetric: 42.8855 - val_loss: 43.3253 - val_MinusLogProbMetric: 43.3253 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 131/1000
2023-10-26 12:27:00.295 
Epoch 131/1000 
	 loss: 42.8684, MinusLogProbMetric: 42.8684, val_loss: 43.2608, val_MinusLogProbMetric: 43.2608

Epoch 131: val_loss did not improve from 43.22921
196/196 - 56s - loss: 42.8684 - MinusLogProbMetric: 42.8684 - val_loss: 43.2608 - val_MinusLogProbMetric: 43.2608 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 132/1000
2023-10-26 12:28:04.329 
Epoch 132/1000 
	 loss: 42.8621, MinusLogProbMetric: 42.8621, val_loss: 43.1977, val_MinusLogProbMetric: 43.1977

Epoch 132: val_loss improved from 43.22921 to 43.19770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 42.8621 - MinusLogProbMetric: 42.8621 - val_loss: 43.1977 - val_MinusLogProbMetric: 43.1977 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 133/1000
2023-10-26 12:29:02.082 
Epoch 133/1000 
	 loss: 42.8430, MinusLogProbMetric: 42.8430, val_loss: 43.1393, val_MinusLogProbMetric: 43.1393

Epoch 133: val_loss improved from 43.19770 to 43.13928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 42.8430 - MinusLogProbMetric: 42.8430 - val_loss: 43.1393 - val_MinusLogProbMetric: 43.1393 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 134/1000
2023-10-26 12:30:05.207 
Epoch 134/1000 
	 loss: 42.8926, MinusLogProbMetric: 42.8926, val_loss: 43.1407, val_MinusLogProbMetric: 43.1407

Epoch 134: val_loss did not improve from 43.13928
196/196 - 62s - loss: 42.8926 - MinusLogProbMetric: 42.8926 - val_loss: 43.1407 - val_MinusLogProbMetric: 43.1407 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 135/1000
2023-10-26 12:31:04.893 
Epoch 135/1000 
	 loss: 42.9759, MinusLogProbMetric: 42.9759, val_loss: 43.2321, val_MinusLogProbMetric: 43.2321

Epoch 135: val_loss did not improve from 43.13928
196/196 - 60s - loss: 42.9759 - MinusLogProbMetric: 42.9759 - val_loss: 43.2321 - val_MinusLogProbMetric: 43.2321 - lr: 4.5725e-07 - 60s/epoch - 305ms/step
Epoch 136/1000
2023-10-26 12:32:05.515 
Epoch 136/1000 
	 loss: 42.8577, MinusLogProbMetric: 42.8577, val_loss: 43.1514, val_MinusLogProbMetric: 43.1514

Epoch 136: val_loss did not improve from 43.13928
196/196 - 61s - loss: 42.8577 - MinusLogProbMetric: 42.8577 - val_loss: 43.1514 - val_MinusLogProbMetric: 43.1514 - lr: 4.5725e-07 - 61s/epoch - 309ms/step
Epoch 137/1000
2023-10-26 12:33:05.924 
Epoch 137/1000 
	 loss: 42.7921, MinusLogProbMetric: 42.7921, val_loss: 43.2927, val_MinusLogProbMetric: 43.2927

Epoch 137: val_loss did not improve from 43.13928
196/196 - 60s - loss: 42.7921 - MinusLogProbMetric: 42.7921 - val_loss: 43.2927 - val_MinusLogProbMetric: 43.2927 - lr: 4.5725e-07 - 60s/epoch - 308ms/step
Epoch 138/1000
2023-10-26 12:34:02.007 
Epoch 138/1000 
	 loss: 42.7960, MinusLogProbMetric: 42.7960, val_loss: 43.2840, val_MinusLogProbMetric: 43.2840

Epoch 138: val_loss did not improve from 43.13928
196/196 - 56s - loss: 42.7960 - MinusLogProbMetric: 42.7960 - val_loss: 43.2840 - val_MinusLogProbMetric: 43.2840 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 139/1000
2023-10-26 12:35:07.707 
Epoch 139/1000 
	 loss: 42.9767, MinusLogProbMetric: 42.9767, val_loss: 43.8867, val_MinusLogProbMetric: 43.8867

Epoch 139: val_loss did not improve from 43.13928
196/196 - 66s - loss: 42.9767 - MinusLogProbMetric: 42.9767 - val_loss: 43.8867 - val_MinusLogProbMetric: 43.8867 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 140/1000
2023-10-26 12:36:03.875 
Epoch 140/1000 
	 loss: 42.8325, MinusLogProbMetric: 42.8325, val_loss: 43.0582, val_MinusLogProbMetric: 43.0582

Epoch 140: val_loss improved from 43.13928 to 43.05818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.8325 - MinusLogProbMetric: 42.8325 - val_loss: 43.0582 - val_MinusLogProbMetric: 43.0582 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 141/1000
2023-10-26 12:37:08.876 
Epoch 141/1000 
	 loss: 43.0908, MinusLogProbMetric: 43.0908, val_loss: 43.1384, val_MinusLogProbMetric: 43.1384

Epoch 141: val_loss did not improve from 43.05818
196/196 - 64s - loss: 43.0908 - MinusLogProbMetric: 43.0908 - val_loss: 43.1384 - val_MinusLogProbMetric: 43.1384 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 142/1000
2023-10-26 12:38:04.627 
Epoch 142/1000 
	 loss: 42.7255, MinusLogProbMetric: 42.7255, val_loss: 43.0042, val_MinusLogProbMetric: 43.0042

Epoch 142: val_loss improved from 43.05818 to 43.00415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.7255 - MinusLogProbMetric: 42.7255 - val_loss: 43.0042 - val_MinusLogProbMetric: 43.0042 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 143/1000
2023-10-26 12:39:05.921 
Epoch 143/1000 
	 loss: 42.6796, MinusLogProbMetric: 42.6796, val_loss: 43.0023, val_MinusLogProbMetric: 43.0023

Epoch 143: val_loss improved from 43.00415 to 43.00230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 42.6796 - MinusLogProbMetric: 42.6796 - val_loss: 43.0023 - val_MinusLogProbMetric: 43.0023 - lr: 4.5725e-07 - 61s/epoch - 314ms/step
Epoch 144/1000
2023-10-26 12:40:04.474 
Epoch 144/1000 
	 loss: 42.6412, MinusLogProbMetric: 42.6412, val_loss: 42.9799, val_MinusLogProbMetric: 42.9799

Epoch 144: val_loss improved from 43.00230 to 42.97989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 42.6412 - MinusLogProbMetric: 42.6412 - val_loss: 42.9799 - val_MinusLogProbMetric: 42.9799 - lr: 4.5725e-07 - 58s/epoch - 297ms/step
Epoch 145/1000
2023-10-26 12:41:01.760 
Epoch 145/1000 
	 loss: 42.7322, MinusLogProbMetric: 42.7322, val_loss: 42.9509, val_MinusLogProbMetric: 42.9509

Epoch 145: val_loss improved from 42.97989 to 42.95086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.7322 - MinusLogProbMetric: 42.7322 - val_loss: 42.9509 - val_MinusLogProbMetric: 42.9509 - lr: 4.5725e-07 - 57s/epoch - 293ms/step
Epoch 146/1000
2023-10-26 12:42:05.306 
Epoch 146/1000 
	 loss: 42.6141, MinusLogProbMetric: 42.6141, val_loss: 42.9771, val_MinusLogProbMetric: 42.9771

Epoch 146: val_loss did not improve from 42.95086
196/196 - 63s - loss: 42.6141 - MinusLogProbMetric: 42.6141 - val_loss: 42.9771 - val_MinusLogProbMetric: 42.9771 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 147/1000
2023-10-26 12:43:01.571 
Epoch 147/1000 
	 loss: 42.6061, MinusLogProbMetric: 42.6061, val_loss: 42.9515, val_MinusLogProbMetric: 42.9515

Epoch 147: val_loss did not improve from 42.95086
196/196 - 56s - loss: 42.6061 - MinusLogProbMetric: 42.6061 - val_loss: 42.9515 - val_MinusLogProbMetric: 42.9515 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 148/1000
2023-10-26 12:44:06.898 
Epoch 148/1000 
	 loss: 42.5759, MinusLogProbMetric: 42.5759, val_loss: 42.9971, val_MinusLogProbMetric: 42.9971

Epoch 148: val_loss did not improve from 42.95086
196/196 - 65s - loss: 42.5759 - MinusLogProbMetric: 42.5759 - val_loss: 42.9971 - val_MinusLogProbMetric: 42.9971 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 149/1000
2023-10-26 12:45:01.667 
Epoch 149/1000 
	 loss: 42.5977, MinusLogProbMetric: 42.5977, val_loss: 42.9352, val_MinusLogProbMetric: 42.9352

Epoch 149: val_loss improved from 42.95086 to 42.93522, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 42.5977 - MinusLogProbMetric: 42.5977 - val_loss: 42.9352 - val_MinusLogProbMetric: 42.9352 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 150/1000
2023-10-26 12:45:53.994 
Epoch 150/1000 
	 loss: 42.5700, MinusLogProbMetric: 42.5700, val_loss: 44.8706, val_MinusLogProbMetric: 44.8706

Epoch 150: val_loss did not improve from 42.93522
196/196 - 52s - loss: 42.5700 - MinusLogProbMetric: 42.5700 - val_loss: 44.8706 - val_MinusLogProbMetric: 44.8706 - lr: 4.5725e-07 - 52s/epoch - 263ms/step
Epoch 151/1000
2023-10-26 12:46:45.641 
Epoch 151/1000 
	 loss: 42.6990, MinusLogProbMetric: 42.6990, val_loss: 42.8607, val_MinusLogProbMetric: 42.8607

Epoch 151: val_loss improved from 42.93522 to 42.86069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 53s - loss: 42.6990 - MinusLogProbMetric: 42.6990 - val_loss: 42.8607 - val_MinusLogProbMetric: 42.8607 - lr: 4.5725e-07 - 53s/epoch - 269ms/step
Epoch 152/1000
2023-10-26 12:47:39.228 
Epoch 152/1000 
	 loss: 42.5389, MinusLogProbMetric: 42.5389, val_loss: 42.8955, val_MinusLogProbMetric: 42.8955

Epoch 152: val_loss did not improve from 42.86069
196/196 - 53s - loss: 42.5389 - MinusLogProbMetric: 42.5389 - val_loss: 42.8955 - val_MinusLogProbMetric: 42.8955 - lr: 4.5725e-07 - 53s/epoch - 268ms/step
Epoch 153/1000
2023-10-26 12:48:31.834 
Epoch 153/1000 
	 loss: 42.7310, MinusLogProbMetric: 42.7310, val_loss: 42.9848, val_MinusLogProbMetric: 42.9848

Epoch 153: val_loss did not improve from 42.86069
196/196 - 53s - loss: 42.7310 - MinusLogProbMetric: 42.7310 - val_loss: 42.9848 - val_MinusLogProbMetric: 42.9848 - lr: 4.5725e-07 - 53s/epoch - 268ms/step
Epoch 154/1000
2023-10-26 12:49:25.910 
Epoch 154/1000 
	 loss: 42.5365, MinusLogProbMetric: 42.5365, val_loss: 42.9244, val_MinusLogProbMetric: 42.9244

Epoch 154: val_loss did not improve from 42.86069
196/196 - 54s - loss: 42.5365 - MinusLogProbMetric: 42.5365 - val_loss: 42.9244 - val_MinusLogProbMetric: 42.9244 - lr: 4.5725e-07 - 54s/epoch - 276ms/step
Epoch 155/1000
2023-10-26 12:50:17.921 
Epoch 155/1000 
	 loss: 42.4950, MinusLogProbMetric: 42.4950, val_loss: 42.8465, val_MinusLogProbMetric: 42.8465

Epoch 155: val_loss improved from 42.86069 to 42.84654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 53s - loss: 42.4950 - MinusLogProbMetric: 42.4950 - val_loss: 42.8465 - val_MinusLogProbMetric: 42.8465 - lr: 4.5725e-07 - 53s/epoch - 269ms/step
Epoch 156/1000
2023-10-26 12:51:11.658 
Epoch 156/1000 
	 loss: 42.4879, MinusLogProbMetric: 42.4879, val_loss: 43.1285, val_MinusLogProbMetric: 43.1285

Epoch 156: val_loss did not improve from 42.84654
196/196 - 53s - loss: 42.4879 - MinusLogProbMetric: 42.4879 - val_loss: 43.1285 - val_MinusLogProbMetric: 43.1285 - lr: 4.5725e-07 - 53s/epoch - 270ms/step
Epoch 157/1000
2023-10-26 12:52:08.019 
Epoch 157/1000 
	 loss: 42.4821, MinusLogProbMetric: 42.4821, val_loss: 42.8697, val_MinusLogProbMetric: 42.8697

Epoch 157: val_loss did not improve from 42.84654
196/196 - 56s - loss: 42.4821 - MinusLogProbMetric: 42.4821 - val_loss: 42.8697 - val_MinusLogProbMetric: 42.8697 - lr: 4.5725e-07 - 56s/epoch - 288ms/step
Epoch 158/1000
2023-10-26 12:53:09.381 
Epoch 158/1000 
	 loss: 42.4329, MinusLogProbMetric: 42.4329, val_loss: 42.7923, val_MinusLogProbMetric: 42.7923

Epoch 158: val_loss improved from 42.84654 to 42.79232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 42.4329 - MinusLogProbMetric: 42.4329 - val_loss: 42.7923 - val_MinusLogProbMetric: 42.7923 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 159/1000
2023-10-26 12:54:11.197 
Epoch 159/1000 
	 loss: 42.5406, MinusLogProbMetric: 42.5406, val_loss: 42.8215, val_MinusLogProbMetric: 42.8215

Epoch 159: val_loss did not improve from 42.79232
196/196 - 61s - loss: 42.5406 - MinusLogProbMetric: 42.5406 - val_loss: 42.8215 - val_MinusLogProbMetric: 42.8215 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 160/1000
2023-10-26 12:55:07.383 
Epoch 160/1000 
	 loss: 42.4211, MinusLogProbMetric: 42.4211, val_loss: 42.7285, val_MinusLogProbMetric: 42.7285

Epoch 160: val_loss improved from 42.79232 to 42.72850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.4211 - MinusLogProbMetric: 42.4211 - val_loss: 42.7285 - val_MinusLogProbMetric: 42.7285 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 161/1000
2023-10-26 12:56:09.973 
Epoch 161/1000 
	 loss: 42.4015, MinusLogProbMetric: 42.4015, val_loss: 42.7784, val_MinusLogProbMetric: 42.7784

Epoch 161: val_loss did not improve from 42.72850
196/196 - 62s - loss: 42.4015 - MinusLogProbMetric: 42.4015 - val_loss: 42.7784 - val_MinusLogProbMetric: 42.7784 - lr: 4.5725e-07 - 62s/epoch - 314ms/step
Epoch 162/1000
2023-10-26 12:57:09.852 
Epoch 162/1000 
	 loss: 42.4243, MinusLogProbMetric: 42.4243, val_loss: 42.7264, val_MinusLogProbMetric: 42.7264

Epoch 162: val_loss improved from 42.72850 to 42.72641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 42.4243 - MinusLogProbMetric: 42.4243 - val_loss: 42.7264 - val_MinusLogProbMetric: 42.7264 - lr: 4.5725e-07 - 61s/epoch - 311ms/step
Epoch 163/1000
2023-10-26 12:58:05.795 
Epoch 163/1000 
	 loss: 42.4852, MinusLogProbMetric: 42.4852, val_loss: 42.8296, val_MinusLogProbMetric: 42.8296

Epoch 163: val_loss did not improve from 42.72641
196/196 - 55s - loss: 42.4852 - MinusLogProbMetric: 42.4852 - val_loss: 42.8296 - val_MinusLogProbMetric: 42.8296 - lr: 4.5725e-07 - 55s/epoch - 280ms/step
Epoch 164/1000
2023-10-26 12:58:57.887 
Epoch 164/1000 
	 loss: 42.3863, MinusLogProbMetric: 42.3863, val_loss: 42.6778, val_MinusLogProbMetric: 42.6778

Epoch 164: val_loss improved from 42.72641 to 42.67778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 53s - loss: 42.3863 - MinusLogProbMetric: 42.3863 - val_loss: 42.6778 - val_MinusLogProbMetric: 42.6778 - lr: 4.5725e-07 - 53s/epoch - 270ms/step
Epoch 165/1000
2023-10-26 12:59:53.294 
Epoch 165/1000 
	 loss: 42.3702, MinusLogProbMetric: 42.3702, val_loss: 42.7524, val_MinusLogProbMetric: 42.7524

Epoch 165: val_loss did not improve from 42.67778
196/196 - 55s - loss: 42.3702 - MinusLogProbMetric: 42.3702 - val_loss: 42.7524 - val_MinusLogProbMetric: 42.7524 - lr: 4.5725e-07 - 55s/epoch - 279ms/step
Epoch 166/1000
2023-10-26 13:00:48.700 
Epoch 166/1000 
	 loss: 42.5140, MinusLogProbMetric: 42.5140, val_loss: 42.7237, val_MinusLogProbMetric: 42.7237

Epoch 166: val_loss did not improve from 42.67778
196/196 - 55s - loss: 42.5140 - MinusLogProbMetric: 42.5140 - val_loss: 42.7237 - val_MinusLogProbMetric: 42.7237 - lr: 4.5725e-07 - 55s/epoch - 283ms/step
Epoch 167/1000
2023-10-26 13:01:53.092 
Epoch 167/1000 
	 loss: 42.3294, MinusLogProbMetric: 42.3294, val_loss: 42.6710, val_MinusLogProbMetric: 42.6710

Epoch 167: val_loss improved from 42.67778 to 42.67097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 42.3294 - MinusLogProbMetric: 42.3294 - val_loss: 42.6710 - val_MinusLogProbMetric: 42.6710 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 168/1000
2023-10-26 13:02:53.454 
Epoch 168/1000 
	 loss: 42.6763, MinusLogProbMetric: 42.6763, val_loss: 42.6404, val_MinusLogProbMetric: 42.6404

Epoch 168: val_loss improved from 42.67097 to 42.64035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 60s - loss: 42.6763 - MinusLogProbMetric: 42.6763 - val_loss: 42.6404 - val_MinusLogProbMetric: 42.6404 - lr: 4.5725e-07 - 60s/epoch - 307ms/step
Epoch 169/1000
2023-10-26 13:03:51.479 
Epoch 169/1000 
	 loss: 42.3775, MinusLogProbMetric: 42.3775, val_loss: 42.6841, val_MinusLogProbMetric: 42.6841

Epoch 169: val_loss did not improve from 42.64035
196/196 - 57s - loss: 42.3775 - MinusLogProbMetric: 42.3775 - val_loss: 42.6841 - val_MinusLogProbMetric: 42.6841 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 170/1000
2023-10-26 13:04:58.121 
Epoch 170/1000 
	 loss: 42.3069, MinusLogProbMetric: 42.3069, val_loss: 42.6585, val_MinusLogProbMetric: 42.6585

Epoch 170: val_loss did not improve from 42.64035
196/196 - 67s - loss: 42.3069 - MinusLogProbMetric: 42.3069 - val_loss: 42.6585 - val_MinusLogProbMetric: 42.6585 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 171/1000
2023-10-26 13:05:58.312 
Epoch 171/1000 
	 loss: 42.4346, MinusLogProbMetric: 42.4346, val_loss: 44.1861, val_MinusLogProbMetric: 44.1861

Epoch 171: val_loss did not improve from 42.64035
196/196 - 60s - loss: 42.4346 - MinusLogProbMetric: 42.4346 - val_loss: 44.1861 - val_MinusLogProbMetric: 44.1861 - lr: 4.5725e-07 - 60s/epoch - 307ms/step
Epoch 172/1000
2023-10-26 13:06:54.247 
Epoch 172/1000 
	 loss: 42.3749, MinusLogProbMetric: 42.3749, val_loss: 42.6334, val_MinusLogProbMetric: 42.6334

Epoch 172: val_loss improved from 42.64035 to 42.63345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.3749 - MinusLogProbMetric: 42.3749 - val_loss: 42.6334 - val_MinusLogProbMetric: 42.6334 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 173/1000
2023-10-26 13:08:01.568 
Epoch 173/1000 
	 loss: 42.2245, MinusLogProbMetric: 42.2245, val_loss: 42.5602, val_MinusLogProbMetric: 42.5602

Epoch 173: val_loss improved from 42.63345 to 42.56020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 42.2245 - MinusLogProbMetric: 42.2245 - val_loss: 42.5602 - val_MinusLogProbMetric: 42.5602 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 174/1000
2023-10-26 13:09:00.593 
Epoch 174/1000 
	 loss: 42.2219, MinusLogProbMetric: 42.2219, val_loss: 42.5830, val_MinusLogProbMetric: 42.5830

Epoch 174: val_loss did not improve from 42.56020
196/196 - 58s - loss: 42.2219 - MinusLogProbMetric: 42.2219 - val_loss: 42.5830 - val_MinusLogProbMetric: 42.5830 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 175/1000
2023-10-26 13:09:56.436 
Epoch 175/1000 
	 loss: 42.1904, MinusLogProbMetric: 42.1904, val_loss: 42.5266, val_MinusLogProbMetric: 42.5266

Epoch 175: val_loss improved from 42.56020 to 42.52663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.1904 - MinusLogProbMetric: 42.1904 - val_loss: 42.5266 - val_MinusLogProbMetric: 42.5266 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 176/1000
2023-10-26 13:10:58.669 
Epoch 176/1000 
	 loss: 42.2560, MinusLogProbMetric: 42.2560, val_loss: 42.7518, val_MinusLogProbMetric: 42.7518

Epoch 176: val_loss did not improve from 42.52663
196/196 - 61s - loss: 42.2560 - MinusLogProbMetric: 42.2560 - val_loss: 42.7518 - val_MinusLogProbMetric: 42.7518 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 177/1000
2023-10-26 13:11:54.509 
Epoch 177/1000 
	 loss: 42.4651, MinusLogProbMetric: 42.4651, val_loss: 42.7121, val_MinusLogProbMetric: 42.7121

Epoch 177: val_loss did not improve from 42.52663
196/196 - 56s - loss: 42.4651 - MinusLogProbMetric: 42.4651 - val_loss: 42.7121 - val_MinusLogProbMetric: 42.7121 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 178/1000
2023-10-26 13:12:50.362 
Epoch 178/1000 
	 loss: 42.1950, MinusLogProbMetric: 42.1950, val_loss: 42.5232, val_MinusLogProbMetric: 42.5232

Epoch 178: val_loss improved from 42.52663 to 42.52315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.1950 - MinusLogProbMetric: 42.1950 - val_loss: 42.5232 - val_MinusLogProbMetric: 42.5232 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 179/1000
2023-10-26 13:13:57.894 
Epoch 179/1000 
	 loss: 42.2094, MinusLogProbMetric: 42.2094, val_loss: 42.4686, val_MinusLogProbMetric: 42.4686

Epoch 179: val_loss improved from 42.52315 to 42.46859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 42.2094 - MinusLogProbMetric: 42.2094 - val_loss: 42.4686 - val_MinusLogProbMetric: 42.4686 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 180/1000
2023-10-26 13:14:55.225 
Epoch 180/1000 
	 loss: 42.1268, MinusLogProbMetric: 42.1268, val_loss: 42.5328, val_MinusLogProbMetric: 42.5328

Epoch 180: val_loss did not improve from 42.46859
196/196 - 56s - loss: 42.1268 - MinusLogProbMetric: 42.1268 - val_loss: 42.5328 - val_MinusLogProbMetric: 42.5328 - lr: 4.5725e-07 - 56s/epoch - 288ms/step
Epoch 181/1000
2023-10-26 13:15:52.603 
Epoch 181/1000 
	 loss: 42.1090, MinusLogProbMetric: 42.1090, val_loss: 42.5000, val_MinusLogProbMetric: 42.5000

Epoch 181: val_loss did not improve from 42.46859
196/196 - 57s - loss: 42.1090 - MinusLogProbMetric: 42.1090 - val_loss: 42.5000 - val_MinusLogProbMetric: 42.5000 - lr: 4.5725e-07 - 57s/epoch - 293ms/step
Epoch 182/1000
2023-10-26 13:17:00.275 
Epoch 182/1000 
	 loss: 42.2488, MinusLogProbMetric: 42.2488, val_loss: 42.5170, val_MinusLogProbMetric: 42.5170

Epoch 182: val_loss did not improve from 42.46859
196/196 - 68s - loss: 42.2488 - MinusLogProbMetric: 42.2488 - val_loss: 42.5170 - val_MinusLogProbMetric: 42.5170 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 183/1000
2023-10-26 13:17:55.665 
Epoch 183/1000 
	 loss: 42.0955, MinusLogProbMetric: 42.0955, val_loss: 42.5384, val_MinusLogProbMetric: 42.5384

Epoch 183: val_loss did not improve from 42.46859
196/196 - 55s - loss: 42.0955 - MinusLogProbMetric: 42.0955 - val_loss: 42.5384 - val_MinusLogProbMetric: 42.5384 - lr: 4.5725e-07 - 55s/epoch - 283ms/step
Epoch 184/1000
2023-10-26 13:18:53.432 
Epoch 184/1000 
	 loss: 42.2010, MinusLogProbMetric: 42.2010, val_loss: 42.5932, val_MinusLogProbMetric: 42.5932

Epoch 184: val_loss did not improve from 42.46859
196/196 - 58s - loss: 42.2010 - MinusLogProbMetric: 42.2010 - val_loss: 42.5932 - val_MinusLogProbMetric: 42.5932 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 185/1000
2023-10-26 13:19:58.409 
Epoch 185/1000 
	 loss: 42.0817, MinusLogProbMetric: 42.0817, val_loss: 42.4287, val_MinusLogProbMetric: 42.4287

Epoch 185: val_loss improved from 42.46859 to 42.42866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 42.0817 - MinusLogProbMetric: 42.0817 - val_loss: 42.4287 - val_MinusLogProbMetric: 42.4287 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 186/1000
2023-10-26 13:20:55.443 
Epoch 186/1000 
	 loss: 42.0471, MinusLogProbMetric: 42.0471, val_loss: 42.4236, val_MinusLogProbMetric: 42.4236

Epoch 186: val_loss improved from 42.42866 to 42.42362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.0471 - MinusLogProbMetric: 42.0471 - val_loss: 42.4236 - val_MinusLogProbMetric: 42.4236 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 187/1000
2023-10-26 13:21:53.712 
Epoch 187/1000 
	 loss: 42.1998, MinusLogProbMetric: 42.1998, val_loss: 42.4096, val_MinusLogProbMetric: 42.4096

Epoch 187: val_loss improved from 42.42362 to 42.40960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 42.1998 - MinusLogProbMetric: 42.1998 - val_loss: 42.4096 - val_MinusLogProbMetric: 42.4096 - lr: 4.5725e-07 - 59s/epoch - 299ms/step
Epoch 188/1000
2023-10-26 13:23:00.774 
Epoch 188/1000 
	 loss: 42.0237, MinusLogProbMetric: 42.0237, val_loss: 42.3732, val_MinusLogProbMetric: 42.3732

Epoch 188: val_loss improved from 42.40960 to 42.37319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 42.0237 - MinusLogProbMetric: 42.0237 - val_loss: 42.3732 - val_MinusLogProbMetric: 42.3732 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 189/1000
2023-10-26 13:23:57.627 
Epoch 189/1000 
	 loss: 42.2027, MinusLogProbMetric: 42.2027, val_loss: 42.3708, val_MinusLogProbMetric: 42.3708

Epoch 189: val_loss improved from 42.37319 to 42.37078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 42.2027 - MinusLogProbMetric: 42.2027 - val_loss: 42.3708 - val_MinusLogProbMetric: 42.3708 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 190/1000
2023-10-26 13:24:55.618 
Epoch 190/1000 
	 loss: 42.0014, MinusLogProbMetric: 42.0014, val_loss: 42.4951, val_MinusLogProbMetric: 42.4951

Epoch 190: val_loss did not improve from 42.37078
196/196 - 57s - loss: 42.0014 - MinusLogProbMetric: 42.0014 - val_loss: 42.4951 - val_MinusLogProbMetric: 42.4951 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 191/1000
2023-10-26 13:26:01.190 
Epoch 191/1000 
	 loss: 41.9556, MinusLogProbMetric: 41.9556, val_loss: 42.3170, val_MinusLogProbMetric: 42.3170

Epoch 191: val_loss improved from 42.37078 to 42.31700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 41.9556 - MinusLogProbMetric: 41.9556 - val_loss: 42.3170 - val_MinusLogProbMetric: 42.3170 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 192/1000
2023-10-26 13:26:58.323 
Epoch 192/1000 
	 loss: 42.0056, MinusLogProbMetric: 42.0056, val_loss: 42.3362, val_MinusLogProbMetric: 42.3362

Epoch 192: val_loss did not improve from 42.31700
196/196 - 56s - loss: 42.0056 - MinusLogProbMetric: 42.0056 - val_loss: 42.3362 - val_MinusLogProbMetric: 42.3362 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 193/1000
2023-10-26 13:27:55.557 
Epoch 193/1000 
	 loss: 41.9503, MinusLogProbMetric: 41.9503, val_loss: 42.3135, val_MinusLogProbMetric: 42.3135

Epoch 193: val_loss improved from 42.31700 to 42.31348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 41.9503 - MinusLogProbMetric: 41.9503 - val_loss: 42.3135 - val_MinusLogProbMetric: 42.3135 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 194/1000
2023-10-26 13:29:02.382 
Epoch 194/1000 
	 loss: 41.9431, MinusLogProbMetric: 41.9431, val_loss: 42.2908, val_MinusLogProbMetric: 42.2908

Epoch 194: val_loss improved from 42.31348 to 42.29079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 41.9431 - MinusLogProbMetric: 41.9431 - val_loss: 42.2908 - val_MinusLogProbMetric: 42.2908 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 195/1000
2023-10-26 13:29:59.483 
Epoch 195/1000 
	 loss: 41.9637, MinusLogProbMetric: 41.9637, val_loss: 42.3455, val_MinusLogProbMetric: 42.3455

Epoch 195: val_loss did not improve from 42.29079
196/196 - 56s - loss: 41.9637 - MinusLogProbMetric: 41.9637 - val_loss: 42.3455 - val_MinusLogProbMetric: 42.3455 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 196/1000
2023-10-26 13:30:55.436 
Epoch 196/1000 
	 loss: 41.9031, MinusLogProbMetric: 41.9031, val_loss: 42.2830, val_MinusLogProbMetric: 42.2830

Epoch 196: val_loss improved from 42.29079 to 42.28300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 41.9031 - MinusLogProbMetric: 41.9031 - val_loss: 42.2830 - val_MinusLogProbMetric: 42.2830 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 197/1000
2023-10-26 13:31:58.162 
Epoch 197/1000 
	 loss: 42.0072, MinusLogProbMetric: 42.0072, val_loss: 42.2610, val_MinusLogProbMetric: 42.2610

Epoch 197: val_loss improved from 42.28300 to 42.26103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 42.0072 - MinusLogProbMetric: 42.0072 - val_loss: 42.2610 - val_MinusLogProbMetric: 42.2610 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 198/1000
2023-10-26 13:32:57.550 
Epoch 198/1000 
	 loss: 41.8815, MinusLogProbMetric: 41.8815, val_loss: 42.2721, val_MinusLogProbMetric: 42.2721

Epoch 198: val_loss did not improve from 42.26103
196/196 - 58s - loss: 41.8815 - MinusLogProbMetric: 41.8815 - val_loss: 42.2721 - val_MinusLogProbMetric: 42.2721 - lr: 4.5725e-07 - 58s/epoch - 298ms/step
Epoch 199/1000
2023-10-26 13:33:53.528 
Epoch 199/1000 
	 loss: 41.9923, MinusLogProbMetric: 41.9923, val_loss: 42.3592, val_MinusLogProbMetric: 42.3592

Epoch 199: val_loss did not improve from 42.26103
196/196 - 56s - loss: 41.9923 - MinusLogProbMetric: 41.9923 - val_loss: 42.3592 - val_MinusLogProbMetric: 42.3592 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 200/1000
2023-10-26 13:34:53.780 
Epoch 200/1000 
	 loss: 41.8751, MinusLogProbMetric: 41.8751, val_loss: 42.2337, val_MinusLogProbMetric: 42.2337

Epoch 200: val_loss improved from 42.26103 to 42.23369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 41.8751 - MinusLogProbMetric: 41.8751 - val_loss: 42.2337 - val_MinusLogProbMetric: 42.2337 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 201/1000
2023-10-26 13:35:55.211 
Epoch 201/1000 
	 loss: 41.8811, MinusLogProbMetric: 41.8811, val_loss: 42.2227, val_MinusLogProbMetric: 42.2227

Epoch 201: val_loss improved from 42.23369 to 42.22269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 41.8811 - MinusLogProbMetric: 41.8811 - val_loss: 42.2227 - val_MinusLogProbMetric: 42.2227 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 202/1000
2023-10-26 13:36:52.528 
Epoch 202/1000 
	 loss: 41.8232, MinusLogProbMetric: 41.8232, val_loss: 42.1927, val_MinusLogProbMetric: 42.1927

Epoch 202: val_loss improved from 42.22269 to 42.19267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 41.8232 - MinusLogProbMetric: 41.8232 - val_loss: 42.1927 - val_MinusLogProbMetric: 42.1927 - lr: 4.5725e-07 - 57s/epoch - 293ms/step
Epoch 203/1000
2023-10-26 13:37:57.612 
Epoch 203/1000 
	 loss: 42.0571, MinusLogProbMetric: 42.0571, val_loss: 42.1842, val_MinusLogProbMetric: 42.1842

Epoch 203: val_loss improved from 42.19267 to 42.18417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 42.0571 - MinusLogProbMetric: 42.0571 - val_loss: 42.1842 - val_MinusLogProbMetric: 42.1842 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 204/1000
2023-10-26 13:39:01.151 
Epoch 204/1000 
	 loss: 41.8229, MinusLogProbMetric: 41.8229, val_loss: 42.1252, val_MinusLogProbMetric: 42.1252

Epoch 204: val_loss improved from 42.18417 to 42.12517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 41.8229 - MinusLogProbMetric: 41.8229 - val_loss: 42.1252 - val_MinusLogProbMetric: 42.1252 - lr: 4.5725e-07 - 63s/epoch - 323ms/step
Epoch 205/1000
2023-10-26 13:39:58.903 
Epoch 205/1000 
	 loss: 41.7981, MinusLogProbMetric: 41.7981, val_loss: 42.2344, val_MinusLogProbMetric: 42.2344

Epoch 205: val_loss did not improve from 42.12517
196/196 - 57s - loss: 41.7981 - MinusLogProbMetric: 41.7981 - val_loss: 42.2344 - val_MinusLogProbMetric: 42.2344 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 206/1000
2023-10-26 13:41:03.414 
Epoch 206/1000 
	 loss: 41.8327, MinusLogProbMetric: 41.8327, val_loss: 42.1234, val_MinusLogProbMetric: 42.1234

Epoch 206: val_loss improved from 42.12517 to 42.12341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.8327 - MinusLogProbMetric: 41.8327 - val_loss: 42.1234 - val_MinusLogProbMetric: 42.1234 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 207/1000
2023-10-26 13:42:06.921 
Epoch 207/1000 
	 loss: 41.7786, MinusLogProbMetric: 41.7786, val_loss: 42.1331, val_MinusLogProbMetric: 42.1331

Epoch 207: val_loss did not improve from 42.12341
196/196 - 62s - loss: 41.7786 - MinusLogProbMetric: 41.7786 - val_loss: 42.1331 - val_MinusLogProbMetric: 42.1331 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 208/1000
2023-10-26 13:43:05.737 
Epoch 208/1000 
	 loss: 41.7720, MinusLogProbMetric: 41.7720, val_loss: 42.1221, val_MinusLogProbMetric: 42.1221

Epoch 208: val_loss improved from 42.12341 to 42.12207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 60s - loss: 41.7720 - MinusLogProbMetric: 41.7720 - val_loss: 42.1221 - val_MinusLogProbMetric: 42.1221 - lr: 4.5725e-07 - 60s/epoch - 304ms/step
Epoch 209/1000
2023-10-26 13:44:09.006 
Epoch 209/1000 
	 loss: 41.7321, MinusLogProbMetric: 41.7321, val_loss: 42.1284, val_MinusLogProbMetric: 42.1284

Epoch 209: val_loss did not improve from 42.12207
196/196 - 62s - loss: 41.7321 - MinusLogProbMetric: 41.7321 - val_loss: 42.1284 - val_MinusLogProbMetric: 42.1284 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 210/1000
2023-10-26 13:45:13.930 
Epoch 210/1000 
	 loss: 41.7417, MinusLogProbMetric: 41.7417, val_loss: 42.1318, val_MinusLogProbMetric: 42.1318

Epoch 210: val_loss did not improve from 42.12207
196/196 - 65s - loss: 41.7417 - MinusLogProbMetric: 41.7417 - val_loss: 42.1318 - val_MinusLogProbMetric: 42.1318 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 211/1000
2023-10-26 13:46:11.156 
Epoch 211/1000 
	 loss: 41.7298, MinusLogProbMetric: 41.7298, val_loss: 42.1237, val_MinusLogProbMetric: 42.1237

Epoch 211: val_loss did not improve from 42.12207
196/196 - 57s - loss: 41.7298 - MinusLogProbMetric: 41.7298 - val_loss: 42.1237 - val_MinusLogProbMetric: 42.1237 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 212/1000
2023-10-26 13:47:10.610 
Epoch 212/1000 
	 loss: 41.7174, MinusLogProbMetric: 41.7174, val_loss: 42.0876, val_MinusLogProbMetric: 42.0876

Epoch 212: val_loss improved from 42.12207 to 42.08764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 41.7174 - MinusLogProbMetric: 41.7174 - val_loss: 42.0876 - val_MinusLogProbMetric: 42.0876 - lr: 4.5725e-07 - 61s/epoch - 309ms/step
Epoch 213/1000
2023-10-26 13:48:18.599 
Epoch 213/1000 
	 loss: 42.0331, MinusLogProbMetric: 42.0331, val_loss: 42.0263, val_MinusLogProbMetric: 42.0263

Epoch 213: val_loss improved from 42.08764 to 42.02627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 42.0331 - MinusLogProbMetric: 42.0331 - val_loss: 42.0263 - val_MinusLogProbMetric: 42.0263 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 214/1000
2023-10-26 13:49:17.376 
Epoch 214/1000 
	 loss: 41.7166, MinusLogProbMetric: 41.7166, val_loss: 42.1187, val_MinusLogProbMetric: 42.1187

Epoch 214: val_loss did not improve from 42.02627
196/196 - 58s - loss: 41.7166 - MinusLogProbMetric: 41.7166 - val_loss: 42.1187 - val_MinusLogProbMetric: 42.1187 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 215/1000
2023-10-26 13:50:15.556 
Epoch 215/1000 
	 loss: 41.6719, MinusLogProbMetric: 41.6719, val_loss: 42.0496, val_MinusLogProbMetric: 42.0496

Epoch 215: val_loss did not improve from 42.02627
196/196 - 58s - loss: 41.6719 - MinusLogProbMetric: 41.6719 - val_loss: 42.0496 - val_MinusLogProbMetric: 42.0496 - lr: 4.5725e-07 - 58s/epoch - 297ms/step
Epoch 216/1000
2023-10-26 13:51:19.806 
Epoch 216/1000 
	 loss: 41.7118, MinusLogProbMetric: 41.7118, val_loss: 42.0070, val_MinusLogProbMetric: 42.0070

Epoch 216: val_loss improved from 42.02627 to 42.00699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 41.7118 - MinusLogProbMetric: 41.7118 - val_loss: 42.0070 - val_MinusLogProbMetric: 42.0070 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 217/1000
2023-10-26 13:52:22.277 
Epoch 217/1000 
	 loss: 41.7151, MinusLogProbMetric: 41.7151, val_loss: 41.9517, val_MinusLogProbMetric: 41.9517

Epoch 217: val_loss improved from 42.00699 to 41.95169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 41.7151 - MinusLogProbMetric: 41.7151 - val_loss: 41.9517 - val_MinusLogProbMetric: 41.9517 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 218/1000
2023-10-26 13:53:19.157 
Epoch 218/1000 
	 loss: 41.6225, MinusLogProbMetric: 41.6225, val_loss: 41.9815, val_MinusLogProbMetric: 41.9815

Epoch 218: val_loss did not improve from 41.95169
196/196 - 56s - loss: 41.6225 - MinusLogProbMetric: 41.6225 - val_loss: 41.9815 - val_MinusLogProbMetric: 41.9815 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 219/1000
2023-10-26 13:54:20.235 
Epoch 219/1000 
	 loss: 41.6193, MinusLogProbMetric: 41.6193, val_loss: 42.2256, val_MinusLogProbMetric: 42.2256

Epoch 219: val_loss did not improve from 41.95169
196/196 - 61s - loss: 41.6193 - MinusLogProbMetric: 41.6193 - val_loss: 42.2256 - val_MinusLogProbMetric: 42.2256 - lr: 4.5725e-07 - 61s/epoch - 312ms/step
Epoch 220/1000
2023-10-26 13:55:24.110 
Epoch 220/1000 
	 loss: 41.6105, MinusLogProbMetric: 41.6105, val_loss: 41.9707, val_MinusLogProbMetric: 41.9707

Epoch 220: val_loss did not improve from 41.95169
196/196 - 64s - loss: 41.6105 - MinusLogProbMetric: 41.6105 - val_loss: 41.9707 - val_MinusLogProbMetric: 41.9707 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 221/1000
2023-10-26 13:56:21.773 
Epoch 221/1000 
	 loss: 41.6484, MinusLogProbMetric: 41.6484, val_loss: 41.9961, val_MinusLogProbMetric: 41.9961

Epoch 221: val_loss did not improve from 41.95169
196/196 - 58s - loss: 41.6484 - MinusLogProbMetric: 41.6484 - val_loss: 41.9961 - val_MinusLogProbMetric: 41.9961 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 222/1000
2023-10-26 13:57:22.443 
Epoch 222/1000 
	 loss: 41.6288, MinusLogProbMetric: 41.6288, val_loss: 42.0024, val_MinusLogProbMetric: 42.0024

Epoch 222: val_loss did not improve from 41.95169
196/196 - 61s - loss: 41.6288 - MinusLogProbMetric: 41.6288 - val_loss: 42.0024 - val_MinusLogProbMetric: 42.0024 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 223/1000
2023-10-26 13:58:26.185 
Epoch 223/1000 
	 loss: 41.5658, MinusLogProbMetric: 41.5658, val_loss: 41.9136, val_MinusLogProbMetric: 41.9136

Epoch 223: val_loss improved from 41.95169 to 41.91357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 41.5658 - MinusLogProbMetric: 41.5658 - val_loss: 41.9136 - val_MinusLogProbMetric: 41.9136 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 224/1000
2023-10-26 13:59:26.444 
Epoch 224/1000 
	 loss: 41.6354, MinusLogProbMetric: 41.6354, val_loss: 41.9391, val_MinusLogProbMetric: 41.9391

Epoch 224: val_loss did not improve from 41.91357
196/196 - 59s - loss: 41.6354 - MinusLogProbMetric: 41.6354 - val_loss: 41.9391 - val_MinusLogProbMetric: 41.9391 - lr: 4.5725e-07 - 59s/epoch - 302ms/step
Epoch 225/1000
2023-10-26 14:00:26.524 
Epoch 225/1000 
	 loss: 41.7219, MinusLogProbMetric: 41.7219, val_loss: 42.2249, val_MinusLogProbMetric: 42.2249

Epoch 225: val_loss did not improve from 41.91357
196/196 - 60s - loss: 41.7219 - MinusLogProbMetric: 41.7219 - val_loss: 42.2249 - val_MinusLogProbMetric: 42.2249 - lr: 4.5725e-07 - 60s/epoch - 306ms/step
Epoch 226/1000
2023-10-26 14:01:27.236 
Epoch 226/1000 
	 loss: 41.5497, MinusLogProbMetric: 41.5497, val_loss: 41.8826, val_MinusLogProbMetric: 41.8826

Epoch 226: val_loss improved from 41.91357 to 41.88263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 41.5497 - MinusLogProbMetric: 41.5497 - val_loss: 41.8826 - val_MinusLogProbMetric: 41.8826 - lr: 4.5725e-07 - 62s/epoch - 314ms/step
Epoch 227/1000
2023-10-26 14:02:31.333 
Epoch 227/1000 
	 loss: 41.5206, MinusLogProbMetric: 41.5206, val_loss: 41.8619, val_MinusLogProbMetric: 41.8619

Epoch 227: val_loss improved from 41.88263 to 41.86185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 41.5206 - MinusLogProbMetric: 41.5206 - val_loss: 41.8619 - val_MinusLogProbMetric: 41.8619 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 228/1000
2023-10-26 14:03:37.542 
Epoch 228/1000 
	 loss: 41.7035, MinusLogProbMetric: 41.7035, val_loss: 41.9136, val_MinusLogProbMetric: 41.9136

Epoch 228: val_loss did not improve from 41.86185
196/196 - 65s - loss: 41.7035 - MinusLogProbMetric: 41.7035 - val_loss: 41.9136 - val_MinusLogProbMetric: 41.9136 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 229/1000
2023-10-26 14:04:44.279 
Epoch 229/1000 
	 loss: 41.5007, MinusLogProbMetric: 41.5007, val_loss: 41.9006, val_MinusLogProbMetric: 41.9006

Epoch 229: val_loss did not improve from 41.86185
196/196 - 67s - loss: 41.5007 - MinusLogProbMetric: 41.5007 - val_loss: 41.9006 - val_MinusLogProbMetric: 41.9006 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 230/1000
2023-10-26 14:05:50.543 
Epoch 230/1000 
	 loss: 41.4994, MinusLogProbMetric: 41.4994, val_loss: 43.4159, val_MinusLogProbMetric: 43.4159

Epoch 230: val_loss did not improve from 41.86185
196/196 - 66s - loss: 41.4994 - MinusLogProbMetric: 41.4994 - val_loss: 43.4159 - val_MinusLogProbMetric: 43.4159 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 231/1000
2023-10-26 14:06:57.125 
Epoch 231/1000 
	 loss: 41.5741, MinusLogProbMetric: 41.5741, val_loss: 41.7949, val_MinusLogProbMetric: 41.7949

Epoch 231: val_loss improved from 41.86185 to 41.79494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 41.5741 - MinusLogProbMetric: 41.5741 - val_loss: 41.7949 - val_MinusLogProbMetric: 41.7949 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 232/1000
2023-10-26 14:08:03.277 
Epoch 232/1000 
	 loss: 41.4507, MinusLogProbMetric: 41.4507, val_loss: 41.9903, val_MinusLogProbMetric: 41.9903

Epoch 232: val_loss did not improve from 41.79494
196/196 - 65s - loss: 41.4507 - MinusLogProbMetric: 41.4507 - val_loss: 41.9903 - val_MinusLogProbMetric: 41.9903 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 233/1000
2023-10-26 14:09:11.110 
Epoch 233/1000 
	 loss: 41.4513, MinusLogProbMetric: 41.4513, val_loss: 41.8206, val_MinusLogProbMetric: 41.8206

Epoch 233: val_loss did not improve from 41.79494
196/196 - 68s - loss: 41.4513 - MinusLogProbMetric: 41.4513 - val_loss: 41.8206 - val_MinusLogProbMetric: 41.8206 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 234/1000
2023-10-26 14:10:08.971 
Epoch 234/1000 
	 loss: 41.4299, MinusLogProbMetric: 41.4299, val_loss: 41.8134, val_MinusLogProbMetric: 41.8134

Epoch 234: val_loss did not improve from 41.79494
196/196 - 58s - loss: 41.4299 - MinusLogProbMetric: 41.4299 - val_loss: 41.8134 - val_MinusLogProbMetric: 41.8134 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 235/1000
2023-10-26 14:10:59.553 
Epoch 235/1000 
	 loss: 41.5225, MinusLogProbMetric: 41.5225, val_loss: 41.7747, val_MinusLogProbMetric: 41.7747

Epoch 235: val_loss improved from 41.79494 to 41.77470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 52s - loss: 41.5225 - MinusLogProbMetric: 41.5225 - val_loss: 41.7747 - val_MinusLogProbMetric: 41.7747 - lr: 4.5725e-07 - 52s/epoch - 263ms/step
Epoch 236/1000
2023-10-26 14:12:02.041 
Epoch 236/1000 
	 loss: 41.5969, MinusLogProbMetric: 41.5969, val_loss: 41.9181, val_MinusLogProbMetric: 41.9181

Epoch 236: val_loss did not improve from 41.77470
196/196 - 61s - loss: 41.5969 - MinusLogProbMetric: 41.5969 - val_loss: 41.9181 - val_MinusLogProbMetric: 41.9181 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 237/1000
2023-10-26 14:13:06.844 
Epoch 237/1000 
	 loss: 41.4587, MinusLogProbMetric: 41.4587, val_loss: 41.7653, val_MinusLogProbMetric: 41.7653

Epoch 237: val_loss improved from 41.77470 to 41.76534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.4587 - MinusLogProbMetric: 41.4587 - val_loss: 41.7653 - val_MinusLogProbMetric: 41.7653 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 238/1000
2023-10-26 14:14:14.500 
Epoch 238/1000 
	 loss: 41.4377, MinusLogProbMetric: 41.4377, val_loss: 42.2928, val_MinusLogProbMetric: 42.2928

Epoch 238: val_loss did not improve from 41.76534
196/196 - 67s - loss: 41.4377 - MinusLogProbMetric: 41.4377 - val_loss: 42.2928 - val_MinusLogProbMetric: 42.2928 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 239/1000
2023-10-26 14:15:19.744 
Epoch 239/1000 
	 loss: 41.4262, MinusLogProbMetric: 41.4262, val_loss: 41.7258, val_MinusLogProbMetric: 41.7258

Epoch 239: val_loss improved from 41.76534 to 41.72577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.4262 - MinusLogProbMetric: 41.4262 - val_loss: 41.7258 - val_MinusLogProbMetric: 41.7258 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 240/1000
2023-10-26 14:16:27.471 
Epoch 240/1000 
	 loss: 41.3912, MinusLogProbMetric: 41.3912, val_loss: 41.8060, val_MinusLogProbMetric: 41.8060

Epoch 240: val_loss did not improve from 41.72577
196/196 - 67s - loss: 41.3912 - MinusLogProbMetric: 41.3912 - val_loss: 41.8060 - val_MinusLogProbMetric: 41.8060 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 241/1000
2023-10-26 14:17:33.416 
Epoch 241/1000 
	 loss: 41.4040, MinusLogProbMetric: 41.4040, val_loss: 41.7360, val_MinusLogProbMetric: 41.7360

Epoch 241: val_loss did not improve from 41.72577
196/196 - 66s - loss: 41.4040 - MinusLogProbMetric: 41.4040 - val_loss: 41.7360 - val_MinusLogProbMetric: 41.7360 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 242/1000
2023-10-26 14:18:28.549 
Epoch 242/1000 
	 loss: 41.6424, MinusLogProbMetric: 41.6424, val_loss: 42.0259, val_MinusLogProbMetric: 42.0259

Epoch 242: val_loss did not improve from 41.72577
196/196 - 55s - loss: 41.6424 - MinusLogProbMetric: 41.6424 - val_loss: 42.0259 - val_MinusLogProbMetric: 42.0259 - lr: 4.5725e-07 - 55s/epoch - 281ms/step
Epoch 243/1000
2023-10-26 14:19:20.876 
Epoch 243/1000 
	 loss: 41.3884, MinusLogProbMetric: 41.3884, val_loss: 41.7020, val_MinusLogProbMetric: 41.7020

Epoch 243: val_loss improved from 41.72577 to 41.70201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 53s - loss: 41.3884 - MinusLogProbMetric: 41.3884 - val_loss: 41.7020 - val_MinusLogProbMetric: 41.7020 - lr: 4.5725e-07 - 53s/epoch - 272ms/step
Epoch 244/1000
2023-10-26 14:20:26.178 
Epoch 244/1000 
	 loss: 41.3175, MinusLogProbMetric: 41.3175, val_loss: 41.6887, val_MinusLogProbMetric: 41.6887

Epoch 244: val_loss improved from 41.70201 to 41.68869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 41.3175 - MinusLogProbMetric: 41.3175 - val_loss: 41.6887 - val_MinusLogProbMetric: 41.6887 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 245/1000
2023-10-26 14:21:32.506 
Epoch 245/1000 
	 loss: 41.3384, MinusLogProbMetric: 41.3384, val_loss: 41.6819, val_MinusLogProbMetric: 41.6819

Epoch 245: val_loss improved from 41.68869 to 41.68192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.3384 - MinusLogProbMetric: 41.3384 - val_loss: 41.6819 - val_MinusLogProbMetric: 41.6819 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 246/1000
2023-10-26 14:22:39.515 
Epoch 246/1000 
	 loss: 41.4045, MinusLogProbMetric: 41.4045, val_loss: 41.9276, val_MinusLogProbMetric: 41.9276

Epoch 246: val_loss did not improve from 41.68192
196/196 - 66s - loss: 41.4045 - MinusLogProbMetric: 41.4045 - val_loss: 41.9276 - val_MinusLogProbMetric: 41.9276 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 247/1000
2023-10-26 14:23:44.872 
Epoch 247/1000 
	 loss: 41.3245, MinusLogProbMetric: 41.3245, val_loss: 41.7144, val_MinusLogProbMetric: 41.7144

Epoch 247: val_loss did not improve from 41.68192
196/196 - 65s - loss: 41.3245 - MinusLogProbMetric: 41.3245 - val_loss: 41.7144 - val_MinusLogProbMetric: 41.7144 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 248/1000
2023-10-26 14:24:49.840 
Epoch 248/1000 
	 loss: 41.3756, MinusLogProbMetric: 41.3756, val_loss: 41.6757, val_MinusLogProbMetric: 41.6757

Epoch 248: val_loss improved from 41.68192 to 41.67573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.3756 - MinusLogProbMetric: 41.3756 - val_loss: 41.6757 - val_MinusLogProbMetric: 41.6757 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 249/1000
2023-10-26 14:25:55.880 
Epoch 249/1000 
	 loss: 41.2588, MinusLogProbMetric: 41.2588, val_loss: 41.6252, val_MinusLogProbMetric: 41.6252

Epoch 249: val_loss improved from 41.67573 to 41.62524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.2588 - MinusLogProbMetric: 41.2588 - val_loss: 41.6252 - val_MinusLogProbMetric: 41.6252 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 250/1000
2023-10-26 14:27:03.628 
Epoch 250/1000 
	 loss: 41.2421, MinusLogProbMetric: 41.2421, val_loss: 41.6244, val_MinusLogProbMetric: 41.6244

Epoch 250: val_loss improved from 41.62524 to 41.62443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 41.2421 - MinusLogProbMetric: 41.2421 - val_loss: 41.6244 - val_MinusLogProbMetric: 41.6244 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 251/1000
2023-10-26 14:28:09.556 
Epoch 251/1000 
	 loss: 41.2313, MinusLogProbMetric: 41.2313, val_loss: 42.1767, val_MinusLogProbMetric: 42.1767

Epoch 251: val_loss did not improve from 41.62443
196/196 - 65s - loss: 41.2313 - MinusLogProbMetric: 41.2313 - val_loss: 42.1767 - val_MinusLogProbMetric: 42.1767 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 252/1000
2023-10-26 14:29:15.008 
Epoch 252/1000 
	 loss: 41.8752, MinusLogProbMetric: 41.8752, val_loss: 41.9285, val_MinusLogProbMetric: 41.9285

Epoch 252: val_loss did not improve from 41.62443
196/196 - 65s - loss: 41.8752 - MinusLogProbMetric: 41.8752 - val_loss: 41.9285 - val_MinusLogProbMetric: 41.9285 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 253/1000
2023-10-26 14:30:20.105 
Epoch 253/1000 
	 loss: 41.2972, MinusLogProbMetric: 41.2972, val_loss: 41.5769, val_MinusLogProbMetric: 41.5769

Epoch 253: val_loss improved from 41.62443 to 41.57689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.2972 - MinusLogProbMetric: 41.2972 - val_loss: 41.5769 - val_MinusLogProbMetric: 41.5769 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 254/1000
2023-10-26 14:31:27.639 
Epoch 254/1000 
	 loss: 41.2064, MinusLogProbMetric: 41.2064, val_loss: 41.5232, val_MinusLogProbMetric: 41.5232

Epoch 254: val_loss improved from 41.57689 to 41.52317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 41.2064 - MinusLogProbMetric: 41.2064 - val_loss: 41.5232 - val_MinusLogProbMetric: 41.5232 - lr: 4.5725e-07 - 68s/epoch - 344ms/step
Epoch 255/1000
2023-10-26 14:32:33.658 
Epoch 255/1000 
	 loss: 41.2116, MinusLogProbMetric: 41.2116, val_loss: 41.5417, val_MinusLogProbMetric: 41.5417

Epoch 255: val_loss did not improve from 41.52317
196/196 - 65s - loss: 41.2116 - MinusLogProbMetric: 41.2116 - val_loss: 41.5417 - val_MinusLogProbMetric: 41.5417 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 256/1000
2023-10-26 14:33:38.079 
Epoch 256/1000 
	 loss: 41.2116, MinusLogProbMetric: 41.2116, val_loss: 41.5686, val_MinusLogProbMetric: 41.5686

Epoch 256: val_loss did not improve from 41.52317
196/196 - 64s - loss: 41.2116 - MinusLogProbMetric: 41.2116 - val_loss: 41.5686 - val_MinusLogProbMetric: 41.5686 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 257/1000
2023-10-26 14:34:44.071 
Epoch 257/1000 
	 loss: 41.2084, MinusLogProbMetric: 41.2084, val_loss: 41.6998, val_MinusLogProbMetric: 41.6998

Epoch 257: val_loss did not improve from 41.52317
196/196 - 66s - loss: 41.2084 - MinusLogProbMetric: 41.2084 - val_loss: 41.6998 - val_MinusLogProbMetric: 41.6998 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 258/1000
2023-10-26 14:35:49.507 
Epoch 258/1000 
	 loss: 41.2165, MinusLogProbMetric: 41.2165, val_loss: 41.5401, val_MinusLogProbMetric: 41.5401

Epoch 258: val_loss did not improve from 41.52317
196/196 - 65s - loss: 41.2165 - MinusLogProbMetric: 41.2165 - val_loss: 41.5401 - val_MinusLogProbMetric: 41.5401 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 259/1000
2023-10-26 14:36:55.184 
Epoch 259/1000 
	 loss: 41.1799, MinusLogProbMetric: 41.1799, val_loss: 45.3325, val_MinusLogProbMetric: 45.3325

Epoch 259: val_loss did not improve from 41.52317
196/196 - 66s - loss: 41.1799 - MinusLogProbMetric: 41.1799 - val_loss: 45.3325 - val_MinusLogProbMetric: 45.3325 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 260/1000
2023-10-26 14:38:00.622 
Epoch 260/1000 
	 loss: 41.4910, MinusLogProbMetric: 41.4910, val_loss: 41.4838, val_MinusLogProbMetric: 41.4838

Epoch 260: val_loss improved from 41.52317 to 41.48385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.4910 - MinusLogProbMetric: 41.4910 - val_loss: 41.4838 - val_MinusLogProbMetric: 41.4838 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 261/1000
2023-10-26 14:39:06.562 
Epoch 261/1000 
	 loss: 41.1286, MinusLogProbMetric: 41.1286, val_loss: 41.4934, val_MinusLogProbMetric: 41.4934

Epoch 261: val_loss did not improve from 41.48385
196/196 - 65s - loss: 41.1286 - MinusLogProbMetric: 41.1286 - val_loss: 41.4934 - val_MinusLogProbMetric: 41.4934 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 262/1000
2023-10-26 14:40:11.735 
Epoch 262/1000 
	 loss: 41.1187, MinusLogProbMetric: 41.1187, val_loss: 41.5081, val_MinusLogProbMetric: 41.5081

Epoch 262: val_loss did not improve from 41.48385
196/196 - 65s - loss: 41.1187 - MinusLogProbMetric: 41.1187 - val_loss: 41.5081 - val_MinusLogProbMetric: 41.5081 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 263/1000
2023-10-26 14:41:15.948 
Epoch 263/1000 
	 loss: 41.4336, MinusLogProbMetric: 41.4336, val_loss: 41.4690, val_MinusLogProbMetric: 41.4690

Epoch 263: val_loss improved from 41.48385 to 41.46896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 41.4336 - MinusLogProbMetric: 41.4336 - val_loss: 41.4690 - val_MinusLogProbMetric: 41.4690 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 264/1000
2023-10-26 14:42:20.699 
Epoch 264/1000 
	 loss: 41.1300, MinusLogProbMetric: 41.1300, val_loss: 41.4604, val_MinusLogProbMetric: 41.4604

Epoch 264: val_loss improved from 41.46896 to 41.46040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 41.1300 - MinusLogProbMetric: 41.1300 - val_loss: 41.4604 - val_MinusLogProbMetric: 41.4604 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 265/1000
2023-10-26 14:43:26.775 
Epoch 265/1000 
	 loss: 41.1075, MinusLogProbMetric: 41.1075, val_loss: 41.4986, val_MinusLogProbMetric: 41.4986

Epoch 265: val_loss did not improve from 41.46040
196/196 - 65s - loss: 41.1075 - MinusLogProbMetric: 41.1075 - val_loss: 41.4986 - val_MinusLogProbMetric: 41.4986 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 266/1000
2023-10-26 14:44:32.191 
Epoch 266/1000 
	 loss: 41.0873, MinusLogProbMetric: 41.0873, val_loss: 41.5120, val_MinusLogProbMetric: 41.5120

Epoch 266: val_loss did not improve from 41.46040
196/196 - 65s - loss: 41.0873 - MinusLogProbMetric: 41.0873 - val_loss: 41.5120 - val_MinusLogProbMetric: 41.5120 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 267/1000
2023-10-26 14:45:38.526 
Epoch 267/1000 
	 loss: 41.0931, MinusLogProbMetric: 41.0931, val_loss: 41.4610, val_MinusLogProbMetric: 41.4610

Epoch 267: val_loss did not improve from 41.46040
196/196 - 66s - loss: 41.0931 - MinusLogProbMetric: 41.0931 - val_loss: 41.4610 - val_MinusLogProbMetric: 41.4610 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 268/1000
2023-10-26 14:46:44.391 
Epoch 268/1000 
	 loss: 41.0541, MinusLogProbMetric: 41.0541, val_loss: 41.4035, val_MinusLogProbMetric: 41.4035

Epoch 268: val_loss improved from 41.46040 to 41.40355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 41.0541 - MinusLogProbMetric: 41.0541 - val_loss: 41.4035 - val_MinusLogProbMetric: 41.4035 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 269/1000
2023-10-26 14:47:51.493 
Epoch 269/1000 
	 loss: 41.0374, MinusLogProbMetric: 41.0374, val_loss: 41.4664, val_MinusLogProbMetric: 41.4664

Epoch 269: val_loss did not improve from 41.40355
196/196 - 66s - loss: 41.0374 - MinusLogProbMetric: 41.0374 - val_loss: 41.4664 - val_MinusLogProbMetric: 41.4664 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 270/1000
2023-10-26 14:48:57.935 
Epoch 270/1000 
	 loss: 41.0320, MinusLogProbMetric: 41.0320, val_loss: 41.4295, val_MinusLogProbMetric: 41.4295

Epoch 270: val_loss did not improve from 41.40355
196/196 - 66s - loss: 41.0320 - MinusLogProbMetric: 41.0320 - val_loss: 41.4295 - val_MinusLogProbMetric: 41.4295 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 271/1000
2023-10-26 14:50:03.756 
Epoch 271/1000 
	 loss: 41.0698, MinusLogProbMetric: 41.0698, val_loss: 41.4029, val_MinusLogProbMetric: 41.4029

Epoch 271: val_loss improved from 41.40355 to 41.40286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 41.0698 - MinusLogProbMetric: 41.0698 - val_loss: 41.4029 - val_MinusLogProbMetric: 41.4029 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 272/1000
2023-10-26 14:51:09.608 
Epoch 272/1000 
	 loss: 41.2181, MinusLogProbMetric: 41.2181, val_loss: 41.3790, val_MinusLogProbMetric: 41.3790

Epoch 272: val_loss improved from 41.40286 to 41.37899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.2181 - MinusLogProbMetric: 41.2181 - val_loss: 41.3790 - val_MinusLogProbMetric: 41.3790 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 273/1000
2023-10-26 14:52:16.081 
Epoch 273/1000 
	 loss: 41.0144, MinusLogProbMetric: 41.0144, val_loss: 41.4119, val_MinusLogProbMetric: 41.4119

Epoch 273: val_loss did not improve from 41.37899
196/196 - 65s - loss: 41.0144 - MinusLogProbMetric: 41.0144 - val_loss: 41.4119 - val_MinusLogProbMetric: 41.4119 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 274/1000
2023-10-26 14:53:22.027 
Epoch 274/1000 
	 loss: 41.2299, MinusLogProbMetric: 41.2299, val_loss: 41.3654, val_MinusLogProbMetric: 41.3654

Epoch 274: val_loss improved from 41.37899 to 41.36542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 41.2299 - MinusLogProbMetric: 41.2299 - val_loss: 41.3654 - val_MinusLogProbMetric: 41.3654 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 275/1000
2023-10-26 14:54:28.691 
Epoch 275/1000 
	 loss: 40.9854, MinusLogProbMetric: 40.9854, val_loss: 41.6410, val_MinusLogProbMetric: 41.6410

Epoch 275: val_loss did not improve from 41.36542
196/196 - 66s - loss: 40.9854 - MinusLogProbMetric: 40.9854 - val_loss: 41.6410 - val_MinusLogProbMetric: 41.6410 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 276/1000
2023-10-26 14:55:33.742 
Epoch 276/1000 
	 loss: 41.1459, MinusLogProbMetric: 41.1459, val_loss: 41.3534, val_MinusLogProbMetric: 41.3534

Epoch 276: val_loss improved from 41.36542 to 41.35344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 41.1459 - MinusLogProbMetric: 41.1459 - val_loss: 41.3534 - val_MinusLogProbMetric: 41.3534 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 277/1000
2023-10-26 14:56:40.046 
Epoch 277/1000 
	 loss: 40.9921, MinusLogProbMetric: 40.9921, val_loss: 41.2678, val_MinusLogProbMetric: 41.2678

Epoch 277: val_loss improved from 41.35344 to 41.26778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 40.9921 - MinusLogProbMetric: 40.9921 - val_loss: 41.2678 - val_MinusLogProbMetric: 41.2678 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 278/1000
2023-10-26 14:57:47.088 
Epoch 278/1000 
	 loss: 41.0651, MinusLogProbMetric: 41.0651, val_loss: 41.3700, val_MinusLogProbMetric: 41.3700

Epoch 278: val_loss did not improve from 41.26778
196/196 - 66s - loss: 41.0651 - MinusLogProbMetric: 41.0651 - val_loss: 41.3700 - val_MinusLogProbMetric: 41.3700 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 279/1000
2023-10-26 14:58:52.051 
Epoch 279/1000 
	 loss: 40.9434, MinusLogProbMetric: 40.9434, val_loss: 41.2920, val_MinusLogProbMetric: 41.2920

Epoch 279: val_loss did not improve from 41.26778
196/196 - 65s - loss: 40.9434 - MinusLogProbMetric: 40.9434 - val_loss: 41.2920 - val_MinusLogProbMetric: 41.2920 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 280/1000
2023-10-26 14:59:56.721 
Epoch 280/1000 
	 loss: 40.9165, MinusLogProbMetric: 40.9165, val_loss: 41.4129, val_MinusLogProbMetric: 41.4129

Epoch 280: val_loss did not improve from 41.26778
196/196 - 65s - loss: 40.9165 - MinusLogProbMetric: 40.9165 - val_loss: 41.4129 - val_MinusLogProbMetric: 41.4129 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 281/1000
2023-10-26 15:01:02.329 
Epoch 281/1000 
	 loss: 41.0787, MinusLogProbMetric: 41.0787, val_loss: 41.3026, val_MinusLogProbMetric: 41.3026

Epoch 281: val_loss did not improve from 41.26778
196/196 - 66s - loss: 41.0787 - MinusLogProbMetric: 41.0787 - val_loss: 41.3026 - val_MinusLogProbMetric: 41.3026 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 282/1000
2023-10-26 15:01:58.640 
Epoch 282/1000 
	 loss: 40.9121, MinusLogProbMetric: 40.9121, val_loss: 41.3125, val_MinusLogProbMetric: 41.3125

Epoch 282: val_loss did not improve from 41.26778
196/196 - 56s - loss: 40.9121 - MinusLogProbMetric: 40.9121 - val_loss: 41.3125 - val_MinusLogProbMetric: 41.3125 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 283/1000
2023-10-26 15:02:48.915 
Epoch 283/1000 
	 loss: 40.8821, MinusLogProbMetric: 40.8821, val_loss: 41.3141, val_MinusLogProbMetric: 41.3141

Epoch 283: val_loss did not improve from 41.26778
196/196 - 50s - loss: 40.8821 - MinusLogProbMetric: 40.8821 - val_loss: 41.3141 - val_MinusLogProbMetric: 41.3141 - lr: 4.5725e-07 - 50s/epoch - 257ms/step
Epoch 284/1000
2023-10-26 15:03:48.663 
Epoch 284/1000 
	 loss: 40.8896, MinusLogProbMetric: 40.8896, val_loss: 41.2278, val_MinusLogProbMetric: 41.2278

Epoch 284: val_loss improved from 41.26778 to 41.22782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 40.8896 - MinusLogProbMetric: 40.8896 - val_loss: 41.2278 - val_MinusLogProbMetric: 41.2278 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 285/1000
2023-10-26 15:04:54.356 
Epoch 285/1000 
	 loss: 40.9491, MinusLogProbMetric: 40.9491, val_loss: 41.2447, val_MinusLogProbMetric: 41.2447

Epoch 285: val_loss did not improve from 41.22782
196/196 - 65s - loss: 40.9491 - MinusLogProbMetric: 40.9491 - val_loss: 41.2447 - val_MinusLogProbMetric: 41.2447 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 286/1000
2023-10-26 15:06:00.722 
Epoch 286/1000 
	 loss: 40.8833, MinusLogProbMetric: 40.8833, val_loss: 41.2848, val_MinusLogProbMetric: 41.2848

Epoch 286: val_loss did not improve from 41.22782
196/196 - 66s - loss: 40.8833 - MinusLogProbMetric: 40.8833 - val_loss: 41.2848 - val_MinusLogProbMetric: 41.2848 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 287/1000
2023-10-26 15:07:06.103 
Epoch 287/1000 
	 loss: 40.8568, MinusLogProbMetric: 40.8568, val_loss: 41.2385, val_MinusLogProbMetric: 41.2385

Epoch 287: val_loss did not improve from 41.22782
196/196 - 65s - loss: 40.8568 - MinusLogProbMetric: 40.8568 - val_loss: 41.2385 - val_MinusLogProbMetric: 41.2385 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 288/1000
2023-10-26 15:08:12.467 
Epoch 288/1000 
	 loss: 41.1816, MinusLogProbMetric: 41.1816, val_loss: 41.2608, val_MinusLogProbMetric: 41.2608

Epoch 288: val_loss did not improve from 41.22782
196/196 - 66s - loss: 41.1816 - MinusLogProbMetric: 41.1816 - val_loss: 41.2608 - val_MinusLogProbMetric: 41.2608 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 289/1000
2023-10-26 15:09:16.831 
Epoch 289/1000 
	 loss: 40.8337, MinusLogProbMetric: 40.8337, val_loss: 41.2262, val_MinusLogProbMetric: 41.2262

Epoch 289: val_loss improved from 41.22782 to 41.22617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.8337 - MinusLogProbMetric: 40.8337 - val_loss: 41.2262 - val_MinusLogProbMetric: 41.2262 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 290/1000
2023-10-26 15:10:18.400 
Epoch 290/1000 
	 loss: 40.8934, MinusLogProbMetric: 40.8934, val_loss: 41.3895, val_MinusLogProbMetric: 41.3895

Epoch 290: val_loss did not improve from 41.22617
196/196 - 61s - loss: 40.8934 - MinusLogProbMetric: 40.8934 - val_loss: 41.3895 - val_MinusLogProbMetric: 41.3895 - lr: 4.5725e-07 - 61s/epoch - 309ms/step
Epoch 291/1000
2023-10-26 15:11:25.736 
Epoch 291/1000 
	 loss: 40.8160, MinusLogProbMetric: 40.8160, val_loss: 41.2037, val_MinusLogProbMetric: 41.2037

Epoch 291: val_loss improved from 41.22617 to 41.20370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 40.8160 - MinusLogProbMetric: 40.8160 - val_loss: 41.2037 - val_MinusLogProbMetric: 41.2037 - lr: 4.5725e-07 - 68s/epoch - 349ms/step
Epoch 292/1000
2023-10-26 15:12:34.255 
Epoch 292/1000 
	 loss: 40.8016, MinusLogProbMetric: 40.8016, val_loss: 41.1538, val_MinusLogProbMetric: 41.1538

Epoch 292: val_loss improved from 41.20370 to 41.15376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 40.8016 - MinusLogProbMetric: 40.8016 - val_loss: 41.1538 - val_MinusLogProbMetric: 41.1538 - lr: 4.5725e-07 - 68s/epoch - 349ms/step
Epoch 293/1000
2023-10-26 15:13:42.241 
Epoch 293/1000 
	 loss: 40.8293, MinusLogProbMetric: 40.8293, val_loss: 41.1695, val_MinusLogProbMetric: 41.1695

Epoch 293: val_loss did not improve from 41.15376
196/196 - 67s - loss: 40.8293 - MinusLogProbMetric: 40.8293 - val_loss: 41.1695 - val_MinusLogProbMetric: 41.1695 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 294/1000
2023-10-26 15:14:47.915 
Epoch 294/1000 
	 loss: 40.7621, MinusLogProbMetric: 40.7621, val_loss: 41.1519, val_MinusLogProbMetric: 41.1519

Epoch 294: val_loss improved from 41.15376 to 41.15192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 40.7621 - MinusLogProbMetric: 40.7621 - val_loss: 41.1519 - val_MinusLogProbMetric: 41.1519 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 295/1000
2023-10-26 15:15:55.595 
Epoch 295/1000 
	 loss: 40.8738, MinusLogProbMetric: 40.8738, val_loss: 41.1760, val_MinusLogProbMetric: 41.1760

Epoch 295: val_loss did not improve from 41.15192
196/196 - 67s - loss: 40.8738 - MinusLogProbMetric: 40.8738 - val_loss: 41.1760 - val_MinusLogProbMetric: 41.1760 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 296/1000
2023-10-26 15:17:01.762 
Epoch 296/1000 
	 loss: 40.9841, MinusLogProbMetric: 40.9841, val_loss: 41.2271, val_MinusLogProbMetric: 41.2271

Epoch 296: val_loss did not improve from 41.15192
196/196 - 66s - loss: 40.9841 - MinusLogProbMetric: 40.9841 - val_loss: 41.2271 - val_MinusLogProbMetric: 41.2271 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 297/1000
2023-10-26 15:18:09.158 
Epoch 297/1000 
	 loss: 40.8859, MinusLogProbMetric: 40.8859, val_loss: 41.3520, val_MinusLogProbMetric: 41.3520

Epoch 297: val_loss did not improve from 41.15192
196/196 - 67s - loss: 40.8859 - MinusLogProbMetric: 40.8859 - val_loss: 41.3520 - val_MinusLogProbMetric: 41.3520 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 298/1000
2023-10-26 15:19:15.608 
Epoch 298/1000 
	 loss: 40.7955, MinusLogProbMetric: 40.7955, val_loss: 41.1453, val_MinusLogProbMetric: 41.1453

Epoch 298: val_loss improved from 41.15192 to 41.14535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 40.7955 - MinusLogProbMetric: 40.7955 - val_loss: 41.1453 - val_MinusLogProbMetric: 41.1453 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 299/1000
2023-10-26 15:20:22.892 
Epoch 299/1000 
	 loss: 40.7452, MinusLogProbMetric: 40.7452, val_loss: 41.2508, val_MinusLogProbMetric: 41.2508

Epoch 299: val_loss did not improve from 41.14535
196/196 - 66s - loss: 40.7452 - MinusLogProbMetric: 40.7452 - val_loss: 41.2508 - val_MinusLogProbMetric: 41.2508 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 300/1000
2023-10-26 15:21:28.495 
Epoch 300/1000 
	 loss: 40.7559, MinusLogProbMetric: 40.7559, val_loss: 41.1778, val_MinusLogProbMetric: 41.1778

Epoch 300: val_loss did not improve from 41.14535
196/196 - 66s - loss: 40.7559 - MinusLogProbMetric: 40.7559 - val_loss: 41.1778 - val_MinusLogProbMetric: 41.1778 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 301/1000
2023-10-26 15:22:34.992 
Epoch 301/1000 
	 loss: 40.7237, MinusLogProbMetric: 40.7237, val_loss: 41.2122, val_MinusLogProbMetric: 41.2122

Epoch 301: val_loss did not improve from 41.14535
196/196 - 66s - loss: 40.7237 - MinusLogProbMetric: 40.7237 - val_loss: 41.2122 - val_MinusLogProbMetric: 41.2122 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 302/1000
2023-10-26 15:23:41.598 
Epoch 302/1000 
	 loss: 40.9095, MinusLogProbMetric: 40.9095, val_loss: 41.0566, val_MinusLogProbMetric: 41.0566

Epoch 302: val_loss improved from 41.14535 to 41.05664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 40.9095 - MinusLogProbMetric: 40.9095 - val_loss: 41.0566 - val_MinusLogProbMetric: 41.0566 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 303/1000
2023-10-26 15:24:48.613 
Epoch 303/1000 
	 loss: 40.6830, MinusLogProbMetric: 40.6830, val_loss: 41.0890, val_MinusLogProbMetric: 41.0890

Epoch 303: val_loss did not improve from 41.05664
196/196 - 66s - loss: 40.6830 - MinusLogProbMetric: 40.6830 - val_loss: 41.0890 - val_MinusLogProbMetric: 41.0890 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 304/1000
2023-10-26 15:25:55.528 
Epoch 304/1000 
	 loss: 40.6760, MinusLogProbMetric: 40.6760, val_loss: 41.1368, val_MinusLogProbMetric: 41.1368

Epoch 304: val_loss did not improve from 41.05664
196/196 - 67s - loss: 40.6760 - MinusLogProbMetric: 40.6760 - val_loss: 41.1368 - val_MinusLogProbMetric: 41.1368 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 305/1000
2023-10-26 15:26:50.604 
Epoch 305/1000 
	 loss: 40.6773, MinusLogProbMetric: 40.6773, val_loss: 41.1128, val_MinusLogProbMetric: 41.1128

Epoch 305: val_loss did not improve from 41.05664
196/196 - 55s - loss: 40.6773 - MinusLogProbMetric: 40.6773 - val_loss: 41.1128 - val_MinusLogProbMetric: 41.1128 - lr: 4.5725e-07 - 55s/epoch - 281ms/step
Epoch 306/1000
2023-10-26 15:27:41.726 
Epoch 306/1000 
	 loss: 40.6766, MinusLogProbMetric: 40.6766, val_loss: 41.1145, val_MinusLogProbMetric: 41.1145

Epoch 306: val_loss did not improve from 41.05664
196/196 - 51s - loss: 40.6766 - MinusLogProbMetric: 40.6766 - val_loss: 41.1145 - val_MinusLogProbMetric: 41.1145 - lr: 4.5725e-07 - 51s/epoch - 261ms/step
Epoch 307/1000
2023-10-26 15:28:45.671 
Epoch 307/1000 
	 loss: 40.7013, MinusLogProbMetric: 40.7013, val_loss: 41.0447, val_MinusLogProbMetric: 41.0447

Epoch 307: val_loss improved from 41.05664 to 41.04468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.7013 - MinusLogProbMetric: 40.7013 - val_loss: 41.0447 - val_MinusLogProbMetric: 41.0447 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 308/1000
2023-10-26 15:29:49.210 
Epoch 308/1000 
	 loss: 41.0736, MinusLogProbMetric: 41.0736, val_loss: 41.0590, val_MinusLogProbMetric: 41.0590

Epoch 308: val_loss did not improve from 41.04468
196/196 - 63s - loss: 41.0736 - MinusLogProbMetric: 41.0736 - val_loss: 41.0590 - val_MinusLogProbMetric: 41.0590 - lr: 4.5725e-07 - 63s/epoch - 319ms/step
Epoch 309/1000
2023-10-26 15:30:40.921 
Epoch 309/1000 
	 loss: 40.6765, MinusLogProbMetric: 40.6765, val_loss: 40.9835, val_MinusLogProbMetric: 40.9835

Epoch 309: val_loss improved from 41.04468 to 40.98351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 53s - loss: 40.6765 - MinusLogProbMetric: 40.6765 - val_loss: 40.9835 - val_MinusLogProbMetric: 40.9835 - lr: 4.5725e-07 - 53s/epoch - 268ms/step
Epoch 310/1000
2023-10-26 15:31:37.406 
Epoch 310/1000 
	 loss: 40.6173, MinusLogProbMetric: 40.6173, val_loss: 41.0132, val_MinusLogProbMetric: 41.0132

Epoch 310: val_loss did not improve from 40.98351
196/196 - 56s - loss: 40.6173 - MinusLogProbMetric: 40.6173 - val_loss: 41.0132 - val_MinusLogProbMetric: 41.0132 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 311/1000
2023-10-26 15:32:34.669 
Epoch 311/1000 
	 loss: 40.6140, MinusLogProbMetric: 40.6140, val_loss: 41.0274, val_MinusLogProbMetric: 41.0274

Epoch 311: val_loss did not improve from 40.98351
196/196 - 57s - loss: 40.6140 - MinusLogProbMetric: 40.6140 - val_loss: 41.0274 - val_MinusLogProbMetric: 41.0274 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 312/1000
2023-10-26 15:33:26.061 
Epoch 312/1000 
	 loss: 40.6241, MinusLogProbMetric: 40.6241, val_loss: 41.0584, val_MinusLogProbMetric: 41.0584

Epoch 312: val_loss did not improve from 40.98351
196/196 - 51s - loss: 40.6241 - MinusLogProbMetric: 40.6241 - val_loss: 41.0584 - val_MinusLogProbMetric: 41.0584 - lr: 4.5725e-07 - 51s/epoch - 262ms/step
Epoch 313/1000
2023-10-26 15:34:26.686 
Epoch 313/1000 
	 loss: 40.6004, MinusLogProbMetric: 40.6004, val_loss: 41.0195, val_MinusLogProbMetric: 41.0195

Epoch 313: val_loss did not improve from 40.98351
196/196 - 61s - loss: 40.6004 - MinusLogProbMetric: 40.6004 - val_loss: 41.0195 - val_MinusLogProbMetric: 41.0195 - lr: 4.5725e-07 - 61s/epoch - 309ms/step
Epoch 314/1000
2023-10-26 15:35:20.683 
Epoch 314/1000 
	 loss: 40.7706, MinusLogProbMetric: 40.7706, val_loss: 40.9486, val_MinusLogProbMetric: 40.9486

Epoch 314: val_loss improved from 40.98351 to 40.94861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 55s - loss: 40.7706 - MinusLogProbMetric: 40.7706 - val_loss: 40.9486 - val_MinusLogProbMetric: 40.9486 - lr: 4.5725e-07 - 55s/epoch - 279ms/step
Epoch 315/1000
2023-10-26 15:36:15.865 
Epoch 315/1000 
	 loss: 40.5852, MinusLogProbMetric: 40.5852, val_loss: 40.9821, val_MinusLogProbMetric: 40.9821

Epoch 315: val_loss did not improve from 40.94861
196/196 - 54s - loss: 40.5852 - MinusLogProbMetric: 40.5852 - val_loss: 40.9821 - val_MinusLogProbMetric: 40.9821 - lr: 4.5725e-07 - 54s/epoch - 278ms/step
Epoch 316/1000
2023-10-26 15:37:16.229 
Epoch 316/1000 
	 loss: 40.5722, MinusLogProbMetric: 40.5722, val_loss: 40.9998, val_MinusLogProbMetric: 40.9998

Epoch 316: val_loss did not improve from 40.94861
196/196 - 60s - loss: 40.5722 - MinusLogProbMetric: 40.5722 - val_loss: 40.9998 - val_MinusLogProbMetric: 40.9998 - lr: 4.5725e-07 - 60s/epoch - 308ms/step
Epoch 317/1000
2023-10-26 15:38:11.896 
Epoch 317/1000 
	 loss: 40.5847, MinusLogProbMetric: 40.5847, val_loss: 41.0512, val_MinusLogProbMetric: 41.0512

Epoch 317: val_loss did not improve from 40.94861
196/196 - 56s - loss: 40.5847 - MinusLogProbMetric: 40.5847 - val_loss: 41.0512 - val_MinusLogProbMetric: 41.0512 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 318/1000
2023-10-26 15:39:08.405 
Epoch 318/1000 
	 loss: 40.6483, MinusLogProbMetric: 40.6483, val_loss: 40.9277, val_MinusLogProbMetric: 40.9277

Epoch 318: val_loss improved from 40.94861 to 40.92773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 40.6483 - MinusLogProbMetric: 40.6483 - val_loss: 40.9277 - val_MinusLogProbMetric: 40.9277 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 319/1000
2023-10-26 15:40:07.864 
Epoch 319/1000 
	 loss: 40.5327, MinusLogProbMetric: 40.5327, val_loss: 40.9219, val_MinusLogProbMetric: 40.9219

Epoch 319: val_loss improved from 40.92773 to 40.92194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 40.5327 - MinusLogProbMetric: 40.5327 - val_loss: 40.9219 - val_MinusLogProbMetric: 40.9219 - lr: 4.5725e-07 - 59s/epoch - 303ms/step
Epoch 320/1000
2023-10-26 15:41:03.169 
Epoch 320/1000 
	 loss: 40.5189, MinusLogProbMetric: 40.5189, val_loss: 40.9196, val_MinusLogProbMetric: 40.9196

Epoch 320: val_loss improved from 40.92194 to 40.91958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 40.5189 - MinusLogProbMetric: 40.5189 - val_loss: 40.9196 - val_MinusLogProbMetric: 40.9196 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 321/1000
2023-10-26 15:41:59.573 
Epoch 321/1000 
	 loss: 40.5323, MinusLogProbMetric: 40.5323, val_loss: 40.9261, val_MinusLogProbMetric: 40.9261

Epoch 321: val_loss did not improve from 40.91958
196/196 - 55s - loss: 40.5323 - MinusLogProbMetric: 40.5323 - val_loss: 40.9261 - val_MinusLogProbMetric: 40.9261 - lr: 4.5725e-07 - 55s/epoch - 282ms/step
Epoch 322/1000
2023-10-26 15:43:00.909 
Epoch 322/1000 
	 loss: 40.5714, MinusLogProbMetric: 40.5714, val_loss: 40.9019, val_MinusLogProbMetric: 40.9019

Epoch 322: val_loss improved from 40.91958 to 40.90187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 40.5714 - MinusLogProbMetric: 40.5714 - val_loss: 40.9019 - val_MinusLogProbMetric: 40.9019 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 323/1000
2023-10-26 15:43:57.834 
Epoch 323/1000 
	 loss: 40.5381, MinusLogProbMetric: 40.5381, val_loss: 40.9016, val_MinusLogProbMetric: 40.9016

Epoch 323: val_loss improved from 40.90187 to 40.90161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 40.5381 - MinusLogProbMetric: 40.5381 - val_loss: 40.9016 - val_MinusLogProbMetric: 40.9016 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 324/1000
2023-10-26 15:44:54.577 
Epoch 324/1000 
	 loss: 40.5055, MinusLogProbMetric: 40.5055, val_loss: 41.0664, val_MinusLogProbMetric: 41.0664

Epoch 324: val_loss did not improve from 40.90161
196/196 - 56s - loss: 40.5055 - MinusLogProbMetric: 40.5055 - val_loss: 41.0664 - val_MinusLogProbMetric: 41.0664 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 325/1000
2023-10-26 15:45:53.411 
Epoch 325/1000 
	 loss: 40.4957, MinusLogProbMetric: 40.4957, val_loss: 41.0094, val_MinusLogProbMetric: 41.0094

Epoch 325: val_loss did not improve from 40.90161
196/196 - 59s - loss: 40.4957 - MinusLogProbMetric: 40.4957 - val_loss: 41.0094 - val_MinusLogProbMetric: 41.0094 - lr: 4.5725e-07 - 59s/epoch - 300ms/step
Epoch 326/1000
2023-10-26 15:46:48.606 
Epoch 326/1000 
	 loss: 40.4971, MinusLogProbMetric: 40.4971, val_loss: 40.9165, val_MinusLogProbMetric: 40.9165

Epoch 326: val_loss did not improve from 40.90161
196/196 - 55s - loss: 40.4971 - MinusLogProbMetric: 40.4971 - val_loss: 40.9165 - val_MinusLogProbMetric: 40.9165 - lr: 4.5725e-07 - 55s/epoch - 282ms/step
Epoch 327/1000
2023-10-26 15:47:43.254 
Epoch 327/1000 
	 loss: 40.6321, MinusLogProbMetric: 40.6321, val_loss: 40.7990, val_MinusLogProbMetric: 40.7990

Epoch 327: val_loss improved from 40.90161 to 40.79903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 55s - loss: 40.6321 - MinusLogProbMetric: 40.6321 - val_loss: 40.7990 - val_MinusLogProbMetric: 40.7990 - lr: 4.5725e-07 - 55s/epoch - 283ms/step
Epoch 328/1000
2023-10-26 15:48:43.276 
Epoch 328/1000 
	 loss: 40.4432, MinusLogProbMetric: 40.4432, val_loss: 40.9370, val_MinusLogProbMetric: 40.9370

Epoch 328: val_loss did not improve from 40.79903
196/196 - 59s - loss: 40.4432 - MinusLogProbMetric: 40.4432 - val_loss: 40.9370 - val_MinusLogProbMetric: 40.9370 - lr: 4.5725e-07 - 59s/epoch - 302ms/step
Epoch 329/1000
2023-10-26 15:49:38.376 
Epoch 329/1000 
	 loss: 40.4437, MinusLogProbMetric: 40.4437, val_loss: 40.8971, val_MinusLogProbMetric: 40.8971

Epoch 329: val_loss did not improve from 40.79903
196/196 - 55s - loss: 40.4437 - MinusLogProbMetric: 40.4437 - val_loss: 40.8971 - val_MinusLogProbMetric: 40.8971 - lr: 4.5725e-07 - 55s/epoch - 281ms/step
Epoch 330/1000
2023-10-26 15:50:38.474 
Epoch 330/1000 
	 loss: 40.8222, MinusLogProbMetric: 40.8222, val_loss: 41.0364, val_MinusLogProbMetric: 41.0364

Epoch 330: val_loss did not improve from 40.79903
196/196 - 60s - loss: 40.8222 - MinusLogProbMetric: 40.8222 - val_loss: 41.0364 - val_MinusLogProbMetric: 41.0364 - lr: 4.5725e-07 - 60s/epoch - 307ms/step
Epoch 331/1000
2023-10-26 15:51:34.378 
Epoch 331/1000 
	 loss: 40.4862, MinusLogProbMetric: 40.4862, val_loss: 40.8903, val_MinusLogProbMetric: 40.8903

Epoch 331: val_loss did not improve from 40.79903
196/196 - 56s - loss: 40.4862 - MinusLogProbMetric: 40.4862 - val_loss: 40.8903 - val_MinusLogProbMetric: 40.8903 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 332/1000
2023-10-26 15:52:31.928 
Epoch 332/1000 
	 loss: 40.4119, MinusLogProbMetric: 40.4119, val_loss: 40.8208, val_MinusLogProbMetric: 40.8208

Epoch 332: val_loss did not improve from 40.79903
196/196 - 58s - loss: 40.4119 - MinusLogProbMetric: 40.4119 - val_loss: 40.8208 - val_MinusLogProbMetric: 40.8208 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 333/1000
2023-10-26 15:53:33.381 
Epoch 333/1000 
	 loss: 40.4392, MinusLogProbMetric: 40.4392, val_loss: 40.8979, val_MinusLogProbMetric: 40.8979

Epoch 333: val_loss did not improve from 40.79903
196/196 - 61s - loss: 40.4392 - MinusLogProbMetric: 40.4392 - val_loss: 40.8979 - val_MinusLogProbMetric: 40.8979 - lr: 4.5725e-07 - 61s/epoch - 314ms/step
Epoch 334/1000
2023-10-26 15:54:29.151 
Epoch 334/1000 
	 loss: 40.4296, MinusLogProbMetric: 40.4296, val_loss: 40.8117, val_MinusLogProbMetric: 40.8117

Epoch 334: val_loss did not improve from 40.79903
196/196 - 56s - loss: 40.4296 - MinusLogProbMetric: 40.4296 - val_loss: 40.8117 - val_MinusLogProbMetric: 40.8117 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 335/1000
2023-10-26 15:55:33.237 
Epoch 335/1000 
	 loss: 40.3948, MinusLogProbMetric: 40.3948, val_loss: 40.7703, val_MinusLogProbMetric: 40.7703

Epoch 335: val_loss improved from 40.79903 to 40.77033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.3948 - MinusLogProbMetric: 40.3948 - val_loss: 40.7703 - val_MinusLogProbMetric: 40.7703 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 336/1000
2023-10-26 15:56:39.091 
Epoch 336/1000 
	 loss: 40.4361, MinusLogProbMetric: 40.4361, val_loss: 41.0310, val_MinusLogProbMetric: 41.0310

Epoch 336: val_loss did not improve from 40.77033
196/196 - 65s - loss: 40.4361 - MinusLogProbMetric: 40.4361 - val_loss: 41.0310 - val_MinusLogProbMetric: 41.0310 - lr: 4.5725e-07 - 65s/epoch - 329ms/step
Epoch 337/1000
2023-10-26 15:57:44.443 
Epoch 337/1000 
	 loss: 40.4198, MinusLogProbMetric: 40.4198, val_loss: 40.7665, val_MinusLogProbMetric: 40.7665

Epoch 337: val_loss improved from 40.77033 to 40.76648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 40.4198 - MinusLogProbMetric: 40.4198 - val_loss: 40.7665 - val_MinusLogProbMetric: 40.7665 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 338/1000
2023-10-26 15:58:41.565 
Epoch 338/1000 
	 loss: 40.3860, MinusLogProbMetric: 40.3860, val_loss: 40.8163, val_MinusLogProbMetric: 40.8163

Epoch 338: val_loss did not improve from 40.76648
196/196 - 56s - loss: 40.3860 - MinusLogProbMetric: 40.3860 - val_loss: 40.8163 - val_MinusLogProbMetric: 40.8163 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 339/1000
2023-10-26 15:59:36.595 
Epoch 339/1000 
	 loss: 40.6853, MinusLogProbMetric: 40.6853, val_loss: 40.7207, val_MinusLogProbMetric: 40.7207

Epoch 339: val_loss improved from 40.76648 to 40.72068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 56s - loss: 40.6853 - MinusLogProbMetric: 40.6853 - val_loss: 40.7207 - val_MinusLogProbMetric: 40.7207 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 340/1000
2023-10-26 16:00:35.469 
Epoch 340/1000 
	 loss: 40.5062, MinusLogProbMetric: 40.5062, val_loss: 41.7333, val_MinusLogProbMetric: 41.7333

Epoch 340: val_loss did not improve from 40.72068
196/196 - 58s - loss: 40.5062 - MinusLogProbMetric: 40.5062 - val_loss: 41.7333 - val_MinusLogProbMetric: 41.7333 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 341/1000
2023-10-26 16:01:39.057 
Epoch 341/1000 
	 loss: 40.4549, MinusLogProbMetric: 40.4549, val_loss: 40.7531, val_MinusLogProbMetric: 40.7531

Epoch 341: val_loss did not improve from 40.72068
196/196 - 64s - loss: 40.4549 - MinusLogProbMetric: 40.4549 - val_loss: 40.7531 - val_MinusLogProbMetric: 40.7531 - lr: 4.5725e-07 - 64s/epoch - 324ms/step
Epoch 342/1000
2023-10-26 16:02:35.852 
Epoch 342/1000 
	 loss: 40.4000, MinusLogProbMetric: 40.4000, val_loss: 40.7371, val_MinusLogProbMetric: 40.7371

Epoch 342: val_loss did not improve from 40.72068
196/196 - 57s - loss: 40.4000 - MinusLogProbMetric: 40.4000 - val_loss: 40.7371 - val_MinusLogProbMetric: 40.7371 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 343/1000
2023-10-26 16:03:35.158 
Epoch 343/1000 
	 loss: 40.3322, MinusLogProbMetric: 40.3322, val_loss: 40.7154, val_MinusLogProbMetric: 40.7154

Epoch 343: val_loss improved from 40.72068 to 40.71539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 60s - loss: 40.3322 - MinusLogProbMetric: 40.3322 - val_loss: 40.7154 - val_MinusLogProbMetric: 40.7154 - lr: 4.5725e-07 - 60s/epoch - 308ms/step
Epoch 344/1000
2023-10-26 16:04:34.875 
Epoch 344/1000 
	 loss: 40.3848, MinusLogProbMetric: 40.3848, val_loss: 40.7175, val_MinusLogProbMetric: 40.7175

Epoch 344: val_loss did not improve from 40.71539
196/196 - 59s - loss: 40.3848 - MinusLogProbMetric: 40.3848 - val_loss: 40.7175 - val_MinusLogProbMetric: 40.7175 - lr: 4.5725e-07 - 59s/epoch - 299ms/step
Epoch 345/1000
2023-10-26 16:05:31.878 
Epoch 345/1000 
	 loss: 40.3384, MinusLogProbMetric: 40.3384, val_loss: 40.7265, val_MinusLogProbMetric: 40.7265

Epoch 345: val_loss did not improve from 40.71539
196/196 - 57s - loss: 40.3384 - MinusLogProbMetric: 40.3384 - val_loss: 40.7265 - val_MinusLogProbMetric: 40.7265 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 346/1000
2023-10-26 16:06:35.440 
Epoch 346/1000 
	 loss: 40.2924, MinusLogProbMetric: 40.2924, val_loss: 40.7022, val_MinusLogProbMetric: 40.7022

Epoch 346: val_loss improved from 40.71539 to 40.70216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.2924 - MinusLogProbMetric: 40.2924 - val_loss: 40.7022 - val_MinusLogProbMetric: 40.7022 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 347/1000
2023-10-26 16:07:38.361 
Epoch 347/1000 
	 loss: 40.2810, MinusLogProbMetric: 40.2810, val_loss: 40.7005, val_MinusLogProbMetric: 40.7005

Epoch 347: val_loss improved from 40.70216 to 40.70049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 40.2810 - MinusLogProbMetric: 40.2810 - val_loss: 40.7005 - val_MinusLogProbMetric: 40.7005 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 348/1000
2023-10-26 16:08:40.776 
Epoch 348/1000 
	 loss: 40.2830, MinusLogProbMetric: 40.2830, val_loss: 40.7047, val_MinusLogProbMetric: 40.7047

Epoch 348: val_loss did not improve from 40.70049
196/196 - 61s - loss: 40.2830 - MinusLogProbMetric: 40.2830 - val_loss: 40.7047 - val_MinusLogProbMetric: 40.7047 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 349/1000
2023-10-26 16:09:45.216 
Epoch 349/1000 
	 loss: 40.2629, MinusLogProbMetric: 40.2629, val_loss: 40.6860, val_MinusLogProbMetric: 40.6860

Epoch 349: val_loss improved from 40.70049 to 40.68602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.2629 - MinusLogProbMetric: 40.2629 - val_loss: 40.6860 - val_MinusLogProbMetric: 40.6860 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 350/1000
2023-10-26 16:10:47.402 
Epoch 350/1000 
	 loss: 40.2602, MinusLogProbMetric: 40.2602, val_loss: 40.6300, val_MinusLogProbMetric: 40.6300

Epoch 350: val_loss improved from 40.68602 to 40.63000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 40.2602 - MinusLogProbMetric: 40.2602 - val_loss: 40.6300 - val_MinusLogProbMetric: 40.6300 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 351/1000
2023-10-26 16:11:52.459 
Epoch 351/1000 
	 loss: 40.2764, MinusLogProbMetric: 40.2764, val_loss: 40.5942, val_MinusLogProbMetric: 40.5942

Epoch 351: val_loss improved from 40.63000 to 40.59425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.2764 - MinusLogProbMetric: 40.2764 - val_loss: 40.5942 - val_MinusLogProbMetric: 40.5942 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 352/1000
2023-10-26 16:12:57.566 
Epoch 352/1000 
	 loss: 40.2747, MinusLogProbMetric: 40.2747, val_loss: 40.6211, val_MinusLogProbMetric: 40.6211

Epoch 352: val_loss did not improve from 40.59425
196/196 - 64s - loss: 40.2747 - MinusLogProbMetric: 40.2747 - val_loss: 40.6211 - val_MinusLogProbMetric: 40.6211 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 353/1000
2023-10-26 16:14:01.162 
Epoch 353/1000 
	 loss: 40.5943, MinusLogProbMetric: 40.5943, val_loss: 40.6105, val_MinusLogProbMetric: 40.6105

Epoch 353: val_loss did not improve from 40.59425
196/196 - 64s - loss: 40.5943 - MinusLogProbMetric: 40.5943 - val_loss: 40.6105 - val_MinusLogProbMetric: 40.6105 - lr: 4.5725e-07 - 64s/epoch - 324ms/step
Epoch 354/1000
2023-10-26 16:15:02.630 
Epoch 354/1000 
	 loss: 40.2577, MinusLogProbMetric: 40.2577, val_loss: 40.6340, val_MinusLogProbMetric: 40.6340

Epoch 354: val_loss did not improve from 40.59425
196/196 - 61s - loss: 40.2577 - MinusLogProbMetric: 40.2577 - val_loss: 40.6340 - val_MinusLogProbMetric: 40.6340 - lr: 4.5725e-07 - 61s/epoch - 314ms/step
Epoch 355/1000
2023-10-26 16:16:07.540 
Epoch 355/1000 
	 loss: 40.2536, MinusLogProbMetric: 40.2536, val_loss: 40.6476, val_MinusLogProbMetric: 40.6476

Epoch 355: val_loss did not improve from 40.59425
196/196 - 65s - loss: 40.2536 - MinusLogProbMetric: 40.2536 - val_loss: 40.6476 - val_MinusLogProbMetric: 40.6476 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 356/1000
2023-10-26 16:17:10.717 
Epoch 356/1000 
	 loss: 40.4529, MinusLogProbMetric: 40.4529, val_loss: 40.5854, val_MinusLogProbMetric: 40.5854

Epoch 356: val_loss improved from 40.59425 to 40.58540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 40.4529 - MinusLogProbMetric: 40.4529 - val_loss: 40.5854 - val_MinusLogProbMetric: 40.5854 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 357/1000
2023-10-26 16:18:13.254 
Epoch 357/1000 
	 loss: 40.2072, MinusLogProbMetric: 40.2072, val_loss: 40.6279, val_MinusLogProbMetric: 40.6279

Epoch 357: val_loss did not improve from 40.58540
196/196 - 62s - loss: 40.2072 - MinusLogProbMetric: 40.2072 - val_loss: 40.6279 - val_MinusLogProbMetric: 40.6279 - lr: 4.5725e-07 - 62s/epoch - 315ms/step
Epoch 358/1000
2023-10-26 16:19:17.418 
Epoch 358/1000 
	 loss: 40.1825, MinusLogProbMetric: 40.1825, val_loss: 40.5956, val_MinusLogProbMetric: 40.5956

Epoch 358: val_loss did not improve from 40.58540
196/196 - 64s - loss: 40.1825 - MinusLogProbMetric: 40.1825 - val_loss: 40.5956 - val_MinusLogProbMetric: 40.5956 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 359/1000
2023-10-26 16:20:21.513 
Epoch 359/1000 
	 loss: 40.1714, MinusLogProbMetric: 40.1714, val_loss: 40.5651, val_MinusLogProbMetric: 40.5651

Epoch 359: val_loss improved from 40.58540 to 40.56506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.1714 - MinusLogProbMetric: 40.1714 - val_loss: 40.5651 - val_MinusLogProbMetric: 40.5651 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 360/1000
2023-10-26 16:21:25.561 
Epoch 360/1000 
	 loss: 40.6353, MinusLogProbMetric: 40.6353, val_loss: 40.5332, val_MinusLogProbMetric: 40.5332

Epoch 360: val_loss improved from 40.56506 to 40.53324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 40.6353 - MinusLogProbMetric: 40.6353 - val_loss: 40.5332 - val_MinusLogProbMetric: 40.5332 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 361/1000
2023-10-26 16:22:30.622 
Epoch 361/1000 
	 loss: 40.1954, MinusLogProbMetric: 40.1954, val_loss: 40.6043, val_MinusLogProbMetric: 40.6043

Epoch 361: val_loss did not improve from 40.53324
196/196 - 64s - loss: 40.1954 - MinusLogProbMetric: 40.1954 - val_loss: 40.6043 - val_MinusLogProbMetric: 40.6043 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 362/1000
2023-10-26 16:23:30.967 
Epoch 362/1000 
	 loss: 40.4397, MinusLogProbMetric: 40.4397, val_loss: 40.5817, val_MinusLogProbMetric: 40.5817

Epoch 362: val_loss did not improve from 40.53324
196/196 - 60s - loss: 40.4397 - MinusLogProbMetric: 40.4397 - val_loss: 40.5817 - val_MinusLogProbMetric: 40.5817 - lr: 4.5725e-07 - 60s/epoch - 308ms/step
Epoch 363/1000
2023-10-26 16:24:32.329 
Epoch 363/1000 
	 loss: 40.1497, MinusLogProbMetric: 40.1497, val_loss: 40.5205, val_MinusLogProbMetric: 40.5205

Epoch 363: val_loss improved from 40.53324 to 40.52050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 40.1497 - MinusLogProbMetric: 40.1497 - val_loss: 40.5205 - val_MinusLogProbMetric: 40.5205 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 364/1000
2023-10-26 16:25:30.053 
Epoch 364/1000 
	 loss: 40.2046, MinusLogProbMetric: 40.2046, val_loss: 40.5071, val_MinusLogProbMetric: 40.5071

Epoch 364: val_loss improved from 40.52050 to 40.50714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 40.2046 - MinusLogProbMetric: 40.2046 - val_loss: 40.5071 - val_MinusLogProbMetric: 40.5071 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 365/1000
2023-10-26 16:26:34.852 
Epoch 365/1000 
	 loss: 40.5361, MinusLogProbMetric: 40.5361, val_loss: 40.5497, val_MinusLogProbMetric: 40.5497

Epoch 365: val_loss did not improve from 40.50714
196/196 - 64s - loss: 40.5361 - MinusLogProbMetric: 40.5361 - val_loss: 40.5497 - val_MinusLogProbMetric: 40.5497 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 366/1000
2023-10-26 16:27:36.904 
Epoch 366/1000 
	 loss: 40.1570, MinusLogProbMetric: 40.1570, val_loss: 40.5264, val_MinusLogProbMetric: 40.5264

Epoch 366: val_loss did not improve from 40.50714
196/196 - 62s - loss: 40.1570 - MinusLogProbMetric: 40.1570 - val_loss: 40.5264 - val_MinusLogProbMetric: 40.5264 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 367/1000
2023-10-26 16:28:40.809 
Epoch 367/1000 
	 loss: 40.1117, MinusLogProbMetric: 40.1117, val_loss: 40.5188, val_MinusLogProbMetric: 40.5188

Epoch 367: val_loss did not improve from 40.50714
196/196 - 64s - loss: 40.1117 - MinusLogProbMetric: 40.1117 - val_loss: 40.5188 - val_MinusLogProbMetric: 40.5188 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 368/1000
2023-10-26 16:29:41.556 
Epoch 368/1000 
	 loss: 40.2247, MinusLogProbMetric: 40.2247, val_loss: 40.5223, val_MinusLogProbMetric: 40.5223

Epoch 368: val_loss did not improve from 40.50714
196/196 - 61s - loss: 40.2247 - MinusLogProbMetric: 40.2247 - val_loss: 40.5223 - val_MinusLogProbMetric: 40.5223 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 369/1000
2023-10-26 16:30:44.009 
Epoch 369/1000 
	 loss: 40.1041, MinusLogProbMetric: 40.1041, val_loss: 40.5377, val_MinusLogProbMetric: 40.5377

Epoch 369: val_loss did not improve from 40.50714
196/196 - 62s - loss: 40.1041 - MinusLogProbMetric: 40.1041 - val_loss: 40.5377 - val_MinusLogProbMetric: 40.5377 - lr: 4.5725e-07 - 62s/epoch - 319ms/step
Epoch 370/1000
2023-10-26 16:31:47.031 
Epoch 370/1000 
	 loss: 40.1114, MinusLogProbMetric: 40.1114, val_loss: 40.7210, val_MinusLogProbMetric: 40.7210

Epoch 370: val_loss did not improve from 40.50714
196/196 - 63s - loss: 40.1114 - MinusLogProbMetric: 40.1114 - val_loss: 40.7210 - val_MinusLogProbMetric: 40.7210 - lr: 4.5725e-07 - 63s/epoch - 322ms/step
Epoch 371/1000
2023-10-26 16:32:52.043 
Epoch 371/1000 
	 loss: 40.5058, MinusLogProbMetric: 40.5058, val_loss: 40.5353, val_MinusLogProbMetric: 40.5353

Epoch 371: val_loss did not improve from 40.50714
196/196 - 65s - loss: 40.5058 - MinusLogProbMetric: 40.5058 - val_loss: 40.5353 - val_MinusLogProbMetric: 40.5353 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 372/1000
2023-10-26 16:33:49.835 
Epoch 372/1000 
	 loss: 40.0822, MinusLogProbMetric: 40.0822, val_loss: 40.4390, val_MinusLogProbMetric: 40.4390

Epoch 372: val_loss improved from 40.50714 to 40.43896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 40.0822 - MinusLogProbMetric: 40.0822 - val_loss: 40.4390 - val_MinusLogProbMetric: 40.4390 - lr: 4.5725e-07 - 59s/epoch - 299ms/step
Epoch 373/1000
2023-10-26 16:35:23.553 
Epoch 373/1000 
	 loss: 40.0771, MinusLogProbMetric: 40.0771, val_loss: 40.4218, val_MinusLogProbMetric: 40.4218

Epoch 373: val_loss improved from 40.43896 to 40.42178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 94s - loss: 40.0771 - MinusLogProbMetric: 40.0771 - val_loss: 40.4218 - val_MinusLogProbMetric: 40.4218 - lr: 4.5725e-07 - 94s/epoch - 479ms/step
Epoch 374/1000
2023-10-26 16:36:29.037 
Epoch 374/1000 
	 loss: 40.0888, MinusLogProbMetric: 40.0888, val_loss: 40.5882, val_MinusLogProbMetric: 40.5882

Epoch 374: val_loss did not improve from 40.42178
196/196 - 64s - loss: 40.0888 - MinusLogProbMetric: 40.0888 - val_loss: 40.5882 - val_MinusLogProbMetric: 40.5882 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 375/1000
2023-10-26 16:37:30.678 
Epoch 375/1000 
	 loss: 40.0658, MinusLogProbMetric: 40.0658, val_loss: 40.4712, val_MinusLogProbMetric: 40.4712

Epoch 375: val_loss did not improve from 40.42178
196/196 - 62s - loss: 40.0658 - MinusLogProbMetric: 40.0658 - val_loss: 40.4712 - val_MinusLogProbMetric: 40.4712 - lr: 4.5725e-07 - 62s/epoch - 314ms/step
Epoch 376/1000
2023-10-26 16:38:29.936 
Epoch 376/1000 
	 loss: 40.0470, MinusLogProbMetric: 40.0470, val_loss: 40.4846, val_MinusLogProbMetric: 40.4846

Epoch 376: val_loss did not improve from 40.42178
196/196 - 59s - loss: 40.0470 - MinusLogProbMetric: 40.0470 - val_loss: 40.4846 - val_MinusLogProbMetric: 40.4846 - lr: 4.5725e-07 - 59s/epoch - 302ms/step
Epoch 377/1000
2023-10-26 16:39:34.753 
Epoch 377/1000 
	 loss: 40.0482, MinusLogProbMetric: 40.0482, val_loss: 40.4591, val_MinusLogProbMetric: 40.4591

Epoch 377: val_loss did not improve from 40.42178
196/196 - 65s - loss: 40.0482 - MinusLogProbMetric: 40.0482 - val_loss: 40.4591 - val_MinusLogProbMetric: 40.4591 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 378/1000
2023-10-26 16:40:37.621 
Epoch 378/1000 
	 loss: 40.4759, MinusLogProbMetric: 40.4759, val_loss: 40.5072, val_MinusLogProbMetric: 40.5072

Epoch 378: val_loss did not improve from 40.42178
196/196 - 63s - loss: 40.4759 - MinusLogProbMetric: 40.4759 - val_loss: 40.5072 - val_MinusLogProbMetric: 40.5072 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 379/1000
2023-10-26 16:41:39.463 
Epoch 379/1000 
	 loss: 40.0347, MinusLogProbMetric: 40.0347, val_loss: 40.4354, val_MinusLogProbMetric: 40.4354

Epoch 379: val_loss did not improve from 40.42178
196/196 - 62s - loss: 40.0347 - MinusLogProbMetric: 40.0347 - val_loss: 40.4354 - val_MinusLogProbMetric: 40.4354 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 380/1000
2023-10-26 16:42:44.039 
Epoch 380/1000 
	 loss: 40.1073, MinusLogProbMetric: 40.1073, val_loss: 40.5663, val_MinusLogProbMetric: 40.5663

Epoch 380: val_loss did not improve from 40.42178
196/196 - 65s - loss: 40.1073 - MinusLogProbMetric: 40.1073 - val_loss: 40.5663 - val_MinusLogProbMetric: 40.5663 - lr: 4.5725e-07 - 65s/epoch - 329ms/step
Epoch 381/1000
2023-10-26 16:43:48.344 
Epoch 381/1000 
	 loss: 40.0215, MinusLogProbMetric: 40.0215, val_loss: 40.4084, val_MinusLogProbMetric: 40.4084

Epoch 381: val_loss improved from 40.42178 to 40.40837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 40.0215 - MinusLogProbMetric: 40.0215 - val_loss: 40.4084 - val_MinusLogProbMetric: 40.4084 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 382/1000
2023-10-26 16:44:52.819 
Epoch 382/1000 
	 loss: 39.9910, MinusLogProbMetric: 39.9910, val_loss: 40.3971, val_MinusLogProbMetric: 40.3971

Epoch 382: val_loss improved from 40.40837 to 40.39714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 39.9910 - MinusLogProbMetric: 39.9910 - val_loss: 40.3971 - val_MinusLogProbMetric: 40.3971 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 383/1000
2023-10-26 16:45:55.876 
Epoch 383/1000 
	 loss: 40.0710, MinusLogProbMetric: 40.0710, val_loss: 40.3275, val_MinusLogProbMetric: 40.3275

Epoch 383: val_loss improved from 40.39714 to 40.32752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 40.0710 - MinusLogProbMetric: 40.0710 - val_loss: 40.3275 - val_MinusLogProbMetric: 40.3275 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 384/1000
2023-10-26 16:46:58.993 
Epoch 384/1000 
	 loss: 40.0202, MinusLogProbMetric: 40.0202, val_loss: 40.4795, val_MinusLogProbMetric: 40.4795

Epoch 384: val_loss did not improve from 40.32752
196/196 - 62s - loss: 40.0202 - MinusLogProbMetric: 40.0202 - val_loss: 40.4795 - val_MinusLogProbMetric: 40.4795 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 385/1000
2023-10-26 16:48:02.564 
Epoch 385/1000 
	 loss: 40.1940, MinusLogProbMetric: 40.1940, val_loss: 40.3976, val_MinusLogProbMetric: 40.3976

Epoch 385: val_loss did not improve from 40.32752
196/196 - 64s - loss: 40.1940 - MinusLogProbMetric: 40.1940 - val_loss: 40.3976 - val_MinusLogProbMetric: 40.3976 - lr: 4.5725e-07 - 64s/epoch - 324ms/step
Epoch 386/1000
2023-10-26 16:49:06.007 
Epoch 386/1000 
	 loss: 39.9838, MinusLogProbMetric: 39.9838, val_loss: 40.3933, val_MinusLogProbMetric: 40.3933

Epoch 386: val_loss did not improve from 40.32752
196/196 - 63s - loss: 39.9838 - MinusLogProbMetric: 39.9838 - val_loss: 40.3933 - val_MinusLogProbMetric: 40.3933 - lr: 4.5725e-07 - 63s/epoch - 324ms/step
Epoch 387/1000
2023-10-26 16:50:04.362 
Epoch 387/1000 
	 loss: 40.2505, MinusLogProbMetric: 40.2505, val_loss: 40.7627, val_MinusLogProbMetric: 40.7627

Epoch 387: val_loss did not improve from 40.32752
196/196 - 58s - loss: 40.2505 - MinusLogProbMetric: 40.2505 - val_loss: 40.7627 - val_MinusLogProbMetric: 40.7627 - lr: 4.5725e-07 - 58s/epoch - 298ms/step
Epoch 388/1000
2023-10-26 16:51:04.282 
Epoch 388/1000 
	 loss: 40.0283, MinusLogProbMetric: 40.0283, val_loss: 40.3386, val_MinusLogProbMetric: 40.3386

Epoch 388: val_loss did not improve from 40.32752
196/196 - 60s - loss: 40.0283 - MinusLogProbMetric: 40.0283 - val_loss: 40.3386 - val_MinusLogProbMetric: 40.3386 - lr: 4.5725e-07 - 60s/epoch - 306ms/step
Epoch 389/1000
2023-10-26 16:52:07.392 
Epoch 389/1000 
	 loss: 39.9308, MinusLogProbMetric: 39.9308, val_loss: 40.3395, val_MinusLogProbMetric: 40.3395

Epoch 389: val_loss did not improve from 40.32752
196/196 - 63s - loss: 39.9308 - MinusLogProbMetric: 39.9308 - val_loss: 40.3395 - val_MinusLogProbMetric: 40.3395 - lr: 4.5725e-07 - 63s/epoch - 322ms/step
Epoch 390/1000
2023-10-26 16:53:07.510 
Epoch 390/1000 
	 loss: 39.9142, MinusLogProbMetric: 39.9142, val_loss: 40.3681, val_MinusLogProbMetric: 40.3681

Epoch 390: val_loss did not improve from 40.32752
196/196 - 60s - loss: 39.9142 - MinusLogProbMetric: 39.9142 - val_loss: 40.3681 - val_MinusLogProbMetric: 40.3681 - lr: 4.5725e-07 - 60s/epoch - 307ms/step
Epoch 391/1000
2023-10-26 16:54:03.253 
Epoch 391/1000 
	 loss: 39.9334, MinusLogProbMetric: 39.9334, val_loss: 40.2969, val_MinusLogProbMetric: 40.2969

Epoch 391: val_loss improved from 40.32752 to 40.29687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 39.9334 - MinusLogProbMetric: 39.9334 - val_loss: 40.2969 - val_MinusLogProbMetric: 40.2969 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 392/1000
2023-10-26 16:55:07.379 
Epoch 392/1000 
	 loss: 40.2178, MinusLogProbMetric: 40.2178, val_loss: 40.4225, val_MinusLogProbMetric: 40.4225

Epoch 392: val_loss did not improve from 40.29687
196/196 - 63s - loss: 40.2178 - MinusLogProbMetric: 40.2178 - val_loss: 40.4225 - val_MinusLogProbMetric: 40.4225 - lr: 4.5725e-07 - 63s/epoch - 322ms/step
Epoch 393/1000
2023-10-26 16:56:12.411 
Epoch 393/1000 
	 loss: 40.0073, MinusLogProbMetric: 40.0073, val_loss: 40.3533, val_MinusLogProbMetric: 40.3533

Epoch 393: val_loss did not improve from 40.29687
196/196 - 65s - loss: 40.0073 - MinusLogProbMetric: 40.0073 - val_loss: 40.3533 - val_MinusLogProbMetric: 40.3533 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 394/1000
2023-10-26 16:57:15.831 
Epoch 394/1000 
	 loss: 39.9274, MinusLogProbMetric: 39.9274, val_loss: 40.3529, val_MinusLogProbMetric: 40.3529

Epoch 394: val_loss did not improve from 40.29687
196/196 - 63s - loss: 39.9274 - MinusLogProbMetric: 39.9274 - val_loss: 40.3529 - val_MinusLogProbMetric: 40.3529 - lr: 4.5725e-07 - 63s/epoch - 323ms/step
Epoch 395/1000
2023-10-26 16:58:20.152 
Epoch 395/1000 
	 loss: 39.8841, MinusLogProbMetric: 39.8841, val_loss: 40.2961, val_MinusLogProbMetric: 40.2961

Epoch 395: val_loss improved from 40.29687 to 40.29605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 39.8841 - MinusLogProbMetric: 39.8841 - val_loss: 40.2961 - val_MinusLogProbMetric: 40.2961 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 396/1000
2023-10-26 16:59:25.634 
Epoch 396/1000 
	 loss: 39.9722, MinusLogProbMetric: 39.9722, val_loss: 40.3829, val_MinusLogProbMetric: 40.3829

Epoch 396: val_loss did not improve from 40.29605
196/196 - 64s - loss: 39.9722 - MinusLogProbMetric: 39.9722 - val_loss: 40.3829 - val_MinusLogProbMetric: 40.3829 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 397/1000
2023-10-26 17:00:27.518 
Epoch 397/1000 
	 loss: 39.9538, MinusLogProbMetric: 39.9538, val_loss: 40.3960, val_MinusLogProbMetric: 40.3960

Epoch 397: val_loss did not improve from 40.29605
196/196 - 62s - loss: 39.9538 - MinusLogProbMetric: 39.9538 - val_loss: 40.3960 - val_MinusLogProbMetric: 40.3960 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 398/1000
2023-10-26 17:01:31.213 
Epoch 398/1000 
	 loss: 39.8634, MinusLogProbMetric: 39.8634, val_loss: 40.2703, val_MinusLogProbMetric: 40.2703

Epoch 398: val_loss improved from 40.29605 to 40.27032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 39.8634 - MinusLogProbMetric: 39.8634 - val_loss: 40.2703 - val_MinusLogProbMetric: 40.2703 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 399/1000
2023-10-26 17:02:33.125 
Epoch 399/1000 
	 loss: 40.0271, MinusLogProbMetric: 40.0271, val_loss: 40.3303, val_MinusLogProbMetric: 40.3303

Epoch 399: val_loss did not improve from 40.27032
196/196 - 61s - loss: 40.0271 - MinusLogProbMetric: 40.0271 - val_loss: 40.3303 - val_MinusLogProbMetric: 40.3303 - lr: 4.5725e-07 - 61s/epoch - 311ms/step
Epoch 400/1000
2023-10-26 17:03:38.246 
Epoch 400/1000 
	 loss: 39.8808, MinusLogProbMetric: 39.8808, val_loss: 40.2274, val_MinusLogProbMetric: 40.2274

Epoch 400: val_loss improved from 40.27032 to 40.22739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.8808 - MinusLogProbMetric: 39.8808 - val_loss: 40.2274 - val_MinusLogProbMetric: 40.2274 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 401/1000
2023-10-26 17:04:40.250 
Epoch 401/1000 
	 loss: 39.8245, MinusLogProbMetric: 39.8245, val_loss: 40.3164, val_MinusLogProbMetric: 40.3164

Epoch 401: val_loss did not improve from 40.22739
196/196 - 61s - loss: 39.8245 - MinusLogProbMetric: 39.8245 - val_loss: 40.3164 - val_MinusLogProbMetric: 40.3164 - lr: 4.5725e-07 - 61s/epoch - 311ms/step
Epoch 402/1000
2023-10-26 17:05:42.903 
Epoch 402/1000 
	 loss: 39.8288, MinusLogProbMetric: 39.8288, val_loss: 40.2548, val_MinusLogProbMetric: 40.2548

Epoch 402: val_loss did not improve from 40.22739
196/196 - 63s - loss: 39.8288 - MinusLogProbMetric: 39.8288 - val_loss: 40.2548 - val_MinusLogProbMetric: 40.2548 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 403/1000
2023-10-26 17:06:44.600 
Epoch 403/1000 
	 loss: 39.9348, MinusLogProbMetric: 39.9348, val_loss: 40.2135, val_MinusLogProbMetric: 40.2135

Epoch 403: val_loss improved from 40.22739 to 40.21352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 39.9348 - MinusLogProbMetric: 39.9348 - val_loss: 40.2135 - val_MinusLogProbMetric: 40.2135 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 404/1000
2023-10-26 17:07:50.574 
Epoch 404/1000 
	 loss: 39.8051, MinusLogProbMetric: 39.8051, val_loss: 40.2448, val_MinusLogProbMetric: 40.2448

Epoch 404: val_loss did not improve from 40.21352
196/196 - 65s - loss: 39.8051 - MinusLogProbMetric: 39.8051 - val_loss: 40.2448 - val_MinusLogProbMetric: 40.2448 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 405/1000
2023-10-26 17:08:54.635 
Epoch 405/1000 
	 loss: 39.7981, MinusLogProbMetric: 39.7981, val_loss: 40.2634, val_MinusLogProbMetric: 40.2634

Epoch 405: val_loss did not improve from 40.21352
196/196 - 64s - loss: 39.7981 - MinusLogProbMetric: 39.7981 - val_loss: 40.2634 - val_MinusLogProbMetric: 40.2634 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 406/1000
2023-10-26 17:09:57.739 
Epoch 406/1000 
	 loss: 39.8228, MinusLogProbMetric: 39.8228, val_loss: 40.2210, val_MinusLogProbMetric: 40.2210

Epoch 406: val_loss did not improve from 40.21352
196/196 - 63s - loss: 39.8228 - MinusLogProbMetric: 39.8228 - val_loss: 40.2210 - val_MinusLogProbMetric: 40.2210 - lr: 4.5725e-07 - 63s/epoch - 322ms/step
Epoch 407/1000
2023-10-26 17:11:02.326 
Epoch 407/1000 
	 loss: 39.7789, MinusLogProbMetric: 39.7789, val_loss: 40.2516, val_MinusLogProbMetric: 40.2516

Epoch 407: val_loss did not improve from 40.21352
196/196 - 65s - loss: 39.7789 - MinusLogProbMetric: 39.7789 - val_loss: 40.2516 - val_MinusLogProbMetric: 40.2516 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 408/1000
2023-10-26 17:12:06.011 
Epoch 408/1000 
	 loss: 39.8518, MinusLogProbMetric: 39.8518, val_loss: 40.1707, val_MinusLogProbMetric: 40.1707

Epoch 408: val_loss improved from 40.21352 to 40.17072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 39.8518 - MinusLogProbMetric: 39.8518 - val_loss: 40.1707 - val_MinusLogProbMetric: 40.1707 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 409/1000
2023-10-26 17:13:07.812 
Epoch 409/1000 
	 loss: 39.7838, MinusLogProbMetric: 39.7838, val_loss: 40.2073, val_MinusLogProbMetric: 40.2073

Epoch 409: val_loss did not improve from 40.17072
196/196 - 61s - loss: 39.7838 - MinusLogProbMetric: 39.7838 - val_loss: 40.2073 - val_MinusLogProbMetric: 40.2073 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 410/1000
2023-10-26 17:14:10.191 
Epoch 410/1000 
	 loss: 40.0264, MinusLogProbMetric: 40.0264, val_loss: 40.2161, val_MinusLogProbMetric: 40.2161

Epoch 410: val_loss did not improve from 40.17072
196/196 - 62s - loss: 40.0264 - MinusLogProbMetric: 40.0264 - val_loss: 40.2161 - val_MinusLogProbMetric: 40.2161 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 411/1000
2023-10-26 17:15:11.537 
Epoch 411/1000 
	 loss: 39.7710, MinusLogProbMetric: 39.7710, val_loss: 40.2658, val_MinusLogProbMetric: 40.2658

Epoch 411: val_loss did not improve from 40.17072
196/196 - 61s - loss: 39.7710 - MinusLogProbMetric: 39.7710 - val_loss: 40.2658 - val_MinusLogProbMetric: 40.2658 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 412/1000
2023-10-26 17:16:14.712 
Epoch 412/1000 
	 loss: 39.7721, MinusLogProbMetric: 39.7721, val_loss: 40.1231, val_MinusLogProbMetric: 40.1231

Epoch 412: val_loss improved from 40.17072 to 40.12313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 39.7721 - MinusLogProbMetric: 39.7721 - val_loss: 40.1231 - val_MinusLogProbMetric: 40.1231 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 413/1000
2023-10-26 17:17:18.618 
Epoch 413/1000 
	 loss: 39.7405, MinusLogProbMetric: 39.7405, val_loss: 40.1365, val_MinusLogProbMetric: 40.1365

Epoch 413: val_loss did not improve from 40.12313
196/196 - 63s - loss: 39.7405 - MinusLogProbMetric: 39.7405 - val_loss: 40.1365 - val_MinusLogProbMetric: 40.1365 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 414/1000
2023-10-26 17:18:22.557 
Epoch 414/1000 
	 loss: 39.7312, MinusLogProbMetric: 39.7312, val_loss: 40.1534, val_MinusLogProbMetric: 40.1534

Epoch 414: val_loss did not improve from 40.12313
196/196 - 64s - loss: 39.7312 - MinusLogProbMetric: 39.7312 - val_loss: 40.1534 - val_MinusLogProbMetric: 40.1534 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 415/1000
2023-10-26 17:19:20.650 
Epoch 415/1000 
	 loss: 39.7275, MinusLogProbMetric: 39.7275, val_loss: 40.1178, val_MinusLogProbMetric: 40.1178

Epoch 415: val_loss improved from 40.12313 to 40.11782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 39.7275 - MinusLogProbMetric: 39.7275 - val_loss: 40.1178 - val_MinusLogProbMetric: 40.1178 - lr: 4.5725e-07 - 59s/epoch - 301ms/step
Epoch 416/1000
2023-10-26 17:20:22.759 
Epoch 416/1000 
	 loss: 39.7255, MinusLogProbMetric: 39.7255, val_loss: 40.1449, val_MinusLogProbMetric: 40.1449

Epoch 416: val_loss did not improve from 40.11782
196/196 - 61s - loss: 39.7255 - MinusLogProbMetric: 39.7255 - val_loss: 40.1449 - val_MinusLogProbMetric: 40.1449 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 417/1000
2023-10-26 17:21:26.404 
Epoch 417/1000 
	 loss: 39.8855, MinusLogProbMetric: 39.8855, val_loss: 40.0463, val_MinusLogProbMetric: 40.0463

Epoch 417: val_loss improved from 40.11782 to 40.04630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 39.8855 - MinusLogProbMetric: 39.8855 - val_loss: 40.0463 - val_MinusLogProbMetric: 40.0463 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 418/1000
2023-10-26 17:22:29.333 
Epoch 418/1000 
	 loss: 40.2757, MinusLogProbMetric: 40.2757, val_loss: 40.2531, val_MinusLogProbMetric: 40.2531

Epoch 418: val_loss did not improve from 40.04630
196/196 - 62s - loss: 40.2757 - MinusLogProbMetric: 40.2757 - val_loss: 40.2531 - val_MinusLogProbMetric: 40.2531 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 419/1000
2023-10-26 17:23:33.820 
Epoch 419/1000 
	 loss: 39.7512, MinusLogProbMetric: 39.7512, val_loss: 40.1210, val_MinusLogProbMetric: 40.1210

Epoch 419: val_loss did not improve from 40.04630
196/196 - 64s - loss: 39.7512 - MinusLogProbMetric: 39.7512 - val_loss: 40.1210 - val_MinusLogProbMetric: 40.1210 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 420/1000
2023-10-26 17:24:35.229 
Epoch 420/1000 
	 loss: 39.7146, MinusLogProbMetric: 39.7146, val_loss: 40.0474, val_MinusLogProbMetric: 40.0474

Epoch 420: val_loss did not improve from 40.04630
196/196 - 61s - loss: 39.7146 - MinusLogProbMetric: 39.7146 - val_loss: 40.0474 - val_MinusLogProbMetric: 40.0474 - lr: 4.5725e-07 - 61s/epoch - 313ms/step
Epoch 421/1000
2023-10-26 17:25:38.515 
Epoch 421/1000 
	 loss: 39.6966, MinusLogProbMetric: 39.6966, val_loss: 40.2071, val_MinusLogProbMetric: 40.2071

Epoch 421: val_loss did not improve from 40.04630
196/196 - 63s - loss: 39.6966 - MinusLogProbMetric: 39.6966 - val_loss: 40.2071 - val_MinusLogProbMetric: 40.2071 - lr: 4.5725e-07 - 63s/epoch - 323ms/step
Epoch 422/1000
2023-10-26 17:26:42.595 
Epoch 422/1000 
	 loss: 39.6769, MinusLogProbMetric: 39.6769, val_loss: 40.2081, val_MinusLogProbMetric: 40.2081

Epoch 422: val_loss did not improve from 40.04630
196/196 - 64s - loss: 39.6769 - MinusLogProbMetric: 39.6769 - val_loss: 40.2081 - val_MinusLogProbMetric: 40.2081 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 423/1000
2023-10-26 17:27:43.546 
Epoch 423/1000 
	 loss: 39.7020, MinusLogProbMetric: 39.7020, val_loss: 40.0500, val_MinusLogProbMetric: 40.0500

Epoch 423: val_loss did not improve from 40.04630
196/196 - 61s - loss: 39.7020 - MinusLogProbMetric: 39.7020 - val_loss: 40.0500 - val_MinusLogProbMetric: 40.0500 - lr: 4.5725e-07 - 61s/epoch - 311ms/step
Epoch 424/1000
2023-10-26 17:28:46.131 
Epoch 424/1000 
	 loss: 39.6611, MinusLogProbMetric: 39.6611, val_loss: 40.0757, val_MinusLogProbMetric: 40.0757

Epoch 424: val_loss did not improve from 40.04630
196/196 - 63s - loss: 39.6611 - MinusLogProbMetric: 39.6611 - val_loss: 40.0757 - val_MinusLogProbMetric: 40.0757 - lr: 4.5725e-07 - 63s/epoch - 319ms/step
Epoch 425/1000
2023-10-26 17:29:48.732 
Epoch 425/1000 
	 loss: 39.6494, MinusLogProbMetric: 39.6494, val_loss: 40.0814, val_MinusLogProbMetric: 40.0814

Epoch 425: val_loss did not improve from 40.04630
196/196 - 63s - loss: 39.6494 - MinusLogProbMetric: 39.6494 - val_loss: 40.0814 - val_MinusLogProbMetric: 40.0814 - lr: 4.5725e-07 - 63s/epoch - 319ms/step
Epoch 426/1000
2023-10-26 17:30:53.128 
Epoch 426/1000 
	 loss: 39.6438, MinusLogProbMetric: 39.6438, val_loss: 40.0863, val_MinusLogProbMetric: 40.0863

Epoch 426: val_loss did not improve from 40.04630
196/196 - 64s - loss: 39.6438 - MinusLogProbMetric: 39.6438 - val_loss: 40.0863 - val_MinusLogProbMetric: 40.0863 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 427/1000
2023-10-26 17:31:56.938 
Epoch 427/1000 
	 loss: 39.6572, MinusLogProbMetric: 39.6572, val_loss: 40.0667, val_MinusLogProbMetric: 40.0667

Epoch 427: val_loss did not improve from 40.04630
196/196 - 64s - loss: 39.6572 - MinusLogProbMetric: 39.6572 - val_loss: 40.0667 - val_MinusLogProbMetric: 40.0667 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 428/1000
2023-10-26 17:33:00.846 
Epoch 428/1000 
	 loss: 39.6432, MinusLogProbMetric: 39.6432, val_loss: 40.0740, val_MinusLogProbMetric: 40.0740

Epoch 428: val_loss did not improve from 40.04630
196/196 - 64s - loss: 39.6432 - MinusLogProbMetric: 39.6432 - val_loss: 40.0740 - val_MinusLogProbMetric: 40.0740 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 429/1000
2023-10-26 17:34:03.659 
Epoch 429/1000 
	 loss: 39.6567, MinusLogProbMetric: 39.6567, val_loss: 40.0009, val_MinusLogProbMetric: 40.0009

Epoch 429: val_loss improved from 40.04630 to 40.00088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 39.6567 - MinusLogProbMetric: 39.6567 - val_loss: 40.0009 - val_MinusLogProbMetric: 40.0009 - lr: 4.5725e-07 - 64s/epoch - 325ms/step
Epoch 430/1000
2023-10-26 17:35:07.648 
Epoch 430/1000 
	 loss: 39.6694, MinusLogProbMetric: 39.6694, val_loss: 40.2349, val_MinusLogProbMetric: 40.2349

Epoch 430: val_loss did not improve from 40.00088
196/196 - 63s - loss: 39.6694 - MinusLogProbMetric: 39.6694 - val_loss: 40.2349 - val_MinusLogProbMetric: 40.2349 - lr: 4.5725e-07 - 63s/epoch - 322ms/step
Epoch 431/1000
2023-10-26 17:36:11.321 
Epoch 431/1000 
	 loss: 39.6146, MinusLogProbMetric: 39.6146, val_loss: 40.0349, val_MinusLogProbMetric: 40.0349

Epoch 431: val_loss did not improve from 40.00088
196/196 - 64s - loss: 39.6146 - MinusLogProbMetric: 39.6146 - val_loss: 40.0349 - val_MinusLogProbMetric: 40.0349 - lr: 4.5725e-07 - 64s/epoch - 325ms/step
Epoch 432/1000
2023-10-26 17:37:16.061 
Epoch 432/1000 
	 loss: 39.6744, MinusLogProbMetric: 39.6744, val_loss: 40.1031, val_MinusLogProbMetric: 40.1031

Epoch 432: val_loss did not improve from 40.00088
196/196 - 65s - loss: 39.6744 - MinusLogProbMetric: 39.6744 - val_loss: 40.1031 - val_MinusLogProbMetric: 40.1031 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 433/1000
2023-10-26 17:38:21.169 
Epoch 433/1000 
	 loss: 39.5920, MinusLogProbMetric: 39.5920, val_loss: 40.0355, val_MinusLogProbMetric: 40.0355

Epoch 433: val_loss did not improve from 40.00088
196/196 - 65s - loss: 39.5920 - MinusLogProbMetric: 39.5920 - val_loss: 40.0355 - val_MinusLogProbMetric: 40.0355 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 434/1000
2023-10-26 17:39:26.068 
Epoch 434/1000 
	 loss: 39.6367, MinusLogProbMetric: 39.6367, val_loss: 39.9971, val_MinusLogProbMetric: 39.9971

Epoch 434: val_loss improved from 40.00088 to 39.99713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.6367 - MinusLogProbMetric: 39.6367 - val_loss: 39.9971 - val_MinusLogProbMetric: 39.9971 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 435/1000
2023-10-26 17:40:31.725 
Epoch 435/1000 
	 loss: 39.5939, MinusLogProbMetric: 39.5939, val_loss: 40.0504, val_MinusLogProbMetric: 40.0504

Epoch 435: val_loss did not improve from 39.99713
196/196 - 65s - loss: 39.5939 - MinusLogProbMetric: 39.5939 - val_loss: 40.0504 - val_MinusLogProbMetric: 40.0504 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 436/1000
2023-10-26 17:41:36.859 
Epoch 436/1000 
	 loss: 39.7241, MinusLogProbMetric: 39.7241, val_loss: 40.3069, val_MinusLogProbMetric: 40.3069

Epoch 436: val_loss did not improve from 39.99713
196/196 - 65s - loss: 39.7241 - MinusLogProbMetric: 39.7241 - val_loss: 40.3069 - val_MinusLogProbMetric: 40.3069 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 437/1000
2023-10-26 17:42:42.210 
Epoch 437/1000 
	 loss: 39.9832, MinusLogProbMetric: 39.9832, val_loss: 40.0568, val_MinusLogProbMetric: 40.0568

Epoch 437: val_loss did not improve from 39.99713
196/196 - 65s - loss: 39.9832 - MinusLogProbMetric: 39.9832 - val_loss: 40.0568 - val_MinusLogProbMetric: 40.0568 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 438/1000
2023-10-26 17:43:45.039 
Epoch 438/1000 
	 loss: 39.6100, MinusLogProbMetric: 39.6100, val_loss: 39.9494, val_MinusLogProbMetric: 39.9494

Epoch 438: val_loss improved from 39.99713 to 39.94937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 39.6100 - MinusLogProbMetric: 39.6100 - val_loss: 39.9494 - val_MinusLogProbMetric: 39.9494 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 439/1000
2023-10-26 17:44:45.759 
Epoch 439/1000 
	 loss: 39.5660, MinusLogProbMetric: 39.5660, val_loss: 40.0300, val_MinusLogProbMetric: 40.0300

Epoch 439: val_loss did not improve from 39.94937
196/196 - 60s - loss: 39.5660 - MinusLogProbMetric: 39.5660 - val_loss: 40.0300 - val_MinusLogProbMetric: 40.0300 - lr: 4.5725e-07 - 60s/epoch - 304ms/step
Epoch 440/1000
2023-10-26 17:45:50.288 
Epoch 440/1000 
	 loss: 39.8538, MinusLogProbMetric: 39.8538, val_loss: 40.0689, val_MinusLogProbMetric: 40.0689

Epoch 440: val_loss did not improve from 39.94937
196/196 - 65s - loss: 39.8538 - MinusLogProbMetric: 39.8538 - val_loss: 40.0689 - val_MinusLogProbMetric: 40.0689 - lr: 4.5725e-07 - 65s/epoch - 329ms/step
Epoch 441/1000
2023-10-26 17:46:55.195 
Epoch 441/1000 
	 loss: 39.5469, MinusLogProbMetric: 39.5469, val_loss: 39.9145, val_MinusLogProbMetric: 39.9145

Epoch 441: val_loss improved from 39.94937 to 39.91446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.5469 - MinusLogProbMetric: 39.5469 - val_loss: 39.9145 - val_MinusLogProbMetric: 39.9145 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 442/1000
2023-10-26 17:48:01.034 
Epoch 442/1000 
	 loss: 39.5151, MinusLogProbMetric: 39.5151, val_loss: 39.8692, val_MinusLogProbMetric: 39.8692

Epoch 442: val_loss improved from 39.91446 to 39.86917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.5151 - MinusLogProbMetric: 39.5151 - val_loss: 39.8692 - val_MinusLogProbMetric: 39.8692 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 443/1000
2023-10-26 17:49:07.136 
Epoch 443/1000 
	 loss: 39.5165, MinusLogProbMetric: 39.5165, val_loss: 39.9562, val_MinusLogProbMetric: 39.9562

Epoch 443: val_loss did not improve from 39.86917
196/196 - 65s - loss: 39.5165 - MinusLogProbMetric: 39.5165 - val_loss: 39.9562 - val_MinusLogProbMetric: 39.9562 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 444/1000
2023-10-26 17:50:12.031 
Epoch 444/1000 
	 loss: 39.5494, MinusLogProbMetric: 39.5494, val_loss: 39.9066, val_MinusLogProbMetric: 39.9066

Epoch 444: val_loss did not improve from 39.86917
196/196 - 65s - loss: 39.5494 - MinusLogProbMetric: 39.5494 - val_loss: 39.9066 - val_MinusLogProbMetric: 39.9066 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 445/1000
2023-10-26 17:51:17.155 
Epoch 445/1000 
	 loss: 39.8554, MinusLogProbMetric: 39.8554, val_loss: 39.8942, val_MinusLogProbMetric: 39.8942

Epoch 445: val_loss did not improve from 39.86917
196/196 - 65s - loss: 39.8554 - MinusLogProbMetric: 39.8554 - val_loss: 39.8942 - val_MinusLogProbMetric: 39.8942 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 446/1000
2023-10-26 17:52:22.455 
Epoch 446/1000 
	 loss: 39.4999, MinusLogProbMetric: 39.4999, val_loss: 39.8725, val_MinusLogProbMetric: 39.8725

Epoch 446: val_loss did not improve from 39.86917
196/196 - 65s - loss: 39.4999 - MinusLogProbMetric: 39.4999 - val_loss: 39.8725 - val_MinusLogProbMetric: 39.8725 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 447/1000
2023-10-26 17:53:27.441 
Epoch 447/1000 
	 loss: 39.6110, MinusLogProbMetric: 39.6110, val_loss: 39.8898, val_MinusLogProbMetric: 39.8898

Epoch 447: val_loss did not improve from 39.86917
196/196 - 65s - loss: 39.6110 - MinusLogProbMetric: 39.6110 - val_loss: 39.8898 - val_MinusLogProbMetric: 39.8898 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 448/1000
2023-10-26 17:54:32.136 
Epoch 448/1000 
	 loss: 39.4747, MinusLogProbMetric: 39.4747, val_loss: 39.8594, val_MinusLogProbMetric: 39.8594

Epoch 448: val_loss improved from 39.86917 to 39.85942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.4747 - MinusLogProbMetric: 39.4747 - val_loss: 39.8594 - val_MinusLogProbMetric: 39.8594 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 449/1000
2023-10-26 17:55:38.356 
Epoch 449/1000 
	 loss: 39.5909, MinusLogProbMetric: 39.5909, val_loss: 40.0488, val_MinusLogProbMetric: 40.0488

Epoch 449: val_loss did not improve from 39.85942
196/196 - 65s - loss: 39.5909 - MinusLogProbMetric: 39.5909 - val_loss: 40.0488 - val_MinusLogProbMetric: 40.0488 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 450/1000
2023-10-26 17:56:43.520 
Epoch 450/1000 
	 loss: 39.4903, MinusLogProbMetric: 39.4903, val_loss: 39.8370, val_MinusLogProbMetric: 39.8370

Epoch 450: val_loss improved from 39.85942 to 39.83704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.4903 - MinusLogProbMetric: 39.4903 - val_loss: 39.8370 - val_MinusLogProbMetric: 39.8370 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 451/1000
2023-10-26 17:57:49.548 
Epoch 451/1000 
	 loss: 39.4560, MinusLogProbMetric: 39.4560, val_loss: 39.9118, val_MinusLogProbMetric: 39.9118

Epoch 451: val_loss did not improve from 39.83704
196/196 - 65s - loss: 39.4560 - MinusLogProbMetric: 39.4560 - val_loss: 39.9118 - val_MinusLogProbMetric: 39.9118 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 452/1000
2023-10-26 17:58:54.744 
Epoch 452/1000 
	 loss: 39.8507, MinusLogProbMetric: 39.8507, val_loss: 40.0816, val_MinusLogProbMetric: 40.0816

Epoch 452: val_loss did not improve from 39.83704
196/196 - 65s - loss: 39.8507 - MinusLogProbMetric: 39.8507 - val_loss: 40.0816 - val_MinusLogProbMetric: 40.0816 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 453/1000
2023-10-26 17:59:59.775 
Epoch 453/1000 
	 loss: 39.5053, MinusLogProbMetric: 39.5053, val_loss: 39.8681, val_MinusLogProbMetric: 39.8681

Epoch 453: val_loss did not improve from 39.83704
196/196 - 65s - loss: 39.5053 - MinusLogProbMetric: 39.5053 - val_loss: 39.8681 - val_MinusLogProbMetric: 39.8681 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 454/1000
2023-10-26 18:01:04.631 
Epoch 454/1000 
	 loss: 39.4386, MinusLogProbMetric: 39.4386, val_loss: 39.8138, val_MinusLogProbMetric: 39.8138

Epoch 454: val_loss improved from 39.83704 to 39.81379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.4386 - MinusLogProbMetric: 39.4386 - val_loss: 39.8138 - val_MinusLogProbMetric: 39.8138 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 455/1000
2023-10-26 18:02:10.678 
Epoch 455/1000 
	 loss: 39.7701, MinusLogProbMetric: 39.7701, val_loss: 39.9835, val_MinusLogProbMetric: 39.9835

Epoch 455: val_loss did not improve from 39.81379
196/196 - 65s - loss: 39.7701 - MinusLogProbMetric: 39.7701 - val_loss: 39.9835 - val_MinusLogProbMetric: 39.9835 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 456/1000
2023-10-26 18:03:15.873 
Epoch 456/1000 
	 loss: 39.4769, MinusLogProbMetric: 39.4769, val_loss: 39.8037, val_MinusLogProbMetric: 39.8037

Epoch 456: val_loss improved from 39.81379 to 39.80375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.4769 - MinusLogProbMetric: 39.4769 - val_loss: 39.8037 - val_MinusLogProbMetric: 39.8037 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 457/1000
2023-10-26 18:04:22.440 
Epoch 457/1000 
	 loss: 39.4243, MinusLogProbMetric: 39.4243, val_loss: 39.8579, val_MinusLogProbMetric: 39.8579

Epoch 457: val_loss did not improve from 39.80375
196/196 - 65s - loss: 39.4243 - MinusLogProbMetric: 39.4243 - val_loss: 39.8579 - val_MinusLogProbMetric: 39.8579 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 458/1000
2023-10-26 18:05:27.629 
Epoch 458/1000 
	 loss: 39.4786, MinusLogProbMetric: 39.4786, val_loss: 39.8726, val_MinusLogProbMetric: 39.8726

Epoch 458: val_loss did not improve from 39.80375
196/196 - 65s - loss: 39.4786 - MinusLogProbMetric: 39.4786 - val_loss: 39.8726 - val_MinusLogProbMetric: 39.8726 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 459/1000
2023-10-26 18:06:29.610 
Epoch 459/1000 
	 loss: 39.4254, MinusLogProbMetric: 39.4254, val_loss: 39.8191, val_MinusLogProbMetric: 39.8191

Epoch 459: val_loss did not improve from 39.80375
196/196 - 62s - loss: 39.4254 - MinusLogProbMetric: 39.4254 - val_loss: 39.8191 - val_MinusLogProbMetric: 39.8191 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 460/1000
2023-10-26 18:07:29.594 
Epoch 460/1000 
	 loss: 39.4673, MinusLogProbMetric: 39.4673, val_loss: 41.4347, val_MinusLogProbMetric: 41.4347

Epoch 460: val_loss did not improve from 39.80375
196/196 - 60s - loss: 39.4673 - MinusLogProbMetric: 39.4673 - val_loss: 41.4347 - val_MinusLogProbMetric: 41.4347 - lr: 4.5725e-07 - 60s/epoch - 306ms/step
Epoch 461/1000
2023-10-26 18:08:31.767 
Epoch 461/1000 
	 loss: 39.5719, MinusLogProbMetric: 39.5719, val_loss: 39.8314, val_MinusLogProbMetric: 39.8314

Epoch 461: val_loss did not improve from 39.80375
196/196 - 62s - loss: 39.5719 - MinusLogProbMetric: 39.5719 - val_loss: 39.8314 - val_MinusLogProbMetric: 39.8314 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 462/1000
2023-10-26 18:09:37.462 
Epoch 462/1000 
	 loss: 39.4087, MinusLogProbMetric: 39.4087, val_loss: 39.7585, val_MinusLogProbMetric: 39.7585

Epoch 462: val_loss improved from 39.80375 to 39.75852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.4087 - MinusLogProbMetric: 39.4087 - val_loss: 39.7585 - val_MinusLogProbMetric: 39.7585 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 463/1000
2023-10-26 18:10:43.701 
Epoch 463/1000 
	 loss: 39.3655, MinusLogProbMetric: 39.3655, val_loss: 39.8711, val_MinusLogProbMetric: 39.8711

Epoch 463: val_loss did not improve from 39.75852
196/196 - 65s - loss: 39.3655 - MinusLogProbMetric: 39.3655 - val_loss: 39.8711 - val_MinusLogProbMetric: 39.8711 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 464/1000
2023-10-26 18:11:49.368 
Epoch 464/1000 
	 loss: 39.3722, MinusLogProbMetric: 39.3722, val_loss: 39.7576, val_MinusLogProbMetric: 39.7576

Epoch 464: val_loss improved from 39.75852 to 39.75760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.3722 - MinusLogProbMetric: 39.3722 - val_loss: 39.7576 - val_MinusLogProbMetric: 39.7576 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 465/1000
2023-10-26 18:12:55.526 
Epoch 465/1000 
	 loss: 39.5957, MinusLogProbMetric: 39.5957, val_loss: 39.8382, val_MinusLogProbMetric: 39.8382

Epoch 465: val_loss did not improve from 39.75760
196/196 - 65s - loss: 39.5957 - MinusLogProbMetric: 39.5957 - val_loss: 39.8382 - val_MinusLogProbMetric: 39.8382 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 466/1000
2023-10-26 18:14:00.595 
Epoch 466/1000 
	 loss: 39.4012, MinusLogProbMetric: 39.4012, val_loss: 39.8909, val_MinusLogProbMetric: 39.8909

Epoch 466: val_loss did not improve from 39.75760
196/196 - 65s - loss: 39.4012 - MinusLogProbMetric: 39.4012 - val_loss: 39.8909 - val_MinusLogProbMetric: 39.8909 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 467/1000
2023-10-26 18:15:05.614 
Epoch 467/1000 
	 loss: 39.3526, MinusLogProbMetric: 39.3526, val_loss: 39.7533, val_MinusLogProbMetric: 39.7533

Epoch 467: val_loss improved from 39.75760 to 39.75333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.3526 - MinusLogProbMetric: 39.3526 - val_loss: 39.7533 - val_MinusLogProbMetric: 39.7533 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 468/1000
2023-10-26 18:16:11.893 
Epoch 468/1000 
	 loss: 39.3296, MinusLogProbMetric: 39.3296, val_loss: 39.7279, val_MinusLogProbMetric: 39.7279

Epoch 468: val_loss improved from 39.75333 to 39.72790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.3296 - MinusLogProbMetric: 39.3296 - val_loss: 39.7279 - val_MinusLogProbMetric: 39.7279 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 469/1000
2023-10-26 18:17:09.998 
Epoch 469/1000 
	 loss: 39.3926, MinusLogProbMetric: 39.3926, val_loss: 39.7931, val_MinusLogProbMetric: 39.7931

Epoch 469: val_loss did not improve from 39.72790
196/196 - 57s - loss: 39.3926 - MinusLogProbMetric: 39.3926 - val_loss: 39.7931 - val_MinusLogProbMetric: 39.7931 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 470/1000
2023-10-26 18:18:14.179 
Epoch 470/1000 
	 loss: 39.3278, MinusLogProbMetric: 39.3278, val_loss: 39.7118, val_MinusLogProbMetric: 39.7118

Epoch 470: val_loss improved from 39.72790 to 39.71184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 39.3278 - MinusLogProbMetric: 39.3278 - val_loss: 39.7118 - val_MinusLogProbMetric: 39.7118 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 471/1000
2023-10-26 18:19:13.397 
Epoch 471/1000 
	 loss: 39.3272, MinusLogProbMetric: 39.3272, val_loss: 39.8492, val_MinusLogProbMetric: 39.8492

Epoch 471: val_loss did not improve from 39.71184
196/196 - 58s - loss: 39.3272 - MinusLogProbMetric: 39.3272 - val_loss: 39.8492 - val_MinusLogProbMetric: 39.8492 - lr: 4.5725e-07 - 58s/epoch - 297ms/step
Epoch 472/1000
2023-10-26 18:20:19.283 
Epoch 472/1000 
	 loss: 39.3135, MinusLogProbMetric: 39.3135, val_loss: 39.7236, val_MinusLogProbMetric: 39.7236

Epoch 472: val_loss did not improve from 39.71184
196/196 - 66s - loss: 39.3135 - MinusLogProbMetric: 39.3135 - val_loss: 39.7236 - val_MinusLogProbMetric: 39.7236 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 473/1000
2023-10-26 18:21:25.013 
Epoch 473/1000 
	 loss: 39.3933, MinusLogProbMetric: 39.3933, val_loss: 39.7331, val_MinusLogProbMetric: 39.7331

Epoch 473: val_loss did not improve from 39.71184
196/196 - 66s - loss: 39.3933 - MinusLogProbMetric: 39.3933 - val_loss: 39.7331 - val_MinusLogProbMetric: 39.7331 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 474/1000
2023-10-26 18:22:30.626 
Epoch 474/1000 
	 loss: 39.3304, MinusLogProbMetric: 39.3304, val_loss: 39.7043, val_MinusLogProbMetric: 39.7043

Epoch 474: val_loss improved from 39.71184 to 39.70433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.3304 - MinusLogProbMetric: 39.3304 - val_loss: 39.7043 - val_MinusLogProbMetric: 39.7043 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 475/1000
2023-10-26 18:23:36.878 
Epoch 475/1000 
	 loss: 39.2949, MinusLogProbMetric: 39.2949, val_loss: 39.7473, val_MinusLogProbMetric: 39.7473

Epoch 475: val_loss did not improve from 39.70433
196/196 - 65s - loss: 39.2949 - MinusLogProbMetric: 39.2949 - val_loss: 39.7473 - val_MinusLogProbMetric: 39.7473 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 476/1000
2023-10-26 18:24:42.924 
Epoch 476/1000 
	 loss: 39.3472, MinusLogProbMetric: 39.3472, val_loss: 39.6451, val_MinusLogProbMetric: 39.6451

Epoch 476: val_loss improved from 39.70433 to 39.64507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.3472 - MinusLogProbMetric: 39.3472 - val_loss: 39.6451 - val_MinusLogProbMetric: 39.6451 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 477/1000
2023-10-26 18:25:49.860 
Epoch 477/1000 
	 loss: 40.0120, MinusLogProbMetric: 40.0120, val_loss: 39.7056, val_MinusLogProbMetric: 39.7056

Epoch 477: val_loss did not improve from 39.64507
196/196 - 66s - loss: 40.0120 - MinusLogProbMetric: 40.0120 - val_loss: 39.7056 - val_MinusLogProbMetric: 39.7056 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 478/1000
2023-10-26 18:26:55.083 
Epoch 478/1000 
	 loss: 39.2839, MinusLogProbMetric: 39.2839, val_loss: 39.6528, val_MinusLogProbMetric: 39.6528

Epoch 478: val_loss did not improve from 39.64507
196/196 - 65s - loss: 39.2839 - MinusLogProbMetric: 39.2839 - val_loss: 39.6528 - val_MinusLogProbMetric: 39.6528 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 479/1000
2023-10-26 18:28:00.854 
Epoch 479/1000 
	 loss: 39.2808, MinusLogProbMetric: 39.2808, val_loss: 39.6419, val_MinusLogProbMetric: 39.6419

Epoch 479: val_loss improved from 39.64507 to 39.64194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.2808 - MinusLogProbMetric: 39.2808 - val_loss: 39.6419 - val_MinusLogProbMetric: 39.6419 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 480/1000
2023-10-26 18:29:07.610 
Epoch 480/1000 
	 loss: 39.3818, MinusLogProbMetric: 39.3818, val_loss: 43.9808, val_MinusLogProbMetric: 43.9808

Epoch 480: val_loss did not improve from 39.64194
196/196 - 66s - loss: 39.3818 - MinusLogProbMetric: 39.3818 - val_loss: 43.9808 - val_MinusLogProbMetric: 43.9808 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 481/1000
2023-10-26 18:30:13.470 
Epoch 481/1000 
	 loss: 39.7289, MinusLogProbMetric: 39.7289, val_loss: 39.6801, val_MinusLogProbMetric: 39.6801

Epoch 481: val_loss did not improve from 39.64194
196/196 - 66s - loss: 39.7289 - MinusLogProbMetric: 39.7289 - val_loss: 39.6801 - val_MinusLogProbMetric: 39.6801 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 482/1000
2023-10-26 18:31:19.013 
Epoch 482/1000 
	 loss: 39.2592, MinusLogProbMetric: 39.2592, val_loss: 39.6228, val_MinusLogProbMetric: 39.6228

Epoch 482: val_loss improved from 39.64194 to 39.62277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.2592 - MinusLogProbMetric: 39.2592 - val_loss: 39.6228 - val_MinusLogProbMetric: 39.6228 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 483/1000
2023-10-26 18:32:25.724 
Epoch 483/1000 
	 loss: 39.2227, MinusLogProbMetric: 39.2227, val_loss: 39.6493, val_MinusLogProbMetric: 39.6493

Epoch 483: val_loss did not improve from 39.62277
196/196 - 66s - loss: 39.2227 - MinusLogProbMetric: 39.2227 - val_loss: 39.6493 - val_MinusLogProbMetric: 39.6493 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 484/1000
2023-10-26 18:33:30.833 
Epoch 484/1000 
	 loss: 39.2204, MinusLogProbMetric: 39.2204, val_loss: 39.7257, val_MinusLogProbMetric: 39.7257

Epoch 484: val_loss did not improve from 39.62277
196/196 - 65s - loss: 39.2204 - MinusLogProbMetric: 39.2204 - val_loss: 39.7257 - val_MinusLogProbMetric: 39.7257 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 485/1000
2023-10-26 18:34:37.263 
Epoch 485/1000 
	 loss: 39.6067, MinusLogProbMetric: 39.6067, val_loss: 39.6090, val_MinusLogProbMetric: 39.6090

Epoch 485: val_loss improved from 39.62277 to 39.60901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.6067 - MinusLogProbMetric: 39.6067 - val_loss: 39.6090 - val_MinusLogProbMetric: 39.6090 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 486/1000
2023-10-26 18:35:44.740 
Epoch 486/1000 
	 loss: 39.2232, MinusLogProbMetric: 39.2232, val_loss: 39.5965, val_MinusLogProbMetric: 39.5965

Epoch 486: val_loss improved from 39.60901 to 39.59651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.2232 - MinusLogProbMetric: 39.2232 - val_loss: 39.5965 - val_MinusLogProbMetric: 39.5965 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 487/1000
2023-10-26 18:36:51.691 
Epoch 487/1000 
	 loss: 39.2180, MinusLogProbMetric: 39.2180, val_loss: 45.2315, val_MinusLogProbMetric: 45.2315

Epoch 487: val_loss did not improve from 39.59651
196/196 - 66s - loss: 39.2180 - MinusLogProbMetric: 39.2180 - val_loss: 45.2315 - val_MinusLogProbMetric: 45.2315 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 488/1000
2023-10-26 18:37:57.229 
Epoch 488/1000 
	 loss: 39.9008, MinusLogProbMetric: 39.9008, val_loss: 39.6759, val_MinusLogProbMetric: 39.6759

Epoch 488: val_loss did not improve from 39.59651
196/196 - 66s - loss: 39.9008 - MinusLogProbMetric: 39.9008 - val_loss: 39.6759 - val_MinusLogProbMetric: 39.6759 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 489/1000
2023-10-26 18:39:03.125 
Epoch 489/1000 
	 loss: 39.2036, MinusLogProbMetric: 39.2036, val_loss: 39.5602, val_MinusLogProbMetric: 39.5602

Epoch 489: val_loss improved from 39.59651 to 39.56015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.2036 - MinusLogProbMetric: 39.2036 - val_loss: 39.5602 - val_MinusLogProbMetric: 39.5602 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 490/1000
2023-10-26 18:40:09.494 
Epoch 490/1000 
	 loss: 39.1757, MinusLogProbMetric: 39.1757, val_loss: 39.7040, val_MinusLogProbMetric: 39.7040

Epoch 490: val_loss did not improve from 39.56015
196/196 - 65s - loss: 39.1757 - MinusLogProbMetric: 39.1757 - val_loss: 39.7040 - val_MinusLogProbMetric: 39.7040 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 491/1000
2023-10-26 18:41:14.936 
Epoch 491/1000 
	 loss: 39.2688, MinusLogProbMetric: 39.2688, val_loss: 39.5658, val_MinusLogProbMetric: 39.5658

Epoch 491: val_loss did not improve from 39.56015
196/196 - 65s - loss: 39.2688 - MinusLogProbMetric: 39.2688 - val_loss: 39.5658 - val_MinusLogProbMetric: 39.5658 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 492/1000
2023-10-26 18:42:20.317 
Epoch 492/1000 
	 loss: 39.1641, MinusLogProbMetric: 39.1641, val_loss: 39.5606, val_MinusLogProbMetric: 39.5606

Epoch 492: val_loss did not improve from 39.56015
196/196 - 65s - loss: 39.1641 - MinusLogProbMetric: 39.1641 - val_loss: 39.5606 - val_MinusLogProbMetric: 39.5606 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 493/1000
2023-10-26 18:43:26.141 
Epoch 493/1000 
	 loss: 39.1530, MinusLogProbMetric: 39.1530, val_loss: 39.5332, val_MinusLogProbMetric: 39.5332

Epoch 493: val_loss improved from 39.56015 to 39.53321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.1530 - MinusLogProbMetric: 39.1530 - val_loss: 39.5332 - val_MinusLogProbMetric: 39.5332 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 494/1000
2023-10-26 18:44:32.772 
Epoch 494/1000 
	 loss: 39.2511, MinusLogProbMetric: 39.2511, val_loss: 39.5800, val_MinusLogProbMetric: 39.5800

Epoch 494: val_loss did not improve from 39.53321
196/196 - 65s - loss: 39.2511 - MinusLogProbMetric: 39.2511 - val_loss: 39.5800 - val_MinusLogProbMetric: 39.5800 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 495/1000
2023-10-26 18:45:37.827 
Epoch 495/1000 
	 loss: 39.1474, MinusLogProbMetric: 39.1474, val_loss: 39.6035, val_MinusLogProbMetric: 39.6035

Epoch 495: val_loss did not improve from 39.53321
196/196 - 65s - loss: 39.1474 - MinusLogProbMetric: 39.1474 - val_loss: 39.6035 - val_MinusLogProbMetric: 39.6035 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 496/1000
2023-10-26 18:46:43.648 
Epoch 496/1000 
	 loss: 39.4484, MinusLogProbMetric: 39.4484, val_loss: 39.5654, val_MinusLogProbMetric: 39.5654

Epoch 496: val_loss did not improve from 39.53321
196/196 - 66s - loss: 39.4484 - MinusLogProbMetric: 39.4484 - val_loss: 39.5654 - val_MinusLogProbMetric: 39.5654 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 497/1000
2023-10-26 18:47:49.312 
Epoch 497/1000 
	 loss: 39.1483, MinusLogProbMetric: 39.1483, val_loss: 39.4929, val_MinusLogProbMetric: 39.4929

Epoch 497: val_loss improved from 39.53321 to 39.49292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.1483 - MinusLogProbMetric: 39.1483 - val_loss: 39.4929 - val_MinusLogProbMetric: 39.4929 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 498/1000
2023-10-26 18:48:56.384 
Epoch 498/1000 
	 loss: 39.1163, MinusLogProbMetric: 39.1163, val_loss: 39.5051, val_MinusLogProbMetric: 39.5051

Epoch 498: val_loss did not improve from 39.49292
196/196 - 66s - loss: 39.1163 - MinusLogProbMetric: 39.1163 - val_loss: 39.5051 - val_MinusLogProbMetric: 39.5051 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 499/1000
2023-10-26 18:50:02.441 
Epoch 499/1000 
	 loss: 39.5104, MinusLogProbMetric: 39.5104, val_loss: 40.0919, val_MinusLogProbMetric: 40.0919

Epoch 499: val_loss did not improve from 39.49292
196/196 - 66s - loss: 39.5104 - MinusLogProbMetric: 39.5104 - val_loss: 40.0919 - val_MinusLogProbMetric: 40.0919 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 500/1000
2023-10-26 18:51:08.390 
Epoch 500/1000 
	 loss: 39.2358, MinusLogProbMetric: 39.2358, val_loss: 39.5134, val_MinusLogProbMetric: 39.5134

Epoch 500: val_loss did not improve from 39.49292
196/196 - 66s - loss: 39.2358 - MinusLogProbMetric: 39.2358 - val_loss: 39.5134 - val_MinusLogProbMetric: 39.5134 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 501/1000
2023-10-26 18:52:15.303 
Epoch 501/1000 
	 loss: 39.1538, MinusLogProbMetric: 39.1538, val_loss: 39.5878, val_MinusLogProbMetric: 39.5878

Epoch 501: val_loss did not improve from 39.49292
196/196 - 67s - loss: 39.1538 - MinusLogProbMetric: 39.1538 - val_loss: 39.5878 - val_MinusLogProbMetric: 39.5878 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 502/1000
2023-10-26 18:53:21.638 
Epoch 502/1000 
	 loss: 39.1301, MinusLogProbMetric: 39.1301, val_loss: 39.4836, val_MinusLogProbMetric: 39.4836

Epoch 502: val_loss improved from 39.49292 to 39.48360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.1301 - MinusLogProbMetric: 39.1301 - val_loss: 39.4836 - val_MinusLogProbMetric: 39.4836 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 503/1000
2023-10-26 18:54:28.802 
Epoch 503/1000 
	 loss: 39.1029, MinusLogProbMetric: 39.1029, val_loss: 39.4456, val_MinusLogProbMetric: 39.4456

Epoch 503: val_loss improved from 39.48360 to 39.44560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.1029 - MinusLogProbMetric: 39.1029 - val_loss: 39.4456 - val_MinusLogProbMetric: 39.4456 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 504/1000
2023-10-26 18:55:36.032 
Epoch 504/1000 
	 loss: 39.1042, MinusLogProbMetric: 39.1042, val_loss: 39.5192, val_MinusLogProbMetric: 39.5192

Epoch 504: val_loss did not improve from 39.44560
196/196 - 66s - loss: 39.1042 - MinusLogProbMetric: 39.1042 - val_loss: 39.5192 - val_MinusLogProbMetric: 39.5192 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 505/1000
2023-10-26 18:56:41.813 
Epoch 505/1000 
	 loss: 39.1121, MinusLogProbMetric: 39.1121, val_loss: 39.4692, val_MinusLogProbMetric: 39.4692

Epoch 505: val_loss did not improve from 39.44560
196/196 - 66s - loss: 39.1121 - MinusLogProbMetric: 39.1121 - val_loss: 39.4692 - val_MinusLogProbMetric: 39.4692 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 506/1000
2023-10-26 18:57:48.334 
Epoch 506/1000 
	 loss: 39.0929, MinusLogProbMetric: 39.0929, val_loss: 39.5739, val_MinusLogProbMetric: 39.5739

Epoch 506: val_loss did not improve from 39.44560
196/196 - 67s - loss: 39.0929 - MinusLogProbMetric: 39.0929 - val_loss: 39.5739 - val_MinusLogProbMetric: 39.5739 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 507/1000
2023-10-26 18:58:54.273 
Epoch 507/1000 
	 loss: 39.0943, MinusLogProbMetric: 39.0943, val_loss: 39.5612, val_MinusLogProbMetric: 39.5612

Epoch 507: val_loss did not improve from 39.44560
196/196 - 66s - loss: 39.0943 - MinusLogProbMetric: 39.0943 - val_loss: 39.5612 - val_MinusLogProbMetric: 39.5612 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 508/1000
2023-10-26 19:00:00.048 
Epoch 508/1000 
	 loss: 39.0855, MinusLogProbMetric: 39.0855, val_loss: 39.4191, val_MinusLogProbMetric: 39.4191

Epoch 508: val_loss improved from 39.44560 to 39.41906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.0855 - MinusLogProbMetric: 39.0855 - val_loss: 39.4191 - val_MinusLogProbMetric: 39.4191 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 509/1000
2023-10-26 19:01:06.317 
Epoch 509/1000 
	 loss: 39.3071, MinusLogProbMetric: 39.3071, val_loss: 39.4977, val_MinusLogProbMetric: 39.4977

Epoch 509: val_loss did not improve from 39.41906
196/196 - 65s - loss: 39.3071 - MinusLogProbMetric: 39.3071 - val_loss: 39.4977 - val_MinusLogProbMetric: 39.4977 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 510/1000
2023-10-26 19:02:12.497 
Epoch 510/1000 
	 loss: 39.0514, MinusLogProbMetric: 39.0514, val_loss: 39.4709, val_MinusLogProbMetric: 39.4709

Epoch 510: val_loss did not improve from 39.41906
196/196 - 66s - loss: 39.0514 - MinusLogProbMetric: 39.0514 - val_loss: 39.4709 - val_MinusLogProbMetric: 39.4709 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 511/1000
2023-10-26 19:03:18.163 
Epoch 511/1000 
	 loss: 39.0478, MinusLogProbMetric: 39.0478, val_loss: 39.4654, val_MinusLogProbMetric: 39.4654

Epoch 511: val_loss did not improve from 39.41906
196/196 - 66s - loss: 39.0478 - MinusLogProbMetric: 39.0478 - val_loss: 39.4654 - val_MinusLogProbMetric: 39.4654 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 512/1000
2023-10-26 19:04:23.552 
Epoch 512/1000 
	 loss: 39.0426, MinusLogProbMetric: 39.0426, val_loss: 39.3866, val_MinusLogProbMetric: 39.3866

Epoch 512: val_loss improved from 39.41906 to 39.38655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.0426 - MinusLogProbMetric: 39.0426 - val_loss: 39.3866 - val_MinusLogProbMetric: 39.3866 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 513/1000
2023-10-26 19:05:29.460 
Epoch 513/1000 
	 loss: 39.0295, MinusLogProbMetric: 39.0295, val_loss: 39.4475, val_MinusLogProbMetric: 39.4475

Epoch 513: val_loss did not improve from 39.38655
196/196 - 65s - loss: 39.0295 - MinusLogProbMetric: 39.0295 - val_loss: 39.4475 - val_MinusLogProbMetric: 39.4475 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 514/1000
2023-10-26 19:06:33.411 
Epoch 514/1000 
	 loss: 39.0644, MinusLogProbMetric: 39.0644, val_loss: 39.3751, val_MinusLogProbMetric: 39.3751

Epoch 514: val_loss improved from 39.38655 to 39.37510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 39.0644 - MinusLogProbMetric: 39.0644 - val_loss: 39.3751 - val_MinusLogProbMetric: 39.3751 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 515/1000
2023-10-26 19:07:37.156 
Epoch 515/1000 
	 loss: 38.9981, MinusLogProbMetric: 38.9981, val_loss: 39.4565, val_MinusLogProbMetric: 39.4565

Epoch 515: val_loss did not improve from 39.37510
196/196 - 63s - loss: 38.9981 - MinusLogProbMetric: 38.9981 - val_loss: 39.4565 - val_MinusLogProbMetric: 39.4565 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 516/1000
2023-10-26 19:08:38.308 
Epoch 516/1000 
	 loss: 39.0566, MinusLogProbMetric: 39.0566, val_loss: 39.3697, val_MinusLogProbMetric: 39.3697

Epoch 516: val_loss improved from 39.37510 to 39.36968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 39.0566 - MinusLogProbMetric: 39.0566 - val_loss: 39.3697 - val_MinusLogProbMetric: 39.3697 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 517/1000
2023-10-26 19:09:42.143 
Epoch 517/1000 
	 loss: 38.9962, MinusLogProbMetric: 38.9962, val_loss: 39.4335, val_MinusLogProbMetric: 39.4335

Epoch 517: val_loss did not improve from 39.36968
196/196 - 63s - loss: 38.9962 - MinusLogProbMetric: 38.9962 - val_loss: 39.4335 - val_MinusLogProbMetric: 39.4335 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 518/1000
2023-10-26 19:10:47.289 
Epoch 518/1000 
	 loss: 38.9922, MinusLogProbMetric: 38.9922, val_loss: 39.4370, val_MinusLogProbMetric: 39.4370

Epoch 518: val_loss did not improve from 39.36968
196/196 - 65s - loss: 38.9922 - MinusLogProbMetric: 38.9922 - val_loss: 39.4370 - val_MinusLogProbMetric: 39.4370 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 519/1000
2023-10-26 19:11:51.719 
Epoch 519/1000 
	 loss: 39.0256, MinusLogProbMetric: 39.0256, val_loss: 39.4020, val_MinusLogProbMetric: 39.4020

Epoch 519: val_loss did not improve from 39.36968
196/196 - 64s - loss: 39.0256 - MinusLogProbMetric: 39.0256 - val_loss: 39.4020 - val_MinusLogProbMetric: 39.4020 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 520/1000
2023-10-26 19:12:57.668 
Epoch 520/1000 
	 loss: 39.1570, MinusLogProbMetric: 39.1570, val_loss: 39.3616, val_MinusLogProbMetric: 39.3616

Epoch 520: val_loss improved from 39.36968 to 39.36159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.1570 - MinusLogProbMetric: 39.1570 - val_loss: 39.3616 - val_MinusLogProbMetric: 39.3616 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 521/1000
2023-10-26 19:14:03.783 
Epoch 521/1000 
	 loss: 39.4460, MinusLogProbMetric: 39.4460, val_loss: 39.5185, val_MinusLogProbMetric: 39.5185

Epoch 521: val_loss did not improve from 39.36159
196/196 - 65s - loss: 39.4460 - MinusLogProbMetric: 39.4460 - val_loss: 39.5185 - val_MinusLogProbMetric: 39.5185 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 522/1000
2023-10-26 19:15:09.392 
Epoch 522/1000 
	 loss: 39.0335, MinusLogProbMetric: 39.0335, val_loss: 39.3565, val_MinusLogProbMetric: 39.3565

Epoch 522: val_loss improved from 39.36159 to 39.35647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 39.0335 - MinusLogProbMetric: 39.0335 - val_loss: 39.3565 - val_MinusLogProbMetric: 39.3565 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 523/1000
2023-10-26 19:16:15.341 
Epoch 523/1000 
	 loss: 38.9793, MinusLogProbMetric: 38.9793, val_loss: 39.4013, val_MinusLogProbMetric: 39.4013

Epoch 523: val_loss did not improve from 39.35647
196/196 - 65s - loss: 38.9793 - MinusLogProbMetric: 38.9793 - val_loss: 39.4013 - val_MinusLogProbMetric: 39.4013 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 524/1000
2023-10-26 19:17:20.433 
Epoch 524/1000 
	 loss: 38.9597, MinusLogProbMetric: 38.9597, val_loss: 39.3302, val_MinusLogProbMetric: 39.3302

Epoch 524: val_loss improved from 39.35647 to 39.33015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.9597 - MinusLogProbMetric: 38.9597 - val_loss: 39.3302 - val_MinusLogProbMetric: 39.3302 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 525/1000
2023-10-26 19:18:26.523 
Epoch 525/1000 
	 loss: 38.9696, MinusLogProbMetric: 38.9696, val_loss: 39.2911, val_MinusLogProbMetric: 39.2911

Epoch 525: val_loss improved from 39.33015 to 39.29112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.9696 - MinusLogProbMetric: 38.9696 - val_loss: 39.2911 - val_MinusLogProbMetric: 39.2911 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 526/1000
2023-10-26 19:19:32.339 
Epoch 526/1000 
	 loss: 39.2963, MinusLogProbMetric: 39.2963, val_loss: 39.9532, val_MinusLogProbMetric: 39.9532

Epoch 526: val_loss did not improve from 39.29112
196/196 - 65s - loss: 39.2963 - MinusLogProbMetric: 39.2963 - val_loss: 39.9532 - val_MinusLogProbMetric: 39.9532 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 527/1000
2023-10-26 19:20:37.605 
Epoch 527/1000 
	 loss: 39.0822, MinusLogProbMetric: 39.0822, val_loss: 39.3154, val_MinusLogProbMetric: 39.3154

Epoch 527: val_loss did not improve from 39.29112
196/196 - 65s - loss: 39.0822 - MinusLogProbMetric: 39.0822 - val_loss: 39.3154 - val_MinusLogProbMetric: 39.3154 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 528/1000
2023-10-26 19:21:42.643 
Epoch 528/1000 
	 loss: 38.9694, MinusLogProbMetric: 38.9694, val_loss: 39.3961, val_MinusLogProbMetric: 39.3961

Epoch 528: val_loss did not improve from 39.29112
196/196 - 65s - loss: 38.9694 - MinusLogProbMetric: 38.9694 - val_loss: 39.3961 - val_MinusLogProbMetric: 39.3961 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 529/1000
2023-10-26 19:22:47.954 
Epoch 529/1000 
	 loss: 38.9287, MinusLogProbMetric: 38.9287, val_loss: 39.4059, val_MinusLogProbMetric: 39.4059

Epoch 529: val_loss did not improve from 39.29112
196/196 - 65s - loss: 38.9287 - MinusLogProbMetric: 38.9287 - val_loss: 39.4059 - val_MinusLogProbMetric: 39.4059 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 530/1000
2023-10-26 19:23:52.676 
Epoch 530/1000 
	 loss: 38.9765, MinusLogProbMetric: 38.9765, val_loss: 39.3294, val_MinusLogProbMetric: 39.3294

Epoch 530: val_loss did not improve from 39.29112
196/196 - 65s - loss: 38.9765 - MinusLogProbMetric: 38.9765 - val_loss: 39.3294 - val_MinusLogProbMetric: 39.3294 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 531/1000
2023-10-26 19:24:57.190 
Epoch 531/1000 
	 loss: 38.9283, MinusLogProbMetric: 38.9283, val_loss: 39.3585, val_MinusLogProbMetric: 39.3585

Epoch 531: val_loss did not improve from 39.29112
196/196 - 65s - loss: 38.9283 - MinusLogProbMetric: 38.9283 - val_loss: 39.3585 - val_MinusLogProbMetric: 39.3585 - lr: 4.5725e-07 - 65s/epoch - 329ms/step
Epoch 532/1000
2023-10-26 19:26:02.005 
Epoch 532/1000 
	 loss: 39.0131, MinusLogProbMetric: 39.0131, val_loss: 39.2843, val_MinusLogProbMetric: 39.2843

Epoch 532: val_loss improved from 39.29112 to 39.28426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 39.0131 - MinusLogProbMetric: 39.0131 - val_loss: 39.2843 - val_MinusLogProbMetric: 39.2843 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 533/1000
2023-10-26 19:27:07.942 
Epoch 533/1000 
	 loss: 38.9484, MinusLogProbMetric: 38.9484, val_loss: 39.2850, val_MinusLogProbMetric: 39.2850

Epoch 533: val_loss did not improve from 39.28426
196/196 - 65s - loss: 38.9484 - MinusLogProbMetric: 38.9484 - val_loss: 39.2850 - val_MinusLogProbMetric: 39.2850 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 534/1000
2023-10-26 19:28:13.599 
Epoch 534/1000 
	 loss: 38.8997, MinusLogProbMetric: 38.8997, val_loss: 39.3476, val_MinusLogProbMetric: 39.3476

Epoch 534: val_loss did not improve from 39.28426
196/196 - 66s - loss: 38.8997 - MinusLogProbMetric: 38.8997 - val_loss: 39.3476 - val_MinusLogProbMetric: 39.3476 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 535/1000
2023-10-26 19:29:19.103 
Epoch 535/1000 
	 loss: 38.9185, MinusLogProbMetric: 38.9185, val_loss: 39.3127, val_MinusLogProbMetric: 39.3127

Epoch 535: val_loss did not improve from 39.28426
196/196 - 65s - loss: 38.9185 - MinusLogProbMetric: 38.9185 - val_loss: 39.3127 - val_MinusLogProbMetric: 39.3127 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 536/1000
2023-10-26 19:30:20.341 
Epoch 536/1000 
	 loss: 39.3559, MinusLogProbMetric: 39.3559, val_loss: 39.5678, val_MinusLogProbMetric: 39.5678

Epoch 536: val_loss did not improve from 39.28426
196/196 - 61s - loss: 39.3559 - MinusLogProbMetric: 39.3559 - val_loss: 39.5678 - val_MinusLogProbMetric: 39.5678 - lr: 4.5725e-07 - 61s/epoch - 312ms/step
Epoch 537/1000
2023-10-26 19:31:21.832 
Epoch 537/1000 
	 loss: 38.9562, MinusLogProbMetric: 38.9562, val_loss: 39.2337, val_MinusLogProbMetric: 39.2337

Epoch 537: val_loss improved from 39.28426 to 39.23375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 38.9562 - MinusLogProbMetric: 38.9562 - val_loss: 39.2337 - val_MinusLogProbMetric: 39.2337 - lr: 4.5725e-07 - 62s/epoch - 319ms/step
Epoch 538/1000
2023-10-26 19:32:27.132 
Epoch 538/1000 
	 loss: 38.8968, MinusLogProbMetric: 38.8968, val_loss: 39.2256, val_MinusLogProbMetric: 39.2256

Epoch 538: val_loss improved from 39.23375 to 39.22558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 65s - loss: 38.8968 - MinusLogProbMetric: 38.8968 - val_loss: 39.2256 - val_MinusLogProbMetric: 39.2256 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 539/1000
2023-10-26 19:33:33.692 
Epoch 539/1000 
	 loss: 39.4438, MinusLogProbMetric: 39.4438, val_loss: 39.3011, val_MinusLogProbMetric: 39.3011

Epoch 539: val_loss did not improve from 39.22558
196/196 - 66s - loss: 39.4438 - MinusLogProbMetric: 39.4438 - val_loss: 39.3011 - val_MinusLogProbMetric: 39.3011 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 540/1000
2023-10-26 19:34:39.013 
Epoch 540/1000 
	 loss: 38.8924, MinusLogProbMetric: 38.8924, val_loss: 39.2580, val_MinusLogProbMetric: 39.2580

Epoch 540: val_loss did not improve from 39.22558
196/196 - 65s - loss: 38.8924 - MinusLogProbMetric: 38.8924 - val_loss: 39.2580 - val_MinusLogProbMetric: 39.2580 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 541/1000
2023-10-26 19:35:43.442 
Epoch 541/1000 
	 loss: 38.8940, MinusLogProbMetric: 38.8940, val_loss: 39.3330, val_MinusLogProbMetric: 39.3330

Epoch 541: val_loss did not improve from 39.22558
196/196 - 64s - loss: 38.8940 - MinusLogProbMetric: 38.8940 - val_loss: 39.3330 - val_MinusLogProbMetric: 39.3330 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 542/1000
2023-10-26 19:36:49.139 
Epoch 542/1000 
	 loss: 38.8854, MinusLogProbMetric: 38.8854, val_loss: 39.2647, val_MinusLogProbMetric: 39.2647

Epoch 542: val_loss did not improve from 39.22558
196/196 - 66s - loss: 38.8854 - MinusLogProbMetric: 38.8854 - val_loss: 39.2647 - val_MinusLogProbMetric: 39.2647 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 543/1000
2023-10-26 19:37:55.005 
Epoch 543/1000 
	 loss: 38.9171, MinusLogProbMetric: 38.9171, val_loss: 39.2449, val_MinusLogProbMetric: 39.2449

Epoch 543: val_loss did not improve from 39.22558
196/196 - 66s - loss: 38.9171 - MinusLogProbMetric: 38.9171 - val_loss: 39.2449 - val_MinusLogProbMetric: 39.2449 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 544/1000
2023-10-26 19:39:00.521 
Epoch 544/1000 
	 loss: 38.8372, MinusLogProbMetric: 38.8372, val_loss: 39.2249, val_MinusLogProbMetric: 39.2249

Epoch 544: val_loss improved from 39.22558 to 39.22488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.8372 - MinusLogProbMetric: 38.8372 - val_loss: 39.2249 - val_MinusLogProbMetric: 39.2249 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 545/1000
2023-10-26 19:40:06.705 
Epoch 545/1000 
	 loss: 38.8984, MinusLogProbMetric: 38.8984, val_loss: 39.2106, val_MinusLogProbMetric: 39.2106

Epoch 545: val_loss improved from 39.22488 to 39.21056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.8984 - MinusLogProbMetric: 38.8984 - val_loss: 39.2106 - val_MinusLogProbMetric: 39.2106 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 546/1000
2023-10-26 19:41:13.393 
Epoch 546/1000 
	 loss: 38.8413, MinusLogProbMetric: 38.8413, val_loss: 39.2123, val_MinusLogProbMetric: 39.2123

Epoch 546: val_loss did not improve from 39.21056
196/196 - 66s - loss: 38.8413 - MinusLogProbMetric: 38.8413 - val_loss: 39.2123 - val_MinusLogProbMetric: 39.2123 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 547/1000
2023-10-26 19:42:19.106 
Epoch 547/1000 
	 loss: 38.9877, MinusLogProbMetric: 38.9877, val_loss: 39.3212, val_MinusLogProbMetric: 39.3212

Epoch 547: val_loss did not improve from 39.21056
196/196 - 66s - loss: 38.9877 - MinusLogProbMetric: 38.9877 - val_loss: 39.3212 - val_MinusLogProbMetric: 39.3212 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 548/1000
2023-10-26 19:43:24.641 
Epoch 548/1000 
	 loss: 38.8745, MinusLogProbMetric: 38.8745, val_loss: 39.2073, val_MinusLogProbMetric: 39.2073

Epoch 548: val_loss improved from 39.21056 to 39.20725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.8745 - MinusLogProbMetric: 38.8745 - val_loss: 39.2073 - val_MinusLogProbMetric: 39.2073 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 549/1000
2023-10-26 19:44:31.959 
Epoch 549/1000 
	 loss: 39.3010, MinusLogProbMetric: 39.3010, val_loss: 39.2451, val_MinusLogProbMetric: 39.2451

Epoch 549: val_loss did not improve from 39.20725
196/196 - 66s - loss: 39.3010 - MinusLogProbMetric: 39.3010 - val_loss: 39.2451 - val_MinusLogProbMetric: 39.2451 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 550/1000
2023-10-26 19:45:37.864 
Epoch 550/1000 
	 loss: 38.8303, MinusLogProbMetric: 38.8303, val_loss: 39.1923, val_MinusLogProbMetric: 39.1923

Epoch 550: val_loss improved from 39.20725 to 39.19234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.8303 - MinusLogProbMetric: 38.8303 - val_loss: 39.1923 - val_MinusLogProbMetric: 39.1923 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 551/1000
2023-10-26 19:46:44.808 
Epoch 551/1000 
	 loss: 38.8920, MinusLogProbMetric: 38.8920, val_loss: 39.3561, val_MinusLogProbMetric: 39.3561

Epoch 551: val_loss did not improve from 39.19234
196/196 - 66s - loss: 38.8920 - MinusLogProbMetric: 38.8920 - val_loss: 39.3561 - val_MinusLogProbMetric: 39.3561 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 552/1000
2023-10-26 19:47:49.950 
Epoch 552/1000 
	 loss: 38.8534, MinusLogProbMetric: 38.8534, val_loss: 39.2007, val_MinusLogProbMetric: 39.2007

Epoch 552: val_loss did not improve from 39.19234
196/196 - 65s - loss: 38.8534 - MinusLogProbMetric: 38.8534 - val_loss: 39.2007 - val_MinusLogProbMetric: 39.2007 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 553/1000
2023-10-26 19:48:55.257 
Epoch 553/1000 
	 loss: 38.8344, MinusLogProbMetric: 38.8344, val_loss: 39.1763, val_MinusLogProbMetric: 39.1763

Epoch 553: val_loss improved from 39.19234 to 39.17633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.8344 - MinusLogProbMetric: 38.8344 - val_loss: 39.1763 - val_MinusLogProbMetric: 39.1763 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 554/1000
2023-10-26 19:50:01.907 
Epoch 554/1000 
	 loss: 39.1997, MinusLogProbMetric: 39.1997, val_loss: 39.3816, val_MinusLogProbMetric: 39.3816

Epoch 554: val_loss did not improve from 39.17633
196/196 - 66s - loss: 39.1997 - MinusLogProbMetric: 39.1997 - val_loss: 39.3816 - val_MinusLogProbMetric: 39.3816 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 555/1000
2023-10-26 19:51:07.226 
Epoch 555/1000 
	 loss: 38.8398, MinusLogProbMetric: 38.8398, val_loss: 39.1881, val_MinusLogProbMetric: 39.1881

Epoch 555: val_loss did not improve from 39.17633
196/196 - 65s - loss: 38.8398 - MinusLogProbMetric: 38.8398 - val_loss: 39.1881 - val_MinusLogProbMetric: 39.1881 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 556/1000
2023-10-26 19:52:12.763 
Epoch 556/1000 
	 loss: 38.7851, MinusLogProbMetric: 38.7851, val_loss: 39.1478, val_MinusLogProbMetric: 39.1478

Epoch 556: val_loss improved from 39.17633 to 39.14779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.7851 - MinusLogProbMetric: 38.7851 - val_loss: 39.1478 - val_MinusLogProbMetric: 39.1478 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 557/1000
2023-10-26 19:53:19.231 
Epoch 557/1000 
	 loss: 38.7658, MinusLogProbMetric: 38.7658, val_loss: 39.1434, val_MinusLogProbMetric: 39.1434

Epoch 557: val_loss improved from 39.14779 to 39.14340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.7658 - MinusLogProbMetric: 38.7658 - val_loss: 39.1434 - val_MinusLogProbMetric: 39.1434 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 558/1000
2023-10-26 19:54:25.730 
Epoch 558/1000 
	 loss: 38.8384, MinusLogProbMetric: 38.8384, val_loss: 39.1843, val_MinusLogProbMetric: 39.1843

Epoch 558: val_loss did not improve from 39.14340
196/196 - 65s - loss: 38.8384 - MinusLogProbMetric: 38.8384 - val_loss: 39.1843 - val_MinusLogProbMetric: 39.1843 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 559/1000
2023-10-26 19:55:31.891 
Epoch 559/1000 
	 loss: 38.7589, MinusLogProbMetric: 38.7589, val_loss: 39.4059, val_MinusLogProbMetric: 39.4059

Epoch 559: val_loss did not improve from 39.14340
196/196 - 66s - loss: 38.7589 - MinusLogProbMetric: 38.7589 - val_loss: 39.4059 - val_MinusLogProbMetric: 39.4059 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 560/1000
2023-10-26 19:56:37.442 
Epoch 560/1000 
	 loss: 38.9934, MinusLogProbMetric: 38.9934, val_loss: 39.1210, val_MinusLogProbMetric: 39.1210

Epoch 560: val_loss improved from 39.14340 to 39.12099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.9934 - MinusLogProbMetric: 38.9934 - val_loss: 39.1210 - val_MinusLogProbMetric: 39.1210 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 561/1000
2023-10-26 19:57:44.356 
Epoch 561/1000 
	 loss: 38.7604, MinusLogProbMetric: 38.7604, val_loss: 39.2301, val_MinusLogProbMetric: 39.2301

Epoch 561: val_loss did not improve from 39.12099
196/196 - 66s - loss: 38.7604 - MinusLogProbMetric: 38.7604 - val_loss: 39.2301 - val_MinusLogProbMetric: 39.2301 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 562/1000
2023-10-26 19:58:51.016 
Epoch 562/1000 
	 loss: 38.7848, MinusLogProbMetric: 38.7848, val_loss: 39.1773, val_MinusLogProbMetric: 39.1773

Epoch 562: val_loss did not improve from 39.12099
196/196 - 67s - loss: 38.7848 - MinusLogProbMetric: 38.7848 - val_loss: 39.1773 - val_MinusLogProbMetric: 39.1773 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 563/1000
2023-10-26 19:59:56.066 
Epoch 563/1000 
	 loss: 38.9767, MinusLogProbMetric: 38.9767, val_loss: 39.1047, val_MinusLogProbMetric: 39.1047

Epoch 563: val_loss improved from 39.12099 to 39.10471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.9767 - MinusLogProbMetric: 38.9767 - val_loss: 39.1047 - val_MinusLogProbMetric: 39.1047 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 564/1000
2023-10-26 20:01:03.292 
Epoch 564/1000 
	 loss: 38.7379, MinusLogProbMetric: 38.7379, val_loss: 39.1402, val_MinusLogProbMetric: 39.1402

Epoch 564: val_loss did not improve from 39.10471
196/196 - 66s - loss: 38.7379 - MinusLogProbMetric: 38.7379 - val_loss: 39.1402 - val_MinusLogProbMetric: 39.1402 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 565/1000
2023-10-26 20:02:08.623 
Epoch 565/1000 
	 loss: 38.7064, MinusLogProbMetric: 38.7064, val_loss: 39.1366, val_MinusLogProbMetric: 39.1366

Epoch 565: val_loss did not improve from 39.10471
196/196 - 65s - loss: 38.7064 - MinusLogProbMetric: 38.7064 - val_loss: 39.1366 - val_MinusLogProbMetric: 39.1366 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 566/1000
2023-10-26 20:03:14.414 
Epoch 566/1000 
	 loss: 38.7018, MinusLogProbMetric: 38.7018, val_loss: 39.0754, val_MinusLogProbMetric: 39.0754

Epoch 566: val_loss improved from 39.10471 to 39.07541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.7018 - MinusLogProbMetric: 38.7018 - val_loss: 39.0754 - val_MinusLogProbMetric: 39.0754 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 567/1000
2023-10-26 20:04:21.242 
Epoch 567/1000 
	 loss: 38.7331, MinusLogProbMetric: 38.7331, val_loss: 39.1260, val_MinusLogProbMetric: 39.1260

Epoch 567: val_loss did not improve from 39.07541
196/196 - 66s - loss: 38.7331 - MinusLogProbMetric: 38.7331 - val_loss: 39.1260 - val_MinusLogProbMetric: 39.1260 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 568/1000
2023-10-26 20:05:26.881 
Epoch 568/1000 
	 loss: 39.0475, MinusLogProbMetric: 39.0475, val_loss: 39.0955, val_MinusLogProbMetric: 39.0955

Epoch 568: val_loss did not improve from 39.07541
196/196 - 66s - loss: 39.0475 - MinusLogProbMetric: 39.0475 - val_loss: 39.0955 - val_MinusLogProbMetric: 39.0955 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 569/1000
2023-10-26 20:06:32.315 
Epoch 569/1000 
	 loss: 38.6940, MinusLogProbMetric: 38.6940, val_loss: 39.1602, val_MinusLogProbMetric: 39.1602

Epoch 569: val_loss did not improve from 39.07541
196/196 - 65s - loss: 38.6940 - MinusLogProbMetric: 38.6940 - val_loss: 39.1602 - val_MinusLogProbMetric: 39.1602 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 570/1000
2023-10-26 20:07:38.303 
Epoch 570/1000 
	 loss: 38.7143, MinusLogProbMetric: 38.7143, val_loss: 39.1276, val_MinusLogProbMetric: 39.1276

Epoch 570: val_loss did not improve from 39.07541
196/196 - 66s - loss: 38.7143 - MinusLogProbMetric: 38.7143 - val_loss: 39.1276 - val_MinusLogProbMetric: 39.1276 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 571/1000
2023-10-26 20:08:44.059 
Epoch 571/1000 
	 loss: 38.7039, MinusLogProbMetric: 38.7039, val_loss: 39.1275, val_MinusLogProbMetric: 39.1275

Epoch 571: val_loss did not improve from 39.07541
196/196 - 66s - loss: 38.7039 - MinusLogProbMetric: 38.7039 - val_loss: 39.1275 - val_MinusLogProbMetric: 39.1275 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 572/1000
2023-10-26 20:09:50.525 
Epoch 572/1000 
	 loss: 38.7102, MinusLogProbMetric: 38.7102, val_loss: 39.2001, val_MinusLogProbMetric: 39.2001

Epoch 572: val_loss did not improve from 39.07541
196/196 - 66s - loss: 38.7102 - MinusLogProbMetric: 38.7102 - val_loss: 39.2001 - val_MinusLogProbMetric: 39.2001 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 573/1000
2023-10-26 20:10:56.825 
Epoch 573/1000 
	 loss: 39.2088, MinusLogProbMetric: 39.2088, val_loss: 39.1529, val_MinusLogProbMetric: 39.1529

Epoch 573: val_loss did not improve from 39.07541
196/196 - 66s - loss: 39.2088 - MinusLogProbMetric: 39.2088 - val_loss: 39.1529 - val_MinusLogProbMetric: 39.1529 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 574/1000
2023-10-26 20:12:01.405 
Epoch 574/1000 
	 loss: 38.7056, MinusLogProbMetric: 38.7056, val_loss: 39.0604, val_MinusLogProbMetric: 39.0604

Epoch 574: val_loss improved from 39.07541 to 39.06044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.7056 - MinusLogProbMetric: 38.7056 - val_loss: 39.0604 - val_MinusLogProbMetric: 39.0604 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 575/1000
2023-10-26 20:13:08.149 
Epoch 575/1000 
	 loss: 38.6677, MinusLogProbMetric: 38.6677, val_loss: 39.1033, val_MinusLogProbMetric: 39.1033

Epoch 575: val_loss did not improve from 39.06044
196/196 - 66s - loss: 38.6677 - MinusLogProbMetric: 38.6677 - val_loss: 39.1033 - val_MinusLogProbMetric: 39.1033 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 576/1000
2023-10-26 20:14:14.037 
Epoch 576/1000 
	 loss: 38.6633, MinusLogProbMetric: 38.6633, val_loss: 39.1000, val_MinusLogProbMetric: 39.1000

Epoch 576: val_loss did not improve from 39.06044
196/196 - 66s - loss: 38.6633 - MinusLogProbMetric: 38.6633 - val_loss: 39.1000 - val_MinusLogProbMetric: 39.1000 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 577/1000
2023-10-26 20:15:20.154 
Epoch 577/1000 
	 loss: 38.6618, MinusLogProbMetric: 38.6618, val_loss: 39.0609, val_MinusLogProbMetric: 39.0609

Epoch 577: val_loss did not improve from 39.06044
196/196 - 66s - loss: 38.6618 - MinusLogProbMetric: 38.6618 - val_loss: 39.0609 - val_MinusLogProbMetric: 39.0609 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 578/1000
2023-10-26 20:16:26.604 
Epoch 578/1000 
	 loss: 38.6605, MinusLogProbMetric: 38.6605, val_loss: 39.0536, val_MinusLogProbMetric: 39.0536

Epoch 578: val_loss improved from 39.06044 to 39.05364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.6605 - MinusLogProbMetric: 38.6605 - val_loss: 39.0536 - val_MinusLogProbMetric: 39.0536 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 579/1000
2023-10-26 20:17:33.284 
Epoch 579/1000 
	 loss: 38.7038, MinusLogProbMetric: 38.7038, val_loss: 39.2339, val_MinusLogProbMetric: 39.2339

Epoch 579: val_loss did not improve from 39.05364
196/196 - 66s - loss: 38.7038 - MinusLogProbMetric: 38.7038 - val_loss: 39.2339 - val_MinusLogProbMetric: 39.2339 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 580/1000
2023-10-26 20:18:38.902 
Epoch 580/1000 
	 loss: 38.6970, MinusLogProbMetric: 38.6970, val_loss: 39.0060, val_MinusLogProbMetric: 39.0060

Epoch 580: val_loss improved from 39.05364 to 39.00597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.6970 - MinusLogProbMetric: 38.6970 - val_loss: 39.0060 - val_MinusLogProbMetric: 39.0060 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 581/1000
2023-10-26 20:19:45.605 
Epoch 581/1000 
	 loss: 39.1517, MinusLogProbMetric: 39.1517, val_loss: 39.2093, val_MinusLogProbMetric: 39.2093

Epoch 581: val_loss did not improve from 39.00597
196/196 - 66s - loss: 39.1517 - MinusLogProbMetric: 39.1517 - val_loss: 39.2093 - val_MinusLogProbMetric: 39.2093 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 582/1000
2023-10-26 20:20:51.093 
Epoch 582/1000 
	 loss: 38.6953, MinusLogProbMetric: 38.6953, val_loss: 39.0757, val_MinusLogProbMetric: 39.0757

Epoch 582: val_loss did not improve from 39.00597
196/196 - 65s - loss: 38.6953 - MinusLogProbMetric: 38.6953 - val_loss: 39.0757 - val_MinusLogProbMetric: 39.0757 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 583/1000
2023-10-26 20:21:56.542 
Epoch 583/1000 
	 loss: 38.6411, MinusLogProbMetric: 38.6411, val_loss: 39.0038, val_MinusLogProbMetric: 39.0038

Epoch 583: val_loss improved from 39.00597 to 39.00381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.6411 - MinusLogProbMetric: 38.6411 - val_loss: 39.0038 - val_MinusLogProbMetric: 39.0038 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 584/1000
2023-10-26 20:23:03.961 
Epoch 584/1000 
	 loss: 38.7117, MinusLogProbMetric: 38.7117, val_loss: 39.0886, val_MinusLogProbMetric: 39.0886

Epoch 584: val_loss did not improve from 39.00381
196/196 - 66s - loss: 38.7117 - MinusLogProbMetric: 38.7117 - val_loss: 39.0886 - val_MinusLogProbMetric: 39.0886 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 585/1000
2023-10-26 20:24:10.374 
Epoch 585/1000 
	 loss: 38.6307, MinusLogProbMetric: 38.6307, val_loss: 39.0017, val_MinusLogProbMetric: 39.0017

Epoch 585: val_loss improved from 39.00381 to 39.00167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.6307 - MinusLogProbMetric: 38.6307 - val_loss: 39.0017 - val_MinusLogProbMetric: 39.0017 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 586/1000
2023-10-26 20:25:17.230 
Epoch 586/1000 
	 loss: 38.6003, MinusLogProbMetric: 38.6003, val_loss: 39.0405, val_MinusLogProbMetric: 39.0405

Epoch 586: val_loss did not improve from 39.00167
196/196 - 66s - loss: 38.6003 - MinusLogProbMetric: 38.6003 - val_loss: 39.0405 - val_MinusLogProbMetric: 39.0405 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 587/1000
2023-10-26 20:26:23.234 
Epoch 587/1000 
	 loss: 38.6088, MinusLogProbMetric: 38.6088, val_loss: 39.0401, val_MinusLogProbMetric: 39.0401

Epoch 587: val_loss did not improve from 39.00167
196/196 - 66s - loss: 38.6088 - MinusLogProbMetric: 38.6088 - val_loss: 39.0401 - val_MinusLogProbMetric: 39.0401 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 588/1000
2023-10-26 20:27:29.191 
Epoch 588/1000 
	 loss: 38.5818, MinusLogProbMetric: 38.5818, val_loss: 38.9765, val_MinusLogProbMetric: 38.9765

Epoch 588: val_loss improved from 39.00167 to 38.97649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.5818 - MinusLogProbMetric: 38.5818 - val_loss: 38.9765 - val_MinusLogProbMetric: 38.9765 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 589/1000
2023-10-26 20:28:35.614 
Epoch 589/1000 
	 loss: 38.6149, MinusLogProbMetric: 38.6149, val_loss: 38.9799, val_MinusLogProbMetric: 38.9799

Epoch 589: val_loss did not improve from 38.97649
196/196 - 66s - loss: 38.6149 - MinusLogProbMetric: 38.6149 - val_loss: 38.9799 - val_MinusLogProbMetric: 38.9799 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 590/1000
2023-10-26 20:29:41.544 
Epoch 590/1000 
	 loss: 38.7228, MinusLogProbMetric: 38.7228, val_loss: 39.4929, val_MinusLogProbMetric: 39.4929

Epoch 590: val_loss did not improve from 38.97649
196/196 - 66s - loss: 38.7228 - MinusLogProbMetric: 38.7228 - val_loss: 39.4929 - val_MinusLogProbMetric: 39.4929 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 591/1000
2023-10-26 20:30:47.674 
Epoch 591/1000 
	 loss: 38.6635, MinusLogProbMetric: 38.6635, val_loss: 38.9615, val_MinusLogProbMetric: 38.9615

Epoch 591: val_loss improved from 38.97649 to 38.96154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.6635 - MinusLogProbMetric: 38.6635 - val_loss: 38.9615 - val_MinusLogProbMetric: 38.9615 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 592/1000
2023-10-26 20:31:54.788 
Epoch 592/1000 
	 loss: 38.5998, MinusLogProbMetric: 38.5998, val_loss: 38.9730, val_MinusLogProbMetric: 38.9730

Epoch 592: val_loss did not improve from 38.96154
196/196 - 66s - loss: 38.5998 - MinusLogProbMetric: 38.5998 - val_loss: 38.9730 - val_MinusLogProbMetric: 38.9730 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 593/1000
2023-10-26 20:33:00.718 
Epoch 593/1000 
	 loss: 38.5601, MinusLogProbMetric: 38.5601, val_loss: 39.0482, val_MinusLogProbMetric: 39.0482

Epoch 593: val_loss did not improve from 38.96154
196/196 - 66s - loss: 38.5601 - MinusLogProbMetric: 38.5601 - val_loss: 39.0482 - val_MinusLogProbMetric: 39.0482 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 594/1000
2023-10-26 20:34:06.135 
Epoch 594/1000 
	 loss: 38.7575, MinusLogProbMetric: 38.7575, val_loss: 38.9162, val_MinusLogProbMetric: 38.9162

Epoch 594: val_loss improved from 38.96154 to 38.91617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.7575 - MinusLogProbMetric: 38.7575 - val_loss: 38.9162 - val_MinusLogProbMetric: 38.9162 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 595/1000
2023-10-26 20:35:13.512 
Epoch 595/1000 
	 loss: 38.5538, MinusLogProbMetric: 38.5538, val_loss: 40.6363, val_MinusLogProbMetric: 40.6363

Epoch 595: val_loss did not improve from 38.91617
196/196 - 66s - loss: 38.5538 - MinusLogProbMetric: 38.5538 - val_loss: 40.6363 - val_MinusLogProbMetric: 40.6363 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 596/1000
2023-10-26 20:36:19.536 
Epoch 596/1000 
	 loss: 38.6999, MinusLogProbMetric: 38.6999, val_loss: 38.9511, val_MinusLogProbMetric: 38.9511

Epoch 596: val_loss did not improve from 38.91617
196/196 - 66s - loss: 38.6999 - MinusLogProbMetric: 38.6999 - val_loss: 38.9511 - val_MinusLogProbMetric: 38.9511 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 597/1000
2023-10-26 20:37:26.340 
Epoch 597/1000 
	 loss: 38.5513, MinusLogProbMetric: 38.5513, val_loss: 38.9245, val_MinusLogProbMetric: 38.9245

Epoch 597: val_loss did not improve from 38.91617
196/196 - 67s - loss: 38.5513 - MinusLogProbMetric: 38.5513 - val_loss: 38.9245 - val_MinusLogProbMetric: 38.9245 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 598/1000
2023-10-26 20:38:32.190 
Epoch 598/1000 
	 loss: 38.5296, MinusLogProbMetric: 38.5296, val_loss: 38.9243, val_MinusLogProbMetric: 38.9243

Epoch 598: val_loss did not improve from 38.91617
196/196 - 66s - loss: 38.5296 - MinusLogProbMetric: 38.5296 - val_loss: 38.9243 - val_MinusLogProbMetric: 38.9243 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 599/1000
2023-10-26 20:39:38.421 
Epoch 599/1000 
	 loss: 38.5124, MinusLogProbMetric: 38.5124, val_loss: 38.9417, val_MinusLogProbMetric: 38.9417

Epoch 599: val_loss did not improve from 38.91617
196/196 - 66s - loss: 38.5124 - MinusLogProbMetric: 38.5124 - val_loss: 38.9417 - val_MinusLogProbMetric: 38.9417 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 600/1000
2023-10-26 20:40:45.093 
Epoch 600/1000 
	 loss: 38.5397, MinusLogProbMetric: 38.5397, val_loss: 38.9014, val_MinusLogProbMetric: 38.9014

Epoch 600: val_loss improved from 38.91617 to 38.90140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 38.5397 - MinusLogProbMetric: 38.5397 - val_loss: 38.9014 - val_MinusLogProbMetric: 38.9014 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 601/1000
2023-10-26 20:41:52.515 
Epoch 601/1000 
	 loss: 38.5823, MinusLogProbMetric: 38.5823, val_loss: 38.8688, val_MinusLogProbMetric: 38.8688

Epoch 601: val_loss improved from 38.90140 to 38.86880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 38.5823 - MinusLogProbMetric: 38.5823 - val_loss: 38.8688 - val_MinusLogProbMetric: 38.8688 - lr: 4.5725e-07 - 68s/epoch - 344ms/step
Epoch 602/1000
2023-10-26 20:42:59.540 
Epoch 602/1000 
	 loss: 38.5064, MinusLogProbMetric: 38.5064, val_loss: 38.9566, val_MinusLogProbMetric: 38.9566

Epoch 602: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.5064 - MinusLogProbMetric: 38.5064 - val_loss: 38.9566 - val_MinusLogProbMetric: 38.9566 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 603/1000
2023-10-26 20:44:05.544 
Epoch 603/1000 
	 loss: 38.7017, MinusLogProbMetric: 38.7017, val_loss: 43.6172, val_MinusLogProbMetric: 43.6172

Epoch 603: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.7017 - MinusLogProbMetric: 38.7017 - val_loss: 43.6172 - val_MinusLogProbMetric: 43.6172 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 604/1000
2023-10-26 20:45:11.216 
Epoch 604/1000 
	 loss: 39.0318, MinusLogProbMetric: 39.0318, val_loss: 38.8813, val_MinusLogProbMetric: 38.8813

Epoch 604: val_loss did not improve from 38.86880
196/196 - 66s - loss: 39.0318 - MinusLogProbMetric: 39.0318 - val_loss: 38.8813 - val_MinusLogProbMetric: 38.8813 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 605/1000
2023-10-26 20:46:17.724 
Epoch 605/1000 
	 loss: 38.5139, MinusLogProbMetric: 38.5139, val_loss: 38.9195, val_MinusLogProbMetric: 38.9195

Epoch 605: val_loss did not improve from 38.86880
196/196 - 67s - loss: 38.5139 - MinusLogProbMetric: 38.5139 - val_loss: 38.9195 - val_MinusLogProbMetric: 38.9195 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 606/1000
2023-10-26 20:47:24.230 
Epoch 606/1000 
	 loss: 38.5135, MinusLogProbMetric: 38.5135, val_loss: 38.9034, val_MinusLogProbMetric: 38.9034

Epoch 606: val_loss did not improve from 38.86880
196/196 - 67s - loss: 38.5135 - MinusLogProbMetric: 38.5135 - val_loss: 38.9034 - val_MinusLogProbMetric: 38.9034 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 607/1000
2023-10-26 20:48:30.503 
Epoch 607/1000 
	 loss: 38.4922, MinusLogProbMetric: 38.4922, val_loss: 38.9661, val_MinusLogProbMetric: 38.9661

Epoch 607: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.4922 - MinusLogProbMetric: 38.4922 - val_loss: 38.9661 - val_MinusLogProbMetric: 38.9661 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 608/1000
2023-10-26 20:49:36.609 
Epoch 608/1000 
	 loss: 38.4961, MinusLogProbMetric: 38.4961, val_loss: 38.9425, val_MinusLogProbMetric: 38.9425

Epoch 608: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.4961 - MinusLogProbMetric: 38.4961 - val_loss: 38.9425 - val_MinusLogProbMetric: 38.9425 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 609/1000
2023-10-26 20:50:42.758 
Epoch 609/1000 
	 loss: 38.6000, MinusLogProbMetric: 38.6000, val_loss: 38.8891, val_MinusLogProbMetric: 38.8891

Epoch 609: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.6000 - MinusLogProbMetric: 38.6000 - val_loss: 38.8891 - val_MinusLogProbMetric: 38.8891 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 610/1000
2023-10-26 20:51:48.567 
Epoch 610/1000 
	 loss: 38.4671, MinusLogProbMetric: 38.4671, val_loss: 38.8923, val_MinusLogProbMetric: 38.8923

Epoch 610: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.4671 - MinusLogProbMetric: 38.4671 - val_loss: 38.8923 - val_MinusLogProbMetric: 38.8923 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 611/1000
2023-10-26 20:52:54.220 
Epoch 611/1000 
	 loss: 38.4514, MinusLogProbMetric: 38.4514, val_loss: 38.9511, val_MinusLogProbMetric: 38.9511

Epoch 611: val_loss did not improve from 38.86880
196/196 - 66s - loss: 38.4514 - MinusLogProbMetric: 38.4514 - val_loss: 38.9511 - val_MinusLogProbMetric: 38.9511 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 612/1000
2023-10-26 20:54:00.660 
Epoch 612/1000 
	 loss: 38.5208, MinusLogProbMetric: 38.5208, val_loss: 38.8669, val_MinusLogProbMetric: 38.8669

Epoch 612: val_loss improved from 38.86880 to 38.86691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.5208 - MinusLogProbMetric: 38.5208 - val_loss: 38.8669 - val_MinusLogProbMetric: 38.8669 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 613/1000
2023-10-26 20:55:07.121 
Epoch 613/1000 
	 loss: 38.4480, MinusLogProbMetric: 38.4480, val_loss: 38.8623, val_MinusLogProbMetric: 38.8623

Epoch 613: val_loss improved from 38.86691 to 38.86235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.4480 - MinusLogProbMetric: 38.4480 - val_loss: 38.8623 - val_MinusLogProbMetric: 38.8623 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 614/1000
2023-10-26 20:56:14.242 
Epoch 614/1000 
	 loss: 38.5296, MinusLogProbMetric: 38.5296, val_loss: 38.9434, val_MinusLogProbMetric: 38.9434

Epoch 614: val_loss did not improve from 38.86235
196/196 - 66s - loss: 38.5296 - MinusLogProbMetric: 38.5296 - val_loss: 38.9434 - val_MinusLogProbMetric: 38.9434 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 615/1000
2023-10-26 20:57:20.137 
Epoch 615/1000 
	 loss: 38.7764, MinusLogProbMetric: 38.7764, val_loss: 39.1399, val_MinusLogProbMetric: 39.1399

Epoch 615: val_loss did not improve from 38.86235
196/196 - 66s - loss: 38.7764 - MinusLogProbMetric: 38.7764 - val_loss: 39.1399 - val_MinusLogProbMetric: 39.1399 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 616/1000
2023-10-26 20:58:25.909 
Epoch 616/1000 
	 loss: 38.5124, MinusLogProbMetric: 38.5124, val_loss: 38.8378, val_MinusLogProbMetric: 38.8378

Epoch 616: val_loss improved from 38.86235 to 38.83779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.5124 - MinusLogProbMetric: 38.5124 - val_loss: 38.8378 - val_MinusLogProbMetric: 38.8378 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 617/1000
2023-10-26 20:59:32.212 
Epoch 617/1000 
	 loss: 38.4225, MinusLogProbMetric: 38.4225, val_loss: 38.8737, val_MinusLogProbMetric: 38.8737

Epoch 617: val_loss did not improve from 38.83779
196/196 - 65s - loss: 38.4225 - MinusLogProbMetric: 38.4225 - val_loss: 38.8737 - val_MinusLogProbMetric: 38.8737 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 618/1000
2023-10-26 21:00:38.010 
Epoch 618/1000 
	 loss: 38.4921, MinusLogProbMetric: 38.4921, val_loss: 38.8684, val_MinusLogProbMetric: 38.8684

Epoch 618: val_loss did not improve from 38.83779
196/196 - 66s - loss: 38.4921 - MinusLogProbMetric: 38.4921 - val_loss: 38.8684 - val_MinusLogProbMetric: 38.8684 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 619/1000
2023-10-26 21:01:44.077 
Epoch 619/1000 
	 loss: 38.4676, MinusLogProbMetric: 38.4676, val_loss: 38.9319, val_MinusLogProbMetric: 38.9319

Epoch 619: val_loss did not improve from 38.83779
196/196 - 66s - loss: 38.4676 - MinusLogProbMetric: 38.4676 - val_loss: 38.9319 - val_MinusLogProbMetric: 38.9319 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 620/1000
2023-10-26 21:02:49.759 
Epoch 620/1000 
	 loss: 38.4063, MinusLogProbMetric: 38.4063, val_loss: 38.8910, val_MinusLogProbMetric: 38.8910

Epoch 620: val_loss did not improve from 38.83779
196/196 - 66s - loss: 38.4063 - MinusLogProbMetric: 38.4063 - val_loss: 38.8910 - val_MinusLogProbMetric: 38.8910 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 621/1000
2023-10-26 21:03:55.376 
Epoch 621/1000 
	 loss: 38.5428, MinusLogProbMetric: 38.5428, val_loss: 38.8212, val_MinusLogProbMetric: 38.8212

Epoch 621: val_loss improved from 38.83779 to 38.82121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.5428 - MinusLogProbMetric: 38.5428 - val_loss: 38.8212 - val_MinusLogProbMetric: 38.8212 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 622/1000
2023-10-26 21:05:03.062 
Epoch 622/1000 
	 loss: 38.4069, MinusLogProbMetric: 38.4069, val_loss: 38.8326, val_MinusLogProbMetric: 38.8326

Epoch 622: val_loss did not improve from 38.82121
196/196 - 66s - loss: 38.4069 - MinusLogProbMetric: 38.4069 - val_loss: 38.8326 - val_MinusLogProbMetric: 38.8326 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 623/1000
2023-10-26 21:06:08.697 
Epoch 623/1000 
	 loss: 38.3957, MinusLogProbMetric: 38.3957, val_loss: 38.8615, val_MinusLogProbMetric: 38.8615

Epoch 623: val_loss did not improve from 38.82121
196/196 - 66s - loss: 38.3957 - MinusLogProbMetric: 38.3957 - val_loss: 38.8615 - val_MinusLogProbMetric: 38.8615 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 624/1000
2023-10-26 21:07:14.090 
Epoch 624/1000 
	 loss: 38.9256, MinusLogProbMetric: 38.9256, val_loss: 38.9115, val_MinusLogProbMetric: 38.9115

Epoch 624: val_loss did not improve from 38.82121
196/196 - 65s - loss: 38.9256 - MinusLogProbMetric: 38.9256 - val_loss: 38.9115 - val_MinusLogProbMetric: 38.9115 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 625/1000
2023-10-26 21:08:19.697 
Epoch 625/1000 
	 loss: 38.4200, MinusLogProbMetric: 38.4200, val_loss: 38.7477, val_MinusLogProbMetric: 38.7477

Epoch 625: val_loss improved from 38.82121 to 38.74766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.4200 - MinusLogProbMetric: 38.4200 - val_loss: 38.7477 - val_MinusLogProbMetric: 38.7477 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 626/1000
2023-10-26 21:09:26.728 
Epoch 626/1000 
	 loss: 38.3858, MinusLogProbMetric: 38.3858, val_loss: 38.8013, val_MinusLogProbMetric: 38.8013

Epoch 626: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.3858 - MinusLogProbMetric: 38.3858 - val_loss: 38.8013 - val_MinusLogProbMetric: 38.8013 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 627/1000
2023-10-26 21:10:32.995 
Epoch 627/1000 
	 loss: 38.3562, MinusLogProbMetric: 38.3562, val_loss: 38.8305, val_MinusLogProbMetric: 38.8305

Epoch 627: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.3562 - MinusLogProbMetric: 38.3562 - val_loss: 38.8305 - val_MinusLogProbMetric: 38.8305 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 628/1000
2023-10-26 21:11:38.811 
Epoch 628/1000 
	 loss: 38.4299, MinusLogProbMetric: 38.4299, val_loss: 38.8179, val_MinusLogProbMetric: 38.8179

Epoch 628: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.4299 - MinusLogProbMetric: 38.4299 - val_loss: 38.8179 - val_MinusLogProbMetric: 38.8179 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 629/1000
2023-10-26 21:12:45.096 
Epoch 629/1000 
	 loss: 38.6687, MinusLogProbMetric: 38.6687, val_loss: 39.1154, val_MinusLogProbMetric: 39.1154

Epoch 629: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.6687 - MinusLogProbMetric: 38.6687 - val_loss: 39.1154 - val_MinusLogProbMetric: 39.1154 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 630/1000
2023-10-26 21:13:51.046 
Epoch 630/1000 
	 loss: 38.4906, MinusLogProbMetric: 38.4906, val_loss: 38.7523, val_MinusLogProbMetric: 38.7523

Epoch 630: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.4906 - MinusLogProbMetric: 38.4906 - val_loss: 38.7523 - val_MinusLogProbMetric: 38.7523 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 631/1000
2023-10-26 21:14:57.315 
Epoch 631/1000 
	 loss: 38.3971, MinusLogProbMetric: 38.3971, val_loss: 38.8757, val_MinusLogProbMetric: 38.8757

Epoch 631: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.3971 - MinusLogProbMetric: 38.3971 - val_loss: 38.8757 - val_MinusLogProbMetric: 38.8757 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 632/1000
2023-10-26 21:16:03.361 
Epoch 632/1000 
	 loss: 38.3426, MinusLogProbMetric: 38.3426, val_loss: 38.8007, val_MinusLogProbMetric: 38.8007

Epoch 632: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.3426 - MinusLogProbMetric: 38.3426 - val_loss: 38.8007 - val_MinusLogProbMetric: 38.8007 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 633/1000
2023-10-26 21:17:09.206 
Epoch 633/1000 
	 loss: 38.3805, MinusLogProbMetric: 38.3805, val_loss: 38.7608, val_MinusLogProbMetric: 38.7608

Epoch 633: val_loss did not improve from 38.74766
196/196 - 66s - loss: 38.3805 - MinusLogProbMetric: 38.3805 - val_loss: 38.7608 - val_MinusLogProbMetric: 38.7608 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 634/1000
2023-10-26 21:18:15.822 
Epoch 634/1000 
	 loss: 38.4353, MinusLogProbMetric: 38.4353, val_loss: 38.7460, val_MinusLogProbMetric: 38.7460

Epoch 634: val_loss improved from 38.74766 to 38.74601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 38.4353 - MinusLogProbMetric: 38.4353 - val_loss: 38.7460 - val_MinusLogProbMetric: 38.7460 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 635/1000
2023-10-26 21:19:22.981 
Epoch 635/1000 
	 loss: 38.3502, MinusLogProbMetric: 38.3502, val_loss: 38.8067, val_MinusLogProbMetric: 38.8067

Epoch 635: val_loss did not improve from 38.74601
196/196 - 66s - loss: 38.3502 - MinusLogProbMetric: 38.3502 - val_loss: 38.8067 - val_MinusLogProbMetric: 38.8067 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 636/1000
2023-10-26 21:20:29.088 
Epoch 636/1000 
	 loss: 38.4209, MinusLogProbMetric: 38.4209, val_loss: 38.7427, val_MinusLogProbMetric: 38.7427

Epoch 636: val_loss improved from 38.74601 to 38.74272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.4209 - MinusLogProbMetric: 38.4209 - val_loss: 38.7427 - val_MinusLogProbMetric: 38.7427 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 637/1000
2023-10-26 21:21:35.804 
Epoch 637/1000 
	 loss: 38.3442, MinusLogProbMetric: 38.3442, val_loss: 38.7986, val_MinusLogProbMetric: 38.7986

Epoch 637: val_loss did not improve from 38.74272
196/196 - 66s - loss: 38.3442 - MinusLogProbMetric: 38.3442 - val_loss: 38.7986 - val_MinusLogProbMetric: 38.7986 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 638/1000
2023-10-26 21:22:41.836 
Epoch 638/1000 
	 loss: 38.3338, MinusLogProbMetric: 38.3338, val_loss: 38.6822, val_MinusLogProbMetric: 38.6822

Epoch 638: val_loss improved from 38.74272 to 38.68221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.3338 - MinusLogProbMetric: 38.3338 - val_loss: 38.6822 - val_MinusLogProbMetric: 38.6822 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 639/1000
2023-10-26 21:23:49.063 
Epoch 639/1000 
	 loss: 38.3183, MinusLogProbMetric: 38.3183, val_loss: 38.7569, val_MinusLogProbMetric: 38.7569

Epoch 639: val_loss did not improve from 38.68221
196/196 - 66s - loss: 38.3183 - MinusLogProbMetric: 38.3183 - val_loss: 38.7569 - val_MinusLogProbMetric: 38.7569 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 640/1000
2023-10-26 21:24:54.899 
Epoch 640/1000 
	 loss: 38.4101, MinusLogProbMetric: 38.4101, val_loss: 38.6893, val_MinusLogProbMetric: 38.6893

Epoch 640: val_loss did not improve from 38.68221
196/196 - 66s - loss: 38.4101 - MinusLogProbMetric: 38.4101 - val_loss: 38.6893 - val_MinusLogProbMetric: 38.6893 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 641/1000
2023-10-26 21:26:00.719 
Epoch 641/1000 
	 loss: 38.3743, MinusLogProbMetric: 38.3743, val_loss: 38.6819, val_MinusLogProbMetric: 38.6819

Epoch 641: val_loss improved from 38.68221 to 38.68186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.3743 - MinusLogProbMetric: 38.3743 - val_loss: 38.6819 - val_MinusLogProbMetric: 38.6819 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 642/1000
2023-10-26 21:27:07.655 
Epoch 642/1000 
	 loss: 38.2881, MinusLogProbMetric: 38.2881, val_loss: 38.7002, val_MinusLogProbMetric: 38.7002

Epoch 642: val_loss did not improve from 38.68186
196/196 - 66s - loss: 38.2881 - MinusLogProbMetric: 38.2881 - val_loss: 38.7002 - val_MinusLogProbMetric: 38.7002 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 643/1000
2023-10-26 21:28:13.260 
Epoch 643/1000 
	 loss: 38.3127, MinusLogProbMetric: 38.3127, val_loss: 38.6714, val_MinusLogProbMetric: 38.6714

Epoch 643: val_loss improved from 38.68186 to 38.67142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.3127 - MinusLogProbMetric: 38.3127 - val_loss: 38.6714 - val_MinusLogProbMetric: 38.6714 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 644/1000
2023-10-26 21:29:19.808 
Epoch 644/1000 
	 loss: 38.3794, MinusLogProbMetric: 38.3794, val_loss: 38.8185, val_MinusLogProbMetric: 38.8185

Epoch 644: val_loss did not improve from 38.67142
196/196 - 65s - loss: 38.3794 - MinusLogProbMetric: 38.3794 - val_loss: 38.8185 - val_MinusLogProbMetric: 38.8185 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 645/1000
2023-10-26 21:30:26.443 
Epoch 645/1000 
	 loss: 38.2928, MinusLogProbMetric: 38.2928, val_loss: 38.7223, val_MinusLogProbMetric: 38.7223

Epoch 645: val_loss did not improve from 38.67142
196/196 - 67s - loss: 38.2928 - MinusLogProbMetric: 38.2928 - val_loss: 38.7223 - val_MinusLogProbMetric: 38.7223 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 646/1000
2023-10-26 21:31:31.765 
Epoch 646/1000 
	 loss: 38.2773, MinusLogProbMetric: 38.2773, val_loss: 38.6647, val_MinusLogProbMetric: 38.6647

Epoch 646: val_loss improved from 38.67142 to 38.66469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.2773 - MinusLogProbMetric: 38.2773 - val_loss: 38.6647 - val_MinusLogProbMetric: 38.6647 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 647/1000
2023-10-26 21:32:39.071 
Epoch 647/1000 
	 loss: 38.6274, MinusLogProbMetric: 38.6274, val_loss: 38.7390, val_MinusLogProbMetric: 38.7390

Epoch 647: val_loss did not improve from 38.66469
196/196 - 66s - loss: 38.6274 - MinusLogProbMetric: 38.6274 - val_loss: 38.7390 - val_MinusLogProbMetric: 38.7390 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 648/1000
2023-10-26 21:33:44.754 
Epoch 648/1000 
	 loss: 38.2841, MinusLogProbMetric: 38.2841, val_loss: 38.6325, val_MinusLogProbMetric: 38.6325

Epoch 648: val_loss improved from 38.66469 to 38.63250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.2841 - MinusLogProbMetric: 38.2841 - val_loss: 38.6325 - val_MinusLogProbMetric: 38.6325 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 649/1000
2023-10-26 21:34:51.259 
Epoch 649/1000 
	 loss: 38.2820, MinusLogProbMetric: 38.2820, val_loss: 38.7361, val_MinusLogProbMetric: 38.7361

Epoch 649: val_loss did not improve from 38.63250
196/196 - 65s - loss: 38.2820 - MinusLogProbMetric: 38.2820 - val_loss: 38.7361 - val_MinusLogProbMetric: 38.7361 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 650/1000
2023-10-26 21:35:57.166 
Epoch 650/1000 
	 loss: 38.8742, MinusLogProbMetric: 38.8742, val_loss: 39.0114, val_MinusLogProbMetric: 39.0114

Epoch 650: val_loss did not improve from 38.63250
196/196 - 66s - loss: 38.8742 - MinusLogProbMetric: 38.8742 - val_loss: 39.0114 - val_MinusLogProbMetric: 39.0114 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 651/1000
2023-10-26 21:37:03.256 
Epoch 651/1000 
	 loss: 38.3614, MinusLogProbMetric: 38.3614, val_loss: 38.6597, val_MinusLogProbMetric: 38.6597

Epoch 651: val_loss did not improve from 38.63250
196/196 - 66s - loss: 38.3614 - MinusLogProbMetric: 38.3614 - val_loss: 38.6597 - val_MinusLogProbMetric: 38.6597 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 652/1000
2023-10-26 21:38:08.821 
Epoch 652/1000 
	 loss: 38.2512, MinusLogProbMetric: 38.2512, val_loss: 38.5812, val_MinusLogProbMetric: 38.5812

Epoch 652: val_loss improved from 38.63250 to 38.58119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.2512 - MinusLogProbMetric: 38.2512 - val_loss: 38.5812 - val_MinusLogProbMetric: 38.5812 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 653/1000
2023-10-26 21:39:15.811 
Epoch 653/1000 
	 loss: 38.2453, MinusLogProbMetric: 38.2453, val_loss: 38.6436, val_MinusLogProbMetric: 38.6436

Epoch 653: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2453 - MinusLogProbMetric: 38.2453 - val_loss: 38.6436 - val_MinusLogProbMetric: 38.6436 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 654/1000
2023-10-26 21:40:21.409 
Epoch 654/1000 
	 loss: 38.2280, MinusLogProbMetric: 38.2280, val_loss: 38.6229, val_MinusLogProbMetric: 38.6229

Epoch 654: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2280 - MinusLogProbMetric: 38.2280 - val_loss: 38.6229 - val_MinusLogProbMetric: 38.6229 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 655/1000
2023-10-26 21:41:27.133 
Epoch 655/1000 
	 loss: 38.2609, MinusLogProbMetric: 38.2609, val_loss: 38.7113, val_MinusLogProbMetric: 38.7113

Epoch 655: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2609 - MinusLogProbMetric: 38.2609 - val_loss: 38.7113 - val_MinusLogProbMetric: 38.7113 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 656/1000
2023-10-26 21:42:33.038 
Epoch 656/1000 
	 loss: 38.2518, MinusLogProbMetric: 38.2518, val_loss: 38.5957, val_MinusLogProbMetric: 38.5957

Epoch 656: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2518 - MinusLogProbMetric: 38.2518 - val_loss: 38.5957 - val_MinusLogProbMetric: 38.5957 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 657/1000
2023-10-26 21:43:38.756 
Epoch 657/1000 
	 loss: 38.2407, MinusLogProbMetric: 38.2407, val_loss: 38.6709, val_MinusLogProbMetric: 38.6709

Epoch 657: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2407 - MinusLogProbMetric: 38.2407 - val_loss: 38.6709 - val_MinusLogProbMetric: 38.6709 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 658/1000
2023-10-26 21:44:44.928 
Epoch 658/1000 
	 loss: 38.2101, MinusLogProbMetric: 38.2101, val_loss: 38.6785, val_MinusLogProbMetric: 38.6785

Epoch 658: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2101 - MinusLogProbMetric: 38.2101 - val_loss: 38.6785 - val_MinusLogProbMetric: 38.6785 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 659/1000
2023-10-26 21:45:51.072 
Epoch 659/1000 
	 loss: 38.2219, MinusLogProbMetric: 38.2219, val_loss: 38.6478, val_MinusLogProbMetric: 38.6478

Epoch 659: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2219 - MinusLogProbMetric: 38.2219 - val_loss: 38.6478 - val_MinusLogProbMetric: 38.6478 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 660/1000
2023-10-26 21:46:56.837 
Epoch 660/1000 
	 loss: 38.1924, MinusLogProbMetric: 38.1924, val_loss: 38.6164, val_MinusLogProbMetric: 38.6164

Epoch 660: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.1924 - MinusLogProbMetric: 38.1924 - val_loss: 38.6164 - val_MinusLogProbMetric: 38.6164 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 661/1000
2023-10-26 21:48:01.956 
Epoch 661/1000 
	 loss: 38.2327, MinusLogProbMetric: 38.2327, val_loss: 38.6443, val_MinusLogProbMetric: 38.6443

Epoch 661: val_loss did not improve from 38.58119
196/196 - 65s - loss: 38.2327 - MinusLogProbMetric: 38.2327 - val_loss: 38.6443 - val_MinusLogProbMetric: 38.6443 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 662/1000
2023-10-26 21:49:08.201 
Epoch 662/1000 
	 loss: 38.8776, MinusLogProbMetric: 38.8776, val_loss: 38.9031, val_MinusLogProbMetric: 38.9031

Epoch 662: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.8776 - MinusLogProbMetric: 38.8776 - val_loss: 38.9031 - val_MinusLogProbMetric: 38.9031 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 663/1000
2023-10-26 21:50:14.154 
Epoch 663/1000 
	 loss: 38.2935, MinusLogProbMetric: 38.2935, val_loss: 38.5824, val_MinusLogProbMetric: 38.5824

Epoch 663: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.2935 - MinusLogProbMetric: 38.2935 - val_loss: 38.5824 - val_MinusLogProbMetric: 38.5824 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 664/1000
2023-10-26 21:51:19.558 
Epoch 664/1000 
	 loss: 38.1887, MinusLogProbMetric: 38.1887, val_loss: 38.6062, val_MinusLogProbMetric: 38.6062

Epoch 664: val_loss did not improve from 38.58119
196/196 - 65s - loss: 38.1887 - MinusLogProbMetric: 38.1887 - val_loss: 38.6062 - val_MinusLogProbMetric: 38.6062 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 665/1000
2023-10-26 21:52:25.382 
Epoch 665/1000 
	 loss: 38.1815, MinusLogProbMetric: 38.1815, val_loss: 38.6156, val_MinusLogProbMetric: 38.6156

Epoch 665: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.1815 - MinusLogProbMetric: 38.1815 - val_loss: 38.6156 - val_MinusLogProbMetric: 38.6156 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 666/1000
2023-10-26 21:53:31.168 
Epoch 666/1000 
	 loss: 38.1799, MinusLogProbMetric: 38.1799, val_loss: 38.6296, val_MinusLogProbMetric: 38.6296

Epoch 666: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.1799 - MinusLogProbMetric: 38.1799 - val_loss: 38.6296 - val_MinusLogProbMetric: 38.6296 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 667/1000
2023-10-26 21:54:37.505 
Epoch 667/1000 
	 loss: 38.1948, MinusLogProbMetric: 38.1948, val_loss: 38.7051, val_MinusLogProbMetric: 38.7051

Epoch 667: val_loss did not improve from 38.58119
196/196 - 66s - loss: 38.1948 - MinusLogProbMetric: 38.1948 - val_loss: 38.7051 - val_MinusLogProbMetric: 38.7051 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 668/1000
2023-10-26 21:55:43.140 
Epoch 668/1000 
	 loss: 38.1696, MinusLogProbMetric: 38.1696, val_loss: 38.5457, val_MinusLogProbMetric: 38.5457

Epoch 668: val_loss improved from 38.58119 to 38.54570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.1696 - MinusLogProbMetric: 38.1696 - val_loss: 38.5457 - val_MinusLogProbMetric: 38.5457 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 669/1000
2023-10-26 21:56:49.860 
Epoch 669/1000 
	 loss: 38.1289, MinusLogProbMetric: 38.1289, val_loss: 38.5360, val_MinusLogProbMetric: 38.5360

Epoch 669: val_loss improved from 38.54570 to 38.53605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.1289 - MinusLogProbMetric: 38.1289 - val_loss: 38.5360 - val_MinusLogProbMetric: 38.5360 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 670/1000
2023-10-26 21:57:54.419 
Epoch 670/1000 
	 loss: 38.1783, MinusLogProbMetric: 38.1783, val_loss: 38.7453, val_MinusLogProbMetric: 38.7453

Epoch 670: val_loss did not improve from 38.53605
196/196 - 64s - loss: 38.1783 - MinusLogProbMetric: 38.1783 - val_loss: 38.7453 - val_MinusLogProbMetric: 38.7453 - lr: 4.5725e-07 - 64s/epoch - 324ms/step
Epoch 671/1000
2023-10-26 21:58:51.413 
Epoch 671/1000 
	 loss: 38.4054, MinusLogProbMetric: 38.4054, val_loss: 39.5813, val_MinusLogProbMetric: 39.5813

Epoch 671: val_loss did not improve from 38.53605
196/196 - 57s - loss: 38.4054 - MinusLogProbMetric: 38.4054 - val_loss: 39.5813 - val_MinusLogProbMetric: 39.5813 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 672/1000
2023-10-26 21:59:55.831 
Epoch 672/1000 
	 loss: 38.2956, MinusLogProbMetric: 38.2956, val_loss: 38.5559, val_MinusLogProbMetric: 38.5559

Epoch 672: val_loss did not improve from 38.53605
196/196 - 64s - loss: 38.2956 - MinusLogProbMetric: 38.2956 - val_loss: 38.5559 - val_MinusLogProbMetric: 38.5559 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 673/1000
2023-10-26 22:01:00.753 
Epoch 673/1000 
	 loss: 38.1308, MinusLogProbMetric: 38.1308, val_loss: 38.5745, val_MinusLogProbMetric: 38.5745

Epoch 673: val_loss did not improve from 38.53605
196/196 - 65s - loss: 38.1308 - MinusLogProbMetric: 38.1308 - val_loss: 38.5745 - val_MinusLogProbMetric: 38.5745 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 674/1000
2023-10-26 22:02:05.832 
Epoch 674/1000 
	 loss: 38.5042, MinusLogProbMetric: 38.5042, val_loss: 38.6010, val_MinusLogProbMetric: 38.6010

Epoch 674: val_loss did not improve from 38.53605
196/196 - 65s - loss: 38.5042 - MinusLogProbMetric: 38.5042 - val_loss: 38.6010 - val_MinusLogProbMetric: 38.6010 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 675/1000
2023-10-26 22:03:11.125 
Epoch 675/1000 
	 loss: 38.1546, MinusLogProbMetric: 38.1546, val_loss: 38.5485, val_MinusLogProbMetric: 38.5485

Epoch 675: val_loss did not improve from 38.53605
196/196 - 65s - loss: 38.1546 - MinusLogProbMetric: 38.1546 - val_loss: 38.5485 - val_MinusLogProbMetric: 38.5485 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 676/1000
2023-10-26 22:04:17.027 
Epoch 676/1000 
	 loss: 38.1148, MinusLogProbMetric: 38.1148, val_loss: 38.5530, val_MinusLogProbMetric: 38.5530

Epoch 676: val_loss did not improve from 38.53605
196/196 - 66s - loss: 38.1148 - MinusLogProbMetric: 38.1148 - val_loss: 38.5530 - val_MinusLogProbMetric: 38.5530 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 677/1000
2023-10-26 22:05:22.080 
Epoch 677/1000 
	 loss: 38.2407, MinusLogProbMetric: 38.2407, val_loss: 38.5288, val_MinusLogProbMetric: 38.5288

Epoch 677: val_loss improved from 38.53605 to 38.52875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.2407 - MinusLogProbMetric: 38.2407 - val_loss: 38.5288 - val_MinusLogProbMetric: 38.5288 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 678/1000
2023-10-26 22:06:28.552 
Epoch 678/1000 
	 loss: 38.1013, MinusLogProbMetric: 38.1013, val_loss: 38.5565, val_MinusLogProbMetric: 38.5565

Epoch 678: val_loss did not improve from 38.52875
196/196 - 65s - loss: 38.1013 - MinusLogProbMetric: 38.1013 - val_loss: 38.5565 - val_MinusLogProbMetric: 38.5565 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 679/1000
2023-10-26 22:07:33.558 
Epoch 679/1000 
	 loss: 38.0932, MinusLogProbMetric: 38.0932, val_loss: 38.5517, val_MinusLogProbMetric: 38.5517

Epoch 679: val_loss did not improve from 38.52875
196/196 - 65s - loss: 38.0932 - MinusLogProbMetric: 38.0932 - val_loss: 38.5517 - val_MinusLogProbMetric: 38.5517 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 680/1000
2023-10-26 22:08:38.883 
Epoch 680/1000 
	 loss: 38.2072, MinusLogProbMetric: 38.2072, val_loss: 38.7563, val_MinusLogProbMetric: 38.7563

Epoch 680: val_loss did not improve from 38.52875
196/196 - 65s - loss: 38.2072 - MinusLogProbMetric: 38.2072 - val_loss: 38.7563 - val_MinusLogProbMetric: 38.7563 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 681/1000
2023-10-26 22:09:44.332 
Epoch 681/1000 
	 loss: 38.1175, MinusLogProbMetric: 38.1175, val_loss: 38.5240, val_MinusLogProbMetric: 38.5240

Epoch 681: val_loss improved from 38.52875 to 38.52400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.1175 - MinusLogProbMetric: 38.1175 - val_loss: 38.5240 - val_MinusLogProbMetric: 38.5240 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 682/1000
2023-10-26 22:10:51.008 
Epoch 682/1000 
	 loss: 38.1179, MinusLogProbMetric: 38.1179, val_loss: 38.4687, val_MinusLogProbMetric: 38.4687

Epoch 682: val_loss improved from 38.52400 to 38.46870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.1179 - MinusLogProbMetric: 38.1179 - val_loss: 38.4687 - val_MinusLogProbMetric: 38.4687 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 683/1000
2023-10-26 22:11:57.580 
Epoch 683/1000 
	 loss: 38.0829, MinusLogProbMetric: 38.0829, val_loss: 38.5089, val_MinusLogProbMetric: 38.5089

Epoch 683: val_loss did not improve from 38.46870
196/196 - 66s - loss: 38.0829 - MinusLogProbMetric: 38.0829 - val_loss: 38.5089 - val_MinusLogProbMetric: 38.5089 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 684/1000
2023-10-26 22:13:03.439 
Epoch 684/1000 
	 loss: 38.0802, MinusLogProbMetric: 38.0802, val_loss: 38.5511, val_MinusLogProbMetric: 38.5511

Epoch 684: val_loss did not improve from 38.46870
196/196 - 66s - loss: 38.0802 - MinusLogProbMetric: 38.0802 - val_loss: 38.5511 - val_MinusLogProbMetric: 38.5511 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 685/1000
2023-10-26 22:14:09.816 
Epoch 685/1000 
	 loss: 38.0730, MinusLogProbMetric: 38.0730, val_loss: 38.4657, val_MinusLogProbMetric: 38.4657

Epoch 685: val_loss improved from 38.46870 to 38.46569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 38.0730 - MinusLogProbMetric: 38.0730 - val_loss: 38.4657 - val_MinusLogProbMetric: 38.4657 - lr: 4.5725e-07 - 68s/epoch - 344ms/step
Epoch 686/1000
2023-10-26 22:15:16.758 
Epoch 686/1000 
	 loss: 38.1103, MinusLogProbMetric: 38.1103, val_loss: 38.6292, val_MinusLogProbMetric: 38.6292

Epoch 686: val_loss did not improve from 38.46569
196/196 - 66s - loss: 38.1103 - MinusLogProbMetric: 38.1103 - val_loss: 38.6292 - val_MinusLogProbMetric: 38.6292 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 687/1000
2023-10-26 22:16:22.780 
Epoch 687/1000 
	 loss: 38.0769, MinusLogProbMetric: 38.0769, val_loss: 38.5101, val_MinusLogProbMetric: 38.5101

Epoch 687: val_loss did not improve from 38.46569
196/196 - 66s - loss: 38.0769 - MinusLogProbMetric: 38.0769 - val_loss: 38.5101 - val_MinusLogProbMetric: 38.5101 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 688/1000
2023-10-26 22:17:28.639 
Epoch 688/1000 
	 loss: 38.0551, MinusLogProbMetric: 38.0551, val_loss: 38.5346, val_MinusLogProbMetric: 38.5346

Epoch 688: val_loss did not improve from 38.46569
196/196 - 66s - loss: 38.0551 - MinusLogProbMetric: 38.0551 - val_loss: 38.5346 - val_MinusLogProbMetric: 38.5346 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 689/1000
2023-10-26 22:18:34.606 
Epoch 689/1000 
	 loss: 38.0717, MinusLogProbMetric: 38.0717, val_loss: 38.5133, val_MinusLogProbMetric: 38.5133

Epoch 689: val_loss did not improve from 38.46569
196/196 - 66s - loss: 38.0717 - MinusLogProbMetric: 38.0717 - val_loss: 38.5133 - val_MinusLogProbMetric: 38.5133 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 690/1000
2023-10-26 22:19:40.150 
Epoch 690/1000 
	 loss: 38.0842, MinusLogProbMetric: 38.0842, val_loss: 38.4360, val_MinusLogProbMetric: 38.4360

Epoch 690: val_loss improved from 38.46569 to 38.43603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.0842 - MinusLogProbMetric: 38.0842 - val_loss: 38.4360 - val_MinusLogProbMetric: 38.4360 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 691/1000
2023-10-26 22:20:46.616 
Epoch 691/1000 
	 loss: 38.1580, MinusLogProbMetric: 38.1580, val_loss: 38.9947, val_MinusLogProbMetric: 38.9947

Epoch 691: val_loss did not improve from 38.43603
196/196 - 65s - loss: 38.1580 - MinusLogProbMetric: 38.1580 - val_loss: 38.9947 - val_MinusLogProbMetric: 38.9947 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 692/1000
2023-10-26 22:21:51.657 
Epoch 692/1000 
	 loss: 38.1599, MinusLogProbMetric: 38.1599, val_loss: 38.5008, val_MinusLogProbMetric: 38.5008

Epoch 692: val_loss did not improve from 38.43603
196/196 - 65s - loss: 38.1599 - MinusLogProbMetric: 38.1599 - val_loss: 38.5008 - val_MinusLogProbMetric: 38.5008 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 693/1000
2023-10-26 22:22:57.770 
Epoch 693/1000 
	 loss: 38.0683, MinusLogProbMetric: 38.0683, val_loss: 38.5281, val_MinusLogProbMetric: 38.5281

Epoch 693: val_loss did not improve from 38.43603
196/196 - 66s - loss: 38.0683 - MinusLogProbMetric: 38.0683 - val_loss: 38.5281 - val_MinusLogProbMetric: 38.5281 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 694/1000
2023-10-26 22:24:03.658 
Epoch 694/1000 
	 loss: 38.0620, MinusLogProbMetric: 38.0620, val_loss: 38.4107, val_MinusLogProbMetric: 38.4107

Epoch 694: val_loss improved from 38.43603 to 38.41071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.0620 - MinusLogProbMetric: 38.0620 - val_loss: 38.4107 - val_MinusLogProbMetric: 38.4107 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 695/1000
2023-10-26 22:25:10.338 
Epoch 695/1000 
	 loss: 38.0510, MinusLogProbMetric: 38.0510, val_loss: 38.4485, val_MinusLogProbMetric: 38.4485

Epoch 695: val_loss did not improve from 38.41071
196/196 - 66s - loss: 38.0510 - MinusLogProbMetric: 38.0510 - val_loss: 38.4485 - val_MinusLogProbMetric: 38.4485 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 696/1000
2023-10-26 22:26:15.803 
Epoch 696/1000 
	 loss: 38.4409, MinusLogProbMetric: 38.4409, val_loss: 38.6884, val_MinusLogProbMetric: 38.6884

Epoch 696: val_loss did not improve from 38.41071
196/196 - 65s - loss: 38.4409 - MinusLogProbMetric: 38.4409 - val_loss: 38.6884 - val_MinusLogProbMetric: 38.6884 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 697/1000
2023-10-26 22:27:21.568 
Epoch 697/1000 
	 loss: 38.0849, MinusLogProbMetric: 38.0849, val_loss: 38.4694, val_MinusLogProbMetric: 38.4694

Epoch 697: val_loss did not improve from 38.41071
196/196 - 66s - loss: 38.0849 - MinusLogProbMetric: 38.0849 - val_loss: 38.4694 - val_MinusLogProbMetric: 38.4694 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 698/1000
2023-10-26 22:28:26.908 
Epoch 698/1000 
	 loss: 38.0107, MinusLogProbMetric: 38.0107, val_loss: 38.4227, val_MinusLogProbMetric: 38.4227

Epoch 698: val_loss did not improve from 38.41071
196/196 - 65s - loss: 38.0107 - MinusLogProbMetric: 38.0107 - val_loss: 38.4227 - val_MinusLogProbMetric: 38.4227 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 699/1000
2023-10-26 22:29:32.311 
Epoch 699/1000 
	 loss: 37.9993, MinusLogProbMetric: 37.9993, val_loss: 38.4364, val_MinusLogProbMetric: 38.4364

Epoch 699: val_loss did not improve from 38.41071
196/196 - 65s - loss: 37.9993 - MinusLogProbMetric: 37.9993 - val_loss: 38.4364 - val_MinusLogProbMetric: 38.4364 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 700/1000
2023-10-26 22:30:37.978 
Epoch 700/1000 
	 loss: 38.0122, MinusLogProbMetric: 38.0122, val_loss: 38.4691, val_MinusLogProbMetric: 38.4691

Epoch 700: val_loss did not improve from 38.41071
196/196 - 66s - loss: 38.0122 - MinusLogProbMetric: 38.0122 - val_loss: 38.4691 - val_MinusLogProbMetric: 38.4691 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 701/1000
2023-10-26 22:31:43.895 
Epoch 701/1000 
	 loss: 37.9876, MinusLogProbMetric: 37.9876, val_loss: 38.4195, val_MinusLogProbMetric: 38.4195

Epoch 701: val_loss did not improve from 38.41071
196/196 - 66s - loss: 37.9876 - MinusLogProbMetric: 37.9876 - val_loss: 38.4195 - val_MinusLogProbMetric: 38.4195 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 702/1000
2023-10-26 22:32:49.558 
Epoch 702/1000 
	 loss: 38.1000, MinusLogProbMetric: 38.1000, val_loss: 38.6184, val_MinusLogProbMetric: 38.6184

Epoch 702: val_loss did not improve from 38.41071
196/196 - 66s - loss: 38.1000 - MinusLogProbMetric: 38.1000 - val_loss: 38.6184 - val_MinusLogProbMetric: 38.6184 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 703/1000
2023-10-26 22:33:55.087 
Epoch 703/1000 
	 loss: 38.0216, MinusLogProbMetric: 38.0216, val_loss: 38.4308, val_MinusLogProbMetric: 38.4308

Epoch 703: val_loss did not improve from 38.41071
196/196 - 66s - loss: 38.0216 - MinusLogProbMetric: 38.0216 - val_loss: 38.4308 - val_MinusLogProbMetric: 38.4308 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 704/1000
2023-10-26 22:35:01.545 
Epoch 704/1000 
	 loss: 38.3508, MinusLogProbMetric: 38.3508, val_loss: 39.3485, val_MinusLogProbMetric: 39.3485

Epoch 704: val_loss did not improve from 38.41071
196/196 - 66s - loss: 38.3508 - MinusLogProbMetric: 38.3508 - val_loss: 39.3485 - val_MinusLogProbMetric: 39.3485 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 705/1000
2023-10-26 22:36:07.332 
Epoch 705/1000 
	 loss: 38.1289, MinusLogProbMetric: 38.1289, val_loss: 38.3849, val_MinusLogProbMetric: 38.3849

Epoch 705: val_loss improved from 38.41071 to 38.38490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.1289 - MinusLogProbMetric: 38.1289 - val_loss: 38.3849 - val_MinusLogProbMetric: 38.3849 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 706/1000
2023-10-26 22:37:14.509 
Epoch 706/1000 
	 loss: 38.0014, MinusLogProbMetric: 38.0014, val_loss: 38.3393, val_MinusLogProbMetric: 38.3393

Epoch 706: val_loss improved from 38.38490 to 38.33931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.0014 - MinusLogProbMetric: 38.0014 - val_loss: 38.3393 - val_MinusLogProbMetric: 38.3393 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 707/1000
2023-10-26 22:38:20.926 
Epoch 707/1000 
	 loss: 37.9666, MinusLogProbMetric: 37.9666, val_loss: 38.4296, val_MinusLogProbMetric: 38.4296

Epoch 707: val_loss did not improve from 38.33931
196/196 - 65s - loss: 37.9666 - MinusLogProbMetric: 37.9666 - val_loss: 38.4296 - val_MinusLogProbMetric: 38.4296 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 708/1000
2023-10-26 22:39:26.948 
Epoch 708/1000 
	 loss: 37.9846, MinusLogProbMetric: 37.9846, val_loss: 38.4739, val_MinusLogProbMetric: 38.4739

Epoch 708: val_loss did not improve from 38.33931
196/196 - 66s - loss: 37.9846 - MinusLogProbMetric: 37.9846 - val_loss: 38.4739 - val_MinusLogProbMetric: 38.4739 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 709/1000
2023-10-26 22:40:32.508 
Epoch 709/1000 
	 loss: 38.0010, MinusLogProbMetric: 38.0010, val_loss: 38.4695, val_MinusLogProbMetric: 38.4695

Epoch 709: val_loss did not improve from 38.33931
196/196 - 66s - loss: 38.0010 - MinusLogProbMetric: 38.0010 - val_loss: 38.4695 - val_MinusLogProbMetric: 38.4695 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 710/1000
2023-10-26 22:41:38.056 
Epoch 710/1000 
	 loss: 38.0200, MinusLogProbMetric: 38.0200, val_loss: 38.9705, val_MinusLogProbMetric: 38.9705

Epoch 710: val_loss did not improve from 38.33931
196/196 - 66s - loss: 38.0200 - MinusLogProbMetric: 38.0200 - val_loss: 38.9705 - val_MinusLogProbMetric: 38.9705 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 711/1000
2023-10-26 22:42:43.850 
Epoch 711/1000 
	 loss: 38.0008, MinusLogProbMetric: 38.0008, val_loss: 38.4077, val_MinusLogProbMetric: 38.4077

Epoch 711: val_loss did not improve from 38.33931
196/196 - 66s - loss: 38.0008 - MinusLogProbMetric: 38.0008 - val_loss: 38.4077 - val_MinusLogProbMetric: 38.4077 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 712/1000
2023-10-26 22:43:50.230 
Epoch 712/1000 
	 loss: 37.9425, MinusLogProbMetric: 37.9425, val_loss: 38.4090, val_MinusLogProbMetric: 38.4090

Epoch 712: val_loss did not improve from 38.33931
196/196 - 66s - loss: 37.9425 - MinusLogProbMetric: 37.9425 - val_loss: 38.4090 - val_MinusLogProbMetric: 38.4090 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 713/1000
2023-10-26 22:44:56.669 
Epoch 713/1000 
	 loss: 37.9281, MinusLogProbMetric: 37.9281, val_loss: 38.3762, val_MinusLogProbMetric: 38.3762

Epoch 713: val_loss did not improve from 38.33931
196/196 - 66s - loss: 37.9281 - MinusLogProbMetric: 37.9281 - val_loss: 38.3762 - val_MinusLogProbMetric: 38.3762 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 714/1000
2023-10-26 22:46:03.403 
Epoch 714/1000 
	 loss: 37.9191, MinusLogProbMetric: 37.9191, val_loss: 38.3139, val_MinusLogProbMetric: 38.3139

Epoch 714: val_loss improved from 38.33931 to 38.31390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.9191 - MinusLogProbMetric: 37.9191 - val_loss: 38.3139 - val_MinusLogProbMetric: 38.3139 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 715/1000
2023-10-26 22:47:10.409 
Epoch 715/1000 
	 loss: 37.9626, MinusLogProbMetric: 37.9626, val_loss: 38.3372, val_MinusLogProbMetric: 38.3372

Epoch 715: val_loss did not improve from 38.31390
196/196 - 66s - loss: 37.9626 - MinusLogProbMetric: 37.9626 - val_loss: 38.3372 - val_MinusLogProbMetric: 38.3372 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 716/1000
2023-10-26 22:48:15.794 
Epoch 716/1000 
	 loss: 37.9245, MinusLogProbMetric: 37.9245, val_loss: 38.3470, val_MinusLogProbMetric: 38.3470

Epoch 716: val_loss did not improve from 38.31390
196/196 - 65s - loss: 37.9245 - MinusLogProbMetric: 37.9245 - val_loss: 38.3470 - val_MinusLogProbMetric: 38.3470 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 717/1000
2023-10-26 22:49:21.464 
Epoch 717/1000 
	 loss: 38.3700, MinusLogProbMetric: 38.3700, val_loss: 38.9042, val_MinusLogProbMetric: 38.9042

Epoch 717: val_loss did not improve from 38.31390
196/196 - 66s - loss: 38.3700 - MinusLogProbMetric: 38.3700 - val_loss: 38.9042 - val_MinusLogProbMetric: 38.9042 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 718/1000
2023-10-26 22:50:26.531 
Epoch 718/1000 
	 loss: 38.0469, MinusLogProbMetric: 38.0469, val_loss: 38.3121, val_MinusLogProbMetric: 38.3121

Epoch 718: val_loss improved from 38.31390 to 38.31213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 38.0469 - MinusLogProbMetric: 38.0469 - val_loss: 38.3121 - val_MinusLogProbMetric: 38.3121 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 719/1000
2023-10-26 22:51:33.960 
Epoch 719/1000 
	 loss: 37.9088, MinusLogProbMetric: 37.9088, val_loss: 38.3170, val_MinusLogProbMetric: 38.3170

Epoch 719: val_loss did not improve from 38.31213
196/196 - 66s - loss: 37.9088 - MinusLogProbMetric: 37.9088 - val_loss: 38.3170 - val_MinusLogProbMetric: 38.3170 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 720/1000
2023-10-26 22:52:40.971 
Epoch 720/1000 
	 loss: 37.8988, MinusLogProbMetric: 37.8988, val_loss: 38.3200, val_MinusLogProbMetric: 38.3200

Epoch 720: val_loss did not improve from 38.31213
196/196 - 67s - loss: 37.8988 - MinusLogProbMetric: 37.8988 - val_loss: 38.3200 - val_MinusLogProbMetric: 38.3200 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 721/1000
2023-10-26 22:53:46.997 
Epoch 721/1000 
	 loss: 37.9144, MinusLogProbMetric: 37.9144, val_loss: 38.3102, val_MinusLogProbMetric: 38.3102

Epoch 721: val_loss improved from 38.31213 to 38.31016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.9144 - MinusLogProbMetric: 37.9144 - val_loss: 38.3102 - val_MinusLogProbMetric: 38.3102 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 722/1000
2023-10-26 22:54:54.550 
Epoch 722/1000 
	 loss: 38.4771, MinusLogProbMetric: 38.4771, val_loss: 38.3629, val_MinusLogProbMetric: 38.3629

Epoch 722: val_loss did not improve from 38.31016
196/196 - 66s - loss: 38.4771 - MinusLogProbMetric: 38.4771 - val_loss: 38.3629 - val_MinusLogProbMetric: 38.3629 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 723/1000
2023-10-26 22:56:00.892 
Epoch 723/1000 
	 loss: 37.9080, MinusLogProbMetric: 37.9080, val_loss: 38.3002, val_MinusLogProbMetric: 38.3002

Epoch 723: val_loss improved from 38.31016 to 38.30017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.9080 - MinusLogProbMetric: 37.9080 - val_loss: 38.3002 - val_MinusLogProbMetric: 38.3002 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 724/1000
2023-10-26 22:57:08.832 
Epoch 724/1000 
	 loss: 37.8964, MinusLogProbMetric: 37.8964, val_loss: 38.3368, val_MinusLogProbMetric: 38.3368

Epoch 724: val_loss did not improve from 38.30017
196/196 - 67s - loss: 37.8964 - MinusLogProbMetric: 37.8964 - val_loss: 38.3368 - val_MinusLogProbMetric: 38.3368 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 725/1000
2023-10-26 22:58:14.837 
Epoch 725/1000 
	 loss: 37.8727, MinusLogProbMetric: 37.8727, val_loss: 38.6659, val_MinusLogProbMetric: 38.6659

Epoch 725: val_loss did not improve from 38.30017
196/196 - 66s - loss: 37.8727 - MinusLogProbMetric: 37.8727 - val_loss: 38.6659 - val_MinusLogProbMetric: 38.6659 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 726/1000
2023-10-26 22:59:21.559 
Epoch 726/1000 
	 loss: 37.8907, MinusLogProbMetric: 37.8907, val_loss: 38.2521, val_MinusLogProbMetric: 38.2521

Epoch 726: val_loss improved from 38.30017 to 38.25211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.8907 - MinusLogProbMetric: 37.8907 - val_loss: 38.2521 - val_MinusLogProbMetric: 38.2521 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 727/1000
2023-10-26 23:00:29.683 
Epoch 727/1000 
	 loss: 37.8592, MinusLogProbMetric: 37.8592, val_loss: 38.3638, val_MinusLogProbMetric: 38.3638

Epoch 727: val_loss did not improve from 38.25211
196/196 - 67s - loss: 37.8592 - MinusLogProbMetric: 37.8592 - val_loss: 38.3638 - val_MinusLogProbMetric: 38.3638 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 728/1000
2023-10-26 23:01:36.419 
Epoch 728/1000 
	 loss: 37.8638, MinusLogProbMetric: 37.8638, val_loss: 38.2435, val_MinusLogProbMetric: 38.2435

Epoch 728: val_loss improved from 38.25211 to 38.24349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.8638 - MinusLogProbMetric: 37.8638 - val_loss: 38.2435 - val_MinusLogProbMetric: 38.2435 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 729/1000
2023-10-26 23:02:43.973 
Epoch 729/1000 
	 loss: 38.0357, MinusLogProbMetric: 38.0357, val_loss: 38.3153, val_MinusLogProbMetric: 38.3153

Epoch 729: val_loss did not improve from 38.24349
196/196 - 66s - loss: 38.0357 - MinusLogProbMetric: 38.0357 - val_loss: 38.3153 - val_MinusLogProbMetric: 38.3153 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 730/1000
2023-10-26 23:03:50.469 
Epoch 730/1000 
	 loss: 38.0079, MinusLogProbMetric: 38.0079, val_loss: 38.4445, val_MinusLogProbMetric: 38.4445

Epoch 730: val_loss did not improve from 38.24349
196/196 - 66s - loss: 38.0079 - MinusLogProbMetric: 38.0079 - val_loss: 38.4445 - val_MinusLogProbMetric: 38.4445 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 731/1000
2023-10-26 23:04:57.297 
Epoch 731/1000 
	 loss: 37.9104, MinusLogProbMetric: 37.9104, val_loss: 38.2591, val_MinusLogProbMetric: 38.2591

Epoch 731: val_loss did not improve from 38.24349
196/196 - 67s - loss: 37.9104 - MinusLogProbMetric: 37.9104 - val_loss: 38.2591 - val_MinusLogProbMetric: 38.2591 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 732/1000
2023-10-26 23:06:03.550 
Epoch 732/1000 
	 loss: 37.8418, MinusLogProbMetric: 37.8418, val_loss: 38.2560, val_MinusLogProbMetric: 38.2560

Epoch 732: val_loss did not improve from 38.24349
196/196 - 66s - loss: 37.8418 - MinusLogProbMetric: 37.8418 - val_loss: 38.2560 - val_MinusLogProbMetric: 38.2560 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 733/1000
2023-10-26 23:07:09.698 
Epoch 733/1000 
	 loss: 37.8546, MinusLogProbMetric: 37.8546, val_loss: 38.2411, val_MinusLogProbMetric: 38.2411

Epoch 733: val_loss improved from 38.24349 to 38.24111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.8546 - MinusLogProbMetric: 37.8546 - val_loss: 38.2411 - val_MinusLogProbMetric: 38.2411 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 734/1000
2023-10-26 23:08:17.066 
Epoch 734/1000 
	 loss: 37.8598, MinusLogProbMetric: 37.8598, val_loss: 38.2404, val_MinusLogProbMetric: 38.2404

Epoch 734: val_loss improved from 38.24111 to 38.24044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.8598 - MinusLogProbMetric: 37.8598 - val_loss: 38.2404 - val_MinusLogProbMetric: 38.2404 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 735/1000
2023-10-26 23:09:24.386 
Epoch 735/1000 
	 loss: 37.8291, MinusLogProbMetric: 37.8291, val_loss: 38.2258, val_MinusLogProbMetric: 38.2258

Epoch 735: val_loss improved from 38.24044 to 38.22576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.8291 - MinusLogProbMetric: 37.8291 - val_loss: 38.2258 - val_MinusLogProbMetric: 38.2258 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 736/1000
2023-10-26 23:10:32.538 
Epoch 736/1000 
	 loss: 37.8263, MinusLogProbMetric: 37.8263, val_loss: 38.2133, val_MinusLogProbMetric: 38.2133

Epoch 736: val_loss improved from 38.22576 to 38.21329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.8263 - MinusLogProbMetric: 37.8263 - val_loss: 38.2133 - val_MinusLogProbMetric: 38.2133 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 737/1000
2023-10-26 23:11:39.493 
Epoch 737/1000 
	 loss: 37.9619, MinusLogProbMetric: 37.9619, val_loss: 38.2382, val_MinusLogProbMetric: 38.2382

Epoch 737: val_loss did not improve from 38.21329
196/196 - 66s - loss: 37.9619 - MinusLogProbMetric: 37.9619 - val_loss: 38.2382 - val_MinusLogProbMetric: 38.2382 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 738/1000
2023-10-26 23:12:45.510 
Epoch 738/1000 
	 loss: 37.8049, MinusLogProbMetric: 37.8049, val_loss: 38.2516, val_MinusLogProbMetric: 38.2516

Epoch 738: val_loss did not improve from 38.21329
196/196 - 66s - loss: 37.8049 - MinusLogProbMetric: 37.8049 - val_loss: 38.2516 - val_MinusLogProbMetric: 38.2516 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 739/1000
2023-10-26 23:13:51.915 
Epoch 739/1000 
	 loss: 37.9143, MinusLogProbMetric: 37.9143, val_loss: 38.3734, val_MinusLogProbMetric: 38.3734

Epoch 739: val_loss did not improve from 38.21329
196/196 - 66s - loss: 37.9143 - MinusLogProbMetric: 37.9143 - val_loss: 38.3734 - val_MinusLogProbMetric: 38.3734 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 740/1000
2023-10-26 23:14:57.838 
Epoch 740/1000 
	 loss: 37.8393, MinusLogProbMetric: 37.8393, val_loss: 38.4517, val_MinusLogProbMetric: 38.4517

Epoch 740: val_loss did not improve from 38.21329
196/196 - 66s - loss: 37.8393 - MinusLogProbMetric: 37.8393 - val_loss: 38.4517 - val_MinusLogProbMetric: 38.4517 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 741/1000
2023-10-26 23:16:04.281 
Epoch 741/1000 
	 loss: 37.8933, MinusLogProbMetric: 37.8933, val_loss: 38.1964, val_MinusLogProbMetric: 38.1964

Epoch 741: val_loss improved from 38.21329 to 38.19637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.8933 - MinusLogProbMetric: 37.8933 - val_loss: 38.1964 - val_MinusLogProbMetric: 38.1964 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 742/1000
2023-10-26 23:17:11.668 
Epoch 742/1000 
	 loss: 37.8161, MinusLogProbMetric: 37.8161, val_loss: 38.2416, val_MinusLogProbMetric: 38.2416

Epoch 742: val_loss did not improve from 38.19637
196/196 - 66s - loss: 37.8161 - MinusLogProbMetric: 37.8161 - val_loss: 38.2416 - val_MinusLogProbMetric: 38.2416 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 743/1000
2023-10-26 23:18:17.856 
Epoch 743/1000 
	 loss: 37.8155, MinusLogProbMetric: 37.8155, val_loss: 40.1270, val_MinusLogProbMetric: 40.1270

Epoch 743: val_loss did not improve from 38.19637
196/196 - 66s - loss: 37.8155 - MinusLogProbMetric: 37.8155 - val_loss: 40.1270 - val_MinusLogProbMetric: 40.1270 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 744/1000
2023-10-26 23:19:24.659 
Epoch 744/1000 
	 loss: 38.0618, MinusLogProbMetric: 38.0618, val_loss: 38.1813, val_MinusLogProbMetric: 38.1813

Epoch 744: val_loss improved from 38.19637 to 38.18131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 38.0618 - MinusLogProbMetric: 38.0618 - val_loss: 38.1813 - val_MinusLogProbMetric: 38.1813 - lr: 4.5725e-07 - 68s/epoch - 347ms/step
Epoch 745/1000
2023-10-26 23:20:31.804 
Epoch 745/1000 
	 loss: 37.8232, MinusLogProbMetric: 37.8232, val_loss: 38.2021, val_MinusLogProbMetric: 38.2021

Epoch 745: val_loss did not improve from 38.18131
196/196 - 66s - loss: 37.8232 - MinusLogProbMetric: 37.8232 - val_loss: 38.2021 - val_MinusLogProbMetric: 38.2021 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 746/1000
2023-10-26 23:21:38.602 
Epoch 746/1000 
	 loss: 37.7761, MinusLogProbMetric: 37.7761, val_loss: 38.1786, val_MinusLogProbMetric: 38.1786

Epoch 746: val_loss improved from 38.18131 to 38.17860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.7761 - MinusLogProbMetric: 37.7761 - val_loss: 38.1786 - val_MinusLogProbMetric: 38.1786 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 747/1000
2023-10-26 23:22:45.602 
Epoch 747/1000 
	 loss: 37.7718, MinusLogProbMetric: 37.7718, val_loss: 38.1982, val_MinusLogProbMetric: 38.1982

Epoch 747: val_loss did not improve from 38.17860
196/196 - 66s - loss: 37.7718 - MinusLogProbMetric: 37.7718 - val_loss: 38.1982 - val_MinusLogProbMetric: 38.1982 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 748/1000
2023-10-26 23:23:51.210 
Epoch 748/1000 
	 loss: 37.7932, MinusLogProbMetric: 37.7932, val_loss: 38.2274, val_MinusLogProbMetric: 38.2274

Epoch 748: val_loss did not improve from 38.17860
196/196 - 66s - loss: 37.7932 - MinusLogProbMetric: 37.7932 - val_loss: 38.2274 - val_MinusLogProbMetric: 38.2274 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 749/1000
2023-10-26 23:24:57.673 
Epoch 749/1000 
	 loss: 37.7504, MinusLogProbMetric: 37.7504, val_loss: 38.2211, val_MinusLogProbMetric: 38.2211

Epoch 749: val_loss did not improve from 38.17860
196/196 - 66s - loss: 37.7504 - MinusLogProbMetric: 37.7504 - val_loss: 38.2211 - val_MinusLogProbMetric: 38.2211 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 750/1000
2023-10-26 23:26:03.521 
Epoch 750/1000 
	 loss: 38.0235, MinusLogProbMetric: 38.0235, val_loss: 38.1742, val_MinusLogProbMetric: 38.1742

Epoch 750: val_loss improved from 38.17860 to 38.17416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 38.0235 - MinusLogProbMetric: 38.0235 - val_loss: 38.1742 - val_MinusLogProbMetric: 38.1742 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 751/1000
2023-10-26 23:27:10.579 
Epoch 751/1000 
	 loss: 37.7514, MinusLogProbMetric: 37.7514, val_loss: 38.1432, val_MinusLogProbMetric: 38.1432

Epoch 751: val_loss improved from 38.17416 to 38.14323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.7514 - MinusLogProbMetric: 37.7514 - val_loss: 38.1432 - val_MinusLogProbMetric: 38.1432 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 752/1000
2023-10-26 23:28:18.139 
Epoch 752/1000 
	 loss: 37.7553, MinusLogProbMetric: 37.7553, val_loss: 38.1980, val_MinusLogProbMetric: 38.1980

Epoch 752: val_loss did not improve from 38.14323
196/196 - 67s - loss: 37.7553 - MinusLogProbMetric: 37.7553 - val_loss: 38.1980 - val_MinusLogProbMetric: 38.1980 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 753/1000
2023-10-26 23:29:25.142 
Epoch 753/1000 
	 loss: 37.7640, MinusLogProbMetric: 37.7640, val_loss: 38.2036, val_MinusLogProbMetric: 38.2036

Epoch 753: val_loss did not improve from 38.14323
196/196 - 67s - loss: 37.7640 - MinusLogProbMetric: 37.7640 - val_loss: 38.2036 - val_MinusLogProbMetric: 38.2036 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 754/1000
2023-10-26 23:30:31.589 
Epoch 754/1000 
	 loss: 37.7532, MinusLogProbMetric: 37.7532, val_loss: 38.1539, val_MinusLogProbMetric: 38.1539

Epoch 754: val_loss did not improve from 38.14323
196/196 - 66s - loss: 37.7532 - MinusLogProbMetric: 37.7532 - val_loss: 38.1539 - val_MinusLogProbMetric: 38.1539 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 755/1000
2023-10-26 23:31:37.282 
Epoch 755/1000 
	 loss: 38.0304, MinusLogProbMetric: 38.0304, val_loss: 38.7154, val_MinusLogProbMetric: 38.7154

Epoch 755: val_loss did not improve from 38.14323
196/196 - 66s - loss: 38.0304 - MinusLogProbMetric: 38.0304 - val_loss: 38.7154 - val_MinusLogProbMetric: 38.7154 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 756/1000
2023-10-26 23:32:42.686 
Epoch 756/1000 
	 loss: 37.8704, MinusLogProbMetric: 37.8704, val_loss: 38.2430, val_MinusLogProbMetric: 38.2430

Epoch 756: val_loss did not improve from 38.14323
196/196 - 65s - loss: 37.8704 - MinusLogProbMetric: 37.8704 - val_loss: 38.2430 - val_MinusLogProbMetric: 38.2430 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 757/1000
2023-10-26 23:33:48.381 
Epoch 757/1000 
	 loss: 37.7483, MinusLogProbMetric: 37.7483, val_loss: 38.1414, val_MinusLogProbMetric: 38.1414

Epoch 757: val_loss improved from 38.14323 to 38.14143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.7483 - MinusLogProbMetric: 37.7483 - val_loss: 38.1414 - val_MinusLogProbMetric: 38.1414 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 758/1000
2023-10-26 23:34:55.287 
Epoch 758/1000 
	 loss: 37.7127, MinusLogProbMetric: 37.7127, val_loss: 38.1362, val_MinusLogProbMetric: 38.1362

Epoch 758: val_loss improved from 38.14143 to 38.13625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.7127 - MinusLogProbMetric: 37.7127 - val_loss: 38.1362 - val_MinusLogProbMetric: 38.1362 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 759/1000
2023-10-26 23:36:02.102 
Epoch 759/1000 
	 loss: 37.7560, MinusLogProbMetric: 37.7560, val_loss: 38.3679, val_MinusLogProbMetric: 38.3679

Epoch 759: val_loss did not improve from 38.13625
196/196 - 66s - loss: 37.7560 - MinusLogProbMetric: 37.7560 - val_loss: 38.3679 - val_MinusLogProbMetric: 38.3679 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 760/1000
2023-10-26 23:37:08.683 
Epoch 760/1000 
	 loss: 38.1691, MinusLogProbMetric: 38.1691, val_loss: 38.1754, val_MinusLogProbMetric: 38.1754

Epoch 760: val_loss did not improve from 38.13625
196/196 - 67s - loss: 38.1691 - MinusLogProbMetric: 38.1691 - val_loss: 38.1754 - val_MinusLogProbMetric: 38.1754 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 761/1000
2023-10-26 23:38:14.789 
Epoch 761/1000 
	 loss: 37.7353, MinusLogProbMetric: 37.7353, val_loss: 38.0863, val_MinusLogProbMetric: 38.0863

Epoch 761: val_loss improved from 38.13625 to 38.08630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.7353 - MinusLogProbMetric: 37.7353 - val_loss: 38.0863 - val_MinusLogProbMetric: 38.0863 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 762/1000
2023-10-26 23:39:22.203 
Epoch 762/1000 
	 loss: 37.7090, MinusLogProbMetric: 37.7090, val_loss: 38.1048, val_MinusLogProbMetric: 38.1048

Epoch 762: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7090 - MinusLogProbMetric: 37.7090 - val_loss: 38.1048 - val_MinusLogProbMetric: 38.1048 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 763/1000
2023-10-26 23:40:28.268 
Epoch 763/1000 
	 loss: 37.7117, MinusLogProbMetric: 37.7117, val_loss: 38.1285, val_MinusLogProbMetric: 38.1285

Epoch 763: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7117 - MinusLogProbMetric: 37.7117 - val_loss: 38.1285 - val_MinusLogProbMetric: 38.1285 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 764/1000
2023-10-26 23:41:35.183 
Epoch 764/1000 
	 loss: 37.7317, MinusLogProbMetric: 37.7317, val_loss: 38.1719, val_MinusLogProbMetric: 38.1719

Epoch 764: val_loss did not improve from 38.08630
196/196 - 67s - loss: 37.7317 - MinusLogProbMetric: 37.7317 - val_loss: 38.1719 - val_MinusLogProbMetric: 38.1719 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 765/1000
2023-10-26 23:42:41.445 
Epoch 765/1000 
	 loss: 37.7578, MinusLogProbMetric: 37.7578, val_loss: 38.1451, val_MinusLogProbMetric: 38.1451

Epoch 765: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7578 - MinusLogProbMetric: 37.7578 - val_loss: 38.1451 - val_MinusLogProbMetric: 38.1451 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 766/1000
2023-10-26 23:43:47.434 
Epoch 766/1000 
	 loss: 37.7072, MinusLogProbMetric: 37.7072, val_loss: 38.1029, val_MinusLogProbMetric: 38.1029

Epoch 766: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7072 - MinusLogProbMetric: 37.7072 - val_loss: 38.1029 - val_MinusLogProbMetric: 38.1029 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 767/1000
2023-10-26 23:44:53.664 
Epoch 767/1000 
	 loss: 37.6764, MinusLogProbMetric: 37.6764, val_loss: 38.1412, val_MinusLogProbMetric: 38.1412

Epoch 767: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.6764 - MinusLogProbMetric: 37.6764 - val_loss: 38.1412 - val_MinusLogProbMetric: 38.1412 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 768/1000
2023-10-26 23:46:00.034 
Epoch 768/1000 
	 loss: 37.8495, MinusLogProbMetric: 37.8495, val_loss: 38.0934, val_MinusLogProbMetric: 38.0934

Epoch 768: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.8495 - MinusLogProbMetric: 37.8495 - val_loss: 38.0934 - val_MinusLogProbMetric: 38.0934 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 769/1000
2023-10-26 23:47:05.959 
Epoch 769/1000 
	 loss: 37.7093, MinusLogProbMetric: 37.7093, val_loss: 38.1026, val_MinusLogProbMetric: 38.1026

Epoch 769: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7093 - MinusLogProbMetric: 37.7093 - val_loss: 38.1026 - val_MinusLogProbMetric: 38.1026 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 770/1000
2023-10-26 23:48:11.773 
Epoch 770/1000 
	 loss: 37.7418, MinusLogProbMetric: 37.7418, val_loss: 38.1355, val_MinusLogProbMetric: 38.1355

Epoch 770: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7418 - MinusLogProbMetric: 37.7418 - val_loss: 38.1355 - val_MinusLogProbMetric: 38.1355 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 771/1000
2023-10-26 23:49:18.500 
Epoch 771/1000 
	 loss: 37.7084, MinusLogProbMetric: 37.7084, val_loss: 38.1384, val_MinusLogProbMetric: 38.1384

Epoch 771: val_loss did not improve from 38.08630
196/196 - 67s - loss: 37.7084 - MinusLogProbMetric: 37.7084 - val_loss: 38.1384 - val_MinusLogProbMetric: 38.1384 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 772/1000
2023-10-26 23:50:24.470 
Epoch 772/1000 
	 loss: 37.7747, MinusLogProbMetric: 37.7747, val_loss: 38.1619, val_MinusLogProbMetric: 38.1619

Epoch 772: val_loss did not improve from 38.08630
196/196 - 66s - loss: 37.7747 - MinusLogProbMetric: 37.7747 - val_loss: 38.1619 - val_MinusLogProbMetric: 38.1619 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 773/1000
2023-10-26 23:51:30.605 
Epoch 773/1000 
	 loss: 37.6794, MinusLogProbMetric: 37.6794, val_loss: 38.0855, val_MinusLogProbMetric: 38.0855

Epoch 773: val_loss improved from 38.08630 to 38.08548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.6794 - MinusLogProbMetric: 37.6794 - val_loss: 38.0855 - val_MinusLogProbMetric: 38.0855 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 774/1000
2023-10-26 23:52:37.398 
Epoch 774/1000 
	 loss: 37.6494, MinusLogProbMetric: 37.6494, val_loss: 38.0930, val_MinusLogProbMetric: 38.0930

Epoch 774: val_loss did not improve from 38.08548
196/196 - 66s - loss: 37.6494 - MinusLogProbMetric: 37.6494 - val_loss: 38.0930 - val_MinusLogProbMetric: 38.0930 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 775/1000
2023-10-26 23:53:43.406 
Epoch 775/1000 
	 loss: 37.6513, MinusLogProbMetric: 37.6513, val_loss: 38.1109, val_MinusLogProbMetric: 38.1109

Epoch 775: val_loss did not improve from 38.08548
196/196 - 66s - loss: 37.6513 - MinusLogProbMetric: 37.6513 - val_loss: 38.1109 - val_MinusLogProbMetric: 38.1109 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 776/1000
2023-10-26 23:54:49.513 
Epoch 776/1000 
	 loss: 37.9121, MinusLogProbMetric: 37.9121, val_loss: 38.3152, val_MinusLogProbMetric: 38.3152

Epoch 776: val_loss did not improve from 38.08548
196/196 - 66s - loss: 37.9121 - MinusLogProbMetric: 37.9121 - val_loss: 38.3152 - val_MinusLogProbMetric: 38.3152 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 777/1000
2023-10-26 23:55:55.619 
Epoch 777/1000 
	 loss: 37.6857, MinusLogProbMetric: 37.6857, val_loss: 38.0191, val_MinusLogProbMetric: 38.0191

Epoch 777: val_loss improved from 38.08548 to 38.01908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.6857 - MinusLogProbMetric: 37.6857 - val_loss: 38.0191 - val_MinusLogProbMetric: 38.0191 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 778/1000
2023-10-26 23:57:03.362 
Epoch 778/1000 
	 loss: 37.6386, MinusLogProbMetric: 37.6386, val_loss: 38.0457, val_MinusLogProbMetric: 38.0457

Epoch 778: val_loss did not improve from 38.01908
196/196 - 67s - loss: 37.6386 - MinusLogProbMetric: 37.6386 - val_loss: 38.0457 - val_MinusLogProbMetric: 38.0457 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 779/1000
2023-10-26 23:58:09.969 
Epoch 779/1000 
	 loss: 37.6728, MinusLogProbMetric: 37.6728, val_loss: 38.0311, val_MinusLogProbMetric: 38.0311

Epoch 779: val_loss did not improve from 38.01908
196/196 - 67s - loss: 37.6728 - MinusLogProbMetric: 37.6728 - val_loss: 38.0311 - val_MinusLogProbMetric: 38.0311 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 780/1000
2023-10-26 23:59:16.007 
Epoch 780/1000 
	 loss: 37.6137, MinusLogProbMetric: 37.6137, val_loss: 38.0961, val_MinusLogProbMetric: 38.0961

Epoch 780: val_loss did not improve from 38.01908
196/196 - 66s - loss: 37.6137 - MinusLogProbMetric: 37.6137 - val_loss: 38.0961 - val_MinusLogProbMetric: 38.0961 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 781/1000
2023-10-27 00:00:22.836 
Epoch 781/1000 
	 loss: 38.1588, MinusLogProbMetric: 38.1588, val_loss: 38.0444, val_MinusLogProbMetric: 38.0444

Epoch 781: val_loss did not improve from 38.01908
196/196 - 67s - loss: 38.1588 - MinusLogProbMetric: 38.1588 - val_loss: 38.0444 - val_MinusLogProbMetric: 38.0444 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 782/1000
2023-10-27 00:01:28.755 
Epoch 782/1000 
	 loss: 37.6506, MinusLogProbMetric: 37.6506, val_loss: 38.0764, val_MinusLogProbMetric: 38.0764

Epoch 782: val_loss did not improve from 38.01908
196/196 - 66s - loss: 37.6506 - MinusLogProbMetric: 37.6506 - val_loss: 38.0764 - val_MinusLogProbMetric: 38.0764 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 783/1000
2023-10-27 00:02:34.817 
Epoch 783/1000 
	 loss: 37.6025, MinusLogProbMetric: 37.6025, val_loss: 38.0394, val_MinusLogProbMetric: 38.0394

Epoch 783: val_loss did not improve from 38.01908
196/196 - 66s - loss: 37.6025 - MinusLogProbMetric: 37.6025 - val_loss: 38.0394 - val_MinusLogProbMetric: 38.0394 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 784/1000
2023-10-27 00:03:41.471 
Epoch 784/1000 
	 loss: 37.6052, MinusLogProbMetric: 37.6052, val_loss: 38.0565, val_MinusLogProbMetric: 38.0565

Epoch 784: val_loss did not improve from 38.01908
196/196 - 67s - loss: 37.6052 - MinusLogProbMetric: 37.6052 - val_loss: 38.0565 - val_MinusLogProbMetric: 38.0565 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 785/1000
2023-10-27 00:04:48.092 
Epoch 785/1000 
	 loss: 38.1448, MinusLogProbMetric: 38.1448, val_loss: 38.0274, val_MinusLogProbMetric: 38.0274

Epoch 785: val_loss did not improve from 38.01908
196/196 - 67s - loss: 38.1448 - MinusLogProbMetric: 38.1448 - val_loss: 38.0274 - val_MinusLogProbMetric: 38.0274 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 786/1000
2023-10-27 00:05:53.279 
Epoch 786/1000 
	 loss: 37.6206, MinusLogProbMetric: 37.6206, val_loss: 38.0123, val_MinusLogProbMetric: 38.0123

Epoch 786: val_loss improved from 38.01908 to 38.01229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.6206 - MinusLogProbMetric: 37.6206 - val_loss: 38.0123 - val_MinusLogProbMetric: 38.0123 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 787/1000
2023-10-27 00:07:00.427 
Epoch 787/1000 
	 loss: 37.6120, MinusLogProbMetric: 37.6120, val_loss: 38.2534, val_MinusLogProbMetric: 38.2534

Epoch 787: val_loss did not improve from 38.01229
196/196 - 66s - loss: 37.6120 - MinusLogProbMetric: 37.6120 - val_loss: 38.2534 - val_MinusLogProbMetric: 38.2534 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 788/1000
2023-10-27 00:08:06.616 
Epoch 788/1000 
	 loss: 37.6385, MinusLogProbMetric: 37.6385, val_loss: 38.0211, val_MinusLogProbMetric: 38.0211

Epoch 788: val_loss did not improve from 38.01229
196/196 - 66s - loss: 37.6385 - MinusLogProbMetric: 37.6385 - val_loss: 38.0211 - val_MinusLogProbMetric: 38.0211 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 789/1000
2023-10-27 00:09:12.523 
Epoch 789/1000 
	 loss: 37.6199, MinusLogProbMetric: 37.6199, val_loss: 38.5665, val_MinusLogProbMetric: 38.5665

Epoch 789: val_loss did not improve from 38.01229
196/196 - 66s - loss: 37.6199 - MinusLogProbMetric: 37.6199 - val_loss: 38.5665 - val_MinusLogProbMetric: 38.5665 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 790/1000
2023-10-27 00:10:18.155 
Epoch 790/1000 
	 loss: 37.6244, MinusLogProbMetric: 37.6244, val_loss: 37.9937, val_MinusLogProbMetric: 37.9937

Epoch 790: val_loss improved from 38.01229 to 37.99374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.6244 - MinusLogProbMetric: 37.6244 - val_loss: 37.9937 - val_MinusLogProbMetric: 37.9937 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 791/1000
2023-10-27 00:11:25.024 
Epoch 791/1000 
	 loss: 37.6008, MinusLogProbMetric: 37.6008, val_loss: 37.9703, val_MinusLogProbMetric: 37.9703

Epoch 791: val_loss improved from 37.99374 to 37.97028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.6008 - MinusLogProbMetric: 37.6008 - val_loss: 37.9703 - val_MinusLogProbMetric: 37.9703 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 792/1000
2023-10-27 00:12:31.638 
Epoch 792/1000 
	 loss: 38.3419, MinusLogProbMetric: 38.3419, val_loss: 38.0825, val_MinusLogProbMetric: 38.0825

Epoch 792: val_loss did not improve from 37.97028
196/196 - 66s - loss: 38.3419 - MinusLogProbMetric: 38.3419 - val_loss: 38.0825 - val_MinusLogProbMetric: 38.0825 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 793/1000
2023-10-27 00:13:36.953 
Epoch 793/1000 
	 loss: 37.6170, MinusLogProbMetric: 37.6170, val_loss: 37.9789, val_MinusLogProbMetric: 37.9789

Epoch 793: val_loss did not improve from 37.97028
196/196 - 65s - loss: 37.6170 - MinusLogProbMetric: 37.6170 - val_loss: 37.9789 - val_MinusLogProbMetric: 37.9789 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 794/1000
2023-10-27 00:14:42.038 
Epoch 794/1000 
	 loss: 37.5664, MinusLogProbMetric: 37.5664, val_loss: 37.9513, val_MinusLogProbMetric: 37.9513

Epoch 794: val_loss improved from 37.97028 to 37.95126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.5664 - MinusLogProbMetric: 37.5664 - val_loss: 37.9513 - val_MinusLogProbMetric: 37.9513 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 795/1000
2023-10-27 00:15:44.988 
Epoch 795/1000 
	 loss: 37.5728, MinusLogProbMetric: 37.5728, val_loss: 37.9238, val_MinusLogProbMetric: 37.9238

Epoch 795: val_loss improved from 37.95126 to 37.92381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 37.5728 - MinusLogProbMetric: 37.5728 - val_loss: 37.9238 - val_MinusLogProbMetric: 37.9238 - lr: 4.5725e-07 - 63s/epoch - 322ms/step
Epoch 796/1000
2023-10-27 00:16:51.340 
Epoch 796/1000 
	 loss: 37.5643, MinusLogProbMetric: 37.5643, val_loss: 37.9930, val_MinusLogProbMetric: 37.9930

Epoch 796: val_loss did not improve from 37.92381
196/196 - 65s - loss: 37.5643 - MinusLogProbMetric: 37.5643 - val_loss: 37.9930 - val_MinusLogProbMetric: 37.9930 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 797/1000
2023-10-27 00:17:56.675 
Epoch 797/1000 
	 loss: 37.7536, MinusLogProbMetric: 37.7536, val_loss: 38.9739, val_MinusLogProbMetric: 38.9739

Epoch 797: val_loss did not improve from 37.92381
196/196 - 65s - loss: 37.7536 - MinusLogProbMetric: 37.7536 - val_loss: 38.9739 - val_MinusLogProbMetric: 38.9739 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 798/1000
2023-10-27 00:19:01.558 
Epoch 798/1000 
	 loss: 37.6879, MinusLogProbMetric: 37.6879, val_loss: 37.9534, val_MinusLogProbMetric: 37.9534

Epoch 798: val_loss did not improve from 37.92381
196/196 - 65s - loss: 37.6879 - MinusLogProbMetric: 37.6879 - val_loss: 37.9534 - val_MinusLogProbMetric: 37.9534 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 799/1000
2023-10-27 00:20:07.001 
Epoch 799/1000 
	 loss: 37.5624, MinusLogProbMetric: 37.5624, val_loss: 37.9684, val_MinusLogProbMetric: 37.9684

Epoch 799: val_loss did not improve from 37.92381
196/196 - 65s - loss: 37.5624 - MinusLogProbMetric: 37.5624 - val_loss: 37.9684 - val_MinusLogProbMetric: 37.9684 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 800/1000
2023-10-27 00:21:12.357 
Epoch 800/1000 
	 loss: 37.5366, MinusLogProbMetric: 37.5366, val_loss: 38.0026, val_MinusLogProbMetric: 38.0026

Epoch 800: val_loss did not improve from 37.92381
196/196 - 65s - loss: 37.5366 - MinusLogProbMetric: 37.5366 - val_loss: 38.0026 - val_MinusLogProbMetric: 38.0026 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 801/1000
2023-10-27 00:22:18.096 
Epoch 801/1000 
	 loss: 37.5303, MinusLogProbMetric: 37.5303, val_loss: 38.0264, val_MinusLogProbMetric: 38.0264

Epoch 801: val_loss did not improve from 37.92381
196/196 - 66s - loss: 37.5303 - MinusLogProbMetric: 37.5303 - val_loss: 38.0264 - val_MinusLogProbMetric: 38.0264 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 802/1000
2023-10-27 00:23:23.884 
Epoch 802/1000 
	 loss: 37.5503, MinusLogProbMetric: 37.5503, val_loss: 37.9292, val_MinusLogProbMetric: 37.9292

Epoch 802: val_loss did not improve from 37.92381
196/196 - 66s - loss: 37.5503 - MinusLogProbMetric: 37.5503 - val_loss: 37.9292 - val_MinusLogProbMetric: 37.9292 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 803/1000
2023-10-27 00:24:29.426 
Epoch 803/1000 
	 loss: 37.6704, MinusLogProbMetric: 37.6704, val_loss: 38.4627, val_MinusLogProbMetric: 38.4627

Epoch 803: val_loss did not improve from 37.92381
196/196 - 66s - loss: 37.6704 - MinusLogProbMetric: 37.6704 - val_loss: 38.4627 - val_MinusLogProbMetric: 38.4627 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 804/1000
2023-10-27 00:25:35.225 
Epoch 804/1000 
	 loss: 37.5958, MinusLogProbMetric: 37.5958, val_loss: 37.9709, val_MinusLogProbMetric: 37.9709

Epoch 804: val_loss did not improve from 37.92381
196/196 - 66s - loss: 37.5958 - MinusLogProbMetric: 37.5958 - val_loss: 37.9709 - val_MinusLogProbMetric: 37.9709 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 805/1000
2023-10-27 00:26:40.136 
Epoch 805/1000 
	 loss: 37.5670, MinusLogProbMetric: 37.5670, val_loss: 37.9409, val_MinusLogProbMetric: 37.9409

Epoch 805: val_loss did not improve from 37.92381
196/196 - 65s - loss: 37.5670 - MinusLogProbMetric: 37.5670 - val_loss: 37.9409 - val_MinusLogProbMetric: 37.9409 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 806/1000
2023-10-27 00:27:45.727 
Epoch 806/1000 
	 loss: 37.5034, MinusLogProbMetric: 37.5034, val_loss: 37.9333, val_MinusLogProbMetric: 37.9333

Epoch 806: val_loss did not improve from 37.92381
196/196 - 66s - loss: 37.5034 - MinusLogProbMetric: 37.5034 - val_loss: 37.9333 - val_MinusLogProbMetric: 37.9333 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 807/1000
2023-10-27 00:28:51.194 
Epoch 807/1000 
	 loss: 37.6405, MinusLogProbMetric: 37.6405, val_loss: 37.9159, val_MinusLogProbMetric: 37.9159

Epoch 807: val_loss improved from 37.92381 to 37.91589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.6405 - MinusLogProbMetric: 37.6405 - val_loss: 37.9159 - val_MinusLogProbMetric: 37.9159 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 808/1000
2023-10-27 00:29:57.585 
Epoch 808/1000 
	 loss: 37.4940, MinusLogProbMetric: 37.4940, val_loss: 37.9925, val_MinusLogProbMetric: 37.9925

Epoch 808: val_loss did not improve from 37.91589
196/196 - 65s - loss: 37.4940 - MinusLogProbMetric: 37.4940 - val_loss: 37.9925 - val_MinusLogProbMetric: 37.9925 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 809/1000
2023-10-27 00:31:02.623 
Epoch 809/1000 
	 loss: 37.5294, MinusLogProbMetric: 37.5294, val_loss: 37.9117, val_MinusLogProbMetric: 37.9117

Epoch 809: val_loss improved from 37.91589 to 37.91172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.5294 - MinusLogProbMetric: 37.5294 - val_loss: 37.9117 - val_MinusLogProbMetric: 37.9117 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 810/1000
2023-10-27 00:32:09.037 
Epoch 810/1000 
	 loss: 37.5396, MinusLogProbMetric: 37.5396, val_loss: 37.8881, val_MinusLogProbMetric: 37.8881

Epoch 810: val_loss improved from 37.91172 to 37.88808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.5396 - MinusLogProbMetric: 37.5396 - val_loss: 37.8881 - val_MinusLogProbMetric: 37.8881 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 811/1000
2023-10-27 00:33:15.758 
Epoch 811/1000 
	 loss: 37.4879, MinusLogProbMetric: 37.4879, val_loss: 37.9715, val_MinusLogProbMetric: 37.9715

Epoch 811: val_loss did not improve from 37.88808
196/196 - 65s - loss: 37.4879 - MinusLogProbMetric: 37.4879 - val_loss: 37.9715 - val_MinusLogProbMetric: 37.9715 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 812/1000
2023-10-27 00:34:21.099 
Epoch 812/1000 
	 loss: 37.4856, MinusLogProbMetric: 37.4856, val_loss: 37.9250, val_MinusLogProbMetric: 37.9250

Epoch 812: val_loss did not improve from 37.88808
196/196 - 65s - loss: 37.4856 - MinusLogProbMetric: 37.4856 - val_loss: 37.9250 - val_MinusLogProbMetric: 37.9250 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 813/1000
2023-10-27 00:35:26.481 
Epoch 813/1000 
	 loss: 37.4791, MinusLogProbMetric: 37.4791, val_loss: 37.9310, val_MinusLogProbMetric: 37.9310

Epoch 813: val_loss did not improve from 37.88808
196/196 - 65s - loss: 37.4791 - MinusLogProbMetric: 37.4791 - val_loss: 37.9310 - val_MinusLogProbMetric: 37.9310 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 814/1000
2023-10-27 00:36:31.622 
Epoch 814/1000 
	 loss: 37.5164, MinusLogProbMetric: 37.5164, val_loss: 37.8877, val_MinusLogProbMetric: 37.8877

Epoch 814: val_loss improved from 37.88808 to 37.88768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.5164 - MinusLogProbMetric: 37.5164 - val_loss: 37.8877 - val_MinusLogProbMetric: 37.8877 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 815/1000
2023-10-27 00:37:37.875 
Epoch 815/1000 
	 loss: 37.4757, MinusLogProbMetric: 37.4757, val_loss: 37.9228, val_MinusLogProbMetric: 37.9228

Epoch 815: val_loss did not improve from 37.88768
196/196 - 65s - loss: 37.4757 - MinusLogProbMetric: 37.4757 - val_loss: 37.9228 - val_MinusLogProbMetric: 37.9228 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 816/1000
2023-10-27 00:38:43.571 
Epoch 816/1000 
	 loss: 37.5019, MinusLogProbMetric: 37.5019, val_loss: 37.8839, val_MinusLogProbMetric: 37.8839

Epoch 816: val_loss improved from 37.88768 to 37.88394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.5019 - MinusLogProbMetric: 37.5019 - val_loss: 37.8839 - val_MinusLogProbMetric: 37.8839 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 817/1000
2023-10-27 00:39:50.403 
Epoch 817/1000 
	 loss: 37.4603, MinusLogProbMetric: 37.4603, val_loss: 37.8913, val_MinusLogProbMetric: 37.8913

Epoch 817: val_loss did not improve from 37.88394
196/196 - 65s - loss: 37.4603 - MinusLogProbMetric: 37.4603 - val_loss: 37.8913 - val_MinusLogProbMetric: 37.8913 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 818/1000
2023-10-27 00:40:55.820 
Epoch 818/1000 
	 loss: 37.6502, MinusLogProbMetric: 37.6502, val_loss: 37.9930, val_MinusLogProbMetric: 37.9930

Epoch 818: val_loss did not improve from 37.88394
196/196 - 65s - loss: 37.6502 - MinusLogProbMetric: 37.6502 - val_loss: 37.9930 - val_MinusLogProbMetric: 37.9930 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 819/1000
2023-10-27 00:42:01.587 
Epoch 819/1000 
	 loss: 37.4622, MinusLogProbMetric: 37.4622, val_loss: 37.8830, val_MinusLogProbMetric: 37.8830

Epoch 819: val_loss improved from 37.88394 to 37.88302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.4622 - MinusLogProbMetric: 37.4622 - val_loss: 37.8830 - val_MinusLogProbMetric: 37.8830 - lr: 4.5725e-07 - 68s/epoch - 346ms/step
Epoch 820/1000
2023-10-27 00:43:09.086 
Epoch 820/1000 
	 loss: 37.4582, MinusLogProbMetric: 37.4582, val_loss: 37.9280, val_MinusLogProbMetric: 37.9280

Epoch 820: val_loss did not improve from 37.88302
196/196 - 66s - loss: 37.4582 - MinusLogProbMetric: 37.4582 - val_loss: 37.9280 - val_MinusLogProbMetric: 37.9280 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 821/1000
2023-10-27 00:44:14.592 
Epoch 821/1000 
	 loss: 37.4910, MinusLogProbMetric: 37.4910, val_loss: 37.9434, val_MinusLogProbMetric: 37.9434

Epoch 821: val_loss did not improve from 37.88302
196/196 - 66s - loss: 37.4910 - MinusLogProbMetric: 37.4910 - val_loss: 37.9434 - val_MinusLogProbMetric: 37.9434 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 822/1000
2023-10-27 00:45:19.625 
Epoch 822/1000 
	 loss: 37.7027, MinusLogProbMetric: 37.7027, val_loss: 37.9006, val_MinusLogProbMetric: 37.9006

Epoch 822: val_loss did not improve from 37.88302
196/196 - 65s - loss: 37.7027 - MinusLogProbMetric: 37.7027 - val_loss: 37.9006 - val_MinusLogProbMetric: 37.9006 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 823/1000
2023-10-27 00:46:24.858 
Epoch 823/1000 
	 loss: 37.4526, MinusLogProbMetric: 37.4526, val_loss: 37.8522, val_MinusLogProbMetric: 37.8522

Epoch 823: val_loss improved from 37.88302 to 37.85219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.4526 - MinusLogProbMetric: 37.4526 - val_loss: 37.8522 - val_MinusLogProbMetric: 37.8522 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 824/1000
2023-10-27 00:47:31.946 
Epoch 824/1000 
	 loss: 37.6286, MinusLogProbMetric: 37.6286, val_loss: 37.8490, val_MinusLogProbMetric: 37.8490

Epoch 824: val_loss improved from 37.85219 to 37.84897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.6286 - MinusLogProbMetric: 37.6286 - val_loss: 37.8490 - val_MinusLogProbMetric: 37.8490 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 825/1000
2023-10-27 00:48:38.079 
Epoch 825/1000 
	 loss: 37.4532, MinusLogProbMetric: 37.4532, val_loss: 37.9039, val_MinusLogProbMetric: 37.9039

Epoch 825: val_loss did not improve from 37.84897
196/196 - 65s - loss: 37.4532 - MinusLogProbMetric: 37.4532 - val_loss: 37.9039 - val_MinusLogProbMetric: 37.9039 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 826/1000
2023-10-27 00:49:43.228 
Epoch 826/1000 
	 loss: 37.4095, MinusLogProbMetric: 37.4095, val_loss: 37.9473, val_MinusLogProbMetric: 37.9473

Epoch 826: val_loss did not improve from 37.84897
196/196 - 65s - loss: 37.4095 - MinusLogProbMetric: 37.4095 - val_loss: 37.9473 - val_MinusLogProbMetric: 37.9473 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 827/1000
2023-10-27 00:50:48.764 
Epoch 827/1000 
	 loss: 37.4256, MinusLogProbMetric: 37.4256, val_loss: 37.8693, val_MinusLogProbMetric: 37.8693

Epoch 827: val_loss did not improve from 37.84897
196/196 - 66s - loss: 37.4256 - MinusLogProbMetric: 37.4256 - val_loss: 37.8693 - val_MinusLogProbMetric: 37.8693 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 828/1000
2023-10-27 00:51:53.192 
Epoch 828/1000 
	 loss: 37.5132, MinusLogProbMetric: 37.5132, val_loss: 37.8198, val_MinusLogProbMetric: 37.8198

Epoch 828: val_loss improved from 37.84897 to 37.81979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.5132 - MinusLogProbMetric: 37.5132 - val_loss: 37.8198 - val_MinusLogProbMetric: 37.8198 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 829/1000
2023-10-27 00:52:59.571 
Epoch 829/1000 
	 loss: 37.4121, MinusLogProbMetric: 37.4121, val_loss: 37.8977, val_MinusLogProbMetric: 37.8977

Epoch 829: val_loss did not improve from 37.81979
196/196 - 65s - loss: 37.4121 - MinusLogProbMetric: 37.4121 - val_loss: 37.8977 - val_MinusLogProbMetric: 37.8977 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 830/1000
2023-10-27 00:54:04.992 
Epoch 830/1000 
	 loss: 37.4952, MinusLogProbMetric: 37.4952, val_loss: 37.8110, val_MinusLogProbMetric: 37.8110

Epoch 830: val_loss improved from 37.81979 to 37.81098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.4952 - MinusLogProbMetric: 37.4952 - val_loss: 37.8110 - val_MinusLogProbMetric: 37.8110 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 831/1000
2023-10-27 00:55:10.935 
Epoch 831/1000 
	 loss: 37.4357, MinusLogProbMetric: 37.4357, val_loss: 37.8181, val_MinusLogProbMetric: 37.8181

Epoch 831: val_loss did not improve from 37.81098
196/196 - 65s - loss: 37.4357 - MinusLogProbMetric: 37.4357 - val_loss: 37.8181 - val_MinusLogProbMetric: 37.8181 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 832/1000
2023-10-27 00:56:15.893 
Epoch 832/1000 
	 loss: 37.4620, MinusLogProbMetric: 37.4620, val_loss: 37.8876, val_MinusLogProbMetric: 37.8876

Epoch 832: val_loss did not improve from 37.81098
196/196 - 65s - loss: 37.4620 - MinusLogProbMetric: 37.4620 - val_loss: 37.8876 - val_MinusLogProbMetric: 37.8876 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 833/1000
2023-10-27 00:57:21.020 
Epoch 833/1000 
	 loss: 37.4001, MinusLogProbMetric: 37.4001, val_loss: 37.8804, val_MinusLogProbMetric: 37.8804

Epoch 833: val_loss did not improve from 37.81098
196/196 - 65s - loss: 37.4001 - MinusLogProbMetric: 37.4001 - val_loss: 37.8804 - val_MinusLogProbMetric: 37.8804 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 834/1000
2023-10-27 00:58:25.576 
Epoch 834/1000 
	 loss: 37.5630, MinusLogProbMetric: 37.5630, val_loss: 37.9449, val_MinusLogProbMetric: 37.9449

Epoch 834: val_loss did not improve from 37.81098
196/196 - 65s - loss: 37.5630 - MinusLogProbMetric: 37.5630 - val_loss: 37.9449 - val_MinusLogProbMetric: 37.9449 - lr: 4.5725e-07 - 65s/epoch - 329ms/step
Epoch 835/1000
2023-10-27 00:59:30.644 
Epoch 835/1000 
	 loss: 37.4543, MinusLogProbMetric: 37.4543, val_loss: 37.8475, val_MinusLogProbMetric: 37.8475

Epoch 835: val_loss did not improve from 37.81098
196/196 - 65s - loss: 37.4543 - MinusLogProbMetric: 37.4543 - val_loss: 37.8475 - val_MinusLogProbMetric: 37.8475 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 836/1000
2023-10-27 01:00:36.420 
Epoch 836/1000 
	 loss: 37.3788, MinusLogProbMetric: 37.3788, val_loss: 37.8038, val_MinusLogProbMetric: 37.8038

Epoch 836: val_loss improved from 37.81098 to 37.80383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.3788 - MinusLogProbMetric: 37.3788 - val_loss: 37.8038 - val_MinusLogProbMetric: 37.8038 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 837/1000
2023-10-27 01:01:43.171 
Epoch 837/1000 
	 loss: 37.4448, MinusLogProbMetric: 37.4448, val_loss: 37.7957, val_MinusLogProbMetric: 37.7957

Epoch 837: val_loss improved from 37.80383 to 37.79571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.4448 - MinusLogProbMetric: 37.4448 - val_loss: 37.7957 - val_MinusLogProbMetric: 37.7957 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 838/1000
2023-10-27 01:02:49.800 
Epoch 838/1000 
	 loss: 37.4574, MinusLogProbMetric: 37.4574, val_loss: 37.7926, val_MinusLogProbMetric: 37.7926

Epoch 838: val_loss improved from 37.79571 to 37.79255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.4574 - MinusLogProbMetric: 37.4574 - val_loss: 37.7926 - val_MinusLogProbMetric: 37.7926 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 839/1000
2023-10-27 01:03:55.679 
Epoch 839/1000 
	 loss: 37.3900, MinusLogProbMetric: 37.3900, val_loss: 37.8244, val_MinusLogProbMetric: 37.8244

Epoch 839: val_loss did not improve from 37.79255
196/196 - 65s - loss: 37.3900 - MinusLogProbMetric: 37.3900 - val_loss: 37.8244 - val_MinusLogProbMetric: 37.8244 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 840/1000
2023-10-27 01:05:00.698 
Epoch 840/1000 
	 loss: 37.3612, MinusLogProbMetric: 37.3612, val_loss: 37.8982, val_MinusLogProbMetric: 37.8982

Epoch 840: val_loss did not improve from 37.79255
196/196 - 65s - loss: 37.3612 - MinusLogProbMetric: 37.3612 - val_loss: 37.8982 - val_MinusLogProbMetric: 37.8982 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 841/1000
2023-10-27 01:06:06.858 
Epoch 841/1000 
	 loss: 37.3706, MinusLogProbMetric: 37.3706, val_loss: 37.8055, val_MinusLogProbMetric: 37.8055

Epoch 841: val_loss did not improve from 37.79255
196/196 - 66s - loss: 37.3706 - MinusLogProbMetric: 37.3706 - val_loss: 37.8055 - val_MinusLogProbMetric: 37.8055 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 842/1000
2023-10-27 01:07:12.547 
Epoch 842/1000 
	 loss: 37.3633, MinusLogProbMetric: 37.3633, val_loss: 37.7531, val_MinusLogProbMetric: 37.7531

Epoch 842: val_loss improved from 37.79255 to 37.75309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.3633 - MinusLogProbMetric: 37.3633 - val_loss: 37.7531 - val_MinusLogProbMetric: 37.7531 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 843/1000
2023-10-27 01:08:19.485 
Epoch 843/1000 
	 loss: 37.3416, MinusLogProbMetric: 37.3416, val_loss: 37.8267, val_MinusLogProbMetric: 37.8267

Epoch 843: val_loss did not improve from 37.75309
196/196 - 66s - loss: 37.3416 - MinusLogProbMetric: 37.3416 - val_loss: 37.8267 - val_MinusLogProbMetric: 37.8267 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 844/1000
2023-10-27 01:09:24.820 
Epoch 844/1000 
	 loss: 37.4864, MinusLogProbMetric: 37.4864, val_loss: 38.2286, val_MinusLogProbMetric: 38.2286

Epoch 844: val_loss did not improve from 37.75309
196/196 - 65s - loss: 37.4864 - MinusLogProbMetric: 37.4864 - val_loss: 38.2286 - val_MinusLogProbMetric: 38.2286 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 845/1000
2023-10-27 01:10:30.260 
Epoch 845/1000 
	 loss: 37.4678, MinusLogProbMetric: 37.4678, val_loss: 37.7749, val_MinusLogProbMetric: 37.7749

Epoch 845: val_loss did not improve from 37.75309
196/196 - 65s - loss: 37.4678 - MinusLogProbMetric: 37.4678 - val_loss: 37.7749 - val_MinusLogProbMetric: 37.7749 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 846/1000
2023-10-27 01:11:35.696 
Epoch 846/1000 
	 loss: 37.3359, MinusLogProbMetric: 37.3359, val_loss: 37.8073, val_MinusLogProbMetric: 37.8073

Epoch 846: val_loss did not improve from 37.75309
196/196 - 65s - loss: 37.3359 - MinusLogProbMetric: 37.3359 - val_loss: 37.8073 - val_MinusLogProbMetric: 37.8073 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 847/1000
2023-10-27 01:12:41.382 
Epoch 847/1000 
	 loss: 37.3290, MinusLogProbMetric: 37.3290, val_loss: 37.8046, val_MinusLogProbMetric: 37.8046

Epoch 847: val_loss did not improve from 37.75309
196/196 - 66s - loss: 37.3290 - MinusLogProbMetric: 37.3290 - val_loss: 37.8046 - val_MinusLogProbMetric: 37.8046 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 848/1000
2023-10-27 01:13:46.953 
Epoch 848/1000 
	 loss: 37.3724, MinusLogProbMetric: 37.3724, val_loss: 37.8004, val_MinusLogProbMetric: 37.8004

Epoch 848: val_loss did not improve from 37.75309
196/196 - 66s - loss: 37.3724 - MinusLogProbMetric: 37.3724 - val_loss: 37.8004 - val_MinusLogProbMetric: 37.8004 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 849/1000
2023-10-27 01:14:52.069 
Epoch 849/1000 
	 loss: 37.3533, MinusLogProbMetric: 37.3533, val_loss: 37.7347, val_MinusLogProbMetric: 37.7347

Epoch 849: val_loss improved from 37.75309 to 37.73475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.3533 - MinusLogProbMetric: 37.3533 - val_loss: 37.7347 - val_MinusLogProbMetric: 37.7347 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 850/1000
2023-10-27 01:15:58.215 
Epoch 850/1000 
	 loss: 37.3808, MinusLogProbMetric: 37.3808, val_loss: 37.8604, val_MinusLogProbMetric: 37.8604

Epoch 850: val_loss did not improve from 37.73475
196/196 - 65s - loss: 37.3808 - MinusLogProbMetric: 37.3808 - val_loss: 37.8604 - val_MinusLogProbMetric: 37.8604 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 851/1000
2023-10-27 01:17:03.396 
Epoch 851/1000 
	 loss: 37.3716, MinusLogProbMetric: 37.3716, val_loss: 37.7305, val_MinusLogProbMetric: 37.7305

Epoch 851: val_loss improved from 37.73475 to 37.73052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.3716 - MinusLogProbMetric: 37.3716 - val_loss: 37.7305 - val_MinusLogProbMetric: 37.7305 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 852/1000
2023-10-27 01:18:10.121 
Epoch 852/1000 
	 loss: 37.3230, MinusLogProbMetric: 37.3230, val_loss: 37.7771, val_MinusLogProbMetric: 37.7771

Epoch 852: val_loss did not improve from 37.73052
196/196 - 66s - loss: 37.3230 - MinusLogProbMetric: 37.3230 - val_loss: 37.7771 - val_MinusLogProbMetric: 37.7771 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 853/1000
2023-10-27 01:19:15.445 
Epoch 853/1000 
	 loss: 37.3977, MinusLogProbMetric: 37.3977, val_loss: 37.6925, val_MinusLogProbMetric: 37.6925

Epoch 853: val_loss improved from 37.73052 to 37.69252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.3977 - MinusLogProbMetric: 37.3977 - val_loss: 37.6925 - val_MinusLogProbMetric: 37.6925 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 854/1000
2023-10-27 01:20:22.151 
Epoch 854/1000 
	 loss: 37.7577, MinusLogProbMetric: 37.7577, val_loss: 37.9341, val_MinusLogProbMetric: 37.9341

Epoch 854: val_loss did not improve from 37.69252
196/196 - 66s - loss: 37.7577 - MinusLogProbMetric: 37.7577 - val_loss: 37.9341 - val_MinusLogProbMetric: 37.9341 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 855/1000
2023-10-27 01:21:27.317 
Epoch 855/1000 
	 loss: 37.3700, MinusLogProbMetric: 37.3700, val_loss: 37.7218, val_MinusLogProbMetric: 37.7218

Epoch 855: val_loss did not improve from 37.69252
196/196 - 65s - loss: 37.3700 - MinusLogProbMetric: 37.3700 - val_loss: 37.7218 - val_MinusLogProbMetric: 37.7218 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 856/1000
2023-10-27 01:22:33.093 
Epoch 856/1000 
	 loss: 37.3576, MinusLogProbMetric: 37.3576, val_loss: 37.7305, val_MinusLogProbMetric: 37.7305

Epoch 856: val_loss did not improve from 37.69252
196/196 - 66s - loss: 37.3576 - MinusLogProbMetric: 37.3576 - val_loss: 37.7305 - val_MinusLogProbMetric: 37.7305 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 857/1000
2023-10-27 01:23:38.051 
Epoch 857/1000 
	 loss: 37.2979, MinusLogProbMetric: 37.2979, val_loss: 37.7059, val_MinusLogProbMetric: 37.7059

Epoch 857: val_loss did not improve from 37.69252
196/196 - 65s - loss: 37.2979 - MinusLogProbMetric: 37.2979 - val_loss: 37.7059 - val_MinusLogProbMetric: 37.7059 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 858/1000
2023-10-27 01:24:44.540 
Epoch 858/1000 
	 loss: 37.5969, MinusLogProbMetric: 37.5969, val_loss: 37.7548, val_MinusLogProbMetric: 37.7548

Epoch 858: val_loss did not improve from 37.69252
196/196 - 66s - loss: 37.5969 - MinusLogProbMetric: 37.5969 - val_loss: 37.7548 - val_MinusLogProbMetric: 37.7548 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 859/1000
2023-10-27 01:25:49.855 
Epoch 859/1000 
	 loss: 37.3383, MinusLogProbMetric: 37.3383, val_loss: 37.6864, val_MinusLogProbMetric: 37.6864

Epoch 859: val_loss improved from 37.69252 to 37.68644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.3383 - MinusLogProbMetric: 37.3383 - val_loss: 37.6864 - val_MinusLogProbMetric: 37.6864 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 860/1000
2023-10-27 01:26:55.762 
Epoch 860/1000 
	 loss: 37.2715, MinusLogProbMetric: 37.2715, val_loss: 37.8151, val_MinusLogProbMetric: 37.8151

Epoch 860: val_loss did not improve from 37.68644
196/196 - 65s - loss: 37.2715 - MinusLogProbMetric: 37.2715 - val_loss: 37.8151 - val_MinusLogProbMetric: 37.8151 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 861/1000
2023-10-27 01:28:01.793 
Epoch 861/1000 
	 loss: 37.3396, MinusLogProbMetric: 37.3396, val_loss: 37.7350, val_MinusLogProbMetric: 37.7350

Epoch 861: val_loss did not improve from 37.68644
196/196 - 66s - loss: 37.3396 - MinusLogProbMetric: 37.3396 - val_loss: 37.7350 - val_MinusLogProbMetric: 37.7350 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 862/1000
2023-10-27 01:29:07.500 
Epoch 862/1000 
	 loss: 37.2824, MinusLogProbMetric: 37.2824, val_loss: 37.7096, val_MinusLogProbMetric: 37.7096

Epoch 862: val_loss did not improve from 37.68644
196/196 - 66s - loss: 37.2824 - MinusLogProbMetric: 37.2824 - val_loss: 37.7096 - val_MinusLogProbMetric: 37.7096 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 863/1000
2023-10-27 01:30:13.653 
Epoch 863/1000 
	 loss: 37.4208, MinusLogProbMetric: 37.4208, val_loss: 38.1750, val_MinusLogProbMetric: 38.1750

Epoch 863: val_loss did not improve from 37.68644
196/196 - 66s - loss: 37.4208 - MinusLogProbMetric: 37.4208 - val_loss: 38.1750 - val_MinusLogProbMetric: 38.1750 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 864/1000
2023-10-27 01:31:18.579 
Epoch 864/1000 
	 loss: 37.3432, MinusLogProbMetric: 37.3432, val_loss: 37.6848, val_MinusLogProbMetric: 37.6848

Epoch 864: val_loss improved from 37.68644 to 37.68484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.3432 - MinusLogProbMetric: 37.3432 - val_loss: 37.6848 - val_MinusLogProbMetric: 37.6848 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 865/1000
2023-10-27 01:32:24.919 
Epoch 865/1000 
	 loss: 37.2656, MinusLogProbMetric: 37.2656, val_loss: 37.7237, val_MinusLogProbMetric: 37.7237

Epoch 865: val_loss did not improve from 37.68484
196/196 - 65s - loss: 37.2656 - MinusLogProbMetric: 37.2656 - val_loss: 37.7237 - val_MinusLogProbMetric: 37.7237 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 866/1000
2023-10-27 01:33:29.930 
Epoch 866/1000 
	 loss: 37.2773, MinusLogProbMetric: 37.2773, val_loss: 37.6922, val_MinusLogProbMetric: 37.6922

Epoch 866: val_loss did not improve from 37.68484
196/196 - 65s - loss: 37.2773 - MinusLogProbMetric: 37.2773 - val_loss: 37.6922 - val_MinusLogProbMetric: 37.6922 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 867/1000
2023-10-27 01:34:35.195 
Epoch 867/1000 
	 loss: 37.2645, MinusLogProbMetric: 37.2645, val_loss: 37.7694, val_MinusLogProbMetric: 37.7694

Epoch 867: val_loss did not improve from 37.68484
196/196 - 65s - loss: 37.2645 - MinusLogProbMetric: 37.2645 - val_loss: 37.7694 - val_MinusLogProbMetric: 37.7694 - lr: 4.5725e-07 - 65s/epoch - 333ms/step
Epoch 868/1000
2023-10-27 01:35:40.573 
Epoch 868/1000 
	 loss: 37.3795, MinusLogProbMetric: 37.3795, val_loss: 37.6461, val_MinusLogProbMetric: 37.6461

Epoch 868: val_loss improved from 37.68484 to 37.64608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 37.3795 - MinusLogProbMetric: 37.3795 - val_loss: 37.6461 - val_MinusLogProbMetric: 37.6461 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 869/1000
2023-10-27 01:36:47.420 
Epoch 869/1000 
	 loss: 37.2559, MinusLogProbMetric: 37.2559, val_loss: 37.6500, val_MinusLogProbMetric: 37.6500

Epoch 869: val_loss did not improve from 37.64608
196/196 - 66s - loss: 37.2559 - MinusLogProbMetric: 37.2559 - val_loss: 37.6500 - val_MinusLogProbMetric: 37.6500 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 870/1000
2023-10-27 01:37:53.676 
Epoch 870/1000 
	 loss: 37.2371, MinusLogProbMetric: 37.2371, val_loss: 37.8928, val_MinusLogProbMetric: 37.8928

Epoch 870: val_loss did not improve from 37.64608
196/196 - 66s - loss: 37.2371 - MinusLogProbMetric: 37.2371 - val_loss: 37.8928 - val_MinusLogProbMetric: 37.8928 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 871/1000
2023-10-27 01:38:59.873 
Epoch 871/1000 
	 loss: 37.3267, MinusLogProbMetric: 37.3267, val_loss: 37.7660, val_MinusLogProbMetric: 37.7660

Epoch 871: val_loss did not improve from 37.64608
196/196 - 66s - loss: 37.3267 - MinusLogProbMetric: 37.3267 - val_loss: 37.7660 - val_MinusLogProbMetric: 37.7660 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 872/1000
2023-10-27 01:40:05.648 
Epoch 872/1000 
	 loss: 37.2698, MinusLogProbMetric: 37.2698, val_loss: 37.6521, val_MinusLogProbMetric: 37.6521

Epoch 872: val_loss did not improve from 37.64608
196/196 - 66s - loss: 37.2698 - MinusLogProbMetric: 37.2698 - val_loss: 37.6521 - val_MinusLogProbMetric: 37.6521 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 873/1000
2023-10-27 01:41:11.775 
Epoch 873/1000 
	 loss: 37.3580, MinusLogProbMetric: 37.3580, val_loss: 37.6357, val_MinusLogProbMetric: 37.6357

Epoch 873: val_loss improved from 37.64608 to 37.63573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.3580 - MinusLogProbMetric: 37.3580 - val_loss: 37.6357 - val_MinusLogProbMetric: 37.6357 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 874/1000
2023-10-27 01:42:19.043 
Epoch 874/1000 
	 loss: 37.2287, MinusLogProbMetric: 37.2287, val_loss: 37.7142, val_MinusLogProbMetric: 37.7142

Epoch 874: val_loss did not improve from 37.63573
196/196 - 66s - loss: 37.2287 - MinusLogProbMetric: 37.2287 - val_loss: 37.7142 - val_MinusLogProbMetric: 37.7142 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 875/1000
2023-10-27 01:43:25.750 
Epoch 875/1000 
	 loss: 37.3197, MinusLogProbMetric: 37.3197, val_loss: 37.6982, val_MinusLogProbMetric: 37.6982

Epoch 875: val_loss did not improve from 37.63573
196/196 - 67s - loss: 37.3197 - MinusLogProbMetric: 37.3197 - val_loss: 37.6982 - val_MinusLogProbMetric: 37.6982 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 876/1000
2023-10-27 01:44:31.713 
Epoch 876/1000 
	 loss: 37.3920, MinusLogProbMetric: 37.3920, val_loss: 37.6296, val_MinusLogProbMetric: 37.6296

Epoch 876: val_loss improved from 37.63573 to 37.62960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.3920 - MinusLogProbMetric: 37.3920 - val_loss: 37.6296 - val_MinusLogProbMetric: 37.6296 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 877/1000
2023-10-27 01:45:37.677 
Epoch 877/1000 
	 loss: 37.2316, MinusLogProbMetric: 37.2316, val_loss: 37.7676, val_MinusLogProbMetric: 37.7676

Epoch 877: val_loss did not improve from 37.62960
196/196 - 65s - loss: 37.2316 - MinusLogProbMetric: 37.2316 - val_loss: 37.7676 - val_MinusLogProbMetric: 37.7676 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 878/1000
2023-10-27 01:46:43.111 
Epoch 878/1000 
	 loss: 37.2233, MinusLogProbMetric: 37.2233, val_loss: 37.6646, val_MinusLogProbMetric: 37.6646

Epoch 878: val_loss did not improve from 37.62960
196/196 - 65s - loss: 37.2233 - MinusLogProbMetric: 37.2233 - val_loss: 37.6646 - val_MinusLogProbMetric: 37.6646 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 879/1000
2023-10-27 01:47:45.560 
Epoch 879/1000 
	 loss: 37.7053, MinusLogProbMetric: 37.7053, val_loss: 38.1023, val_MinusLogProbMetric: 38.1023

Epoch 879: val_loss did not improve from 37.62960
196/196 - 62s - loss: 37.7053 - MinusLogProbMetric: 37.7053 - val_loss: 38.1023 - val_MinusLogProbMetric: 38.1023 - lr: 4.5725e-07 - 62s/epoch - 319ms/step
Epoch 880/1000
2023-10-27 01:48:51.150 
Epoch 880/1000 
	 loss: 37.3052, MinusLogProbMetric: 37.3052, val_loss: 37.6164, val_MinusLogProbMetric: 37.6164

Epoch 880: val_loss improved from 37.62960 to 37.61636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.3052 - MinusLogProbMetric: 37.3052 - val_loss: 37.6164 - val_MinusLogProbMetric: 37.6164 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 881/1000
2023-10-27 01:49:58.710 
Epoch 881/1000 
	 loss: 37.2023, MinusLogProbMetric: 37.2023, val_loss: 37.6423, val_MinusLogProbMetric: 37.6423

Epoch 881: val_loss did not improve from 37.61636
196/196 - 66s - loss: 37.2023 - MinusLogProbMetric: 37.2023 - val_loss: 37.6423 - val_MinusLogProbMetric: 37.6423 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 882/1000
2023-10-27 01:51:05.168 
Epoch 882/1000 
	 loss: 37.1990, MinusLogProbMetric: 37.1990, val_loss: 37.6418, val_MinusLogProbMetric: 37.6418

Epoch 882: val_loss did not improve from 37.61636
196/196 - 66s - loss: 37.1990 - MinusLogProbMetric: 37.1990 - val_loss: 37.6418 - val_MinusLogProbMetric: 37.6418 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 883/1000
2023-10-27 01:52:11.646 
Epoch 883/1000 
	 loss: 37.2099, MinusLogProbMetric: 37.2099, val_loss: 37.9423, val_MinusLogProbMetric: 37.9423

Epoch 883: val_loss did not improve from 37.61636
196/196 - 66s - loss: 37.2099 - MinusLogProbMetric: 37.2099 - val_loss: 37.9423 - val_MinusLogProbMetric: 37.9423 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 884/1000
2023-10-27 01:53:18.480 
Epoch 884/1000 
	 loss: 37.6263, MinusLogProbMetric: 37.6263, val_loss: 37.6396, val_MinusLogProbMetric: 37.6396

Epoch 884: val_loss did not improve from 37.61636
196/196 - 67s - loss: 37.6263 - MinusLogProbMetric: 37.6263 - val_loss: 37.6396 - val_MinusLogProbMetric: 37.6396 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 885/1000
2023-10-27 01:54:24.954 
Epoch 885/1000 
	 loss: 37.1948, MinusLogProbMetric: 37.1948, val_loss: 37.7092, val_MinusLogProbMetric: 37.7092

Epoch 885: val_loss did not improve from 37.61636
196/196 - 66s - loss: 37.1948 - MinusLogProbMetric: 37.1948 - val_loss: 37.7092 - val_MinusLogProbMetric: 37.7092 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 886/1000
2023-10-27 01:55:31.235 
Epoch 886/1000 
	 loss: 37.1827, MinusLogProbMetric: 37.1827, val_loss: 37.6195, val_MinusLogProbMetric: 37.6195

Epoch 886: val_loss did not improve from 37.61636
196/196 - 66s - loss: 37.1827 - MinusLogProbMetric: 37.1827 - val_loss: 37.6195 - val_MinusLogProbMetric: 37.6195 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 887/1000
2023-10-27 01:56:37.182 
Epoch 887/1000 
	 loss: 37.1863, MinusLogProbMetric: 37.1863, val_loss: 37.6056, val_MinusLogProbMetric: 37.6056

Epoch 887: val_loss improved from 37.61636 to 37.60560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.1863 - MinusLogProbMetric: 37.1863 - val_loss: 37.6056 - val_MinusLogProbMetric: 37.6056 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 888/1000
2023-10-27 01:57:44.310 
Epoch 888/1000 
	 loss: 37.1711, MinusLogProbMetric: 37.1711, val_loss: 37.6184, val_MinusLogProbMetric: 37.6184

Epoch 888: val_loss did not improve from 37.60560
196/196 - 66s - loss: 37.1711 - MinusLogProbMetric: 37.1711 - val_loss: 37.6184 - val_MinusLogProbMetric: 37.6184 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 889/1000
2023-10-27 01:58:51.159 
Epoch 889/1000 
	 loss: 37.8217, MinusLogProbMetric: 37.8217, val_loss: 37.6612, val_MinusLogProbMetric: 37.6612

Epoch 889: val_loss did not improve from 37.60560
196/196 - 67s - loss: 37.8217 - MinusLogProbMetric: 37.8217 - val_loss: 37.6612 - val_MinusLogProbMetric: 37.6612 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 890/1000
2023-10-27 01:59:58.083 
Epoch 890/1000 
	 loss: 37.1941, MinusLogProbMetric: 37.1941, val_loss: 37.5840, val_MinusLogProbMetric: 37.5840

Epoch 890: val_loss improved from 37.60560 to 37.58401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.1941 - MinusLogProbMetric: 37.1941 - val_loss: 37.5840 - val_MinusLogProbMetric: 37.5840 - lr: 4.5725e-07 - 68s/epoch - 347ms/step
Epoch 891/1000
2023-10-27 02:01:05.153 
Epoch 891/1000 
	 loss: 37.1683, MinusLogProbMetric: 37.1683, val_loss: 37.5754, val_MinusLogProbMetric: 37.5754

Epoch 891: val_loss improved from 37.58401 to 37.57537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.1683 - MinusLogProbMetric: 37.1683 - val_loss: 37.5754 - val_MinusLogProbMetric: 37.5754 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 892/1000
2023-10-27 02:02:12.812 
Epoch 892/1000 
	 loss: 37.1770, MinusLogProbMetric: 37.1770, val_loss: 37.5423, val_MinusLogProbMetric: 37.5423

Epoch 892: val_loss improved from 37.57537 to 37.54232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.1770 - MinusLogProbMetric: 37.1770 - val_loss: 37.5423 - val_MinusLogProbMetric: 37.5423 - lr: 4.5725e-07 - 68s/epoch - 345ms/step
Epoch 893/1000
2023-10-27 02:03:19.939 
Epoch 893/1000 
	 loss: 37.1355, MinusLogProbMetric: 37.1355, val_loss: 37.5805, val_MinusLogProbMetric: 37.5805

Epoch 893: val_loss did not improve from 37.54232
196/196 - 66s - loss: 37.1355 - MinusLogProbMetric: 37.1355 - val_loss: 37.5805 - val_MinusLogProbMetric: 37.5805 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 894/1000
2023-10-27 02:04:19.614 
Epoch 894/1000 
	 loss: 37.1711, MinusLogProbMetric: 37.1711, val_loss: 37.6426, val_MinusLogProbMetric: 37.6426

Epoch 894: val_loss did not improve from 37.54232
196/196 - 60s - loss: 37.1711 - MinusLogProbMetric: 37.1711 - val_loss: 37.6426 - val_MinusLogProbMetric: 37.6426 - lr: 4.5725e-07 - 60s/epoch - 304ms/step
Epoch 895/1000
2023-10-27 02:05:25.798 
Epoch 895/1000 
	 loss: 37.1885, MinusLogProbMetric: 37.1885, val_loss: 37.5797, val_MinusLogProbMetric: 37.5797

Epoch 895: val_loss did not improve from 37.54232
196/196 - 66s - loss: 37.1885 - MinusLogProbMetric: 37.1885 - val_loss: 37.5797 - val_MinusLogProbMetric: 37.5797 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 896/1000
2023-10-27 02:06:31.862 
Epoch 896/1000 
	 loss: 37.1392, MinusLogProbMetric: 37.1392, val_loss: 37.5409, val_MinusLogProbMetric: 37.5409

Epoch 896: val_loss improved from 37.54232 to 37.54091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.1392 - MinusLogProbMetric: 37.1392 - val_loss: 37.5409 - val_MinusLogProbMetric: 37.5409 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 897/1000
2023-10-27 02:07:39.253 
Epoch 897/1000 
	 loss: 37.1302, MinusLogProbMetric: 37.1302, val_loss: 37.6062, val_MinusLogProbMetric: 37.6062

Epoch 897: val_loss did not improve from 37.54091
196/196 - 66s - loss: 37.1302 - MinusLogProbMetric: 37.1302 - val_loss: 37.6062 - val_MinusLogProbMetric: 37.6062 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 898/1000
2023-10-27 02:08:45.171 
Epoch 898/1000 
	 loss: 37.3257, MinusLogProbMetric: 37.3257, val_loss: 37.5889, val_MinusLogProbMetric: 37.5889

Epoch 898: val_loss did not improve from 37.54091
196/196 - 66s - loss: 37.3257 - MinusLogProbMetric: 37.3257 - val_loss: 37.5889 - val_MinusLogProbMetric: 37.5889 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 899/1000
2023-10-27 02:09:51.758 
Epoch 899/1000 
	 loss: 37.1710, MinusLogProbMetric: 37.1710, val_loss: 37.5791, val_MinusLogProbMetric: 37.5791

Epoch 899: val_loss did not improve from 37.54091
196/196 - 67s - loss: 37.1710 - MinusLogProbMetric: 37.1710 - val_loss: 37.5791 - val_MinusLogProbMetric: 37.5791 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 900/1000
2023-10-27 02:10:58.032 
Epoch 900/1000 
	 loss: 37.6275, MinusLogProbMetric: 37.6275, val_loss: 37.5879, val_MinusLogProbMetric: 37.5879

Epoch 900: val_loss did not improve from 37.54091
196/196 - 66s - loss: 37.6275 - MinusLogProbMetric: 37.6275 - val_loss: 37.5879 - val_MinusLogProbMetric: 37.5879 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 901/1000
2023-10-27 02:12:04.958 
Epoch 901/1000 
	 loss: 37.1541, MinusLogProbMetric: 37.1541, val_loss: 37.5321, val_MinusLogProbMetric: 37.5321

Epoch 901: val_loss improved from 37.54091 to 37.53214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 68s - loss: 37.1541 - MinusLogProbMetric: 37.1541 - val_loss: 37.5321 - val_MinusLogProbMetric: 37.5321 - lr: 4.5725e-07 - 68s/epoch - 347ms/step
Epoch 902/1000
2023-10-27 02:13:12.125 
Epoch 902/1000 
	 loss: 37.1158, MinusLogProbMetric: 37.1158, val_loss: 37.5463, val_MinusLogProbMetric: 37.5463

Epoch 902: val_loss did not improve from 37.53214
196/196 - 66s - loss: 37.1158 - MinusLogProbMetric: 37.1158 - val_loss: 37.5463 - val_MinusLogProbMetric: 37.5463 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 903/1000
2023-10-27 02:14:18.610 
Epoch 903/1000 
	 loss: 37.1946, MinusLogProbMetric: 37.1946, val_loss: 37.5468, val_MinusLogProbMetric: 37.5468

Epoch 903: val_loss did not improve from 37.53214
196/196 - 66s - loss: 37.1946 - MinusLogProbMetric: 37.1946 - val_loss: 37.5468 - val_MinusLogProbMetric: 37.5468 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 904/1000
2023-10-27 02:15:24.597 
Epoch 904/1000 
	 loss: 37.2150, MinusLogProbMetric: 37.2150, val_loss: 37.5145, val_MinusLogProbMetric: 37.5145

Epoch 904: val_loss improved from 37.53214 to 37.51453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.2150 - MinusLogProbMetric: 37.2150 - val_loss: 37.5145 - val_MinusLogProbMetric: 37.5145 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 905/1000
2023-10-27 02:16:31.797 
Epoch 905/1000 
	 loss: 37.1107, MinusLogProbMetric: 37.1107, val_loss: 37.6011, val_MinusLogProbMetric: 37.6011

Epoch 905: val_loss did not improve from 37.51453
196/196 - 66s - loss: 37.1107 - MinusLogProbMetric: 37.1107 - val_loss: 37.6011 - val_MinusLogProbMetric: 37.6011 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 906/1000
2023-10-27 02:17:37.433 
Epoch 906/1000 
	 loss: 37.1509, MinusLogProbMetric: 37.1509, val_loss: 37.5325, val_MinusLogProbMetric: 37.5325

Epoch 906: val_loss did not improve from 37.51453
196/196 - 66s - loss: 37.1509 - MinusLogProbMetric: 37.1509 - val_loss: 37.5325 - val_MinusLogProbMetric: 37.5325 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 907/1000
2023-10-27 02:18:43.244 
Epoch 907/1000 
	 loss: 37.1022, MinusLogProbMetric: 37.1022, val_loss: 37.5230, val_MinusLogProbMetric: 37.5230

Epoch 907: val_loss did not improve from 37.51453
196/196 - 66s - loss: 37.1022 - MinusLogProbMetric: 37.1022 - val_loss: 37.5230 - val_MinusLogProbMetric: 37.5230 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 908/1000
2023-10-27 02:19:48.688 
Epoch 908/1000 
	 loss: 37.1801, MinusLogProbMetric: 37.1801, val_loss: 37.5398, val_MinusLogProbMetric: 37.5398

Epoch 908: val_loss did not improve from 37.51453
196/196 - 65s - loss: 37.1801 - MinusLogProbMetric: 37.1801 - val_loss: 37.5398 - val_MinusLogProbMetric: 37.5398 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 909/1000
2023-10-27 02:20:54.546 
Epoch 909/1000 
	 loss: 37.1023, MinusLogProbMetric: 37.1023, val_loss: 37.4813, val_MinusLogProbMetric: 37.4813

Epoch 909: val_loss improved from 37.51453 to 37.48135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.1023 - MinusLogProbMetric: 37.1023 - val_loss: 37.4813 - val_MinusLogProbMetric: 37.4813 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 910/1000
2023-10-27 02:22:01.958 
Epoch 910/1000 
	 loss: 37.0685, MinusLogProbMetric: 37.0685, val_loss: 37.4923, val_MinusLogProbMetric: 37.4923

Epoch 910: val_loss did not improve from 37.48135
196/196 - 66s - loss: 37.0685 - MinusLogProbMetric: 37.0685 - val_loss: 37.4923 - val_MinusLogProbMetric: 37.4923 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 911/1000
2023-10-27 02:23:07.922 
Epoch 911/1000 
	 loss: 37.0881, MinusLogProbMetric: 37.0881, val_loss: 37.5563, val_MinusLogProbMetric: 37.5563

Epoch 911: val_loss did not improve from 37.48135
196/196 - 66s - loss: 37.0881 - MinusLogProbMetric: 37.0881 - val_loss: 37.5563 - val_MinusLogProbMetric: 37.5563 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 912/1000
2023-10-27 02:24:13.572 
Epoch 912/1000 
	 loss: 37.1781, MinusLogProbMetric: 37.1781, val_loss: 37.5634, val_MinusLogProbMetric: 37.5634

Epoch 912: val_loss did not improve from 37.48135
196/196 - 66s - loss: 37.1781 - MinusLogProbMetric: 37.1781 - val_loss: 37.5634 - val_MinusLogProbMetric: 37.5634 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 913/1000
2023-10-27 02:25:19.225 
Epoch 913/1000 
	 loss: 37.0692, MinusLogProbMetric: 37.0692, val_loss: 37.4899, val_MinusLogProbMetric: 37.4899

Epoch 913: val_loss did not improve from 37.48135
196/196 - 66s - loss: 37.0692 - MinusLogProbMetric: 37.0692 - val_loss: 37.4899 - val_MinusLogProbMetric: 37.4899 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 914/1000
2023-10-27 02:26:25.952 
Epoch 914/1000 
	 loss: 37.4673, MinusLogProbMetric: 37.4673, val_loss: 37.8029, val_MinusLogProbMetric: 37.8029

Epoch 914: val_loss did not improve from 37.48135
196/196 - 67s - loss: 37.4673 - MinusLogProbMetric: 37.4673 - val_loss: 37.8029 - val_MinusLogProbMetric: 37.8029 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 915/1000
2023-10-27 02:27:32.408 
Epoch 915/1000 
	 loss: 37.1686, MinusLogProbMetric: 37.1686, val_loss: 37.6499, val_MinusLogProbMetric: 37.6499

Epoch 915: val_loss did not improve from 37.48135
196/196 - 66s - loss: 37.1686 - MinusLogProbMetric: 37.1686 - val_loss: 37.6499 - val_MinusLogProbMetric: 37.6499 - lr: 4.5725e-07 - 66s/epoch - 339ms/step
Epoch 916/1000
2023-10-27 02:28:38.344 
Epoch 916/1000 
	 loss: 37.1068, MinusLogProbMetric: 37.1068, val_loss: 37.4575, val_MinusLogProbMetric: 37.4575

Epoch 916: val_loss improved from 37.48135 to 37.45752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.1068 - MinusLogProbMetric: 37.1068 - val_loss: 37.4575 - val_MinusLogProbMetric: 37.4575 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 917/1000
2023-10-27 02:29:45.850 
Epoch 917/1000 
	 loss: 37.1727, MinusLogProbMetric: 37.1727, val_loss: 37.4856, val_MinusLogProbMetric: 37.4856

Epoch 917: val_loss did not improve from 37.45752
196/196 - 66s - loss: 37.1727 - MinusLogProbMetric: 37.1727 - val_loss: 37.4856 - val_MinusLogProbMetric: 37.4856 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 918/1000
2023-10-27 02:30:52.156 
Epoch 918/1000 
	 loss: 37.0551, MinusLogProbMetric: 37.0551, val_loss: 37.5131, val_MinusLogProbMetric: 37.5131

Epoch 918: val_loss did not improve from 37.45752
196/196 - 66s - loss: 37.0551 - MinusLogProbMetric: 37.0551 - val_loss: 37.5131 - val_MinusLogProbMetric: 37.5131 - lr: 4.5725e-07 - 66s/epoch - 338ms/step
Epoch 919/1000
2023-10-27 02:31:57.854 
Epoch 919/1000 
	 loss: 37.0572, MinusLogProbMetric: 37.0572, val_loss: 37.6884, val_MinusLogProbMetric: 37.6884

Epoch 919: val_loss did not improve from 37.45752
196/196 - 66s - loss: 37.0572 - MinusLogProbMetric: 37.0572 - val_loss: 37.6884 - val_MinusLogProbMetric: 37.6884 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 920/1000
2023-10-27 02:33:03.402 
Epoch 920/1000 
	 loss: 37.0796, MinusLogProbMetric: 37.0796, val_loss: 37.4885, val_MinusLogProbMetric: 37.4885

Epoch 920: val_loss did not improve from 37.45752
196/196 - 66s - loss: 37.0796 - MinusLogProbMetric: 37.0796 - val_loss: 37.4885 - val_MinusLogProbMetric: 37.4885 - lr: 4.5725e-07 - 66s/epoch - 334ms/step
Epoch 921/1000
2023-10-27 02:34:09.201 
Epoch 921/1000 
	 loss: 37.0792, MinusLogProbMetric: 37.0792, val_loss: 37.4484, val_MinusLogProbMetric: 37.4484

Epoch 921: val_loss improved from 37.45752 to 37.44845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.0792 - MinusLogProbMetric: 37.0792 - val_loss: 37.4484 - val_MinusLogProbMetric: 37.4484 - lr: 4.5725e-07 - 67s/epoch - 342ms/step
Epoch 922/1000
2023-10-27 02:35:16.971 
Epoch 922/1000 
	 loss: 37.0356, MinusLogProbMetric: 37.0356, val_loss: 37.4567, val_MinusLogProbMetric: 37.4567

Epoch 922: val_loss did not improve from 37.44845
196/196 - 67s - loss: 37.0356 - MinusLogProbMetric: 37.0356 - val_loss: 37.4567 - val_MinusLogProbMetric: 37.4567 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 923/1000
2023-10-27 02:36:22.979 
Epoch 923/1000 
	 loss: 37.8751, MinusLogProbMetric: 37.8751, val_loss: 37.4811, val_MinusLogProbMetric: 37.4811

Epoch 923: val_loss did not improve from 37.44845
196/196 - 66s - loss: 37.8751 - MinusLogProbMetric: 37.8751 - val_loss: 37.4811 - val_MinusLogProbMetric: 37.4811 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 924/1000
2023-10-27 02:37:29.097 
Epoch 924/1000 
	 loss: 37.0554, MinusLogProbMetric: 37.0554, val_loss: 37.4272, val_MinusLogProbMetric: 37.4272

Epoch 924: val_loss improved from 37.44845 to 37.42725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.0554 - MinusLogProbMetric: 37.0554 - val_loss: 37.4272 - val_MinusLogProbMetric: 37.4272 - lr: 4.5725e-07 - 67s/epoch - 343ms/step
Epoch 925/1000
2023-10-27 02:38:36.806 
Epoch 925/1000 
	 loss: 37.0414, MinusLogProbMetric: 37.0414, val_loss: 37.4434, val_MinusLogProbMetric: 37.4434

Epoch 925: val_loss did not improve from 37.42725
196/196 - 67s - loss: 37.0414 - MinusLogProbMetric: 37.0414 - val_loss: 37.4434 - val_MinusLogProbMetric: 37.4434 - lr: 4.5725e-07 - 67s/epoch - 339ms/step
Epoch 926/1000
2023-10-27 02:39:43.452 
Epoch 926/1000 
	 loss: 37.0199, MinusLogProbMetric: 37.0199, val_loss: 37.5303, val_MinusLogProbMetric: 37.5303

Epoch 926: val_loss did not improve from 37.42725
196/196 - 67s - loss: 37.0199 - MinusLogProbMetric: 37.0199 - val_loss: 37.5303 - val_MinusLogProbMetric: 37.5303 - lr: 4.5725e-07 - 67s/epoch - 340ms/step
Epoch 927/1000
2023-10-27 02:40:49.338 
Epoch 927/1000 
	 loss: 37.0271, MinusLogProbMetric: 37.0271, val_loss: 37.5123, val_MinusLogProbMetric: 37.5123

Epoch 927: val_loss did not improve from 37.42725
196/196 - 66s - loss: 37.0271 - MinusLogProbMetric: 37.0271 - val_loss: 37.5123 - val_MinusLogProbMetric: 37.5123 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 928/1000
2023-10-27 02:41:55.134 
Epoch 928/1000 
	 loss: 37.0253, MinusLogProbMetric: 37.0253, val_loss: 37.4568, val_MinusLogProbMetric: 37.4568

Epoch 928: val_loss did not improve from 37.42725
196/196 - 66s - loss: 37.0253 - MinusLogProbMetric: 37.0253 - val_loss: 37.4568 - val_MinusLogProbMetric: 37.4568 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 929/1000
2023-10-27 02:43:01.092 
Epoch 929/1000 
	 loss: 37.0348, MinusLogProbMetric: 37.0348, val_loss: 37.4381, val_MinusLogProbMetric: 37.4381

Epoch 929: val_loss did not improve from 37.42725
196/196 - 66s - loss: 37.0348 - MinusLogProbMetric: 37.0348 - val_loss: 37.4381 - val_MinusLogProbMetric: 37.4381 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 930/1000
2023-10-27 02:44:07.084 
Epoch 930/1000 
	 loss: 37.0053, MinusLogProbMetric: 37.0053, val_loss: 37.5326, val_MinusLogProbMetric: 37.5326

Epoch 930: val_loss did not improve from 37.42725
196/196 - 66s - loss: 37.0053 - MinusLogProbMetric: 37.0053 - val_loss: 37.5326 - val_MinusLogProbMetric: 37.5326 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 931/1000
2023-10-27 02:45:13.173 
Epoch 931/1000 
	 loss: 37.0131, MinusLogProbMetric: 37.0131, val_loss: 37.6014, val_MinusLogProbMetric: 37.6014

Epoch 931: val_loss did not improve from 37.42725
196/196 - 66s - loss: 37.0131 - MinusLogProbMetric: 37.0131 - val_loss: 37.6014 - val_MinusLogProbMetric: 37.6014 - lr: 4.5725e-07 - 66s/epoch - 337ms/step
Epoch 932/1000
2023-10-27 02:46:18.670 
Epoch 932/1000 
	 loss: 37.1151, MinusLogProbMetric: 37.1151, val_loss: 37.3987, val_MinusLogProbMetric: 37.3987

Epoch 932: val_loss improved from 37.42725 to 37.39868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.1151 - MinusLogProbMetric: 37.1151 - val_loss: 37.3987 - val_MinusLogProbMetric: 37.3987 - lr: 4.5725e-07 - 67s/epoch - 341ms/step
Epoch 933/1000
2023-10-27 02:47:25.992 
Epoch 933/1000 
	 loss: 37.2005, MinusLogProbMetric: 37.2005, val_loss: 37.8441, val_MinusLogProbMetric: 37.8441

Epoch 933: val_loss did not improve from 37.39868
196/196 - 66s - loss: 37.2005 - MinusLogProbMetric: 37.2005 - val_loss: 37.8441 - val_MinusLogProbMetric: 37.8441 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 934/1000
2023-10-27 02:48:32.149 
Epoch 934/1000 
	 loss: 37.0984, MinusLogProbMetric: 37.0984, val_loss: 37.3896, val_MinusLogProbMetric: 37.3896

Epoch 934: val_loss improved from 37.39868 to 37.38961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 67s - loss: 37.0984 - MinusLogProbMetric: 37.0984 - val_loss: 37.3896 - val_MinusLogProbMetric: 37.3896 - lr: 4.5725e-07 - 67s/epoch - 344ms/step
Epoch 935/1000
2023-10-27 02:49:39.445 
Epoch 935/1000 
	 loss: 36.9796, MinusLogProbMetric: 36.9796, val_loss: 37.4177, val_MinusLogProbMetric: 37.4177

Epoch 935: val_loss did not improve from 37.38961
196/196 - 66s - loss: 36.9796 - MinusLogProbMetric: 36.9796 - val_loss: 37.4177 - val_MinusLogProbMetric: 37.4177 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 936/1000
2023-10-27 02:50:42.316 
Epoch 936/1000 
	 loss: 37.0643, MinusLogProbMetric: 37.0643, val_loss: 37.4090, val_MinusLogProbMetric: 37.4090

Epoch 936: val_loss did not improve from 37.38961
196/196 - 63s - loss: 37.0643 - MinusLogProbMetric: 37.0643 - val_loss: 37.4090 - val_MinusLogProbMetric: 37.4090 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 937/1000
2023-10-27 02:51:40.684 
Epoch 937/1000 
	 loss: 36.9762, MinusLogProbMetric: 36.9762, val_loss: 37.5148, val_MinusLogProbMetric: 37.5148

Epoch 937: val_loss did not improve from 37.38961
196/196 - 58s - loss: 36.9762 - MinusLogProbMetric: 36.9762 - val_loss: 37.5148 - val_MinusLogProbMetric: 37.5148 - lr: 4.5725e-07 - 58s/epoch - 298ms/step
Epoch 938/1000
2023-10-27 02:52:37.380 
Epoch 938/1000 
	 loss: 37.0557, MinusLogProbMetric: 37.0557, val_loss: 37.4091, val_MinusLogProbMetric: 37.4091

Epoch 938: val_loss did not improve from 37.38961
196/196 - 57s - loss: 37.0557 - MinusLogProbMetric: 37.0557 - val_loss: 37.4091 - val_MinusLogProbMetric: 37.4091 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 939/1000
2023-10-27 02:53:39.500 
Epoch 939/1000 
	 loss: 36.9595, MinusLogProbMetric: 36.9595, val_loss: 37.4649, val_MinusLogProbMetric: 37.4649

Epoch 939: val_loss did not improve from 37.38961
196/196 - 62s - loss: 36.9595 - MinusLogProbMetric: 36.9595 - val_loss: 37.4649 - val_MinusLogProbMetric: 37.4649 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 940/1000
2023-10-27 02:54:39.797 
Epoch 940/1000 
	 loss: 36.9760, MinusLogProbMetric: 36.9760, val_loss: 37.4224, val_MinusLogProbMetric: 37.4224

Epoch 940: val_loss did not improve from 37.38961
196/196 - 60s - loss: 36.9760 - MinusLogProbMetric: 36.9760 - val_loss: 37.4224 - val_MinusLogProbMetric: 37.4224 - lr: 4.5725e-07 - 60s/epoch - 308ms/step
Epoch 941/1000
2023-10-27 02:55:38.308 
Epoch 941/1000 
	 loss: 37.0517, MinusLogProbMetric: 37.0517, val_loss: 37.4563, val_MinusLogProbMetric: 37.4563

Epoch 941: val_loss did not improve from 37.38961
196/196 - 59s - loss: 37.0517 - MinusLogProbMetric: 37.0517 - val_loss: 37.4563 - val_MinusLogProbMetric: 37.4563 - lr: 4.5725e-07 - 59s/epoch - 299ms/step
Epoch 942/1000
2023-10-27 02:56:40.322 
Epoch 942/1000 
	 loss: 36.9705, MinusLogProbMetric: 36.9705, val_loss: 37.3850, val_MinusLogProbMetric: 37.3850

Epoch 942: val_loss improved from 37.38961 to 37.38498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 36.9705 - MinusLogProbMetric: 36.9705 - val_loss: 37.3850 - val_MinusLogProbMetric: 37.3850 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 943/1000
2023-10-27 02:57:40.240 
Epoch 943/1000 
	 loss: 36.9566, MinusLogProbMetric: 36.9566, val_loss: 37.4394, val_MinusLogProbMetric: 37.4394

Epoch 943: val_loss did not improve from 37.38498
196/196 - 59s - loss: 36.9566 - MinusLogProbMetric: 36.9566 - val_loss: 37.4394 - val_MinusLogProbMetric: 37.4394 - lr: 4.5725e-07 - 59s/epoch - 301ms/step
Epoch 944/1000
2023-10-27 02:58:40.237 
Epoch 944/1000 
	 loss: 36.9657, MinusLogProbMetric: 36.9657, val_loss: 37.4145, val_MinusLogProbMetric: 37.4145

Epoch 944: val_loss did not improve from 37.38498
196/196 - 60s - loss: 36.9657 - MinusLogProbMetric: 36.9657 - val_loss: 37.4145 - val_MinusLogProbMetric: 37.4145 - lr: 4.5725e-07 - 60s/epoch - 306ms/step
Epoch 945/1000
2023-10-27 02:59:42.364 
Epoch 945/1000 
	 loss: 36.9529, MinusLogProbMetric: 36.9529, val_loss: 37.4294, val_MinusLogProbMetric: 37.4294

Epoch 945: val_loss did not improve from 37.38498
196/196 - 62s - loss: 36.9529 - MinusLogProbMetric: 36.9529 - val_loss: 37.4294 - val_MinusLogProbMetric: 37.4294 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 946/1000
2023-10-27 03:00:38.430 
Epoch 946/1000 
	 loss: 37.0186, MinusLogProbMetric: 37.0186, val_loss: 37.3406, val_MinusLogProbMetric: 37.3406

Epoch 946: val_loss improved from 37.38498 to 37.34065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 57s - loss: 37.0186 - MinusLogProbMetric: 37.0186 - val_loss: 37.3406 - val_MinusLogProbMetric: 37.3406 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 947/1000
2023-10-27 03:01:39.255 
Epoch 947/1000 
	 loss: 36.9464, MinusLogProbMetric: 36.9464, val_loss: 37.3429, val_MinusLogProbMetric: 37.3429

Epoch 947: val_loss did not improve from 37.34065
196/196 - 60s - loss: 36.9464 - MinusLogProbMetric: 36.9464 - val_loss: 37.3429 - val_MinusLogProbMetric: 37.3429 - lr: 4.5725e-07 - 60s/epoch - 306ms/step
Epoch 948/1000
2023-10-27 03:02:40.006 
Epoch 948/1000 
	 loss: 36.9476, MinusLogProbMetric: 36.9476, val_loss: 37.3983, val_MinusLogProbMetric: 37.3983

Epoch 948: val_loss did not improve from 37.34065
196/196 - 61s - loss: 36.9476 - MinusLogProbMetric: 36.9476 - val_loss: 37.3983 - val_MinusLogProbMetric: 37.3983 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 949/1000
2023-10-27 03:03:38.074 
Epoch 949/1000 
	 loss: 36.9756, MinusLogProbMetric: 36.9756, val_loss: 37.3479, val_MinusLogProbMetric: 37.3479

Epoch 949: val_loss did not improve from 37.34065
196/196 - 58s - loss: 36.9756 - MinusLogProbMetric: 36.9756 - val_loss: 37.3479 - val_MinusLogProbMetric: 37.3479 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 950/1000
2023-10-27 03:04:39.831 
Epoch 950/1000 
	 loss: 37.0579, MinusLogProbMetric: 37.0579, val_loss: 37.3806, val_MinusLogProbMetric: 37.3806

Epoch 950: val_loss did not improve from 37.34065
196/196 - 62s - loss: 37.0579 - MinusLogProbMetric: 37.0579 - val_loss: 37.3806 - val_MinusLogProbMetric: 37.3806 - lr: 4.5725e-07 - 62s/epoch - 315ms/step
Epoch 951/1000
2023-10-27 03:05:36.749 
Epoch 951/1000 
	 loss: 36.9369, MinusLogProbMetric: 36.9369, val_loss: 37.3504, val_MinusLogProbMetric: 37.3504

Epoch 951: val_loss did not improve from 37.34065
196/196 - 57s - loss: 36.9369 - MinusLogProbMetric: 36.9369 - val_loss: 37.3504 - val_MinusLogProbMetric: 37.3504 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 952/1000
2023-10-27 03:06:38.572 
Epoch 952/1000 
	 loss: 37.0378, MinusLogProbMetric: 37.0378, val_loss: 37.6772, val_MinusLogProbMetric: 37.6772

Epoch 952: val_loss did not improve from 37.34065
196/196 - 62s - loss: 37.0378 - MinusLogProbMetric: 37.0378 - val_loss: 37.6772 - val_MinusLogProbMetric: 37.6772 - lr: 4.5725e-07 - 62s/epoch - 315ms/step
Epoch 953/1000
2023-10-27 03:07:39.215 
Epoch 953/1000 
	 loss: 36.9711, MinusLogProbMetric: 36.9711, val_loss: 37.3573, val_MinusLogProbMetric: 37.3573

Epoch 953: val_loss did not improve from 37.34065
196/196 - 61s - loss: 36.9711 - MinusLogProbMetric: 36.9711 - val_loss: 37.3573 - val_MinusLogProbMetric: 37.3573 - lr: 4.5725e-07 - 61s/epoch - 309ms/step
Epoch 954/1000
2023-10-27 03:08:35.279 
Epoch 954/1000 
	 loss: 36.9088, MinusLogProbMetric: 36.9088, val_loss: 37.3532, val_MinusLogProbMetric: 37.3532

Epoch 954: val_loss did not improve from 37.34065
196/196 - 56s - loss: 36.9088 - MinusLogProbMetric: 36.9088 - val_loss: 37.3532 - val_MinusLogProbMetric: 37.3532 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 955/1000
2023-10-27 03:09:37.248 
Epoch 955/1000 
	 loss: 36.9375, MinusLogProbMetric: 36.9375, val_loss: 37.3112, val_MinusLogProbMetric: 37.3112

Epoch 955: val_loss improved from 37.34065 to 37.31117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 63s - loss: 36.9375 - MinusLogProbMetric: 36.9375 - val_loss: 37.3112 - val_MinusLogProbMetric: 37.3112 - lr: 4.5725e-07 - 63s/epoch - 321ms/step
Epoch 956/1000
2023-10-27 03:10:37.952 
Epoch 956/1000 
	 loss: 36.9492, MinusLogProbMetric: 36.9492, val_loss: 37.3342, val_MinusLogProbMetric: 37.3342

Epoch 956: val_loss did not improve from 37.31117
196/196 - 60s - loss: 36.9492 - MinusLogProbMetric: 36.9492 - val_loss: 37.3342 - val_MinusLogProbMetric: 37.3342 - lr: 4.5725e-07 - 60s/epoch - 304ms/step
Epoch 957/1000
2023-10-27 03:11:38.811 
Epoch 957/1000 
	 loss: 36.9905, MinusLogProbMetric: 36.9905, val_loss: 37.3013, val_MinusLogProbMetric: 37.3013

Epoch 957: val_loss improved from 37.31117 to 37.30126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 62s - loss: 36.9905 - MinusLogProbMetric: 36.9905 - val_loss: 37.3013 - val_MinusLogProbMetric: 37.3013 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 958/1000
2023-10-27 03:12:38.760 
Epoch 958/1000 
	 loss: 36.8959, MinusLogProbMetric: 36.8959, val_loss: 39.4207, val_MinusLogProbMetric: 39.4207

Epoch 958: val_loss did not improve from 37.30126
196/196 - 59s - loss: 36.8959 - MinusLogProbMetric: 36.8959 - val_loss: 39.4207 - val_MinusLogProbMetric: 39.4207 - lr: 4.5725e-07 - 59s/epoch - 300ms/step
Epoch 959/1000
2023-10-27 03:13:35.173 
Epoch 959/1000 
	 loss: 37.1588, MinusLogProbMetric: 37.1588, val_loss: 37.3263, val_MinusLogProbMetric: 37.3263

Epoch 959: val_loss did not improve from 37.30126
196/196 - 56s - loss: 37.1588 - MinusLogProbMetric: 37.1588 - val_loss: 37.3263 - val_MinusLogProbMetric: 37.3263 - lr: 4.5725e-07 - 56s/epoch - 288ms/step
Epoch 960/1000
2023-10-27 03:14:39.360 
Epoch 960/1000 
	 loss: 36.9007, MinusLogProbMetric: 36.9007, val_loss: 37.3285, val_MinusLogProbMetric: 37.3285

Epoch 960: val_loss did not improve from 37.30126
196/196 - 64s - loss: 36.9007 - MinusLogProbMetric: 36.9007 - val_loss: 37.3285 - val_MinusLogProbMetric: 37.3285 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 961/1000
2023-10-27 03:15:35.946 
Epoch 961/1000 
	 loss: 37.0000, MinusLogProbMetric: 37.0000, val_loss: 38.6394, val_MinusLogProbMetric: 38.6394

Epoch 961: val_loss did not improve from 37.30126
196/196 - 57s - loss: 37.0000 - MinusLogProbMetric: 37.0000 - val_loss: 38.6394 - val_MinusLogProbMetric: 38.6394 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 962/1000
2023-10-27 03:16:35.646 
Epoch 962/1000 
	 loss: 37.0929, MinusLogProbMetric: 37.0929, val_loss: 37.2975, val_MinusLogProbMetric: 37.2975

Epoch 962: val_loss improved from 37.30126 to 37.29745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 61s - loss: 37.0929 - MinusLogProbMetric: 37.0929 - val_loss: 37.2975 - val_MinusLogProbMetric: 37.2975 - lr: 4.5725e-07 - 61s/epoch - 310ms/step
Epoch 963/1000
2023-10-27 03:17:35.801 
Epoch 963/1000 
	 loss: 36.8869, MinusLogProbMetric: 36.8869, val_loss: 37.3621, val_MinusLogProbMetric: 37.3621

Epoch 963: val_loss did not improve from 37.29745
196/196 - 59s - loss: 36.8869 - MinusLogProbMetric: 36.8869 - val_loss: 37.3621 - val_MinusLogProbMetric: 37.3621 - lr: 4.5725e-07 - 59s/epoch - 302ms/step
Epoch 964/1000
2023-10-27 03:18:32.405 
Epoch 964/1000 
	 loss: 36.8726, MinusLogProbMetric: 36.8726, val_loss: 37.3129, val_MinusLogProbMetric: 37.3129

Epoch 964: val_loss did not improve from 37.29745
196/196 - 57s - loss: 36.8726 - MinusLogProbMetric: 36.8726 - val_loss: 37.3129 - val_MinusLogProbMetric: 37.3129 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 965/1000
2023-10-27 03:19:34.490 
Epoch 965/1000 
	 loss: 36.8830, MinusLogProbMetric: 36.8830, val_loss: 37.4590, val_MinusLogProbMetric: 37.4590

Epoch 965: val_loss did not improve from 37.29745
196/196 - 62s - loss: 36.8830 - MinusLogProbMetric: 36.8830 - val_loss: 37.4590 - val_MinusLogProbMetric: 37.4590 - lr: 4.5725e-07 - 62s/epoch - 317ms/step
Epoch 966/1000
2023-10-27 03:20:30.403 
Epoch 966/1000 
	 loss: 36.8860, MinusLogProbMetric: 36.8860, val_loss: 37.3512, val_MinusLogProbMetric: 37.3512

Epoch 966: val_loss did not improve from 37.29745
196/196 - 56s - loss: 36.8860 - MinusLogProbMetric: 36.8860 - val_loss: 37.3512 - val_MinusLogProbMetric: 37.3512 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 967/1000
2023-10-27 03:21:32.674 
Epoch 967/1000 
	 loss: 36.8820, MinusLogProbMetric: 36.8820, val_loss: 37.3506, val_MinusLogProbMetric: 37.3506

Epoch 967: val_loss did not improve from 37.29745
196/196 - 62s - loss: 36.8820 - MinusLogProbMetric: 36.8820 - val_loss: 37.3506 - val_MinusLogProbMetric: 37.3506 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 968/1000
2023-10-27 03:22:31.218 
Epoch 968/1000 
	 loss: 36.8731, MinusLogProbMetric: 36.8731, val_loss: 37.2981, val_MinusLogProbMetric: 37.2981

Epoch 968: val_loss did not improve from 37.29745
196/196 - 59s - loss: 36.8731 - MinusLogProbMetric: 36.8731 - val_loss: 37.2981 - val_MinusLogProbMetric: 37.2981 - lr: 4.5725e-07 - 59s/epoch - 299ms/step
Epoch 969/1000
2023-10-27 03:23:29.568 
Epoch 969/1000 
	 loss: 36.8581, MinusLogProbMetric: 36.8581, val_loss: 37.3123, val_MinusLogProbMetric: 37.3123

Epoch 969: val_loss did not improve from 37.29745
196/196 - 58s - loss: 36.8581 - MinusLogProbMetric: 36.8581 - val_loss: 37.3123 - val_MinusLogProbMetric: 37.3123 - lr: 4.5725e-07 - 58s/epoch - 298ms/step
Epoch 970/1000
2023-10-27 03:24:33.025 
Epoch 970/1000 
	 loss: 36.8555, MinusLogProbMetric: 36.8555, val_loss: 37.3124, val_MinusLogProbMetric: 37.3124

Epoch 970: val_loss did not improve from 37.29745
196/196 - 63s - loss: 36.8555 - MinusLogProbMetric: 36.8555 - val_loss: 37.3124 - val_MinusLogProbMetric: 37.3124 - lr: 4.5725e-07 - 63s/epoch - 324ms/step
Epoch 971/1000
2023-10-27 03:25:36.115 
Epoch 971/1000 
	 loss: 37.1560, MinusLogProbMetric: 37.1560, val_loss: 37.2589, val_MinusLogProbMetric: 37.2589

Epoch 971: val_loss improved from 37.29745 to 37.25894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 64s - loss: 37.1560 - MinusLogProbMetric: 37.1560 - val_loss: 37.2589 - val_MinusLogProbMetric: 37.2589 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 972/1000
2023-10-27 03:26:39.989 
Epoch 972/1000 
	 loss: 36.8524, MinusLogProbMetric: 36.8524, val_loss: 37.2736, val_MinusLogProbMetric: 37.2736

Epoch 972: val_loss did not improve from 37.25894
196/196 - 63s - loss: 36.8524 - MinusLogProbMetric: 36.8524 - val_loss: 37.2736 - val_MinusLogProbMetric: 37.2736 - lr: 4.5725e-07 - 63s/epoch - 319ms/step
Epoch 973/1000
2023-10-27 03:27:36.954 
Epoch 973/1000 
	 loss: 36.9193, MinusLogProbMetric: 36.9193, val_loss: 37.2496, val_MinusLogProbMetric: 37.2496

Epoch 973: val_loss improved from 37.25894 to 37.24958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 58s - loss: 36.9193 - MinusLogProbMetric: 36.9193 - val_loss: 37.2496 - val_MinusLogProbMetric: 37.2496 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 974/1000
2023-10-27 03:28:41.847 
Epoch 974/1000 
	 loss: 36.8273, MinusLogProbMetric: 36.8273, val_loss: 37.3272, val_MinusLogProbMetric: 37.3272

Epoch 974: val_loss did not improve from 37.24958
196/196 - 64s - loss: 36.8273 - MinusLogProbMetric: 36.8273 - val_loss: 37.3272 - val_MinusLogProbMetric: 37.3272 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 975/1000
2023-10-27 03:29:47.735 
Epoch 975/1000 
	 loss: 36.8301, MinusLogProbMetric: 36.8301, val_loss: 37.3041, val_MinusLogProbMetric: 37.3041

Epoch 975: val_loss did not improve from 37.24958
196/196 - 66s - loss: 36.8301 - MinusLogProbMetric: 36.8301 - val_loss: 37.3041 - val_MinusLogProbMetric: 37.3041 - lr: 4.5725e-07 - 66s/epoch - 336ms/step
Epoch 976/1000
2023-10-27 03:30:53.351 
Epoch 976/1000 
	 loss: 36.8206, MinusLogProbMetric: 36.8206, val_loss: 37.3022, val_MinusLogProbMetric: 37.3022

Epoch 976: val_loss did not improve from 37.24958
196/196 - 66s - loss: 36.8206 - MinusLogProbMetric: 36.8206 - val_loss: 37.3022 - val_MinusLogProbMetric: 37.3022 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 977/1000
2023-10-27 03:31:58.726 
Epoch 977/1000 
	 loss: 36.8411, MinusLogProbMetric: 36.8411, val_loss: 37.2972, val_MinusLogProbMetric: 37.2972

Epoch 977: val_loss did not improve from 37.24958
196/196 - 65s - loss: 36.8411 - MinusLogProbMetric: 36.8411 - val_loss: 37.2972 - val_MinusLogProbMetric: 37.2972 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 978/1000
2023-10-27 03:33:03.231 
Epoch 978/1000 
	 loss: 36.8458, MinusLogProbMetric: 36.8458, val_loss: 37.2235, val_MinusLogProbMetric: 37.2235

Epoch 978: val_loss improved from 37.24958 to 37.22348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 66s - loss: 36.8458 - MinusLogProbMetric: 36.8458 - val_loss: 37.2235 - val_MinusLogProbMetric: 37.2235 - lr: 4.5725e-07 - 66s/epoch - 335ms/step
Epoch 979/1000
2023-10-27 03:34:08.681 
Epoch 979/1000 
	 loss: 36.8338, MinusLogProbMetric: 36.8338, val_loss: 37.2820, val_MinusLogProbMetric: 37.2820

Epoch 979: val_loss did not improve from 37.22348
196/196 - 64s - loss: 36.8338 - MinusLogProbMetric: 36.8338 - val_loss: 37.2820 - val_MinusLogProbMetric: 37.2820 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 980/1000
2023-10-27 03:35:13.400 
Epoch 980/1000 
	 loss: 36.8173, MinusLogProbMetric: 36.8173, val_loss: 37.2949, val_MinusLogProbMetric: 37.2949

Epoch 980: val_loss did not improve from 37.22348
196/196 - 65s - loss: 36.8173 - MinusLogProbMetric: 36.8173 - val_loss: 37.2949 - val_MinusLogProbMetric: 37.2949 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 981/1000
2023-10-27 03:36:17.437 
Epoch 981/1000 
	 loss: 37.1223, MinusLogProbMetric: 37.1223, val_loss: 37.2287, val_MinusLogProbMetric: 37.2287

Epoch 981: val_loss did not improve from 37.22348
196/196 - 64s - loss: 37.1223 - MinusLogProbMetric: 37.1223 - val_loss: 37.2287 - val_MinusLogProbMetric: 37.2287 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 982/1000
2023-10-27 03:37:21.875 
Epoch 982/1000 
	 loss: 36.8101, MinusLogProbMetric: 36.8101, val_loss: 37.3092, val_MinusLogProbMetric: 37.3092

Epoch 982: val_loss did not improve from 37.22348
196/196 - 64s - loss: 36.8101 - MinusLogProbMetric: 36.8101 - val_loss: 37.3092 - val_MinusLogProbMetric: 37.3092 - lr: 4.5725e-07 - 64s/epoch - 329ms/step
Epoch 983/1000
2023-10-27 03:38:26.107 
Epoch 983/1000 
	 loss: 36.8570, MinusLogProbMetric: 36.8570, val_loss: 37.2250, val_MinusLogProbMetric: 37.2250

Epoch 983: val_loss did not improve from 37.22348
196/196 - 64s - loss: 36.8570 - MinusLogProbMetric: 36.8570 - val_loss: 37.2250 - val_MinusLogProbMetric: 37.2250 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 984/1000
2023-10-27 03:39:30.850 
Epoch 984/1000 
	 loss: 36.8246, MinusLogProbMetric: 36.8246, val_loss: 37.2397, val_MinusLogProbMetric: 37.2397

Epoch 984: val_loss did not improve from 37.22348
196/196 - 65s - loss: 36.8246 - MinusLogProbMetric: 36.8246 - val_loss: 37.2397 - val_MinusLogProbMetric: 37.2397 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 985/1000
2023-10-27 03:40:34.766 
Epoch 985/1000 
	 loss: 36.8102, MinusLogProbMetric: 36.8102, val_loss: 37.3592, val_MinusLogProbMetric: 37.3592

Epoch 985: val_loss did not improve from 37.22348
196/196 - 64s - loss: 36.8102 - MinusLogProbMetric: 36.8102 - val_loss: 37.3592 - val_MinusLogProbMetric: 37.3592 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 986/1000
2023-10-27 03:41:39.308 
Epoch 986/1000 
	 loss: 36.7982, MinusLogProbMetric: 36.7982, val_loss: 37.2458, val_MinusLogProbMetric: 37.2458

Epoch 986: val_loss did not improve from 37.22348
196/196 - 65s - loss: 36.7982 - MinusLogProbMetric: 36.7982 - val_loss: 37.2458 - val_MinusLogProbMetric: 37.2458 - lr: 4.5725e-07 - 65s/epoch - 329ms/step
Epoch 987/1000
2023-10-27 03:42:44.131 
Epoch 987/1000 
	 loss: 37.2728, MinusLogProbMetric: 37.2728, val_loss: 37.3566, val_MinusLogProbMetric: 37.3566

Epoch 987: val_loss did not improve from 37.22348
196/196 - 65s - loss: 37.2728 - MinusLogProbMetric: 37.2728 - val_loss: 37.3566 - val_MinusLogProbMetric: 37.3566 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 988/1000
2023-10-27 03:43:48.941 
Epoch 988/1000 
	 loss: 36.8459, MinusLogProbMetric: 36.8459, val_loss: 37.2251, val_MinusLogProbMetric: 37.2251

Epoch 988: val_loss did not improve from 37.22348
196/196 - 65s - loss: 36.8459 - MinusLogProbMetric: 36.8459 - val_loss: 37.2251 - val_MinusLogProbMetric: 37.2251 - lr: 4.5725e-07 - 65s/epoch - 331ms/step
Epoch 989/1000
2023-10-27 03:44:53.314 
Epoch 989/1000 
	 loss: 36.7962, MinusLogProbMetric: 36.7962, val_loss: 37.2747, val_MinusLogProbMetric: 37.2747

Epoch 989: val_loss did not improve from 37.22348
196/196 - 64s - loss: 36.7962 - MinusLogProbMetric: 36.7962 - val_loss: 37.2747 - val_MinusLogProbMetric: 37.2747 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 990/1000
2023-10-27 03:45:58.452 
Epoch 990/1000 
	 loss: 36.8089, MinusLogProbMetric: 36.8089, val_loss: 37.2552, val_MinusLogProbMetric: 37.2552

Epoch 990: val_loss did not improve from 37.22348
196/196 - 65s - loss: 36.8089 - MinusLogProbMetric: 36.8089 - val_loss: 37.2552 - val_MinusLogProbMetric: 37.2552 - lr: 4.5725e-07 - 65s/epoch - 332ms/step
Epoch 991/1000
2023-10-27 03:46:55.474 
Epoch 991/1000 
	 loss: 36.9068, MinusLogProbMetric: 36.9068, val_loss: 37.2576, val_MinusLogProbMetric: 37.2576

Epoch 991: val_loss did not improve from 37.22348
196/196 - 57s - loss: 36.9068 - MinusLogProbMetric: 36.9068 - val_loss: 37.2576 - val_MinusLogProbMetric: 37.2576 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 992/1000
2023-10-27 03:47:46.585 
Epoch 992/1000 
	 loss: 36.7963, MinusLogProbMetric: 36.7963, val_loss: 37.2379, val_MinusLogProbMetric: 37.2379

Epoch 992: val_loss did not improve from 37.22348
196/196 - 51s - loss: 36.7963 - MinusLogProbMetric: 36.7963 - val_loss: 37.2379 - val_MinusLogProbMetric: 37.2379 - lr: 4.5725e-07 - 51s/epoch - 261ms/step
Epoch 993/1000
2023-10-27 03:48:42.595 
Epoch 993/1000 
	 loss: 37.0291, MinusLogProbMetric: 37.0291, val_loss: 37.2642, val_MinusLogProbMetric: 37.2642

Epoch 993: val_loss did not improve from 37.22348
196/196 - 56s - loss: 37.0291 - MinusLogProbMetric: 37.0291 - val_loss: 37.2642 - val_MinusLogProbMetric: 37.2642 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 994/1000
2023-10-27 03:49:41.232 
Epoch 994/1000 
	 loss: 36.7955, MinusLogProbMetric: 36.7955, val_loss: 37.2009, val_MinusLogProbMetric: 37.2009

Epoch 994: val_loss improved from 37.22348 to 37.20092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 59s - loss: 36.7955 - MinusLogProbMetric: 36.7955 - val_loss: 37.2009 - val_MinusLogProbMetric: 37.2009 - lr: 4.5725e-07 - 59s/epoch - 304ms/step
Epoch 995/1000
2023-10-27 03:50:33.345 
Epoch 995/1000 
	 loss: 36.7577, MinusLogProbMetric: 36.7577, val_loss: 37.2109, val_MinusLogProbMetric: 37.2109

Epoch 995: val_loss did not improve from 37.20092
196/196 - 51s - loss: 36.7577 - MinusLogProbMetric: 36.7577 - val_loss: 37.2109 - val_MinusLogProbMetric: 37.2109 - lr: 4.5725e-07 - 51s/epoch - 261ms/step
Epoch 996/1000
2023-10-27 03:51:27.944 
Epoch 996/1000 
	 loss: 36.7988, MinusLogProbMetric: 36.7988, val_loss: 37.2024, val_MinusLogProbMetric: 37.2024

Epoch 996: val_loss did not improve from 37.20092
196/196 - 55s - loss: 36.7988 - MinusLogProbMetric: 36.7988 - val_loss: 37.2024 - val_MinusLogProbMetric: 37.2024 - lr: 4.5725e-07 - 55s/epoch - 279ms/step
Epoch 997/1000
2023-10-27 03:52:29.814 
Epoch 997/1000 
	 loss: 36.8212, MinusLogProbMetric: 36.8212, val_loss: 37.2066, val_MinusLogProbMetric: 37.2066

Epoch 997: val_loss did not improve from 37.20092
196/196 - 62s - loss: 36.8212 - MinusLogProbMetric: 36.8212 - val_loss: 37.2066 - val_MinusLogProbMetric: 37.2066 - lr: 4.5725e-07 - 62s/epoch - 316ms/step
Epoch 998/1000
2023-10-27 03:53:24.445 
Epoch 998/1000 
	 loss: 36.7574, MinusLogProbMetric: 36.7574, val_loss: 37.2212, val_MinusLogProbMetric: 37.2212

Epoch 998: val_loss did not improve from 37.20092
196/196 - 55s - loss: 36.7574 - MinusLogProbMetric: 36.7574 - val_loss: 37.2212 - val_MinusLogProbMetric: 37.2212 - lr: 4.5725e-07 - 55s/epoch - 279ms/step
Epoch 999/1000
2023-10-27 03:54:16.491 
Epoch 999/1000 
	 loss: 36.8050, MinusLogProbMetric: 36.8050, val_loss: 37.1712, val_MinusLogProbMetric: 37.1712

Epoch 999: val_loss improved from 37.20092 to 37.17117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_374/weights/best_weights.h5
196/196 - 53s - loss: 36.8050 - MinusLogProbMetric: 36.8050 - val_loss: 37.1712 - val_MinusLogProbMetric: 37.1712 - lr: 4.5725e-07 - 53s/epoch - 271ms/step
Epoch 1000/1000
2023-10-27 03:55:08.118 
Epoch 1000/1000 
	 loss: 36.7415, MinusLogProbMetric: 36.7415, val_loss: 37.2154, val_MinusLogProbMetric: 37.2154

Epoch 1000: val_loss did not improve from 37.17117
196/196 - 51s - loss: 36.7415 - MinusLogProbMetric: 36.7415 - val_loss: 37.2154 - val_MinusLogProbMetric: 37.2154 - lr: 4.5725e-07 - 51s/epoch - 258ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 721.
Model trained in 64488.66 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.04 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.30 s.
===========
Run 374/720 done in 83175.52 s.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

===========
Generating train data for run 401.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_347"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_348 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7fee244d9270>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feaf46e37f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feaf46e37f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea29e09ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee04340f10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee04341000>, <keras.callbacks.ModelCheckpoint object at 0x7fee043402b0>, <keras.callbacks.EarlyStopping object at 0x7fee04342050>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee04342c20>, <keras.callbacks.TerminateOnNaN object at 0x7fee04342a10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 03:55:13.164878
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:56:12.808 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 60s/epoch - 304ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 401.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_353"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_354 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7febd9057820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea2abc95d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea2abc95d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febda7f0d60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7febda7e4fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7febda7e5510>, <keras.callbacks.ModelCheckpoint object at 0x7febda7e55d0>, <keras.callbacks.EarlyStopping object at 0x7febda7e5840>, <keras.callbacks.ReduceLROnPlateau object at 0x7febda7e5870>, <keras.callbacks.TerminateOnNaN object at 0x7febda7e54b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 03:56:16.809905
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:57:13.258 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 56s/epoch - 287ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 401.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_359"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_360 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7ff32b990130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb9f495300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb9f495300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff32b91fdf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff32b314c10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff32b315180>, <keras.callbacks.ModelCheckpoint object at 0x7ff32b315240>, <keras.callbacks.EarlyStopping object at 0x7ff32b3154b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff32b3154e0>, <keras.callbacks.TerminateOnNaN object at 0x7ff32b315120>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 03:57:17.406692
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:58:24.370 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 67s/epoch - 341ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 401.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_365"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_366 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7feb87ca2d70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec4c98e590>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec4c98e590>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feda83ba0e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7febda7d6da0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7febda7d6ef0>, <keras.callbacks.ModelCheckpoint object at 0x7febda7d5c30>, <keras.callbacks.EarlyStopping object at 0x7febda7d6710>, <keras.callbacks.ReduceLROnPlateau object at 0x7febda7d6740>, <keras.callbacks.TerminateOnNaN object at 0x7febda7d7eb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 03:58:28.251246
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:59:27.613 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 59s/epoch - 302ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 401.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_371"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_372 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7feb87b956f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb87b236d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb87b236d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb9e2ce410>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb9da272e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb9da27850>, <keras.callbacks.ModelCheckpoint object at 0x7feb9da27910>, <keras.callbacks.EarlyStopping object at 0x7feb9da27b80>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb9da27bb0>, <keras.callbacks.TerminateOnNaN object at 0x7feb9da277f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 03:59:31.464314
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:00:30.700 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 59s/epoch - 302ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 401.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_377"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_378 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7fec764423b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb9f127d00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb9f127d00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb9d977460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff34c3fde10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff34c3fe380>, <keras.callbacks.ModelCheckpoint object at 0x7ff34c3fe440>, <keras.callbacks.EarlyStopping object at 0x7ff34c3fe6b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff34c3fe6e0>, <keras.callbacks.TerminateOnNaN object at 0x7ff34c3fe320>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 04:00:34.684316
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:01:42.015 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 67s/epoch - 343ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 401.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_383"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_384 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7fec4c345b70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec76f82bf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec76f82bf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb9e3a9540>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee84fd3fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee84fd30a0>, <keras.callbacks.ModelCheckpoint object at 0x7fee84fd0490>, <keras.callbacks.EarlyStopping object at 0x7fee84fd22c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee84fd1ff0>, <keras.callbacks.TerminateOnNaN object at 0x7fee84fd19f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 04:01:46.253891
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:02:45.459 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 59s/epoch - 301ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 401.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_389"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_390 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7fec86814640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff32b17ef20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff32b17ef20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4dca7fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec271c21d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec271c2740>, <keras.callbacks.ModelCheckpoint object at 0x7fec271c2800>, <keras.callbacks.EarlyStopping object at 0x7fec271c2a70>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec271c2aa0>, <keras.callbacks.TerminateOnNaN object at 0x7fec271c26e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 04:02:50.333164
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:03:49.507 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 59s/epoch - 301ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 401.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_395"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_396 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7fec2745be20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec4df590c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec4df590c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea23912890>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb18128b80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb181290f0>, <keras.callbacks.ModelCheckpoint object at 0x7feb181291b0>, <keras.callbacks.EarlyStopping object at 0x7feb18129420>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb18129450>, <keras.callbacks.TerminateOnNaN object at 0x7feb18129090>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 04:03:53.127060
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:04:49.688 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 56s/epoch - 288ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 401.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_401"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_402 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7ff322b1bbb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb9e4d98a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb9e4d98a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff31a444220>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff31a2f0760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff31a2f0cd0>, <keras.callbacks.ModelCheckpoint object at 0x7ff31a2f0d90>, <keras.callbacks.EarlyStopping object at 0x7ff31a2f1000>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff31a2f1030>, <keras.callbacks.TerminateOnNaN object at 0x7ff31a2f0c70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 04:04:53.877725
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:06:01.365 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 67s/epoch - 343ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 401.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_401/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_401
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_407"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_408 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7fec274109d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecd1b96800>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecd1b96800>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb9ec458d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fede8760cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_401/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fede8761db0>, <keras.callbacks.ModelCheckpoint object at 0x7fede8760490>, <keras.callbacks.EarlyStopping object at 0x7fede87621a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fede87612a0>, <keras.callbacks.TerminateOnNaN object at 0x7fede8763e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_401/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 401/720 with hyperparameters:
timestamp = 2023-10-27 04:06:06.406771
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:07:04.113 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10280.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10280.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 58s/epoch - 294ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 401/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 402.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_402/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_402/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_402/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_402
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_413"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_414 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7feba7f58cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecaddb3e50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecaddb3e50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea12ccf100>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feba5eb08b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feba5eb0e20>, <keras.callbacks.ModelCheckpoint object at 0x7feba5eb0ee0>, <keras.callbacks.EarlyStopping object at 0x7feba5eb1150>, <keras.callbacks.ReduceLROnPlateau object at 0x7feba5eb1180>, <keras.callbacks.TerminateOnNaN object at 0x7feba5eb0dc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_402/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 402/720 with hyperparameters:
timestamp = 2023-10-27 04:07:08.375610
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:08:10.772 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6422.6685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 62s - loss: nan - MinusLogProbMetric: 6422.6685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 62s/epoch - 318ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 402.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_402/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_402/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_402/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_402
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_419"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_420 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7feba5ef5840>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb00dd0160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb00dd0160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb20e977c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb5c9904c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb5c990a30>, <keras.callbacks.ModelCheckpoint object at 0x7feb5c990af0>, <keras.callbacks.EarlyStopping object at 0x7feb5c990d60>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb5c990d90>, <keras.callbacks.TerminateOnNaN object at 0x7feb5c9909d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_402/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 402/720 with hyperparameters:
timestamp = 2023-10-27 04:08:14.478683
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
2023-10-27 04:10:00.728 
Epoch 1/1000 
	 loss: 1788.9440, MinusLogProbMetric: 1788.9440, val_loss: 601.6552, val_MinusLogProbMetric: 601.6552

Epoch 1: val_loss improved from inf to 601.65515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 106s - loss: 1788.9440 - MinusLogProbMetric: 1788.9440 - val_loss: 601.6552 - val_MinusLogProbMetric: 601.6552 - lr: 3.3333e-04 - 106s/epoch - 543ms/step
Epoch 2/1000
2023-10-27 04:10:29.269 
Epoch 2/1000 
	 loss: 535.7795, MinusLogProbMetric: 535.7795, val_loss: 407.9900, val_MinusLogProbMetric: 407.9900

Epoch 2: val_loss improved from 601.65515 to 407.98999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 535.7795 - MinusLogProbMetric: 535.7795 - val_loss: 407.9900 - val_MinusLogProbMetric: 407.9900 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 3/1000
2023-10-27 04:10:57.419 
Epoch 3/1000 
	 loss: 429.9239, MinusLogProbMetric: 429.9239, val_loss: 565.6874, val_MinusLogProbMetric: 565.6874

Epoch 3: val_loss did not improve from 407.98999
196/196 - 28s - loss: 429.9239 - MinusLogProbMetric: 429.9239 - val_loss: 565.6874 - val_MinusLogProbMetric: 565.6874 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 4/1000
2023-10-27 04:11:24.944 
Epoch 4/1000 
	 loss: 352.8313, MinusLogProbMetric: 352.8313, val_loss: 293.6467, val_MinusLogProbMetric: 293.6467

Epoch 4: val_loss improved from 407.98999 to 293.64670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 352.8313 - MinusLogProbMetric: 352.8313 - val_loss: 293.6467 - val_MinusLogProbMetric: 293.6467 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 5/1000
2023-10-27 04:11:54.728 
Epoch 5/1000 
	 loss: 276.1421, MinusLogProbMetric: 276.1421, val_loss: 265.3279, val_MinusLogProbMetric: 265.3279

Epoch 5: val_loss improved from 293.64670 to 265.32794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 276.1421 - MinusLogProbMetric: 276.1421 - val_loss: 265.3279 - val_MinusLogProbMetric: 265.3279 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 6/1000
2023-10-27 04:12:28.501 
Epoch 6/1000 
	 loss: 231.8492, MinusLogProbMetric: 231.8492, val_loss: 191.5089, val_MinusLogProbMetric: 191.5089

Epoch 6: val_loss improved from 265.32794 to 191.50893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 231.8492 - MinusLogProbMetric: 231.8492 - val_loss: 191.5089 - val_MinusLogProbMetric: 191.5089 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 7/1000
2023-10-27 04:12:57.710 
Epoch 7/1000 
	 loss: 174.3337, MinusLogProbMetric: 174.3337, val_loss: 157.8924, val_MinusLogProbMetric: 157.8924

Epoch 7: val_loss improved from 191.50893 to 157.89244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 29s - loss: 174.3337 - MinusLogProbMetric: 174.3337 - val_loss: 157.8924 - val_MinusLogProbMetric: 157.8924 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 8/1000
2023-10-27 04:13:25.486 
Epoch 8/1000 
	 loss: 148.0448, MinusLogProbMetric: 148.0448, val_loss: 140.1449, val_MinusLogProbMetric: 140.1449

Epoch 8: val_loss improved from 157.89244 to 140.14491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 148.0448 - MinusLogProbMetric: 148.0448 - val_loss: 140.1449 - val_MinusLogProbMetric: 140.1449 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 9/1000
2023-10-27 04:13:53.109 
Epoch 9/1000 
	 loss: 135.4491, MinusLogProbMetric: 135.4491, val_loss: 130.5246, val_MinusLogProbMetric: 130.5246

Epoch 9: val_loss improved from 140.14491 to 130.52457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 27s - loss: 135.4491 - MinusLogProbMetric: 135.4491 - val_loss: 130.5246 - val_MinusLogProbMetric: 130.5246 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 10/1000
2023-10-27 04:14:20.857 
Epoch 10/1000 
	 loss: 129.6906, MinusLogProbMetric: 129.6906, val_loss: 130.3102, val_MinusLogProbMetric: 130.3102

Epoch 10: val_loss improved from 130.52457 to 130.31020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 129.6906 - MinusLogProbMetric: 129.6906 - val_loss: 130.3102 - val_MinusLogProbMetric: 130.3102 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 11/1000
2023-10-27 04:14:48.110 
Epoch 11/1000 
	 loss: 123.7827, MinusLogProbMetric: 123.7827, val_loss: 118.6059, val_MinusLogProbMetric: 118.6059

Epoch 11: val_loss improved from 130.31020 to 118.60592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 27s - loss: 123.7827 - MinusLogProbMetric: 123.7827 - val_loss: 118.6059 - val_MinusLogProbMetric: 118.6059 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 12/1000
2023-10-27 04:15:20.628 
Epoch 12/1000 
	 loss: 113.9265, MinusLogProbMetric: 113.9265, val_loss: 109.8411, val_MinusLogProbMetric: 109.8411

Epoch 12: val_loss improved from 118.60592 to 109.84113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 113.9265 - MinusLogProbMetric: 113.9265 - val_loss: 109.8411 - val_MinusLogProbMetric: 109.8411 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 13/1000
2023-10-27 04:15:50.422 
Epoch 13/1000 
	 loss: 109.8003, MinusLogProbMetric: 109.8003, val_loss: 111.4933, val_MinusLogProbMetric: 111.4933

Epoch 13: val_loss did not improve from 109.84113
196/196 - 29s - loss: 109.8003 - MinusLogProbMetric: 109.8003 - val_loss: 111.4933 - val_MinusLogProbMetric: 111.4933 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 14/1000
2023-10-27 04:16:17.922 
Epoch 14/1000 
	 loss: 105.2283, MinusLogProbMetric: 105.2283, val_loss: 101.9708, val_MinusLogProbMetric: 101.9708

Epoch 14: val_loss improved from 109.84113 to 101.97083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 105.2283 - MinusLogProbMetric: 105.2283 - val_loss: 101.9708 - val_MinusLogProbMetric: 101.9708 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 15/1000
2023-10-27 04:16:46.477 
Epoch 15/1000 
	 loss: 99.6989, MinusLogProbMetric: 99.6989, val_loss: 97.9649, val_MinusLogProbMetric: 97.9649

Epoch 15: val_loss improved from 101.97083 to 97.96485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 29s - loss: 99.6989 - MinusLogProbMetric: 99.6989 - val_loss: 97.9649 - val_MinusLogProbMetric: 97.9649 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 16/1000
2023-10-27 04:17:14.577 
Epoch 16/1000 
	 loss: 97.1196, MinusLogProbMetric: 97.1196, val_loss: 99.1266, val_MinusLogProbMetric: 99.1266

Epoch 16: val_loss did not improve from 97.96485
196/196 - 28s - loss: 97.1196 - MinusLogProbMetric: 97.1196 - val_loss: 99.1266 - val_MinusLogProbMetric: 99.1266 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 17/1000
2023-10-27 04:17:42.610 
Epoch 17/1000 
	 loss: 98.7252, MinusLogProbMetric: 98.7252, val_loss: 94.0666, val_MinusLogProbMetric: 94.0666

Epoch 17: val_loss improved from 97.96485 to 94.06660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 29s - loss: 98.7252 - MinusLogProbMetric: 98.7252 - val_loss: 94.0666 - val_MinusLogProbMetric: 94.0666 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 18/1000
2023-10-27 04:18:13.825 
Epoch 18/1000 
	 loss: 91.8182, MinusLogProbMetric: 91.8182, val_loss: 90.5397, val_MinusLogProbMetric: 90.5397

Epoch 18: val_loss improved from 94.06660 to 90.53973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 91.8182 - MinusLogProbMetric: 91.8182 - val_loss: 90.5397 - val_MinusLogProbMetric: 90.5397 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 19/1000
2023-10-27 04:18:45.687 
Epoch 19/1000 
	 loss: 89.4007, MinusLogProbMetric: 89.4007, val_loss: 88.3527, val_MinusLogProbMetric: 88.3527

Epoch 19: val_loss improved from 90.53973 to 88.35270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 32s - loss: 89.4007 - MinusLogProbMetric: 89.4007 - val_loss: 88.3527 - val_MinusLogProbMetric: 88.3527 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 20/1000
2023-10-27 04:19:14.982 
Epoch 20/1000 
	 loss: 86.9764, MinusLogProbMetric: 86.9764, val_loss: 87.5122, val_MinusLogProbMetric: 87.5122

Epoch 20: val_loss improved from 88.35270 to 87.51223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 29s - loss: 86.9764 - MinusLogProbMetric: 86.9764 - val_loss: 87.5122 - val_MinusLogProbMetric: 87.5122 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 21/1000
2023-10-27 04:19:42.776 
Epoch 21/1000 
	 loss: 85.5917, MinusLogProbMetric: 85.5917, val_loss: 84.3667, val_MinusLogProbMetric: 84.3667

Epoch 21: val_loss improved from 87.51223 to 84.36674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 85.5917 - MinusLogProbMetric: 85.5917 - val_loss: 84.3667 - val_MinusLogProbMetric: 84.3667 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 22/1000
2023-10-27 04:20:12.528 
Epoch 22/1000 
	 loss: 83.6489, MinusLogProbMetric: 83.6489, val_loss: 83.6919, val_MinusLogProbMetric: 83.6919

Epoch 22: val_loss improved from 84.36674 to 83.69189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 83.6489 - MinusLogProbMetric: 83.6489 - val_loss: 83.6919 - val_MinusLogProbMetric: 83.6919 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 23/1000
2023-10-27 04:20:41.811 
Epoch 23/1000 
	 loss: 82.3402, MinusLogProbMetric: 82.3402, val_loss: 81.0851, val_MinusLogProbMetric: 81.0851

Epoch 23: val_loss improved from 83.69189 to 81.08507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 29s - loss: 82.3402 - MinusLogProbMetric: 82.3402 - val_loss: 81.0851 - val_MinusLogProbMetric: 81.0851 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 24/1000
2023-10-27 04:21:10.335 
Epoch 24/1000 
	 loss: 81.0027, MinusLogProbMetric: 81.0027, val_loss: 83.5613, val_MinusLogProbMetric: 83.5613

Epoch 24: val_loss did not improve from 81.08507
196/196 - 28s - loss: 81.0027 - MinusLogProbMetric: 81.0027 - val_loss: 83.5613 - val_MinusLogProbMetric: 83.5613 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 25/1000
2023-10-27 04:21:42.584 
Epoch 25/1000 
	 loss: 79.2731, MinusLogProbMetric: 79.2731, val_loss: 80.4625, val_MinusLogProbMetric: 80.4625

Epoch 25: val_loss improved from 81.08507 to 80.46254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 79.2731 - MinusLogProbMetric: 79.2731 - val_loss: 80.4625 - val_MinusLogProbMetric: 80.4625 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 26/1000
2023-10-27 04:22:11.903 
Epoch 26/1000 
	 loss: 83.7552, MinusLogProbMetric: 83.7552, val_loss: 85.3342, val_MinusLogProbMetric: 85.3342

Epoch 26: val_loss did not improve from 80.46254
196/196 - 29s - loss: 83.7552 - MinusLogProbMetric: 83.7552 - val_loss: 85.3342 - val_MinusLogProbMetric: 85.3342 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 27/1000
2023-10-27 04:22:39.557 
Epoch 27/1000 
	 loss: 83.9426, MinusLogProbMetric: 83.9426, val_loss: 83.1633, val_MinusLogProbMetric: 83.1633

Epoch 27: val_loss did not improve from 80.46254
196/196 - 28s - loss: 83.9426 - MinusLogProbMetric: 83.9426 - val_loss: 83.1633 - val_MinusLogProbMetric: 83.1633 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 28/1000
2023-10-27 04:23:10.330 
Epoch 28/1000 
	 loss: 80.0415, MinusLogProbMetric: 80.0415, val_loss: 78.6344, val_MinusLogProbMetric: 78.6344

Epoch 28: val_loss improved from 80.46254 to 78.63442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 80.0415 - MinusLogProbMetric: 80.0415 - val_loss: 78.6344 - val_MinusLogProbMetric: 78.6344 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 29/1000
2023-10-27 04:23:39.582 
Epoch 29/1000 
	 loss: 77.8480, MinusLogProbMetric: 77.8480, val_loss: 76.7428, val_MinusLogProbMetric: 76.7428

Epoch 29: val_loss improved from 78.63442 to 76.74280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 29s - loss: 77.8480 - MinusLogProbMetric: 77.8480 - val_loss: 76.7428 - val_MinusLogProbMetric: 76.7428 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 30/1000
2023-10-27 04:24:09.096 
Epoch 30/1000 
	 loss: 77.0973, MinusLogProbMetric: 77.0973, val_loss: 76.8419, val_MinusLogProbMetric: 76.8419

Epoch 30: val_loss did not improve from 76.74280
196/196 - 29s - loss: 77.0973 - MinusLogProbMetric: 77.0973 - val_loss: 76.8419 - val_MinusLogProbMetric: 76.8419 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 31/1000
2023-10-27 04:24:38.213 
Epoch 31/1000 
	 loss: 75.2876, MinusLogProbMetric: 75.2876, val_loss: 74.8821, val_MinusLogProbMetric: 74.8821

Epoch 31: val_loss improved from 76.74280 to 74.88207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 75.2876 - MinusLogProbMetric: 75.2876 - val_loss: 74.8821 - val_MinusLogProbMetric: 74.8821 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 32/1000
2023-10-27 04:25:07.936 
Epoch 32/1000 
	 loss: 74.5264, MinusLogProbMetric: 74.5264, val_loss: 74.9297, val_MinusLogProbMetric: 74.9297

Epoch 32: val_loss did not improve from 74.88207
196/196 - 29s - loss: 74.5264 - MinusLogProbMetric: 74.5264 - val_loss: 74.9297 - val_MinusLogProbMetric: 74.9297 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 33/1000
2023-10-27 04:25:35.460 
Epoch 33/1000 
	 loss: 73.3953, MinusLogProbMetric: 73.3953, val_loss: 72.8994, val_MinusLogProbMetric: 72.8994

Epoch 33: val_loss improved from 74.88207 to 72.89940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 73.3953 - MinusLogProbMetric: 73.3953 - val_loss: 72.8994 - val_MinusLogProbMetric: 72.8994 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 34/1000
2023-10-27 04:26:06.473 
Epoch 34/1000 
	 loss: 72.7153, MinusLogProbMetric: 72.7153, val_loss: 111.0189, val_MinusLogProbMetric: 111.0189

Epoch 34: val_loss did not improve from 72.89940
196/196 - 31s - loss: 72.7153 - MinusLogProbMetric: 72.7153 - val_loss: 111.0189 - val_MinusLogProbMetric: 111.0189 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 35/1000
2023-10-27 04:26:36.659 
Epoch 35/1000 
	 loss: 163.5481, MinusLogProbMetric: 163.5481, val_loss: 104.8311, val_MinusLogProbMetric: 104.8311

Epoch 35: val_loss did not improve from 72.89940
196/196 - 30s - loss: 163.5481 - MinusLogProbMetric: 163.5481 - val_loss: 104.8311 - val_MinusLogProbMetric: 104.8311 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 36/1000
2023-10-27 04:27:04.886 
Epoch 36/1000 
	 loss: 98.7469, MinusLogProbMetric: 98.7469, val_loss: 91.4313, val_MinusLogProbMetric: 91.4313

Epoch 36: val_loss did not improve from 72.89940
196/196 - 28s - loss: 98.7469 - MinusLogProbMetric: 98.7469 - val_loss: 91.4313 - val_MinusLogProbMetric: 91.4313 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 37/1000
2023-10-27 04:27:37.241 
Epoch 37/1000 
	 loss: 87.4965, MinusLogProbMetric: 87.4965, val_loss: 84.1501, val_MinusLogProbMetric: 84.1501

Epoch 37: val_loss did not improve from 72.89940
196/196 - 32s - loss: 87.4965 - MinusLogProbMetric: 87.4965 - val_loss: 84.1501 - val_MinusLogProbMetric: 84.1501 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 38/1000
2023-10-27 04:28:07.279 
Epoch 38/1000 
	 loss: 82.4802, MinusLogProbMetric: 82.4802, val_loss: 81.0652, val_MinusLogProbMetric: 81.0652

Epoch 38: val_loss did not improve from 72.89940
196/196 - 30s - loss: 82.4802 - MinusLogProbMetric: 82.4802 - val_loss: 81.0652 - val_MinusLogProbMetric: 81.0652 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 39/1000
2023-10-27 04:28:35.303 
Epoch 39/1000 
	 loss: 79.2512, MinusLogProbMetric: 79.2512, val_loss: 78.2300, val_MinusLogProbMetric: 78.2300

Epoch 39: val_loss did not improve from 72.89940
196/196 - 28s - loss: 79.2512 - MinusLogProbMetric: 79.2512 - val_loss: 78.2300 - val_MinusLogProbMetric: 78.2300 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 40/1000
2023-10-27 04:29:07.451 
Epoch 40/1000 
	 loss: 77.1031, MinusLogProbMetric: 77.1031, val_loss: 77.1669, val_MinusLogProbMetric: 77.1669

Epoch 40: val_loss did not improve from 72.89940
196/196 - 32s - loss: 77.1031 - MinusLogProbMetric: 77.1031 - val_loss: 77.1669 - val_MinusLogProbMetric: 77.1669 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 41/1000
2023-10-27 04:29:37.621 
Epoch 41/1000 
	 loss: 75.5308, MinusLogProbMetric: 75.5308, val_loss: 74.8533, val_MinusLogProbMetric: 74.8533

Epoch 41: val_loss did not improve from 72.89940
196/196 - 30s - loss: 75.5308 - MinusLogProbMetric: 75.5308 - val_loss: 74.8533 - val_MinusLogProbMetric: 74.8533 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 42/1000
2023-10-27 04:30:04.692 
Epoch 42/1000 
	 loss: 74.6736, MinusLogProbMetric: 74.6736, val_loss: 74.0683, val_MinusLogProbMetric: 74.0683

Epoch 42: val_loss did not improve from 72.89940
196/196 - 27s - loss: 74.6736 - MinusLogProbMetric: 74.6736 - val_loss: 74.0683 - val_MinusLogProbMetric: 74.0683 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 43/1000
2023-10-27 04:30:34.422 
Epoch 43/1000 
	 loss: 74.3195, MinusLogProbMetric: 74.3195, val_loss: 73.4030, val_MinusLogProbMetric: 73.4030

Epoch 43: val_loss did not improve from 72.89940
196/196 - 30s - loss: 74.3195 - MinusLogProbMetric: 74.3195 - val_loss: 73.4030 - val_MinusLogProbMetric: 73.4030 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 44/1000
2023-10-27 04:31:04.230 
Epoch 44/1000 
	 loss: 72.7428, MinusLogProbMetric: 72.7428, val_loss: 71.6574, val_MinusLogProbMetric: 71.6574

Epoch 44: val_loss improved from 72.89940 to 71.65740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 72.7428 - MinusLogProbMetric: 72.7428 - val_loss: 71.6574 - val_MinusLogProbMetric: 71.6574 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 45/1000
2023-10-27 04:31:32.335 
Epoch 45/1000 
	 loss: 197.3045, MinusLogProbMetric: 197.3045, val_loss: 495.9547, val_MinusLogProbMetric: 495.9547

Epoch 45: val_loss did not improve from 71.65740
196/196 - 28s - loss: 197.3045 - MinusLogProbMetric: 197.3045 - val_loss: 495.9547 - val_MinusLogProbMetric: 495.9547 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 46/1000
2023-10-27 04:32:03.926 
Epoch 46/1000 
	 loss: 236.2451, MinusLogProbMetric: 236.2451, val_loss: 178.5673, val_MinusLogProbMetric: 178.5673

Epoch 46: val_loss did not improve from 71.65740
196/196 - 32s - loss: 236.2451 - MinusLogProbMetric: 236.2451 - val_loss: 178.5673 - val_MinusLogProbMetric: 178.5673 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 47/1000
2023-10-27 04:32:34.006 
Epoch 47/1000 
	 loss: 154.4119, MinusLogProbMetric: 154.4119, val_loss: 135.9008, val_MinusLogProbMetric: 135.9008

Epoch 47: val_loss did not improve from 71.65740
196/196 - 30s - loss: 154.4119 - MinusLogProbMetric: 154.4119 - val_loss: 135.9008 - val_MinusLogProbMetric: 135.9008 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 48/1000
2023-10-27 04:33:02.778 
Epoch 48/1000 
	 loss: 134.1028, MinusLogProbMetric: 134.1028, val_loss: 127.1992, val_MinusLogProbMetric: 127.1992

Epoch 48: val_loss did not improve from 71.65740
196/196 - 29s - loss: 134.1028 - MinusLogProbMetric: 134.1028 - val_loss: 127.1992 - val_MinusLogProbMetric: 127.1992 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 49/1000
2023-10-27 04:33:30.193 
Epoch 49/1000 
	 loss: 118.7409, MinusLogProbMetric: 118.7409, val_loss: 113.6773, val_MinusLogProbMetric: 113.6773

Epoch 49: val_loss did not improve from 71.65740
196/196 - 27s - loss: 118.7409 - MinusLogProbMetric: 118.7409 - val_loss: 113.6773 - val_MinusLogProbMetric: 113.6773 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 50/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 72: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:33:42.958 
Epoch 50/1000 
	 loss: nan, MinusLogProbMetric: 251.4593, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 50: val_loss did not improve from 71.65740
196/196 - 13s - loss: nan - MinusLogProbMetric: 251.4593 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 13s/epoch - 65ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 402.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_402/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_402/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_402/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_402
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_425"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_426 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7fec4c395690>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb87e7f070>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb87e7f070>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef6871a170>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb87ec2da0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb87ec00d0>, <keras.callbacks.ModelCheckpoint object at 0x7feb87ec3c10>, <keras.callbacks.EarlyStopping object at 0x7feb87ec0b20>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb87ec37c0>, <keras.callbacks.TerminateOnNaN object at 0x7feb87ec3e20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 402/720 with hyperparameters:
timestamp = 2023-10-27 04:33:48.494026
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
2023-10-27 04:35:19.940 
Epoch 1/1000 
	 loss: 96.9680, MinusLogProbMetric: 96.9680, val_loss: 74.5674, val_MinusLogProbMetric: 74.5674

Epoch 1: val_loss improved from inf to 74.56738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 92s - loss: 96.9680 - MinusLogProbMetric: 96.9680 - val_loss: 74.5674 - val_MinusLogProbMetric: 74.5674 - lr: 1.1111e-04 - 92s/epoch - 468ms/step
Epoch 2/1000
2023-10-27 04:35:48.071 
Epoch 2/1000 
	 loss: 77.3476, MinusLogProbMetric: 77.3476, val_loss: 73.2648, val_MinusLogProbMetric: 73.2648

Epoch 2: val_loss improved from 74.56738 to 73.26479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 77.3476 - MinusLogProbMetric: 77.3476 - val_loss: 73.2648 - val_MinusLogProbMetric: 73.2648 - lr: 1.1111e-04 - 28s/epoch - 143ms/step
Epoch 3/1000
2023-10-27 04:36:18.061 
Epoch 3/1000 
	 loss: 71.3421, MinusLogProbMetric: 71.3421, val_loss: 74.0808, val_MinusLogProbMetric: 74.0808

Epoch 3: val_loss did not improve from 73.26479
196/196 - 30s - loss: 71.3421 - MinusLogProbMetric: 71.3421 - val_loss: 74.0808 - val_MinusLogProbMetric: 74.0808 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 4/1000
2023-10-27 04:36:49.249 
Epoch 4/1000 
	 loss: 105.7249, MinusLogProbMetric: 105.7249, val_loss: 83.4907, val_MinusLogProbMetric: 83.4907

Epoch 4: val_loss did not improve from 73.26479
196/196 - 31s - loss: 105.7249 - MinusLogProbMetric: 105.7249 - val_loss: 83.4907 - val_MinusLogProbMetric: 83.4907 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 5/1000
2023-10-27 04:37:16.624 
Epoch 5/1000 
	 loss: 76.3858, MinusLogProbMetric: 76.3858, val_loss: 73.8476, val_MinusLogProbMetric: 73.8476

Epoch 5: val_loss did not improve from 73.26479
196/196 - 27s - loss: 76.3858 - MinusLogProbMetric: 76.3858 - val_loss: 73.8476 - val_MinusLogProbMetric: 73.8476 - lr: 1.1111e-04 - 27s/epoch - 140ms/step
Epoch 6/1000
2023-10-27 04:37:44.399 
Epoch 6/1000 
	 loss: 70.1722, MinusLogProbMetric: 70.1722, val_loss: 68.6350, val_MinusLogProbMetric: 68.6350

Epoch 6: val_loss improved from 73.26479 to 68.63496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 70.1722 - MinusLogProbMetric: 70.1722 - val_loss: 68.6350 - val_MinusLogProbMetric: 68.6350 - lr: 1.1111e-04 - 28s/epoch - 144ms/step
Epoch 7/1000
2023-10-27 04:38:16.247 
Epoch 7/1000 
	 loss: 67.4576, MinusLogProbMetric: 67.4576, val_loss: 66.9617, val_MinusLogProbMetric: 66.9617

Epoch 7: val_loss improved from 68.63496 to 66.96165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 32s - loss: 67.4576 - MinusLogProbMetric: 67.4576 - val_loss: 66.9617 - val_MinusLogProbMetric: 66.9617 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 8/1000
2023-10-27 04:38:50.496 
Epoch 8/1000 
	 loss: 65.5320, MinusLogProbMetric: 65.5320, val_loss: 65.1183, val_MinusLogProbMetric: 65.1183

Epoch 8: val_loss improved from 66.96165 to 65.11827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 65.5320 - MinusLogProbMetric: 65.5320 - val_loss: 65.1183 - val_MinusLogProbMetric: 65.1183 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 9/1000
2023-10-27 04:39:21.341 
Epoch 9/1000 
	 loss: 63.9573, MinusLogProbMetric: 63.9573, val_loss: 63.1888, val_MinusLogProbMetric: 63.1888

Epoch 9: val_loss improved from 65.11827 to 63.18878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 63.9573 - MinusLogProbMetric: 63.9573 - val_loss: 63.1888 - val_MinusLogProbMetric: 63.1888 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 10/1000
2023-10-27 04:39:50.982 
Epoch 10/1000 
	 loss: 63.2131, MinusLogProbMetric: 63.2131, val_loss: 62.7173, val_MinusLogProbMetric: 62.7173

Epoch 10: val_loss improved from 63.18878 to 62.71727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 63.2131 - MinusLogProbMetric: 63.2131 - val_loss: 62.7173 - val_MinusLogProbMetric: 62.7173 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 11/1000
2023-10-27 04:40:23.014 
Epoch 11/1000 
	 loss: 62.2416, MinusLogProbMetric: 62.2416, val_loss: 64.0875, val_MinusLogProbMetric: 64.0875

Epoch 11: val_loss did not improve from 62.71727
196/196 - 31s - loss: 62.2416 - MinusLogProbMetric: 62.2416 - val_loss: 64.0875 - val_MinusLogProbMetric: 64.0875 - lr: 1.1111e-04 - 31s/epoch - 161ms/step
Epoch 12/1000
2023-10-27 04:40:57.131 
Epoch 12/1000 
	 loss: 60.8717, MinusLogProbMetric: 60.8717, val_loss: 61.4476, val_MinusLogProbMetric: 61.4476

Epoch 12: val_loss improved from 62.71727 to 61.44757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 60.8717 - MinusLogProbMetric: 60.8717 - val_loss: 61.4476 - val_MinusLogProbMetric: 61.4476 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 13/1000
2023-10-27 04:41:30.746 
Epoch 13/1000 
	 loss: 60.6308, MinusLogProbMetric: 60.6308, val_loss: 60.3770, val_MinusLogProbMetric: 60.3770

Epoch 13: val_loss improved from 61.44757 to 60.37705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 60.6308 - MinusLogProbMetric: 60.6308 - val_loss: 60.3770 - val_MinusLogProbMetric: 60.3770 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 14/1000
2023-10-27 04:42:05.462 
Epoch 14/1000 
	 loss: 60.0986, MinusLogProbMetric: 60.0986, val_loss: 60.8174, val_MinusLogProbMetric: 60.8174

Epoch 14: val_loss did not improve from 60.37705
196/196 - 34s - loss: 60.0986 - MinusLogProbMetric: 60.0986 - val_loss: 60.8174 - val_MinusLogProbMetric: 60.8174 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 15/1000
2023-10-27 04:42:39.170 
Epoch 15/1000 
	 loss: 59.4878, MinusLogProbMetric: 59.4878, val_loss: 60.7201, val_MinusLogProbMetric: 60.7201

Epoch 15: val_loss did not improve from 60.37705
196/196 - 34s - loss: 59.4878 - MinusLogProbMetric: 59.4878 - val_loss: 60.7201 - val_MinusLogProbMetric: 60.7201 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 16/1000
2023-10-27 04:43:12.915 
Epoch 16/1000 
	 loss: 59.0107, MinusLogProbMetric: 59.0107, val_loss: 57.9800, val_MinusLogProbMetric: 57.9800

Epoch 16: val_loss improved from 60.37705 to 57.98004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 59.0107 - MinusLogProbMetric: 59.0107 - val_loss: 57.9800 - val_MinusLogProbMetric: 57.9800 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 17/1000
2023-10-27 04:43:47.324 
Epoch 17/1000 
	 loss: 120.2410, MinusLogProbMetric: 120.2410, val_loss: 116.0126, val_MinusLogProbMetric: 116.0126

Epoch 17: val_loss did not improve from 57.98004
196/196 - 34s - loss: 120.2410 - MinusLogProbMetric: 120.2410 - val_loss: 116.0126 - val_MinusLogProbMetric: 116.0126 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 18/1000
2023-10-27 04:44:21.015 
Epoch 18/1000 
	 loss: 94.6473, MinusLogProbMetric: 94.6473, val_loss: 83.7585, val_MinusLogProbMetric: 83.7585

Epoch 18: val_loss did not improve from 57.98004
196/196 - 34s - loss: 94.6473 - MinusLogProbMetric: 94.6473 - val_loss: 83.7585 - val_MinusLogProbMetric: 83.7585 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 19/1000
2023-10-27 04:44:54.788 
Epoch 19/1000 
	 loss: 79.3045, MinusLogProbMetric: 79.3045, val_loss: 75.3529, val_MinusLogProbMetric: 75.3529

Epoch 19: val_loss did not improve from 57.98004
196/196 - 34s - loss: 79.3045 - MinusLogProbMetric: 79.3045 - val_loss: 75.3529 - val_MinusLogProbMetric: 75.3529 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 20/1000
2023-10-27 04:45:28.772 
Epoch 20/1000 
	 loss: 73.5428, MinusLogProbMetric: 73.5428, val_loss: 70.9525, val_MinusLogProbMetric: 70.9525

Epoch 20: val_loss did not improve from 57.98004
196/196 - 34s - loss: 73.5428 - MinusLogProbMetric: 73.5428 - val_loss: 70.9525 - val_MinusLogProbMetric: 70.9525 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 21/1000
2023-10-27 04:46:02.767 
Epoch 21/1000 
	 loss: 74.1532, MinusLogProbMetric: 74.1532, val_loss: 70.8604, val_MinusLogProbMetric: 70.8604

Epoch 21: val_loss did not improve from 57.98004
196/196 - 34s - loss: 74.1532 - MinusLogProbMetric: 74.1532 - val_loss: 70.8604 - val_MinusLogProbMetric: 70.8604 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 22/1000
2023-10-27 04:46:36.152 
Epoch 22/1000 
	 loss: 67.9748, MinusLogProbMetric: 67.9748, val_loss: 66.2512, val_MinusLogProbMetric: 66.2512

Epoch 22: val_loss did not improve from 57.98004
196/196 - 33s - loss: 67.9748 - MinusLogProbMetric: 67.9748 - val_loss: 66.2512 - val_MinusLogProbMetric: 66.2512 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 23/1000
2023-10-27 04:47:09.581 
Epoch 23/1000 
	 loss: 64.7463, MinusLogProbMetric: 64.7463, val_loss: 63.5239, val_MinusLogProbMetric: 63.5239

Epoch 23: val_loss did not improve from 57.98004
196/196 - 33s - loss: 64.7463 - MinusLogProbMetric: 64.7463 - val_loss: 63.5239 - val_MinusLogProbMetric: 63.5239 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 24/1000
2023-10-27 04:47:43.614 
Epoch 24/1000 
	 loss: 61.3703, MinusLogProbMetric: 61.3703, val_loss: 62.2654, val_MinusLogProbMetric: 62.2654

Epoch 24: val_loss did not improve from 57.98004
196/196 - 34s - loss: 61.3703 - MinusLogProbMetric: 61.3703 - val_loss: 62.2654 - val_MinusLogProbMetric: 62.2654 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 25/1000
2023-10-27 04:48:17.165 
Epoch 25/1000 
	 loss: 60.4679, MinusLogProbMetric: 60.4679, val_loss: 59.5728, val_MinusLogProbMetric: 59.5728

Epoch 25: val_loss did not improve from 57.98004
196/196 - 34s - loss: 60.4679 - MinusLogProbMetric: 60.4679 - val_loss: 59.5728 - val_MinusLogProbMetric: 59.5728 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 26/1000
2023-10-27 04:48:49.750 
Epoch 26/1000 
	 loss: 59.3975, MinusLogProbMetric: 59.3975, val_loss: 59.4010, val_MinusLogProbMetric: 59.4010

Epoch 26: val_loss did not improve from 57.98004
196/196 - 33s - loss: 59.3975 - MinusLogProbMetric: 59.3975 - val_loss: 59.4010 - val_MinusLogProbMetric: 59.4010 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 27/1000
2023-10-27 04:49:22.108 
Epoch 27/1000 
	 loss: 58.7959, MinusLogProbMetric: 58.7959, val_loss: 59.2587, val_MinusLogProbMetric: 59.2587

Epoch 27: val_loss did not improve from 57.98004
196/196 - 32s - loss: 58.7959 - MinusLogProbMetric: 58.7959 - val_loss: 59.2587 - val_MinusLogProbMetric: 59.2587 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 28/1000
2023-10-27 04:49:55.866 
Epoch 28/1000 
	 loss: 58.0641, MinusLogProbMetric: 58.0641, val_loss: 58.6614, val_MinusLogProbMetric: 58.6614

Epoch 28: val_loss did not improve from 57.98004
196/196 - 34s - loss: 58.0641 - MinusLogProbMetric: 58.0641 - val_loss: 58.6614 - val_MinusLogProbMetric: 58.6614 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 29/1000
2023-10-27 04:50:29.659 
Epoch 29/1000 
	 loss: 57.7930, MinusLogProbMetric: 57.7930, val_loss: 56.7365, val_MinusLogProbMetric: 56.7365

Epoch 29: val_loss improved from 57.98004 to 56.73649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 57.7930 - MinusLogProbMetric: 57.7930 - val_loss: 56.7365 - val_MinusLogProbMetric: 56.7365 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 30/1000
2023-10-27 04:51:03.840 
Epoch 30/1000 
	 loss: 57.2509, MinusLogProbMetric: 57.2509, val_loss: 56.6205, val_MinusLogProbMetric: 56.6205

Epoch 30: val_loss improved from 56.73649 to 56.62049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 57.2509 - MinusLogProbMetric: 57.2509 - val_loss: 56.6205 - val_MinusLogProbMetric: 56.6205 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 31/1000
2023-10-27 04:51:37.136 
Epoch 31/1000 
	 loss: 56.9481, MinusLogProbMetric: 56.9481, val_loss: 57.3591, val_MinusLogProbMetric: 57.3591

Epoch 31: val_loss did not improve from 56.62049
196/196 - 33s - loss: 56.9481 - MinusLogProbMetric: 56.9481 - val_loss: 57.3591 - val_MinusLogProbMetric: 57.3591 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 32/1000
2023-10-27 04:52:10.781 
Epoch 32/1000 
	 loss: 56.2890, MinusLogProbMetric: 56.2890, val_loss: 57.7338, val_MinusLogProbMetric: 57.7338

Epoch 32: val_loss did not improve from 56.62049
196/196 - 34s - loss: 56.2890 - MinusLogProbMetric: 56.2890 - val_loss: 57.7338 - val_MinusLogProbMetric: 57.7338 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 33/1000
2023-10-27 04:52:44.353 
Epoch 33/1000 
	 loss: 55.8696, MinusLogProbMetric: 55.8696, val_loss: 57.4885, val_MinusLogProbMetric: 57.4885

Epoch 33: val_loss did not improve from 56.62049
196/196 - 34s - loss: 55.8696 - MinusLogProbMetric: 55.8696 - val_loss: 57.4885 - val_MinusLogProbMetric: 57.4885 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 34/1000
2023-10-27 04:53:17.511 
Epoch 34/1000 
	 loss: 56.3061, MinusLogProbMetric: 56.3061, val_loss: 55.9506, val_MinusLogProbMetric: 55.9506

Epoch 34: val_loss improved from 56.62049 to 55.95060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 56.3061 - MinusLogProbMetric: 56.3061 - val_loss: 55.9506 - val_MinusLogProbMetric: 55.9506 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 35/1000
2023-10-27 04:53:51.988 
Epoch 35/1000 
	 loss: 55.5626, MinusLogProbMetric: 55.5626, val_loss: 56.4839, val_MinusLogProbMetric: 56.4839

Epoch 35: val_loss did not improve from 55.95060
196/196 - 34s - loss: 55.5626 - MinusLogProbMetric: 55.5626 - val_loss: 56.4839 - val_MinusLogProbMetric: 56.4839 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 36/1000
2023-10-27 04:54:25.664 
Epoch 36/1000 
	 loss: 55.0955, MinusLogProbMetric: 55.0955, val_loss: 55.3657, val_MinusLogProbMetric: 55.3657

Epoch 36: val_loss improved from 55.95060 to 55.36568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 55.0955 - MinusLogProbMetric: 55.0955 - val_loss: 55.3657 - val_MinusLogProbMetric: 55.3657 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 37/1000
2023-10-27 04:55:00.222 
Epoch 37/1000 
	 loss: 54.8420, MinusLogProbMetric: 54.8420, val_loss: 54.3613, val_MinusLogProbMetric: 54.3613

Epoch 37: val_loss improved from 55.36568 to 54.36133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 54.8420 - MinusLogProbMetric: 54.8420 - val_loss: 54.3613 - val_MinusLogProbMetric: 54.3613 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 38/1000
2023-10-27 04:55:34.453 
Epoch 38/1000 
	 loss: 54.7319, MinusLogProbMetric: 54.7319, val_loss: 56.0968, val_MinusLogProbMetric: 56.0968

Epoch 38: val_loss did not improve from 54.36133
196/196 - 34s - loss: 54.7319 - MinusLogProbMetric: 54.7319 - val_loss: 56.0968 - val_MinusLogProbMetric: 56.0968 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 39/1000
2023-10-27 04:56:08.147 
Epoch 39/1000 
	 loss: 54.0079, MinusLogProbMetric: 54.0079, val_loss: 56.2904, val_MinusLogProbMetric: 56.2904

Epoch 39: val_loss did not improve from 54.36133
196/196 - 34s - loss: 54.0079 - MinusLogProbMetric: 54.0079 - val_loss: 56.2904 - val_MinusLogProbMetric: 56.2904 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 40/1000
2023-10-27 04:56:41.835 
Epoch 40/1000 
	 loss: 54.4473, MinusLogProbMetric: 54.4473, val_loss: 56.1861, val_MinusLogProbMetric: 56.1861

Epoch 40: val_loss did not improve from 54.36133
196/196 - 34s - loss: 54.4473 - MinusLogProbMetric: 54.4473 - val_loss: 56.1861 - val_MinusLogProbMetric: 56.1861 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 41/1000
2023-10-27 04:57:15.711 
Epoch 41/1000 
	 loss: 54.0410, MinusLogProbMetric: 54.0410, val_loss: 57.0142, val_MinusLogProbMetric: 57.0142

Epoch 41: val_loss did not improve from 54.36133
196/196 - 34s - loss: 54.0410 - MinusLogProbMetric: 54.0410 - val_loss: 57.0142 - val_MinusLogProbMetric: 57.0142 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 42/1000
2023-10-27 04:57:49.692 
Epoch 42/1000 
	 loss: 53.9539, MinusLogProbMetric: 53.9539, val_loss: 54.7045, val_MinusLogProbMetric: 54.7045

Epoch 42: val_loss did not improve from 54.36133
196/196 - 34s - loss: 53.9539 - MinusLogProbMetric: 53.9539 - val_loss: 54.7045 - val_MinusLogProbMetric: 54.7045 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 43/1000
2023-10-27 04:58:23.058 
Epoch 43/1000 
	 loss: 53.7782, MinusLogProbMetric: 53.7782, val_loss: 54.2164, val_MinusLogProbMetric: 54.2164

Epoch 43: val_loss improved from 54.36133 to 54.21637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 53.7782 - MinusLogProbMetric: 53.7782 - val_loss: 54.2164 - val_MinusLogProbMetric: 54.2164 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 44/1000
2023-10-27 04:58:57.193 
Epoch 44/1000 
	 loss: 53.5163, MinusLogProbMetric: 53.5163, val_loss: 53.5371, val_MinusLogProbMetric: 53.5371

Epoch 44: val_loss improved from 54.21637 to 53.53706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 53.5163 - MinusLogProbMetric: 53.5163 - val_loss: 53.5371 - val_MinusLogProbMetric: 53.5371 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 45/1000
2023-10-27 04:59:31.472 
Epoch 45/1000 
	 loss: 53.6630, MinusLogProbMetric: 53.6630, val_loss: 54.2247, val_MinusLogProbMetric: 54.2247

Epoch 45: val_loss did not improve from 53.53706
196/196 - 34s - loss: 53.6630 - MinusLogProbMetric: 53.6630 - val_loss: 54.2247 - val_MinusLogProbMetric: 54.2247 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 46/1000
2023-10-27 05:00:04.912 
Epoch 46/1000 
	 loss: 53.4635, MinusLogProbMetric: 53.4635, val_loss: 52.8776, val_MinusLogProbMetric: 52.8776

Epoch 46: val_loss improved from 53.53706 to 52.87756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 53.4635 - MinusLogProbMetric: 53.4635 - val_loss: 52.8776 - val_MinusLogProbMetric: 52.8776 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 47/1000
2023-10-27 05:00:39.051 
Epoch 47/1000 
	 loss: 53.1814, MinusLogProbMetric: 53.1814, val_loss: 52.9744, val_MinusLogProbMetric: 52.9744

Epoch 47: val_loss did not improve from 52.87756
196/196 - 33s - loss: 53.1814 - MinusLogProbMetric: 53.1814 - val_loss: 52.9744 - val_MinusLogProbMetric: 52.9744 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 48/1000
2023-10-27 05:01:12.753 
Epoch 48/1000 
	 loss: 53.8265, MinusLogProbMetric: 53.8265, val_loss: 53.3083, val_MinusLogProbMetric: 53.3083

Epoch 48: val_loss did not improve from 52.87756
196/196 - 34s - loss: 53.8265 - MinusLogProbMetric: 53.8265 - val_loss: 53.3083 - val_MinusLogProbMetric: 53.3083 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 49/1000
2023-10-27 05:01:46.094 
Epoch 49/1000 
	 loss: 53.0566, MinusLogProbMetric: 53.0566, val_loss: 53.8719, val_MinusLogProbMetric: 53.8719

Epoch 49: val_loss did not improve from 52.87756
196/196 - 33s - loss: 53.0566 - MinusLogProbMetric: 53.0566 - val_loss: 53.8719 - val_MinusLogProbMetric: 53.8719 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 50/1000
2023-10-27 05:02:19.426 
Epoch 50/1000 
	 loss: 52.8823, MinusLogProbMetric: 52.8823, val_loss: 54.0308, val_MinusLogProbMetric: 54.0308

Epoch 50: val_loss did not improve from 52.87756
196/196 - 33s - loss: 52.8823 - MinusLogProbMetric: 52.8823 - val_loss: 54.0308 - val_MinusLogProbMetric: 54.0308 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 51/1000
2023-10-27 05:02:52.934 
Epoch 51/1000 
	 loss: 52.9827, MinusLogProbMetric: 52.9827, val_loss: 52.3310, val_MinusLogProbMetric: 52.3310

Epoch 51: val_loss improved from 52.87756 to 52.33104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 52.9827 - MinusLogProbMetric: 52.9827 - val_loss: 52.3310 - val_MinusLogProbMetric: 52.3310 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 52/1000
2023-10-27 05:03:27.124 
Epoch 52/1000 
	 loss: 52.5678, MinusLogProbMetric: 52.5678, val_loss: 52.6542, val_MinusLogProbMetric: 52.6542

Epoch 52: val_loss did not improve from 52.33104
196/196 - 34s - loss: 52.5678 - MinusLogProbMetric: 52.5678 - val_loss: 52.6542 - val_MinusLogProbMetric: 52.6542 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 53/1000
2023-10-27 05:04:00.461 
Epoch 53/1000 
	 loss: 52.4234, MinusLogProbMetric: 52.4234, val_loss: 52.1114, val_MinusLogProbMetric: 52.1114

Epoch 53: val_loss improved from 52.33104 to 52.11140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 52.4234 - MinusLogProbMetric: 52.4234 - val_loss: 52.1114 - val_MinusLogProbMetric: 52.1114 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 54/1000
2023-10-27 05:04:34.350 
Epoch 54/1000 
	 loss: 51.7371, MinusLogProbMetric: 51.7371, val_loss: 55.8415, val_MinusLogProbMetric: 55.8415

Epoch 54: val_loss did not improve from 52.11140
196/196 - 33s - loss: 51.7371 - MinusLogProbMetric: 51.7371 - val_loss: 55.8415 - val_MinusLogProbMetric: 55.8415 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 55/1000
2023-10-27 05:05:07.255 
Epoch 55/1000 
	 loss: 52.6434, MinusLogProbMetric: 52.6434, val_loss: 54.8871, val_MinusLogProbMetric: 54.8871

Epoch 55: val_loss did not improve from 52.11140
196/196 - 33s - loss: 52.6434 - MinusLogProbMetric: 52.6434 - val_loss: 54.8871 - val_MinusLogProbMetric: 54.8871 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 56/1000
2023-10-27 05:05:41.005 
Epoch 56/1000 
	 loss: 52.0225, MinusLogProbMetric: 52.0225, val_loss: 51.6603, val_MinusLogProbMetric: 51.6603

Epoch 56: val_loss improved from 52.11140 to 51.66029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 52.0225 - MinusLogProbMetric: 52.0225 - val_loss: 51.6603 - val_MinusLogProbMetric: 51.6603 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 57/1000
2023-10-27 05:06:15.186 
Epoch 57/1000 
	 loss: 51.7144, MinusLogProbMetric: 51.7144, val_loss: 51.6078, val_MinusLogProbMetric: 51.6078

Epoch 57: val_loss improved from 51.66029 to 51.60782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 51.7144 - MinusLogProbMetric: 51.7144 - val_loss: 51.6078 - val_MinusLogProbMetric: 51.6078 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 58/1000
2023-10-27 05:06:49.694 
Epoch 58/1000 
	 loss: 51.7937, MinusLogProbMetric: 51.7937, val_loss: 50.5811, val_MinusLogProbMetric: 50.5811

Epoch 58: val_loss improved from 51.60782 to 50.58110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 51.7937 - MinusLogProbMetric: 51.7937 - val_loss: 50.5811 - val_MinusLogProbMetric: 50.5811 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 59/1000
2023-10-27 05:07:23.922 
Epoch 59/1000 
	 loss: 51.3780, MinusLogProbMetric: 51.3780, val_loss: 51.0148, val_MinusLogProbMetric: 51.0148

Epoch 59: val_loss did not improve from 50.58110
196/196 - 34s - loss: 51.3780 - MinusLogProbMetric: 51.3780 - val_loss: 51.0148 - val_MinusLogProbMetric: 51.0148 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 60/1000
2023-10-27 05:07:57.255 
Epoch 60/1000 
	 loss: 50.7180, MinusLogProbMetric: 50.7180, val_loss: 50.7914, val_MinusLogProbMetric: 50.7914

Epoch 60: val_loss did not improve from 50.58110
196/196 - 33s - loss: 50.7180 - MinusLogProbMetric: 50.7180 - val_loss: 50.7914 - val_MinusLogProbMetric: 50.7914 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 61/1000
2023-10-27 05:08:30.579 
Epoch 61/1000 
	 loss: 50.5898, MinusLogProbMetric: 50.5898, val_loss: 49.8069, val_MinusLogProbMetric: 49.8069

Epoch 61: val_loss improved from 50.58110 to 49.80692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 50.5898 - MinusLogProbMetric: 50.5898 - val_loss: 49.8069 - val_MinusLogProbMetric: 49.8069 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 62/1000
2023-10-27 05:09:04.383 
Epoch 62/1000 
	 loss: 50.0788, MinusLogProbMetric: 50.0788, val_loss: 51.1595, val_MinusLogProbMetric: 51.1595

Epoch 62: val_loss did not improve from 49.80692
196/196 - 33s - loss: 50.0788 - MinusLogProbMetric: 50.0788 - val_loss: 51.1595 - val_MinusLogProbMetric: 51.1595 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 63/1000
2023-10-27 05:09:37.833 
Epoch 63/1000 
	 loss: 50.1684, MinusLogProbMetric: 50.1684, val_loss: 50.0834, val_MinusLogProbMetric: 50.0834

Epoch 63: val_loss did not improve from 49.80692
196/196 - 33s - loss: 50.1684 - MinusLogProbMetric: 50.1684 - val_loss: 50.0834 - val_MinusLogProbMetric: 50.0834 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 64/1000
2023-10-27 05:10:11.051 
Epoch 64/1000 
	 loss: 50.8205, MinusLogProbMetric: 50.8205, val_loss: 51.2758, val_MinusLogProbMetric: 51.2758

Epoch 64: val_loss did not improve from 49.80692
196/196 - 33s - loss: 50.8205 - MinusLogProbMetric: 50.8205 - val_loss: 51.2758 - val_MinusLogProbMetric: 51.2758 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 65/1000
2023-10-27 05:10:44.593 
Epoch 65/1000 
	 loss: 49.9861, MinusLogProbMetric: 49.9861, val_loss: 50.4386, val_MinusLogProbMetric: 50.4386

Epoch 65: val_loss did not improve from 49.80692
196/196 - 34s - loss: 49.9861 - MinusLogProbMetric: 49.9861 - val_loss: 50.4386 - val_MinusLogProbMetric: 50.4386 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 66/1000
2023-10-27 05:11:17.805 
Epoch 66/1000 
	 loss: 49.7586, MinusLogProbMetric: 49.7586, val_loss: 49.6355, val_MinusLogProbMetric: 49.6355

Epoch 66: val_loss improved from 49.80692 to 49.63549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 49.7586 - MinusLogProbMetric: 49.7586 - val_loss: 49.6355 - val_MinusLogProbMetric: 49.6355 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 67/1000
2023-10-27 05:11:52.439 
Epoch 67/1000 
	 loss: 49.7471, MinusLogProbMetric: 49.7471, val_loss: 49.8399, val_MinusLogProbMetric: 49.8399

Epoch 67: val_loss did not improve from 49.63549
196/196 - 34s - loss: 49.7471 - MinusLogProbMetric: 49.7471 - val_loss: 49.8399 - val_MinusLogProbMetric: 49.8399 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 68/1000
2023-10-27 05:12:25.943 
Epoch 68/1000 
	 loss: 49.3903, MinusLogProbMetric: 49.3903, val_loss: 49.5236, val_MinusLogProbMetric: 49.5236

Epoch 68: val_loss improved from 49.63549 to 49.52362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 49.3903 - MinusLogProbMetric: 49.3903 - val_loss: 49.5236 - val_MinusLogProbMetric: 49.5236 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 69/1000
2023-10-27 05:13:00.221 
Epoch 69/1000 
	 loss: 49.7405, MinusLogProbMetric: 49.7405, val_loss: 49.3860, val_MinusLogProbMetric: 49.3860

Epoch 69: val_loss improved from 49.52362 to 49.38596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 49.7405 - MinusLogProbMetric: 49.7405 - val_loss: 49.3860 - val_MinusLogProbMetric: 49.3860 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 70/1000
2023-10-27 05:13:34.612 
Epoch 70/1000 
	 loss: 49.4206, MinusLogProbMetric: 49.4206, val_loss: 48.9269, val_MinusLogProbMetric: 48.9269

Epoch 70: val_loss improved from 49.38596 to 48.92693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 49.4206 - MinusLogProbMetric: 49.4206 - val_loss: 48.9269 - val_MinusLogProbMetric: 48.9269 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 71/1000
2023-10-27 05:14:09.327 
Epoch 71/1000 
	 loss: 50.1444, MinusLogProbMetric: 50.1444, val_loss: 48.7543, val_MinusLogProbMetric: 48.7543

Epoch 71: val_loss improved from 48.92693 to 48.75426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 50.1444 - MinusLogProbMetric: 50.1444 - val_loss: 48.7543 - val_MinusLogProbMetric: 48.7543 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 72/1000
2023-10-27 05:14:43.588 
Epoch 72/1000 
	 loss: 49.2351, MinusLogProbMetric: 49.2351, val_loss: 48.5810, val_MinusLogProbMetric: 48.5810

Epoch 72: val_loss improved from 48.75426 to 48.58105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 49.2351 - MinusLogProbMetric: 49.2351 - val_loss: 48.5810 - val_MinusLogProbMetric: 48.5810 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 73/1000
2023-10-27 05:15:18.290 
Epoch 73/1000 
	 loss: 49.3364, MinusLogProbMetric: 49.3364, val_loss: 49.7677, val_MinusLogProbMetric: 49.7677

Epoch 73: val_loss did not improve from 48.58105
196/196 - 34s - loss: 49.3364 - MinusLogProbMetric: 49.3364 - val_loss: 49.7677 - val_MinusLogProbMetric: 49.7677 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 74/1000
2023-10-27 05:15:52.161 
Epoch 74/1000 
	 loss: 49.4434, MinusLogProbMetric: 49.4434, val_loss: 50.4313, val_MinusLogProbMetric: 50.4313

Epoch 74: val_loss did not improve from 48.58105
196/196 - 34s - loss: 49.4434 - MinusLogProbMetric: 49.4434 - val_loss: 50.4313 - val_MinusLogProbMetric: 50.4313 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 75/1000
2023-10-27 05:16:25.602 
Epoch 75/1000 
	 loss: 48.8544, MinusLogProbMetric: 48.8544, val_loss: 51.0558, val_MinusLogProbMetric: 51.0558

Epoch 75: val_loss did not improve from 48.58105
196/196 - 33s - loss: 48.8544 - MinusLogProbMetric: 48.8544 - val_loss: 51.0558 - val_MinusLogProbMetric: 51.0558 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 76/1000
2023-10-27 05:16:59.169 
Epoch 76/1000 
	 loss: 49.0418, MinusLogProbMetric: 49.0418, val_loss: 48.0615, val_MinusLogProbMetric: 48.0615

Epoch 76: val_loss improved from 48.58105 to 48.06150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 49.0418 - MinusLogProbMetric: 49.0418 - val_loss: 48.0615 - val_MinusLogProbMetric: 48.0615 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 77/1000
2023-10-27 05:17:33.947 
Epoch 77/1000 
	 loss: 48.7156, MinusLogProbMetric: 48.7156, val_loss: 49.2098, val_MinusLogProbMetric: 49.2098

Epoch 77: val_loss did not improve from 48.06150
196/196 - 34s - loss: 48.7156 - MinusLogProbMetric: 48.7156 - val_loss: 49.2098 - val_MinusLogProbMetric: 49.2098 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 78/1000
2023-10-27 05:18:08.087 
Epoch 78/1000 
	 loss: 48.5270, MinusLogProbMetric: 48.5270, val_loss: 51.5861, val_MinusLogProbMetric: 51.5861

Epoch 78: val_loss did not improve from 48.06150
196/196 - 34s - loss: 48.5270 - MinusLogProbMetric: 48.5270 - val_loss: 51.5861 - val_MinusLogProbMetric: 51.5861 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 79/1000
2023-10-27 05:18:42.468 
Epoch 79/1000 
	 loss: 48.5698, MinusLogProbMetric: 48.5698, val_loss: 48.7896, val_MinusLogProbMetric: 48.7896

Epoch 79: val_loss did not improve from 48.06150
196/196 - 34s - loss: 48.5698 - MinusLogProbMetric: 48.5698 - val_loss: 48.7896 - val_MinusLogProbMetric: 48.7896 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 80/1000
2023-10-27 05:19:17.099 
Epoch 80/1000 
	 loss: 49.2870, MinusLogProbMetric: 49.2870, val_loss: 48.3397, val_MinusLogProbMetric: 48.3397

Epoch 80: val_loss did not improve from 48.06150
196/196 - 35s - loss: 49.2870 - MinusLogProbMetric: 49.2870 - val_loss: 48.3397 - val_MinusLogProbMetric: 48.3397 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 81/1000
2023-10-27 05:19:50.852 
Epoch 81/1000 
	 loss: 48.0730, MinusLogProbMetric: 48.0730, val_loss: 47.7551, val_MinusLogProbMetric: 47.7551

Epoch 81: val_loss improved from 48.06150 to 47.75510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 48.0730 - MinusLogProbMetric: 48.0730 - val_loss: 47.7551 - val_MinusLogProbMetric: 47.7551 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 82/1000
2023-10-27 05:20:24.972 
Epoch 82/1000 
	 loss: 48.2846, MinusLogProbMetric: 48.2846, val_loss: 49.6307, val_MinusLogProbMetric: 49.6307

Epoch 82: val_loss did not improve from 47.75510
196/196 - 34s - loss: 48.2846 - MinusLogProbMetric: 48.2846 - val_loss: 49.6307 - val_MinusLogProbMetric: 49.6307 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 83/1000
2023-10-27 05:20:58.602 
Epoch 83/1000 
	 loss: 48.5353, MinusLogProbMetric: 48.5353, val_loss: 48.4489, val_MinusLogProbMetric: 48.4489

Epoch 83: val_loss did not improve from 47.75510
196/196 - 34s - loss: 48.5353 - MinusLogProbMetric: 48.5353 - val_loss: 48.4489 - val_MinusLogProbMetric: 48.4489 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 84/1000
2023-10-27 05:21:32.793 
Epoch 84/1000 
	 loss: 48.8130, MinusLogProbMetric: 48.8130, val_loss: 48.5573, val_MinusLogProbMetric: 48.5573

Epoch 84: val_loss did not improve from 47.75510
196/196 - 34s - loss: 48.8130 - MinusLogProbMetric: 48.8130 - val_loss: 48.5573 - val_MinusLogProbMetric: 48.5573 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 85/1000
2023-10-27 05:22:06.779 
Epoch 85/1000 
	 loss: 48.2711, MinusLogProbMetric: 48.2711, val_loss: 48.1481, val_MinusLogProbMetric: 48.1481

Epoch 85: val_loss did not improve from 47.75510
196/196 - 34s - loss: 48.2711 - MinusLogProbMetric: 48.2711 - val_loss: 48.1481 - val_MinusLogProbMetric: 48.1481 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 86/1000
2023-10-27 05:22:41.605 
Epoch 86/1000 
	 loss: 48.0235, MinusLogProbMetric: 48.0235, val_loss: 47.3293, val_MinusLogProbMetric: 47.3293

Epoch 86: val_loss improved from 47.75510 to 47.32935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 48.0235 - MinusLogProbMetric: 48.0235 - val_loss: 47.3293 - val_MinusLogProbMetric: 47.3293 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 87/1000
2023-10-27 05:23:15.987 
Epoch 87/1000 
	 loss: 48.1391, MinusLogProbMetric: 48.1391, val_loss: 47.9044, val_MinusLogProbMetric: 47.9044

Epoch 87: val_loss did not improve from 47.32935
196/196 - 34s - loss: 48.1391 - MinusLogProbMetric: 48.1391 - val_loss: 47.9044 - val_MinusLogProbMetric: 47.9044 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 88/1000
2023-10-27 05:23:50.574 
Epoch 88/1000 
	 loss: 53.6052, MinusLogProbMetric: 53.6052, val_loss: 48.2101, val_MinusLogProbMetric: 48.2101

Epoch 88: val_loss did not improve from 47.32935
196/196 - 35s - loss: 53.6052 - MinusLogProbMetric: 53.6052 - val_loss: 48.2101 - val_MinusLogProbMetric: 48.2101 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 89/1000
2023-10-27 05:24:24.640 
Epoch 89/1000 
	 loss: 48.2754, MinusLogProbMetric: 48.2754, val_loss: 53.2918, val_MinusLogProbMetric: 53.2918

Epoch 89: val_loss did not improve from 47.32935
196/196 - 34s - loss: 48.2754 - MinusLogProbMetric: 48.2754 - val_loss: 53.2918 - val_MinusLogProbMetric: 53.2918 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 90/1000
2023-10-27 05:24:58.296 
Epoch 90/1000 
	 loss: 48.1740, MinusLogProbMetric: 48.1740, val_loss: 50.1789, val_MinusLogProbMetric: 50.1789

Epoch 90: val_loss did not improve from 47.32935
196/196 - 34s - loss: 48.1740 - MinusLogProbMetric: 48.1740 - val_loss: 50.1789 - val_MinusLogProbMetric: 50.1789 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 91/1000
2023-10-27 05:25:31.621 
Epoch 91/1000 
	 loss: 48.0691, MinusLogProbMetric: 48.0691, val_loss: 51.9073, val_MinusLogProbMetric: 51.9073

Epoch 91: val_loss did not improve from 47.32935
196/196 - 33s - loss: 48.0691 - MinusLogProbMetric: 48.0691 - val_loss: 51.9073 - val_MinusLogProbMetric: 51.9073 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 92/1000
2023-10-27 05:25:59.822 
Epoch 92/1000 
	 loss: 48.1032, MinusLogProbMetric: 48.1032, val_loss: 48.0051, val_MinusLogProbMetric: 48.0051

Epoch 92: val_loss did not improve from 47.32935
196/196 - 28s - loss: 48.1032 - MinusLogProbMetric: 48.1032 - val_loss: 48.0051 - val_MinusLogProbMetric: 48.0051 - lr: 1.1111e-04 - 28s/epoch - 144ms/step
Epoch 93/1000
2023-10-27 05:26:27.119 
Epoch 93/1000 
	 loss: 47.8929, MinusLogProbMetric: 47.8929, val_loss: 50.3604, val_MinusLogProbMetric: 50.3604

Epoch 93: val_loss did not improve from 47.32935
196/196 - 27s - loss: 47.8929 - MinusLogProbMetric: 47.8929 - val_loss: 50.3604 - val_MinusLogProbMetric: 50.3604 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 94/1000
2023-10-27 05:26:54.327 
Epoch 94/1000 
	 loss: 47.8412, MinusLogProbMetric: 47.8412, val_loss: 47.8165, val_MinusLogProbMetric: 47.8165

Epoch 94: val_loss did not improve from 47.32935
196/196 - 27s - loss: 47.8412 - MinusLogProbMetric: 47.8412 - val_loss: 47.8165 - val_MinusLogProbMetric: 47.8165 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 95/1000
2023-10-27 05:27:21.502 
Epoch 95/1000 
	 loss: 47.9338, MinusLogProbMetric: 47.9338, val_loss: 47.5031, val_MinusLogProbMetric: 47.5031

Epoch 95: val_loss did not improve from 47.32935
196/196 - 27s - loss: 47.9338 - MinusLogProbMetric: 47.9338 - val_loss: 47.5031 - val_MinusLogProbMetric: 47.5031 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 96/1000
2023-10-27 05:27:55.282 
Epoch 96/1000 
	 loss: 47.7294, MinusLogProbMetric: 47.7294, val_loss: 47.6773, val_MinusLogProbMetric: 47.6773

Epoch 96: val_loss did not improve from 47.32935
196/196 - 34s - loss: 47.7294 - MinusLogProbMetric: 47.7294 - val_loss: 47.6773 - val_MinusLogProbMetric: 47.6773 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 97/1000
2023-10-27 05:28:29.695 
Epoch 97/1000 
	 loss: 47.6420, MinusLogProbMetric: 47.6420, val_loss: 49.1969, val_MinusLogProbMetric: 49.1969

Epoch 97: val_loss did not improve from 47.32935
196/196 - 34s - loss: 47.6420 - MinusLogProbMetric: 47.6420 - val_loss: 49.1969 - val_MinusLogProbMetric: 49.1969 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 98/1000
2023-10-27 05:29:03.403 
Epoch 98/1000 
	 loss: 47.7611, MinusLogProbMetric: 47.7611, val_loss: 48.5631, val_MinusLogProbMetric: 48.5631

Epoch 98: val_loss did not improve from 47.32935
196/196 - 34s - loss: 47.7611 - MinusLogProbMetric: 47.7611 - val_loss: 48.5631 - val_MinusLogProbMetric: 48.5631 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 99/1000
2023-10-27 05:29:35.955 
Epoch 99/1000 
	 loss: 47.4111, MinusLogProbMetric: 47.4111, val_loss: 47.1929, val_MinusLogProbMetric: 47.1929

Epoch 99: val_loss improved from 47.32935 to 47.19288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 47.4111 - MinusLogProbMetric: 47.4111 - val_loss: 47.1929 - val_MinusLogProbMetric: 47.1929 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 100/1000
2023-10-27 05:30:10.538 
Epoch 100/1000 
	 loss: 48.0162, MinusLogProbMetric: 48.0162, val_loss: 48.5344, val_MinusLogProbMetric: 48.5344

Epoch 100: val_loss did not improve from 47.19288
196/196 - 34s - loss: 48.0162 - MinusLogProbMetric: 48.0162 - val_loss: 48.5344 - val_MinusLogProbMetric: 48.5344 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 101/1000
2023-10-27 05:30:44.368 
Epoch 101/1000 
	 loss: 47.2038, MinusLogProbMetric: 47.2038, val_loss: 48.2083, val_MinusLogProbMetric: 48.2083

Epoch 101: val_loss did not improve from 47.19288
196/196 - 34s - loss: 47.2038 - MinusLogProbMetric: 47.2038 - val_loss: 48.2083 - val_MinusLogProbMetric: 48.2083 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 102/1000
2023-10-27 05:31:17.837 
Epoch 102/1000 
	 loss: 47.6914, MinusLogProbMetric: 47.6914, val_loss: 47.5534, val_MinusLogProbMetric: 47.5534

Epoch 102: val_loss did not improve from 47.19288
196/196 - 33s - loss: 47.6914 - MinusLogProbMetric: 47.6914 - val_loss: 47.5534 - val_MinusLogProbMetric: 47.5534 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 103/1000
2023-10-27 05:31:51.870 
Epoch 103/1000 
	 loss: 47.6606, MinusLogProbMetric: 47.6606, val_loss: 49.0666, val_MinusLogProbMetric: 49.0666

Epoch 103: val_loss did not improve from 47.19288
196/196 - 34s - loss: 47.6606 - MinusLogProbMetric: 47.6606 - val_loss: 49.0666 - val_MinusLogProbMetric: 49.0666 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 104/1000
2023-10-27 05:32:25.549 
Epoch 104/1000 
	 loss: 47.5068, MinusLogProbMetric: 47.5068, val_loss: 49.7684, val_MinusLogProbMetric: 49.7684

Epoch 104: val_loss did not improve from 47.19288
196/196 - 34s - loss: 47.5068 - MinusLogProbMetric: 47.5068 - val_loss: 49.7684 - val_MinusLogProbMetric: 49.7684 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 105/1000
2023-10-27 05:32:58.926 
Epoch 105/1000 
	 loss: 47.2574, MinusLogProbMetric: 47.2574, val_loss: 49.2542, val_MinusLogProbMetric: 49.2542

Epoch 105: val_loss did not improve from 47.19288
196/196 - 33s - loss: 47.2574 - MinusLogProbMetric: 47.2574 - val_loss: 49.2542 - val_MinusLogProbMetric: 49.2542 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 106/1000
2023-10-27 05:33:32.504 
Epoch 106/1000 
	 loss: 47.7693, MinusLogProbMetric: 47.7693, val_loss: 48.2643, val_MinusLogProbMetric: 48.2643

Epoch 106: val_loss did not improve from 47.19288
196/196 - 34s - loss: 47.7693 - MinusLogProbMetric: 47.7693 - val_loss: 48.2643 - val_MinusLogProbMetric: 48.2643 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 107/1000
2023-10-27 05:34:06.229 
Epoch 107/1000 
	 loss: 47.3096, MinusLogProbMetric: 47.3096, val_loss: 47.9926, val_MinusLogProbMetric: 47.9926

Epoch 107: val_loss did not improve from 47.19288
196/196 - 34s - loss: 47.3096 - MinusLogProbMetric: 47.3096 - val_loss: 47.9926 - val_MinusLogProbMetric: 47.9926 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 108/1000
2023-10-27 05:34:40.021 
Epoch 108/1000 
	 loss: 47.2738, MinusLogProbMetric: 47.2738, val_loss: 47.4891, val_MinusLogProbMetric: 47.4891

Epoch 108: val_loss did not improve from 47.19288
196/196 - 34s - loss: 47.2738 - MinusLogProbMetric: 47.2738 - val_loss: 47.4891 - val_MinusLogProbMetric: 47.4891 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 109/1000
2023-10-27 05:35:13.492 
Epoch 109/1000 
	 loss: 47.3783, MinusLogProbMetric: 47.3783, val_loss: 48.7092, val_MinusLogProbMetric: 48.7092

Epoch 109: val_loss did not improve from 47.19288
196/196 - 33s - loss: 47.3783 - MinusLogProbMetric: 47.3783 - val_loss: 48.7092 - val_MinusLogProbMetric: 48.7092 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 110/1000
2023-10-27 05:35:47.101 
Epoch 110/1000 
	 loss: 46.9357, MinusLogProbMetric: 46.9357, val_loss: 46.5251, val_MinusLogProbMetric: 46.5251

Epoch 110: val_loss improved from 47.19288 to 46.52514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.9357 - MinusLogProbMetric: 46.9357 - val_loss: 46.5251 - val_MinusLogProbMetric: 46.5251 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 111/1000
2023-10-27 05:36:21.418 
Epoch 111/1000 
	 loss: 47.4966, MinusLogProbMetric: 47.4966, val_loss: 48.1288, val_MinusLogProbMetric: 48.1288

Epoch 111: val_loss did not improve from 46.52514
196/196 - 34s - loss: 47.4966 - MinusLogProbMetric: 47.4966 - val_loss: 48.1288 - val_MinusLogProbMetric: 48.1288 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 112/1000
2023-10-27 05:36:54.998 
Epoch 112/1000 
	 loss: 46.8020, MinusLogProbMetric: 46.8020, val_loss: 47.3930, val_MinusLogProbMetric: 47.3930

Epoch 112: val_loss did not improve from 46.52514
196/196 - 34s - loss: 46.8020 - MinusLogProbMetric: 46.8020 - val_loss: 47.3930 - val_MinusLogProbMetric: 47.3930 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 113/1000
2023-10-27 05:37:28.379 
Epoch 113/1000 
	 loss: 46.9215, MinusLogProbMetric: 46.9215, val_loss: 47.9079, val_MinusLogProbMetric: 47.9079

Epoch 113: val_loss did not improve from 46.52514
196/196 - 33s - loss: 46.9215 - MinusLogProbMetric: 46.9215 - val_loss: 47.9079 - val_MinusLogProbMetric: 47.9079 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 114/1000
2023-10-27 05:38:01.699 
Epoch 114/1000 
	 loss: 47.2791, MinusLogProbMetric: 47.2791, val_loss: 48.4641, val_MinusLogProbMetric: 48.4641

Epoch 114: val_loss did not improve from 46.52514
196/196 - 33s - loss: 47.2791 - MinusLogProbMetric: 47.2791 - val_loss: 48.4641 - val_MinusLogProbMetric: 48.4641 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 115/1000
2023-10-27 05:38:35.401 
Epoch 115/1000 
	 loss: 49.5094, MinusLogProbMetric: 49.5094, val_loss: 46.9387, val_MinusLogProbMetric: 46.9387

Epoch 115: val_loss did not improve from 46.52514
196/196 - 34s - loss: 49.5094 - MinusLogProbMetric: 49.5094 - val_loss: 46.9387 - val_MinusLogProbMetric: 46.9387 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 116/1000
2023-10-27 05:39:08.626 
Epoch 116/1000 
	 loss: 47.1344, MinusLogProbMetric: 47.1344, val_loss: 47.7579, val_MinusLogProbMetric: 47.7579

Epoch 116: val_loss did not improve from 46.52514
196/196 - 33s - loss: 47.1344 - MinusLogProbMetric: 47.1344 - val_loss: 47.7579 - val_MinusLogProbMetric: 47.7579 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 117/1000
2023-10-27 05:39:41.905 
Epoch 117/1000 
	 loss: 47.1790, MinusLogProbMetric: 47.1790, val_loss: 47.6581, val_MinusLogProbMetric: 47.6581

Epoch 117: val_loss did not improve from 46.52514
196/196 - 33s - loss: 47.1790 - MinusLogProbMetric: 47.1790 - val_loss: 47.6581 - val_MinusLogProbMetric: 47.6581 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 118/1000
2023-10-27 05:40:15.489 
Epoch 118/1000 
	 loss: 47.0651, MinusLogProbMetric: 47.0651, val_loss: 46.8399, val_MinusLogProbMetric: 46.8399

Epoch 118: val_loss did not improve from 46.52514
196/196 - 34s - loss: 47.0651 - MinusLogProbMetric: 47.0651 - val_loss: 46.8399 - val_MinusLogProbMetric: 46.8399 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 119/1000
2023-10-27 05:40:48.581 
Epoch 119/1000 
	 loss: 46.8664, MinusLogProbMetric: 46.8664, val_loss: 46.2619, val_MinusLogProbMetric: 46.2619

Epoch 119: val_loss improved from 46.52514 to 46.26189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.8664 - MinusLogProbMetric: 46.8664 - val_loss: 46.2619 - val_MinusLogProbMetric: 46.2619 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 120/1000
2023-10-27 05:41:22.441 
Epoch 120/1000 
	 loss: 46.9147, MinusLogProbMetric: 46.9147, val_loss: 49.5921, val_MinusLogProbMetric: 49.5921

Epoch 120: val_loss did not improve from 46.26189
196/196 - 33s - loss: 46.9147 - MinusLogProbMetric: 46.9147 - val_loss: 49.5921 - val_MinusLogProbMetric: 49.5921 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 121/1000
2023-10-27 05:41:55.615 
Epoch 121/1000 
	 loss: 46.8642, MinusLogProbMetric: 46.8642, val_loss: 52.5772, val_MinusLogProbMetric: 52.5772

Epoch 121: val_loss did not improve from 46.26189
196/196 - 33s - loss: 46.8642 - MinusLogProbMetric: 46.8642 - val_loss: 52.5772 - val_MinusLogProbMetric: 52.5772 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 122/1000
2023-10-27 05:42:28.878 
Epoch 122/1000 
	 loss: 47.0941, MinusLogProbMetric: 47.0941, val_loss: 46.9803, val_MinusLogProbMetric: 46.9803

Epoch 122: val_loss did not improve from 46.26189
196/196 - 33s - loss: 47.0941 - MinusLogProbMetric: 47.0941 - val_loss: 46.9803 - val_MinusLogProbMetric: 46.9803 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 123/1000
2023-10-27 05:43:01.898 
Epoch 123/1000 
	 loss: 46.7284, MinusLogProbMetric: 46.7284, val_loss: 46.8173, val_MinusLogProbMetric: 46.8173

Epoch 123: val_loss did not improve from 46.26189
196/196 - 33s - loss: 46.7284 - MinusLogProbMetric: 46.7284 - val_loss: 46.8173 - val_MinusLogProbMetric: 46.8173 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 124/1000
2023-10-27 05:43:35.445 
Epoch 124/1000 
	 loss: 46.5423, MinusLogProbMetric: 46.5423, val_loss: 46.2726, val_MinusLogProbMetric: 46.2726

Epoch 124: val_loss did not improve from 46.26189
196/196 - 34s - loss: 46.5423 - MinusLogProbMetric: 46.5423 - val_loss: 46.2726 - val_MinusLogProbMetric: 46.2726 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 125/1000
2023-10-27 05:44:08.827 
Epoch 125/1000 
	 loss: 46.3777, MinusLogProbMetric: 46.3777, val_loss: 46.1875, val_MinusLogProbMetric: 46.1875

Epoch 125: val_loss improved from 46.26189 to 46.18746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.3777 - MinusLogProbMetric: 46.3777 - val_loss: 46.1875 - val_MinusLogProbMetric: 46.1875 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 126/1000
2023-10-27 05:44:42.671 
Epoch 126/1000 
	 loss: 46.8349, MinusLogProbMetric: 46.8349, val_loss: 47.5965, val_MinusLogProbMetric: 47.5965

Epoch 126: val_loss did not improve from 46.18746
196/196 - 33s - loss: 46.8349 - MinusLogProbMetric: 46.8349 - val_loss: 47.5965 - val_MinusLogProbMetric: 47.5965 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 127/1000
2023-10-27 05:45:16.179 
Epoch 127/1000 
	 loss: 46.5316, MinusLogProbMetric: 46.5316, val_loss: 47.0217, val_MinusLogProbMetric: 47.0217

Epoch 127: val_loss did not improve from 46.18746
196/196 - 34s - loss: 46.5316 - MinusLogProbMetric: 46.5316 - val_loss: 47.0217 - val_MinusLogProbMetric: 47.0217 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 128/1000
2023-10-27 05:45:49.831 
Epoch 128/1000 
	 loss: 46.3190, MinusLogProbMetric: 46.3190, val_loss: 46.0658, val_MinusLogProbMetric: 46.0658

Epoch 128: val_loss improved from 46.18746 to 46.06580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.3190 - MinusLogProbMetric: 46.3190 - val_loss: 46.0658 - val_MinusLogProbMetric: 46.0658 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 129/1000
2023-10-27 05:46:24.134 
Epoch 129/1000 
	 loss: 46.6471, MinusLogProbMetric: 46.6471, val_loss: 46.4530, val_MinusLogProbMetric: 46.4530

Epoch 129: val_loss did not improve from 46.06580
196/196 - 34s - loss: 46.6471 - MinusLogProbMetric: 46.6471 - val_loss: 46.4530 - val_MinusLogProbMetric: 46.4530 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 130/1000
2023-10-27 05:46:58.308 
Epoch 130/1000 
	 loss: 46.4157, MinusLogProbMetric: 46.4157, val_loss: 46.6215, val_MinusLogProbMetric: 46.6215

Epoch 130: val_loss did not improve from 46.06580
196/196 - 34s - loss: 46.4157 - MinusLogProbMetric: 46.4157 - val_loss: 46.6215 - val_MinusLogProbMetric: 46.6215 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 131/1000
2023-10-27 05:47:31.808 
Epoch 131/1000 
	 loss: 46.5944, MinusLogProbMetric: 46.5944, val_loss: 49.2634, val_MinusLogProbMetric: 49.2634

Epoch 131: val_loss did not improve from 46.06580
196/196 - 33s - loss: 46.5944 - MinusLogProbMetric: 46.5944 - val_loss: 49.2634 - val_MinusLogProbMetric: 49.2634 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 132/1000
2023-10-27 05:48:05.418 
Epoch 132/1000 
	 loss: 46.9002, MinusLogProbMetric: 46.9002, val_loss: 47.4167, val_MinusLogProbMetric: 47.4167

Epoch 132: val_loss did not improve from 46.06580
196/196 - 34s - loss: 46.9002 - MinusLogProbMetric: 46.9002 - val_loss: 47.4167 - val_MinusLogProbMetric: 47.4167 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 133/1000
2023-10-27 05:48:38.736 
Epoch 133/1000 
	 loss: 46.2551, MinusLogProbMetric: 46.2551, val_loss: 46.0450, val_MinusLogProbMetric: 46.0450

Epoch 133: val_loss improved from 46.06580 to 46.04500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.2551 - MinusLogProbMetric: 46.2551 - val_loss: 46.0450 - val_MinusLogProbMetric: 46.0450 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 134/1000
2023-10-27 05:49:12.749 
Epoch 134/1000 
	 loss: 46.4370, MinusLogProbMetric: 46.4370, val_loss: 50.1130, val_MinusLogProbMetric: 50.1130

Epoch 134: val_loss did not improve from 46.04500
196/196 - 33s - loss: 46.4370 - MinusLogProbMetric: 46.4370 - val_loss: 50.1130 - val_MinusLogProbMetric: 50.1130 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 135/1000
2023-10-27 05:49:46.170 
Epoch 135/1000 
	 loss: 46.3949, MinusLogProbMetric: 46.3949, val_loss: 49.4923, val_MinusLogProbMetric: 49.4923

Epoch 135: val_loss did not improve from 46.04500
196/196 - 33s - loss: 46.3949 - MinusLogProbMetric: 46.3949 - val_loss: 49.4923 - val_MinusLogProbMetric: 49.4923 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 136/1000
2023-10-27 05:50:19.580 
Epoch 136/1000 
	 loss: 46.5028, MinusLogProbMetric: 46.5028, val_loss: 47.9183, val_MinusLogProbMetric: 47.9183

Epoch 136: val_loss did not improve from 46.04500
196/196 - 33s - loss: 46.5028 - MinusLogProbMetric: 46.5028 - val_loss: 47.9183 - val_MinusLogProbMetric: 47.9183 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 137/1000
2023-10-27 05:50:53.080 
Epoch 137/1000 
	 loss: 46.4749, MinusLogProbMetric: 46.4749, val_loss: 46.3554, val_MinusLogProbMetric: 46.3554

Epoch 137: val_loss did not improve from 46.04500
196/196 - 33s - loss: 46.4749 - MinusLogProbMetric: 46.4749 - val_loss: 46.3554 - val_MinusLogProbMetric: 46.3554 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 138/1000
2023-10-27 05:51:26.920 
Epoch 138/1000 
	 loss: 46.4025, MinusLogProbMetric: 46.4025, val_loss: 48.5371, val_MinusLogProbMetric: 48.5371

Epoch 138: val_loss did not improve from 46.04500
196/196 - 34s - loss: 46.4025 - MinusLogProbMetric: 46.4025 - val_loss: 48.5371 - val_MinusLogProbMetric: 48.5371 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 139/1000
2023-10-27 05:52:00.221 
Epoch 139/1000 
	 loss: 46.1313, MinusLogProbMetric: 46.1313, val_loss: 47.1941, val_MinusLogProbMetric: 47.1941

Epoch 139: val_loss did not improve from 46.04500
196/196 - 33s - loss: 46.1313 - MinusLogProbMetric: 46.1313 - val_loss: 47.1941 - val_MinusLogProbMetric: 47.1941 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 140/1000
2023-10-27 05:52:33.844 
Epoch 140/1000 
	 loss: 45.9856, MinusLogProbMetric: 45.9856, val_loss: 46.4524, val_MinusLogProbMetric: 46.4524

Epoch 140: val_loss did not improve from 46.04500
196/196 - 34s - loss: 45.9856 - MinusLogProbMetric: 45.9856 - val_loss: 46.4524 - val_MinusLogProbMetric: 46.4524 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 141/1000
2023-10-27 05:53:07.661 
Epoch 141/1000 
	 loss: 46.4669, MinusLogProbMetric: 46.4669, val_loss: 45.6020, val_MinusLogProbMetric: 45.6020

Epoch 141: val_loss improved from 46.04500 to 45.60201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.4669 - MinusLogProbMetric: 46.4669 - val_loss: 45.6020 - val_MinusLogProbMetric: 45.6020 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 142/1000
2023-10-27 05:53:41.983 
Epoch 142/1000 
	 loss: 46.2878, MinusLogProbMetric: 46.2878, val_loss: 46.3040, val_MinusLogProbMetric: 46.3040

Epoch 142: val_loss did not improve from 45.60201
196/196 - 34s - loss: 46.2878 - MinusLogProbMetric: 46.2878 - val_loss: 46.3040 - val_MinusLogProbMetric: 46.3040 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 143/1000
2023-10-27 05:54:15.453 
Epoch 143/1000 
	 loss: 46.1570, MinusLogProbMetric: 46.1570, val_loss: 46.7484, val_MinusLogProbMetric: 46.7484

Epoch 143: val_loss did not improve from 45.60201
196/196 - 33s - loss: 46.1570 - MinusLogProbMetric: 46.1570 - val_loss: 46.7484 - val_MinusLogProbMetric: 46.7484 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 144/1000
2023-10-27 05:54:49.142 
Epoch 144/1000 
	 loss: 46.5631, MinusLogProbMetric: 46.5631, val_loss: 46.9393, val_MinusLogProbMetric: 46.9393

Epoch 144: val_loss did not improve from 45.60201
196/196 - 34s - loss: 46.5631 - MinusLogProbMetric: 46.5631 - val_loss: 46.9393 - val_MinusLogProbMetric: 46.9393 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 145/1000
2023-10-27 05:55:22.520 
Epoch 145/1000 
	 loss: 45.7808, MinusLogProbMetric: 45.7808, val_loss: 48.5391, val_MinusLogProbMetric: 48.5391

Epoch 145: val_loss did not improve from 45.60201
196/196 - 33s - loss: 45.7808 - MinusLogProbMetric: 45.7808 - val_loss: 48.5391 - val_MinusLogProbMetric: 48.5391 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 146/1000
2023-10-27 05:55:56.189 
Epoch 146/1000 
	 loss: 46.0113, MinusLogProbMetric: 46.0113, val_loss: 45.3776, val_MinusLogProbMetric: 45.3776

Epoch 146: val_loss improved from 45.60201 to 45.37757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.0113 - MinusLogProbMetric: 46.0113 - val_loss: 45.3776 - val_MinusLogProbMetric: 45.3776 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 147/1000
2023-10-27 05:56:30.513 
Epoch 147/1000 
	 loss: 52.4411, MinusLogProbMetric: 52.4411, val_loss: 49.0542, val_MinusLogProbMetric: 49.0542

Epoch 147: val_loss did not improve from 45.37757
196/196 - 34s - loss: 52.4411 - MinusLogProbMetric: 52.4411 - val_loss: 49.0542 - val_MinusLogProbMetric: 49.0542 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 148/1000
2023-10-27 05:57:04.208 
Epoch 148/1000 
	 loss: 46.9407, MinusLogProbMetric: 46.9407, val_loss: 46.3647, val_MinusLogProbMetric: 46.3647

Epoch 148: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.9407 - MinusLogProbMetric: 46.9407 - val_loss: 46.3647 - val_MinusLogProbMetric: 46.3647 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 149/1000
2023-10-27 05:57:37.970 
Epoch 149/1000 
	 loss: 46.5827, MinusLogProbMetric: 46.5827, val_loss: 46.5091, val_MinusLogProbMetric: 46.5091

Epoch 149: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.5827 - MinusLogProbMetric: 46.5827 - val_loss: 46.5091 - val_MinusLogProbMetric: 46.5091 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 150/1000
2023-10-27 05:58:11.396 
Epoch 150/1000 
	 loss: 46.3719, MinusLogProbMetric: 46.3719, val_loss: 49.1649, val_MinusLogProbMetric: 49.1649

Epoch 150: val_loss did not improve from 45.37757
196/196 - 33s - loss: 46.3719 - MinusLogProbMetric: 46.3719 - val_loss: 49.1649 - val_MinusLogProbMetric: 49.1649 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 151/1000
2023-10-27 05:58:44.992 
Epoch 151/1000 
	 loss: 46.2848, MinusLogProbMetric: 46.2848, val_loss: 46.8653, val_MinusLogProbMetric: 46.8653

Epoch 151: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.2848 - MinusLogProbMetric: 46.2848 - val_loss: 46.8653 - val_MinusLogProbMetric: 46.8653 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 152/1000
2023-10-27 05:59:17.969 
Epoch 152/1000 
	 loss: 46.3144, MinusLogProbMetric: 46.3144, val_loss: 47.8442, val_MinusLogProbMetric: 47.8442

Epoch 152: val_loss did not improve from 45.37757
196/196 - 33s - loss: 46.3144 - MinusLogProbMetric: 46.3144 - val_loss: 47.8442 - val_MinusLogProbMetric: 47.8442 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 153/1000
2023-10-27 05:59:51.619 
Epoch 153/1000 
	 loss: 46.0302, MinusLogProbMetric: 46.0302, val_loss: 46.3892, val_MinusLogProbMetric: 46.3892

Epoch 153: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.0302 - MinusLogProbMetric: 46.0302 - val_loss: 46.3892 - val_MinusLogProbMetric: 46.3892 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 154/1000
2023-10-27 06:00:25.503 
Epoch 154/1000 
	 loss: 46.1748, MinusLogProbMetric: 46.1748, val_loss: 46.4931, val_MinusLogProbMetric: 46.4931

Epoch 154: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.1748 - MinusLogProbMetric: 46.1748 - val_loss: 46.4931 - val_MinusLogProbMetric: 46.4931 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 155/1000
2023-10-27 06:00:59.189 
Epoch 155/1000 
	 loss: 45.9119, MinusLogProbMetric: 45.9119, val_loss: 46.3602, val_MinusLogProbMetric: 46.3602

Epoch 155: val_loss did not improve from 45.37757
196/196 - 34s - loss: 45.9119 - MinusLogProbMetric: 45.9119 - val_loss: 46.3602 - val_MinusLogProbMetric: 46.3602 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 156/1000
2023-10-27 06:01:32.664 
Epoch 156/1000 
	 loss: 46.1695, MinusLogProbMetric: 46.1695, val_loss: 46.1549, val_MinusLogProbMetric: 46.1549

Epoch 156: val_loss did not improve from 45.37757
196/196 - 33s - loss: 46.1695 - MinusLogProbMetric: 46.1695 - val_loss: 46.1549 - val_MinusLogProbMetric: 46.1549 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 157/1000
2023-10-27 06:02:06.510 
Epoch 157/1000 
	 loss: 46.3448, MinusLogProbMetric: 46.3448, val_loss: 46.0860, val_MinusLogProbMetric: 46.0860

Epoch 157: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.3448 - MinusLogProbMetric: 46.3448 - val_loss: 46.0860 - val_MinusLogProbMetric: 46.0860 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 158/1000
2023-10-27 06:02:40.003 
Epoch 158/1000 
	 loss: 46.0795, MinusLogProbMetric: 46.0795, val_loss: 47.4757, val_MinusLogProbMetric: 47.4757

Epoch 158: val_loss did not improve from 45.37757
196/196 - 33s - loss: 46.0795 - MinusLogProbMetric: 46.0795 - val_loss: 47.4757 - val_MinusLogProbMetric: 47.4757 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 159/1000
2023-10-27 06:03:13.975 
Epoch 159/1000 
	 loss: 45.9275, MinusLogProbMetric: 45.9275, val_loss: 46.7608, val_MinusLogProbMetric: 46.7608

Epoch 159: val_loss did not improve from 45.37757
196/196 - 34s - loss: 45.9275 - MinusLogProbMetric: 45.9275 - val_loss: 46.7608 - val_MinusLogProbMetric: 46.7608 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 160/1000
2023-10-27 06:03:47.861 
Epoch 160/1000 
	 loss: 46.6022, MinusLogProbMetric: 46.6022, val_loss: 46.1708, val_MinusLogProbMetric: 46.1708

Epoch 160: val_loss did not improve from 45.37757
196/196 - 34s - loss: 46.6022 - MinusLogProbMetric: 46.6022 - val_loss: 46.1708 - val_MinusLogProbMetric: 46.1708 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 161/1000
2023-10-27 06:04:22.162 
Epoch 161/1000 
	 loss: 45.8522, MinusLogProbMetric: 45.8522, val_loss: 46.2052, val_MinusLogProbMetric: 46.2052

Epoch 161: val_loss did not improve from 45.37757
196/196 - 34s - loss: 45.8522 - MinusLogProbMetric: 45.8522 - val_loss: 46.2052 - val_MinusLogProbMetric: 46.2052 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 162/1000
2023-10-27 06:04:56.970 
Epoch 162/1000 
	 loss: 45.7391, MinusLogProbMetric: 45.7391, val_loss: 45.3648, val_MinusLogProbMetric: 45.3648

Epoch 162: val_loss improved from 45.37757 to 45.36478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 45.7391 - MinusLogProbMetric: 45.7391 - val_loss: 45.3648 - val_MinusLogProbMetric: 45.3648 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 163/1000
2023-10-27 06:05:32.380 
Epoch 163/1000 
	 loss: 46.1433, MinusLogProbMetric: 46.1433, val_loss: 46.4525, val_MinusLogProbMetric: 46.4525

Epoch 163: val_loss did not improve from 45.36478
196/196 - 35s - loss: 46.1433 - MinusLogProbMetric: 46.1433 - val_loss: 46.4525 - val_MinusLogProbMetric: 46.4525 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 164/1000
2023-10-27 06:06:06.126 
Epoch 164/1000 
	 loss: 46.1195, MinusLogProbMetric: 46.1195, val_loss: 45.8372, val_MinusLogProbMetric: 45.8372

Epoch 164: val_loss did not improve from 45.36478
196/196 - 34s - loss: 46.1195 - MinusLogProbMetric: 46.1195 - val_loss: 45.8372 - val_MinusLogProbMetric: 45.8372 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 165/1000
2023-10-27 06:06:39.661 
Epoch 165/1000 
	 loss: 45.7654, MinusLogProbMetric: 45.7654, val_loss: 46.0753, val_MinusLogProbMetric: 46.0753

Epoch 165: val_loss did not improve from 45.36478
196/196 - 34s - loss: 45.7654 - MinusLogProbMetric: 45.7654 - val_loss: 46.0753 - val_MinusLogProbMetric: 46.0753 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 166/1000
2023-10-27 06:07:13.889 
Epoch 166/1000 
	 loss: 45.5795, MinusLogProbMetric: 45.5795, val_loss: 49.1355, val_MinusLogProbMetric: 49.1355

Epoch 166: val_loss did not improve from 45.36478
196/196 - 34s - loss: 45.5795 - MinusLogProbMetric: 45.5795 - val_loss: 49.1355 - val_MinusLogProbMetric: 49.1355 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 167/1000
2023-10-27 06:07:48.782 
Epoch 167/1000 
	 loss: 46.5291, MinusLogProbMetric: 46.5291, val_loss: 45.7598, val_MinusLogProbMetric: 45.7598

Epoch 167: val_loss did not improve from 45.36478
196/196 - 35s - loss: 46.5291 - MinusLogProbMetric: 46.5291 - val_loss: 45.7598 - val_MinusLogProbMetric: 45.7598 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 168/1000
2023-10-27 06:08:23.677 
Epoch 168/1000 
	 loss: 46.1280, MinusLogProbMetric: 46.1280, val_loss: 45.3366, val_MinusLogProbMetric: 45.3366

Epoch 168: val_loss improved from 45.36478 to 45.33655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 36s - loss: 46.1280 - MinusLogProbMetric: 46.1280 - val_loss: 45.3366 - val_MinusLogProbMetric: 45.3366 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 169/1000
2023-10-27 06:08:57.612 
Epoch 169/1000 
	 loss: 45.7593, MinusLogProbMetric: 45.7593, val_loss: 46.3570, val_MinusLogProbMetric: 46.3570

Epoch 169: val_loss did not improve from 45.33655
196/196 - 33s - loss: 45.7593 - MinusLogProbMetric: 45.7593 - val_loss: 46.3570 - val_MinusLogProbMetric: 46.3570 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 170/1000
2023-10-27 06:09:31.634 
Epoch 170/1000 
	 loss: 45.6107, MinusLogProbMetric: 45.6107, val_loss: 46.4627, val_MinusLogProbMetric: 46.4627

Epoch 170: val_loss did not improve from 45.33655
196/196 - 34s - loss: 45.6107 - MinusLogProbMetric: 45.6107 - val_loss: 46.4627 - val_MinusLogProbMetric: 46.4627 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 171/1000
2023-10-27 06:10:07.015 
Epoch 171/1000 
	 loss: 45.8359, MinusLogProbMetric: 45.8359, val_loss: 45.3651, val_MinusLogProbMetric: 45.3651

Epoch 171: val_loss did not improve from 45.33655
196/196 - 35s - loss: 45.8359 - MinusLogProbMetric: 45.8359 - val_loss: 45.3651 - val_MinusLogProbMetric: 45.3651 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 172/1000
2023-10-27 06:10:40.413 
Epoch 172/1000 
	 loss: 45.4413, MinusLogProbMetric: 45.4413, val_loss: 45.6314, val_MinusLogProbMetric: 45.6314

Epoch 172: val_loss did not improve from 45.33655
196/196 - 33s - loss: 45.4413 - MinusLogProbMetric: 45.4413 - val_loss: 45.6314 - val_MinusLogProbMetric: 45.6314 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 173/1000
2023-10-27 06:11:14.601 
Epoch 173/1000 
	 loss: 45.5199, MinusLogProbMetric: 45.5199, val_loss: 45.2454, val_MinusLogProbMetric: 45.2454

Epoch 173: val_loss improved from 45.33655 to 45.24544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 45.5199 - MinusLogProbMetric: 45.5199 - val_loss: 45.2454 - val_MinusLogProbMetric: 45.2454 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 174/1000
2023-10-27 06:11:49.758 
Epoch 174/1000 
	 loss: 45.9913, MinusLogProbMetric: 45.9913, val_loss: 47.7043, val_MinusLogProbMetric: 47.7043

Epoch 174: val_loss did not improve from 45.24544
196/196 - 35s - loss: 45.9913 - MinusLogProbMetric: 45.9913 - val_loss: 47.7043 - val_MinusLogProbMetric: 47.7043 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 175/1000
2023-10-27 06:12:24.005 
Epoch 175/1000 
	 loss: 45.4241, MinusLogProbMetric: 45.4241, val_loss: 47.7485, val_MinusLogProbMetric: 47.7485

Epoch 175: val_loss did not improve from 45.24544
196/196 - 34s - loss: 45.4241 - MinusLogProbMetric: 45.4241 - val_loss: 47.7485 - val_MinusLogProbMetric: 47.7485 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 176/1000
2023-10-27 06:12:59.575 
Epoch 176/1000 
	 loss: 45.9504, MinusLogProbMetric: 45.9504, val_loss: 45.9150, val_MinusLogProbMetric: 45.9150

Epoch 176: val_loss did not improve from 45.24544
196/196 - 36s - loss: 45.9504 - MinusLogProbMetric: 45.9504 - val_loss: 45.9150 - val_MinusLogProbMetric: 45.9150 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 177/1000
2023-10-27 06:13:33.659 
Epoch 177/1000 
	 loss: 45.3367, MinusLogProbMetric: 45.3367, val_loss: 45.6336, val_MinusLogProbMetric: 45.6336

Epoch 177: val_loss did not improve from 45.24544
196/196 - 34s - loss: 45.3367 - MinusLogProbMetric: 45.3367 - val_loss: 45.6336 - val_MinusLogProbMetric: 45.6336 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 178/1000
2023-10-27 06:14:08.495 
Epoch 178/1000 
	 loss: 45.4865, MinusLogProbMetric: 45.4865, val_loss: 46.4947, val_MinusLogProbMetric: 46.4947

Epoch 178: val_loss did not improve from 45.24544
196/196 - 35s - loss: 45.4865 - MinusLogProbMetric: 45.4865 - val_loss: 46.4947 - val_MinusLogProbMetric: 46.4947 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 179/1000
2023-10-27 06:14:43.597 
Epoch 179/1000 
	 loss: 45.5006, MinusLogProbMetric: 45.5006, val_loss: 45.8091, val_MinusLogProbMetric: 45.8091

Epoch 179: val_loss did not improve from 45.24544
196/196 - 35s - loss: 45.5006 - MinusLogProbMetric: 45.5006 - val_loss: 45.8091 - val_MinusLogProbMetric: 45.8091 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 180/1000
2023-10-27 06:15:18.115 
Epoch 180/1000 
	 loss: 45.2297, MinusLogProbMetric: 45.2297, val_loss: 46.1123, val_MinusLogProbMetric: 46.1123

Epoch 180: val_loss did not improve from 45.24544
196/196 - 35s - loss: 45.2297 - MinusLogProbMetric: 45.2297 - val_loss: 46.1123 - val_MinusLogProbMetric: 46.1123 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 181/1000
2023-10-27 06:15:51.899 
Epoch 181/1000 
	 loss: 45.7332, MinusLogProbMetric: 45.7332, val_loss: 47.9439, val_MinusLogProbMetric: 47.9439

Epoch 181: val_loss did not improve from 45.24544
196/196 - 34s - loss: 45.7332 - MinusLogProbMetric: 45.7332 - val_loss: 47.9439 - val_MinusLogProbMetric: 47.9439 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 182/1000
2023-10-27 06:16:25.689 
Epoch 182/1000 
	 loss: 45.6010, MinusLogProbMetric: 45.6010, val_loss: 46.0227, val_MinusLogProbMetric: 46.0227

Epoch 182: val_loss did not improve from 45.24544
196/196 - 34s - loss: 45.6010 - MinusLogProbMetric: 45.6010 - val_loss: 46.0227 - val_MinusLogProbMetric: 46.0227 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 183/1000
2023-10-27 06:17:00.106 
Epoch 183/1000 
	 loss: 45.7981, MinusLogProbMetric: 45.7981, val_loss: 45.5478, val_MinusLogProbMetric: 45.5478

Epoch 183: val_loss did not improve from 45.24544
196/196 - 34s - loss: 45.7981 - MinusLogProbMetric: 45.7981 - val_loss: 45.5478 - val_MinusLogProbMetric: 45.5478 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 184/1000
2023-10-27 06:17:33.931 
Epoch 184/1000 
	 loss: 45.8530, MinusLogProbMetric: 45.8530, val_loss: 45.1773, val_MinusLogProbMetric: 45.1773

Epoch 184: val_loss improved from 45.24544 to 45.17732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 45.8530 - MinusLogProbMetric: 45.8530 - val_loss: 45.1773 - val_MinusLogProbMetric: 45.1773 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 185/1000
2023-10-27 06:18:08.485 
Epoch 185/1000 
	 loss: 45.6406, MinusLogProbMetric: 45.6406, val_loss: 45.9421, val_MinusLogProbMetric: 45.9421

Epoch 185: val_loss did not improve from 45.17732
196/196 - 34s - loss: 45.6406 - MinusLogProbMetric: 45.6406 - val_loss: 45.9421 - val_MinusLogProbMetric: 45.9421 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 186/1000
2023-10-27 06:18:42.931 
Epoch 186/1000 
	 loss: 45.4495, MinusLogProbMetric: 45.4495, val_loss: 45.1475, val_MinusLogProbMetric: 45.1475

Epoch 186: val_loss improved from 45.17732 to 45.14745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 45.4495 - MinusLogProbMetric: 45.4495 - val_loss: 45.1475 - val_MinusLogProbMetric: 45.1475 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 187/1000
2023-10-27 06:19:17.908 
Epoch 187/1000 
	 loss: 45.0555, MinusLogProbMetric: 45.0555, val_loss: 48.9939, val_MinusLogProbMetric: 48.9939

Epoch 187: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.0555 - MinusLogProbMetric: 45.0555 - val_loss: 48.9939 - val_MinusLogProbMetric: 48.9939 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 188/1000
2023-10-27 06:19:52.423 
Epoch 188/1000 
	 loss: 45.6672, MinusLogProbMetric: 45.6672, val_loss: 45.5194, val_MinusLogProbMetric: 45.5194

Epoch 188: val_loss did not improve from 45.14745
196/196 - 35s - loss: 45.6672 - MinusLogProbMetric: 45.6672 - val_loss: 45.5194 - val_MinusLogProbMetric: 45.5194 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 189/1000
2023-10-27 06:20:26.720 
Epoch 189/1000 
	 loss: 45.7951, MinusLogProbMetric: 45.7951, val_loss: 50.0693, val_MinusLogProbMetric: 50.0693

Epoch 189: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.7951 - MinusLogProbMetric: 45.7951 - val_loss: 50.0693 - val_MinusLogProbMetric: 50.0693 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 190/1000
2023-10-27 06:21:01.462 
Epoch 190/1000 
	 loss: 45.3070, MinusLogProbMetric: 45.3070, val_loss: 45.5660, val_MinusLogProbMetric: 45.5660

Epoch 190: val_loss did not improve from 45.14745
196/196 - 35s - loss: 45.3070 - MinusLogProbMetric: 45.3070 - val_loss: 45.5660 - val_MinusLogProbMetric: 45.5660 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 191/1000
2023-10-27 06:21:36.588 
Epoch 191/1000 
	 loss: 45.2449, MinusLogProbMetric: 45.2449, val_loss: 45.6286, val_MinusLogProbMetric: 45.6286

Epoch 191: val_loss did not improve from 45.14745
196/196 - 35s - loss: 45.2449 - MinusLogProbMetric: 45.2449 - val_loss: 45.6286 - val_MinusLogProbMetric: 45.6286 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 192/1000
2023-10-27 06:22:10.479 
Epoch 192/1000 
	 loss: 45.3586, MinusLogProbMetric: 45.3586, val_loss: 45.8574, val_MinusLogProbMetric: 45.8574

Epoch 192: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.3586 - MinusLogProbMetric: 45.3586 - val_loss: 45.8574 - val_MinusLogProbMetric: 45.8574 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 193/1000
2023-10-27 06:22:44.772 
Epoch 193/1000 
	 loss: 45.3949, MinusLogProbMetric: 45.3949, val_loss: 46.8972, val_MinusLogProbMetric: 46.8972

Epoch 193: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.3949 - MinusLogProbMetric: 45.3949 - val_loss: 46.8972 - val_MinusLogProbMetric: 46.8972 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 194/1000
2023-10-27 06:23:19.500 
Epoch 194/1000 
	 loss: 45.4219, MinusLogProbMetric: 45.4219, val_loss: 45.4009, val_MinusLogProbMetric: 45.4009

Epoch 194: val_loss did not improve from 45.14745
196/196 - 35s - loss: 45.4219 - MinusLogProbMetric: 45.4219 - val_loss: 45.4009 - val_MinusLogProbMetric: 45.4009 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 195/1000
2023-10-27 06:23:54.419 
Epoch 195/1000 
	 loss: 44.8310, MinusLogProbMetric: 44.8310, val_loss: 46.3339, val_MinusLogProbMetric: 46.3339

Epoch 195: val_loss did not improve from 45.14745
196/196 - 35s - loss: 44.8310 - MinusLogProbMetric: 44.8310 - val_loss: 46.3339 - val_MinusLogProbMetric: 46.3339 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 196/1000
2023-10-27 06:24:29.034 
Epoch 196/1000 
	 loss: 45.4999, MinusLogProbMetric: 45.4999, val_loss: 45.4773, val_MinusLogProbMetric: 45.4773

Epoch 196: val_loss did not improve from 45.14745
196/196 - 35s - loss: 45.4999 - MinusLogProbMetric: 45.4999 - val_loss: 45.4773 - val_MinusLogProbMetric: 45.4773 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 197/1000
2023-10-27 06:25:03.798 
Epoch 197/1000 
	 loss: 45.3663, MinusLogProbMetric: 45.3663, val_loss: 45.2792, val_MinusLogProbMetric: 45.2792

Epoch 197: val_loss did not improve from 45.14745
196/196 - 35s - loss: 45.3663 - MinusLogProbMetric: 45.3663 - val_loss: 45.2792 - val_MinusLogProbMetric: 45.2792 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 198/1000
2023-10-27 06:25:38.162 
Epoch 198/1000 
	 loss: 45.1108, MinusLogProbMetric: 45.1108, val_loss: 46.2427, val_MinusLogProbMetric: 46.2427

Epoch 198: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.1108 - MinusLogProbMetric: 45.1108 - val_loss: 46.2427 - val_MinusLogProbMetric: 46.2427 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 199/1000
2023-10-27 06:26:12.212 
Epoch 199/1000 
	 loss: 45.5665, MinusLogProbMetric: 45.5665, val_loss: 46.3778, val_MinusLogProbMetric: 46.3778

Epoch 199: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.5665 - MinusLogProbMetric: 45.5665 - val_loss: 46.3778 - val_MinusLogProbMetric: 46.3778 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 200/1000
2023-10-27 06:26:45.937 
Epoch 200/1000 
	 loss: 45.4097, MinusLogProbMetric: 45.4097, val_loss: 45.7491, val_MinusLogProbMetric: 45.7491

Epoch 200: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.4097 - MinusLogProbMetric: 45.4097 - val_loss: 45.7491 - val_MinusLogProbMetric: 45.7491 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 201/1000
2023-10-27 06:27:20.103 
Epoch 201/1000 
	 loss: 45.0550, MinusLogProbMetric: 45.0550, val_loss: 45.3061, val_MinusLogProbMetric: 45.3061

Epoch 201: val_loss did not improve from 45.14745
196/196 - 34s - loss: 45.0550 - MinusLogProbMetric: 45.0550 - val_loss: 45.3061 - val_MinusLogProbMetric: 45.3061 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 202/1000
2023-10-27 06:27:54.905 
Epoch 202/1000 
	 loss: 44.8074, MinusLogProbMetric: 44.8074, val_loss: 45.1670, val_MinusLogProbMetric: 45.1670

Epoch 202: val_loss did not improve from 45.14745
196/196 - 35s - loss: 44.8074 - MinusLogProbMetric: 44.8074 - val_loss: 45.1670 - val_MinusLogProbMetric: 45.1670 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 203/1000
2023-10-27 06:28:24.136 
Epoch 203/1000 
	 loss: 45.2541, MinusLogProbMetric: 45.2541, val_loss: 46.0108, val_MinusLogProbMetric: 46.0108

Epoch 203: val_loss did not improve from 45.14745
196/196 - 29s - loss: 45.2541 - MinusLogProbMetric: 45.2541 - val_loss: 46.0108 - val_MinusLogProbMetric: 46.0108 - lr: 1.1111e-04 - 29s/epoch - 149ms/step
Epoch 204/1000
2023-10-27 06:28:51.913 
Epoch 204/1000 
	 loss: 45.1984, MinusLogProbMetric: 45.1984, val_loss: 46.2521, val_MinusLogProbMetric: 46.2521

Epoch 204: val_loss did not improve from 45.14745
196/196 - 28s - loss: 45.1984 - MinusLogProbMetric: 45.1984 - val_loss: 46.2521 - val_MinusLogProbMetric: 46.2521 - lr: 1.1111e-04 - 28s/epoch - 142ms/step
Epoch 205/1000
2023-10-27 06:29:23.331 
Epoch 205/1000 
	 loss: 44.8427, MinusLogProbMetric: 44.8427, val_loss: 48.2698, val_MinusLogProbMetric: 48.2698

Epoch 205: val_loss did not improve from 45.14745
196/196 - 31s - loss: 44.8427 - MinusLogProbMetric: 44.8427 - val_loss: 48.2698 - val_MinusLogProbMetric: 48.2698 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 206/1000
2023-10-27 06:29:53.212 
Epoch 206/1000 
	 loss: 45.0421, MinusLogProbMetric: 45.0421, val_loss: 46.3891, val_MinusLogProbMetric: 46.3891

Epoch 206: val_loss did not improve from 45.14745
196/196 - 30s - loss: 45.0421 - MinusLogProbMetric: 45.0421 - val_loss: 46.3891 - val_MinusLogProbMetric: 46.3891 - lr: 1.1111e-04 - 30s/epoch - 152ms/step
Epoch 207/1000
2023-10-27 06:30:21.804 
Epoch 207/1000 
	 loss: 44.9975, MinusLogProbMetric: 44.9975, val_loss: 45.5457, val_MinusLogProbMetric: 45.5457

Epoch 207: val_loss did not improve from 45.14745
196/196 - 29s - loss: 44.9975 - MinusLogProbMetric: 44.9975 - val_loss: 45.5457 - val_MinusLogProbMetric: 45.5457 - lr: 1.1111e-04 - 29s/epoch - 146ms/step
Epoch 208/1000
2023-10-27 06:30:52.760 
Epoch 208/1000 
	 loss: 45.1601, MinusLogProbMetric: 45.1601, val_loss: 46.9487, val_MinusLogProbMetric: 46.9487

Epoch 208: val_loss did not improve from 45.14745
196/196 - 31s - loss: 45.1601 - MinusLogProbMetric: 45.1601 - val_loss: 46.9487 - val_MinusLogProbMetric: 46.9487 - lr: 1.1111e-04 - 31s/epoch - 158ms/step
Epoch 209/1000
2023-10-27 06:31:26.474 
Epoch 209/1000 
	 loss: 45.0100, MinusLogProbMetric: 45.0100, val_loss: 44.9176, val_MinusLogProbMetric: 44.9176

Epoch 209: val_loss improved from 45.14745 to 44.91763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 45.0100 - MinusLogProbMetric: 45.0100 - val_loss: 44.9176 - val_MinusLogProbMetric: 44.9176 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 210/1000
2023-10-27 06:32:00.703 
Epoch 210/1000 
	 loss: 44.9767, MinusLogProbMetric: 44.9767, val_loss: 45.6903, val_MinusLogProbMetric: 45.6903

Epoch 210: val_loss did not improve from 44.91763
196/196 - 34s - loss: 44.9767 - MinusLogProbMetric: 44.9767 - val_loss: 45.6903 - val_MinusLogProbMetric: 45.6903 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 211/1000
2023-10-27 06:32:34.400 
Epoch 211/1000 
	 loss: 45.0895, MinusLogProbMetric: 45.0895, val_loss: 45.2090, val_MinusLogProbMetric: 45.2090

Epoch 211: val_loss did not improve from 44.91763
196/196 - 34s - loss: 45.0895 - MinusLogProbMetric: 45.0895 - val_loss: 45.2090 - val_MinusLogProbMetric: 45.2090 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 212/1000
2023-10-27 06:33:07.905 
Epoch 212/1000 
	 loss: 44.8631, MinusLogProbMetric: 44.8631, val_loss: 45.8338, val_MinusLogProbMetric: 45.8338

Epoch 212: val_loss did not improve from 44.91763
196/196 - 34s - loss: 44.8631 - MinusLogProbMetric: 44.8631 - val_loss: 45.8338 - val_MinusLogProbMetric: 45.8338 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 213/1000
2023-10-27 06:33:41.403 
Epoch 213/1000 
	 loss: 45.1479, MinusLogProbMetric: 45.1479, val_loss: 46.1502, val_MinusLogProbMetric: 46.1502

Epoch 213: val_loss did not improve from 44.91763
196/196 - 33s - loss: 45.1479 - MinusLogProbMetric: 45.1479 - val_loss: 46.1502 - val_MinusLogProbMetric: 46.1502 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 214/1000
2023-10-27 06:34:15.509 
Epoch 214/1000 
	 loss: 44.6424, MinusLogProbMetric: 44.6424, val_loss: 44.8985, val_MinusLogProbMetric: 44.8985

Epoch 214: val_loss improved from 44.91763 to 44.89850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 35s - loss: 44.6424 - MinusLogProbMetric: 44.6424 - val_loss: 44.8985 - val_MinusLogProbMetric: 44.8985 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 215/1000
2023-10-27 06:34:49.778 
Epoch 215/1000 
	 loss: 46.0059, MinusLogProbMetric: 46.0059, val_loss: 44.7961, val_MinusLogProbMetric: 44.7961

Epoch 215: val_loss improved from 44.89850 to 44.79612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 46.0059 - MinusLogProbMetric: 46.0059 - val_loss: 44.7961 - val_MinusLogProbMetric: 44.7961 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 216/1000
2023-10-27 06:35:23.305 
Epoch 216/1000 
	 loss: 44.7632, MinusLogProbMetric: 44.7632, val_loss: 52.9234, val_MinusLogProbMetric: 52.9234

Epoch 216: val_loss did not improve from 44.79612
196/196 - 33s - loss: 44.7632 - MinusLogProbMetric: 44.7632 - val_loss: 52.9234 - val_MinusLogProbMetric: 52.9234 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 217/1000
2023-10-27 06:35:56.858 
Epoch 217/1000 
	 loss: 47.3539, MinusLogProbMetric: 47.3539, val_loss: 46.8994, val_MinusLogProbMetric: 46.8994

Epoch 217: val_loss did not improve from 44.79612
196/196 - 34s - loss: 47.3539 - MinusLogProbMetric: 47.3539 - val_loss: 46.8994 - val_MinusLogProbMetric: 46.8994 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 218/1000
2023-10-27 06:36:30.364 
Epoch 218/1000 
	 loss: 45.0286, MinusLogProbMetric: 45.0286, val_loss: 45.2378, val_MinusLogProbMetric: 45.2378

Epoch 218: val_loss did not improve from 44.79612
196/196 - 34s - loss: 45.0286 - MinusLogProbMetric: 45.0286 - val_loss: 45.2378 - val_MinusLogProbMetric: 45.2378 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 219/1000
2023-10-27 06:37:04.098 
Epoch 219/1000 
	 loss: 45.1314, MinusLogProbMetric: 45.1314, val_loss: 45.5324, val_MinusLogProbMetric: 45.5324

Epoch 219: val_loss did not improve from 44.79612
196/196 - 34s - loss: 45.1314 - MinusLogProbMetric: 45.1314 - val_loss: 45.5324 - val_MinusLogProbMetric: 45.5324 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 220/1000
2023-10-27 06:37:37.775 
Epoch 220/1000 
	 loss: 45.1340, MinusLogProbMetric: 45.1340, val_loss: 45.2788, val_MinusLogProbMetric: 45.2788

Epoch 220: val_loss did not improve from 44.79612
196/196 - 34s - loss: 45.1340 - MinusLogProbMetric: 45.1340 - val_loss: 45.2788 - val_MinusLogProbMetric: 45.2788 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 221/1000
2023-10-27 06:38:11.236 
Epoch 221/1000 
	 loss: 45.1190, MinusLogProbMetric: 45.1190, val_loss: 45.2624, val_MinusLogProbMetric: 45.2624

Epoch 221: val_loss did not improve from 44.79612
196/196 - 33s - loss: 45.1190 - MinusLogProbMetric: 45.1190 - val_loss: 45.2624 - val_MinusLogProbMetric: 45.2624 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 222/1000
2023-10-27 06:38:44.899 
Epoch 222/1000 
	 loss: 44.8575, MinusLogProbMetric: 44.8575, val_loss: 47.3343, val_MinusLogProbMetric: 47.3343

Epoch 222: val_loss did not improve from 44.79612
196/196 - 34s - loss: 44.8575 - MinusLogProbMetric: 44.8575 - val_loss: 47.3343 - val_MinusLogProbMetric: 47.3343 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 223/1000
2023-10-27 06:39:17.822 
Epoch 223/1000 
	 loss: 44.6240, MinusLogProbMetric: 44.6240, val_loss: 47.0787, val_MinusLogProbMetric: 47.0787

Epoch 223: val_loss did not improve from 44.79612
196/196 - 33s - loss: 44.6240 - MinusLogProbMetric: 44.6240 - val_loss: 47.0787 - val_MinusLogProbMetric: 47.0787 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 224/1000
2023-10-27 06:39:50.007 
Epoch 224/1000 
	 loss: 44.9613, MinusLogProbMetric: 44.9613, val_loss: 44.6629, val_MinusLogProbMetric: 44.6629

Epoch 224: val_loss improved from 44.79612 to 44.66294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 44.9613 - MinusLogProbMetric: 44.9613 - val_loss: 44.6629 - val_MinusLogProbMetric: 44.6629 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 225/1000
2023-10-27 06:40:24.213 
Epoch 225/1000 
	 loss: 45.1562, MinusLogProbMetric: 45.1562, val_loss: 45.4077, val_MinusLogProbMetric: 45.4077

Epoch 225: val_loss did not improve from 44.66294
196/196 - 34s - loss: 45.1562 - MinusLogProbMetric: 45.1562 - val_loss: 45.4077 - val_MinusLogProbMetric: 45.4077 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 226/1000
2023-10-27 06:40:57.659 
Epoch 226/1000 
	 loss: 44.6539, MinusLogProbMetric: 44.6539, val_loss: 44.8327, val_MinusLogProbMetric: 44.8327

Epoch 226: val_loss did not improve from 44.66294
196/196 - 33s - loss: 44.6539 - MinusLogProbMetric: 44.6539 - val_loss: 44.8327 - val_MinusLogProbMetric: 44.8327 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 227/1000
2023-10-27 06:41:31.198 
Epoch 227/1000 
	 loss: 44.9291, MinusLogProbMetric: 44.9291, val_loss: 44.8939, val_MinusLogProbMetric: 44.8939

Epoch 227: val_loss did not improve from 44.66294
196/196 - 34s - loss: 44.9291 - MinusLogProbMetric: 44.9291 - val_loss: 44.8939 - val_MinusLogProbMetric: 44.8939 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 228/1000
2023-10-27 06:42:04.180 
Epoch 228/1000 
	 loss: 44.8033, MinusLogProbMetric: 44.8033, val_loss: 46.3266, val_MinusLogProbMetric: 46.3266

Epoch 228: val_loss did not improve from 44.66294
196/196 - 33s - loss: 44.8033 - MinusLogProbMetric: 44.8033 - val_loss: 46.3266 - val_MinusLogProbMetric: 46.3266 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 229/1000
2023-10-27 06:42:37.750 
Epoch 229/1000 
	 loss: 44.6243, MinusLogProbMetric: 44.6243, val_loss: 45.9411, val_MinusLogProbMetric: 45.9411

Epoch 229: val_loss did not improve from 44.66294
196/196 - 34s - loss: 44.6243 - MinusLogProbMetric: 44.6243 - val_loss: 45.9411 - val_MinusLogProbMetric: 45.9411 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 230/1000
2023-10-27 06:43:10.219 
Epoch 230/1000 
	 loss: 45.2277, MinusLogProbMetric: 45.2277, val_loss: 45.9462, val_MinusLogProbMetric: 45.9462

Epoch 230: val_loss did not improve from 44.66294
196/196 - 32s - loss: 45.2277 - MinusLogProbMetric: 45.2277 - val_loss: 45.9462 - val_MinusLogProbMetric: 45.9462 - lr: 1.1111e-04 - 32s/epoch - 166ms/step
Epoch 231/1000
2023-10-27 06:43:43.839 
Epoch 231/1000 
	 loss: 44.8710, MinusLogProbMetric: 44.8710, val_loss: 44.9468, val_MinusLogProbMetric: 44.9468

Epoch 231: val_loss did not improve from 44.66294
196/196 - 34s - loss: 44.8710 - MinusLogProbMetric: 44.8710 - val_loss: 44.9468 - val_MinusLogProbMetric: 44.9468 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 232/1000
2023-10-27 06:44:17.333 
Epoch 232/1000 
	 loss: 44.8063, MinusLogProbMetric: 44.8063, val_loss: 44.9447, val_MinusLogProbMetric: 44.9447

Epoch 232: val_loss did not improve from 44.66294
196/196 - 33s - loss: 44.8063 - MinusLogProbMetric: 44.8063 - val_loss: 44.9447 - val_MinusLogProbMetric: 44.9447 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 233/1000
2023-10-27 06:44:50.643 
Epoch 233/1000 
	 loss: 44.3963, MinusLogProbMetric: 44.3963, val_loss: 44.5183, val_MinusLogProbMetric: 44.5183

Epoch 233: val_loss improved from 44.66294 to 44.51826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 44.3963 - MinusLogProbMetric: 44.3963 - val_loss: 44.5183 - val_MinusLogProbMetric: 44.5183 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 234/1000
2023-10-27 06:45:24.611 
Epoch 234/1000 
	 loss: 44.8270, MinusLogProbMetric: 44.8270, val_loss: 44.3123, val_MinusLogProbMetric: 44.3123

Epoch 234: val_loss improved from 44.51826 to 44.31226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 44.8270 - MinusLogProbMetric: 44.8270 - val_loss: 44.3123 - val_MinusLogProbMetric: 44.3123 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 235/1000
2023-10-27 06:45:58.469 
Epoch 235/1000 
	 loss: 44.5358, MinusLogProbMetric: 44.5358, val_loss: 45.0007, val_MinusLogProbMetric: 45.0007

Epoch 235: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.5358 - MinusLogProbMetric: 44.5358 - val_loss: 45.0007 - val_MinusLogProbMetric: 45.0007 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 236/1000
2023-10-27 06:46:31.702 
Epoch 236/1000 
	 loss: 44.8348, MinusLogProbMetric: 44.8348, val_loss: 44.4020, val_MinusLogProbMetric: 44.4020

Epoch 236: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.8348 - MinusLogProbMetric: 44.8348 - val_loss: 44.4020 - val_MinusLogProbMetric: 44.4020 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 237/1000
2023-10-27 06:47:05.445 
Epoch 237/1000 
	 loss: 44.6775, MinusLogProbMetric: 44.6775, val_loss: 45.5336, val_MinusLogProbMetric: 45.5336

Epoch 237: val_loss did not improve from 44.31226
196/196 - 34s - loss: 44.6775 - MinusLogProbMetric: 44.6775 - val_loss: 45.5336 - val_MinusLogProbMetric: 45.5336 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 238/1000
2023-10-27 06:47:38.862 
Epoch 238/1000 
	 loss: 44.6670, MinusLogProbMetric: 44.6670, val_loss: 44.7020, val_MinusLogProbMetric: 44.7020

Epoch 238: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.6670 - MinusLogProbMetric: 44.6670 - val_loss: 44.7020 - val_MinusLogProbMetric: 44.7020 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 239/1000
2023-10-27 06:48:12.459 
Epoch 239/1000 
	 loss: 44.2946, MinusLogProbMetric: 44.2946, val_loss: 45.1773, val_MinusLogProbMetric: 45.1773

Epoch 239: val_loss did not improve from 44.31226
196/196 - 34s - loss: 44.2946 - MinusLogProbMetric: 44.2946 - val_loss: 45.1773 - val_MinusLogProbMetric: 45.1773 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 240/1000
2023-10-27 06:48:45.539 
Epoch 240/1000 
	 loss: 44.7255, MinusLogProbMetric: 44.7255, val_loss: 45.0654, val_MinusLogProbMetric: 45.0654

Epoch 240: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.7255 - MinusLogProbMetric: 44.7255 - val_loss: 45.0654 - val_MinusLogProbMetric: 45.0654 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 241/1000
2023-10-27 06:49:18.694 
Epoch 241/1000 
	 loss: 44.6516, MinusLogProbMetric: 44.6516, val_loss: 44.6669, val_MinusLogProbMetric: 44.6669

Epoch 241: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.6516 - MinusLogProbMetric: 44.6516 - val_loss: 44.6669 - val_MinusLogProbMetric: 44.6669 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 242/1000
2023-10-27 06:49:52.049 
Epoch 242/1000 
	 loss: 44.2990, MinusLogProbMetric: 44.2990, val_loss: 44.4848, val_MinusLogProbMetric: 44.4848

Epoch 242: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.2990 - MinusLogProbMetric: 44.2990 - val_loss: 44.4848 - val_MinusLogProbMetric: 44.4848 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 243/1000
2023-10-27 06:50:25.371 
Epoch 243/1000 
	 loss: 44.6976, MinusLogProbMetric: 44.6976, val_loss: 48.3634, val_MinusLogProbMetric: 48.3634

Epoch 243: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.6976 - MinusLogProbMetric: 44.6976 - val_loss: 48.3634 - val_MinusLogProbMetric: 48.3634 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 244/1000
2023-10-27 06:50:57.820 
Epoch 244/1000 
	 loss: 44.8106, MinusLogProbMetric: 44.8106, val_loss: 46.0356, val_MinusLogProbMetric: 46.0356

Epoch 244: val_loss did not improve from 44.31226
196/196 - 32s - loss: 44.8106 - MinusLogProbMetric: 44.8106 - val_loss: 46.0356 - val_MinusLogProbMetric: 46.0356 - lr: 1.1111e-04 - 32s/epoch - 166ms/step
Epoch 245/1000
2023-10-27 06:51:31.508 
Epoch 245/1000 
	 loss: 44.5354, MinusLogProbMetric: 44.5354, val_loss: 45.5793, val_MinusLogProbMetric: 45.5793

Epoch 245: val_loss did not improve from 44.31226
196/196 - 34s - loss: 44.5354 - MinusLogProbMetric: 44.5354 - val_loss: 45.5793 - val_MinusLogProbMetric: 45.5793 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 246/1000
2023-10-27 06:52:04.246 
Epoch 246/1000 
	 loss: 44.1232, MinusLogProbMetric: 44.1232, val_loss: 47.3432, val_MinusLogProbMetric: 47.3432

Epoch 246: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.1232 - MinusLogProbMetric: 44.1232 - val_loss: 47.3432 - val_MinusLogProbMetric: 47.3432 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 247/1000
2023-10-27 06:52:37.579 
Epoch 247/1000 
	 loss: 44.3226, MinusLogProbMetric: 44.3226, val_loss: 46.3200, val_MinusLogProbMetric: 46.3200

Epoch 247: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.3226 - MinusLogProbMetric: 44.3226 - val_loss: 46.3200 - val_MinusLogProbMetric: 46.3200 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 248/1000
2023-10-27 06:53:10.644 
Epoch 248/1000 
	 loss: 44.7546, MinusLogProbMetric: 44.7546, val_loss: 45.5207, val_MinusLogProbMetric: 45.5207

Epoch 248: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.7546 - MinusLogProbMetric: 44.7546 - val_loss: 45.5207 - val_MinusLogProbMetric: 45.5207 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 249/1000
2023-10-27 06:53:43.311 
Epoch 249/1000 
	 loss: 44.5151, MinusLogProbMetric: 44.5151, val_loss: 45.0934, val_MinusLogProbMetric: 45.0934

Epoch 249: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.5151 - MinusLogProbMetric: 44.5151 - val_loss: 45.0934 - val_MinusLogProbMetric: 45.0934 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 250/1000
2023-10-27 06:54:15.373 
Epoch 250/1000 
	 loss: 44.5491, MinusLogProbMetric: 44.5491, val_loss: 47.0023, val_MinusLogProbMetric: 47.0023

Epoch 250: val_loss did not improve from 44.31226
196/196 - 32s - loss: 44.5491 - MinusLogProbMetric: 44.5491 - val_loss: 47.0023 - val_MinusLogProbMetric: 47.0023 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 251/1000
2023-10-27 06:54:48.258 
Epoch 251/1000 
	 loss: 44.5642, MinusLogProbMetric: 44.5642, val_loss: 45.7420, val_MinusLogProbMetric: 45.7420

Epoch 251: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.5642 - MinusLogProbMetric: 44.5642 - val_loss: 45.7420 - val_MinusLogProbMetric: 45.7420 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 252/1000
2023-10-27 06:55:20.385 
Epoch 252/1000 
	 loss: 44.4105, MinusLogProbMetric: 44.4105, val_loss: 46.7132, val_MinusLogProbMetric: 46.7132

Epoch 252: val_loss did not improve from 44.31226
196/196 - 32s - loss: 44.4105 - MinusLogProbMetric: 44.4105 - val_loss: 46.7132 - val_MinusLogProbMetric: 46.7132 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 253/1000
2023-10-27 06:55:54.014 
Epoch 253/1000 
	 loss: 44.8479, MinusLogProbMetric: 44.8479, val_loss: 46.0170, val_MinusLogProbMetric: 46.0170

Epoch 253: val_loss did not improve from 44.31226
196/196 - 34s - loss: 44.8479 - MinusLogProbMetric: 44.8479 - val_loss: 46.0170 - val_MinusLogProbMetric: 46.0170 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 254/1000
2023-10-27 06:56:27.303 
Epoch 254/1000 
	 loss: 44.2227, MinusLogProbMetric: 44.2227, val_loss: 44.9565, val_MinusLogProbMetric: 44.9565

Epoch 254: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.2227 - MinusLogProbMetric: 44.2227 - val_loss: 44.9565 - val_MinusLogProbMetric: 44.9565 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 255/1000
2023-10-27 06:57:00.996 
Epoch 255/1000 
	 loss: 44.3402, MinusLogProbMetric: 44.3402, val_loss: 45.0305, val_MinusLogProbMetric: 45.0305

Epoch 255: val_loss did not improve from 44.31226
196/196 - 34s - loss: 44.3402 - MinusLogProbMetric: 44.3402 - val_loss: 45.0305 - val_MinusLogProbMetric: 45.0305 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 256/1000
2023-10-27 06:57:33.574 
Epoch 256/1000 
	 loss: 44.4071, MinusLogProbMetric: 44.4071, val_loss: 45.0992, val_MinusLogProbMetric: 45.0992

Epoch 256: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.4071 - MinusLogProbMetric: 44.4071 - val_loss: 45.0992 - val_MinusLogProbMetric: 45.0992 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 257/1000
2023-10-27 06:58:06.748 
Epoch 257/1000 
	 loss: 44.4208, MinusLogProbMetric: 44.4208, val_loss: 44.4603, val_MinusLogProbMetric: 44.4603

Epoch 257: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.4208 - MinusLogProbMetric: 44.4208 - val_loss: 44.4603 - val_MinusLogProbMetric: 44.4603 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 258/1000
2023-10-27 06:58:39.694 
Epoch 258/1000 
	 loss: 44.4282, MinusLogProbMetric: 44.4282, val_loss: 45.1293, val_MinusLogProbMetric: 45.1293

Epoch 258: val_loss did not improve from 44.31226
196/196 - 33s - loss: 44.4282 - MinusLogProbMetric: 44.4282 - val_loss: 45.1293 - val_MinusLogProbMetric: 45.1293 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 259/1000
2023-10-27 06:59:13.482 
Epoch 259/1000 
	 loss: 44.2985, MinusLogProbMetric: 44.2985, val_loss: 44.1903, val_MinusLogProbMetric: 44.1903

Epoch 259: val_loss improved from 44.31226 to 44.19027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 44.2985 - MinusLogProbMetric: 44.2985 - val_loss: 44.1903 - val_MinusLogProbMetric: 44.1903 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 260/1000
2023-10-27 06:59:47.543 
Epoch 260/1000 
	 loss: 44.0439, MinusLogProbMetric: 44.0439, val_loss: 46.2822, val_MinusLogProbMetric: 46.2822

Epoch 260: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.0439 - MinusLogProbMetric: 44.0439 - val_loss: 46.2822 - val_MinusLogProbMetric: 46.2822 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 261/1000
2023-10-27 07:00:20.688 
Epoch 261/1000 
	 loss: 44.6294, MinusLogProbMetric: 44.6294, val_loss: 44.6948, val_MinusLogProbMetric: 44.6948

Epoch 261: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.6294 - MinusLogProbMetric: 44.6294 - val_loss: 44.6948 - val_MinusLogProbMetric: 44.6948 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 262/1000
2023-10-27 07:00:54.204 
Epoch 262/1000 
	 loss: 44.2336, MinusLogProbMetric: 44.2336, val_loss: 44.4870, val_MinusLogProbMetric: 44.4870

Epoch 262: val_loss did not improve from 44.19027
196/196 - 34s - loss: 44.2336 - MinusLogProbMetric: 44.2336 - val_loss: 44.4870 - val_MinusLogProbMetric: 44.4870 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 263/1000
2023-10-27 07:01:27.670 
Epoch 263/1000 
	 loss: 44.2167, MinusLogProbMetric: 44.2167, val_loss: 46.9982, val_MinusLogProbMetric: 46.9982

Epoch 263: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.2167 - MinusLogProbMetric: 44.2167 - val_loss: 46.9982 - val_MinusLogProbMetric: 46.9982 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 264/1000
2023-10-27 07:02:01.259 
Epoch 264/1000 
	 loss: 44.4748, MinusLogProbMetric: 44.4748, val_loss: 46.5564, val_MinusLogProbMetric: 46.5564

Epoch 264: val_loss did not improve from 44.19027
196/196 - 34s - loss: 44.4748 - MinusLogProbMetric: 44.4748 - val_loss: 46.5564 - val_MinusLogProbMetric: 46.5564 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 265/1000
2023-10-27 07:02:34.594 
Epoch 265/1000 
	 loss: 44.1061, MinusLogProbMetric: 44.1061, val_loss: 45.9976, val_MinusLogProbMetric: 45.9976

Epoch 265: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.1061 - MinusLogProbMetric: 44.1061 - val_loss: 45.9976 - val_MinusLogProbMetric: 45.9976 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 266/1000
2023-10-27 07:03:07.843 
Epoch 266/1000 
	 loss: 44.3359, MinusLogProbMetric: 44.3359, val_loss: 45.2206, val_MinusLogProbMetric: 45.2206

Epoch 266: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.3359 - MinusLogProbMetric: 44.3359 - val_loss: 45.2206 - val_MinusLogProbMetric: 45.2206 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 267/1000
2023-10-27 07:03:41.369 
Epoch 267/1000 
	 loss: 44.1101, MinusLogProbMetric: 44.1101, val_loss: 44.2566, val_MinusLogProbMetric: 44.2566

Epoch 267: val_loss did not improve from 44.19027
196/196 - 34s - loss: 44.1101 - MinusLogProbMetric: 44.1101 - val_loss: 44.2566 - val_MinusLogProbMetric: 44.2566 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 268/1000
2023-10-27 07:04:14.518 
Epoch 268/1000 
	 loss: 44.4314, MinusLogProbMetric: 44.4314, val_loss: 44.6615, val_MinusLogProbMetric: 44.6615

Epoch 268: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.4314 - MinusLogProbMetric: 44.4314 - val_loss: 44.6615 - val_MinusLogProbMetric: 44.6615 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 269/1000
2023-10-27 07:04:47.990 
Epoch 269/1000 
	 loss: 44.0196, MinusLogProbMetric: 44.0196, val_loss: 45.6002, val_MinusLogProbMetric: 45.6002

Epoch 269: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.0196 - MinusLogProbMetric: 44.0196 - val_loss: 45.6002 - val_MinusLogProbMetric: 45.6002 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 270/1000
2023-10-27 07:05:21.771 
Epoch 270/1000 
	 loss: 44.1806, MinusLogProbMetric: 44.1806, val_loss: 45.0343, val_MinusLogProbMetric: 45.0343

Epoch 270: val_loss did not improve from 44.19027
196/196 - 34s - loss: 44.1806 - MinusLogProbMetric: 44.1806 - val_loss: 45.0343 - val_MinusLogProbMetric: 45.0343 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 271/1000
2023-10-27 07:05:55.048 
Epoch 271/1000 
	 loss: 44.4516, MinusLogProbMetric: 44.4516, val_loss: 45.0883, val_MinusLogProbMetric: 45.0883

Epoch 271: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.4516 - MinusLogProbMetric: 44.4516 - val_loss: 45.0883 - val_MinusLogProbMetric: 45.0883 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 272/1000
2023-10-27 07:06:25.201 
Epoch 272/1000 
	 loss: 44.2212, MinusLogProbMetric: 44.2212, val_loss: 45.3323, val_MinusLogProbMetric: 45.3323

Epoch 272: val_loss did not improve from 44.19027
196/196 - 30s - loss: 44.2212 - MinusLogProbMetric: 44.2212 - val_loss: 45.3323 - val_MinusLogProbMetric: 45.3323 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 273/1000
2023-10-27 07:06:55.760 
Epoch 273/1000 
	 loss: 43.9567, MinusLogProbMetric: 43.9567, val_loss: 46.0548, val_MinusLogProbMetric: 46.0548

Epoch 273: val_loss did not improve from 44.19027
196/196 - 31s - loss: 43.9567 - MinusLogProbMetric: 43.9567 - val_loss: 46.0548 - val_MinusLogProbMetric: 46.0548 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 274/1000
2023-10-27 07:07:25.272 
Epoch 274/1000 
	 loss: 44.2545, MinusLogProbMetric: 44.2545, val_loss: 45.3456, val_MinusLogProbMetric: 45.3456

Epoch 274: val_loss did not improve from 44.19027
196/196 - 30s - loss: 44.2545 - MinusLogProbMetric: 44.2545 - val_loss: 45.3456 - val_MinusLogProbMetric: 45.3456 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 275/1000
2023-10-27 07:07:54.820 
Epoch 275/1000 
	 loss: 44.1775, MinusLogProbMetric: 44.1775, val_loss: 44.5762, val_MinusLogProbMetric: 44.5762

Epoch 275: val_loss did not improve from 44.19027
196/196 - 30s - loss: 44.1775 - MinusLogProbMetric: 44.1775 - val_loss: 44.5762 - val_MinusLogProbMetric: 44.5762 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 276/1000
2023-10-27 07:08:27.867 
Epoch 276/1000 
	 loss: 44.3340, MinusLogProbMetric: 44.3340, val_loss: 44.4701, val_MinusLogProbMetric: 44.4701

Epoch 276: val_loss did not improve from 44.19027
196/196 - 33s - loss: 44.3340 - MinusLogProbMetric: 44.3340 - val_loss: 44.4701 - val_MinusLogProbMetric: 44.4701 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 277/1000
2023-10-27 07:09:01.649 
Epoch 277/1000 
	 loss: 44.1585, MinusLogProbMetric: 44.1585, val_loss: 45.4234, val_MinusLogProbMetric: 45.4234

Epoch 277: val_loss did not improve from 44.19027
196/196 - 34s - loss: 44.1585 - MinusLogProbMetric: 44.1585 - val_loss: 45.4234 - val_MinusLogProbMetric: 45.4234 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 278/1000
2023-10-27 07:09:35.370 
Epoch 278/1000 
	 loss: 44.3307, MinusLogProbMetric: 44.3307, val_loss: 45.2740, val_MinusLogProbMetric: 45.2740

Epoch 278: val_loss did not improve from 44.19027
196/196 - 34s - loss: 44.3307 - MinusLogProbMetric: 44.3307 - val_loss: 45.2740 - val_MinusLogProbMetric: 45.2740 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 279/1000
2023-10-27 07:10:08.404 
Epoch 279/1000 
	 loss: 44.2345, MinusLogProbMetric: 44.2345, val_loss: 43.8520, val_MinusLogProbMetric: 43.8520

Epoch 279: val_loss improved from 44.19027 to 43.85202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 44.2345 - MinusLogProbMetric: 44.2345 - val_loss: 43.8520 - val_MinusLogProbMetric: 43.8520 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 280/1000
2023-10-27 07:10:42.925 
Epoch 280/1000 
	 loss: 44.2203, MinusLogProbMetric: 44.2203, val_loss: 45.2792, val_MinusLogProbMetric: 45.2792

Epoch 280: val_loss did not improve from 43.85202
196/196 - 34s - loss: 44.2203 - MinusLogProbMetric: 44.2203 - val_loss: 45.2792 - val_MinusLogProbMetric: 45.2792 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 281/1000
2023-10-27 07:11:16.138 
Epoch 281/1000 
	 loss: 44.2290, MinusLogProbMetric: 44.2290, val_loss: 45.2749, val_MinusLogProbMetric: 45.2749

Epoch 281: val_loss did not improve from 43.85202
196/196 - 33s - loss: 44.2290 - MinusLogProbMetric: 44.2290 - val_loss: 45.2749 - val_MinusLogProbMetric: 45.2749 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 282/1000
2023-10-27 07:11:49.625 
Epoch 282/1000 
	 loss: 44.4176, MinusLogProbMetric: 44.4176, val_loss: 45.6168, val_MinusLogProbMetric: 45.6168

Epoch 282: val_loss did not improve from 43.85202
196/196 - 33s - loss: 44.4176 - MinusLogProbMetric: 44.4176 - val_loss: 45.6168 - val_MinusLogProbMetric: 45.6168 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 283/1000
2023-10-27 07:12:23.077 
Epoch 283/1000 
	 loss: 43.8617, MinusLogProbMetric: 43.8617, val_loss: 44.6889, val_MinusLogProbMetric: 44.6889

Epoch 283: val_loss did not improve from 43.85202
196/196 - 33s - loss: 43.8617 - MinusLogProbMetric: 43.8617 - val_loss: 44.6889 - val_MinusLogProbMetric: 44.6889 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 284/1000
2023-10-27 07:12:56.643 
Epoch 284/1000 
	 loss: 44.1956, MinusLogProbMetric: 44.1956, val_loss: 45.3436, val_MinusLogProbMetric: 45.3436

Epoch 284: val_loss did not improve from 43.85202
196/196 - 34s - loss: 44.1956 - MinusLogProbMetric: 44.1956 - val_loss: 45.3436 - val_MinusLogProbMetric: 45.3436 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 285/1000
2023-10-27 07:13:28.396 
Epoch 285/1000 
	 loss: 43.9864, MinusLogProbMetric: 43.9864, val_loss: 45.7702, val_MinusLogProbMetric: 45.7702

Epoch 285: val_loss did not improve from 43.85202
196/196 - 32s - loss: 43.9864 - MinusLogProbMetric: 43.9864 - val_loss: 45.7702 - val_MinusLogProbMetric: 45.7702 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 286/1000
2023-10-27 07:13:55.937 
Epoch 286/1000 
	 loss: 43.9127, MinusLogProbMetric: 43.9127, val_loss: 45.4807, val_MinusLogProbMetric: 45.4807

Epoch 286: val_loss did not improve from 43.85202
196/196 - 28s - loss: 43.9127 - MinusLogProbMetric: 43.9127 - val_loss: 45.4807 - val_MinusLogProbMetric: 45.4807 - lr: 1.1111e-04 - 28s/epoch - 141ms/step
Epoch 287/1000
2023-10-27 07:14:23.403 
Epoch 287/1000 
	 loss: 43.9200, MinusLogProbMetric: 43.9200, val_loss: 43.7992, val_MinusLogProbMetric: 43.7992

Epoch 287: val_loss improved from 43.85202 to 43.79919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 28s - loss: 43.9200 - MinusLogProbMetric: 43.9200 - val_loss: 43.7992 - val_MinusLogProbMetric: 43.7992 - lr: 1.1111e-04 - 28s/epoch - 142ms/step
Epoch 288/1000
2023-10-27 07:14:51.210 
Epoch 288/1000 
	 loss: 43.9448, MinusLogProbMetric: 43.9448, val_loss: 46.2621, val_MinusLogProbMetric: 46.2621

Epoch 288: val_loss did not improve from 43.79919
196/196 - 27s - loss: 43.9448 - MinusLogProbMetric: 43.9448 - val_loss: 46.2621 - val_MinusLogProbMetric: 46.2621 - lr: 1.1111e-04 - 27s/epoch - 140ms/step
Epoch 289/1000
2023-10-27 07:15:20.984 
Epoch 289/1000 
	 loss: 44.2797, MinusLogProbMetric: 44.2797, val_loss: 45.5863, val_MinusLogProbMetric: 45.5863

Epoch 289: val_loss did not improve from 43.79919
196/196 - 30s - loss: 44.2797 - MinusLogProbMetric: 44.2797 - val_loss: 45.5863 - val_MinusLogProbMetric: 45.5863 - lr: 1.1111e-04 - 30s/epoch - 152ms/step
Epoch 290/1000
2023-10-27 07:15:52.898 
Epoch 290/1000 
	 loss: 44.0414, MinusLogProbMetric: 44.0414, val_loss: 44.6527, val_MinusLogProbMetric: 44.6527

Epoch 290: val_loss did not improve from 43.79919
196/196 - 32s - loss: 44.0414 - MinusLogProbMetric: 44.0414 - val_loss: 44.6527 - val_MinusLogProbMetric: 44.6527 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 291/1000
2023-10-27 07:16:26.014 
Epoch 291/1000 
	 loss: 43.9179, MinusLogProbMetric: 43.9179, val_loss: 45.9208, val_MinusLogProbMetric: 45.9208

Epoch 291: val_loss did not improve from 43.79919
196/196 - 33s - loss: 43.9179 - MinusLogProbMetric: 43.9179 - val_loss: 45.9208 - val_MinusLogProbMetric: 45.9208 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 292/1000
2023-10-27 07:16:59.494 
Epoch 292/1000 
	 loss: 44.2971, MinusLogProbMetric: 44.2971, val_loss: 44.6373, val_MinusLogProbMetric: 44.6373

Epoch 292: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.2971 - MinusLogProbMetric: 44.2971 - val_loss: 44.6373 - val_MinusLogProbMetric: 44.6373 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 293/1000
2023-10-27 07:17:31.495 
Epoch 293/1000 
	 loss: 44.0930, MinusLogProbMetric: 44.0930, val_loss: 44.0158, val_MinusLogProbMetric: 44.0158

Epoch 293: val_loss did not improve from 43.79919
196/196 - 32s - loss: 44.0930 - MinusLogProbMetric: 44.0930 - val_loss: 44.0158 - val_MinusLogProbMetric: 44.0158 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 294/1000
2023-10-27 07:18:04.882 
Epoch 294/1000 
	 loss: 43.8978, MinusLogProbMetric: 43.8978, val_loss: 45.1615, val_MinusLogProbMetric: 45.1615

Epoch 294: val_loss did not improve from 43.79919
196/196 - 33s - loss: 43.8978 - MinusLogProbMetric: 43.8978 - val_loss: 45.1615 - val_MinusLogProbMetric: 45.1615 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 295/1000
2023-10-27 07:18:37.357 
Epoch 295/1000 
	 loss: 44.1150, MinusLogProbMetric: 44.1150, val_loss: 45.4800, val_MinusLogProbMetric: 45.4800

Epoch 295: val_loss did not improve from 43.79919
196/196 - 32s - loss: 44.1150 - MinusLogProbMetric: 44.1150 - val_loss: 45.4800 - val_MinusLogProbMetric: 45.4800 - lr: 1.1111e-04 - 32s/epoch - 166ms/step
Epoch 296/1000
2023-10-27 07:19:10.289 
Epoch 296/1000 
	 loss: 44.1564, MinusLogProbMetric: 44.1564, val_loss: 44.7375, val_MinusLogProbMetric: 44.7375

Epoch 296: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.1564 - MinusLogProbMetric: 44.1564 - val_loss: 44.7375 - val_MinusLogProbMetric: 44.7375 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 297/1000
2023-10-27 07:19:43.914 
Epoch 297/1000 
	 loss: 43.9696, MinusLogProbMetric: 43.9696, val_loss: 44.4291, val_MinusLogProbMetric: 44.4291

Epoch 297: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.9696 - MinusLogProbMetric: 43.9696 - val_loss: 44.4291 - val_MinusLogProbMetric: 44.4291 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 298/1000
2023-10-27 07:20:16.216 
Epoch 298/1000 
	 loss: 43.9307, MinusLogProbMetric: 43.9307, val_loss: 44.3031, val_MinusLogProbMetric: 44.3031

Epoch 298: val_loss did not improve from 43.79919
196/196 - 32s - loss: 43.9307 - MinusLogProbMetric: 43.9307 - val_loss: 44.3031 - val_MinusLogProbMetric: 44.3031 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 299/1000
2023-10-27 07:20:49.988 
Epoch 299/1000 
	 loss: 44.1546, MinusLogProbMetric: 44.1546, val_loss: 43.9264, val_MinusLogProbMetric: 43.9264

Epoch 299: val_loss did not improve from 43.79919
196/196 - 34s - loss: 44.1546 - MinusLogProbMetric: 44.1546 - val_loss: 43.9264 - val_MinusLogProbMetric: 43.9264 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 300/1000
2023-10-27 07:21:23.269 
Epoch 300/1000 
	 loss: 44.3347, MinusLogProbMetric: 44.3347, val_loss: 45.1774, val_MinusLogProbMetric: 45.1774

Epoch 300: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.3347 - MinusLogProbMetric: 44.3347 - val_loss: 45.1774 - val_MinusLogProbMetric: 45.1774 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 301/1000
2023-10-27 07:21:55.922 
Epoch 301/1000 
	 loss: 43.9620, MinusLogProbMetric: 43.9620, val_loss: 45.0819, val_MinusLogProbMetric: 45.0819

Epoch 301: val_loss did not improve from 43.79919
196/196 - 33s - loss: 43.9620 - MinusLogProbMetric: 43.9620 - val_loss: 45.0819 - val_MinusLogProbMetric: 45.0819 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 302/1000
2023-10-27 07:22:28.644 
Epoch 302/1000 
	 loss: 44.0475, MinusLogProbMetric: 44.0475, val_loss: 43.9359, val_MinusLogProbMetric: 43.9359

Epoch 302: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.0475 - MinusLogProbMetric: 44.0475 - val_loss: 43.9359 - val_MinusLogProbMetric: 43.9359 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 303/1000
2023-10-27 07:23:02.301 
Epoch 303/1000 
	 loss: 43.8600, MinusLogProbMetric: 43.8600, val_loss: 44.8290, val_MinusLogProbMetric: 44.8290

Epoch 303: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.8600 - MinusLogProbMetric: 43.8600 - val_loss: 44.8290 - val_MinusLogProbMetric: 44.8290 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 304/1000
2023-10-27 07:23:36.304 
Epoch 304/1000 
	 loss: 44.0683, MinusLogProbMetric: 44.0683, val_loss: 45.4153, val_MinusLogProbMetric: 45.4153

Epoch 304: val_loss did not improve from 43.79919
196/196 - 34s - loss: 44.0683 - MinusLogProbMetric: 44.0683 - val_loss: 45.4153 - val_MinusLogProbMetric: 45.4153 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 305/1000
2023-10-27 07:24:10.141 
Epoch 305/1000 
	 loss: 43.7871, MinusLogProbMetric: 43.7871, val_loss: 44.3951, val_MinusLogProbMetric: 44.3951

Epoch 305: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.7871 - MinusLogProbMetric: 43.7871 - val_loss: 44.3951 - val_MinusLogProbMetric: 44.3951 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 306/1000
2023-10-27 07:24:43.395 
Epoch 306/1000 
	 loss: 43.9547, MinusLogProbMetric: 43.9547, val_loss: 44.2046, val_MinusLogProbMetric: 44.2046

Epoch 306: val_loss did not improve from 43.79919
196/196 - 33s - loss: 43.9547 - MinusLogProbMetric: 43.9547 - val_loss: 44.2046 - val_MinusLogProbMetric: 44.2046 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 307/1000
2023-10-27 07:25:16.948 
Epoch 307/1000 
	 loss: 43.7083, MinusLogProbMetric: 43.7083, val_loss: 44.0668, val_MinusLogProbMetric: 44.0668

Epoch 307: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.7083 - MinusLogProbMetric: 43.7083 - val_loss: 44.0668 - val_MinusLogProbMetric: 44.0668 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 308/1000
2023-10-27 07:25:50.613 
Epoch 308/1000 
	 loss: 43.5398, MinusLogProbMetric: 43.5398, val_loss: 43.9892, val_MinusLogProbMetric: 43.9892

Epoch 308: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.5398 - MinusLogProbMetric: 43.5398 - val_loss: 43.9892 - val_MinusLogProbMetric: 43.9892 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 309/1000
2023-10-27 07:26:24.323 
Epoch 309/1000 
	 loss: 44.0177, MinusLogProbMetric: 44.0177, val_loss: 45.7859, val_MinusLogProbMetric: 45.7859

Epoch 309: val_loss did not improve from 43.79919
196/196 - 34s - loss: 44.0177 - MinusLogProbMetric: 44.0177 - val_loss: 45.7859 - val_MinusLogProbMetric: 45.7859 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 310/1000
2023-10-27 07:26:57.488 
Epoch 310/1000 
	 loss: 44.0566, MinusLogProbMetric: 44.0566, val_loss: 44.5041, val_MinusLogProbMetric: 44.5041

Epoch 310: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.0566 - MinusLogProbMetric: 44.0566 - val_loss: 44.5041 - val_MinusLogProbMetric: 44.5041 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 311/1000
2023-10-27 07:27:30.928 
Epoch 311/1000 
	 loss: 44.1892, MinusLogProbMetric: 44.1892, val_loss: 47.3673, val_MinusLogProbMetric: 47.3673

Epoch 311: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.1892 - MinusLogProbMetric: 44.1892 - val_loss: 47.3673 - val_MinusLogProbMetric: 47.3673 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 312/1000
2023-10-27 07:28:04.577 
Epoch 312/1000 
	 loss: 44.0324, MinusLogProbMetric: 44.0324, val_loss: 44.1271, val_MinusLogProbMetric: 44.1271

Epoch 312: val_loss did not improve from 43.79919
196/196 - 34s - loss: 44.0324 - MinusLogProbMetric: 44.0324 - val_loss: 44.1271 - val_MinusLogProbMetric: 44.1271 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 313/1000
2023-10-27 07:28:38.428 
Epoch 313/1000 
	 loss: 43.7753, MinusLogProbMetric: 43.7753, val_loss: 43.9233, val_MinusLogProbMetric: 43.9233

Epoch 313: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.7753 - MinusLogProbMetric: 43.7753 - val_loss: 43.9233 - val_MinusLogProbMetric: 43.9233 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 314/1000
2023-10-27 07:29:11.272 
Epoch 314/1000 
	 loss: 43.8341, MinusLogProbMetric: 43.8341, val_loss: 46.6979, val_MinusLogProbMetric: 46.6979

Epoch 314: val_loss did not improve from 43.79919
196/196 - 33s - loss: 43.8341 - MinusLogProbMetric: 43.8341 - val_loss: 46.6979 - val_MinusLogProbMetric: 46.6979 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 315/1000
2023-10-27 07:29:45.203 
Epoch 315/1000 
	 loss: 43.8679, MinusLogProbMetric: 43.8679, val_loss: 44.6491, val_MinusLogProbMetric: 44.6491

Epoch 315: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.8679 - MinusLogProbMetric: 43.8679 - val_loss: 44.6491 - val_MinusLogProbMetric: 44.6491 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 316/1000
2023-10-27 07:30:18.744 
Epoch 316/1000 
	 loss: 43.7398, MinusLogProbMetric: 43.7398, val_loss: 44.3239, val_MinusLogProbMetric: 44.3239

Epoch 316: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.7398 - MinusLogProbMetric: 43.7398 - val_loss: 44.3239 - val_MinusLogProbMetric: 44.3239 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 317/1000
2023-10-27 07:30:52.577 
Epoch 317/1000 
	 loss: 43.7839, MinusLogProbMetric: 43.7839, val_loss: 44.4006, val_MinusLogProbMetric: 44.4006

Epoch 317: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.7839 - MinusLogProbMetric: 43.7839 - val_loss: 44.4006 - val_MinusLogProbMetric: 44.4006 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 318/1000
2023-10-27 07:31:26.327 
Epoch 318/1000 
	 loss: 43.7909, MinusLogProbMetric: 43.7909, val_loss: 45.7671, val_MinusLogProbMetric: 45.7671

Epoch 318: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.7909 - MinusLogProbMetric: 43.7909 - val_loss: 45.7671 - val_MinusLogProbMetric: 45.7671 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 319/1000
2023-10-27 07:31:59.190 
Epoch 319/1000 
	 loss: 44.1250, MinusLogProbMetric: 44.1250, val_loss: 44.3412, val_MinusLogProbMetric: 44.3412

Epoch 319: val_loss did not improve from 43.79919
196/196 - 33s - loss: 44.1250 - MinusLogProbMetric: 44.1250 - val_loss: 44.3412 - val_MinusLogProbMetric: 44.3412 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 320/1000
2023-10-27 07:32:33.183 
Epoch 320/1000 
	 loss: 43.5655, MinusLogProbMetric: 43.5655, val_loss: 45.2084, val_MinusLogProbMetric: 45.2084

Epoch 320: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.5655 - MinusLogProbMetric: 43.5655 - val_loss: 45.2084 - val_MinusLogProbMetric: 45.2084 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 321/1000
2023-10-27 07:33:06.783 
Epoch 321/1000 
	 loss: 43.9283, MinusLogProbMetric: 43.9283, val_loss: 44.3666, val_MinusLogProbMetric: 44.3666

Epoch 321: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.9283 - MinusLogProbMetric: 43.9283 - val_loss: 44.3666 - val_MinusLogProbMetric: 44.3666 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 322/1000
2023-10-27 07:33:40.348 
Epoch 322/1000 
	 loss: 43.5934, MinusLogProbMetric: 43.5934, val_loss: 44.4646, val_MinusLogProbMetric: 44.4646

Epoch 322: val_loss did not improve from 43.79919
196/196 - 34s - loss: 43.5934 - MinusLogProbMetric: 43.5934 - val_loss: 44.4646 - val_MinusLogProbMetric: 44.4646 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 323/1000
2023-10-27 07:34:13.139 
Epoch 323/1000 
	 loss: 43.6216, MinusLogProbMetric: 43.6216, val_loss: 43.8389, val_MinusLogProbMetric: 43.8389

Epoch 323: val_loss did not improve from 43.79919
196/196 - 33s - loss: 43.6216 - MinusLogProbMetric: 43.6216 - val_loss: 43.8389 - val_MinusLogProbMetric: 43.8389 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 324/1000
2023-10-27 07:34:46.725 
Epoch 324/1000 
	 loss: 43.7925, MinusLogProbMetric: 43.7925, val_loss: 43.7967, val_MinusLogProbMetric: 43.7967

Epoch 324: val_loss improved from 43.79919 to 43.79673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 43.7925 - MinusLogProbMetric: 43.7925 - val_loss: 43.7967 - val_MinusLogProbMetric: 43.7967 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 325/1000
2023-10-27 07:35:20.831 
Epoch 325/1000 
	 loss: 43.3816, MinusLogProbMetric: 43.3816, val_loss: 43.6116, val_MinusLogProbMetric: 43.6116

Epoch 325: val_loss improved from 43.79673 to 43.61160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 43.3816 - MinusLogProbMetric: 43.3816 - val_loss: 43.6116 - val_MinusLogProbMetric: 43.6116 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 326/1000
2023-10-27 07:35:55.227 
Epoch 326/1000 
	 loss: 43.6934, MinusLogProbMetric: 43.6934, val_loss: 44.0982, val_MinusLogProbMetric: 44.0982

Epoch 326: val_loss did not improve from 43.61160
196/196 - 34s - loss: 43.6934 - MinusLogProbMetric: 43.6934 - val_loss: 44.0982 - val_MinusLogProbMetric: 44.0982 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 327/1000
2023-10-27 07:36:28.812 
Epoch 327/1000 
	 loss: 44.1107, MinusLogProbMetric: 44.1107, val_loss: 45.7902, val_MinusLogProbMetric: 45.7902

Epoch 327: val_loss did not improve from 43.61160
196/196 - 34s - loss: 44.1107 - MinusLogProbMetric: 44.1107 - val_loss: 45.7902 - val_MinusLogProbMetric: 45.7902 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 328/1000
2023-10-27 07:37:02.733 
Epoch 328/1000 
	 loss: 44.0908, MinusLogProbMetric: 44.0908, val_loss: 44.2643, val_MinusLogProbMetric: 44.2643

Epoch 328: val_loss did not improve from 43.61160
196/196 - 34s - loss: 44.0908 - MinusLogProbMetric: 44.0908 - val_loss: 44.2643 - val_MinusLogProbMetric: 44.2643 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 329/1000
2023-10-27 07:37:36.028 
Epoch 329/1000 
	 loss: 43.4760, MinusLogProbMetric: 43.4760, val_loss: 44.1525, val_MinusLogProbMetric: 44.1525

Epoch 329: val_loss did not improve from 43.61160
196/196 - 33s - loss: 43.4760 - MinusLogProbMetric: 43.4760 - val_loss: 44.1525 - val_MinusLogProbMetric: 44.1525 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 330/1000
2023-10-27 07:38:09.417 
Epoch 330/1000 
	 loss: 43.5134, MinusLogProbMetric: 43.5134, val_loss: 43.7250, val_MinusLogProbMetric: 43.7250

Epoch 330: val_loss did not improve from 43.61160
196/196 - 33s - loss: 43.5134 - MinusLogProbMetric: 43.5134 - val_loss: 43.7250 - val_MinusLogProbMetric: 43.7250 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 331/1000
2023-10-27 07:38:40.346 
Epoch 331/1000 
	 loss: 43.2738, MinusLogProbMetric: 43.2738, val_loss: 45.2941, val_MinusLogProbMetric: 45.2941

Epoch 331: val_loss did not improve from 43.61160
196/196 - 31s - loss: 43.2738 - MinusLogProbMetric: 43.2738 - val_loss: 45.2941 - val_MinusLogProbMetric: 45.2941 - lr: 1.1111e-04 - 31s/epoch - 158ms/step
Epoch 332/1000
2023-10-27 07:39:09.681 
Epoch 332/1000 
	 loss: 43.6660, MinusLogProbMetric: 43.6660, val_loss: 44.0915, val_MinusLogProbMetric: 44.0915

Epoch 332: val_loss did not improve from 43.61160
196/196 - 29s - loss: 43.6660 - MinusLogProbMetric: 43.6660 - val_loss: 44.0915 - val_MinusLogProbMetric: 44.0915 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 333/1000
2023-10-27 07:39:39.164 
Epoch 333/1000 
	 loss: 43.7704, MinusLogProbMetric: 43.7704, val_loss: 43.8494, val_MinusLogProbMetric: 43.8494

Epoch 333: val_loss did not improve from 43.61160
196/196 - 29s - loss: 43.7704 - MinusLogProbMetric: 43.7704 - val_loss: 43.8494 - val_MinusLogProbMetric: 43.8494 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 334/1000
2023-10-27 07:40:08.625 
Epoch 334/1000 
	 loss: 43.6082, MinusLogProbMetric: 43.6082, val_loss: 44.8115, val_MinusLogProbMetric: 44.8115

Epoch 334: val_loss did not improve from 43.61160
196/196 - 29s - loss: 43.6082 - MinusLogProbMetric: 43.6082 - val_loss: 44.8115 - val_MinusLogProbMetric: 44.8115 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 335/1000
2023-10-27 07:40:41.699 
Epoch 335/1000 
	 loss: 43.4822, MinusLogProbMetric: 43.4822, val_loss: 44.4541, val_MinusLogProbMetric: 44.4541

Epoch 335: val_loss did not improve from 43.61160
196/196 - 33s - loss: 43.4822 - MinusLogProbMetric: 43.4822 - val_loss: 44.4541 - val_MinusLogProbMetric: 44.4541 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 336/1000
2023-10-27 07:41:15.735 
Epoch 336/1000 
	 loss: 43.6320, MinusLogProbMetric: 43.6320, val_loss: 44.3718, val_MinusLogProbMetric: 44.3718

Epoch 336: val_loss did not improve from 43.61160
196/196 - 34s - loss: 43.6320 - MinusLogProbMetric: 43.6320 - val_loss: 44.3718 - val_MinusLogProbMetric: 44.3718 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 337/1000
2023-10-27 07:41:49.414 
Epoch 337/1000 
	 loss: 43.3507, MinusLogProbMetric: 43.3507, val_loss: 43.7804, val_MinusLogProbMetric: 43.7804

Epoch 337: val_loss did not improve from 43.61160
196/196 - 34s - loss: 43.3507 - MinusLogProbMetric: 43.3507 - val_loss: 43.7804 - val_MinusLogProbMetric: 43.7804 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 338/1000
2023-10-27 07:42:22.296 
Epoch 338/1000 
	 loss: 43.8146, MinusLogProbMetric: 43.8146, val_loss: 44.0511, val_MinusLogProbMetric: 44.0511

Epoch 338: val_loss did not improve from 43.61160
196/196 - 33s - loss: 43.8146 - MinusLogProbMetric: 43.8146 - val_loss: 44.0511 - val_MinusLogProbMetric: 44.0511 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 339/1000
2023-10-27 07:42:55.730 
Epoch 339/1000 
	 loss: 43.2059, MinusLogProbMetric: 43.2059, val_loss: 43.6701, val_MinusLogProbMetric: 43.6701

Epoch 339: val_loss did not improve from 43.61160
196/196 - 33s - loss: 43.2059 - MinusLogProbMetric: 43.2059 - val_loss: 43.6701 - val_MinusLogProbMetric: 43.6701 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 340/1000
2023-10-27 07:43:29.586 
Epoch 340/1000 
	 loss: 43.6298, MinusLogProbMetric: 43.6298, val_loss: 44.7690, val_MinusLogProbMetric: 44.7690

Epoch 340: val_loss did not improve from 43.61160
196/196 - 34s - loss: 43.6298 - MinusLogProbMetric: 43.6298 - val_loss: 44.7690 - val_MinusLogProbMetric: 44.7690 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 341/1000
2023-10-27 07:44:03.359 
Epoch 341/1000 
	 loss: 43.6394, MinusLogProbMetric: 43.6394, val_loss: 45.2926, val_MinusLogProbMetric: 45.2926

Epoch 341: val_loss did not improve from 43.61160
196/196 - 34s - loss: 43.6394 - MinusLogProbMetric: 43.6394 - val_loss: 45.2926 - val_MinusLogProbMetric: 45.2926 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 342/1000
2023-10-27 07:44:37.245 
Epoch 342/1000 
	 loss: 43.5760, MinusLogProbMetric: 43.5760, val_loss: 44.7572, val_MinusLogProbMetric: 44.7572

Epoch 342: val_loss did not improve from 43.61160
196/196 - 34s - loss: 43.5760 - MinusLogProbMetric: 43.5760 - val_loss: 44.7572 - val_MinusLogProbMetric: 44.7572 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 343/1000
2023-10-27 07:45:11.118 
Epoch 343/1000 
	 loss: 44.3495, MinusLogProbMetric: 44.3495, val_loss: 43.5515, val_MinusLogProbMetric: 43.5515

Epoch 343: val_loss improved from 43.61160 to 43.55147, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 44.3495 - MinusLogProbMetric: 44.3495 - val_loss: 43.5515 - val_MinusLogProbMetric: 43.5515 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 344/1000
2023-10-27 07:45:45.721 
Epoch 344/1000 
	 loss: 43.3916, MinusLogProbMetric: 43.3916, val_loss: 43.7159, val_MinusLogProbMetric: 43.7159

Epoch 344: val_loss did not improve from 43.55147
196/196 - 34s - loss: 43.3916 - MinusLogProbMetric: 43.3916 - val_loss: 43.7159 - val_MinusLogProbMetric: 43.7159 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 345/1000
2023-10-27 07:46:19.196 
Epoch 345/1000 
	 loss: 43.5051, MinusLogProbMetric: 43.5051, val_loss: 45.1899, val_MinusLogProbMetric: 45.1899

Epoch 345: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.5051 - MinusLogProbMetric: 43.5051 - val_loss: 45.1899 - val_MinusLogProbMetric: 45.1899 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 346/1000
2023-10-27 07:46:52.459 
Epoch 346/1000 
	 loss: 43.2568, MinusLogProbMetric: 43.2568, val_loss: 45.3695, val_MinusLogProbMetric: 45.3695

Epoch 346: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.2568 - MinusLogProbMetric: 43.2568 - val_loss: 45.3695 - val_MinusLogProbMetric: 45.3695 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 347/1000
2023-10-27 07:47:25.786 
Epoch 347/1000 
	 loss: 43.5793, MinusLogProbMetric: 43.5793, val_loss: 44.8314, val_MinusLogProbMetric: 44.8314

Epoch 347: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.5793 - MinusLogProbMetric: 43.5793 - val_loss: 44.8314 - val_MinusLogProbMetric: 44.8314 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 348/1000
2023-10-27 07:47:59.673 
Epoch 348/1000 
	 loss: 43.7562, MinusLogProbMetric: 43.7562, val_loss: 44.0413, val_MinusLogProbMetric: 44.0413

Epoch 348: val_loss did not improve from 43.55147
196/196 - 34s - loss: 43.7562 - MinusLogProbMetric: 43.7562 - val_loss: 44.0413 - val_MinusLogProbMetric: 44.0413 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 349/1000
2023-10-27 07:48:33.117 
Epoch 349/1000 
	 loss: 43.1970, MinusLogProbMetric: 43.1970, val_loss: 44.4664, val_MinusLogProbMetric: 44.4664

Epoch 349: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.1970 - MinusLogProbMetric: 43.1970 - val_loss: 44.4664 - val_MinusLogProbMetric: 44.4664 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 350/1000
2023-10-27 07:49:06.586 
Epoch 350/1000 
	 loss: 43.4657, MinusLogProbMetric: 43.4657, val_loss: 44.8979, val_MinusLogProbMetric: 44.8979

Epoch 350: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.4657 - MinusLogProbMetric: 43.4657 - val_loss: 44.8979 - val_MinusLogProbMetric: 44.8979 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 351/1000
2023-10-27 07:49:40.326 
Epoch 351/1000 
	 loss: 43.2080, MinusLogProbMetric: 43.2080, val_loss: 45.4497, val_MinusLogProbMetric: 45.4497

Epoch 351: val_loss did not improve from 43.55147
196/196 - 34s - loss: 43.2080 - MinusLogProbMetric: 43.2080 - val_loss: 45.4497 - val_MinusLogProbMetric: 45.4497 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 352/1000
2023-10-27 07:50:13.586 
Epoch 352/1000 
	 loss: 43.5430, MinusLogProbMetric: 43.5430, val_loss: 43.8807, val_MinusLogProbMetric: 43.8807

Epoch 352: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.5430 - MinusLogProbMetric: 43.5430 - val_loss: 43.8807 - val_MinusLogProbMetric: 43.8807 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 353/1000
2023-10-27 07:50:46.310 
Epoch 353/1000 
	 loss: 43.6602, MinusLogProbMetric: 43.6602, val_loss: 45.0093, val_MinusLogProbMetric: 45.0093

Epoch 353: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.6602 - MinusLogProbMetric: 43.6602 - val_loss: 45.0093 - val_MinusLogProbMetric: 45.0093 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 354/1000
2023-10-27 07:51:19.807 
Epoch 354/1000 
	 loss: 43.2572, MinusLogProbMetric: 43.2572, val_loss: 44.4782, val_MinusLogProbMetric: 44.4782

Epoch 354: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.2572 - MinusLogProbMetric: 43.2572 - val_loss: 44.4782 - val_MinusLogProbMetric: 44.4782 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 355/1000
2023-10-27 07:51:53.527 
Epoch 355/1000 
	 loss: 43.5288, MinusLogProbMetric: 43.5288, val_loss: 46.3122, val_MinusLogProbMetric: 46.3122

Epoch 355: val_loss did not improve from 43.55147
196/196 - 34s - loss: 43.5288 - MinusLogProbMetric: 43.5288 - val_loss: 46.3122 - val_MinusLogProbMetric: 46.3122 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 356/1000
2023-10-27 07:52:26.834 
Epoch 356/1000 
	 loss: 43.7112, MinusLogProbMetric: 43.7112, val_loss: 46.2941, val_MinusLogProbMetric: 46.2941

Epoch 356: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.7112 - MinusLogProbMetric: 43.7112 - val_loss: 46.2941 - val_MinusLogProbMetric: 46.2941 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 357/1000
2023-10-27 07:53:00.295 
Epoch 357/1000 
	 loss: 43.2325, MinusLogProbMetric: 43.2325, val_loss: 43.9991, val_MinusLogProbMetric: 43.9991

Epoch 357: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.2325 - MinusLogProbMetric: 43.2325 - val_loss: 43.9991 - val_MinusLogProbMetric: 43.9991 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 358/1000
2023-10-27 07:53:34.147 
Epoch 358/1000 
	 loss: 43.4076, MinusLogProbMetric: 43.4076, val_loss: 43.7251, val_MinusLogProbMetric: 43.7251

Epoch 358: val_loss did not improve from 43.55147
196/196 - 34s - loss: 43.4076 - MinusLogProbMetric: 43.4076 - val_loss: 43.7251 - val_MinusLogProbMetric: 43.7251 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 359/1000
2023-10-27 07:54:07.570 
Epoch 359/1000 
	 loss: 43.4468, MinusLogProbMetric: 43.4468, val_loss: 45.0329, val_MinusLogProbMetric: 45.0329

Epoch 359: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.4468 - MinusLogProbMetric: 43.4468 - val_loss: 45.0329 - val_MinusLogProbMetric: 45.0329 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 360/1000
2023-10-27 07:54:40.761 
Epoch 360/1000 
	 loss: 47.0855, MinusLogProbMetric: 47.0855, val_loss: 45.6248, val_MinusLogProbMetric: 45.6248

Epoch 360: val_loss did not improve from 43.55147
196/196 - 33s - loss: 47.0855 - MinusLogProbMetric: 47.0855 - val_loss: 45.6248 - val_MinusLogProbMetric: 45.6248 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 361/1000
2023-10-27 07:55:13.275 
Epoch 361/1000 
	 loss: 43.7877, MinusLogProbMetric: 43.7877, val_loss: 44.4470, val_MinusLogProbMetric: 44.4470

Epoch 361: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.7877 - MinusLogProbMetric: 43.7877 - val_loss: 44.4470 - val_MinusLogProbMetric: 44.4470 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 362/1000
2023-10-27 07:55:46.587 
Epoch 362/1000 
	 loss: 43.7336, MinusLogProbMetric: 43.7336, val_loss: 44.2664, val_MinusLogProbMetric: 44.2664

Epoch 362: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.7336 - MinusLogProbMetric: 43.7336 - val_loss: 44.2664 - val_MinusLogProbMetric: 44.2664 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 363/1000
2023-10-27 07:56:18.236 
Epoch 363/1000 
	 loss: 43.8314, MinusLogProbMetric: 43.8314, val_loss: 44.5456, val_MinusLogProbMetric: 44.5456

Epoch 363: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.8314 - MinusLogProbMetric: 43.8314 - val_loss: 44.5456 - val_MinusLogProbMetric: 44.5456 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 364/1000
2023-10-27 07:56:48.037 
Epoch 364/1000 
	 loss: 43.9672, MinusLogProbMetric: 43.9672, val_loss: 45.6181, val_MinusLogProbMetric: 45.6181

Epoch 364: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.9672 - MinusLogProbMetric: 43.9672 - val_loss: 45.6181 - val_MinusLogProbMetric: 45.6181 - lr: 1.1111e-04 - 30s/epoch - 152ms/step
Epoch 365/1000
2023-10-27 07:57:17.551 
Epoch 365/1000 
	 loss: 43.3378, MinusLogProbMetric: 43.3378, val_loss: 44.1791, val_MinusLogProbMetric: 44.1791

Epoch 365: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.3378 - MinusLogProbMetric: 43.3378 - val_loss: 44.1791 - val_MinusLogProbMetric: 44.1791 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 366/1000
2023-10-27 07:57:48.076 
Epoch 366/1000 
	 loss: 43.7732, MinusLogProbMetric: 43.7732, val_loss: 47.2639, val_MinusLogProbMetric: 47.2639

Epoch 366: val_loss did not improve from 43.55147
196/196 - 31s - loss: 43.7732 - MinusLogProbMetric: 43.7732 - val_loss: 47.2639 - val_MinusLogProbMetric: 47.2639 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 367/1000
2023-10-27 07:58:21.698 
Epoch 367/1000 
	 loss: 43.2869, MinusLogProbMetric: 43.2869, val_loss: 45.4182, val_MinusLogProbMetric: 45.4182

Epoch 367: val_loss did not improve from 43.55147
196/196 - 34s - loss: 43.2869 - MinusLogProbMetric: 43.2869 - val_loss: 45.4182 - val_MinusLogProbMetric: 45.4182 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 368/1000
2023-10-27 07:58:53.438 
Epoch 368/1000 
	 loss: 43.6906, MinusLogProbMetric: 43.6906, val_loss: 44.7507, val_MinusLogProbMetric: 44.7507

Epoch 368: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.6906 - MinusLogProbMetric: 43.6906 - val_loss: 44.7507 - val_MinusLogProbMetric: 44.7507 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 369/1000
2023-10-27 07:59:22.826 
Epoch 369/1000 
	 loss: 43.4107, MinusLogProbMetric: 43.4107, val_loss: 45.9263, val_MinusLogProbMetric: 45.9263

Epoch 369: val_loss did not improve from 43.55147
196/196 - 29s - loss: 43.4107 - MinusLogProbMetric: 43.4107 - val_loss: 45.9263 - val_MinusLogProbMetric: 45.9263 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 370/1000
2023-10-27 07:59:53.426 
Epoch 370/1000 
	 loss: 43.5204, MinusLogProbMetric: 43.5204, val_loss: 44.4934, val_MinusLogProbMetric: 44.4934

Epoch 370: val_loss did not improve from 43.55147
196/196 - 31s - loss: 43.5204 - MinusLogProbMetric: 43.5204 - val_loss: 44.4934 - val_MinusLogProbMetric: 44.4934 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 371/1000
2023-10-27 08:00:25.069 
Epoch 371/1000 
	 loss: 43.2693, MinusLogProbMetric: 43.2693, val_loss: 44.0048, val_MinusLogProbMetric: 44.0048

Epoch 371: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.2693 - MinusLogProbMetric: 43.2693 - val_loss: 44.0048 - val_MinusLogProbMetric: 44.0048 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 372/1000
2023-10-27 08:00:56.620 
Epoch 372/1000 
	 loss: 43.5272, MinusLogProbMetric: 43.5272, val_loss: 44.5842, val_MinusLogProbMetric: 44.5842

Epoch 372: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.5272 - MinusLogProbMetric: 43.5272 - val_loss: 44.5842 - val_MinusLogProbMetric: 44.5842 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 373/1000
2023-10-27 08:01:25.889 
Epoch 373/1000 
	 loss: 43.2735, MinusLogProbMetric: 43.2735, val_loss: 45.3311, val_MinusLogProbMetric: 45.3311

Epoch 373: val_loss did not improve from 43.55147
196/196 - 29s - loss: 43.2735 - MinusLogProbMetric: 43.2735 - val_loss: 45.3311 - val_MinusLogProbMetric: 45.3311 - lr: 1.1111e-04 - 29s/epoch - 149ms/step
Epoch 374/1000
2023-10-27 08:01:55.131 
Epoch 374/1000 
	 loss: 43.5185, MinusLogProbMetric: 43.5185, val_loss: 44.4871, val_MinusLogProbMetric: 44.4871

Epoch 374: val_loss did not improve from 43.55147
196/196 - 29s - loss: 43.5185 - MinusLogProbMetric: 43.5185 - val_loss: 44.4871 - val_MinusLogProbMetric: 44.4871 - lr: 1.1111e-04 - 29s/epoch - 149ms/step
Epoch 375/1000
2023-10-27 08:02:25.267 
Epoch 375/1000 
	 loss: 43.3118, MinusLogProbMetric: 43.3118, val_loss: 44.7211, val_MinusLogProbMetric: 44.7211

Epoch 375: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.3118 - MinusLogProbMetric: 43.3118 - val_loss: 44.7211 - val_MinusLogProbMetric: 44.7211 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 376/1000
2023-10-27 08:02:58.415 
Epoch 376/1000 
	 loss: 43.2483, MinusLogProbMetric: 43.2483, val_loss: 44.1456, val_MinusLogProbMetric: 44.1456

Epoch 376: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.2483 - MinusLogProbMetric: 43.2483 - val_loss: 44.1456 - val_MinusLogProbMetric: 44.1456 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 377/1000
2023-10-27 08:03:30.337 
Epoch 377/1000 
	 loss: 43.6727, MinusLogProbMetric: 43.6727, val_loss: 44.6232, val_MinusLogProbMetric: 44.6232

Epoch 377: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.6727 - MinusLogProbMetric: 43.6727 - val_loss: 44.6232 - val_MinusLogProbMetric: 44.6232 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 378/1000
2023-10-27 08:04:00.647 
Epoch 378/1000 
	 loss: 43.3427, MinusLogProbMetric: 43.3427, val_loss: 48.1389, val_MinusLogProbMetric: 48.1389

Epoch 378: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.3427 - MinusLogProbMetric: 43.3427 - val_loss: 48.1389 - val_MinusLogProbMetric: 48.1389 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 379/1000
2023-10-27 08:04:30.670 
Epoch 379/1000 
	 loss: 43.3410, MinusLogProbMetric: 43.3410, val_loss: 44.2604, val_MinusLogProbMetric: 44.2604

Epoch 379: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.3410 - MinusLogProbMetric: 43.3410 - val_loss: 44.2604 - val_MinusLogProbMetric: 44.2604 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 380/1000
2023-10-27 08:05:00.180 
Epoch 380/1000 
	 loss: 43.3590, MinusLogProbMetric: 43.3590, val_loss: 43.7098, val_MinusLogProbMetric: 43.7098

Epoch 380: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.3590 - MinusLogProbMetric: 43.3590 - val_loss: 43.7098 - val_MinusLogProbMetric: 43.7098 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 381/1000
2023-10-27 08:05:31.867 
Epoch 381/1000 
	 loss: 43.0686, MinusLogProbMetric: 43.0686, val_loss: 43.5633, val_MinusLogProbMetric: 43.5633

Epoch 381: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.0686 - MinusLogProbMetric: 43.0686 - val_loss: 43.5633 - val_MinusLogProbMetric: 43.5633 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 382/1000
2023-10-27 08:06:04.397 
Epoch 382/1000 
	 loss: 43.2738, MinusLogProbMetric: 43.2738, val_loss: 44.7448, val_MinusLogProbMetric: 44.7448

Epoch 382: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.2738 - MinusLogProbMetric: 43.2738 - val_loss: 44.7448 - val_MinusLogProbMetric: 44.7448 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 383/1000
2023-10-27 08:06:34.320 
Epoch 383/1000 
	 loss: 43.2865, MinusLogProbMetric: 43.2865, val_loss: 44.2023, val_MinusLogProbMetric: 44.2023

Epoch 383: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.2865 - MinusLogProbMetric: 43.2865 - val_loss: 44.2023 - val_MinusLogProbMetric: 44.2023 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 384/1000
2023-10-27 08:07:03.622 
Epoch 384/1000 
	 loss: 43.0918, MinusLogProbMetric: 43.0918, val_loss: 45.3993, val_MinusLogProbMetric: 45.3993

Epoch 384: val_loss did not improve from 43.55147
196/196 - 29s - loss: 43.0918 - MinusLogProbMetric: 43.0918 - val_loss: 45.3993 - val_MinusLogProbMetric: 45.3993 - lr: 1.1111e-04 - 29s/epoch - 149ms/step
Epoch 385/1000
2023-10-27 08:07:33.775 
Epoch 385/1000 
	 loss: 43.6306, MinusLogProbMetric: 43.6306, val_loss: 44.0698, val_MinusLogProbMetric: 44.0698

Epoch 385: val_loss did not improve from 43.55147
196/196 - 30s - loss: 43.6306 - MinusLogProbMetric: 43.6306 - val_loss: 44.0698 - val_MinusLogProbMetric: 44.0698 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 386/1000
2023-10-27 08:08:06.906 
Epoch 386/1000 
	 loss: 43.2368, MinusLogProbMetric: 43.2368, val_loss: 45.9111, val_MinusLogProbMetric: 45.9111

Epoch 386: val_loss did not improve from 43.55147
196/196 - 33s - loss: 43.2368 - MinusLogProbMetric: 43.2368 - val_loss: 45.9111 - val_MinusLogProbMetric: 45.9111 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 387/1000
2023-10-27 08:08:37.997 
Epoch 387/1000 
	 loss: 43.2843, MinusLogProbMetric: 43.2843, val_loss: 44.9338, val_MinusLogProbMetric: 44.9338

Epoch 387: val_loss did not improve from 43.55147
196/196 - 31s - loss: 43.2843 - MinusLogProbMetric: 43.2843 - val_loss: 44.9338 - val_MinusLogProbMetric: 44.9338 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 388/1000
2023-10-27 08:09:10.376 
Epoch 388/1000 
	 loss: 43.1343, MinusLogProbMetric: 43.1343, val_loss: 43.8428, val_MinusLogProbMetric: 43.8428

Epoch 388: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.1343 - MinusLogProbMetric: 43.1343 - val_loss: 43.8428 - val_MinusLogProbMetric: 43.8428 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 389/1000
2023-10-27 08:09:41.716 
Epoch 389/1000 
	 loss: 43.6343, MinusLogProbMetric: 43.6343, val_loss: 44.6631, val_MinusLogProbMetric: 44.6631

Epoch 389: val_loss did not improve from 43.55147
196/196 - 31s - loss: 43.6343 - MinusLogProbMetric: 43.6343 - val_loss: 44.6631 - val_MinusLogProbMetric: 44.6631 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 390/1000
2023-10-27 08:10:10.559 
Epoch 390/1000 
	 loss: 43.5935, MinusLogProbMetric: 43.5935, val_loss: 43.8506, val_MinusLogProbMetric: 43.8506

Epoch 390: val_loss did not improve from 43.55147
196/196 - 29s - loss: 43.5935 - MinusLogProbMetric: 43.5935 - val_loss: 43.8506 - val_MinusLogProbMetric: 43.8506 - lr: 1.1111e-04 - 29s/epoch - 147ms/step
Epoch 391/1000
2023-10-27 08:10:42.362 
Epoch 391/1000 
	 loss: 43.2623, MinusLogProbMetric: 43.2623, val_loss: 44.4644, val_MinusLogProbMetric: 44.4644

Epoch 391: val_loss did not improve from 43.55147
196/196 - 32s - loss: 43.2623 - MinusLogProbMetric: 43.2623 - val_loss: 44.4644 - val_MinusLogProbMetric: 44.4644 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 392/1000
2023-10-27 08:11:15.381 
Epoch 392/1000 
	 loss: 43.4140, MinusLogProbMetric: 43.4140, val_loss: 43.5300, val_MinusLogProbMetric: 43.5300

Epoch 392: val_loss improved from 43.55147 to 43.53002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 43.4140 - MinusLogProbMetric: 43.4140 - val_loss: 43.5300 - val_MinusLogProbMetric: 43.5300 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 393/1000
2023-10-27 08:11:45.324 
Epoch 393/1000 
	 loss: 43.1322, MinusLogProbMetric: 43.1322, val_loss: 46.0723, val_MinusLogProbMetric: 46.0723

Epoch 393: val_loss did not improve from 43.53002
196/196 - 29s - loss: 43.1322 - MinusLogProbMetric: 43.1322 - val_loss: 46.0723 - val_MinusLogProbMetric: 46.0723 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 394/1000
2023-10-27 08:12:16.121 
Epoch 394/1000 
	 loss: 43.5717, MinusLogProbMetric: 43.5717, val_loss: 44.0454, val_MinusLogProbMetric: 44.0454

Epoch 394: val_loss did not improve from 43.53002
196/196 - 31s - loss: 43.5717 - MinusLogProbMetric: 43.5717 - val_loss: 44.0454 - val_MinusLogProbMetric: 44.0454 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 395/1000
2023-10-27 08:12:46.933 
Epoch 395/1000 
	 loss: 43.2243, MinusLogProbMetric: 43.2243, val_loss: 43.5102, val_MinusLogProbMetric: 43.5102

Epoch 395: val_loss improved from 43.53002 to 43.51018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 43.2243 - MinusLogProbMetric: 43.2243 - val_loss: 43.5102 - val_MinusLogProbMetric: 43.5102 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 396/1000
2023-10-27 08:13:20.537 
Epoch 396/1000 
	 loss: 43.0031, MinusLogProbMetric: 43.0031, val_loss: 44.3477, val_MinusLogProbMetric: 44.3477

Epoch 396: val_loss did not improve from 43.51018
196/196 - 33s - loss: 43.0031 - MinusLogProbMetric: 43.0031 - val_loss: 44.3477 - val_MinusLogProbMetric: 44.3477 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 397/1000
2023-10-27 08:13:52.079 
Epoch 397/1000 
	 loss: 43.2040, MinusLogProbMetric: 43.2040, val_loss: 44.2629, val_MinusLogProbMetric: 44.2629

Epoch 397: val_loss did not improve from 43.51018
196/196 - 32s - loss: 43.2040 - MinusLogProbMetric: 43.2040 - val_loss: 44.2629 - val_MinusLogProbMetric: 44.2629 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 398/1000
2023-10-27 08:14:22.914 
Epoch 398/1000 
	 loss: 43.0350, MinusLogProbMetric: 43.0350, val_loss: 44.0926, val_MinusLogProbMetric: 44.0926

Epoch 398: val_loss did not improve from 43.51018
196/196 - 31s - loss: 43.0350 - MinusLogProbMetric: 43.0350 - val_loss: 44.0926 - val_MinusLogProbMetric: 44.0926 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 399/1000
2023-10-27 08:14:52.535 
Epoch 399/1000 
	 loss: 43.2166, MinusLogProbMetric: 43.2166, val_loss: 45.1934, val_MinusLogProbMetric: 45.1934

Epoch 399: val_loss did not improve from 43.51018
196/196 - 30s - loss: 43.2166 - MinusLogProbMetric: 43.2166 - val_loss: 45.1934 - val_MinusLogProbMetric: 45.1934 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 400/1000
2023-10-27 08:15:21.706 
Epoch 400/1000 
	 loss: 43.0246, MinusLogProbMetric: 43.0246, val_loss: 46.2708, val_MinusLogProbMetric: 46.2708

Epoch 400: val_loss did not improve from 43.51018
196/196 - 29s - loss: 43.0246 - MinusLogProbMetric: 43.0246 - val_loss: 46.2708 - val_MinusLogProbMetric: 46.2708 - lr: 1.1111e-04 - 29s/epoch - 149ms/step
Epoch 401/1000
2023-10-27 08:15:52.535 
Epoch 401/1000 
	 loss: 43.6643, MinusLogProbMetric: 43.6643, val_loss: 43.6245, val_MinusLogProbMetric: 43.6245

Epoch 401: val_loss did not improve from 43.51018
196/196 - 31s - loss: 43.6643 - MinusLogProbMetric: 43.6643 - val_loss: 43.6245 - val_MinusLogProbMetric: 43.6245 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 402/1000
2023-10-27 08:16:25.441 
Epoch 402/1000 
	 loss: 42.8510, MinusLogProbMetric: 42.8510, val_loss: 44.7072, val_MinusLogProbMetric: 44.7072

Epoch 402: val_loss did not improve from 43.51018
196/196 - 33s - loss: 42.8510 - MinusLogProbMetric: 42.8510 - val_loss: 44.7072 - val_MinusLogProbMetric: 44.7072 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 403/1000
2023-10-27 08:16:55.821 
Epoch 403/1000 
	 loss: 43.0979, MinusLogProbMetric: 43.0979, val_loss: 43.3227, val_MinusLogProbMetric: 43.3227

Epoch 403: val_loss improved from 43.51018 to 43.32274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 43.0979 - MinusLogProbMetric: 43.0979 - val_loss: 43.3227 - val_MinusLogProbMetric: 43.3227 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 404/1000
2023-10-27 08:17:26.302 
Epoch 404/1000 
	 loss: 42.9849, MinusLogProbMetric: 42.9849, val_loss: 45.4330, val_MinusLogProbMetric: 45.4330

Epoch 404: val_loss did not improve from 43.32274
196/196 - 30s - loss: 42.9849 - MinusLogProbMetric: 42.9849 - val_loss: 45.4330 - val_MinusLogProbMetric: 45.4330 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 405/1000
2023-10-27 08:17:57.854 
Epoch 405/1000 
	 loss: 43.5816, MinusLogProbMetric: 43.5816, val_loss: 44.3115, val_MinusLogProbMetric: 44.3115

Epoch 405: val_loss did not improve from 43.32274
196/196 - 32s - loss: 43.5816 - MinusLogProbMetric: 43.5816 - val_loss: 44.3115 - val_MinusLogProbMetric: 44.3115 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 406/1000
2023-10-27 08:18:27.784 
Epoch 406/1000 
	 loss: 43.1674, MinusLogProbMetric: 43.1674, val_loss: 44.7005, val_MinusLogProbMetric: 44.7005

Epoch 406: val_loss did not improve from 43.32274
196/196 - 30s - loss: 43.1674 - MinusLogProbMetric: 43.1674 - val_loss: 44.7005 - val_MinusLogProbMetric: 44.7005 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 407/1000
2023-10-27 08:19:00.535 
Epoch 407/1000 
	 loss: 42.9320, MinusLogProbMetric: 42.9320, val_loss: 44.4368, val_MinusLogProbMetric: 44.4368

Epoch 407: val_loss did not improve from 43.32274
196/196 - 33s - loss: 42.9320 - MinusLogProbMetric: 42.9320 - val_loss: 44.4368 - val_MinusLogProbMetric: 44.4368 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 408/1000
2023-10-27 08:19:32.770 
Epoch 408/1000 
	 loss: 42.8887, MinusLogProbMetric: 42.8887, val_loss: 43.6319, val_MinusLogProbMetric: 43.6319

Epoch 408: val_loss did not improve from 43.32274
196/196 - 32s - loss: 42.8887 - MinusLogProbMetric: 42.8887 - val_loss: 43.6319 - val_MinusLogProbMetric: 43.6319 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 409/1000
2023-10-27 08:20:04.180 
Epoch 409/1000 
	 loss: 43.2521, MinusLogProbMetric: 43.2521, val_loss: 44.0871, val_MinusLogProbMetric: 44.0871

Epoch 409: val_loss did not improve from 43.32274
196/196 - 31s - loss: 43.2521 - MinusLogProbMetric: 43.2521 - val_loss: 44.0871 - val_MinusLogProbMetric: 44.0871 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 410/1000
2023-10-27 08:20:35.313 
Epoch 410/1000 
	 loss: 42.9944, MinusLogProbMetric: 42.9944, val_loss: 44.2349, val_MinusLogProbMetric: 44.2349

Epoch 410: val_loss did not improve from 43.32274
196/196 - 31s - loss: 42.9944 - MinusLogProbMetric: 42.9944 - val_loss: 44.2349 - val_MinusLogProbMetric: 44.2349 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 411/1000
2023-10-27 08:21:06.397 
Epoch 411/1000 
	 loss: 43.1280, MinusLogProbMetric: 43.1280, val_loss: 46.1809, val_MinusLogProbMetric: 46.1809

Epoch 411: val_loss did not improve from 43.32274
196/196 - 31s - loss: 43.1280 - MinusLogProbMetric: 43.1280 - val_loss: 46.1809 - val_MinusLogProbMetric: 46.1809 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 412/1000
2023-10-27 08:21:38.355 
Epoch 412/1000 
	 loss: 42.8767, MinusLogProbMetric: 42.8767, val_loss: 43.5426, val_MinusLogProbMetric: 43.5426

Epoch 412: val_loss did not improve from 43.32274
196/196 - 32s - loss: 42.8767 - MinusLogProbMetric: 42.8767 - val_loss: 43.5426 - val_MinusLogProbMetric: 43.5426 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 413/1000
2023-10-27 08:22:11.833 
Epoch 413/1000 
	 loss: 42.9331, MinusLogProbMetric: 42.9331, val_loss: 43.7404, val_MinusLogProbMetric: 43.7404

Epoch 413: val_loss did not improve from 43.32274
196/196 - 33s - loss: 42.9331 - MinusLogProbMetric: 42.9331 - val_loss: 43.7404 - val_MinusLogProbMetric: 43.7404 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 414/1000
2023-10-27 08:22:42.930 
Epoch 414/1000 
	 loss: 42.8362, MinusLogProbMetric: 42.8362, val_loss: 45.0956, val_MinusLogProbMetric: 45.0956

Epoch 414: val_loss did not improve from 43.32274
196/196 - 31s - loss: 42.8362 - MinusLogProbMetric: 42.8362 - val_loss: 45.0956 - val_MinusLogProbMetric: 45.0956 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 415/1000
2023-10-27 08:23:15.363 
Epoch 415/1000 
	 loss: 43.3452, MinusLogProbMetric: 43.3452, val_loss: 43.8929, val_MinusLogProbMetric: 43.8929

Epoch 415: val_loss did not improve from 43.32274
196/196 - 32s - loss: 43.3452 - MinusLogProbMetric: 43.3452 - val_loss: 43.8929 - val_MinusLogProbMetric: 43.8929 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 416/1000
2023-10-27 08:23:44.880 
Epoch 416/1000 
	 loss: 42.5557, MinusLogProbMetric: 42.5557, val_loss: 44.1028, val_MinusLogProbMetric: 44.1028

Epoch 416: val_loss did not improve from 43.32274
196/196 - 30s - loss: 42.5557 - MinusLogProbMetric: 42.5557 - val_loss: 44.1028 - val_MinusLogProbMetric: 44.1028 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 417/1000
2023-10-27 08:24:16.822 
Epoch 417/1000 
	 loss: 43.1237, MinusLogProbMetric: 43.1237, val_loss: 45.2136, val_MinusLogProbMetric: 45.2136

Epoch 417: val_loss did not improve from 43.32274
196/196 - 32s - loss: 43.1237 - MinusLogProbMetric: 43.1237 - val_loss: 45.2136 - val_MinusLogProbMetric: 45.2136 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 418/1000
2023-10-27 08:24:49.543 
Epoch 418/1000 
	 loss: 42.7470, MinusLogProbMetric: 42.7470, val_loss: 43.2937, val_MinusLogProbMetric: 43.2937

Epoch 418: val_loss improved from 43.32274 to 43.29366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 42.7470 - MinusLogProbMetric: 42.7470 - val_loss: 43.2937 - val_MinusLogProbMetric: 43.2937 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 419/1000
2023-10-27 08:25:23.502 
Epoch 419/1000 
	 loss: 43.1126, MinusLogProbMetric: 43.1126, val_loss: 43.8441, val_MinusLogProbMetric: 43.8441

Epoch 419: val_loss did not improve from 43.29366
196/196 - 33s - loss: 43.1126 - MinusLogProbMetric: 43.1126 - val_loss: 43.8441 - val_MinusLogProbMetric: 43.8441 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 420/1000
2023-10-27 08:25:54.253 
Epoch 420/1000 
	 loss: 42.8327, MinusLogProbMetric: 42.8327, val_loss: 43.3028, val_MinusLogProbMetric: 43.3028

Epoch 420: val_loss did not improve from 43.29366
196/196 - 31s - loss: 42.8327 - MinusLogProbMetric: 42.8327 - val_loss: 43.3028 - val_MinusLogProbMetric: 43.3028 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 421/1000
2023-10-27 08:26:24.041 
Epoch 421/1000 
	 loss: 42.8074, MinusLogProbMetric: 42.8074, val_loss: 43.2522, val_MinusLogProbMetric: 43.2522

Epoch 421: val_loss improved from 43.29366 to 43.25225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 42.8074 - MinusLogProbMetric: 42.8074 - val_loss: 43.2522 - val_MinusLogProbMetric: 43.2522 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 422/1000
2023-10-27 08:26:57.783 
Epoch 422/1000 
	 loss: 42.9304, MinusLogProbMetric: 42.9304, val_loss: 43.7107, val_MinusLogProbMetric: 43.7107

Epoch 422: val_loss did not improve from 43.25225
196/196 - 33s - loss: 42.9304 - MinusLogProbMetric: 42.9304 - val_loss: 43.7107 - val_MinusLogProbMetric: 43.7107 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 423/1000
2023-10-27 08:27:29.769 
Epoch 423/1000 
	 loss: 42.8794, MinusLogProbMetric: 42.8794, val_loss: 43.4671, val_MinusLogProbMetric: 43.4671

Epoch 423: val_loss did not improve from 43.25225
196/196 - 32s - loss: 42.8794 - MinusLogProbMetric: 42.8794 - val_loss: 43.4671 - val_MinusLogProbMetric: 43.4671 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 424/1000
2023-10-27 08:28:03.017 
Epoch 424/1000 
	 loss: 42.7950, MinusLogProbMetric: 42.7950, val_loss: 43.6857, val_MinusLogProbMetric: 43.6857

Epoch 424: val_loss did not improve from 43.25225
196/196 - 33s - loss: 42.7950 - MinusLogProbMetric: 42.7950 - val_loss: 43.6857 - val_MinusLogProbMetric: 43.6857 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 425/1000
2023-10-27 08:28:36.199 
Epoch 425/1000 
	 loss: 42.8734, MinusLogProbMetric: 42.8734, val_loss: 44.7636, val_MinusLogProbMetric: 44.7636

Epoch 425: val_loss did not improve from 43.25225
196/196 - 33s - loss: 42.8734 - MinusLogProbMetric: 42.8734 - val_loss: 44.7636 - val_MinusLogProbMetric: 44.7636 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 426/1000
2023-10-27 08:29:06.381 
Epoch 426/1000 
	 loss: 42.9646, MinusLogProbMetric: 42.9646, val_loss: 44.8316, val_MinusLogProbMetric: 44.8316

Epoch 426: val_loss did not improve from 43.25225
196/196 - 30s - loss: 42.9646 - MinusLogProbMetric: 42.9646 - val_loss: 44.8316 - val_MinusLogProbMetric: 44.8316 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 427/1000
2023-10-27 08:29:37.799 
Epoch 427/1000 
	 loss: 42.9465, MinusLogProbMetric: 42.9465, val_loss: 44.4238, val_MinusLogProbMetric: 44.4238

Epoch 427: val_loss did not improve from 43.25225
196/196 - 31s - loss: 42.9465 - MinusLogProbMetric: 42.9465 - val_loss: 44.4238 - val_MinusLogProbMetric: 44.4238 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 428/1000
2023-10-27 08:30:09.778 
Epoch 428/1000 
	 loss: 42.8700, MinusLogProbMetric: 42.8700, val_loss: 44.1947, val_MinusLogProbMetric: 44.1947

Epoch 428: val_loss did not improve from 43.25225
196/196 - 32s - loss: 42.8700 - MinusLogProbMetric: 42.8700 - val_loss: 44.1947 - val_MinusLogProbMetric: 44.1947 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 429/1000
2023-10-27 08:30:41.894 
Epoch 429/1000 
	 loss: 42.8365, MinusLogProbMetric: 42.8365, val_loss: 43.7457, val_MinusLogProbMetric: 43.7457

Epoch 429: val_loss did not improve from 43.25225
196/196 - 32s - loss: 42.8365 - MinusLogProbMetric: 42.8365 - val_loss: 43.7457 - val_MinusLogProbMetric: 43.7457 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 430/1000
2023-10-27 08:31:15.240 
Epoch 430/1000 
	 loss: 42.9147, MinusLogProbMetric: 42.9147, val_loss: 43.7788, val_MinusLogProbMetric: 43.7788

Epoch 430: val_loss did not improve from 43.25225
196/196 - 33s - loss: 42.9147 - MinusLogProbMetric: 42.9147 - val_loss: 43.7788 - val_MinusLogProbMetric: 43.7788 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 431/1000
2023-10-27 08:31:45.248 
Epoch 431/1000 
	 loss: 42.7529, MinusLogProbMetric: 42.7529, val_loss: 43.0653, val_MinusLogProbMetric: 43.0653

Epoch 431: val_loss improved from 43.25225 to 43.06530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 42.7529 - MinusLogProbMetric: 42.7529 - val_loss: 43.0653 - val_MinusLogProbMetric: 43.0653 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 432/1000
2023-10-27 08:32:15.004 
Epoch 432/1000 
	 loss: 42.9978, MinusLogProbMetric: 42.9978, val_loss: 43.6072, val_MinusLogProbMetric: 43.6072

Epoch 432: val_loss did not improve from 43.06530
196/196 - 29s - loss: 42.9978 - MinusLogProbMetric: 42.9978 - val_loss: 43.6072 - val_MinusLogProbMetric: 43.6072 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 433/1000
2023-10-27 08:32:44.281 
Epoch 433/1000 
	 loss: 42.7180, MinusLogProbMetric: 42.7180, val_loss: 45.7401, val_MinusLogProbMetric: 45.7401

Epoch 433: val_loss did not improve from 43.06530
196/196 - 29s - loss: 42.7180 - MinusLogProbMetric: 42.7180 - val_loss: 45.7401 - val_MinusLogProbMetric: 45.7401 - lr: 1.1111e-04 - 29s/epoch - 149ms/step
Epoch 434/1000
2023-10-27 08:33:15.954 
Epoch 434/1000 
	 loss: 42.8741, MinusLogProbMetric: 42.8741, val_loss: 44.3306, val_MinusLogProbMetric: 44.3306

Epoch 434: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.8741 - MinusLogProbMetric: 42.8741 - val_loss: 44.3306 - val_MinusLogProbMetric: 44.3306 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 435/1000
2023-10-27 08:33:47.507 
Epoch 435/1000 
	 loss: 42.9105, MinusLogProbMetric: 42.9105, val_loss: 44.1947, val_MinusLogProbMetric: 44.1947

Epoch 435: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.9105 - MinusLogProbMetric: 42.9105 - val_loss: 44.1947 - val_MinusLogProbMetric: 44.1947 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 436/1000
2023-10-27 08:34:16.838 
Epoch 436/1000 
	 loss: 42.7172, MinusLogProbMetric: 42.7172, val_loss: 46.2727, val_MinusLogProbMetric: 46.2727

Epoch 436: val_loss did not improve from 43.06530
196/196 - 29s - loss: 42.7172 - MinusLogProbMetric: 42.7172 - val_loss: 46.2727 - val_MinusLogProbMetric: 46.2727 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 437/1000
2023-10-27 08:34:48.202 
Epoch 437/1000 
	 loss: 43.3709, MinusLogProbMetric: 43.3709, val_loss: 43.9010, val_MinusLogProbMetric: 43.9010

Epoch 437: val_loss did not improve from 43.06530
196/196 - 31s - loss: 43.3709 - MinusLogProbMetric: 43.3709 - val_loss: 43.9010 - val_MinusLogProbMetric: 43.9010 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 438/1000
2023-10-27 08:35:17.546 
Epoch 438/1000 
	 loss: 42.6480, MinusLogProbMetric: 42.6480, val_loss: 44.5683, val_MinusLogProbMetric: 44.5683

Epoch 438: val_loss did not improve from 43.06530
196/196 - 29s - loss: 42.6480 - MinusLogProbMetric: 42.6480 - val_loss: 44.5683 - val_MinusLogProbMetric: 44.5683 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 439/1000
2023-10-27 08:35:50.432 
Epoch 439/1000 
	 loss: 42.6558, MinusLogProbMetric: 42.6558, val_loss: 44.0069, val_MinusLogProbMetric: 44.0069

Epoch 439: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.6558 - MinusLogProbMetric: 42.6558 - val_loss: 44.0069 - val_MinusLogProbMetric: 44.0069 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 440/1000
2023-10-27 08:36:23.147 
Epoch 440/1000 
	 loss: 42.8731, MinusLogProbMetric: 42.8731, val_loss: 44.2722, val_MinusLogProbMetric: 44.2722

Epoch 440: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.8731 - MinusLogProbMetric: 42.8731 - val_loss: 44.2722 - val_MinusLogProbMetric: 44.2722 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 441/1000
2023-10-27 08:36:55.771 
Epoch 441/1000 
	 loss: 42.8441, MinusLogProbMetric: 42.8441, val_loss: 43.7528, val_MinusLogProbMetric: 43.7528

Epoch 441: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.8441 - MinusLogProbMetric: 42.8441 - val_loss: 43.7528 - val_MinusLogProbMetric: 43.7528 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 442/1000
2023-10-27 08:37:27.200 
Epoch 442/1000 
	 loss: 42.9059, MinusLogProbMetric: 42.9059, val_loss: 43.3590, val_MinusLogProbMetric: 43.3590

Epoch 442: val_loss did not improve from 43.06530
196/196 - 31s - loss: 42.9059 - MinusLogProbMetric: 42.9059 - val_loss: 43.3590 - val_MinusLogProbMetric: 43.3590 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 443/1000
2023-10-27 08:37:58.514 
Epoch 443/1000 
	 loss: 42.8273, MinusLogProbMetric: 42.8273, val_loss: 44.0232, val_MinusLogProbMetric: 44.0232

Epoch 443: val_loss did not improve from 43.06530
196/196 - 31s - loss: 42.8273 - MinusLogProbMetric: 42.8273 - val_loss: 44.0232 - val_MinusLogProbMetric: 44.0232 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 444/1000
2023-10-27 08:38:30.309 
Epoch 444/1000 
	 loss: 42.6196, MinusLogProbMetric: 42.6196, val_loss: 43.1497, val_MinusLogProbMetric: 43.1497

Epoch 444: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.6196 - MinusLogProbMetric: 42.6196 - val_loss: 43.1497 - val_MinusLogProbMetric: 43.1497 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 445/1000
2023-10-27 08:39:03.465 
Epoch 445/1000 
	 loss: 43.0620, MinusLogProbMetric: 43.0620, val_loss: 43.6875, val_MinusLogProbMetric: 43.6875

Epoch 445: val_loss did not improve from 43.06530
196/196 - 33s - loss: 43.0620 - MinusLogProbMetric: 43.0620 - val_loss: 43.6875 - val_MinusLogProbMetric: 43.6875 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 446/1000
2023-10-27 08:39:37.157 
Epoch 446/1000 
	 loss: 42.7691, MinusLogProbMetric: 42.7691, val_loss: 44.9219, val_MinusLogProbMetric: 44.9219

Epoch 446: val_loss did not improve from 43.06530
196/196 - 34s - loss: 42.7691 - MinusLogProbMetric: 42.7691 - val_loss: 44.9219 - val_MinusLogProbMetric: 44.9219 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 447/1000
2023-10-27 08:40:09.254 
Epoch 447/1000 
	 loss: 42.6019, MinusLogProbMetric: 42.6019, val_loss: 43.8500, val_MinusLogProbMetric: 43.8500

Epoch 447: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.6019 - MinusLogProbMetric: 42.6019 - val_loss: 43.8500 - val_MinusLogProbMetric: 43.8500 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 448/1000
2023-10-27 08:40:39.240 
Epoch 448/1000 
	 loss: 42.7610, MinusLogProbMetric: 42.7610, val_loss: 43.1659, val_MinusLogProbMetric: 43.1659

Epoch 448: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.7610 - MinusLogProbMetric: 42.7610 - val_loss: 43.1659 - val_MinusLogProbMetric: 43.1659 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 449/1000
2023-10-27 08:41:09.493 
Epoch 449/1000 
	 loss: 42.9856, MinusLogProbMetric: 42.9856, val_loss: 45.8965, val_MinusLogProbMetric: 45.8965

Epoch 449: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.9856 - MinusLogProbMetric: 42.9856 - val_loss: 45.8965 - val_MinusLogProbMetric: 45.8965 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 450/1000
2023-10-27 08:41:41.628 
Epoch 450/1000 
	 loss: 42.5499, MinusLogProbMetric: 42.5499, val_loss: 45.9742, val_MinusLogProbMetric: 45.9742

Epoch 450: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.5499 - MinusLogProbMetric: 42.5499 - val_loss: 45.9742 - val_MinusLogProbMetric: 45.9742 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 451/1000
2023-10-27 08:42:13.757 
Epoch 451/1000 
	 loss: 43.3551, MinusLogProbMetric: 43.3551, val_loss: 43.7186, val_MinusLogProbMetric: 43.7186

Epoch 451: val_loss did not improve from 43.06530
196/196 - 32s - loss: 43.3551 - MinusLogProbMetric: 43.3551 - val_loss: 43.7186 - val_MinusLogProbMetric: 43.7186 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 452/1000
2023-10-27 08:42:44.412 
Epoch 452/1000 
	 loss: 42.7192, MinusLogProbMetric: 42.7192, val_loss: 45.0014, val_MinusLogProbMetric: 45.0014

Epoch 452: val_loss did not improve from 43.06530
196/196 - 31s - loss: 42.7192 - MinusLogProbMetric: 42.7192 - val_loss: 45.0014 - val_MinusLogProbMetric: 45.0014 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 453/1000
2023-10-27 08:43:15.177 
Epoch 453/1000 
	 loss: 43.1535, MinusLogProbMetric: 43.1535, val_loss: 43.7475, val_MinusLogProbMetric: 43.7475

Epoch 453: val_loss did not improve from 43.06530
196/196 - 31s - loss: 43.1535 - MinusLogProbMetric: 43.1535 - val_loss: 43.7475 - val_MinusLogProbMetric: 43.7475 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 454/1000
2023-10-27 08:43:45.177 
Epoch 454/1000 
	 loss: 42.5920, MinusLogProbMetric: 42.5920, val_loss: 43.5662, val_MinusLogProbMetric: 43.5662

Epoch 454: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.5920 - MinusLogProbMetric: 42.5920 - val_loss: 43.5662 - val_MinusLogProbMetric: 43.5662 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 455/1000
2023-10-27 08:44:18.085 
Epoch 455/1000 
	 loss: 42.6610, MinusLogProbMetric: 42.6610, val_loss: 43.6340, val_MinusLogProbMetric: 43.6340

Epoch 455: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.6610 - MinusLogProbMetric: 42.6610 - val_loss: 43.6340 - val_MinusLogProbMetric: 43.6340 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 456/1000
2023-10-27 08:44:49.409 
Epoch 456/1000 
	 loss: 42.5829, MinusLogProbMetric: 42.5829, val_loss: 43.8480, val_MinusLogProbMetric: 43.8480

Epoch 456: val_loss did not improve from 43.06530
196/196 - 31s - loss: 42.5829 - MinusLogProbMetric: 42.5829 - val_loss: 43.8480 - val_MinusLogProbMetric: 43.8480 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 457/1000
2023-10-27 08:45:21.214 
Epoch 457/1000 
	 loss: 42.6000, MinusLogProbMetric: 42.6000, val_loss: 44.5543, val_MinusLogProbMetric: 44.5543

Epoch 457: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.6000 - MinusLogProbMetric: 42.6000 - val_loss: 44.5543 - val_MinusLogProbMetric: 44.5543 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 458/1000
2023-10-27 08:45:51.339 
Epoch 458/1000 
	 loss: 42.7340, MinusLogProbMetric: 42.7340, val_loss: 43.8642, val_MinusLogProbMetric: 43.8642

Epoch 458: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.7340 - MinusLogProbMetric: 42.7340 - val_loss: 43.8642 - val_MinusLogProbMetric: 43.8642 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 459/1000
2023-10-27 08:46:21.739 
Epoch 459/1000 
	 loss: 42.5335, MinusLogProbMetric: 42.5335, val_loss: 43.2033, val_MinusLogProbMetric: 43.2033

Epoch 459: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.5335 - MinusLogProbMetric: 42.5335 - val_loss: 43.2033 - val_MinusLogProbMetric: 43.2033 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 460/1000
2023-10-27 08:46:52.272 
Epoch 460/1000 
	 loss: 42.8296, MinusLogProbMetric: 42.8296, val_loss: 43.7191, val_MinusLogProbMetric: 43.7191

Epoch 460: val_loss did not improve from 43.06530
196/196 - 31s - loss: 42.8296 - MinusLogProbMetric: 42.8296 - val_loss: 43.7191 - val_MinusLogProbMetric: 43.7191 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 461/1000
2023-10-27 08:47:24.582 
Epoch 461/1000 
	 loss: 42.5183, MinusLogProbMetric: 42.5183, val_loss: 43.7309, val_MinusLogProbMetric: 43.7309

Epoch 461: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.5183 - MinusLogProbMetric: 42.5183 - val_loss: 43.7309 - val_MinusLogProbMetric: 43.7309 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 462/1000
2023-10-27 08:47:57.420 
Epoch 462/1000 
	 loss: 42.6655, MinusLogProbMetric: 42.6655, val_loss: 44.0082, val_MinusLogProbMetric: 44.0082

Epoch 462: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.6655 - MinusLogProbMetric: 42.6655 - val_loss: 44.0082 - val_MinusLogProbMetric: 44.0082 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 463/1000
2023-10-27 08:48:29.784 
Epoch 463/1000 
	 loss: 42.7712, MinusLogProbMetric: 42.7712, val_loss: 43.3558, val_MinusLogProbMetric: 43.3558

Epoch 463: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.7712 - MinusLogProbMetric: 42.7712 - val_loss: 43.3558 - val_MinusLogProbMetric: 43.3558 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 464/1000
2023-10-27 08:48:59.382 
Epoch 464/1000 
	 loss: 42.7524, MinusLogProbMetric: 42.7524, val_loss: 43.2011, val_MinusLogProbMetric: 43.2011

Epoch 464: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.7524 - MinusLogProbMetric: 42.7524 - val_loss: 43.2011 - val_MinusLogProbMetric: 43.2011 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 465/1000
2023-10-27 08:49:28.927 
Epoch 465/1000 
	 loss: 42.9738, MinusLogProbMetric: 42.9738, val_loss: 43.9470, val_MinusLogProbMetric: 43.9470

Epoch 465: val_loss did not improve from 43.06530
196/196 - 30s - loss: 42.9738 - MinusLogProbMetric: 42.9738 - val_loss: 43.9470 - val_MinusLogProbMetric: 43.9470 - lr: 1.1111e-04 - 30s/epoch - 151ms/step
Epoch 466/1000
2023-10-27 08:50:00.572 
Epoch 466/1000 
	 loss: 42.6815, MinusLogProbMetric: 42.6815, val_loss: 43.6506, val_MinusLogProbMetric: 43.6506

Epoch 466: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.6815 - MinusLogProbMetric: 42.6815 - val_loss: 43.6506 - val_MinusLogProbMetric: 43.6506 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 467/1000
2023-10-27 08:50:32.611 
Epoch 467/1000 
	 loss: 42.6124, MinusLogProbMetric: 42.6124, val_loss: 43.5407, val_MinusLogProbMetric: 43.5407

Epoch 467: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.6124 - MinusLogProbMetric: 42.6124 - val_loss: 43.5407 - val_MinusLogProbMetric: 43.5407 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 468/1000
2023-10-27 08:51:04.785 
Epoch 468/1000 
	 loss: 42.5220, MinusLogProbMetric: 42.5220, val_loss: 43.6308, val_MinusLogProbMetric: 43.6308

Epoch 468: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.5220 - MinusLogProbMetric: 42.5220 - val_loss: 43.6308 - val_MinusLogProbMetric: 43.6308 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 469/1000
2023-10-27 08:51:37.368 
Epoch 469/1000 
	 loss: 42.7132, MinusLogProbMetric: 42.7132, val_loss: 43.5696, val_MinusLogProbMetric: 43.5696

Epoch 469: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.7132 - MinusLogProbMetric: 42.7132 - val_loss: 43.5696 - val_MinusLogProbMetric: 43.5696 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 470/1000
2023-10-27 08:52:10.361 
Epoch 470/1000 
	 loss: 42.5507, MinusLogProbMetric: 42.5507, val_loss: 43.8456, val_MinusLogProbMetric: 43.8456

Epoch 470: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.5507 - MinusLogProbMetric: 42.5507 - val_loss: 43.8456 - val_MinusLogProbMetric: 43.8456 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 471/1000
2023-10-27 08:52:43.947 
Epoch 471/1000 
	 loss: 42.6123, MinusLogProbMetric: 42.6123, val_loss: 43.5336, val_MinusLogProbMetric: 43.5336

Epoch 471: val_loss did not improve from 43.06530
196/196 - 34s - loss: 42.6123 - MinusLogProbMetric: 42.6123 - val_loss: 43.5336 - val_MinusLogProbMetric: 43.5336 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 472/1000
2023-10-27 08:53:17.424 
Epoch 472/1000 
	 loss: 42.6233, MinusLogProbMetric: 42.6233, val_loss: 44.0787, val_MinusLogProbMetric: 44.0787

Epoch 472: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.6233 - MinusLogProbMetric: 42.6233 - val_loss: 44.0787 - val_MinusLogProbMetric: 44.0787 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 473/1000
2023-10-27 08:53:50.805 
Epoch 473/1000 
	 loss: 42.5662, MinusLogProbMetric: 42.5662, val_loss: 43.1838, val_MinusLogProbMetric: 43.1838

Epoch 473: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.5662 - MinusLogProbMetric: 42.5662 - val_loss: 43.1838 - val_MinusLogProbMetric: 43.1838 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 474/1000
2023-10-27 08:54:23.971 
Epoch 474/1000 
	 loss: 42.8118, MinusLogProbMetric: 42.8118, val_loss: 43.3006, val_MinusLogProbMetric: 43.3006

Epoch 474: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.8118 - MinusLogProbMetric: 42.8118 - val_loss: 43.3006 - val_MinusLogProbMetric: 43.3006 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 475/1000
2023-10-27 08:54:57.259 
Epoch 475/1000 
	 loss: 42.9621, MinusLogProbMetric: 42.9621, val_loss: 45.5460, val_MinusLogProbMetric: 45.5460

Epoch 475: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.9621 - MinusLogProbMetric: 42.9621 - val_loss: 45.5460 - val_MinusLogProbMetric: 45.5460 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 476/1000
2023-10-27 08:55:29.131 
Epoch 476/1000 
	 loss: 42.9364, MinusLogProbMetric: 42.9364, val_loss: 43.6237, val_MinusLogProbMetric: 43.6237

Epoch 476: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.9364 - MinusLogProbMetric: 42.9364 - val_loss: 43.6237 - val_MinusLogProbMetric: 43.6237 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 477/1000
2023-10-27 08:56:02.403 
Epoch 477/1000 
	 loss: 42.5858, MinusLogProbMetric: 42.5858, val_loss: 43.6982, val_MinusLogProbMetric: 43.6982

Epoch 477: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.5858 - MinusLogProbMetric: 42.5858 - val_loss: 43.6982 - val_MinusLogProbMetric: 43.6982 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 478/1000
2023-10-27 08:56:34.029 
Epoch 478/1000 
	 loss: 42.7298, MinusLogProbMetric: 42.7298, val_loss: 43.1502, val_MinusLogProbMetric: 43.1502

Epoch 478: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.7298 - MinusLogProbMetric: 42.7298 - val_loss: 43.1502 - val_MinusLogProbMetric: 43.1502 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 479/1000
2023-10-27 08:57:06.776 
Epoch 479/1000 
	 loss: 42.4636, MinusLogProbMetric: 42.4636, val_loss: 44.4334, val_MinusLogProbMetric: 44.4334

Epoch 479: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.4636 - MinusLogProbMetric: 42.4636 - val_loss: 44.4334 - val_MinusLogProbMetric: 44.4334 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 480/1000
2023-10-27 08:57:40.000 
Epoch 480/1000 
	 loss: 42.6139, MinusLogProbMetric: 42.6139, val_loss: 43.4812, val_MinusLogProbMetric: 43.4812

Epoch 480: val_loss did not improve from 43.06530
196/196 - 33s - loss: 42.6139 - MinusLogProbMetric: 42.6139 - val_loss: 43.4812 - val_MinusLogProbMetric: 43.4812 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 481/1000
2023-10-27 08:58:11.774 
Epoch 481/1000 
	 loss: 42.5404, MinusLogProbMetric: 42.5404, val_loss: 43.5382, val_MinusLogProbMetric: 43.5382

Epoch 481: val_loss did not improve from 43.06530
196/196 - 32s - loss: 42.5404 - MinusLogProbMetric: 42.5404 - val_loss: 43.5382 - val_MinusLogProbMetric: 43.5382 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 482/1000
2023-10-27 08:58:44.963 
Epoch 482/1000 
	 loss: 41.1785, MinusLogProbMetric: 41.1785, val_loss: 42.5813, val_MinusLogProbMetric: 42.5813

Epoch 482: val_loss improved from 43.06530 to 42.58126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.1785 - MinusLogProbMetric: 41.1785 - val_loss: 42.5813 - val_MinusLogProbMetric: 42.5813 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 483/1000
2023-10-27 08:59:17.766 
Epoch 483/1000 
	 loss: 41.1959, MinusLogProbMetric: 41.1959, val_loss: 43.5316, val_MinusLogProbMetric: 43.5316

Epoch 483: val_loss did not improve from 42.58126
196/196 - 32s - loss: 41.1959 - MinusLogProbMetric: 41.1959 - val_loss: 43.5316 - val_MinusLogProbMetric: 43.5316 - lr: 5.5556e-05 - 32s/epoch - 164ms/step
Epoch 484/1000
2023-10-27 08:59:50.220 
Epoch 484/1000 
	 loss: 41.2920, MinusLogProbMetric: 41.2920, val_loss: 42.8811, val_MinusLogProbMetric: 42.8811

Epoch 484: val_loss did not improve from 42.58126
196/196 - 32s - loss: 41.2920 - MinusLogProbMetric: 41.2920 - val_loss: 42.8811 - val_MinusLogProbMetric: 42.8811 - lr: 5.5556e-05 - 32s/epoch - 166ms/step
Epoch 485/1000
2023-10-27 09:00:23.487 
Epoch 485/1000 
	 loss: 41.2668, MinusLogProbMetric: 41.2668, val_loss: 42.6090, val_MinusLogProbMetric: 42.6090

Epoch 485: val_loss did not improve from 42.58126
196/196 - 33s - loss: 41.2668 - MinusLogProbMetric: 41.2668 - val_loss: 42.6090 - val_MinusLogProbMetric: 42.6090 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 486/1000
2023-10-27 09:00:57.198 
Epoch 486/1000 
	 loss: 41.2483, MinusLogProbMetric: 41.2483, val_loss: 42.7487, val_MinusLogProbMetric: 42.7487

Epoch 486: val_loss did not improve from 42.58126
196/196 - 34s - loss: 41.2483 - MinusLogProbMetric: 41.2483 - val_loss: 42.7487 - val_MinusLogProbMetric: 42.7487 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 487/1000
2023-10-27 09:01:29.428 
Epoch 487/1000 
	 loss: 41.2922, MinusLogProbMetric: 41.2922, val_loss: 42.4973, val_MinusLogProbMetric: 42.4973

Epoch 487: val_loss improved from 42.58126 to 42.49729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 41.2922 - MinusLogProbMetric: 41.2922 - val_loss: 42.4973 - val_MinusLogProbMetric: 42.4973 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 488/1000
2023-10-27 09:02:03.222 
Epoch 488/1000 
	 loss: 41.2867, MinusLogProbMetric: 41.2867, val_loss: 42.7565, val_MinusLogProbMetric: 42.7565

Epoch 488: val_loss did not improve from 42.49729
196/196 - 33s - loss: 41.2867 - MinusLogProbMetric: 41.2867 - val_loss: 42.7565 - val_MinusLogProbMetric: 42.7565 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 489/1000
2023-10-27 09:02:34.915 
Epoch 489/1000 
	 loss: 41.2569, MinusLogProbMetric: 41.2569, val_loss: 42.4190, val_MinusLogProbMetric: 42.4190

Epoch 489: val_loss improved from 42.49729 to 42.41898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 32s - loss: 41.2569 - MinusLogProbMetric: 41.2569 - val_loss: 42.4190 - val_MinusLogProbMetric: 42.4190 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 490/1000
2023-10-27 09:03:08.204 
Epoch 490/1000 
	 loss: 41.3611, MinusLogProbMetric: 41.3611, val_loss: 42.4721, val_MinusLogProbMetric: 42.4721

Epoch 490: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.3611 - MinusLogProbMetric: 41.3611 - val_loss: 42.4721 - val_MinusLogProbMetric: 42.4721 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 491/1000
2023-10-27 09:03:40.042 
Epoch 491/1000 
	 loss: 41.2719, MinusLogProbMetric: 41.2719, val_loss: 42.6982, val_MinusLogProbMetric: 42.6982

Epoch 491: val_loss did not improve from 42.41898
196/196 - 32s - loss: 41.2719 - MinusLogProbMetric: 41.2719 - val_loss: 42.6982 - val_MinusLogProbMetric: 42.6982 - lr: 5.5556e-05 - 32s/epoch - 162ms/step
Epoch 492/1000
2023-10-27 09:04:12.664 
Epoch 492/1000 
	 loss: 41.1896, MinusLogProbMetric: 41.1896, val_loss: 42.5990, val_MinusLogProbMetric: 42.5990

Epoch 492: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.1896 - MinusLogProbMetric: 41.1896 - val_loss: 42.5990 - val_MinusLogProbMetric: 42.5990 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 493/1000
2023-10-27 09:04:45.755 
Epoch 493/1000 
	 loss: 41.2565, MinusLogProbMetric: 41.2565, val_loss: 42.4604, val_MinusLogProbMetric: 42.4604

Epoch 493: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.2565 - MinusLogProbMetric: 41.2565 - val_loss: 42.4604 - val_MinusLogProbMetric: 42.4604 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 494/1000
2023-10-27 09:05:17.311 
Epoch 494/1000 
	 loss: 41.5056, MinusLogProbMetric: 41.5056, val_loss: 42.8851, val_MinusLogProbMetric: 42.8851

Epoch 494: val_loss did not improve from 42.41898
196/196 - 32s - loss: 41.5056 - MinusLogProbMetric: 41.5056 - val_loss: 42.8851 - val_MinusLogProbMetric: 42.8851 - lr: 5.5556e-05 - 32s/epoch - 161ms/step
Epoch 495/1000
2023-10-27 09:05:50.383 
Epoch 495/1000 
	 loss: 41.3156, MinusLogProbMetric: 41.3156, val_loss: 43.1723, val_MinusLogProbMetric: 43.1723

Epoch 495: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.3156 - MinusLogProbMetric: 41.3156 - val_loss: 43.1723 - val_MinusLogProbMetric: 43.1723 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 496/1000
2023-10-27 09:06:22.959 
Epoch 496/1000 
	 loss: 41.2778, MinusLogProbMetric: 41.2778, val_loss: 42.5776, val_MinusLogProbMetric: 42.5776

Epoch 496: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.2778 - MinusLogProbMetric: 41.2778 - val_loss: 42.5776 - val_MinusLogProbMetric: 42.5776 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 497/1000
2023-10-27 09:06:56.563 
Epoch 497/1000 
	 loss: 41.3497, MinusLogProbMetric: 41.3497, val_loss: 42.6197, val_MinusLogProbMetric: 42.6197

Epoch 497: val_loss did not improve from 42.41898
196/196 - 34s - loss: 41.3497 - MinusLogProbMetric: 41.3497 - val_loss: 42.6197 - val_MinusLogProbMetric: 42.6197 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 498/1000
2023-10-27 09:07:28.961 
Epoch 498/1000 
	 loss: 41.3373, MinusLogProbMetric: 41.3373, val_loss: 42.6051, val_MinusLogProbMetric: 42.6051

Epoch 498: val_loss did not improve from 42.41898
196/196 - 32s - loss: 41.3373 - MinusLogProbMetric: 41.3373 - val_loss: 42.6051 - val_MinusLogProbMetric: 42.6051 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 499/1000
2023-10-27 09:08:02.236 
Epoch 499/1000 
	 loss: 41.2545, MinusLogProbMetric: 41.2545, val_loss: 42.4852, val_MinusLogProbMetric: 42.4852

Epoch 499: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.2545 - MinusLogProbMetric: 41.2545 - val_loss: 42.4852 - val_MinusLogProbMetric: 42.4852 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 500/1000
2023-10-27 09:08:35.472 
Epoch 500/1000 
	 loss: 41.2845, MinusLogProbMetric: 41.2845, val_loss: 42.4197, val_MinusLogProbMetric: 42.4197

Epoch 500: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.2845 - MinusLogProbMetric: 41.2845 - val_loss: 42.4197 - val_MinusLogProbMetric: 42.4197 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 501/1000
2023-10-27 09:09:07.252 
Epoch 501/1000 
	 loss: 41.1733, MinusLogProbMetric: 41.1733, val_loss: 42.7149, val_MinusLogProbMetric: 42.7149

Epoch 501: val_loss did not improve from 42.41898
196/196 - 32s - loss: 41.1733 - MinusLogProbMetric: 41.1733 - val_loss: 42.7149 - val_MinusLogProbMetric: 42.7149 - lr: 5.5556e-05 - 32s/epoch - 162ms/step
Epoch 502/1000
2023-10-27 09:09:39.811 
Epoch 502/1000 
	 loss: 41.3476, MinusLogProbMetric: 41.3476, val_loss: 42.8652, val_MinusLogProbMetric: 42.8652

Epoch 502: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.3476 - MinusLogProbMetric: 41.3476 - val_loss: 42.8652 - val_MinusLogProbMetric: 42.8652 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 503/1000
2023-10-27 09:10:12.939 
Epoch 503/1000 
	 loss: 41.3774, MinusLogProbMetric: 41.3774, val_loss: 42.6697, val_MinusLogProbMetric: 42.6697

Epoch 503: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.3774 - MinusLogProbMetric: 41.3774 - val_loss: 42.6697 - val_MinusLogProbMetric: 42.6697 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 504/1000
2023-10-27 09:10:46.225 
Epoch 504/1000 
	 loss: 41.3963, MinusLogProbMetric: 41.3963, val_loss: 43.6431, val_MinusLogProbMetric: 43.6431

Epoch 504: val_loss did not improve from 42.41898
196/196 - 33s - loss: 41.3963 - MinusLogProbMetric: 41.3963 - val_loss: 43.6431 - val_MinusLogProbMetric: 43.6431 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 505/1000
2023-10-27 09:11:19.530 
Epoch 505/1000 
	 loss: 41.2627, MinusLogProbMetric: 41.2627, val_loss: 42.4126, val_MinusLogProbMetric: 42.4126

Epoch 505: val_loss improved from 42.41898 to 42.41256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.2627 - MinusLogProbMetric: 41.2627 - val_loss: 42.4126 - val_MinusLogProbMetric: 42.4126 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 506/1000
2023-10-27 09:11:53.101 
Epoch 506/1000 
	 loss: 41.1954, MinusLogProbMetric: 41.1954, val_loss: 42.5975, val_MinusLogProbMetric: 42.5975

Epoch 506: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.1954 - MinusLogProbMetric: 41.1954 - val_loss: 42.5975 - val_MinusLogProbMetric: 42.5975 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 507/1000
2023-10-27 09:12:26.172 
Epoch 507/1000 
	 loss: 41.2352, MinusLogProbMetric: 41.2352, val_loss: 42.5748, val_MinusLogProbMetric: 42.5748

Epoch 507: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.2352 - MinusLogProbMetric: 41.2352 - val_loss: 42.5748 - val_MinusLogProbMetric: 42.5748 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 508/1000
2023-10-27 09:12:59.467 
Epoch 508/1000 
	 loss: 41.4073, MinusLogProbMetric: 41.4073, val_loss: 43.4660, val_MinusLogProbMetric: 43.4660

Epoch 508: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.4073 - MinusLogProbMetric: 41.4073 - val_loss: 43.4660 - val_MinusLogProbMetric: 43.4660 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 509/1000
2023-10-27 09:13:32.365 
Epoch 509/1000 
	 loss: 41.1667, MinusLogProbMetric: 41.1667, val_loss: 42.9509, val_MinusLogProbMetric: 42.9509

Epoch 509: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.1667 - MinusLogProbMetric: 41.1667 - val_loss: 42.9509 - val_MinusLogProbMetric: 42.9509 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 510/1000
2023-10-27 09:14:05.675 
Epoch 510/1000 
	 loss: 41.2594, MinusLogProbMetric: 41.2594, val_loss: 42.5122, val_MinusLogProbMetric: 42.5122

Epoch 510: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.2594 - MinusLogProbMetric: 41.2594 - val_loss: 42.5122 - val_MinusLogProbMetric: 42.5122 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 511/1000
2023-10-27 09:14:39.356 
Epoch 511/1000 
	 loss: 41.6401, MinusLogProbMetric: 41.6401, val_loss: 42.6564, val_MinusLogProbMetric: 42.6564

Epoch 511: val_loss did not improve from 42.41256
196/196 - 34s - loss: 41.6401 - MinusLogProbMetric: 41.6401 - val_loss: 42.6564 - val_MinusLogProbMetric: 42.6564 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 512/1000
2023-10-27 09:15:12.533 
Epoch 512/1000 
	 loss: 41.3056, MinusLogProbMetric: 41.3056, val_loss: 43.6050, val_MinusLogProbMetric: 43.6050

Epoch 512: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.3056 - MinusLogProbMetric: 41.3056 - val_loss: 43.6050 - val_MinusLogProbMetric: 43.6050 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 513/1000
2023-10-27 09:15:45.874 
Epoch 513/1000 
	 loss: 41.1759, MinusLogProbMetric: 41.1759, val_loss: 42.6172, val_MinusLogProbMetric: 42.6172

Epoch 513: val_loss did not improve from 42.41256
196/196 - 33s - loss: 41.1759 - MinusLogProbMetric: 41.1759 - val_loss: 42.6172 - val_MinusLogProbMetric: 42.6172 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 514/1000
2023-10-27 09:16:19.172 
Epoch 514/1000 
	 loss: 41.1962, MinusLogProbMetric: 41.1962, val_loss: 42.3602, val_MinusLogProbMetric: 42.3602

Epoch 514: val_loss improved from 42.41256 to 42.36023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.1962 - MinusLogProbMetric: 41.1962 - val_loss: 42.3602 - val_MinusLogProbMetric: 42.3602 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 515/1000
2023-10-27 09:16:52.598 
Epoch 515/1000 
	 loss: 41.3947, MinusLogProbMetric: 41.3947, val_loss: 42.6517, val_MinusLogProbMetric: 42.6517

Epoch 515: val_loss did not improve from 42.36023
196/196 - 33s - loss: 41.3947 - MinusLogProbMetric: 41.3947 - val_loss: 42.6517 - val_MinusLogProbMetric: 42.6517 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 516/1000
2023-10-27 09:17:25.394 
Epoch 516/1000 
	 loss: 41.2114, MinusLogProbMetric: 41.2114, val_loss: 42.4605, val_MinusLogProbMetric: 42.4605

Epoch 516: val_loss did not improve from 42.36023
196/196 - 33s - loss: 41.2114 - MinusLogProbMetric: 41.2114 - val_loss: 42.4605 - val_MinusLogProbMetric: 42.4605 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 517/1000
2023-10-27 09:17:57.900 
Epoch 517/1000 
	 loss: 41.3554, MinusLogProbMetric: 41.3554, val_loss: 43.1416, val_MinusLogProbMetric: 43.1416

Epoch 517: val_loss did not improve from 42.36023
196/196 - 33s - loss: 41.3554 - MinusLogProbMetric: 41.3554 - val_loss: 43.1416 - val_MinusLogProbMetric: 43.1416 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 518/1000
2023-10-27 09:18:29.893 
Epoch 518/1000 
	 loss: 41.2531, MinusLogProbMetric: 41.2531, val_loss: 42.3279, val_MinusLogProbMetric: 42.3279

Epoch 518: val_loss improved from 42.36023 to 42.32789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 41.2531 - MinusLogProbMetric: 41.2531 - val_loss: 42.3279 - val_MinusLogProbMetric: 42.3279 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 519/1000
2023-10-27 09:19:03.520 
Epoch 519/1000 
	 loss: 41.4751, MinusLogProbMetric: 41.4751, val_loss: 42.3848, val_MinusLogProbMetric: 42.3848

Epoch 519: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.4751 - MinusLogProbMetric: 41.4751 - val_loss: 42.3848 - val_MinusLogProbMetric: 42.3848 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 520/1000
2023-10-27 09:19:36.354 
Epoch 520/1000 
	 loss: 41.2768, MinusLogProbMetric: 41.2768, val_loss: 43.3541, val_MinusLogProbMetric: 43.3541

Epoch 520: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.2768 - MinusLogProbMetric: 41.2768 - val_loss: 43.3541 - val_MinusLogProbMetric: 43.3541 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 521/1000
2023-10-27 09:20:09.092 
Epoch 521/1000 
	 loss: 41.1321, MinusLogProbMetric: 41.1321, val_loss: 43.1798, val_MinusLogProbMetric: 43.1798

Epoch 521: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.1321 - MinusLogProbMetric: 41.1321 - val_loss: 43.1798 - val_MinusLogProbMetric: 43.1798 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 522/1000
2023-10-27 09:20:42.396 
Epoch 522/1000 
	 loss: 41.2394, MinusLogProbMetric: 41.2394, val_loss: 42.4603, val_MinusLogProbMetric: 42.4603

Epoch 522: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.2394 - MinusLogProbMetric: 41.2394 - val_loss: 42.4603 - val_MinusLogProbMetric: 42.4603 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 523/1000
2023-10-27 09:21:15.795 
Epoch 523/1000 
	 loss: 41.4431, MinusLogProbMetric: 41.4431, val_loss: 42.6053, val_MinusLogProbMetric: 42.6053

Epoch 523: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.4431 - MinusLogProbMetric: 41.4431 - val_loss: 42.6053 - val_MinusLogProbMetric: 42.6053 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 524/1000
2023-10-27 09:21:49.635 
Epoch 524/1000 
	 loss: 41.2564, MinusLogProbMetric: 41.2564, val_loss: 43.4098, val_MinusLogProbMetric: 43.4098

Epoch 524: val_loss did not improve from 42.32789
196/196 - 34s - loss: 41.2564 - MinusLogProbMetric: 41.2564 - val_loss: 43.4098 - val_MinusLogProbMetric: 43.4098 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 525/1000
2023-10-27 09:22:22.831 
Epoch 525/1000 
	 loss: 41.4576, MinusLogProbMetric: 41.4576, val_loss: 42.3914, val_MinusLogProbMetric: 42.3914

Epoch 525: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.4576 - MinusLogProbMetric: 41.4576 - val_loss: 42.3914 - val_MinusLogProbMetric: 42.3914 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 526/1000
2023-10-27 09:22:55.226 
Epoch 526/1000 
	 loss: 41.1935, MinusLogProbMetric: 41.1935, val_loss: 42.5132, val_MinusLogProbMetric: 42.5132

Epoch 526: val_loss did not improve from 42.32789
196/196 - 32s - loss: 41.1935 - MinusLogProbMetric: 41.1935 - val_loss: 42.5132 - val_MinusLogProbMetric: 42.5132 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 527/1000
2023-10-27 09:23:28.710 
Epoch 527/1000 
	 loss: 41.1048, MinusLogProbMetric: 41.1048, val_loss: 43.4192, val_MinusLogProbMetric: 43.4192

Epoch 527: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.1048 - MinusLogProbMetric: 41.1048 - val_loss: 43.4192 - val_MinusLogProbMetric: 43.4192 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 528/1000
2023-10-27 09:24:01.351 
Epoch 528/1000 
	 loss: 41.1730, MinusLogProbMetric: 41.1730, val_loss: 42.5720, val_MinusLogProbMetric: 42.5720

Epoch 528: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.1730 - MinusLogProbMetric: 41.1730 - val_loss: 42.5720 - val_MinusLogProbMetric: 42.5720 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 529/1000
2023-10-27 09:24:34.501 
Epoch 529/1000 
	 loss: 41.2788, MinusLogProbMetric: 41.2788, val_loss: 42.4792, val_MinusLogProbMetric: 42.4792

Epoch 529: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.2788 - MinusLogProbMetric: 41.2788 - val_loss: 42.4792 - val_MinusLogProbMetric: 42.4792 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 530/1000
2023-10-27 09:25:07.614 
Epoch 530/1000 
	 loss: 41.3295, MinusLogProbMetric: 41.3295, val_loss: 43.4601, val_MinusLogProbMetric: 43.4601

Epoch 530: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.3295 - MinusLogProbMetric: 41.3295 - val_loss: 43.4601 - val_MinusLogProbMetric: 43.4601 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 531/1000
2023-10-27 09:25:40.918 
Epoch 531/1000 
	 loss: 41.2298, MinusLogProbMetric: 41.2298, val_loss: 42.4564, val_MinusLogProbMetric: 42.4564

Epoch 531: val_loss did not improve from 42.32789
196/196 - 33s - loss: 41.2298 - MinusLogProbMetric: 41.2298 - val_loss: 42.4564 - val_MinusLogProbMetric: 42.4564 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 532/1000
2023-10-27 09:26:13.936 
Epoch 532/1000 
	 loss: 41.2248, MinusLogProbMetric: 41.2248, val_loss: 42.3054, val_MinusLogProbMetric: 42.3054

Epoch 532: val_loss improved from 42.32789 to 42.30536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.2248 - MinusLogProbMetric: 41.2248 - val_loss: 42.3054 - val_MinusLogProbMetric: 42.3054 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 533/1000
2023-10-27 09:26:46.837 
Epoch 533/1000 
	 loss: 41.4620, MinusLogProbMetric: 41.4620, val_loss: 42.5422, val_MinusLogProbMetric: 42.5422

Epoch 533: val_loss did not improve from 42.30536
196/196 - 32s - loss: 41.4620 - MinusLogProbMetric: 41.4620 - val_loss: 42.5422 - val_MinusLogProbMetric: 42.5422 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 534/1000
2023-10-27 09:27:19.342 
Epoch 534/1000 
	 loss: 41.2729, MinusLogProbMetric: 41.2729, val_loss: 43.8804, val_MinusLogProbMetric: 43.8804

Epoch 534: val_loss did not improve from 42.30536
196/196 - 33s - loss: 41.2729 - MinusLogProbMetric: 41.2729 - val_loss: 43.8804 - val_MinusLogProbMetric: 43.8804 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 535/1000
2023-10-27 09:27:52.406 
Epoch 535/1000 
	 loss: 41.2373, MinusLogProbMetric: 41.2373, val_loss: 42.8909, val_MinusLogProbMetric: 42.8909

Epoch 535: val_loss did not improve from 42.30536
196/196 - 33s - loss: 41.2373 - MinusLogProbMetric: 41.2373 - val_loss: 42.8909 - val_MinusLogProbMetric: 42.8909 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 536/1000
2023-10-27 09:28:25.337 
Epoch 536/1000 
	 loss: 41.3842, MinusLogProbMetric: 41.3842, val_loss: 42.4556, val_MinusLogProbMetric: 42.4556

Epoch 536: val_loss did not improve from 42.30536
196/196 - 33s - loss: 41.3842 - MinusLogProbMetric: 41.3842 - val_loss: 42.4556 - val_MinusLogProbMetric: 42.4556 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 537/1000
2023-10-27 09:28:56.750 
Epoch 537/1000 
	 loss: 41.1864, MinusLogProbMetric: 41.1864, val_loss: 42.7901, val_MinusLogProbMetric: 42.7901

Epoch 537: val_loss did not improve from 42.30536
196/196 - 31s - loss: 41.1864 - MinusLogProbMetric: 41.1864 - val_loss: 42.7901 - val_MinusLogProbMetric: 42.7901 - lr: 5.5556e-05 - 31s/epoch - 160ms/step
Epoch 538/1000
2023-10-27 09:29:30.016 
Epoch 538/1000 
	 loss: 41.4606, MinusLogProbMetric: 41.4606, val_loss: 42.9114, val_MinusLogProbMetric: 42.9114

Epoch 538: val_loss did not improve from 42.30536
196/196 - 33s - loss: 41.4606 - MinusLogProbMetric: 41.4606 - val_loss: 42.9114 - val_MinusLogProbMetric: 42.9114 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 539/1000
2023-10-27 09:30:03.568 
Epoch 539/1000 
	 loss: 41.1493, MinusLogProbMetric: 41.1493, val_loss: 42.5035, val_MinusLogProbMetric: 42.5035

Epoch 539: val_loss did not improve from 42.30536
196/196 - 34s - loss: 41.1493 - MinusLogProbMetric: 41.1493 - val_loss: 42.5035 - val_MinusLogProbMetric: 42.5035 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 540/1000
2023-10-27 09:30:37.379 
Epoch 540/1000 
	 loss: 41.1326, MinusLogProbMetric: 41.1326, val_loss: 42.8129, val_MinusLogProbMetric: 42.8129

Epoch 540: val_loss did not improve from 42.30536
196/196 - 34s - loss: 41.1326 - MinusLogProbMetric: 41.1326 - val_loss: 42.8129 - val_MinusLogProbMetric: 42.8129 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 541/1000
2023-10-27 09:31:11.133 
Epoch 541/1000 
	 loss: 41.1370, MinusLogProbMetric: 41.1370, val_loss: 42.3173, val_MinusLogProbMetric: 42.3173

Epoch 541: val_loss did not improve from 42.30536
196/196 - 34s - loss: 41.1370 - MinusLogProbMetric: 41.1370 - val_loss: 42.3173 - val_MinusLogProbMetric: 42.3173 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 542/1000
2023-10-27 09:31:44.249 
Epoch 542/1000 
	 loss: 41.1624, MinusLogProbMetric: 41.1624, val_loss: 42.5508, val_MinusLogProbMetric: 42.5508

Epoch 542: val_loss did not improve from 42.30536
196/196 - 33s - loss: 41.1624 - MinusLogProbMetric: 41.1624 - val_loss: 42.5508 - val_MinusLogProbMetric: 42.5508 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 543/1000
2023-10-27 09:32:17.300 
Epoch 543/1000 
	 loss: 41.3825, MinusLogProbMetric: 41.3825, val_loss: 42.4647, val_MinusLogProbMetric: 42.4647

Epoch 543: val_loss did not improve from 42.30536
196/196 - 33s - loss: 41.3825 - MinusLogProbMetric: 41.3825 - val_loss: 42.4647 - val_MinusLogProbMetric: 42.4647 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 544/1000
2023-10-27 09:32:49.653 
Epoch 544/1000 
	 loss: 41.8078, MinusLogProbMetric: 41.8078, val_loss: 42.4182, val_MinusLogProbMetric: 42.4182

Epoch 544: val_loss did not improve from 42.30536
196/196 - 32s - loss: 41.8078 - MinusLogProbMetric: 41.8078 - val_loss: 42.4182 - val_MinusLogProbMetric: 42.4182 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 545/1000
2023-10-27 09:33:22.055 
Epoch 545/1000 
	 loss: 41.2933, MinusLogProbMetric: 41.2933, val_loss: 42.9873, val_MinusLogProbMetric: 42.9873

Epoch 545: val_loss did not improve from 42.30536
196/196 - 32s - loss: 41.2933 - MinusLogProbMetric: 41.2933 - val_loss: 42.9873 - val_MinusLogProbMetric: 42.9873 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 546/1000
2023-10-27 09:33:54.990 
Epoch 546/1000 
	 loss: 41.3066, MinusLogProbMetric: 41.3066, val_loss: 42.2756, val_MinusLogProbMetric: 42.2756

Epoch 546: val_loss improved from 42.30536 to 42.27560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 41.3066 - MinusLogProbMetric: 41.3066 - val_loss: 42.2756 - val_MinusLogProbMetric: 42.2756 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 547/1000
2023-10-27 09:34:28.484 
Epoch 547/1000 
	 loss: 41.2828, MinusLogProbMetric: 41.2828, val_loss: 42.4594, val_MinusLogProbMetric: 42.4594

Epoch 547: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.2828 - MinusLogProbMetric: 41.2828 - val_loss: 42.4594 - val_MinusLogProbMetric: 42.4594 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 548/1000
2023-10-27 09:35:01.729 
Epoch 548/1000 
	 loss: 41.0420, MinusLogProbMetric: 41.0420, val_loss: 42.4412, val_MinusLogProbMetric: 42.4412

Epoch 548: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.0420 - MinusLogProbMetric: 41.0420 - val_loss: 42.4412 - val_MinusLogProbMetric: 42.4412 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 549/1000
2023-10-27 09:35:34.343 
Epoch 549/1000 
	 loss: 41.4694, MinusLogProbMetric: 41.4694, val_loss: 42.8426, val_MinusLogProbMetric: 42.8426

Epoch 549: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.4694 - MinusLogProbMetric: 41.4694 - val_loss: 42.8426 - val_MinusLogProbMetric: 42.8426 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 550/1000
2023-10-27 09:36:08.300 
Epoch 550/1000 
	 loss: 41.0724, MinusLogProbMetric: 41.0724, val_loss: 42.5535, val_MinusLogProbMetric: 42.5535

Epoch 550: val_loss did not improve from 42.27560
196/196 - 34s - loss: 41.0724 - MinusLogProbMetric: 41.0724 - val_loss: 42.5535 - val_MinusLogProbMetric: 42.5535 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 551/1000
2023-10-27 09:36:41.807 
Epoch 551/1000 
	 loss: 41.1299, MinusLogProbMetric: 41.1299, val_loss: 43.1530, val_MinusLogProbMetric: 43.1530

Epoch 551: val_loss did not improve from 42.27560
196/196 - 34s - loss: 41.1299 - MinusLogProbMetric: 41.1299 - val_loss: 43.1530 - val_MinusLogProbMetric: 43.1530 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 552/1000
2023-10-27 09:37:14.832 
Epoch 552/1000 
	 loss: 41.3454, MinusLogProbMetric: 41.3454, val_loss: 42.8552, val_MinusLogProbMetric: 42.8552

Epoch 552: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.3454 - MinusLogProbMetric: 41.3454 - val_loss: 42.8552 - val_MinusLogProbMetric: 42.8552 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 553/1000
2023-10-27 09:37:48.136 
Epoch 553/1000 
	 loss: 41.2597, MinusLogProbMetric: 41.2597, val_loss: 42.5687, val_MinusLogProbMetric: 42.5687

Epoch 553: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.2597 - MinusLogProbMetric: 41.2597 - val_loss: 42.5687 - val_MinusLogProbMetric: 42.5687 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 554/1000
2023-10-27 09:38:19.058 
Epoch 554/1000 
	 loss: 41.2202, MinusLogProbMetric: 41.2202, val_loss: 42.9268, val_MinusLogProbMetric: 42.9268

Epoch 554: val_loss did not improve from 42.27560
196/196 - 31s - loss: 41.2202 - MinusLogProbMetric: 41.2202 - val_loss: 42.9268 - val_MinusLogProbMetric: 42.9268 - lr: 5.5556e-05 - 31s/epoch - 158ms/step
Epoch 555/1000
2023-10-27 09:38:51.424 
Epoch 555/1000 
	 loss: 41.3567, MinusLogProbMetric: 41.3567, val_loss: 42.5095, val_MinusLogProbMetric: 42.5095

Epoch 555: val_loss did not improve from 42.27560
196/196 - 32s - loss: 41.3567 - MinusLogProbMetric: 41.3567 - val_loss: 42.5095 - val_MinusLogProbMetric: 42.5095 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 556/1000
2023-10-27 09:39:25.053 
Epoch 556/1000 
	 loss: 41.4878, MinusLogProbMetric: 41.4878, val_loss: 43.7278, val_MinusLogProbMetric: 43.7278

Epoch 556: val_loss did not improve from 42.27560
196/196 - 34s - loss: 41.4878 - MinusLogProbMetric: 41.4878 - val_loss: 43.7278 - val_MinusLogProbMetric: 43.7278 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 557/1000
2023-10-27 09:39:56.891 
Epoch 557/1000 
	 loss: 41.2971, MinusLogProbMetric: 41.2971, val_loss: 42.8048, val_MinusLogProbMetric: 42.8048

Epoch 557: val_loss did not improve from 42.27560
196/196 - 32s - loss: 41.2971 - MinusLogProbMetric: 41.2971 - val_loss: 42.8048 - val_MinusLogProbMetric: 42.8048 - lr: 5.5556e-05 - 32s/epoch - 162ms/step
Epoch 558/1000
2023-10-27 09:40:30.226 
Epoch 558/1000 
	 loss: 41.1059, MinusLogProbMetric: 41.1059, val_loss: 42.5280, val_MinusLogProbMetric: 42.5280

Epoch 558: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.1059 - MinusLogProbMetric: 41.1059 - val_loss: 42.5280 - val_MinusLogProbMetric: 42.5280 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 559/1000
2023-10-27 09:41:03.360 
Epoch 559/1000 
	 loss: 41.2272, MinusLogProbMetric: 41.2272, val_loss: 42.3815, val_MinusLogProbMetric: 42.3815

Epoch 559: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.2272 - MinusLogProbMetric: 41.2272 - val_loss: 42.3815 - val_MinusLogProbMetric: 42.3815 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 560/1000
2023-10-27 09:41:36.705 
Epoch 560/1000 
	 loss: 41.3197, MinusLogProbMetric: 41.3197, val_loss: 42.8102, val_MinusLogProbMetric: 42.8102

Epoch 560: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.3197 - MinusLogProbMetric: 41.3197 - val_loss: 42.8102 - val_MinusLogProbMetric: 42.8102 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 561/1000
2023-10-27 09:42:09.930 
Epoch 561/1000 
	 loss: 41.2748, MinusLogProbMetric: 41.2748, val_loss: 43.3440, val_MinusLogProbMetric: 43.3440

Epoch 561: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.2748 - MinusLogProbMetric: 41.2748 - val_loss: 43.3440 - val_MinusLogProbMetric: 43.3440 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 562/1000
2023-10-27 09:42:43.372 
Epoch 562/1000 
	 loss: 40.9989, MinusLogProbMetric: 40.9989, val_loss: 42.6946, val_MinusLogProbMetric: 42.6946

Epoch 562: val_loss did not improve from 42.27560
196/196 - 33s - loss: 40.9989 - MinusLogProbMetric: 40.9989 - val_loss: 42.6946 - val_MinusLogProbMetric: 42.6946 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 563/1000
2023-10-27 09:43:15.876 
Epoch 563/1000 
	 loss: 41.3808, MinusLogProbMetric: 41.3808, val_loss: 42.5581, val_MinusLogProbMetric: 42.5581

Epoch 563: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.3808 - MinusLogProbMetric: 41.3808 - val_loss: 42.5581 - val_MinusLogProbMetric: 42.5581 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 564/1000
2023-10-27 09:43:49.166 
Epoch 564/1000 
	 loss: 41.1127, MinusLogProbMetric: 41.1127, val_loss: 42.3816, val_MinusLogProbMetric: 42.3816

Epoch 564: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.1127 - MinusLogProbMetric: 41.1127 - val_loss: 42.3816 - val_MinusLogProbMetric: 42.3816 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 565/1000
2023-10-27 09:44:22.262 
Epoch 565/1000 
	 loss: 41.2354, MinusLogProbMetric: 41.2354, val_loss: 42.5870, val_MinusLogProbMetric: 42.5870

Epoch 565: val_loss did not improve from 42.27560
196/196 - 33s - loss: 41.2354 - MinusLogProbMetric: 41.2354 - val_loss: 42.5870 - val_MinusLogProbMetric: 42.5870 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 566/1000
2023-10-27 09:44:55.906 
Epoch 566/1000 
	 loss: 41.1820, MinusLogProbMetric: 41.1820, val_loss: 42.5965, val_MinusLogProbMetric: 42.5965

Epoch 566: val_loss did not improve from 42.27560
196/196 - 34s - loss: 41.1820 - MinusLogProbMetric: 41.1820 - val_loss: 42.5965 - val_MinusLogProbMetric: 42.5965 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 567/1000
2023-10-27 09:45:28.918 
Epoch 567/1000 
	 loss: 41.0960, MinusLogProbMetric: 41.0960, val_loss: 42.2594, val_MinusLogProbMetric: 42.2594

Epoch 567: val_loss improved from 42.27560 to 42.25941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.0960 - MinusLogProbMetric: 41.0960 - val_loss: 42.2594 - val_MinusLogProbMetric: 42.2594 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 568/1000
2023-10-27 09:46:01.846 
Epoch 568/1000 
	 loss: 41.3938, MinusLogProbMetric: 41.3938, val_loss: 42.3283, val_MinusLogProbMetric: 42.3283

Epoch 568: val_loss did not improve from 42.25941
196/196 - 32s - loss: 41.3938 - MinusLogProbMetric: 41.3938 - val_loss: 42.3283 - val_MinusLogProbMetric: 42.3283 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 569/1000
2023-10-27 09:46:34.178 
Epoch 569/1000 
	 loss: 41.0723, MinusLogProbMetric: 41.0723, val_loss: 42.8098, val_MinusLogProbMetric: 42.8098

Epoch 569: val_loss did not improve from 42.25941
196/196 - 32s - loss: 41.0723 - MinusLogProbMetric: 41.0723 - val_loss: 42.8098 - val_MinusLogProbMetric: 42.8098 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 570/1000
2023-10-27 09:47:05.831 
Epoch 570/1000 
	 loss: 41.0635, MinusLogProbMetric: 41.0635, val_loss: 42.6539, val_MinusLogProbMetric: 42.6539

Epoch 570: val_loss did not improve from 42.25941
196/196 - 32s - loss: 41.0635 - MinusLogProbMetric: 41.0635 - val_loss: 42.6539 - val_MinusLogProbMetric: 42.6539 - lr: 5.5556e-05 - 32s/epoch - 161ms/step
Epoch 571/1000
2023-10-27 09:47:38.629 
Epoch 571/1000 
	 loss: 41.2889, MinusLogProbMetric: 41.2889, val_loss: 42.8993, val_MinusLogProbMetric: 42.8993

Epoch 571: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.2889 - MinusLogProbMetric: 41.2889 - val_loss: 42.8993 - val_MinusLogProbMetric: 42.8993 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 572/1000
2023-10-27 09:48:11.690 
Epoch 572/1000 
	 loss: 41.1996, MinusLogProbMetric: 41.1996, val_loss: 42.4850, val_MinusLogProbMetric: 42.4850

Epoch 572: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.1996 - MinusLogProbMetric: 41.1996 - val_loss: 42.4850 - val_MinusLogProbMetric: 42.4850 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 573/1000
2023-10-27 09:48:44.210 
Epoch 573/1000 
	 loss: 41.2877, MinusLogProbMetric: 41.2877, val_loss: 43.4756, val_MinusLogProbMetric: 43.4756

Epoch 573: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.2877 - MinusLogProbMetric: 41.2877 - val_loss: 43.4756 - val_MinusLogProbMetric: 43.4756 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 574/1000
2023-10-27 09:49:17.492 
Epoch 574/1000 
	 loss: 41.0458, MinusLogProbMetric: 41.0458, val_loss: 42.9556, val_MinusLogProbMetric: 42.9556

Epoch 574: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.0458 - MinusLogProbMetric: 41.0458 - val_loss: 42.9556 - val_MinusLogProbMetric: 42.9556 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 575/1000
2023-10-27 09:49:50.673 
Epoch 575/1000 
	 loss: 41.1206, MinusLogProbMetric: 41.1206, val_loss: 42.2942, val_MinusLogProbMetric: 42.2942

Epoch 575: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.1206 - MinusLogProbMetric: 41.1206 - val_loss: 42.2942 - val_MinusLogProbMetric: 42.2942 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 576/1000
2023-10-27 09:50:24.479 
Epoch 576/1000 
	 loss: 41.0643, MinusLogProbMetric: 41.0643, val_loss: 42.7675, val_MinusLogProbMetric: 42.7675

Epoch 576: val_loss did not improve from 42.25941
196/196 - 34s - loss: 41.0643 - MinusLogProbMetric: 41.0643 - val_loss: 42.7675 - val_MinusLogProbMetric: 42.7675 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 577/1000
2023-10-27 09:50:57.283 
Epoch 577/1000 
	 loss: 41.0892, MinusLogProbMetric: 41.0892, val_loss: 43.0673, val_MinusLogProbMetric: 43.0673

Epoch 577: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.0892 - MinusLogProbMetric: 41.0892 - val_loss: 43.0673 - val_MinusLogProbMetric: 43.0673 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 578/1000
2023-10-27 09:51:29.865 
Epoch 578/1000 
	 loss: 41.2628, MinusLogProbMetric: 41.2628, val_loss: 42.3112, val_MinusLogProbMetric: 42.3112

Epoch 578: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.2628 - MinusLogProbMetric: 41.2628 - val_loss: 42.3112 - val_MinusLogProbMetric: 42.3112 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 579/1000
2023-10-27 09:52:03.354 
Epoch 579/1000 
	 loss: 41.3017, MinusLogProbMetric: 41.3017, val_loss: 42.9004, val_MinusLogProbMetric: 42.9004

Epoch 579: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.3017 - MinusLogProbMetric: 41.3017 - val_loss: 42.9004 - val_MinusLogProbMetric: 42.9004 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 580/1000
2023-10-27 09:52:36.489 
Epoch 580/1000 
	 loss: 41.0077, MinusLogProbMetric: 41.0077, val_loss: 42.4959, val_MinusLogProbMetric: 42.4959

Epoch 580: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.0077 - MinusLogProbMetric: 41.0077 - val_loss: 42.4959 - val_MinusLogProbMetric: 42.4959 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 581/1000
2023-10-27 09:53:09.336 
Epoch 581/1000 
	 loss: 41.1300, MinusLogProbMetric: 41.1300, val_loss: 42.6435, val_MinusLogProbMetric: 42.6435

Epoch 581: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.1300 - MinusLogProbMetric: 41.1300 - val_loss: 42.6435 - val_MinusLogProbMetric: 42.6435 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 582/1000
2023-10-27 09:53:41.744 
Epoch 582/1000 
	 loss: 41.1566, MinusLogProbMetric: 41.1566, val_loss: 42.9388, val_MinusLogProbMetric: 42.9388

Epoch 582: val_loss did not improve from 42.25941
196/196 - 32s - loss: 41.1566 - MinusLogProbMetric: 41.1566 - val_loss: 42.9388 - val_MinusLogProbMetric: 42.9388 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 583/1000
2023-10-27 09:54:14.930 
Epoch 583/1000 
	 loss: 41.0860, MinusLogProbMetric: 41.0860, val_loss: 43.0625, val_MinusLogProbMetric: 43.0625

Epoch 583: val_loss did not improve from 42.25941
196/196 - 33s - loss: 41.0860 - MinusLogProbMetric: 41.0860 - val_loss: 43.0625 - val_MinusLogProbMetric: 43.0625 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 584/1000
2023-10-27 09:54:47.881 
Epoch 584/1000 
	 loss: 41.2100, MinusLogProbMetric: 41.2100, val_loss: 42.2495, val_MinusLogProbMetric: 42.2495

Epoch 584: val_loss improved from 42.25941 to 42.24948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.2100 - MinusLogProbMetric: 41.2100 - val_loss: 42.2495 - val_MinusLogProbMetric: 42.2495 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 585/1000
2023-10-27 09:55:21.818 
Epoch 585/1000 
	 loss: 41.0935, MinusLogProbMetric: 41.0935, val_loss: 42.3718, val_MinusLogProbMetric: 42.3718

Epoch 585: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.0935 - MinusLogProbMetric: 41.0935 - val_loss: 42.3718 - val_MinusLogProbMetric: 42.3718 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 586/1000
2023-10-27 09:55:54.216 
Epoch 586/1000 
	 loss: 41.1263, MinusLogProbMetric: 41.1263, val_loss: 42.2718, val_MinusLogProbMetric: 42.2718

Epoch 586: val_loss did not improve from 42.24948
196/196 - 32s - loss: 41.1263 - MinusLogProbMetric: 41.1263 - val_loss: 42.2718 - val_MinusLogProbMetric: 42.2718 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 587/1000
2023-10-27 09:56:27.393 
Epoch 587/1000 
	 loss: 41.1713, MinusLogProbMetric: 41.1713, val_loss: 42.7727, val_MinusLogProbMetric: 42.7727

Epoch 587: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.1713 - MinusLogProbMetric: 41.1713 - val_loss: 42.7727 - val_MinusLogProbMetric: 42.7727 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 588/1000
2023-10-27 09:57:00.652 
Epoch 588/1000 
	 loss: 41.0807, MinusLogProbMetric: 41.0807, val_loss: 43.0101, val_MinusLogProbMetric: 43.0101

Epoch 588: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.0807 - MinusLogProbMetric: 41.0807 - val_loss: 43.0101 - val_MinusLogProbMetric: 43.0101 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 589/1000
2023-10-27 09:57:34.174 
Epoch 589/1000 
	 loss: 41.2266, MinusLogProbMetric: 41.2266, val_loss: 42.8663, val_MinusLogProbMetric: 42.8663

Epoch 589: val_loss did not improve from 42.24948
196/196 - 34s - loss: 41.2266 - MinusLogProbMetric: 41.2266 - val_loss: 42.8663 - val_MinusLogProbMetric: 42.8663 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 590/1000
2023-10-27 09:58:07.693 
Epoch 590/1000 
	 loss: 41.0314, MinusLogProbMetric: 41.0314, val_loss: 42.8543, val_MinusLogProbMetric: 42.8543

Epoch 590: val_loss did not improve from 42.24948
196/196 - 34s - loss: 41.0314 - MinusLogProbMetric: 41.0314 - val_loss: 42.8543 - val_MinusLogProbMetric: 42.8543 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 591/1000
2023-10-27 09:58:39.963 
Epoch 591/1000 
	 loss: 41.2072, MinusLogProbMetric: 41.2072, val_loss: 42.5521, val_MinusLogProbMetric: 42.5521

Epoch 591: val_loss did not improve from 42.24948
196/196 - 32s - loss: 41.2072 - MinusLogProbMetric: 41.2072 - val_loss: 42.5521 - val_MinusLogProbMetric: 42.5521 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 592/1000
2023-10-27 09:59:12.936 
Epoch 592/1000 
	 loss: 41.1140, MinusLogProbMetric: 41.1140, val_loss: 42.9624, val_MinusLogProbMetric: 42.9624

Epoch 592: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.1140 - MinusLogProbMetric: 41.1140 - val_loss: 42.9624 - val_MinusLogProbMetric: 42.9624 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 593/1000
2023-10-27 09:59:45.797 
Epoch 593/1000 
	 loss: 41.3870, MinusLogProbMetric: 41.3870, val_loss: 42.3481, val_MinusLogProbMetric: 42.3481

Epoch 593: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.3870 - MinusLogProbMetric: 41.3870 - val_loss: 42.3481 - val_MinusLogProbMetric: 42.3481 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 594/1000
2023-10-27 10:00:18.138 
Epoch 594/1000 
	 loss: 41.2500, MinusLogProbMetric: 41.2500, val_loss: 42.6115, val_MinusLogProbMetric: 42.6115

Epoch 594: val_loss did not improve from 42.24948
196/196 - 32s - loss: 41.2500 - MinusLogProbMetric: 41.2500 - val_loss: 42.6115 - val_MinusLogProbMetric: 42.6115 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 595/1000
2023-10-27 10:00:51.362 
Epoch 595/1000 
	 loss: 41.0607, MinusLogProbMetric: 41.0607, val_loss: 42.8112, val_MinusLogProbMetric: 42.8112

Epoch 595: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.0607 - MinusLogProbMetric: 41.0607 - val_loss: 42.8112 - val_MinusLogProbMetric: 42.8112 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 596/1000
2023-10-27 10:01:24.220 
Epoch 596/1000 
	 loss: 41.0742, MinusLogProbMetric: 41.0742, val_loss: 43.5769, val_MinusLogProbMetric: 43.5769

Epoch 596: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.0742 - MinusLogProbMetric: 41.0742 - val_loss: 43.5769 - val_MinusLogProbMetric: 43.5769 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 597/1000
2023-10-27 10:01:58.018 
Epoch 597/1000 
	 loss: 41.1174, MinusLogProbMetric: 41.1174, val_loss: 42.5145, val_MinusLogProbMetric: 42.5145

Epoch 597: val_loss did not improve from 42.24948
196/196 - 34s - loss: 41.1174 - MinusLogProbMetric: 41.1174 - val_loss: 42.5145 - val_MinusLogProbMetric: 42.5145 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 598/1000
2023-10-27 10:02:31.319 
Epoch 598/1000 
	 loss: 41.3913, MinusLogProbMetric: 41.3913, val_loss: 42.3159, val_MinusLogProbMetric: 42.3159

Epoch 598: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.3913 - MinusLogProbMetric: 41.3913 - val_loss: 42.3159 - val_MinusLogProbMetric: 42.3159 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 599/1000
2023-10-27 10:03:04.331 
Epoch 599/1000 
	 loss: 41.1081, MinusLogProbMetric: 41.1081, val_loss: 43.0239, val_MinusLogProbMetric: 43.0239

Epoch 599: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.1081 - MinusLogProbMetric: 41.1081 - val_loss: 43.0239 - val_MinusLogProbMetric: 43.0239 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 600/1000
2023-10-27 10:03:36.777 
Epoch 600/1000 
	 loss: 41.0101, MinusLogProbMetric: 41.0101, val_loss: 42.4791, val_MinusLogProbMetric: 42.4791

Epoch 600: val_loss did not improve from 42.24948
196/196 - 32s - loss: 41.0101 - MinusLogProbMetric: 41.0101 - val_loss: 42.4791 - val_MinusLogProbMetric: 42.4791 - lr: 5.5556e-05 - 32s/epoch - 166ms/step
Epoch 601/1000
2023-10-27 10:04:09.726 
Epoch 601/1000 
	 loss: 41.1163, MinusLogProbMetric: 41.1163, val_loss: 42.9994, val_MinusLogProbMetric: 42.9994

Epoch 601: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.1163 - MinusLogProbMetric: 41.1163 - val_loss: 42.9994 - val_MinusLogProbMetric: 42.9994 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 602/1000
2023-10-27 10:04:42.540 
Epoch 602/1000 
	 loss: 41.1148, MinusLogProbMetric: 41.1148, val_loss: 42.4197, val_MinusLogProbMetric: 42.4197

Epoch 602: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.1148 - MinusLogProbMetric: 41.1148 - val_loss: 42.4197 - val_MinusLogProbMetric: 42.4197 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 603/1000
2023-10-27 10:05:15.642 
Epoch 603/1000 
	 loss: 41.2120, MinusLogProbMetric: 41.2120, val_loss: 42.5497, val_MinusLogProbMetric: 42.5497

Epoch 603: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.2120 - MinusLogProbMetric: 41.2120 - val_loss: 42.5497 - val_MinusLogProbMetric: 42.5497 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 604/1000
2023-10-27 10:05:48.478 
Epoch 604/1000 
	 loss: 41.2639, MinusLogProbMetric: 41.2639, val_loss: 42.3042, val_MinusLogProbMetric: 42.3042

Epoch 604: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.2639 - MinusLogProbMetric: 41.2639 - val_loss: 42.3042 - val_MinusLogProbMetric: 42.3042 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 605/1000
2023-10-27 10:06:21.479 
Epoch 605/1000 
	 loss: 41.0710, MinusLogProbMetric: 41.0710, val_loss: 43.9114, val_MinusLogProbMetric: 43.9114

Epoch 605: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.0710 - MinusLogProbMetric: 41.0710 - val_loss: 43.9114 - val_MinusLogProbMetric: 43.9114 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 606/1000
2023-10-27 10:06:54.563 
Epoch 606/1000 
	 loss: 41.2938, MinusLogProbMetric: 41.2938, val_loss: 42.2630, val_MinusLogProbMetric: 42.2630

Epoch 606: val_loss did not improve from 42.24948
196/196 - 33s - loss: 41.2938 - MinusLogProbMetric: 41.2938 - val_loss: 42.2630 - val_MinusLogProbMetric: 42.2630 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 607/1000
2023-10-27 10:07:28.142 
Epoch 607/1000 
	 loss: 41.0810, MinusLogProbMetric: 41.0810, val_loss: 42.3231, val_MinusLogProbMetric: 42.3231

Epoch 607: val_loss did not improve from 42.24948
196/196 - 34s - loss: 41.0810 - MinusLogProbMetric: 41.0810 - val_loss: 42.3231 - val_MinusLogProbMetric: 42.3231 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 608/1000
2023-10-27 10:08:01.665 
Epoch 608/1000 
	 loss: 41.0901, MinusLogProbMetric: 41.0901, val_loss: 42.1940, val_MinusLogProbMetric: 42.1940

Epoch 608: val_loss improved from 42.24948 to 42.19402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.0901 - MinusLogProbMetric: 41.0901 - val_loss: 42.1940 - val_MinusLogProbMetric: 42.1940 - lr: 5.5556e-05 - 34s/epoch - 174ms/step
Epoch 609/1000
2023-10-27 10:08:35.738 
Epoch 609/1000 
	 loss: 41.0800, MinusLogProbMetric: 41.0800, val_loss: 42.4436, val_MinusLogProbMetric: 42.4436

Epoch 609: val_loss did not improve from 42.19402
196/196 - 33s - loss: 41.0800 - MinusLogProbMetric: 41.0800 - val_loss: 42.4436 - val_MinusLogProbMetric: 42.4436 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 610/1000
2023-10-27 10:09:09.091 
Epoch 610/1000 
	 loss: 41.1381, MinusLogProbMetric: 41.1381, val_loss: 42.2682, val_MinusLogProbMetric: 42.2682

Epoch 610: val_loss did not improve from 42.19402
196/196 - 33s - loss: 41.1381 - MinusLogProbMetric: 41.1381 - val_loss: 42.2682 - val_MinusLogProbMetric: 42.2682 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 611/1000
2023-10-27 10:09:42.656 
Epoch 611/1000 
	 loss: 40.9873, MinusLogProbMetric: 40.9873, val_loss: 42.1977, val_MinusLogProbMetric: 42.1977

Epoch 611: val_loss did not improve from 42.19402
196/196 - 34s - loss: 40.9873 - MinusLogProbMetric: 40.9873 - val_loss: 42.1977 - val_MinusLogProbMetric: 42.1977 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 612/1000
2023-10-27 10:10:15.727 
Epoch 612/1000 
	 loss: 41.0164, MinusLogProbMetric: 41.0164, val_loss: 42.1439, val_MinusLogProbMetric: 42.1439

Epoch 612: val_loss improved from 42.19402 to 42.14387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 41.0164 - MinusLogProbMetric: 41.0164 - val_loss: 42.1439 - val_MinusLogProbMetric: 42.1439 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 613/1000
2023-10-27 10:10:49.886 
Epoch 613/1000 
	 loss: 41.0235, MinusLogProbMetric: 41.0235, val_loss: 42.4050, val_MinusLogProbMetric: 42.4050

Epoch 613: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0235 - MinusLogProbMetric: 41.0235 - val_loss: 42.4050 - val_MinusLogProbMetric: 42.4050 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 614/1000
2023-10-27 10:11:22.782 
Epoch 614/1000 
	 loss: 41.2224, MinusLogProbMetric: 41.2224, val_loss: 42.6382, val_MinusLogProbMetric: 42.6382

Epoch 614: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.2224 - MinusLogProbMetric: 41.2224 - val_loss: 42.6382 - val_MinusLogProbMetric: 42.6382 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 615/1000
2023-10-27 10:11:55.805 
Epoch 615/1000 
	 loss: 41.2348, MinusLogProbMetric: 41.2348, val_loss: 42.2711, val_MinusLogProbMetric: 42.2711

Epoch 615: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.2348 - MinusLogProbMetric: 41.2348 - val_loss: 42.2711 - val_MinusLogProbMetric: 42.2711 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 616/1000
2023-10-27 10:12:29.023 
Epoch 616/1000 
	 loss: 41.0345, MinusLogProbMetric: 41.0345, val_loss: 42.4167, val_MinusLogProbMetric: 42.4167

Epoch 616: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0345 - MinusLogProbMetric: 41.0345 - val_loss: 42.4167 - val_MinusLogProbMetric: 42.4167 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 617/1000
2023-10-27 10:13:01.956 
Epoch 617/1000 
	 loss: 41.0827, MinusLogProbMetric: 41.0827, val_loss: 42.9570, val_MinusLogProbMetric: 42.9570

Epoch 617: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0827 - MinusLogProbMetric: 41.0827 - val_loss: 42.9570 - val_MinusLogProbMetric: 42.9570 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 618/1000
2023-10-27 10:13:34.968 
Epoch 618/1000 
	 loss: 41.2537, MinusLogProbMetric: 41.2537, val_loss: 42.1881, val_MinusLogProbMetric: 42.1881

Epoch 618: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.2537 - MinusLogProbMetric: 41.2537 - val_loss: 42.1881 - val_MinusLogProbMetric: 42.1881 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 619/1000
2023-10-27 10:14:08.850 
Epoch 619/1000 
	 loss: 41.0803, MinusLogProbMetric: 41.0803, val_loss: 42.5477, val_MinusLogProbMetric: 42.5477

Epoch 619: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0803 - MinusLogProbMetric: 41.0803 - val_loss: 42.5477 - val_MinusLogProbMetric: 42.5477 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 620/1000
2023-10-27 10:14:42.843 
Epoch 620/1000 
	 loss: 41.0057, MinusLogProbMetric: 41.0057, val_loss: 42.8007, val_MinusLogProbMetric: 42.8007

Epoch 620: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0057 - MinusLogProbMetric: 41.0057 - val_loss: 42.8007 - val_MinusLogProbMetric: 42.8007 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 621/1000
2023-10-27 10:15:17.308 
Epoch 621/1000 
	 loss: 44.5242, MinusLogProbMetric: 44.5242, val_loss: 44.7000, val_MinusLogProbMetric: 44.7000

Epoch 621: val_loss did not improve from 42.14387
196/196 - 34s - loss: 44.5242 - MinusLogProbMetric: 44.5242 - val_loss: 44.7000 - val_MinusLogProbMetric: 44.7000 - lr: 5.5556e-05 - 34s/epoch - 176ms/step
Epoch 622/1000
2023-10-27 10:15:51.631 
Epoch 622/1000 
	 loss: 41.9761, MinusLogProbMetric: 41.9761, val_loss: 42.4818, val_MinusLogProbMetric: 42.4818

Epoch 622: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.9761 - MinusLogProbMetric: 41.9761 - val_loss: 42.4818 - val_MinusLogProbMetric: 42.4818 - lr: 5.5556e-05 - 34s/epoch - 175ms/step
Epoch 623/1000
2023-10-27 10:16:25.766 
Epoch 623/1000 
	 loss: 41.3295, MinusLogProbMetric: 41.3295, val_loss: 42.5198, val_MinusLogProbMetric: 42.5198

Epoch 623: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.3295 - MinusLogProbMetric: 41.3295 - val_loss: 42.5198 - val_MinusLogProbMetric: 42.5198 - lr: 5.5556e-05 - 34s/epoch - 174ms/step
Epoch 624/1000
2023-10-27 10:16:59.517 
Epoch 624/1000 
	 loss: 41.3605, MinusLogProbMetric: 41.3605, val_loss: 43.2487, val_MinusLogProbMetric: 43.2487

Epoch 624: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.3605 - MinusLogProbMetric: 41.3605 - val_loss: 43.2487 - val_MinusLogProbMetric: 43.2487 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 625/1000
2023-10-27 10:17:33.464 
Epoch 625/1000 
	 loss: 41.1996, MinusLogProbMetric: 41.1996, val_loss: 43.4133, val_MinusLogProbMetric: 43.4133

Epoch 625: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.1996 - MinusLogProbMetric: 41.1996 - val_loss: 43.4133 - val_MinusLogProbMetric: 43.4133 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 626/1000
2023-10-27 10:18:07.271 
Epoch 626/1000 
	 loss: 41.1367, MinusLogProbMetric: 41.1367, val_loss: 42.9816, val_MinusLogProbMetric: 42.9816

Epoch 626: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.1367 - MinusLogProbMetric: 41.1367 - val_loss: 42.9816 - val_MinusLogProbMetric: 42.9816 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 627/1000
2023-10-27 10:18:41.451 
Epoch 627/1000 
	 loss: 41.2439, MinusLogProbMetric: 41.2439, val_loss: 42.5295, val_MinusLogProbMetric: 42.5295

Epoch 627: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.2439 - MinusLogProbMetric: 41.2439 - val_loss: 42.5295 - val_MinusLogProbMetric: 42.5295 - lr: 5.5556e-05 - 34s/epoch - 174ms/step
Epoch 628/1000
2023-10-27 10:19:15.381 
Epoch 628/1000 
	 loss: 41.1869, MinusLogProbMetric: 41.1869, val_loss: 42.6413, val_MinusLogProbMetric: 42.6413

Epoch 628: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.1869 - MinusLogProbMetric: 41.1869 - val_loss: 42.6413 - val_MinusLogProbMetric: 42.6413 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 629/1000
2023-10-27 10:19:49.514 
Epoch 629/1000 
	 loss: 41.1534, MinusLogProbMetric: 41.1534, val_loss: 42.3059, val_MinusLogProbMetric: 42.3059

Epoch 629: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.1534 - MinusLogProbMetric: 41.1534 - val_loss: 42.3059 - val_MinusLogProbMetric: 42.3059 - lr: 5.5556e-05 - 34s/epoch - 174ms/step
Epoch 630/1000
2023-10-27 10:20:23.376 
Epoch 630/1000 
	 loss: 41.1423, MinusLogProbMetric: 41.1423, val_loss: 42.6514, val_MinusLogProbMetric: 42.6514

Epoch 630: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.1423 - MinusLogProbMetric: 41.1423 - val_loss: 42.6514 - val_MinusLogProbMetric: 42.6514 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 631/1000
2023-10-27 10:20:57.599 
Epoch 631/1000 
	 loss: 41.0605, MinusLogProbMetric: 41.0605, val_loss: 42.7750, val_MinusLogProbMetric: 42.7750

Epoch 631: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0605 - MinusLogProbMetric: 41.0605 - val_loss: 42.7750 - val_MinusLogProbMetric: 42.7750 - lr: 5.5556e-05 - 34s/epoch - 174ms/step
Epoch 632/1000
2023-10-27 10:21:31.546 
Epoch 632/1000 
	 loss: 41.3550, MinusLogProbMetric: 41.3550, val_loss: 42.8758, val_MinusLogProbMetric: 42.8758

Epoch 632: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.3550 - MinusLogProbMetric: 41.3550 - val_loss: 42.8758 - val_MinusLogProbMetric: 42.8758 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 633/1000
2023-10-27 10:22:05.398 
Epoch 633/1000 
	 loss: 41.0049, MinusLogProbMetric: 41.0049, val_loss: 42.2940, val_MinusLogProbMetric: 42.2940

Epoch 633: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0049 - MinusLogProbMetric: 41.0049 - val_loss: 42.2940 - val_MinusLogProbMetric: 42.2940 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 634/1000
2023-10-27 10:22:39.549 
Epoch 634/1000 
	 loss: 41.1767, MinusLogProbMetric: 41.1767, val_loss: 42.4777, val_MinusLogProbMetric: 42.4777

Epoch 634: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.1767 - MinusLogProbMetric: 41.1767 - val_loss: 42.4777 - val_MinusLogProbMetric: 42.4777 - lr: 5.5556e-05 - 34s/epoch - 174ms/step
Epoch 635/1000
2023-10-27 10:23:13.860 
Epoch 635/1000 
	 loss: 41.0013, MinusLogProbMetric: 41.0013, val_loss: 42.7228, val_MinusLogProbMetric: 42.7228

Epoch 635: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0013 - MinusLogProbMetric: 41.0013 - val_loss: 42.7228 - val_MinusLogProbMetric: 42.7228 - lr: 5.5556e-05 - 34s/epoch - 175ms/step
Epoch 636/1000
2023-10-27 10:23:47.797 
Epoch 636/1000 
	 loss: 41.0327, MinusLogProbMetric: 41.0327, val_loss: 42.9327, val_MinusLogProbMetric: 42.9327

Epoch 636: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0327 - MinusLogProbMetric: 41.0327 - val_loss: 42.9327 - val_MinusLogProbMetric: 42.9327 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 637/1000
2023-10-27 10:24:21.519 
Epoch 637/1000 
	 loss: 41.0380, MinusLogProbMetric: 41.0380, val_loss: 43.3469, val_MinusLogProbMetric: 43.3469

Epoch 637: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0380 - MinusLogProbMetric: 41.0380 - val_loss: 43.3469 - val_MinusLogProbMetric: 43.3469 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 638/1000
2023-10-27 10:24:55.148 
Epoch 638/1000 
	 loss: 41.2504, MinusLogProbMetric: 41.2504, val_loss: 43.5713, val_MinusLogProbMetric: 43.5713

Epoch 638: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.2504 - MinusLogProbMetric: 41.2504 - val_loss: 43.5713 - val_MinusLogProbMetric: 43.5713 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 639/1000
2023-10-27 10:25:27.279 
Epoch 639/1000 
	 loss: 41.2331, MinusLogProbMetric: 41.2331, val_loss: 43.0327, val_MinusLogProbMetric: 43.0327

Epoch 639: val_loss did not improve from 42.14387
196/196 - 32s - loss: 41.2331 - MinusLogProbMetric: 41.2331 - val_loss: 43.0327 - val_MinusLogProbMetric: 43.0327 - lr: 5.5556e-05 - 32s/epoch - 164ms/step
Epoch 640/1000
2023-10-27 10:26:00.603 
Epoch 640/1000 
	 loss: 40.9210, MinusLogProbMetric: 40.9210, val_loss: 44.1908, val_MinusLogProbMetric: 44.1908

Epoch 640: val_loss did not improve from 42.14387
196/196 - 33s - loss: 40.9210 - MinusLogProbMetric: 40.9210 - val_loss: 44.1908 - val_MinusLogProbMetric: 44.1908 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 641/1000
2023-10-27 10:26:34.419 
Epoch 641/1000 
	 loss: 40.9824, MinusLogProbMetric: 40.9824, val_loss: 42.2467, val_MinusLogProbMetric: 42.2467

Epoch 641: val_loss did not improve from 42.14387
196/196 - 34s - loss: 40.9824 - MinusLogProbMetric: 40.9824 - val_loss: 42.2467 - val_MinusLogProbMetric: 42.2467 - lr: 5.5556e-05 - 34s/epoch - 173ms/step
Epoch 642/1000
2023-10-27 10:27:08.171 
Epoch 642/1000 
	 loss: 41.0090, MinusLogProbMetric: 41.0090, val_loss: 44.0323, val_MinusLogProbMetric: 44.0323

Epoch 642: val_loss did not improve from 42.14387
196/196 - 34s - loss: 41.0090 - MinusLogProbMetric: 41.0090 - val_loss: 44.0323 - val_MinusLogProbMetric: 44.0323 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 643/1000
2023-10-27 10:27:40.537 
Epoch 643/1000 
	 loss: 40.9777, MinusLogProbMetric: 40.9777, val_loss: 42.6681, val_MinusLogProbMetric: 42.6681

Epoch 643: val_loss did not improve from 42.14387
196/196 - 32s - loss: 40.9777 - MinusLogProbMetric: 40.9777 - val_loss: 42.6681 - val_MinusLogProbMetric: 42.6681 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 644/1000
2023-10-27 10:28:13.638 
Epoch 644/1000 
	 loss: 41.0389, MinusLogProbMetric: 41.0389, val_loss: 43.0328, val_MinusLogProbMetric: 43.0328

Epoch 644: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0389 - MinusLogProbMetric: 41.0389 - val_loss: 43.0328 - val_MinusLogProbMetric: 43.0328 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 645/1000
2023-10-27 10:28:46.653 
Epoch 645/1000 
	 loss: 41.0077, MinusLogProbMetric: 41.0077, val_loss: 44.5654, val_MinusLogProbMetric: 44.5654

Epoch 645: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0077 - MinusLogProbMetric: 41.0077 - val_loss: 44.5654 - val_MinusLogProbMetric: 44.5654 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 646/1000
2023-10-27 10:29:19.965 
Epoch 646/1000 
	 loss: 41.0754, MinusLogProbMetric: 41.0754, val_loss: 42.2497, val_MinusLogProbMetric: 42.2497

Epoch 646: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0754 - MinusLogProbMetric: 41.0754 - val_loss: 42.2497 - val_MinusLogProbMetric: 42.2497 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 647/1000
2023-10-27 10:29:52.912 
Epoch 647/1000 
	 loss: 40.9347, MinusLogProbMetric: 40.9347, val_loss: 43.5052, val_MinusLogProbMetric: 43.5052

Epoch 647: val_loss did not improve from 42.14387
196/196 - 33s - loss: 40.9347 - MinusLogProbMetric: 40.9347 - val_loss: 43.5052 - val_MinusLogProbMetric: 43.5052 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 648/1000
2023-10-27 10:30:26.176 
Epoch 648/1000 
	 loss: 41.0289, MinusLogProbMetric: 41.0289, val_loss: 42.5193, val_MinusLogProbMetric: 42.5193

Epoch 648: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0289 - MinusLogProbMetric: 41.0289 - val_loss: 42.5193 - val_MinusLogProbMetric: 42.5193 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 649/1000
2023-10-27 10:30:59.654 
Epoch 649/1000 
	 loss: 41.0737, MinusLogProbMetric: 41.0737, val_loss: 42.2974, val_MinusLogProbMetric: 42.2974

Epoch 649: val_loss did not improve from 42.14387
196/196 - 33s - loss: 41.0737 - MinusLogProbMetric: 41.0737 - val_loss: 42.2974 - val_MinusLogProbMetric: 42.2974 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 650/1000
2023-10-27 10:31:31.056 
Epoch 650/1000 
	 loss: 41.0602, MinusLogProbMetric: 41.0602, val_loss: 42.1036, val_MinusLogProbMetric: 42.1036

Epoch 650: val_loss improved from 42.14387 to 42.10363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 32s - loss: 41.0602 - MinusLogProbMetric: 41.0602 - val_loss: 42.1036 - val_MinusLogProbMetric: 42.1036 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 651/1000
2023-10-27 10:32:04.766 
Epoch 651/1000 
	 loss: 40.9700, MinusLogProbMetric: 40.9700, val_loss: 43.6954, val_MinusLogProbMetric: 43.6954

Epoch 651: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9700 - MinusLogProbMetric: 40.9700 - val_loss: 43.6954 - val_MinusLogProbMetric: 43.6954 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 652/1000
2023-10-27 10:32:35.566 
Epoch 652/1000 
	 loss: 41.1812, MinusLogProbMetric: 41.1812, val_loss: 43.7640, val_MinusLogProbMetric: 43.7640

Epoch 652: val_loss did not improve from 42.10363
196/196 - 31s - loss: 41.1812 - MinusLogProbMetric: 41.1812 - val_loss: 43.7640 - val_MinusLogProbMetric: 43.7640 - lr: 5.5556e-05 - 31s/epoch - 157ms/step
Epoch 653/1000
2023-10-27 10:33:06.042 
Epoch 653/1000 
	 loss: 41.0707, MinusLogProbMetric: 41.0707, val_loss: 42.3661, val_MinusLogProbMetric: 42.3661

Epoch 653: val_loss did not improve from 42.10363
196/196 - 30s - loss: 41.0707 - MinusLogProbMetric: 41.0707 - val_loss: 42.3661 - val_MinusLogProbMetric: 42.3661 - lr: 5.5556e-05 - 30s/epoch - 155ms/step
Epoch 654/1000
2023-10-27 10:33:38.839 
Epoch 654/1000 
	 loss: 41.1765, MinusLogProbMetric: 41.1765, val_loss: 45.2410, val_MinusLogProbMetric: 45.2410

Epoch 654: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.1765 - MinusLogProbMetric: 41.1765 - val_loss: 45.2410 - val_MinusLogProbMetric: 45.2410 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 655/1000
2023-10-27 10:34:11.424 
Epoch 655/1000 
	 loss: 40.9778, MinusLogProbMetric: 40.9778, val_loss: 42.9562, val_MinusLogProbMetric: 42.9562

Epoch 655: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9778 - MinusLogProbMetric: 40.9778 - val_loss: 42.9562 - val_MinusLogProbMetric: 42.9562 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 656/1000
2023-10-27 10:34:43.090 
Epoch 656/1000 
	 loss: 41.1080, MinusLogProbMetric: 41.1080, val_loss: 42.9144, val_MinusLogProbMetric: 42.9144

Epoch 656: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.1080 - MinusLogProbMetric: 41.1080 - val_loss: 42.9144 - val_MinusLogProbMetric: 42.9144 - lr: 5.5556e-05 - 32s/epoch - 162ms/step
Epoch 657/1000
2023-10-27 10:35:14.581 
Epoch 657/1000 
	 loss: 40.9201, MinusLogProbMetric: 40.9201, val_loss: 42.6067, val_MinusLogProbMetric: 42.6067

Epoch 657: val_loss did not improve from 42.10363
196/196 - 31s - loss: 40.9201 - MinusLogProbMetric: 40.9201 - val_loss: 42.6067 - val_MinusLogProbMetric: 42.6067 - lr: 5.5556e-05 - 31s/epoch - 161ms/step
Epoch 658/1000
2023-10-27 10:35:46.451 
Epoch 658/1000 
	 loss: 41.2731, MinusLogProbMetric: 41.2731, val_loss: 42.8399, val_MinusLogProbMetric: 42.8399

Epoch 658: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.2731 - MinusLogProbMetric: 41.2731 - val_loss: 42.8399 - val_MinusLogProbMetric: 42.8399 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 659/1000
2023-10-27 10:36:18.610 
Epoch 659/1000 
	 loss: 40.9064, MinusLogProbMetric: 40.9064, val_loss: 42.9742, val_MinusLogProbMetric: 42.9742

Epoch 659: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.9064 - MinusLogProbMetric: 40.9064 - val_loss: 42.9742 - val_MinusLogProbMetric: 42.9742 - lr: 5.5556e-05 - 32s/epoch - 164ms/step
Epoch 660/1000
2023-10-27 10:36:50.613 
Epoch 660/1000 
	 loss: 40.8657, MinusLogProbMetric: 40.8657, val_loss: 42.9912, val_MinusLogProbMetric: 42.9912

Epoch 660: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.8657 - MinusLogProbMetric: 40.8657 - val_loss: 42.9912 - val_MinusLogProbMetric: 42.9912 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 661/1000
2023-10-27 10:37:23.402 
Epoch 661/1000 
	 loss: 41.0218, MinusLogProbMetric: 41.0218, val_loss: 42.4917, val_MinusLogProbMetric: 42.4917

Epoch 661: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0218 - MinusLogProbMetric: 41.0218 - val_loss: 42.4917 - val_MinusLogProbMetric: 42.4917 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 662/1000
2023-10-27 10:37:54.961 
Epoch 662/1000 
	 loss: 40.9467, MinusLogProbMetric: 40.9467, val_loss: 42.3800, val_MinusLogProbMetric: 42.3800

Epoch 662: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.9467 - MinusLogProbMetric: 40.9467 - val_loss: 42.3800 - val_MinusLogProbMetric: 42.3800 - lr: 5.5556e-05 - 32s/epoch - 161ms/step
Epoch 663/1000
2023-10-27 10:38:26.502 
Epoch 663/1000 
	 loss: 40.9815, MinusLogProbMetric: 40.9815, val_loss: 42.4491, val_MinusLogProbMetric: 42.4491

Epoch 663: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.9815 - MinusLogProbMetric: 40.9815 - val_loss: 42.4491 - val_MinusLogProbMetric: 42.4491 - lr: 5.5556e-05 - 32s/epoch - 161ms/step
Epoch 664/1000
2023-10-27 10:38:59.116 
Epoch 664/1000 
	 loss: 41.0003, MinusLogProbMetric: 41.0003, val_loss: 42.3228, val_MinusLogProbMetric: 42.3228

Epoch 664: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0003 - MinusLogProbMetric: 41.0003 - val_loss: 42.3228 - val_MinusLogProbMetric: 42.3228 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 665/1000
2023-10-27 10:39:31.990 
Epoch 665/1000 
	 loss: 41.1557, MinusLogProbMetric: 41.1557, val_loss: 42.3715, val_MinusLogProbMetric: 42.3715

Epoch 665: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.1557 - MinusLogProbMetric: 41.1557 - val_loss: 42.3715 - val_MinusLogProbMetric: 42.3715 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 666/1000
2023-10-27 10:40:04.667 
Epoch 666/1000 
	 loss: 40.9652, MinusLogProbMetric: 40.9652, val_loss: 42.9576, val_MinusLogProbMetric: 42.9576

Epoch 666: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9652 - MinusLogProbMetric: 40.9652 - val_loss: 42.9576 - val_MinusLogProbMetric: 42.9576 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 667/1000
2023-10-27 10:40:37.906 
Epoch 667/1000 
	 loss: 41.9184, MinusLogProbMetric: 41.9184, val_loss: 42.5151, val_MinusLogProbMetric: 42.5151

Epoch 667: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.9184 - MinusLogProbMetric: 41.9184 - val_loss: 42.5151 - val_MinusLogProbMetric: 42.5151 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 668/1000
2023-10-27 10:41:09.930 
Epoch 668/1000 
	 loss: 41.3371, MinusLogProbMetric: 41.3371, val_loss: 42.4504, val_MinusLogProbMetric: 42.4504

Epoch 668: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.3371 - MinusLogProbMetric: 41.3371 - val_loss: 42.4504 - val_MinusLogProbMetric: 42.4504 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 669/1000
2023-10-27 10:41:43.236 
Epoch 669/1000 
	 loss: 40.9368, MinusLogProbMetric: 40.9368, val_loss: 43.6361, val_MinusLogProbMetric: 43.6361

Epoch 669: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9368 - MinusLogProbMetric: 40.9368 - val_loss: 43.6361 - val_MinusLogProbMetric: 43.6361 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 670/1000
2023-10-27 10:42:15.978 
Epoch 670/1000 
	 loss: 40.9632, MinusLogProbMetric: 40.9632, val_loss: 42.4452, val_MinusLogProbMetric: 42.4452

Epoch 670: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9632 - MinusLogProbMetric: 40.9632 - val_loss: 42.4452 - val_MinusLogProbMetric: 42.4452 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 671/1000
2023-10-27 10:42:49.368 
Epoch 671/1000 
	 loss: 40.9078, MinusLogProbMetric: 40.9078, val_loss: 43.6068, val_MinusLogProbMetric: 43.6068

Epoch 671: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9078 - MinusLogProbMetric: 40.9078 - val_loss: 43.6068 - val_MinusLogProbMetric: 43.6068 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 672/1000
2023-10-27 10:43:22.629 
Epoch 672/1000 
	 loss: 41.0398, MinusLogProbMetric: 41.0398, val_loss: 42.4265, val_MinusLogProbMetric: 42.4265

Epoch 672: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0398 - MinusLogProbMetric: 41.0398 - val_loss: 42.4265 - val_MinusLogProbMetric: 42.4265 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 673/1000
2023-10-27 10:43:54.649 
Epoch 673/1000 
	 loss: 40.9738, MinusLogProbMetric: 40.9738, val_loss: 43.1842, val_MinusLogProbMetric: 43.1842

Epoch 673: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.9738 - MinusLogProbMetric: 40.9738 - val_loss: 43.1842 - val_MinusLogProbMetric: 43.1842 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 674/1000
2023-10-27 10:44:27.073 
Epoch 674/1000 
	 loss: 40.9603, MinusLogProbMetric: 40.9603, val_loss: 42.6536, val_MinusLogProbMetric: 42.6536

Epoch 674: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.9603 - MinusLogProbMetric: 40.9603 - val_loss: 42.6536 - val_MinusLogProbMetric: 42.6536 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 675/1000
2023-10-27 10:44:58.693 
Epoch 675/1000 
	 loss: 41.0049, MinusLogProbMetric: 41.0049, val_loss: 42.6006, val_MinusLogProbMetric: 42.6006

Epoch 675: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.0049 - MinusLogProbMetric: 41.0049 - val_loss: 42.6006 - val_MinusLogProbMetric: 42.6006 - lr: 5.5556e-05 - 32s/epoch - 161ms/step
Epoch 676/1000
2023-10-27 10:45:31.714 
Epoch 676/1000 
	 loss: 41.0958, MinusLogProbMetric: 41.0958, val_loss: 45.2016, val_MinusLogProbMetric: 45.2016

Epoch 676: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0958 - MinusLogProbMetric: 41.0958 - val_loss: 45.2016 - val_MinusLogProbMetric: 45.2016 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 677/1000
2023-10-27 10:46:03.659 
Epoch 677/1000 
	 loss: 41.0867, MinusLogProbMetric: 41.0867, val_loss: 42.2588, val_MinusLogProbMetric: 42.2588

Epoch 677: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.0867 - MinusLogProbMetric: 41.0867 - val_loss: 42.2588 - val_MinusLogProbMetric: 42.2588 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 678/1000
2023-10-27 10:46:36.580 
Epoch 678/1000 
	 loss: 40.9260, MinusLogProbMetric: 40.9260, val_loss: 42.6558, val_MinusLogProbMetric: 42.6558

Epoch 678: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9260 - MinusLogProbMetric: 40.9260 - val_loss: 42.6558 - val_MinusLogProbMetric: 42.6558 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 679/1000
2023-10-27 10:47:09.116 
Epoch 679/1000 
	 loss: 41.0727, MinusLogProbMetric: 41.0727, val_loss: 43.3170, val_MinusLogProbMetric: 43.3170

Epoch 679: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0727 - MinusLogProbMetric: 41.0727 - val_loss: 43.3170 - val_MinusLogProbMetric: 43.3170 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 680/1000
2023-10-27 10:47:41.275 
Epoch 680/1000 
	 loss: 41.0351, MinusLogProbMetric: 41.0351, val_loss: 42.3408, val_MinusLogProbMetric: 42.3408

Epoch 680: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.0351 - MinusLogProbMetric: 41.0351 - val_loss: 42.3408 - val_MinusLogProbMetric: 42.3408 - lr: 5.5556e-05 - 32s/epoch - 164ms/step
Epoch 681/1000
2023-10-27 10:48:14.563 
Epoch 681/1000 
	 loss: 41.0127, MinusLogProbMetric: 41.0127, val_loss: 42.6918, val_MinusLogProbMetric: 42.6918

Epoch 681: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0127 - MinusLogProbMetric: 41.0127 - val_loss: 42.6918 - val_MinusLogProbMetric: 42.6918 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 682/1000
2023-10-27 10:48:48.136 
Epoch 682/1000 
	 loss: 40.9254, MinusLogProbMetric: 40.9254, val_loss: 42.3253, val_MinusLogProbMetric: 42.3253

Epoch 682: val_loss did not improve from 42.10363
196/196 - 34s - loss: 40.9254 - MinusLogProbMetric: 40.9254 - val_loss: 42.3253 - val_MinusLogProbMetric: 42.3253 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 683/1000
2023-10-27 10:49:20.855 
Epoch 683/1000 
	 loss: 41.0881, MinusLogProbMetric: 41.0881, val_loss: 42.2771, val_MinusLogProbMetric: 42.2771

Epoch 683: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.0881 - MinusLogProbMetric: 41.0881 - val_loss: 42.2771 - val_MinusLogProbMetric: 42.2771 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 684/1000
2023-10-27 10:49:54.396 
Epoch 684/1000 
	 loss: 41.4529, MinusLogProbMetric: 41.4529, val_loss: 42.4196, val_MinusLogProbMetric: 42.4196

Epoch 684: val_loss did not improve from 42.10363
196/196 - 34s - loss: 41.4529 - MinusLogProbMetric: 41.4529 - val_loss: 42.4196 - val_MinusLogProbMetric: 42.4196 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 685/1000
2023-10-27 10:50:27.275 
Epoch 685/1000 
	 loss: 40.8157, MinusLogProbMetric: 40.8157, val_loss: 42.3140, val_MinusLogProbMetric: 42.3140

Epoch 685: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.8157 - MinusLogProbMetric: 40.8157 - val_loss: 42.3140 - val_MinusLogProbMetric: 42.3140 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 686/1000
2023-10-27 10:50:58.120 
Epoch 686/1000 
	 loss: 41.0658, MinusLogProbMetric: 41.0658, val_loss: 44.0527, val_MinusLogProbMetric: 44.0527

Epoch 686: val_loss did not improve from 42.10363
196/196 - 31s - loss: 41.0658 - MinusLogProbMetric: 41.0658 - val_loss: 44.0527 - val_MinusLogProbMetric: 44.0527 - lr: 5.5556e-05 - 31s/epoch - 157ms/step
Epoch 687/1000
2023-10-27 10:51:31.755 
Epoch 687/1000 
	 loss: 41.0429, MinusLogProbMetric: 41.0429, val_loss: 42.8666, val_MinusLogProbMetric: 42.8666

Epoch 687: val_loss did not improve from 42.10363
196/196 - 34s - loss: 41.0429 - MinusLogProbMetric: 41.0429 - val_loss: 42.8666 - val_MinusLogProbMetric: 42.8666 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 688/1000
2023-10-27 10:52:05.157 
Epoch 688/1000 
	 loss: 40.9827, MinusLogProbMetric: 40.9827, val_loss: 44.8890, val_MinusLogProbMetric: 44.8890

Epoch 688: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.9827 - MinusLogProbMetric: 40.9827 - val_loss: 44.8890 - val_MinusLogProbMetric: 44.8890 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 689/1000
2023-10-27 10:52:38.673 
Epoch 689/1000 
	 loss: 41.0358, MinusLogProbMetric: 41.0358, val_loss: 43.7426, val_MinusLogProbMetric: 43.7426

Epoch 689: val_loss did not improve from 42.10363
196/196 - 34s - loss: 41.0358 - MinusLogProbMetric: 41.0358 - val_loss: 43.7426 - val_MinusLogProbMetric: 43.7426 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 690/1000
2023-10-27 10:53:11.885 
Epoch 690/1000 
	 loss: 40.8404, MinusLogProbMetric: 40.8404, val_loss: 42.6185, val_MinusLogProbMetric: 42.6185

Epoch 690: val_loss did not improve from 42.10363
196/196 - 33s - loss: 40.8404 - MinusLogProbMetric: 40.8404 - val_loss: 42.6185 - val_MinusLogProbMetric: 42.6185 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 691/1000
2023-10-27 10:53:43.890 
Epoch 691/1000 
	 loss: 40.8822, MinusLogProbMetric: 40.8822, val_loss: 43.1696, val_MinusLogProbMetric: 43.1696

Epoch 691: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.8822 - MinusLogProbMetric: 40.8822 - val_loss: 43.1696 - val_MinusLogProbMetric: 43.1696 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 692/1000
2023-10-27 10:54:15.796 
Epoch 692/1000 
	 loss: 41.0826, MinusLogProbMetric: 41.0826, val_loss: 42.3671, val_MinusLogProbMetric: 42.3671

Epoch 692: val_loss did not improve from 42.10363
196/196 - 32s - loss: 41.0826 - MinusLogProbMetric: 41.0826 - val_loss: 42.3671 - val_MinusLogProbMetric: 42.3671 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 693/1000
2023-10-27 10:54:48.306 
Epoch 693/1000 
	 loss: 41.1022, MinusLogProbMetric: 41.1022, val_loss: 42.5871, val_MinusLogProbMetric: 42.5871

Epoch 693: val_loss did not improve from 42.10363
196/196 - 33s - loss: 41.1022 - MinusLogProbMetric: 41.1022 - val_loss: 42.5871 - val_MinusLogProbMetric: 42.5871 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 694/1000
2023-10-27 10:55:19.848 
Epoch 694/1000 
	 loss: 40.9930, MinusLogProbMetric: 40.9930, val_loss: 42.6644, val_MinusLogProbMetric: 42.6644

Epoch 694: val_loss did not improve from 42.10363
196/196 - 32s - loss: 40.9930 - MinusLogProbMetric: 40.9930 - val_loss: 42.6644 - val_MinusLogProbMetric: 42.6644 - lr: 5.5556e-05 - 32s/epoch - 161ms/step
Epoch 695/1000
2023-10-27 10:55:52.963 
Epoch 695/1000 
	 loss: 40.9058, MinusLogProbMetric: 40.9058, val_loss: 42.0697, val_MinusLogProbMetric: 42.0697

Epoch 695: val_loss improved from 42.10363 to 42.06968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 40.9058 - MinusLogProbMetric: 40.9058 - val_loss: 42.0697 - val_MinusLogProbMetric: 42.0697 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 696/1000
2023-10-27 10:56:25.759 
Epoch 696/1000 
	 loss: 41.0305, MinusLogProbMetric: 41.0305, val_loss: 42.4854, val_MinusLogProbMetric: 42.4854

Epoch 696: val_loss did not improve from 42.06968
196/196 - 32s - loss: 41.0305 - MinusLogProbMetric: 41.0305 - val_loss: 42.4854 - val_MinusLogProbMetric: 42.4854 - lr: 5.5556e-05 - 32s/epoch - 164ms/step
Epoch 697/1000
2023-10-27 10:56:57.746 
Epoch 697/1000 
	 loss: 40.8924, MinusLogProbMetric: 40.8924, val_loss: 42.2165, val_MinusLogProbMetric: 42.2165

Epoch 697: val_loss did not improve from 42.06968
196/196 - 32s - loss: 40.8924 - MinusLogProbMetric: 40.8924 - val_loss: 42.2165 - val_MinusLogProbMetric: 42.2165 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 698/1000
2023-10-27 10:57:31.002 
Epoch 698/1000 
	 loss: 41.1922, MinusLogProbMetric: 41.1922, val_loss: 42.9989, val_MinusLogProbMetric: 42.9989

Epoch 698: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.1922 - MinusLogProbMetric: 41.1922 - val_loss: 42.9989 - val_MinusLogProbMetric: 42.9989 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 699/1000
2023-10-27 10:58:03.383 
Epoch 699/1000 
	 loss: 40.7826, MinusLogProbMetric: 40.7826, val_loss: 42.2767, val_MinusLogProbMetric: 42.2767

Epoch 699: val_loss did not improve from 42.06968
196/196 - 32s - loss: 40.7826 - MinusLogProbMetric: 40.7826 - val_loss: 42.2767 - val_MinusLogProbMetric: 42.2767 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 700/1000
2023-10-27 10:58:36.720 
Epoch 700/1000 
	 loss: 40.9077, MinusLogProbMetric: 40.9077, val_loss: 42.2437, val_MinusLogProbMetric: 42.2437

Epoch 700: val_loss did not improve from 42.06968
196/196 - 33s - loss: 40.9077 - MinusLogProbMetric: 40.9077 - val_loss: 42.2437 - val_MinusLogProbMetric: 42.2437 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 701/1000
2023-10-27 10:59:07.589 
Epoch 701/1000 
	 loss: 40.8830, MinusLogProbMetric: 40.8830, val_loss: 42.9322, val_MinusLogProbMetric: 42.9322

Epoch 701: val_loss did not improve from 42.06968
196/196 - 31s - loss: 40.8830 - MinusLogProbMetric: 40.8830 - val_loss: 42.9322 - val_MinusLogProbMetric: 42.9322 - lr: 5.5556e-05 - 31s/epoch - 157ms/step
Epoch 702/1000
2023-10-27 10:59:39.062 
Epoch 702/1000 
	 loss: 50.6247, MinusLogProbMetric: 50.6247, val_loss: 46.2480, val_MinusLogProbMetric: 46.2480

Epoch 702: val_loss did not improve from 42.06968
196/196 - 31s - loss: 50.6247 - MinusLogProbMetric: 50.6247 - val_loss: 46.2480 - val_MinusLogProbMetric: 46.2480 - lr: 5.5556e-05 - 31s/epoch - 161ms/step
Epoch 703/1000
2023-10-27 11:00:12.249 
Epoch 703/1000 
	 loss: 43.4052, MinusLogProbMetric: 43.4052, val_loss: 43.8371, val_MinusLogProbMetric: 43.8371

Epoch 703: val_loss did not improve from 42.06968
196/196 - 33s - loss: 43.4052 - MinusLogProbMetric: 43.4052 - val_loss: 43.8371 - val_MinusLogProbMetric: 43.8371 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 704/1000
2023-10-27 11:00:45.376 
Epoch 704/1000 
	 loss: 43.3611, MinusLogProbMetric: 43.3611, val_loss: 44.2871, val_MinusLogProbMetric: 44.2871

Epoch 704: val_loss did not improve from 42.06968
196/196 - 33s - loss: 43.3611 - MinusLogProbMetric: 43.3611 - val_loss: 44.2871 - val_MinusLogProbMetric: 44.2871 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 705/1000
2023-10-27 11:01:17.305 
Epoch 705/1000 
	 loss: 42.6348, MinusLogProbMetric: 42.6348, val_loss: 44.5303, val_MinusLogProbMetric: 44.5303

Epoch 705: val_loss did not improve from 42.06968
196/196 - 32s - loss: 42.6348 - MinusLogProbMetric: 42.6348 - val_loss: 44.5303 - val_MinusLogProbMetric: 44.5303 - lr: 5.5556e-05 - 32s/epoch - 163ms/step
Epoch 706/1000
2023-10-27 11:01:49.460 
Epoch 706/1000 
	 loss: 42.5573, MinusLogProbMetric: 42.5573, val_loss: 43.3187, val_MinusLogProbMetric: 43.3187

Epoch 706: val_loss did not improve from 42.06968
196/196 - 32s - loss: 42.5573 - MinusLogProbMetric: 42.5573 - val_loss: 43.3187 - val_MinusLogProbMetric: 43.3187 - lr: 5.5556e-05 - 32s/epoch - 164ms/step
Epoch 707/1000
2023-10-27 11:02:22.786 
Epoch 707/1000 
	 loss: 42.4130, MinusLogProbMetric: 42.4130, val_loss: 43.2132, val_MinusLogProbMetric: 43.2132

Epoch 707: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.4130 - MinusLogProbMetric: 42.4130 - val_loss: 43.2132 - val_MinusLogProbMetric: 43.2132 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 708/1000
2023-10-27 11:02:55.849 
Epoch 708/1000 
	 loss: 42.2503, MinusLogProbMetric: 42.2503, val_loss: 43.5223, val_MinusLogProbMetric: 43.5223

Epoch 708: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.2503 - MinusLogProbMetric: 42.2503 - val_loss: 43.5223 - val_MinusLogProbMetric: 43.5223 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 709/1000
2023-10-27 11:03:29.137 
Epoch 709/1000 
	 loss: 42.3755, MinusLogProbMetric: 42.3755, val_loss: 43.5821, val_MinusLogProbMetric: 43.5821

Epoch 709: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.3755 - MinusLogProbMetric: 42.3755 - val_loss: 43.5821 - val_MinusLogProbMetric: 43.5821 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 710/1000
2023-10-27 11:04:01.914 
Epoch 710/1000 
	 loss: 42.2189, MinusLogProbMetric: 42.2189, val_loss: 43.3774, val_MinusLogProbMetric: 43.3774

Epoch 710: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.2189 - MinusLogProbMetric: 42.2189 - val_loss: 43.3774 - val_MinusLogProbMetric: 43.3774 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 711/1000
2023-10-27 11:04:35.124 
Epoch 711/1000 
	 loss: 42.0780, MinusLogProbMetric: 42.0780, val_loss: 43.1129, val_MinusLogProbMetric: 43.1129

Epoch 711: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.0780 - MinusLogProbMetric: 42.0780 - val_loss: 43.1129 - val_MinusLogProbMetric: 43.1129 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 712/1000
2023-10-27 11:05:08.408 
Epoch 712/1000 
	 loss: 42.0554, MinusLogProbMetric: 42.0554, val_loss: 43.7887, val_MinusLogProbMetric: 43.7887

Epoch 712: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.0554 - MinusLogProbMetric: 42.0554 - val_loss: 43.7887 - val_MinusLogProbMetric: 43.7887 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 713/1000
2023-10-27 11:05:41.398 
Epoch 713/1000 
	 loss: 42.2929, MinusLogProbMetric: 42.2929, val_loss: 43.4235, val_MinusLogProbMetric: 43.4235

Epoch 713: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.2929 - MinusLogProbMetric: 42.2929 - val_loss: 43.4235 - val_MinusLogProbMetric: 43.4235 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 714/1000
2023-10-27 11:06:14.532 
Epoch 714/1000 
	 loss: 42.0811, MinusLogProbMetric: 42.0811, val_loss: 44.8473, val_MinusLogProbMetric: 44.8473

Epoch 714: val_loss did not improve from 42.06968
196/196 - 33s - loss: 42.0811 - MinusLogProbMetric: 42.0811 - val_loss: 44.8473 - val_MinusLogProbMetric: 44.8473 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 715/1000
2023-10-27 11:06:46.196 
Epoch 715/1000 
	 loss: 42.0379, MinusLogProbMetric: 42.0379, val_loss: 43.3443, val_MinusLogProbMetric: 43.3443

Epoch 715: val_loss did not improve from 42.06968
196/196 - 32s - loss: 42.0379 - MinusLogProbMetric: 42.0379 - val_loss: 43.3443 - val_MinusLogProbMetric: 43.3443 - lr: 5.5556e-05 - 32s/epoch - 162ms/step
Epoch 716/1000
2023-10-27 11:07:19.018 
Epoch 716/1000 
	 loss: 41.9332, MinusLogProbMetric: 41.9332, val_loss: 42.8448, val_MinusLogProbMetric: 42.8448

Epoch 716: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.9332 - MinusLogProbMetric: 41.9332 - val_loss: 42.8448 - val_MinusLogProbMetric: 42.8448 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 717/1000
2023-10-27 11:07:52.543 
Epoch 717/1000 
	 loss: 41.9602, MinusLogProbMetric: 41.9602, val_loss: 42.6617, val_MinusLogProbMetric: 42.6617

Epoch 717: val_loss did not improve from 42.06968
196/196 - 34s - loss: 41.9602 - MinusLogProbMetric: 41.9602 - val_loss: 42.6617 - val_MinusLogProbMetric: 42.6617 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 718/1000
2023-10-27 11:08:26.164 
Epoch 718/1000 
	 loss: 41.9965, MinusLogProbMetric: 41.9965, val_loss: 42.9097, val_MinusLogProbMetric: 42.9097

Epoch 718: val_loss did not improve from 42.06968
196/196 - 34s - loss: 41.9965 - MinusLogProbMetric: 41.9965 - val_loss: 42.9097 - val_MinusLogProbMetric: 42.9097 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 719/1000
2023-10-27 11:08:58.779 
Epoch 719/1000 
	 loss: 41.9777, MinusLogProbMetric: 41.9777, val_loss: 42.6752, val_MinusLogProbMetric: 42.6752

Epoch 719: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.9777 - MinusLogProbMetric: 41.9777 - val_loss: 42.6752 - val_MinusLogProbMetric: 42.6752 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 720/1000
2023-10-27 11:09:32.054 
Epoch 720/1000 
	 loss: 41.8083, MinusLogProbMetric: 41.8083, val_loss: 43.0685, val_MinusLogProbMetric: 43.0685

Epoch 720: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.8083 - MinusLogProbMetric: 41.8083 - val_loss: 43.0685 - val_MinusLogProbMetric: 43.0685 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 721/1000
2023-10-27 11:10:04.394 
Epoch 721/1000 
	 loss: 41.7774, MinusLogProbMetric: 41.7774, val_loss: 43.8369, val_MinusLogProbMetric: 43.8369

Epoch 721: val_loss did not improve from 42.06968
196/196 - 32s - loss: 41.7774 - MinusLogProbMetric: 41.7774 - val_loss: 43.8369 - val_MinusLogProbMetric: 43.8369 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 722/1000
2023-10-27 11:10:36.918 
Epoch 722/1000 
	 loss: 41.7612, MinusLogProbMetric: 41.7612, val_loss: 42.5421, val_MinusLogProbMetric: 42.5421

Epoch 722: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.7612 - MinusLogProbMetric: 41.7612 - val_loss: 42.5421 - val_MinusLogProbMetric: 42.5421 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 723/1000
2023-10-27 11:11:10.676 
Epoch 723/1000 
	 loss: 41.8915, MinusLogProbMetric: 41.8915, val_loss: 42.5195, val_MinusLogProbMetric: 42.5195

Epoch 723: val_loss did not improve from 42.06968
196/196 - 34s - loss: 41.8915 - MinusLogProbMetric: 41.8915 - val_loss: 42.5195 - val_MinusLogProbMetric: 42.5195 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 724/1000
2023-10-27 11:11:44.459 
Epoch 724/1000 
	 loss: 41.7133, MinusLogProbMetric: 41.7133, val_loss: 42.7588, val_MinusLogProbMetric: 42.7588

Epoch 724: val_loss did not improve from 42.06968
196/196 - 34s - loss: 41.7133 - MinusLogProbMetric: 41.7133 - val_loss: 42.7588 - val_MinusLogProbMetric: 42.7588 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 725/1000
2023-10-27 11:12:17.905 
Epoch 725/1000 
	 loss: 41.7785, MinusLogProbMetric: 41.7785, val_loss: 42.6385, val_MinusLogProbMetric: 42.6385

Epoch 725: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.7785 - MinusLogProbMetric: 41.7785 - val_loss: 42.6385 - val_MinusLogProbMetric: 42.6385 - lr: 5.5556e-05 - 33s/epoch - 171ms/step
Epoch 726/1000
2023-10-27 11:12:50.912 
Epoch 726/1000 
	 loss: 41.7360, MinusLogProbMetric: 41.7360, val_loss: 42.7579, val_MinusLogProbMetric: 42.7579

Epoch 726: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.7360 - MinusLogProbMetric: 41.7360 - val_loss: 42.7579 - val_MinusLogProbMetric: 42.7579 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 727/1000
2023-10-27 11:13:24.114 
Epoch 727/1000 
	 loss: 41.8441, MinusLogProbMetric: 41.8441, val_loss: 42.5946, val_MinusLogProbMetric: 42.5946

Epoch 727: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.8441 - MinusLogProbMetric: 41.8441 - val_loss: 42.5946 - val_MinusLogProbMetric: 42.5946 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 728/1000
2023-10-27 11:13:57.747 
Epoch 728/1000 
	 loss: 41.6460, MinusLogProbMetric: 41.6460, val_loss: 42.4819, val_MinusLogProbMetric: 42.4819

Epoch 728: val_loss did not improve from 42.06968
196/196 - 34s - loss: 41.6460 - MinusLogProbMetric: 41.6460 - val_loss: 42.4819 - val_MinusLogProbMetric: 42.4819 - lr: 5.5556e-05 - 34s/epoch - 172ms/step
Epoch 729/1000
2023-10-27 11:14:30.178 
Epoch 729/1000 
	 loss: 41.7458, MinusLogProbMetric: 41.7458, val_loss: 42.4714, val_MinusLogProbMetric: 42.4714

Epoch 729: val_loss did not improve from 42.06968
196/196 - 32s - loss: 41.7458 - MinusLogProbMetric: 41.7458 - val_loss: 42.4714 - val_MinusLogProbMetric: 42.4714 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 730/1000
2023-10-27 11:15:03.687 
Epoch 730/1000 
	 loss: 41.6240, MinusLogProbMetric: 41.6240, val_loss: 42.7202, val_MinusLogProbMetric: 42.7202

Epoch 730: val_loss did not improve from 42.06968
196/196 - 34s - loss: 41.6240 - MinusLogProbMetric: 41.6240 - val_loss: 42.7202 - val_MinusLogProbMetric: 42.7202 - lr: 5.5556e-05 - 34s/epoch - 171ms/step
Epoch 731/1000
2023-10-27 11:15:36.629 
Epoch 731/1000 
	 loss: 41.6559, MinusLogProbMetric: 41.6559, val_loss: 43.4154, val_MinusLogProbMetric: 43.4154

Epoch 731: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.6559 - MinusLogProbMetric: 41.6559 - val_loss: 43.4154 - val_MinusLogProbMetric: 43.4154 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 732/1000
2023-10-27 11:16:09.817 
Epoch 732/1000 
	 loss: 41.6626, MinusLogProbMetric: 41.6626, val_loss: 42.3515, val_MinusLogProbMetric: 42.3515

Epoch 732: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.6626 - MinusLogProbMetric: 41.6626 - val_loss: 42.3515 - val_MinusLogProbMetric: 42.3515 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 733/1000
2023-10-27 11:16:42.793 
Epoch 733/1000 
	 loss: 41.9572, MinusLogProbMetric: 41.9572, val_loss: 42.7635, val_MinusLogProbMetric: 42.7635

Epoch 733: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.9572 - MinusLogProbMetric: 41.9572 - val_loss: 42.7635 - val_MinusLogProbMetric: 42.7635 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 734/1000
2023-10-27 11:17:15.596 
Epoch 734/1000 
	 loss: 41.5361, MinusLogProbMetric: 41.5361, val_loss: 42.8466, val_MinusLogProbMetric: 42.8466

Epoch 734: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.5361 - MinusLogProbMetric: 41.5361 - val_loss: 42.8466 - val_MinusLogProbMetric: 42.8466 - lr: 5.5556e-05 - 33s/epoch - 167ms/step
Epoch 735/1000
2023-10-27 11:17:48.976 
Epoch 735/1000 
	 loss: 41.5815, MinusLogProbMetric: 41.5815, val_loss: 42.6576, val_MinusLogProbMetric: 42.6576

Epoch 735: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.5815 - MinusLogProbMetric: 41.5815 - val_loss: 42.6576 - val_MinusLogProbMetric: 42.6576 - lr: 5.5556e-05 - 33s/epoch - 170ms/step
Epoch 736/1000
2023-10-27 11:18:21.952 
Epoch 736/1000 
	 loss: 41.6567, MinusLogProbMetric: 41.6567, val_loss: 42.4996, val_MinusLogProbMetric: 42.4996

Epoch 736: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.6567 - MinusLogProbMetric: 41.6567 - val_loss: 42.4996 - val_MinusLogProbMetric: 42.4996 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 737/1000
2023-10-27 11:18:54.890 
Epoch 737/1000 
	 loss: 41.5767, MinusLogProbMetric: 41.5767, val_loss: 42.5362, val_MinusLogProbMetric: 42.5362

Epoch 737: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.5767 - MinusLogProbMetric: 41.5767 - val_loss: 42.5362 - val_MinusLogProbMetric: 42.5362 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 738/1000
2023-10-27 11:19:27.759 
Epoch 738/1000 
	 loss: 41.5986, MinusLogProbMetric: 41.5986, val_loss: 45.0268, val_MinusLogProbMetric: 45.0268

Epoch 738: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.5986 - MinusLogProbMetric: 41.5986 - val_loss: 45.0268 - val_MinusLogProbMetric: 45.0268 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 739/1000
2023-10-27 11:20:00.873 
Epoch 739/1000 
	 loss: 41.7288, MinusLogProbMetric: 41.7288, val_loss: 42.4182, val_MinusLogProbMetric: 42.4182

Epoch 739: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.7288 - MinusLogProbMetric: 41.7288 - val_loss: 42.4182 - val_MinusLogProbMetric: 42.4182 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 740/1000
2023-10-27 11:20:33.791 
Epoch 740/1000 
	 loss: 41.5450, MinusLogProbMetric: 41.5450, val_loss: 42.2809, val_MinusLogProbMetric: 42.2809

Epoch 740: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.5450 - MinusLogProbMetric: 41.5450 - val_loss: 42.2809 - val_MinusLogProbMetric: 42.2809 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 741/1000
2023-10-27 11:21:06.992 
Epoch 741/1000 
	 loss: 41.7551, MinusLogProbMetric: 41.7551, val_loss: 42.8225, val_MinusLogProbMetric: 42.8225

Epoch 741: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.7551 - MinusLogProbMetric: 41.7551 - val_loss: 42.8225 - val_MinusLogProbMetric: 42.8225 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 742/1000
2023-10-27 11:21:39.478 
Epoch 742/1000 
	 loss: 41.7366, MinusLogProbMetric: 41.7366, val_loss: 42.7046, val_MinusLogProbMetric: 42.7046

Epoch 742: val_loss did not improve from 42.06968
196/196 - 32s - loss: 41.7366 - MinusLogProbMetric: 41.7366 - val_loss: 42.7046 - val_MinusLogProbMetric: 42.7046 - lr: 5.5556e-05 - 32s/epoch - 166ms/step
Epoch 743/1000
2023-10-27 11:22:12.659 
Epoch 743/1000 
	 loss: 41.4510, MinusLogProbMetric: 41.4510, val_loss: 42.3263, val_MinusLogProbMetric: 42.3263

Epoch 743: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.4510 - MinusLogProbMetric: 41.4510 - val_loss: 42.3263 - val_MinusLogProbMetric: 42.3263 - lr: 5.5556e-05 - 33s/epoch - 169ms/step
Epoch 744/1000
2023-10-27 11:22:45.560 
Epoch 744/1000 
	 loss: 41.7325, MinusLogProbMetric: 41.7325, val_loss: 42.4081, val_MinusLogProbMetric: 42.4081

Epoch 744: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.7325 - MinusLogProbMetric: 41.7325 - val_loss: 42.4081 - val_MinusLogProbMetric: 42.4081 - lr: 5.5556e-05 - 33s/epoch - 168ms/step
Epoch 745/1000
2023-10-27 11:23:17.982 
Epoch 745/1000 
	 loss: 41.5602, MinusLogProbMetric: 41.5602, val_loss: 42.7421, val_MinusLogProbMetric: 42.7421

Epoch 745: val_loss did not improve from 42.06968
196/196 - 32s - loss: 41.5602 - MinusLogProbMetric: 41.5602 - val_loss: 42.7421 - val_MinusLogProbMetric: 42.7421 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 746/1000
2023-10-27 11:23:51.188 
Epoch 746/1000 
	 loss: 41.0390, MinusLogProbMetric: 41.0390, val_loss: 42.1443, val_MinusLogProbMetric: 42.1443

Epoch 746: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.0390 - MinusLogProbMetric: 41.0390 - val_loss: 42.1443 - val_MinusLogProbMetric: 42.1443 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 747/1000
2023-10-27 11:24:24.617 
Epoch 747/1000 
	 loss: 40.9793, MinusLogProbMetric: 40.9793, val_loss: 42.2448, val_MinusLogProbMetric: 42.2448

Epoch 747: val_loss did not improve from 42.06968
196/196 - 33s - loss: 40.9793 - MinusLogProbMetric: 40.9793 - val_loss: 42.2448 - val_MinusLogProbMetric: 42.2448 - lr: 2.7778e-05 - 33s/epoch - 171ms/step
Epoch 748/1000
2023-10-27 11:24:57.424 
Epoch 748/1000 
	 loss: 40.9840, MinusLogProbMetric: 40.9840, val_loss: 42.1243, val_MinusLogProbMetric: 42.1243

Epoch 748: val_loss did not improve from 42.06968
196/196 - 33s - loss: 40.9840 - MinusLogProbMetric: 40.9840 - val_loss: 42.1243 - val_MinusLogProbMetric: 42.1243 - lr: 2.7778e-05 - 33s/epoch - 167ms/step
Epoch 749/1000
2023-10-27 11:25:30.723 
Epoch 749/1000 
	 loss: 41.1993, MinusLogProbMetric: 41.1993, val_loss: 42.1766, val_MinusLogProbMetric: 42.1766

Epoch 749: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.1993 - MinusLogProbMetric: 41.1993 - val_loss: 42.1766 - val_MinusLogProbMetric: 42.1766 - lr: 2.7778e-05 - 33s/epoch - 170ms/step
Epoch 750/1000
2023-10-27 11:26:04.404 
Epoch 750/1000 
	 loss: 40.9730, MinusLogProbMetric: 40.9730, val_loss: 42.1263, val_MinusLogProbMetric: 42.1263

Epoch 750: val_loss did not improve from 42.06968
196/196 - 34s - loss: 40.9730 - MinusLogProbMetric: 40.9730 - val_loss: 42.1263 - val_MinusLogProbMetric: 42.1263 - lr: 2.7778e-05 - 34s/epoch - 172ms/step
Epoch 751/1000
2023-10-27 11:26:37.094 
Epoch 751/1000 
	 loss: 40.9898, MinusLogProbMetric: 40.9898, val_loss: 42.2858, val_MinusLogProbMetric: 42.2858

Epoch 751: val_loss did not improve from 42.06968
196/196 - 33s - loss: 40.9898 - MinusLogProbMetric: 40.9898 - val_loss: 42.2858 - val_MinusLogProbMetric: 42.2858 - lr: 2.7778e-05 - 33s/epoch - 167ms/step
Epoch 752/1000
2023-10-27 11:27:10.248 
Epoch 752/1000 
	 loss: 40.9891, MinusLogProbMetric: 40.9891, val_loss: 42.1132, val_MinusLogProbMetric: 42.1132

Epoch 752: val_loss did not improve from 42.06968
196/196 - 33s - loss: 40.9891 - MinusLogProbMetric: 40.9891 - val_loss: 42.1132 - val_MinusLogProbMetric: 42.1132 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 753/1000
2023-10-27 11:27:43.103 
Epoch 753/1000 
	 loss: 41.1431, MinusLogProbMetric: 41.1431, val_loss: 42.0980, val_MinusLogProbMetric: 42.0980

Epoch 753: val_loss did not improve from 42.06968
196/196 - 33s - loss: 41.1431 - MinusLogProbMetric: 41.1431 - val_loss: 42.0980 - val_MinusLogProbMetric: 42.0980 - lr: 2.7778e-05 - 33s/epoch - 168ms/step
Epoch 754/1000
2023-10-27 11:28:16.250 
Epoch 754/1000 
	 loss: 40.9503, MinusLogProbMetric: 40.9503, val_loss: 42.0569, val_MinusLogProbMetric: 42.0569

Epoch 754: val_loss improved from 42.06968 to 42.05692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 40.9503 - MinusLogProbMetric: 40.9503 - val_loss: 42.0569 - val_MinusLogProbMetric: 42.0569 - lr: 2.7778e-05 - 34s/epoch - 172ms/step
Epoch 755/1000
2023-10-27 11:28:50.029 
Epoch 755/1000 
	 loss: 40.9869, MinusLogProbMetric: 40.9869, val_loss: 42.0102, val_MinusLogProbMetric: 42.0102

Epoch 755: val_loss improved from 42.05692 to 42.01021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 34s - loss: 40.9869 - MinusLogProbMetric: 40.9869 - val_loss: 42.0102 - val_MinusLogProbMetric: 42.0102 - lr: 2.7778e-05 - 34s/epoch - 172ms/step
Epoch 756/1000
2023-10-27 11:29:23.534 
Epoch 756/1000 
	 loss: 40.9479, MinusLogProbMetric: 40.9479, val_loss: 42.0354, val_MinusLogProbMetric: 42.0354

Epoch 756: val_loss did not improve from 42.01021
196/196 - 33s - loss: 40.9479 - MinusLogProbMetric: 40.9479 - val_loss: 42.0354 - val_MinusLogProbMetric: 42.0354 - lr: 2.7778e-05 - 33s/epoch - 168ms/step
Epoch 757/1000
2023-10-27 11:29:57.156 
Epoch 757/1000 
	 loss: 41.0518, MinusLogProbMetric: 41.0518, val_loss: 42.0940, val_MinusLogProbMetric: 42.0940

Epoch 757: val_loss did not improve from 42.01021
196/196 - 34s - loss: 41.0518 - MinusLogProbMetric: 41.0518 - val_loss: 42.0940 - val_MinusLogProbMetric: 42.0940 - lr: 2.7778e-05 - 34s/epoch - 172ms/step
Epoch 758/1000
2023-10-27 11:30:29.533 
Epoch 758/1000 
	 loss: 41.0364, MinusLogProbMetric: 41.0364, val_loss: 42.1410, val_MinusLogProbMetric: 42.1410

Epoch 758: val_loss did not improve from 42.01021
196/196 - 32s - loss: 41.0364 - MinusLogProbMetric: 41.0364 - val_loss: 42.1410 - val_MinusLogProbMetric: 42.1410 - lr: 2.7778e-05 - 32s/epoch - 165ms/step
Epoch 759/1000
2023-10-27 11:31:02.911 
Epoch 759/1000 
	 loss: 41.0184, MinusLogProbMetric: 41.0184, val_loss: 42.1584, val_MinusLogProbMetric: 42.1584

Epoch 759: val_loss did not improve from 42.01021
196/196 - 33s - loss: 41.0184 - MinusLogProbMetric: 41.0184 - val_loss: 42.1584 - val_MinusLogProbMetric: 42.1584 - lr: 2.7778e-05 - 33s/epoch - 170ms/step
Epoch 760/1000
2023-10-27 11:31:36.289 
Epoch 760/1000 
	 loss: 40.9368, MinusLogProbMetric: 40.9368, val_loss: 42.0792, val_MinusLogProbMetric: 42.0792

Epoch 760: val_loss did not improve from 42.01021
196/196 - 33s - loss: 40.9368 - MinusLogProbMetric: 40.9368 - val_loss: 42.0792 - val_MinusLogProbMetric: 42.0792 - lr: 2.7778e-05 - 33s/epoch - 170ms/step
Epoch 761/1000
2023-10-27 11:32:09.464 
Epoch 761/1000 
	 loss: 41.0012, MinusLogProbMetric: 41.0012, val_loss: 42.2286, val_MinusLogProbMetric: 42.2286

Epoch 761: val_loss did not improve from 42.01021
196/196 - 33s - loss: 41.0012 - MinusLogProbMetric: 41.0012 - val_loss: 42.2286 - val_MinusLogProbMetric: 42.2286 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 762/1000
2023-10-27 11:32:42.144 
Epoch 762/1000 
	 loss: 40.9723, MinusLogProbMetric: 40.9723, val_loss: 41.9913, val_MinusLogProbMetric: 41.9913

Epoch 762: val_loss improved from 42.01021 to 41.99129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 33s - loss: 40.9723 - MinusLogProbMetric: 40.9723 - val_loss: 41.9913 - val_MinusLogProbMetric: 41.9913 - lr: 2.7778e-05 - 33s/epoch - 170ms/step
Epoch 763/1000
2023-10-27 11:33:13.154 
Epoch 763/1000 
	 loss: 40.9498, MinusLogProbMetric: 40.9498, val_loss: 42.1599, val_MinusLogProbMetric: 42.1599

Epoch 763: val_loss did not improve from 41.99129
196/196 - 30s - loss: 40.9498 - MinusLogProbMetric: 40.9498 - val_loss: 42.1599 - val_MinusLogProbMetric: 42.1599 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 764/1000
2023-10-27 11:33:42.373 
Epoch 764/1000 
	 loss: 40.9661, MinusLogProbMetric: 40.9661, val_loss: 42.1725, val_MinusLogProbMetric: 42.1725

Epoch 764: val_loss did not improve from 41.99129
196/196 - 29s - loss: 40.9661 - MinusLogProbMetric: 40.9661 - val_loss: 42.1725 - val_MinusLogProbMetric: 42.1725 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 765/1000
2023-10-27 11:34:11.867 
Epoch 765/1000 
	 loss: 40.9656, MinusLogProbMetric: 40.9656, val_loss: 42.2430, val_MinusLogProbMetric: 42.2430

Epoch 765: val_loss did not improve from 41.99129
196/196 - 29s - loss: 40.9656 - MinusLogProbMetric: 40.9656 - val_loss: 42.2430 - val_MinusLogProbMetric: 42.2430 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 766/1000
2023-10-27 11:34:40.834 
Epoch 766/1000 
	 loss: 40.9414, MinusLogProbMetric: 40.9414, val_loss: 42.0507, val_MinusLogProbMetric: 42.0507

Epoch 766: val_loss did not improve from 41.99129
196/196 - 29s - loss: 40.9414 - MinusLogProbMetric: 40.9414 - val_loss: 42.0507 - val_MinusLogProbMetric: 42.0507 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 767/1000
2023-10-27 11:35:11.724 
Epoch 767/1000 
	 loss: 40.9314, MinusLogProbMetric: 40.9314, val_loss: 42.1416, val_MinusLogProbMetric: 42.1416

Epoch 767: val_loss did not improve from 41.99129
196/196 - 31s - loss: 40.9314 - MinusLogProbMetric: 40.9314 - val_loss: 42.1416 - val_MinusLogProbMetric: 42.1416 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 768/1000
2023-10-27 11:35:42.361 
Epoch 768/1000 
	 loss: 40.9010, MinusLogProbMetric: 40.9010, val_loss: 42.2659, val_MinusLogProbMetric: 42.2659

Epoch 768: val_loss did not improve from 41.99129
196/196 - 31s - loss: 40.9010 - MinusLogProbMetric: 40.9010 - val_loss: 42.2659 - val_MinusLogProbMetric: 42.2659 - lr: 2.7778e-05 - 31s/epoch - 156ms/step
Epoch 769/1000
2023-10-27 11:36:12.042 
Epoch 769/1000 
	 loss: 40.9189, MinusLogProbMetric: 40.9189, val_loss: 42.1344, val_MinusLogProbMetric: 42.1344

Epoch 769: val_loss did not improve from 41.99129
196/196 - 30s - loss: 40.9189 - MinusLogProbMetric: 40.9189 - val_loss: 42.1344 - val_MinusLogProbMetric: 42.1344 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 770/1000
2023-10-27 11:36:41.258 
Epoch 770/1000 
	 loss: 40.9219, MinusLogProbMetric: 40.9219, val_loss: 42.1842, val_MinusLogProbMetric: 42.1842

Epoch 770: val_loss did not improve from 41.99129
196/196 - 29s - loss: 40.9219 - MinusLogProbMetric: 40.9219 - val_loss: 42.1842 - val_MinusLogProbMetric: 42.1842 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 771/1000
2023-10-27 11:37:10.466 
Epoch 771/1000 
	 loss: 40.9215, MinusLogProbMetric: 40.9215, val_loss: 42.2847, val_MinusLogProbMetric: 42.2847

Epoch 771: val_loss did not improve from 41.99129
196/196 - 29s - loss: 40.9215 - MinusLogProbMetric: 40.9215 - val_loss: 42.2847 - val_MinusLogProbMetric: 42.2847 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 772/1000
2023-10-27 11:37:40.206 
Epoch 772/1000 
	 loss: 40.9495, MinusLogProbMetric: 40.9495, val_loss: 42.1856, val_MinusLogProbMetric: 42.1856

Epoch 772: val_loss did not improve from 41.99129
196/196 - 30s - loss: 40.9495 - MinusLogProbMetric: 40.9495 - val_loss: 42.1856 - val_MinusLogProbMetric: 42.1856 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 773/1000
2023-10-27 11:38:11.353 
Epoch 773/1000 
	 loss: 40.9203, MinusLogProbMetric: 40.9203, val_loss: 42.1833, val_MinusLogProbMetric: 42.1833

Epoch 773: val_loss did not improve from 41.99129
196/196 - 31s - loss: 40.9203 - MinusLogProbMetric: 40.9203 - val_loss: 42.1833 - val_MinusLogProbMetric: 42.1833 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 774/1000
2023-10-27 11:38:41.964 
Epoch 774/1000 
	 loss: 40.9182, MinusLogProbMetric: 40.9182, val_loss: 42.0685, val_MinusLogProbMetric: 42.0685

Epoch 774: val_loss did not improve from 41.99129
196/196 - 31s - loss: 40.9182 - MinusLogProbMetric: 40.9182 - val_loss: 42.0685 - val_MinusLogProbMetric: 42.0685 - lr: 2.7778e-05 - 31s/epoch - 156ms/step
Epoch 775/1000
2023-10-27 11:39:11.124 
Epoch 775/1000 
	 loss: 40.9513, MinusLogProbMetric: 40.9513, val_loss: 41.9827, val_MinusLogProbMetric: 41.9827

Epoch 775: val_loss improved from 41.99129 to 41.98267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.9513 - MinusLogProbMetric: 40.9513 - val_loss: 41.9827 - val_MinusLogProbMetric: 41.9827 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 776/1000
2023-10-27 11:39:40.763 
Epoch 776/1000 
	 loss: 40.8837, MinusLogProbMetric: 40.8837, val_loss: 42.0923, val_MinusLogProbMetric: 42.0923

Epoch 776: val_loss did not improve from 41.98267
196/196 - 29s - loss: 40.8837 - MinusLogProbMetric: 40.8837 - val_loss: 42.0923 - val_MinusLogProbMetric: 42.0923 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 777/1000
2023-10-27 11:40:10.020 
Epoch 777/1000 
	 loss: 40.9279, MinusLogProbMetric: 40.9279, val_loss: 42.1957, val_MinusLogProbMetric: 42.1957

Epoch 777: val_loss did not improve from 41.98267
196/196 - 29s - loss: 40.9279 - MinusLogProbMetric: 40.9279 - val_loss: 42.1957 - val_MinusLogProbMetric: 42.1957 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 778/1000
2023-10-27 11:40:40.719 
Epoch 778/1000 
	 loss: 40.9058, MinusLogProbMetric: 40.9058, val_loss: 42.0551, val_MinusLogProbMetric: 42.0551

Epoch 778: val_loss did not improve from 41.98267
196/196 - 31s - loss: 40.9058 - MinusLogProbMetric: 40.9058 - val_loss: 42.0551 - val_MinusLogProbMetric: 42.0551 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 779/1000
2023-10-27 11:41:13.670 
Epoch 779/1000 
	 loss: 40.9230, MinusLogProbMetric: 40.9230, val_loss: 42.0343, val_MinusLogProbMetric: 42.0343

Epoch 779: val_loss did not improve from 41.98267
196/196 - 33s - loss: 40.9230 - MinusLogProbMetric: 40.9230 - val_loss: 42.0343 - val_MinusLogProbMetric: 42.0343 - lr: 2.7778e-05 - 33s/epoch - 168ms/step
Epoch 780/1000
2023-10-27 11:41:43.144 
Epoch 780/1000 
	 loss: 40.8677, MinusLogProbMetric: 40.8677, val_loss: 42.1794, val_MinusLogProbMetric: 42.1794

Epoch 780: val_loss did not improve from 41.98267
196/196 - 29s - loss: 40.8677 - MinusLogProbMetric: 40.8677 - val_loss: 42.1794 - val_MinusLogProbMetric: 42.1794 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 781/1000
2023-10-27 11:42:12.493 
Epoch 781/1000 
	 loss: 40.9205, MinusLogProbMetric: 40.9205, val_loss: 42.1548, val_MinusLogProbMetric: 42.1548

Epoch 781: val_loss did not improve from 41.98267
196/196 - 29s - loss: 40.9205 - MinusLogProbMetric: 40.9205 - val_loss: 42.1548 - val_MinusLogProbMetric: 42.1548 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 782/1000
2023-10-27 11:42:41.646 
Epoch 782/1000 
	 loss: 40.9210, MinusLogProbMetric: 40.9210, val_loss: 42.0031, val_MinusLogProbMetric: 42.0031

Epoch 782: val_loss did not improve from 41.98267
196/196 - 29s - loss: 40.9210 - MinusLogProbMetric: 40.9210 - val_loss: 42.0031 - val_MinusLogProbMetric: 42.0031 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 783/1000
2023-10-27 11:43:10.883 
Epoch 783/1000 
	 loss: 40.8939, MinusLogProbMetric: 40.8939, val_loss: 42.1057, val_MinusLogProbMetric: 42.1057

Epoch 783: val_loss did not improve from 41.98267
196/196 - 29s - loss: 40.8939 - MinusLogProbMetric: 40.8939 - val_loss: 42.1057 - val_MinusLogProbMetric: 42.1057 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 784/1000
2023-10-27 11:43:43.700 
Epoch 784/1000 
	 loss: 40.9136, MinusLogProbMetric: 40.9136, val_loss: 42.1140, val_MinusLogProbMetric: 42.1140

Epoch 784: val_loss did not improve from 41.98267
196/196 - 33s - loss: 40.9136 - MinusLogProbMetric: 40.9136 - val_loss: 42.1140 - val_MinusLogProbMetric: 42.1140 - lr: 2.7778e-05 - 33s/epoch - 167ms/step
Epoch 785/1000
2023-10-27 11:44:14.619 
Epoch 785/1000 
	 loss: 40.8858, MinusLogProbMetric: 40.8858, val_loss: 42.0084, val_MinusLogProbMetric: 42.0084

Epoch 785: val_loss did not improve from 41.98267
196/196 - 31s - loss: 40.8858 - MinusLogProbMetric: 40.8858 - val_loss: 42.0084 - val_MinusLogProbMetric: 42.0084 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 786/1000
2023-10-27 11:44:43.587 
Epoch 786/1000 
	 loss: 40.8744, MinusLogProbMetric: 40.8744, val_loss: 41.9023, val_MinusLogProbMetric: 41.9023

Epoch 786: val_loss improved from 41.98267 to 41.90231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.8744 - MinusLogProbMetric: 40.8744 - val_loss: 41.9023 - val_MinusLogProbMetric: 41.9023 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 787/1000
2023-10-27 11:45:13.009 
Epoch 787/1000 
	 loss: 40.9243, MinusLogProbMetric: 40.9243, val_loss: 42.0352, val_MinusLogProbMetric: 42.0352

Epoch 787: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.9243 - MinusLogProbMetric: 40.9243 - val_loss: 42.0352 - val_MinusLogProbMetric: 42.0352 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 788/1000
2023-10-27 11:45:41.800 
Epoch 788/1000 
	 loss: 40.9196, MinusLogProbMetric: 40.9196, val_loss: 41.9700, val_MinusLogProbMetric: 41.9700

Epoch 788: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.9196 - MinusLogProbMetric: 40.9196 - val_loss: 41.9700 - val_MinusLogProbMetric: 41.9700 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 789/1000
2023-10-27 11:46:12.258 
Epoch 789/1000 
	 loss: 40.8486, MinusLogProbMetric: 40.8486, val_loss: 42.3936, val_MinusLogProbMetric: 42.3936

Epoch 789: val_loss did not improve from 41.90231
196/196 - 30s - loss: 40.8486 - MinusLogProbMetric: 40.8486 - val_loss: 42.3936 - val_MinusLogProbMetric: 42.3936 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 790/1000
2023-10-27 11:46:43.798 
Epoch 790/1000 
	 loss: 40.9278, MinusLogProbMetric: 40.9278, val_loss: 41.9785, val_MinusLogProbMetric: 41.9785

Epoch 790: val_loss did not improve from 41.90231
196/196 - 32s - loss: 40.9278 - MinusLogProbMetric: 40.9278 - val_loss: 41.9785 - val_MinusLogProbMetric: 41.9785 - lr: 2.7778e-05 - 32s/epoch - 161ms/step
Epoch 791/1000
2023-10-27 11:47:13.362 
Epoch 791/1000 
	 loss: 40.9351, MinusLogProbMetric: 40.9351, val_loss: 42.1181, val_MinusLogProbMetric: 42.1181

Epoch 791: val_loss did not improve from 41.90231
196/196 - 30s - loss: 40.9351 - MinusLogProbMetric: 40.9351 - val_loss: 42.1181 - val_MinusLogProbMetric: 42.1181 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 792/1000
2023-10-27 11:47:42.364 
Epoch 792/1000 
	 loss: 40.8338, MinusLogProbMetric: 40.8338, val_loss: 42.2306, val_MinusLogProbMetric: 42.2306

Epoch 792: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.8338 - MinusLogProbMetric: 40.8338 - val_loss: 42.2306 - val_MinusLogProbMetric: 42.2306 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 793/1000
2023-10-27 11:48:11.658 
Epoch 793/1000 
	 loss: 40.9155, MinusLogProbMetric: 40.9155, val_loss: 42.3890, val_MinusLogProbMetric: 42.3890

Epoch 793: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.9155 - MinusLogProbMetric: 40.9155 - val_loss: 42.3890 - val_MinusLogProbMetric: 42.3890 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 794/1000
2023-10-27 11:48:40.497 
Epoch 794/1000 
	 loss: 40.9123, MinusLogProbMetric: 40.9123, val_loss: 41.9620, val_MinusLogProbMetric: 41.9620

Epoch 794: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.9123 - MinusLogProbMetric: 40.9123 - val_loss: 41.9620 - val_MinusLogProbMetric: 41.9620 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 795/1000
2023-10-27 11:49:10.171 
Epoch 795/1000 
	 loss: 40.9315, MinusLogProbMetric: 40.9315, val_loss: 42.1176, val_MinusLogProbMetric: 42.1176

Epoch 795: val_loss did not improve from 41.90231
196/196 - 30s - loss: 40.9315 - MinusLogProbMetric: 40.9315 - val_loss: 42.1176 - val_MinusLogProbMetric: 42.1176 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 796/1000
2023-10-27 11:49:40.959 
Epoch 796/1000 
	 loss: 40.8859, MinusLogProbMetric: 40.8859, val_loss: 41.9998, val_MinusLogProbMetric: 41.9998

Epoch 796: val_loss did not improve from 41.90231
196/196 - 31s - loss: 40.8859 - MinusLogProbMetric: 40.8859 - val_loss: 41.9998 - val_MinusLogProbMetric: 41.9998 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 797/1000
2023-10-27 11:50:10.370 
Epoch 797/1000 
	 loss: 40.8448, MinusLogProbMetric: 40.8448, val_loss: 42.1816, val_MinusLogProbMetric: 42.1816

Epoch 797: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.8448 - MinusLogProbMetric: 40.8448 - val_loss: 42.1816 - val_MinusLogProbMetric: 42.1816 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 798/1000
2023-10-27 11:50:39.504 
Epoch 798/1000 
	 loss: 40.9164, MinusLogProbMetric: 40.9164, val_loss: 42.0667, val_MinusLogProbMetric: 42.0667

Epoch 798: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.9164 - MinusLogProbMetric: 40.9164 - val_loss: 42.0667 - val_MinusLogProbMetric: 42.0667 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 799/1000
2023-10-27 11:51:08.603 
Epoch 799/1000 
	 loss: 41.0499, MinusLogProbMetric: 41.0499, val_loss: 42.0125, val_MinusLogProbMetric: 42.0125

Epoch 799: val_loss did not improve from 41.90231
196/196 - 29s - loss: 41.0499 - MinusLogProbMetric: 41.0499 - val_loss: 42.0125 - val_MinusLogProbMetric: 42.0125 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 800/1000
2023-10-27 11:51:38.190 
Epoch 800/1000 
	 loss: 40.7944, MinusLogProbMetric: 40.7944, val_loss: 41.9404, val_MinusLogProbMetric: 41.9404

Epoch 800: val_loss did not improve from 41.90231
196/196 - 30s - loss: 40.7944 - MinusLogProbMetric: 40.7944 - val_loss: 41.9404 - val_MinusLogProbMetric: 41.9404 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 801/1000
2023-10-27 11:52:10.028 
Epoch 801/1000 
	 loss: 40.8977, MinusLogProbMetric: 40.8977, val_loss: 41.9547, val_MinusLogProbMetric: 41.9547

Epoch 801: val_loss did not improve from 41.90231
196/196 - 32s - loss: 40.8977 - MinusLogProbMetric: 40.8977 - val_loss: 41.9547 - val_MinusLogProbMetric: 41.9547 - lr: 2.7778e-05 - 32s/epoch - 162ms/step
Epoch 802/1000
2023-10-27 11:52:41.158 
Epoch 802/1000 
	 loss: 40.8175, MinusLogProbMetric: 40.8175, val_loss: 41.9149, val_MinusLogProbMetric: 41.9149

Epoch 802: val_loss did not improve from 41.90231
196/196 - 31s - loss: 40.8175 - MinusLogProbMetric: 40.8175 - val_loss: 41.9149 - val_MinusLogProbMetric: 41.9149 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 803/1000
2023-10-27 11:53:10.004 
Epoch 803/1000 
	 loss: 40.8043, MinusLogProbMetric: 40.8043, val_loss: 41.9487, val_MinusLogProbMetric: 41.9487

Epoch 803: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.8043 - MinusLogProbMetric: 40.8043 - val_loss: 41.9487 - val_MinusLogProbMetric: 41.9487 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 804/1000
2023-10-27 11:53:38.837 
Epoch 804/1000 
	 loss: 40.9018, MinusLogProbMetric: 40.9018, val_loss: 41.9868, val_MinusLogProbMetric: 41.9868

Epoch 804: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.9018 - MinusLogProbMetric: 40.9018 - val_loss: 41.9868 - val_MinusLogProbMetric: 41.9868 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 805/1000
2023-10-27 11:54:08.360 
Epoch 805/1000 
	 loss: 40.8454, MinusLogProbMetric: 40.8454, val_loss: 42.2927, val_MinusLogProbMetric: 42.2927

Epoch 805: val_loss did not improve from 41.90231
196/196 - 30s - loss: 40.8454 - MinusLogProbMetric: 40.8454 - val_loss: 42.2927 - val_MinusLogProbMetric: 42.2927 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 806/1000
2023-10-27 11:54:39.204 
Epoch 806/1000 
	 loss: 40.8481, MinusLogProbMetric: 40.8481, val_loss: 42.1662, val_MinusLogProbMetric: 42.1662

Epoch 806: val_loss did not improve from 41.90231
196/196 - 31s - loss: 40.8481 - MinusLogProbMetric: 40.8481 - val_loss: 42.1662 - val_MinusLogProbMetric: 42.1662 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 807/1000
2023-10-27 11:55:09.917 
Epoch 807/1000 
	 loss: 40.9165, MinusLogProbMetric: 40.9165, val_loss: 42.1928, val_MinusLogProbMetric: 42.1928

Epoch 807: val_loss did not improve from 41.90231
196/196 - 31s - loss: 40.9165 - MinusLogProbMetric: 40.9165 - val_loss: 42.1928 - val_MinusLogProbMetric: 42.1928 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 808/1000
2023-10-27 11:55:39.578 
Epoch 808/1000 
	 loss: 40.8049, MinusLogProbMetric: 40.8049, val_loss: 42.1925, val_MinusLogProbMetric: 42.1925

Epoch 808: val_loss did not improve from 41.90231
196/196 - 30s - loss: 40.8049 - MinusLogProbMetric: 40.8049 - val_loss: 42.1925 - val_MinusLogProbMetric: 42.1925 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 809/1000
2023-10-27 11:56:08.970 
Epoch 809/1000 
	 loss: 41.1794, MinusLogProbMetric: 41.1794, val_loss: 42.0766, val_MinusLogProbMetric: 42.0766

Epoch 809: val_loss did not improve from 41.90231
196/196 - 29s - loss: 41.1794 - MinusLogProbMetric: 41.1794 - val_loss: 42.0766 - val_MinusLogProbMetric: 42.0766 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 810/1000
2023-10-27 11:56:38.050 
Epoch 810/1000 
	 loss: 40.8180, MinusLogProbMetric: 40.8180, val_loss: 42.0421, val_MinusLogProbMetric: 42.0421

Epoch 810: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.8180 - MinusLogProbMetric: 40.8180 - val_loss: 42.0421 - val_MinusLogProbMetric: 42.0421 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 811/1000
2023-10-27 11:57:07.236 
Epoch 811/1000 
	 loss: 40.7813, MinusLogProbMetric: 40.7813, val_loss: 42.1335, val_MinusLogProbMetric: 42.1335

Epoch 811: val_loss did not improve from 41.90231
196/196 - 29s - loss: 40.7813 - MinusLogProbMetric: 40.7813 - val_loss: 42.1335 - val_MinusLogProbMetric: 42.1335 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 812/1000
2023-10-27 11:57:37.515 
Epoch 812/1000 
	 loss: 40.8036, MinusLogProbMetric: 40.8036, val_loss: 41.8959, val_MinusLogProbMetric: 41.8959

Epoch 812: val_loss improved from 41.90231 to 41.89592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 40.8036 - MinusLogProbMetric: 40.8036 - val_loss: 41.8959 - val_MinusLogProbMetric: 41.8959 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 813/1000
2023-10-27 11:58:09.432 
Epoch 813/1000 
	 loss: 40.8012, MinusLogProbMetric: 40.8012, val_loss: 41.9591, val_MinusLogProbMetric: 41.9591

Epoch 813: val_loss did not improve from 41.89592
196/196 - 31s - loss: 40.8012 - MinusLogProbMetric: 40.8012 - val_loss: 41.9591 - val_MinusLogProbMetric: 41.9591 - lr: 2.7778e-05 - 31s/epoch - 160ms/step
Epoch 814/1000
2023-10-27 11:58:38.685 
Epoch 814/1000 
	 loss: 40.8063, MinusLogProbMetric: 40.8063, val_loss: 41.9750, val_MinusLogProbMetric: 41.9750

Epoch 814: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8063 - MinusLogProbMetric: 40.8063 - val_loss: 41.9750 - val_MinusLogProbMetric: 41.9750 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 815/1000
2023-10-27 11:59:07.981 
Epoch 815/1000 
	 loss: 40.7820, MinusLogProbMetric: 40.7820, val_loss: 42.0877, val_MinusLogProbMetric: 42.0877

Epoch 815: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.7820 - MinusLogProbMetric: 40.7820 - val_loss: 42.0877 - val_MinusLogProbMetric: 42.0877 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 816/1000
2023-10-27 11:59:36.947 
Epoch 816/1000 
	 loss: 40.8072, MinusLogProbMetric: 40.8072, val_loss: 42.1581, val_MinusLogProbMetric: 42.1581

Epoch 816: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8072 - MinusLogProbMetric: 40.8072 - val_loss: 42.1581 - val_MinusLogProbMetric: 42.1581 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 817/1000
2023-10-27 12:00:07.053 
Epoch 817/1000 
	 loss: 40.7976, MinusLogProbMetric: 40.7976, val_loss: 42.1695, val_MinusLogProbMetric: 42.1695

Epoch 817: val_loss did not improve from 41.89592
196/196 - 30s - loss: 40.7976 - MinusLogProbMetric: 40.7976 - val_loss: 42.1695 - val_MinusLogProbMetric: 42.1695 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 818/1000
2023-10-27 12:00:37.712 
Epoch 818/1000 
	 loss: 40.8279, MinusLogProbMetric: 40.8279, val_loss: 42.0270, val_MinusLogProbMetric: 42.0270

Epoch 818: val_loss did not improve from 41.89592
196/196 - 31s - loss: 40.8279 - MinusLogProbMetric: 40.8279 - val_loss: 42.0270 - val_MinusLogProbMetric: 42.0270 - lr: 2.7778e-05 - 31s/epoch - 156ms/step
Epoch 819/1000
2023-10-27 12:01:07.537 
Epoch 819/1000 
	 loss: 40.8076, MinusLogProbMetric: 40.8076, val_loss: 41.9968, val_MinusLogProbMetric: 41.9968

Epoch 819: val_loss did not improve from 41.89592
196/196 - 30s - loss: 40.8076 - MinusLogProbMetric: 40.8076 - val_loss: 41.9968 - val_MinusLogProbMetric: 41.9968 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 820/1000
2023-10-27 12:01:36.793 
Epoch 820/1000 
	 loss: 40.8289, MinusLogProbMetric: 40.8289, val_loss: 42.0891, val_MinusLogProbMetric: 42.0891

Epoch 820: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8289 - MinusLogProbMetric: 40.8289 - val_loss: 42.0891 - val_MinusLogProbMetric: 42.0891 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 821/1000
2023-10-27 12:02:06.226 
Epoch 821/1000 
	 loss: 40.8333, MinusLogProbMetric: 40.8333, val_loss: 42.0027, val_MinusLogProbMetric: 42.0027

Epoch 821: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8333 - MinusLogProbMetric: 40.8333 - val_loss: 42.0027 - val_MinusLogProbMetric: 42.0027 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 822/1000
2023-10-27 12:02:35.340 
Epoch 822/1000 
	 loss: 40.8048, MinusLogProbMetric: 40.8048, val_loss: 42.7423, val_MinusLogProbMetric: 42.7423

Epoch 822: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8048 - MinusLogProbMetric: 40.8048 - val_loss: 42.7423 - val_MinusLogProbMetric: 42.7423 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 823/1000
2023-10-27 12:03:06.064 
Epoch 823/1000 
	 loss: 40.7968, MinusLogProbMetric: 40.7968, val_loss: 42.2831, val_MinusLogProbMetric: 42.2831

Epoch 823: val_loss did not improve from 41.89592
196/196 - 31s - loss: 40.7968 - MinusLogProbMetric: 40.7968 - val_loss: 42.2831 - val_MinusLogProbMetric: 42.2831 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 824/1000
2023-10-27 12:03:37.813 
Epoch 824/1000 
	 loss: 40.7487, MinusLogProbMetric: 40.7487, val_loss: 41.9712, val_MinusLogProbMetric: 41.9712

Epoch 824: val_loss did not improve from 41.89592
196/196 - 32s - loss: 40.7487 - MinusLogProbMetric: 40.7487 - val_loss: 41.9712 - val_MinusLogProbMetric: 41.9712 - lr: 2.7778e-05 - 32s/epoch - 162ms/step
Epoch 825/1000
2023-10-27 12:04:08.235 
Epoch 825/1000 
	 loss: 40.7984, MinusLogProbMetric: 40.7984, val_loss: 42.9512, val_MinusLogProbMetric: 42.9512

Epoch 825: val_loss did not improve from 41.89592
196/196 - 30s - loss: 40.7984 - MinusLogProbMetric: 40.7984 - val_loss: 42.9512 - val_MinusLogProbMetric: 42.9512 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 826/1000
2023-10-27 12:04:37.483 
Epoch 826/1000 
	 loss: 40.7322, MinusLogProbMetric: 40.7322, val_loss: 41.9507, val_MinusLogProbMetric: 41.9507

Epoch 826: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.7322 - MinusLogProbMetric: 40.7322 - val_loss: 41.9507 - val_MinusLogProbMetric: 41.9507 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 827/1000
2023-10-27 12:05:06.724 
Epoch 827/1000 
	 loss: 40.8000, MinusLogProbMetric: 40.8000, val_loss: 42.2956, val_MinusLogProbMetric: 42.2956

Epoch 827: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8000 - MinusLogProbMetric: 40.8000 - val_loss: 42.2956 - val_MinusLogProbMetric: 42.2956 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 828/1000
2023-10-27 12:05:36.220 
Epoch 828/1000 
	 loss: 40.8111, MinusLogProbMetric: 40.8111, val_loss: 41.9698, val_MinusLogProbMetric: 41.9698

Epoch 828: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8111 - MinusLogProbMetric: 40.8111 - val_loss: 41.9698 - val_MinusLogProbMetric: 41.9698 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 829/1000
2023-10-27 12:06:09.374 
Epoch 829/1000 
	 loss: 40.7896, MinusLogProbMetric: 40.7896, val_loss: 41.9235, val_MinusLogProbMetric: 41.9235

Epoch 829: val_loss did not improve from 41.89592
196/196 - 33s - loss: 40.7896 - MinusLogProbMetric: 40.7896 - val_loss: 41.9235 - val_MinusLogProbMetric: 41.9235 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 830/1000
2023-10-27 12:06:40.851 
Epoch 830/1000 
	 loss: 40.8520, MinusLogProbMetric: 40.8520, val_loss: 43.0653, val_MinusLogProbMetric: 43.0653

Epoch 830: val_loss did not improve from 41.89592
196/196 - 31s - loss: 40.8520 - MinusLogProbMetric: 40.8520 - val_loss: 43.0653 - val_MinusLogProbMetric: 43.0653 - lr: 2.7778e-05 - 31s/epoch - 161ms/step
Epoch 831/1000
2023-10-27 12:07:10.307 
Epoch 831/1000 
	 loss: 40.8085, MinusLogProbMetric: 40.8085, val_loss: 41.9592, val_MinusLogProbMetric: 41.9592

Epoch 831: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.8085 - MinusLogProbMetric: 40.8085 - val_loss: 41.9592 - val_MinusLogProbMetric: 41.9592 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 832/1000
2023-10-27 12:07:39.658 
Epoch 832/1000 
	 loss: 40.7882, MinusLogProbMetric: 40.7882, val_loss: 42.1775, val_MinusLogProbMetric: 42.1775

Epoch 832: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.7882 - MinusLogProbMetric: 40.7882 - val_loss: 42.1775 - val_MinusLogProbMetric: 42.1775 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 833/1000
2023-10-27 12:08:08.727 
Epoch 833/1000 
	 loss: 40.7676, MinusLogProbMetric: 40.7676, val_loss: 42.1128, val_MinusLogProbMetric: 42.1128

Epoch 833: val_loss did not improve from 41.89592
196/196 - 29s - loss: 40.7676 - MinusLogProbMetric: 40.7676 - val_loss: 42.1128 - val_MinusLogProbMetric: 42.1128 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 834/1000
2023-10-27 12:08:39.752 
Epoch 834/1000 
	 loss: 40.9635, MinusLogProbMetric: 40.9635, val_loss: 42.8973, val_MinusLogProbMetric: 42.8973

Epoch 834: val_loss did not improve from 41.89592
196/196 - 31s - loss: 40.9635 - MinusLogProbMetric: 40.9635 - val_loss: 42.8973 - val_MinusLogProbMetric: 42.8973 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 835/1000
2023-10-27 12:09:11.645 
Epoch 835/1000 
	 loss: 40.8049, MinusLogProbMetric: 40.8049, val_loss: 41.8934, val_MinusLogProbMetric: 41.8934

Epoch 835: val_loss improved from 41.89592 to 41.89338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 32s - loss: 40.8049 - MinusLogProbMetric: 40.8049 - val_loss: 41.8934 - val_MinusLogProbMetric: 41.8934 - lr: 2.7778e-05 - 32s/epoch - 166ms/step
Epoch 836/1000
2023-10-27 12:09:41.398 
Epoch 836/1000 
	 loss: 40.7138, MinusLogProbMetric: 40.7138, val_loss: 41.8419, val_MinusLogProbMetric: 41.8419

Epoch 836: val_loss improved from 41.89338 to 41.84192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.7138 - MinusLogProbMetric: 40.7138 - val_loss: 41.8419 - val_MinusLogProbMetric: 41.8419 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 837/1000
2023-10-27 12:10:10.762 
Epoch 837/1000 
	 loss: 40.7559, MinusLogProbMetric: 40.7559, val_loss: 41.9580, val_MinusLogProbMetric: 41.9580

Epoch 837: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7559 - MinusLogProbMetric: 40.7559 - val_loss: 41.9580 - val_MinusLogProbMetric: 41.9580 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 838/1000
2023-10-27 12:10:39.796 
Epoch 838/1000 
	 loss: 40.7523, MinusLogProbMetric: 40.7523, val_loss: 42.1266, val_MinusLogProbMetric: 42.1266

Epoch 838: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7523 - MinusLogProbMetric: 40.7523 - val_loss: 42.1266 - val_MinusLogProbMetric: 42.1266 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 839/1000
2023-10-27 12:11:08.847 
Epoch 839/1000 
	 loss: 40.7944, MinusLogProbMetric: 40.7944, val_loss: 41.8600, val_MinusLogProbMetric: 41.8600

Epoch 839: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7944 - MinusLogProbMetric: 40.7944 - val_loss: 41.8600 - val_MinusLogProbMetric: 41.8600 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 840/1000
2023-10-27 12:11:39.571 
Epoch 840/1000 
	 loss: 40.7576, MinusLogProbMetric: 40.7576, val_loss: 42.0288, val_MinusLogProbMetric: 42.0288

Epoch 840: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.7576 - MinusLogProbMetric: 40.7576 - val_loss: 42.0288 - val_MinusLogProbMetric: 42.0288 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 841/1000
2023-10-27 12:12:09.867 
Epoch 841/1000 
	 loss: 40.7319, MinusLogProbMetric: 40.7319, val_loss: 41.9651, val_MinusLogProbMetric: 41.9651

Epoch 841: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7319 - MinusLogProbMetric: 40.7319 - val_loss: 41.9651 - val_MinusLogProbMetric: 41.9651 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 842/1000
2023-10-27 12:12:39.460 
Epoch 842/1000 
	 loss: 40.7182, MinusLogProbMetric: 40.7182, val_loss: 42.0291, val_MinusLogProbMetric: 42.0291

Epoch 842: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7182 - MinusLogProbMetric: 40.7182 - val_loss: 42.0291 - val_MinusLogProbMetric: 42.0291 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 843/1000
2023-10-27 12:13:08.497 
Epoch 843/1000 
	 loss: 40.7492, MinusLogProbMetric: 40.7492, val_loss: 41.8611, val_MinusLogProbMetric: 41.8611

Epoch 843: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7492 - MinusLogProbMetric: 40.7492 - val_loss: 41.8611 - val_MinusLogProbMetric: 41.8611 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 844/1000
2023-10-27 12:13:37.956 
Epoch 844/1000 
	 loss: 40.7643, MinusLogProbMetric: 40.7643, val_loss: 41.8992, val_MinusLogProbMetric: 41.8992

Epoch 844: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7643 - MinusLogProbMetric: 40.7643 - val_loss: 41.8992 - val_MinusLogProbMetric: 41.8992 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 845/1000
2023-10-27 12:14:08.105 
Epoch 845/1000 
	 loss: 40.7894, MinusLogProbMetric: 40.7894, val_loss: 42.3852, val_MinusLogProbMetric: 42.3852

Epoch 845: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7894 - MinusLogProbMetric: 40.7894 - val_loss: 42.3852 - val_MinusLogProbMetric: 42.3852 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 846/1000
2023-10-27 12:14:41.032 
Epoch 846/1000 
	 loss: 40.7339, MinusLogProbMetric: 40.7339, val_loss: 41.8967, val_MinusLogProbMetric: 41.8967

Epoch 846: val_loss did not improve from 41.84192
196/196 - 33s - loss: 40.7339 - MinusLogProbMetric: 40.7339 - val_loss: 41.8967 - val_MinusLogProbMetric: 41.8967 - lr: 2.7778e-05 - 33s/epoch - 168ms/step
Epoch 847/1000
2023-10-27 12:15:11.922 
Epoch 847/1000 
	 loss: 40.7858, MinusLogProbMetric: 40.7858, val_loss: 41.9132, val_MinusLogProbMetric: 41.9132

Epoch 847: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.7858 - MinusLogProbMetric: 40.7858 - val_loss: 41.9132 - val_MinusLogProbMetric: 41.9132 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 848/1000
2023-10-27 12:15:41.018 
Epoch 848/1000 
	 loss: 40.7124, MinusLogProbMetric: 40.7124, val_loss: 41.8456, val_MinusLogProbMetric: 41.8456

Epoch 848: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7124 - MinusLogProbMetric: 40.7124 - val_loss: 41.8456 - val_MinusLogProbMetric: 41.8456 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 849/1000
2023-10-27 12:16:09.863 
Epoch 849/1000 
	 loss: 40.7721, MinusLogProbMetric: 40.7721, val_loss: 41.9199, val_MinusLogProbMetric: 41.9199

Epoch 849: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7721 - MinusLogProbMetric: 40.7721 - val_loss: 41.9199 - val_MinusLogProbMetric: 41.9199 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 850/1000
2023-10-27 12:16:39.004 
Epoch 850/1000 
	 loss: 40.9582, MinusLogProbMetric: 40.9582, val_loss: 41.9286, val_MinusLogProbMetric: 41.9286

Epoch 850: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.9582 - MinusLogProbMetric: 40.9582 - val_loss: 41.9286 - val_MinusLogProbMetric: 41.9286 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 851/1000
2023-10-27 12:17:10.412 
Epoch 851/1000 
	 loss: 40.6942, MinusLogProbMetric: 40.6942, val_loss: 42.2559, val_MinusLogProbMetric: 42.2559

Epoch 851: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.6942 - MinusLogProbMetric: 40.6942 - val_loss: 42.2559 - val_MinusLogProbMetric: 42.2559 - lr: 2.7778e-05 - 31s/epoch - 160ms/step
Epoch 852/1000
2023-10-27 12:17:41.284 
Epoch 852/1000 
	 loss: 40.6945, MinusLogProbMetric: 40.6945, val_loss: 42.1145, val_MinusLogProbMetric: 42.1145

Epoch 852: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.6945 - MinusLogProbMetric: 40.6945 - val_loss: 42.1145 - val_MinusLogProbMetric: 42.1145 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 853/1000
2023-10-27 12:18:10.731 
Epoch 853/1000 
	 loss: 40.7435, MinusLogProbMetric: 40.7435, val_loss: 41.9833, val_MinusLogProbMetric: 41.9833

Epoch 853: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7435 - MinusLogProbMetric: 40.7435 - val_loss: 41.9833 - val_MinusLogProbMetric: 41.9833 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 854/1000
2023-10-27 12:18:40.492 
Epoch 854/1000 
	 loss: 40.7167, MinusLogProbMetric: 40.7167, val_loss: 42.1024, val_MinusLogProbMetric: 42.1024

Epoch 854: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7167 - MinusLogProbMetric: 40.7167 - val_loss: 42.1024 - val_MinusLogProbMetric: 42.1024 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 855/1000
2023-10-27 12:19:09.662 
Epoch 855/1000 
	 loss: 40.9473, MinusLogProbMetric: 40.9473, val_loss: 41.9716, val_MinusLogProbMetric: 41.9716

Epoch 855: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.9473 - MinusLogProbMetric: 40.9473 - val_loss: 41.9716 - val_MinusLogProbMetric: 41.9716 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 856/1000
2023-10-27 12:19:39.142 
Epoch 856/1000 
	 loss: 40.6912, MinusLogProbMetric: 40.6912, val_loss: 41.8616, val_MinusLogProbMetric: 41.8616

Epoch 856: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6912 - MinusLogProbMetric: 40.6912 - val_loss: 41.8616 - val_MinusLogProbMetric: 41.8616 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 857/1000
2023-10-27 12:20:10.254 
Epoch 857/1000 
	 loss: 40.7269, MinusLogProbMetric: 40.7269, val_loss: 42.1046, val_MinusLogProbMetric: 42.1046

Epoch 857: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.7269 - MinusLogProbMetric: 40.7269 - val_loss: 42.1046 - val_MinusLogProbMetric: 42.1046 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 858/1000
2023-10-27 12:20:42.121 
Epoch 858/1000 
	 loss: 40.7243, MinusLogProbMetric: 40.7243, val_loss: 42.2502, val_MinusLogProbMetric: 42.2502

Epoch 858: val_loss did not improve from 41.84192
196/196 - 32s - loss: 40.7243 - MinusLogProbMetric: 40.7243 - val_loss: 42.2502 - val_MinusLogProbMetric: 42.2502 - lr: 2.7778e-05 - 32s/epoch - 163ms/step
Epoch 859/1000
2023-10-27 12:21:11.539 
Epoch 859/1000 
	 loss: 40.7975, MinusLogProbMetric: 40.7975, val_loss: 42.4583, val_MinusLogProbMetric: 42.4583

Epoch 859: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7975 - MinusLogProbMetric: 40.7975 - val_loss: 42.4583 - val_MinusLogProbMetric: 42.4583 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 860/1000
2023-10-27 12:21:40.757 
Epoch 860/1000 
	 loss: 40.7661, MinusLogProbMetric: 40.7661, val_loss: 41.8713, val_MinusLogProbMetric: 41.8713

Epoch 860: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7661 - MinusLogProbMetric: 40.7661 - val_loss: 41.8713 - val_MinusLogProbMetric: 41.8713 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 861/1000
2023-10-27 12:22:10.234 
Epoch 861/1000 
	 loss: 40.7250, MinusLogProbMetric: 40.7250, val_loss: 41.9297, val_MinusLogProbMetric: 41.9297

Epoch 861: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7250 - MinusLogProbMetric: 40.7250 - val_loss: 41.9297 - val_MinusLogProbMetric: 41.9297 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 862/1000
2023-10-27 12:22:40.099 
Epoch 862/1000 
	 loss: 40.7053, MinusLogProbMetric: 40.7053, val_loss: 42.0341, val_MinusLogProbMetric: 42.0341

Epoch 862: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7053 - MinusLogProbMetric: 40.7053 - val_loss: 42.0341 - val_MinusLogProbMetric: 42.0341 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 863/1000
2023-10-27 12:23:11.998 
Epoch 863/1000 
	 loss: 40.7608, MinusLogProbMetric: 40.7608, val_loss: 42.3195, val_MinusLogProbMetric: 42.3195

Epoch 863: val_loss did not improve from 41.84192
196/196 - 32s - loss: 40.7608 - MinusLogProbMetric: 40.7608 - val_loss: 42.3195 - val_MinusLogProbMetric: 42.3195 - lr: 2.7778e-05 - 32s/epoch - 163ms/step
Epoch 864/1000
2023-10-27 12:23:40.897 
Epoch 864/1000 
	 loss: 40.7059, MinusLogProbMetric: 40.7059, val_loss: 41.8933, val_MinusLogProbMetric: 41.8933

Epoch 864: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7059 - MinusLogProbMetric: 40.7059 - val_loss: 41.8933 - val_MinusLogProbMetric: 41.8933 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 865/1000
2023-10-27 12:24:10.136 
Epoch 865/1000 
	 loss: 40.7814, MinusLogProbMetric: 40.7814, val_loss: 41.9679, val_MinusLogProbMetric: 41.9679

Epoch 865: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7814 - MinusLogProbMetric: 40.7814 - val_loss: 41.9679 - val_MinusLogProbMetric: 41.9679 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 866/1000
2023-10-27 12:24:39.201 
Epoch 866/1000 
	 loss: 40.6894, MinusLogProbMetric: 40.6894, val_loss: 41.8603, val_MinusLogProbMetric: 41.8603

Epoch 866: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6894 - MinusLogProbMetric: 40.6894 - val_loss: 41.8603 - val_MinusLogProbMetric: 41.8603 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 867/1000
2023-10-27 12:25:08.331 
Epoch 867/1000 
	 loss: 40.7043, MinusLogProbMetric: 40.7043, val_loss: 41.8961, val_MinusLogProbMetric: 41.8961

Epoch 867: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7043 - MinusLogProbMetric: 40.7043 - val_loss: 41.8961 - val_MinusLogProbMetric: 41.8961 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 868/1000
2023-10-27 12:25:38.538 
Epoch 868/1000 
	 loss: 40.7506, MinusLogProbMetric: 40.7506, val_loss: 41.9580, val_MinusLogProbMetric: 41.9580

Epoch 868: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7506 - MinusLogProbMetric: 40.7506 - val_loss: 41.9580 - val_MinusLogProbMetric: 41.9580 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 869/1000
2023-10-27 12:26:09.615 
Epoch 869/1000 
	 loss: 40.7209, MinusLogProbMetric: 40.7209, val_loss: 42.0978, val_MinusLogProbMetric: 42.0978

Epoch 869: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.7209 - MinusLogProbMetric: 40.7209 - val_loss: 42.0978 - val_MinusLogProbMetric: 42.0978 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 870/1000
2023-10-27 12:26:40.073 
Epoch 870/1000 
	 loss: 40.7268, MinusLogProbMetric: 40.7268, val_loss: 42.0088, val_MinusLogProbMetric: 42.0088

Epoch 870: val_loss did not improve from 41.84192
196/196 - 30s - loss: 40.7268 - MinusLogProbMetric: 40.7268 - val_loss: 42.0088 - val_MinusLogProbMetric: 42.0088 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 871/1000
2023-10-27 12:27:09.029 
Epoch 871/1000 
	 loss: 40.6954, MinusLogProbMetric: 40.6954, val_loss: 41.9934, val_MinusLogProbMetric: 41.9934

Epoch 871: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6954 - MinusLogProbMetric: 40.6954 - val_loss: 41.9934 - val_MinusLogProbMetric: 41.9934 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 872/1000
2023-10-27 12:27:38.281 
Epoch 872/1000 
	 loss: 40.6960, MinusLogProbMetric: 40.6960, val_loss: 42.0439, val_MinusLogProbMetric: 42.0439

Epoch 872: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6960 - MinusLogProbMetric: 40.6960 - val_loss: 42.0439 - val_MinusLogProbMetric: 42.0439 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 873/1000
2023-10-27 12:28:07.504 
Epoch 873/1000 
	 loss: 40.7064, MinusLogProbMetric: 40.7064, val_loss: 42.7426, val_MinusLogProbMetric: 42.7426

Epoch 873: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.7064 - MinusLogProbMetric: 40.7064 - val_loss: 42.7426 - val_MinusLogProbMetric: 42.7426 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 874/1000
2023-10-27 12:28:38.757 
Epoch 874/1000 
	 loss: 40.6750, MinusLogProbMetric: 40.6750, val_loss: 41.9703, val_MinusLogProbMetric: 41.9703

Epoch 874: val_loss did not improve from 41.84192
196/196 - 31s - loss: 40.6750 - MinusLogProbMetric: 40.6750 - val_loss: 41.9703 - val_MinusLogProbMetric: 41.9703 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 875/1000
2023-10-27 12:29:10.351 
Epoch 875/1000 
	 loss: 40.8424, MinusLogProbMetric: 40.8424, val_loss: 41.9260, val_MinusLogProbMetric: 41.9260

Epoch 875: val_loss did not improve from 41.84192
196/196 - 32s - loss: 40.8424 - MinusLogProbMetric: 40.8424 - val_loss: 41.9260 - val_MinusLogProbMetric: 41.9260 - lr: 2.7778e-05 - 32s/epoch - 161ms/step
Epoch 876/1000
2023-10-27 12:29:39.084 
Epoch 876/1000 
	 loss: 40.6948, MinusLogProbMetric: 40.6948, val_loss: 41.9126, val_MinusLogProbMetric: 41.9126

Epoch 876: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6948 - MinusLogProbMetric: 40.6948 - val_loss: 41.9126 - val_MinusLogProbMetric: 41.9126 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 877/1000
2023-10-27 12:30:07.870 
Epoch 877/1000 
	 loss: 40.6688, MinusLogProbMetric: 40.6688, val_loss: 41.9323, val_MinusLogProbMetric: 41.9323

Epoch 877: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6688 - MinusLogProbMetric: 40.6688 - val_loss: 41.9323 - val_MinusLogProbMetric: 41.9323 - lr: 2.7778e-05 - 29s/epoch - 147ms/step
Epoch 878/1000
2023-10-27 12:30:37.350 
Epoch 878/1000 
	 loss: 40.6688, MinusLogProbMetric: 40.6688, val_loss: 42.2846, val_MinusLogProbMetric: 42.2846

Epoch 878: val_loss did not improve from 41.84192
196/196 - 29s - loss: 40.6688 - MinusLogProbMetric: 40.6688 - val_loss: 42.2846 - val_MinusLogProbMetric: 42.2846 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 879/1000
2023-10-27 12:31:06.779 
Epoch 879/1000 
	 loss: 40.6993, MinusLogProbMetric: 40.6993, val_loss: 41.8386, val_MinusLogProbMetric: 41.8386

Epoch 879: val_loss improved from 41.84192 to 41.83859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.6993 - MinusLogProbMetric: 40.6993 - val_loss: 41.8386 - val_MinusLogProbMetric: 41.8386 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 880/1000
2023-10-27 12:31:39.393 
Epoch 880/1000 
	 loss: 40.6810, MinusLogProbMetric: 40.6810, val_loss: 42.2246, val_MinusLogProbMetric: 42.2246

Epoch 880: val_loss did not improve from 41.83859
196/196 - 32s - loss: 40.6810 - MinusLogProbMetric: 40.6810 - val_loss: 42.2246 - val_MinusLogProbMetric: 42.2246 - lr: 2.7778e-05 - 32s/epoch - 164ms/step
Epoch 881/1000
2023-10-27 12:32:11.006 
Epoch 881/1000 
	 loss: 40.7512, MinusLogProbMetric: 40.7512, val_loss: 42.1214, val_MinusLogProbMetric: 42.1214

Epoch 881: val_loss did not improve from 41.83859
196/196 - 32s - loss: 40.7512 - MinusLogProbMetric: 40.7512 - val_loss: 42.1214 - val_MinusLogProbMetric: 42.1214 - lr: 2.7778e-05 - 32s/epoch - 161ms/step
Epoch 882/1000
2023-10-27 12:32:40.558 
Epoch 882/1000 
	 loss: 40.8553, MinusLogProbMetric: 40.8553, val_loss: 42.8759, val_MinusLogProbMetric: 42.8759

Epoch 882: val_loss did not improve from 41.83859
196/196 - 30s - loss: 40.8553 - MinusLogProbMetric: 40.8553 - val_loss: 42.8759 - val_MinusLogProbMetric: 42.8759 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 883/1000
2023-10-27 12:33:09.582 
Epoch 883/1000 
	 loss: 40.6912, MinusLogProbMetric: 40.6912, val_loss: 41.8382, val_MinusLogProbMetric: 41.8382

Epoch 883: val_loss improved from 41.83859 to 41.83817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.6912 - MinusLogProbMetric: 40.6912 - val_loss: 41.8382 - val_MinusLogProbMetric: 41.8382 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 884/1000
2023-10-27 12:33:39.389 
Epoch 884/1000 
	 loss: 40.6477, MinusLogProbMetric: 40.6477, val_loss: 41.8465, val_MinusLogProbMetric: 41.8465

Epoch 884: val_loss did not improve from 41.83817
196/196 - 29s - loss: 40.6477 - MinusLogProbMetric: 40.6477 - val_loss: 41.8465 - val_MinusLogProbMetric: 41.8465 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 885/1000
2023-10-27 12:34:09.606 
Epoch 885/1000 
	 loss: 40.6738, MinusLogProbMetric: 40.6738, val_loss: 42.0780, val_MinusLogProbMetric: 42.0780

Epoch 885: val_loss did not improve from 41.83817
196/196 - 30s - loss: 40.6738 - MinusLogProbMetric: 40.6738 - val_loss: 42.0780 - val_MinusLogProbMetric: 42.0780 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 886/1000
2023-10-27 12:34:40.890 
Epoch 886/1000 
	 loss: 40.7004, MinusLogProbMetric: 40.7004, val_loss: 41.9062, val_MinusLogProbMetric: 41.9062

Epoch 886: val_loss did not improve from 41.83817
196/196 - 31s - loss: 40.7004 - MinusLogProbMetric: 40.7004 - val_loss: 41.9062 - val_MinusLogProbMetric: 41.9062 - lr: 2.7778e-05 - 31s/epoch - 160ms/step
Epoch 887/1000
2023-10-27 12:35:10.652 
Epoch 887/1000 
	 loss: 40.6483, MinusLogProbMetric: 40.6483, val_loss: 41.9202, val_MinusLogProbMetric: 41.9202

Epoch 887: val_loss did not improve from 41.83817
196/196 - 30s - loss: 40.6483 - MinusLogProbMetric: 40.6483 - val_loss: 41.9202 - val_MinusLogProbMetric: 41.9202 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 888/1000
2023-10-27 12:35:39.993 
Epoch 888/1000 
	 loss: 40.6958, MinusLogProbMetric: 40.6958, val_loss: 41.9594, val_MinusLogProbMetric: 41.9594

Epoch 888: val_loss did not improve from 41.83817
196/196 - 29s - loss: 40.6958 - MinusLogProbMetric: 40.6958 - val_loss: 41.9594 - val_MinusLogProbMetric: 41.9594 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 889/1000
2023-10-27 12:36:11.781 
Epoch 889/1000 
	 loss: 40.6521, MinusLogProbMetric: 40.6521, val_loss: 42.5411, val_MinusLogProbMetric: 42.5411

Epoch 889: val_loss did not improve from 41.83817
196/196 - 32s - loss: 40.6521 - MinusLogProbMetric: 40.6521 - val_loss: 42.5411 - val_MinusLogProbMetric: 42.5411 - lr: 2.7778e-05 - 32s/epoch - 162ms/step
Epoch 890/1000
2023-10-27 12:36:41.417 
Epoch 890/1000 
	 loss: 40.7326, MinusLogProbMetric: 40.7326, val_loss: 41.8031, val_MinusLogProbMetric: 41.8031

Epoch 890: val_loss improved from 41.83817 to 41.80312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.7326 - MinusLogProbMetric: 40.7326 - val_loss: 41.8031 - val_MinusLogProbMetric: 41.8031 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 891/1000
2023-10-27 12:37:15.038 
Epoch 891/1000 
	 loss: 40.7101, MinusLogProbMetric: 40.7101, val_loss: 41.8909, val_MinusLogProbMetric: 41.8909

Epoch 891: val_loss did not improve from 41.80312
196/196 - 33s - loss: 40.7101 - MinusLogProbMetric: 40.7101 - val_loss: 41.8909 - val_MinusLogProbMetric: 41.8909 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 892/1000
2023-10-27 12:37:44.147 
Epoch 892/1000 
	 loss: 40.6960, MinusLogProbMetric: 40.6960, val_loss: 41.9772, val_MinusLogProbMetric: 41.9772

Epoch 892: val_loss did not improve from 41.80312
196/196 - 29s - loss: 40.6960 - MinusLogProbMetric: 40.6960 - val_loss: 41.9772 - val_MinusLogProbMetric: 41.9772 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 893/1000
2023-10-27 12:38:17.420 
Epoch 893/1000 
	 loss: 40.6617, MinusLogProbMetric: 40.6617, val_loss: 42.4908, val_MinusLogProbMetric: 42.4908

Epoch 893: val_loss did not improve from 41.80312
196/196 - 33s - loss: 40.6617 - MinusLogProbMetric: 40.6617 - val_loss: 42.4908 - val_MinusLogProbMetric: 42.4908 - lr: 2.7778e-05 - 33s/epoch - 170ms/step
Epoch 894/1000
2023-10-27 12:38:46.401 
Epoch 894/1000 
	 loss: 40.7470, MinusLogProbMetric: 40.7470, val_loss: 41.9404, val_MinusLogProbMetric: 41.9404

Epoch 894: val_loss did not improve from 41.80312
196/196 - 29s - loss: 40.7470 - MinusLogProbMetric: 40.7470 - val_loss: 41.9404 - val_MinusLogProbMetric: 41.9404 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 895/1000
2023-10-27 12:39:15.797 
Epoch 895/1000 
	 loss: 40.6529, MinusLogProbMetric: 40.6529, val_loss: 42.0685, val_MinusLogProbMetric: 42.0685

Epoch 895: val_loss did not improve from 41.80312
196/196 - 29s - loss: 40.6529 - MinusLogProbMetric: 40.6529 - val_loss: 42.0685 - val_MinusLogProbMetric: 42.0685 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 896/1000
2023-10-27 12:39:46.125 
Epoch 896/1000 
	 loss: 40.6956, MinusLogProbMetric: 40.6956, val_loss: 42.0783, val_MinusLogProbMetric: 42.0783

Epoch 896: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6956 - MinusLogProbMetric: 40.6956 - val_loss: 42.0783 - val_MinusLogProbMetric: 42.0783 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 897/1000
2023-10-27 12:40:16.373 
Epoch 897/1000 
	 loss: 40.6335, MinusLogProbMetric: 40.6335, val_loss: 41.8885, val_MinusLogProbMetric: 41.8885

Epoch 897: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6335 - MinusLogProbMetric: 40.6335 - val_loss: 41.8885 - val_MinusLogProbMetric: 41.8885 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 898/1000
2023-10-27 12:40:47.171 
Epoch 898/1000 
	 loss: 40.6640, MinusLogProbMetric: 40.6640, val_loss: 41.8437, val_MinusLogProbMetric: 41.8437

Epoch 898: val_loss did not improve from 41.80312
196/196 - 31s - loss: 40.6640 - MinusLogProbMetric: 40.6640 - val_loss: 41.8437 - val_MinusLogProbMetric: 41.8437 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 899/1000
2023-10-27 12:41:16.962 
Epoch 899/1000 
	 loss: 40.6766, MinusLogProbMetric: 40.6766, val_loss: 42.0786, val_MinusLogProbMetric: 42.0786

Epoch 899: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6766 - MinusLogProbMetric: 40.6766 - val_loss: 42.0786 - val_MinusLogProbMetric: 42.0786 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 900/1000
2023-10-27 12:41:47.379 
Epoch 900/1000 
	 loss: 40.6672, MinusLogProbMetric: 40.6672, val_loss: 41.8662, val_MinusLogProbMetric: 41.8662

Epoch 900: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6672 - MinusLogProbMetric: 40.6672 - val_loss: 41.8662 - val_MinusLogProbMetric: 41.8662 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 901/1000
2023-10-27 12:42:16.385 
Epoch 901/1000 
	 loss: 40.6979, MinusLogProbMetric: 40.6979, val_loss: 42.4042, val_MinusLogProbMetric: 42.4042

Epoch 901: val_loss did not improve from 41.80312
196/196 - 29s - loss: 40.6979 - MinusLogProbMetric: 40.6979 - val_loss: 42.4042 - val_MinusLogProbMetric: 42.4042 - lr: 2.7778e-05 - 29s/epoch - 148ms/step
Epoch 902/1000
2023-10-27 12:42:46.495 
Epoch 902/1000 
	 loss: 40.8671, MinusLogProbMetric: 40.8671, val_loss: 41.9072, val_MinusLogProbMetric: 41.9072

Epoch 902: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.8671 - MinusLogProbMetric: 40.8671 - val_loss: 41.9072 - val_MinusLogProbMetric: 41.9072 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 903/1000
2023-10-27 12:43:15.752 
Epoch 903/1000 
	 loss: 40.6692, MinusLogProbMetric: 40.6692, val_loss: 41.8476, val_MinusLogProbMetric: 41.8476

Epoch 903: val_loss did not improve from 41.80312
196/196 - 29s - loss: 40.6692 - MinusLogProbMetric: 40.6692 - val_loss: 41.8476 - val_MinusLogProbMetric: 41.8476 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 904/1000
2023-10-27 12:43:46.811 
Epoch 904/1000 
	 loss: 40.6851, MinusLogProbMetric: 40.6851, val_loss: 42.0890, val_MinusLogProbMetric: 42.0890

Epoch 904: val_loss did not improve from 41.80312
196/196 - 31s - loss: 40.6851 - MinusLogProbMetric: 40.6851 - val_loss: 42.0890 - val_MinusLogProbMetric: 42.0890 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 905/1000
2023-10-27 12:44:17.056 
Epoch 905/1000 
	 loss: 40.6648, MinusLogProbMetric: 40.6648, val_loss: 41.8217, val_MinusLogProbMetric: 41.8217

Epoch 905: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6648 - MinusLogProbMetric: 40.6648 - val_loss: 41.8217 - val_MinusLogProbMetric: 41.8217 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 906/1000
2023-10-27 12:44:47.502 
Epoch 906/1000 
	 loss: 40.6942, MinusLogProbMetric: 40.6942, val_loss: 42.0528, val_MinusLogProbMetric: 42.0528

Epoch 906: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6942 - MinusLogProbMetric: 40.6942 - val_loss: 42.0528 - val_MinusLogProbMetric: 42.0528 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 907/1000
2023-10-27 12:45:18.201 
Epoch 907/1000 
	 loss: 40.6690, MinusLogProbMetric: 40.6690, val_loss: 41.9394, val_MinusLogProbMetric: 41.9394

Epoch 907: val_loss did not improve from 41.80312
196/196 - 31s - loss: 40.6690 - MinusLogProbMetric: 40.6690 - val_loss: 41.9394 - val_MinusLogProbMetric: 41.9394 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 908/1000
2023-10-27 12:45:47.823 
Epoch 908/1000 
	 loss: 40.6309, MinusLogProbMetric: 40.6309, val_loss: 42.1055, val_MinusLogProbMetric: 42.1055

Epoch 908: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6309 - MinusLogProbMetric: 40.6309 - val_loss: 42.1055 - val_MinusLogProbMetric: 42.1055 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 909/1000
2023-10-27 12:46:17.335 
Epoch 909/1000 
	 loss: 40.7042, MinusLogProbMetric: 40.7042, val_loss: 42.9041, val_MinusLogProbMetric: 42.9041

Epoch 909: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.7042 - MinusLogProbMetric: 40.7042 - val_loss: 42.9041 - val_MinusLogProbMetric: 42.9041 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 910/1000
2023-10-27 12:46:46.861 
Epoch 910/1000 
	 loss: 40.6584, MinusLogProbMetric: 40.6584, val_loss: 41.8875, val_MinusLogProbMetric: 41.8875

Epoch 910: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6584 - MinusLogProbMetric: 40.6584 - val_loss: 41.8875 - val_MinusLogProbMetric: 41.8875 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 911/1000
2023-10-27 12:47:19.280 
Epoch 911/1000 
	 loss: 40.6521, MinusLogProbMetric: 40.6521, val_loss: 41.8631, val_MinusLogProbMetric: 41.8631

Epoch 911: val_loss did not improve from 41.80312
196/196 - 32s - loss: 40.6521 - MinusLogProbMetric: 40.6521 - val_loss: 41.8631 - val_MinusLogProbMetric: 41.8631 - lr: 2.7778e-05 - 32s/epoch - 165ms/step
Epoch 912/1000
2023-10-27 12:47:49.718 
Epoch 912/1000 
	 loss: 40.6646, MinusLogProbMetric: 40.6646, val_loss: 42.5442, val_MinusLogProbMetric: 42.5442

Epoch 912: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6646 - MinusLogProbMetric: 40.6646 - val_loss: 42.5442 - val_MinusLogProbMetric: 42.5442 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 913/1000
2023-10-27 12:48:22.334 
Epoch 913/1000 
	 loss: 40.6752, MinusLogProbMetric: 40.6752, val_loss: 41.9948, val_MinusLogProbMetric: 41.9948

Epoch 913: val_loss did not improve from 41.80312
196/196 - 33s - loss: 40.6752 - MinusLogProbMetric: 40.6752 - val_loss: 41.9948 - val_MinusLogProbMetric: 41.9948 - lr: 2.7778e-05 - 33s/epoch - 166ms/step
Epoch 914/1000
2023-10-27 12:48:53.543 
Epoch 914/1000 
	 loss: 40.6370, MinusLogProbMetric: 40.6370, val_loss: 41.8415, val_MinusLogProbMetric: 41.8415

Epoch 914: val_loss did not improve from 41.80312
196/196 - 31s - loss: 40.6370 - MinusLogProbMetric: 40.6370 - val_loss: 41.8415 - val_MinusLogProbMetric: 41.8415 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 915/1000
2023-10-27 12:49:23.558 
Epoch 915/1000 
	 loss: 40.6666, MinusLogProbMetric: 40.6666, val_loss: 41.8873, val_MinusLogProbMetric: 41.8873

Epoch 915: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6666 - MinusLogProbMetric: 40.6666 - val_loss: 41.8873 - val_MinusLogProbMetric: 41.8873 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 916/1000
2023-10-27 12:49:56.760 
Epoch 916/1000 
	 loss: 40.6491, MinusLogProbMetric: 40.6491, val_loss: 42.3456, val_MinusLogProbMetric: 42.3456

Epoch 916: val_loss did not improve from 41.80312
196/196 - 33s - loss: 40.6491 - MinusLogProbMetric: 40.6491 - val_loss: 42.3456 - val_MinusLogProbMetric: 42.3456 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 917/1000
2023-10-27 12:50:27.786 
Epoch 917/1000 
	 loss: 40.6866, MinusLogProbMetric: 40.6866, val_loss: 42.0441, val_MinusLogProbMetric: 42.0441

Epoch 917: val_loss did not improve from 41.80312
196/196 - 31s - loss: 40.6866 - MinusLogProbMetric: 40.6866 - val_loss: 42.0441 - val_MinusLogProbMetric: 42.0441 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 918/1000
2023-10-27 12:51:00.828 
Epoch 918/1000 
	 loss: 40.6228, MinusLogProbMetric: 40.6228, val_loss: 41.9633, val_MinusLogProbMetric: 41.9633

Epoch 918: val_loss did not improve from 41.80312
196/196 - 33s - loss: 40.6228 - MinusLogProbMetric: 40.6228 - val_loss: 41.9633 - val_MinusLogProbMetric: 41.9633 - lr: 2.7778e-05 - 33s/epoch - 169ms/step
Epoch 919/1000
2023-10-27 12:51:34.821 
Epoch 919/1000 
	 loss: 40.6549, MinusLogProbMetric: 40.6549, val_loss: 42.3863, val_MinusLogProbMetric: 42.3863

Epoch 919: val_loss did not improve from 41.80312
196/196 - 34s - loss: 40.6549 - MinusLogProbMetric: 40.6549 - val_loss: 42.3863 - val_MinusLogProbMetric: 42.3863 - lr: 2.7778e-05 - 34s/epoch - 173ms/step
Epoch 920/1000
2023-10-27 12:52:06.926 
Epoch 920/1000 
	 loss: 40.6575, MinusLogProbMetric: 40.6575, val_loss: 41.8457, val_MinusLogProbMetric: 41.8457

Epoch 920: val_loss did not improve from 41.80312
196/196 - 32s - loss: 40.6575 - MinusLogProbMetric: 40.6575 - val_loss: 41.8457 - val_MinusLogProbMetric: 41.8457 - lr: 2.7778e-05 - 32s/epoch - 164ms/step
Epoch 921/1000
2023-10-27 12:52:39.105 
Epoch 921/1000 
	 loss: 40.6916, MinusLogProbMetric: 40.6916, val_loss: 42.0116, val_MinusLogProbMetric: 42.0116

Epoch 921: val_loss did not improve from 41.80312
196/196 - 32s - loss: 40.6916 - MinusLogProbMetric: 40.6916 - val_loss: 42.0116 - val_MinusLogProbMetric: 42.0116 - lr: 2.7778e-05 - 32s/epoch - 164ms/step
Epoch 922/1000
2023-10-27 12:53:12.952 
Epoch 922/1000 
	 loss: 40.6508, MinusLogProbMetric: 40.6508, val_loss: 41.9130, val_MinusLogProbMetric: 41.9130

Epoch 922: val_loss did not improve from 41.80312
196/196 - 34s - loss: 40.6508 - MinusLogProbMetric: 40.6508 - val_loss: 41.9130 - val_MinusLogProbMetric: 41.9130 - lr: 2.7778e-05 - 34s/epoch - 173ms/step
Epoch 923/1000
2023-10-27 12:53:46.927 
Epoch 923/1000 
	 loss: 40.6311, MinusLogProbMetric: 40.6311, val_loss: 41.8443, val_MinusLogProbMetric: 41.8443

Epoch 923: val_loss did not improve from 41.80312
196/196 - 34s - loss: 40.6311 - MinusLogProbMetric: 40.6311 - val_loss: 41.8443 - val_MinusLogProbMetric: 41.8443 - lr: 2.7778e-05 - 34s/epoch - 173ms/step
Epoch 924/1000
2023-10-27 12:54:21.019 
Epoch 924/1000 
	 loss: 40.6810, MinusLogProbMetric: 40.6810, val_loss: 42.1242, val_MinusLogProbMetric: 42.1242

Epoch 924: val_loss did not improve from 41.80312
196/196 - 34s - loss: 40.6810 - MinusLogProbMetric: 40.6810 - val_loss: 42.1242 - val_MinusLogProbMetric: 42.1242 - lr: 2.7778e-05 - 34s/epoch - 174ms/step
Epoch 925/1000
2023-10-27 12:54:54.712 
Epoch 925/1000 
	 loss: 40.6408, MinusLogProbMetric: 40.6408, val_loss: 42.2570, val_MinusLogProbMetric: 42.2570

Epoch 925: val_loss did not improve from 41.80312
196/196 - 34s - loss: 40.6408 - MinusLogProbMetric: 40.6408 - val_loss: 42.2570 - val_MinusLogProbMetric: 42.2570 - lr: 2.7778e-05 - 34s/epoch - 172ms/step
Epoch 926/1000
2023-10-27 12:55:26.355 
Epoch 926/1000 
	 loss: 40.6617, MinusLogProbMetric: 40.6617, val_loss: 41.9024, val_MinusLogProbMetric: 41.9024

Epoch 926: val_loss did not improve from 41.80312
196/196 - 32s - loss: 40.6617 - MinusLogProbMetric: 40.6617 - val_loss: 41.9024 - val_MinusLogProbMetric: 41.9024 - lr: 2.7778e-05 - 32s/epoch - 161ms/step
Epoch 927/1000
2023-10-27 12:55:58.849 
Epoch 927/1000 
	 loss: 40.6404, MinusLogProbMetric: 40.6404, val_loss: 41.8844, val_MinusLogProbMetric: 41.8844

Epoch 927: val_loss did not improve from 41.80312
196/196 - 32s - loss: 40.6404 - MinusLogProbMetric: 40.6404 - val_loss: 41.8844 - val_MinusLogProbMetric: 41.8844 - lr: 2.7778e-05 - 32s/epoch - 166ms/step
Epoch 928/1000
2023-10-27 12:56:27.152 
Epoch 928/1000 
	 loss: 40.6288, MinusLogProbMetric: 40.6288, val_loss: 41.9670, val_MinusLogProbMetric: 41.9670

Epoch 928: val_loss did not improve from 41.80312
196/196 - 28s - loss: 40.6288 - MinusLogProbMetric: 40.6288 - val_loss: 41.9670 - val_MinusLogProbMetric: 41.9670 - lr: 2.7778e-05 - 28s/epoch - 144ms/step
Epoch 929/1000
2023-10-27 12:56:57.440 
Epoch 929/1000 
	 loss: 40.6674, MinusLogProbMetric: 40.6674, val_loss: 41.9364, val_MinusLogProbMetric: 41.9364

Epoch 929: val_loss did not improve from 41.80312
196/196 - 30s - loss: 40.6674 - MinusLogProbMetric: 40.6674 - val_loss: 41.9364 - val_MinusLogProbMetric: 41.9364 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 930/1000
2023-10-27 12:57:25.317 
Epoch 930/1000 
	 loss: 40.6160, MinusLogProbMetric: 40.6160, val_loss: 41.9725, val_MinusLogProbMetric: 41.9725

Epoch 930: val_loss did not improve from 41.80312
196/196 - 28s - loss: 40.6160 - MinusLogProbMetric: 40.6160 - val_loss: 41.9725 - val_MinusLogProbMetric: 41.9725 - lr: 2.7778e-05 - 28s/epoch - 142ms/step
Epoch 931/1000
2023-10-27 12:57:53.743 
Epoch 931/1000 
	 loss: 40.7104, MinusLogProbMetric: 40.7104, val_loss: 41.9879, val_MinusLogProbMetric: 41.9879

Epoch 931: val_loss did not improve from 41.80312
196/196 - 28s - loss: 40.7104 - MinusLogProbMetric: 40.7104 - val_loss: 41.9879 - val_MinusLogProbMetric: 41.9879 - lr: 2.7778e-05 - 28s/epoch - 145ms/step
Epoch 932/1000
2023-10-27 12:58:22.324 
Epoch 932/1000 
	 loss: 40.7039, MinusLogProbMetric: 40.7039, val_loss: 41.9558, val_MinusLogProbMetric: 41.9558

Epoch 932: val_loss did not improve from 41.80312
196/196 - 29s - loss: 40.7039 - MinusLogProbMetric: 40.7039 - val_loss: 41.9558 - val_MinusLogProbMetric: 41.9558 - lr: 2.7778e-05 - 29s/epoch - 146ms/step
Epoch 933/1000
2023-10-27 12:58:50.090 
Epoch 933/1000 
	 loss: 40.6406, MinusLogProbMetric: 40.6406, val_loss: 42.5280, val_MinusLogProbMetric: 42.5280

Epoch 933: val_loss did not improve from 41.80312
196/196 - 28s - loss: 40.6406 - MinusLogProbMetric: 40.6406 - val_loss: 42.5280 - val_MinusLogProbMetric: 42.5280 - lr: 2.7778e-05 - 28s/epoch - 142ms/step
Epoch 934/1000
2023-10-27 12:59:19.780 
Epoch 934/1000 
	 loss: 40.6025, MinusLogProbMetric: 40.6025, val_loss: 41.7803, val_MinusLogProbMetric: 41.7803

Epoch 934: val_loss improved from 41.80312 to 41.78026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.6025 - MinusLogProbMetric: 40.6025 - val_loss: 41.7803 - val_MinusLogProbMetric: 41.7803 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 935/1000
2023-10-27 12:59:53.989 
Epoch 935/1000 
	 loss: 40.6263, MinusLogProbMetric: 40.6263, val_loss: 41.9399, val_MinusLogProbMetric: 41.9399

Epoch 935: val_loss did not improve from 41.78026
196/196 - 34s - loss: 40.6263 - MinusLogProbMetric: 40.6263 - val_loss: 41.9399 - val_MinusLogProbMetric: 41.9399 - lr: 2.7778e-05 - 34s/epoch - 171ms/step
Epoch 936/1000
2023-10-27 13:00:23.663 
Epoch 936/1000 
	 loss: 40.6248, MinusLogProbMetric: 40.6248, val_loss: 42.1179, val_MinusLogProbMetric: 42.1179

Epoch 936: val_loss did not improve from 41.78026
196/196 - 30s - loss: 40.6248 - MinusLogProbMetric: 40.6248 - val_loss: 42.1179 - val_MinusLogProbMetric: 42.1179 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 937/1000
2023-10-27 13:00:53.767 
Epoch 937/1000 
	 loss: 40.6509, MinusLogProbMetric: 40.6509, val_loss: 41.7760, val_MinusLogProbMetric: 41.7760

Epoch 937: val_loss improved from 41.78026 to 41.77596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 40.6509 - MinusLogProbMetric: 40.6509 - val_loss: 41.7760 - val_MinusLogProbMetric: 41.7760 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 938/1000
2023-10-27 13:01:24.798 
Epoch 938/1000 
	 loss: 40.5823, MinusLogProbMetric: 40.5823, val_loss: 41.7833, val_MinusLogProbMetric: 41.7833

Epoch 938: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5823 - MinusLogProbMetric: 40.5823 - val_loss: 41.7833 - val_MinusLogProbMetric: 41.7833 - lr: 2.7778e-05 - 30s/epoch - 155ms/step
Epoch 939/1000
2023-10-27 13:01:56.976 
Epoch 939/1000 
	 loss: 40.6022, MinusLogProbMetric: 40.6022, val_loss: 41.8204, val_MinusLogProbMetric: 41.8204

Epoch 939: val_loss did not improve from 41.77596
196/196 - 32s - loss: 40.6022 - MinusLogProbMetric: 40.6022 - val_loss: 41.8204 - val_MinusLogProbMetric: 41.8204 - lr: 2.7778e-05 - 32s/epoch - 164ms/step
Epoch 940/1000
2023-10-27 13:02:27.626 
Epoch 940/1000 
	 loss: 40.6152, MinusLogProbMetric: 40.6152, val_loss: 42.0953, val_MinusLogProbMetric: 42.0953

Epoch 940: val_loss did not improve from 41.77596
196/196 - 31s - loss: 40.6152 - MinusLogProbMetric: 40.6152 - val_loss: 42.0953 - val_MinusLogProbMetric: 42.0953 - lr: 2.7778e-05 - 31s/epoch - 156ms/step
Epoch 941/1000
2023-10-27 13:02:56.916 
Epoch 941/1000 
	 loss: 40.6880, MinusLogProbMetric: 40.6880, val_loss: 41.9508, val_MinusLogProbMetric: 41.9508

Epoch 941: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.6880 - MinusLogProbMetric: 40.6880 - val_loss: 41.9508 - val_MinusLogProbMetric: 41.9508 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 942/1000
2023-10-27 13:03:26.557 
Epoch 942/1000 
	 loss: 40.5821, MinusLogProbMetric: 40.5821, val_loss: 41.9012, val_MinusLogProbMetric: 41.9012

Epoch 942: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5821 - MinusLogProbMetric: 40.5821 - val_loss: 41.9012 - val_MinusLogProbMetric: 41.9012 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 943/1000
2023-10-27 13:03:56.661 
Epoch 943/1000 
	 loss: 40.6239, MinusLogProbMetric: 40.6239, val_loss: 42.4324, val_MinusLogProbMetric: 42.4324

Epoch 943: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6239 - MinusLogProbMetric: 40.6239 - val_loss: 42.4324 - val_MinusLogProbMetric: 42.4324 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 944/1000
2023-10-27 13:04:27.346 
Epoch 944/1000 
	 loss: 40.5830, MinusLogProbMetric: 40.5830, val_loss: 42.1688, val_MinusLogProbMetric: 42.1688

Epoch 944: val_loss did not improve from 41.77596
196/196 - 31s - loss: 40.5830 - MinusLogProbMetric: 40.5830 - val_loss: 42.1688 - val_MinusLogProbMetric: 42.1688 - lr: 2.7778e-05 - 31s/epoch - 157ms/step
Epoch 945/1000
2023-10-27 13:04:57.361 
Epoch 945/1000 
	 loss: 40.6554, MinusLogProbMetric: 40.6554, val_loss: 41.8465, val_MinusLogProbMetric: 41.8465

Epoch 945: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6554 - MinusLogProbMetric: 40.6554 - val_loss: 41.8465 - val_MinusLogProbMetric: 41.8465 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 946/1000
2023-10-27 13:05:27.406 
Epoch 946/1000 
	 loss: 40.6359, MinusLogProbMetric: 40.6359, val_loss: 41.8359, val_MinusLogProbMetric: 41.8359

Epoch 946: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6359 - MinusLogProbMetric: 40.6359 - val_loss: 41.8359 - val_MinusLogProbMetric: 41.8359 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 947/1000
2023-10-27 13:05:57.425 
Epoch 947/1000 
	 loss: 40.6344, MinusLogProbMetric: 40.6344, val_loss: 41.9738, val_MinusLogProbMetric: 41.9738

Epoch 947: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6344 - MinusLogProbMetric: 40.6344 - val_loss: 41.9738 - val_MinusLogProbMetric: 41.9738 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 948/1000
2023-10-27 13:06:27.534 
Epoch 948/1000 
	 loss: 40.6256, MinusLogProbMetric: 40.6256, val_loss: 42.8413, val_MinusLogProbMetric: 42.8413

Epoch 948: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6256 - MinusLogProbMetric: 40.6256 - val_loss: 42.8413 - val_MinusLogProbMetric: 42.8413 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 949/1000
2023-10-27 13:06:57.405 
Epoch 949/1000 
	 loss: 40.6259, MinusLogProbMetric: 40.6259, val_loss: 41.9938, val_MinusLogProbMetric: 41.9938

Epoch 949: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6259 - MinusLogProbMetric: 40.6259 - val_loss: 41.9938 - val_MinusLogProbMetric: 41.9938 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 950/1000
2023-10-27 13:07:26.694 
Epoch 950/1000 
	 loss: 40.6081, MinusLogProbMetric: 40.6081, val_loss: 41.9978, val_MinusLogProbMetric: 41.9978

Epoch 950: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.6081 - MinusLogProbMetric: 40.6081 - val_loss: 41.9978 - val_MinusLogProbMetric: 41.9978 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 951/1000
2023-10-27 13:07:56.410 
Epoch 951/1000 
	 loss: 40.6288, MinusLogProbMetric: 40.6288, val_loss: 41.9499, val_MinusLogProbMetric: 41.9499

Epoch 951: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6288 - MinusLogProbMetric: 40.6288 - val_loss: 41.9499 - val_MinusLogProbMetric: 41.9499 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 952/1000
2023-10-27 13:08:26.220 
Epoch 952/1000 
	 loss: 40.6162, MinusLogProbMetric: 40.6162, val_loss: 42.1140, val_MinusLogProbMetric: 42.1140

Epoch 952: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6162 - MinusLogProbMetric: 40.6162 - val_loss: 42.1140 - val_MinusLogProbMetric: 42.1140 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 953/1000
2023-10-27 13:08:55.724 
Epoch 953/1000 
	 loss: 40.6009, MinusLogProbMetric: 40.6009, val_loss: 41.9705, val_MinusLogProbMetric: 41.9705

Epoch 953: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.6009 - MinusLogProbMetric: 40.6009 - val_loss: 41.9705 - val_MinusLogProbMetric: 41.9705 - lr: 2.7778e-05 - 29s/epoch - 151ms/step
Epoch 954/1000
2023-10-27 13:09:25.583 
Epoch 954/1000 
	 loss: 40.6380, MinusLogProbMetric: 40.6380, val_loss: 41.8717, val_MinusLogProbMetric: 41.8717

Epoch 954: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6380 - MinusLogProbMetric: 40.6380 - val_loss: 41.8717 - val_MinusLogProbMetric: 41.8717 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 955/1000
2023-10-27 13:09:55.269 
Epoch 955/1000 
	 loss: 40.5966, MinusLogProbMetric: 40.5966, val_loss: 42.0873, val_MinusLogProbMetric: 42.0873

Epoch 955: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5966 - MinusLogProbMetric: 40.5966 - val_loss: 42.0873 - val_MinusLogProbMetric: 42.0873 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 956/1000
2023-10-27 13:10:24.860 
Epoch 956/1000 
	 loss: 40.6501, MinusLogProbMetric: 40.6501, val_loss: 41.9997, val_MinusLogProbMetric: 41.9997

Epoch 956: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6501 - MinusLogProbMetric: 40.6501 - val_loss: 41.9997 - val_MinusLogProbMetric: 41.9997 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 957/1000
2023-10-27 13:10:54.954 
Epoch 957/1000 
	 loss: 40.5627, MinusLogProbMetric: 40.5627, val_loss: 41.9624, val_MinusLogProbMetric: 41.9624

Epoch 957: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5627 - MinusLogProbMetric: 40.5627 - val_loss: 41.9624 - val_MinusLogProbMetric: 41.9624 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 958/1000
2023-10-27 13:11:24.343 
Epoch 958/1000 
	 loss: 40.6354, MinusLogProbMetric: 40.6354, val_loss: 41.8019, val_MinusLogProbMetric: 41.8019

Epoch 958: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.6354 - MinusLogProbMetric: 40.6354 - val_loss: 41.8019 - val_MinusLogProbMetric: 41.8019 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 959/1000
2023-10-27 13:11:54.009 
Epoch 959/1000 
	 loss: 40.6143, MinusLogProbMetric: 40.6143, val_loss: 41.8592, val_MinusLogProbMetric: 41.8592

Epoch 959: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6143 - MinusLogProbMetric: 40.6143 - val_loss: 41.8592 - val_MinusLogProbMetric: 41.8592 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 960/1000
2023-10-27 13:12:23.309 
Epoch 960/1000 
	 loss: 40.6153, MinusLogProbMetric: 40.6153, val_loss: 42.0112, val_MinusLogProbMetric: 42.0112

Epoch 960: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.6153 - MinusLogProbMetric: 40.6153 - val_loss: 42.0112 - val_MinusLogProbMetric: 42.0112 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 961/1000
2023-10-27 13:12:53.132 
Epoch 961/1000 
	 loss: 40.6166, MinusLogProbMetric: 40.6166, val_loss: 42.0565, val_MinusLogProbMetric: 42.0565

Epoch 961: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6166 - MinusLogProbMetric: 40.6166 - val_loss: 42.0565 - val_MinusLogProbMetric: 42.0565 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 962/1000
2023-10-27 13:13:22.967 
Epoch 962/1000 
	 loss: 40.5997, MinusLogProbMetric: 40.5997, val_loss: 41.9381, val_MinusLogProbMetric: 41.9381

Epoch 962: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5997 - MinusLogProbMetric: 40.5997 - val_loss: 41.9381 - val_MinusLogProbMetric: 41.9381 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 963/1000
2023-10-27 13:13:53.033 
Epoch 963/1000 
	 loss: 40.5351, MinusLogProbMetric: 40.5351, val_loss: 42.0059, val_MinusLogProbMetric: 42.0059

Epoch 963: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5351 - MinusLogProbMetric: 40.5351 - val_loss: 42.0059 - val_MinusLogProbMetric: 42.0059 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 964/1000
2023-10-27 13:14:22.634 
Epoch 964/1000 
	 loss: 40.6573, MinusLogProbMetric: 40.6573, val_loss: 42.2615, val_MinusLogProbMetric: 42.2615

Epoch 964: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6573 - MinusLogProbMetric: 40.6573 - val_loss: 42.2615 - val_MinusLogProbMetric: 42.2615 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 965/1000
2023-10-27 13:14:52.148 
Epoch 965/1000 
	 loss: 40.5864, MinusLogProbMetric: 40.5864, val_loss: 41.8499, val_MinusLogProbMetric: 41.8499

Epoch 965: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5864 - MinusLogProbMetric: 40.5864 - val_loss: 41.8499 - val_MinusLogProbMetric: 41.8499 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 966/1000
2023-10-27 13:15:21.880 
Epoch 966/1000 
	 loss: 40.5709, MinusLogProbMetric: 40.5709, val_loss: 41.8997, val_MinusLogProbMetric: 41.8997

Epoch 966: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5709 - MinusLogProbMetric: 40.5709 - val_loss: 41.8997 - val_MinusLogProbMetric: 41.8997 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 967/1000
2023-10-27 13:15:51.429 
Epoch 967/1000 
	 loss: 40.6059, MinusLogProbMetric: 40.6059, val_loss: 41.9095, val_MinusLogProbMetric: 41.9095

Epoch 967: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6059 - MinusLogProbMetric: 40.6059 - val_loss: 41.9095 - val_MinusLogProbMetric: 41.9095 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 968/1000
2023-10-27 13:16:21.365 
Epoch 968/1000 
	 loss: 40.5782, MinusLogProbMetric: 40.5782, val_loss: 41.8180, val_MinusLogProbMetric: 41.8180

Epoch 968: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5782 - MinusLogProbMetric: 40.5782 - val_loss: 41.8180 - val_MinusLogProbMetric: 41.8180 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 969/1000
2023-10-27 13:16:51.480 
Epoch 969/1000 
	 loss: 40.6667, MinusLogProbMetric: 40.6667, val_loss: 41.9834, val_MinusLogProbMetric: 41.9834

Epoch 969: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6667 - MinusLogProbMetric: 40.6667 - val_loss: 41.9834 - val_MinusLogProbMetric: 41.9834 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 970/1000
2023-10-27 13:17:21.370 
Epoch 970/1000 
	 loss: 40.5720, MinusLogProbMetric: 40.5720, val_loss: 41.8119, val_MinusLogProbMetric: 41.8119

Epoch 970: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5720 - MinusLogProbMetric: 40.5720 - val_loss: 41.8119 - val_MinusLogProbMetric: 41.8119 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 971/1000
2023-10-27 13:17:50.664 
Epoch 971/1000 
	 loss: 40.5581, MinusLogProbMetric: 40.5581, val_loss: 41.8676, val_MinusLogProbMetric: 41.8676

Epoch 971: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.5581 - MinusLogProbMetric: 40.5581 - val_loss: 41.8676 - val_MinusLogProbMetric: 41.8676 - lr: 2.7778e-05 - 29s/epoch - 149ms/step
Epoch 972/1000
2023-10-27 13:18:20.235 
Epoch 972/1000 
	 loss: 40.5867, MinusLogProbMetric: 40.5867, val_loss: 41.8366, val_MinusLogProbMetric: 41.8366

Epoch 972: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5867 - MinusLogProbMetric: 40.5867 - val_loss: 41.8366 - val_MinusLogProbMetric: 41.8366 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 973/1000
2023-10-27 13:18:49.946 
Epoch 973/1000 
	 loss: 40.5446, MinusLogProbMetric: 40.5446, val_loss: 41.9079, val_MinusLogProbMetric: 41.9079

Epoch 973: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5446 - MinusLogProbMetric: 40.5446 - val_loss: 41.9079 - val_MinusLogProbMetric: 41.9079 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 974/1000
2023-10-27 13:19:19.673 
Epoch 974/1000 
	 loss: 40.5891, MinusLogProbMetric: 40.5891, val_loss: 42.0897, val_MinusLogProbMetric: 42.0897

Epoch 974: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5891 - MinusLogProbMetric: 40.5891 - val_loss: 42.0897 - val_MinusLogProbMetric: 42.0897 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 975/1000
2023-10-27 13:19:50.842 
Epoch 975/1000 
	 loss: 40.5545, MinusLogProbMetric: 40.5545, val_loss: 42.0745, val_MinusLogProbMetric: 42.0745

Epoch 975: val_loss did not improve from 41.77596
196/196 - 31s - loss: 40.5545 - MinusLogProbMetric: 40.5545 - val_loss: 42.0745 - val_MinusLogProbMetric: 42.0745 - lr: 2.7778e-05 - 31s/epoch - 159ms/step
Epoch 976/1000
2023-10-27 13:20:20.529 
Epoch 976/1000 
	 loss: 40.5841, MinusLogProbMetric: 40.5841, val_loss: 42.2489, val_MinusLogProbMetric: 42.2489

Epoch 976: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5841 - MinusLogProbMetric: 40.5841 - val_loss: 42.2489 - val_MinusLogProbMetric: 42.2489 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 977/1000
2023-10-27 13:20:50.495 
Epoch 977/1000 
	 loss: 40.6147, MinusLogProbMetric: 40.6147, val_loss: 41.9230, val_MinusLogProbMetric: 41.9230

Epoch 977: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6147 - MinusLogProbMetric: 40.6147 - val_loss: 41.9230 - val_MinusLogProbMetric: 41.9230 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 978/1000
2023-10-27 13:21:20.062 
Epoch 978/1000 
	 loss: 40.5736, MinusLogProbMetric: 40.5736, val_loss: 41.9374, val_MinusLogProbMetric: 41.9374

Epoch 978: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5736 - MinusLogProbMetric: 40.5736 - val_loss: 41.9374 - val_MinusLogProbMetric: 41.9374 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 979/1000
2023-10-27 13:21:50.272 
Epoch 979/1000 
	 loss: 40.5475, MinusLogProbMetric: 40.5475, val_loss: 41.8169, val_MinusLogProbMetric: 41.8169

Epoch 979: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5475 - MinusLogProbMetric: 40.5475 - val_loss: 41.8169 - val_MinusLogProbMetric: 41.8169 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 980/1000
2023-10-27 13:22:20.450 
Epoch 980/1000 
	 loss: 40.5955, MinusLogProbMetric: 40.5955, val_loss: 42.1992, val_MinusLogProbMetric: 42.1992

Epoch 980: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5955 - MinusLogProbMetric: 40.5955 - val_loss: 42.1992 - val_MinusLogProbMetric: 42.1992 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 981/1000
2023-10-27 13:22:50.307 
Epoch 981/1000 
	 loss: 40.5761, MinusLogProbMetric: 40.5761, val_loss: 41.8256, val_MinusLogProbMetric: 41.8256

Epoch 981: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5761 - MinusLogProbMetric: 40.5761 - val_loss: 41.8256 - val_MinusLogProbMetric: 41.8256 - lr: 2.7778e-05 - 30s/epoch - 152ms/step
Epoch 982/1000
2023-10-27 13:23:20.470 
Epoch 982/1000 
	 loss: 40.5657, MinusLogProbMetric: 40.5657, val_loss: 41.7816, val_MinusLogProbMetric: 41.7816

Epoch 982: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5657 - MinusLogProbMetric: 40.5657 - val_loss: 41.7816 - val_MinusLogProbMetric: 41.7816 - lr: 2.7778e-05 - 30s/epoch - 154ms/step
Epoch 983/1000
2023-10-27 13:23:51.429 
Epoch 983/1000 
	 loss: 40.5580, MinusLogProbMetric: 40.5580, val_loss: 41.9569, val_MinusLogProbMetric: 41.9569

Epoch 983: val_loss did not improve from 41.77596
196/196 - 31s - loss: 40.5580 - MinusLogProbMetric: 40.5580 - val_loss: 41.9569 - val_MinusLogProbMetric: 41.9569 - lr: 2.7778e-05 - 31s/epoch - 158ms/step
Epoch 984/1000
2023-10-27 13:24:21.052 
Epoch 984/1000 
	 loss: 40.5876, MinusLogProbMetric: 40.5876, val_loss: 41.8366, val_MinusLogProbMetric: 41.8366

Epoch 984: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5876 - MinusLogProbMetric: 40.5876 - val_loss: 41.8366 - val_MinusLogProbMetric: 41.8366 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 985/1000
2023-10-27 13:24:51.073 
Epoch 985/1000 
	 loss: 40.5964, MinusLogProbMetric: 40.5964, val_loss: 41.9146, val_MinusLogProbMetric: 41.9146

Epoch 985: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.5964 - MinusLogProbMetric: 40.5964 - val_loss: 41.9146 - val_MinusLogProbMetric: 41.9146 - lr: 2.7778e-05 - 30s/epoch - 153ms/step
Epoch 986/1000
2023-10-27 13:25:20.565 
Epoch 986/1000 
	 loss: 40.5760, MinusLogProbMetric: 40.5760, val_loss: 42.2698, val_MinusLogProbMetric: 42.2698

Epoch 986: val_loss did not improve from 41.77596
196/196 - 29s - loss: 40.5760 - MinusLogProbMetric: 40.5760 - val_loss: 42.2698 - val_MinusLogProbMetric: 42.2698 - lr: 2.7778e-05 - 29s/epoch - 150ms/step
Epoch 987/1000
2023-10-27 13:25:50.220 
Epoch 987/1000 
	 loss: 40.6054, MinusLogProbMetric: 40.6054, val_loss: 42.0501, val_MinusLogProbMetric: 42.0501

Epoch 987: val_loss did not improve from 41.77596
196/196 - 30s - loss: 40.6054 - MinusLogProbMetric: 40.6054 - val_loss: 42.0501 - val_MinusLogProbMetric: 42.0501 - lr: 2.7778e-05 - 30s/epoch - 151ms/step
Epoch 988/1000
2023-10-27 13:26:21.513 
Epoch 988/1000 
	 loss: 40.3918, MinusLogProbMetric: 40.3918, val_loss: 41.8406, val_MinusLogProbMetric: 41.8406

Epoch 988: val_loss did not improve from 41.77596
196/196 - 31s - loss: 40.3918 - MinusLogProbMetric: 40.3918 - val_loss: 41.8406 - val_MinusLogProbMetric: 41.8406 - lr: 1.3889e-05 - 31s/epoch - 160ms/step
Epoch 989/1000
2023-10-27 13:26:52.414 
Epoch 989/1000 
	 loss: 40.3765, MinusLogProbMetric: 40.3765, val_loss: 41.7292, val_MinusLogProbMetric: 41.7292

Epoch 989: val_loss improved from 41.77596 to 41.72916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 31s - loss: 40.3765 - MinusLogProbMetric: 40.3765 - val_loss: 41.7292 - val_MinusLogProbMetric: 41.7292 - lr: 1.3889e-05 - 31s/epoch - 160ms/step
Epoch 990/1000
2023-10-27 13:27:22.014 
Epoch 990/1000 
	 loss: 40.3606, MinusLogProbMetric: 40.3606, val_loss: 41.7450, val_MinusLogProbMetric: 41.7450

Epoch 990: val_loss did not improve from 41.72916
196/196 - 29s - loss: 40.3606 - MinusLogProbMetric: 40.3606 - val_loss: 41.7450 - val_MinusLogProbMetric: 41.7450 - lr: 1.3889e-05 - 29s/epoch - 149ms/step
Epoch 991/1000
2023-10-27 13:27:51.209 
Epoch 991/1000 
	 loss: 40.3761, MinusLogProbMetric: 40.3761, val_loss: 41.7096, val_MinusLogProbMetric: 41.7096

Epoch 991: val_loss improved from 41.72916 to 41.70960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.3761 - MinusLogProbMetric: 40.3761 - val_loss: 41.7096 - val_MinusLogProbMetric: 41.7096 - lr: 1.3889e-05 - 30s/epoch - 152ms/step
Epoch 992/1000
2023-10-27 13:28:21.501 
Epoch 992/1000 
	 loss: 40.3684, MinusLogProbMetric: 40.3684, val_loss: 41.6894, val_MinusLogProbMetric: 41.6894

Epoch 992: val_loss improved from 41.70960 to 41.68944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_402/weights/best_weights.h5
196/196 - 30s - loss: 40.3684 - MinusLogProbMetric: 40.3684 - val_loss: 41.6894 - val_MinusLogProbMetric: 41.6894 - lr: 1.3889e-05 - 30s/epoch - 154ms/step
Epoch 993/1000
2023-10-27 13:28:51.916 
Epoch 993/1000 
	 loss: 40.3674, MinusLogProbMetric: 40.3674, val_loss: 41.7654, val_MinusLogProbMetric: 41.7654

Epoch 993: val_loss did not improve from 41.68944
196/196 - 30s - loss: 40.3674 - MinusLogProbMetric: 40.3674 - val_loss: 41.7654 - val_MinusLogProbMetric: 41.7654 - lr: 1.3889e-05 - 30s/epoch - 153ms/step
Epoch 994/1000
2023-10-27 13:29:23.228 
Epoch 994/1000 
	 loss: 40.3685, MinusLogProbMetric: 40.3685, val_loss: 41.7253, val_MinusLogProbMetric: 41.7253

Epoch 994: val_loss did not improve from 41.68944
196/196 - 31s - loss: 40.3685 - MinusLogProbMetric: 40.3685 - val_loss: 41.7253 - val_MinusLogProbMetric: 41.7253 - lr: 1.3889e-05 - 31s/epoch - 160ms/step
Epoch 995/1000
2023-10-27 13:29:55.300 
Epoch 995/1000 
	 loss: 40.3806, MinusLogProbMetric: 40.3806, val_loss: 41.7108, val_MinusLogProbMetric: 41.7108

Epoch 995: val_loss did not improve from 41.68944
196/196 - 32s - loss: 40.3806 - MinusLogProbMetric: 40.3806 - val_loss: 41.7108 - val_MinusLogProbMetric: 41.7108 - lr: 1.3889e-05 - 32s/epoch - 164ms/step
Epoch 996/1000
2023-10-27 13:30:24.854 
Epoch 996/1000 
	 loss: 40.3669, MinusLogProbMetric: 40.3669, val_loss: 41.9419, val_MinusLogProbMetric: 41.9419

Epoch 996: val_loss did not improve from 41.68944
196/196 - 30s - loss: 40.3669 - MinusLogProbMetric: 40.3669 - val_loss: 41.9419 - val_MinusLogProbMetric: 41.9419 - lr: 1.3889e-05 - 30s/epoch - 151ms/step
Epoch 997/1000
2023-10-27 13:30:54.485 
Epoch 997/1000 
	 loss: 40.3699, MinusLogProbMetric: 40.3699, val_loss: 41.7487, val_MinusLogProbMetric: 41.7487

Epoch 997: val_loss did not improve from 41.68944
196/196 - 30s - loss: 40.3699 - MinusLogProbMetric: 40.3699 - val_loss: 41.7487 - val_MinusLogProbMetric: 41.7487 - lr: 1.3889e-05 - 30s/epoch - 151ms/step
Epoch 998/1000
2023-10-27 13:31:24.017 
Epoch 998/1000 
	 loss: 40.3718, MinusLogProbMetric: 40.3718, val_loss: 41.7030, val_MinusLogProbMetric: 41.7030

Epoch 998: val_loss did not improve from 41.68944
196/196 - 30s - loss: 40.3718 - MinusLogProbMetric: 40.3718 - val_loss: 41.7030 - val_MinusLogProbMetric: 41.7030 - lr: 1.3889e-05 - 30s/epoch - 151ms/step
Epoch 999/1000
2023-10-27 13:31:56.260 
Epoch 999/1000 
	 loss: 40.3646, MinusLogProbMetric: 40.3646, val_loss: 41.7173, val_MinusLogProbMetric: 41.7173

Epoch 999: val_loss did not improve from 41.68944
196/196 - 32s - loss: 40.3646 - MinusLogProbMetric: 40.3646 - val_loss: 41.7173 - val_MinusLogProbMetric: 41.7173 - lr: 1.3889e-05 - 32s/epoch - 164ms/step
Epoch 1000/1000
2023-10-27 13:32:29.684 
Epoch 1000/1000 
	 loss: 40.3667, MinusLogProbMetric: 40.3667, val_loss: 41.7838, val_MinusLogProbMetric: 41.7838

Epoch 1000: val_loss did not improve from 41.68944
196/196 - 33s - loss: 40.3667 - MinusLogProbMetric: 40.3667 - val_loss: 41.7838 - val_MinusLogProbMetric: 41.7838 - lr: 1.3889e-05 - 33s/epoch - 171ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 0.
Model trained in 32321.29 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.07 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.41 s.
===========
Run 402/720 done in 33926.83 s.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_414/ already exists.
Skipping it.
===========
Run 414/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_417/ already exists.
Skipping it.
===========
Run 417/720 already exists. Skipping it.
===========

===========
Generating train data for run 418.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_418/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_418/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_418/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_418
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_431"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_432 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7feb50415d50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb5dc0a020>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb5dc0a020>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb85901d20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb85941ff0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb85942560>, <keras.callbacks.ModelCheckpoint object at 0x7feb85942620>, <keras.callbacks.EarlyStopping object at 0x7feb85942890>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb859428c0>, <keras.callbacks.TerminateOnNaN object at 0x7feb85942500>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_418/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 418/720 with hyperparameters:
timestamp = 2023-10-27 13:32:35.370105
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:33:36.366 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5889.6812, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 5889.6812 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 61s/epoch - 311ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 418.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_418/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_418/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_418/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_418
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_437"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_438 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7feb9f0b5180>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec76740f10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec76740f10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb0170eb30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb9d911e70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb9d9123e0>, <keras.callbacks.ModelCheckpoint object at 0x7feb9d9124a0>, <keras.callbacks.EarlyStopping object at 0x7feb9d912710>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb9d912740>, <keras.callbacks.TerminateOnNaN object at 0x7feb9d912380>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_418/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 418/720 with hyperparameters:
timestamp = 2023-10-27 13:33:39.509808
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
2023-10-27 13:35:21.981 
Epoch 1/1000 
	 loss: 1232.5677, MinusLogProbMetric: 1232.5677, val_loss: 371.7600, val_MinusLogProbMetric: 371.7600

Epoch 1: val_loss improved from inf to 371.76004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 103s - loss: 1232.5677 - MinusLogProbMetric: 1232.5677 - val_loss: 371.7600 - val_MinusLogProbMetric: 371.7600 - lr: 3.3333e-04 - 103s/epoch - 525ms/step
Epoch 2/1000
2023-10-27 13:35:52.092 
Epoch 2/1000 
	 loss: 304.1665, MinusLogProbMetric: 304.1665, val_loss: 257.6862, val_MinusLogProbMetric: 257.6862

Epoch 2: val_loss improved from 371.76004 to 257.68619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 304.1665 - MinusLogProbMetric: 304.1665 - val_loss: 257.6862 - val_MinusLogProbMetric: 257.6862 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 3/1000
2023-10-27 13:36:22.368 
Epoch 3/1000 
	 loss: 228.5738, MinusLogProbMetric: 228.5738, val_loss: 208.1374, val_MinusLogProbMetric: 208.1374

Epoch 3: val_loss improved from 257.68619 to 208.13736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 228.5738 - MinusLogProbMetric: 228.5738 - val_loss: 208.1374 - val_MinusLogProbMetric: 208.1374 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 4/1000
2023-10-27 13:36:52.369 
Epoch 4/1000 
	 loss: 204.2275, MinusLogProbMetric: 204.2275, val_loss: 183.2753, val_MinusLogProbMetric: 183.2753

Epoch 4: val_loss improved from 208.13736 to 183.27531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 204.2275 - MinusLogProbMetric: 204.2275 - val_loss: 183.2753 - val_MinusLogProbMetric: 183.2753 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 5/1000
2023-10-27 13:37:22.209 
Epoch 5/1000 
	 loss: 172.3699, MinusLogProbMetric: 172.3699, val_loss: 161.3811, val_MinusLogProbMetric: 161.3811

Epoch 5: val_loss improved from 183.27531 to 161.38112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 172.3699 - MinusLogProbMetric: 172.3699 - val_loss: 161.3811 - val_MinusLogProbMetric: 161.3811 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 6/1000
2023-10-27 13:37:56.786 
Epoch 6/1000 
	 loss: 166.0320, MinusLogProbMetric: 166.0320, val_loss: 151.9945, val_MinusLogProbMetric: 151.9945

Epoch 6: val_loss improved from 161.38112 to 151.99454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 35s - loss: 166.0320 - MinusLogProbMetric: 166.0320 - val_loss: 151.9945 - val_MinusLogProbMetric: 151.9945 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 7/1000
2023-10-27 13:38:30.155 
Epoch 7/1000 
	 loss: 142.7539, MinusLogProbMetric: 142.7539, val_loss: 134.9393, val_MinusLogProbMetric: 134.9393

Epoch 7: val_loss improved from 151.99454 to 134.93935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 142.7539 - MinusLogProbMetric: 142.7539 - val_loss: 134.9393 - val_MinusLogProbMetric: 134.9393 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 8/1000
2023-10-27 13:39:00.287 
Epoch 8/1000 
	 loss: 127.8031, MinusLogProbMetric: 127.8031, val_loss: 124.4169, val_MinusLogProbMetric: 124.4169

Epoch 8: val_loss improved from 134.93935 to 124.41694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 127.8031 - MinusLogProbMetric: 127.8031 - val_loss: 124.4169 - val_MinusLogProbMetric: 124.4169 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 9/1000
2023-10-27 13:39:30.153 
Epoch 9/1000 
	 loss: 117.5457, MinusLogProbMetric: 117.5457, val_loss: 113.5589, val_MinusLogProbMetric: 113.5589

Epoch 9: val_loss improved from 124.41694 to 113.55888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 117.5457 - MinusLogProbMetric: 117.5457 - val_loss: 113.5589 - val_MinusLogProbMetric: 113.5589 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 10/1000
2023-10-27 13:40:00.008 
Epoch 10/1000 
	 loss: 109.5025, MinusLogProbMetric: 109.5025, val_loss: 106.3961, val_MinusLogProbMetric: 106.3961

Epoch 10: val_loss improved from 113.55888 to 106.39612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 109.5025 - MinusLogProbMetric: 109.5025 - val_loss: 106.3961 - val_MinusLogProbMetric: 106.3961 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 11/1000
2023-10-27 13:40:31.485 
Epoch 11/1000 
	 loss: 111.8467, MinusLogProbMetric: 111.8467, val_loss: 109.0172, val_MinusLogProbMetric: 109.0172

Epoch 11: val_loss did not improve from 106.39612
196/196 - 31s - loss: 111.8467 - MinusLogProbMetric: 111.8467 - val_loss: 109.0172 - val_MinusLogProbMetric: 109.0172 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 12/1000
2023-10-27 13:41:05.363 
Epoch 12/1000 
	 loss: 115.9646, MinusLogProbMetric: 115.9646, val_loss: 108.6967, val_MinusLogProbMetric: 108.6967

Epoch 12: val_loss did not improve from 106.39612
196/196 - 34s - loss: 115.9646 - MinusLogProbMetric: 115.9646 - val_loss: 108.6967 - val_MinusLogProbMetric: 108.6967 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 13/1000
2023-10-27 13:41:35.843 
Epoch 13/1000 
	 loss: 101.1668, MinusLogProbMetric: 101.1668, val_loss: 96.4049, val_MinusLogProbMetric: 96.4049

Epoch 13: val_loss improved from 106.39612 to 96.40489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 101.1668 - MinusLogProbMetric: 101.1668 - val_loss: 96.4049 - val_MinusLogProbMetric: 96.4049 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 14/1000
2023-10-27 13:42:05.609 
Epoch 14/1000 
	 loss: 94.1505, MinusLogProbMetric: 94.1505, val_loss: 91.6875, val_MinusLogProbMetric: 91.6875

Epoch 14: val_loss improved from 96.40489 to 91.68752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 94.1505 - MinusLogProbMetric: 94.1505 - val_loss: 91.6875 - val_MinusLogProbMetric: 91.6875 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 15/1000
2023-10-27 13:42:35.525 
Epoch 15/1000 
	 loss: 89.5478, MinusLogProbMetric: 89.5478, val_loss: 92.0773, val_MinusLogProbMetric: 92.0773

Epoch 15: val_loss did not improve from 91.68752
196/196 - 29s - loss: 89.5478 - MinusLogProbMetric: 89.5478 - val_loss: 92.0773 - val_MinusLogProbMetric: 92.0773 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 16/1000
2023-10-27 13:43:05.131 
Epoch 16/1000 
	 loss: 85.9717, MinusLogProbMetric: 85.9717, val_loss: 85.3181, val_MinusLogProbMetric: 85.3181

Epoch 16: val_loss improved from 91.68752 to 85.31810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 85.9717 - MinusLogProbMetric: 85.9717 - val_loss: 85.3181 - val_MinusLogProbMetric: 85.3181 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 17/1000
2023-10-27 13:43:36.606 
Epoch 17/1000 
	 loss: 82.8341, MinusLogProbMetric: 82.8341, val_loss: 81.9836, val_MinusLogProbMetric: 81.9836

Epoch 17: val_loss improved from 85.31810 to 81.98361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 82.8341 - MinusLogProbMetric: 82.8341 - val_loss: 81.9836 - val_MinusLogProbMetric: 81.9836 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 18/1000
2023-10-27 13:44:10.151 
Epoch 18/1000 
	 loss: 80.2315, MinusLogProbMetric: 80.2315, val_loss: 80.0928, val_MinusLogProbMetric: 80.0928

Epoch 18: val_loss improved from 81.98361 to 80.09275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 80.2315 - MinusLogProbMetric: 80.2315 - val_loss: 80.0928 - val_MinusLogProbMetric: 80.0928 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 19/1000
2023-10-27 13:44:42.112 
Epoch 19/1000 
	 loss: 80.9281, MinusLogProbMetric: 80.9281, val_loss: 77.2369, val_MinusLogProbMetric: 77.2369

Epoch 19: val_loss improved from 80.09275 to 77.23690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 80.9281 - MinusLogProbMetric: 80.9281 - val_loss: 77.2369 - val_MinusLogProbMetric: 77.2369 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 20/1000
2023-10-27 13:45:12.659 
Epoch 20/1000 
	 loss: 76.0374, MinusLogProbMetric: 76.0374, val_loss: 75.0444, val_MinusLogProbMetric: 75.0444

Epoch 20: val_loss improved from 77.23690 to 75.04445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 76.0374 - MinusLogProbMetric: 76.0374 - val_loss: 75.0444 - val_MinusLogProbMetric: 75.0444 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 21/1000
2023-10-27 13:45:44.955 
Epoch 21/1000 
	 loss: 74.0645, MinusLogProbMetric: 74.0645, val_loss: 74.9847, val_MinusLogProbMetric: 74.9847

Epoch 21: val_loss improved from 75.04445 to 74.98475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 74.0645 - MinusLogProbMetric: 74.0645 - val_loss: 74.9847 - val_MinusLogProbMetric: 74.9847 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 22/1000
2023-10-27 13:46:14.821 
Epoch 22/1000 
	 loss: 72.6748, MinusLogProbMetric: 72.6748, val_loss: 71.9577, val_MinusLogProbMetric: 71.9577

Epoch 22: val_loss improved from 74.98475 to 71.95773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 72.6748 - MinusLogProbMetric: 72.6748 - val_loss: 71.9577 - val_MinusLogProbMetric: 71.9577 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 23/1000
2023-10-27 13:46:46.880 
Epoch 23/1000 
	 loss: 71.0389, MinusLogProbMetric: 71.0389, val_loss: 71.6278, val_MinusLogProbMetric: 71.6278

Epoch 23: val_loss improved from 71.95773 to 71.62783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 71.0389 - MinusLogProbMetric: 71.0389 - val_loss: 71.6278 - val_MinusLogProbMetric: 71.6278 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 24/1000
2023-10-27 13:47:19.570 
Epoch 24/1000 
	 loss: 69.7906, MinusLogProbMetric: 69.7906, val_loss: 70.0291, val_MinusLogProbMetric: 70.0291

Epoch 24: val_loss improved from 71.62783 to 70.02911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 69.7906 - MinusLogProbMetric: 69.7906 - val_loss: 70.0291 - val_MinusLogProbMetric: 70.0291 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 25/1000
2023-10-27 13:47:49.439 
Epoch 25/1000 
	 loss: 68.4976, MinusLogProbMetric: 68.4976, val_loss: 68.5712, val_MinusLogProbMetric: 68.5712

Epoch 25: val_loss improved from 70.02911 to 68.57123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 68.4976 - MinusLogProbMetric: 68.4976 - val_loss: 68.5712 - val_MinusLogProbMetric: 68.5712 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 26/1000
2023-10-27 13:48:23.512 
Epoch 26/1000 
	 loss: 67.5782, MinusLogProbMetric: 67.5782, val_loss: 68.4742, val_MinusLogProbMetric: 68.4742

Epoch 26: val_loss improved from 68.57123 to 68.47415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 67.5782 - MinusLogProbMetric: 67.5782 - val_loss: 68.4742 - val_MinusLogProbMetric: 68.4742 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 27/1000
2023-10-27 13:48:54.506 
Epoch 27/1000 
	 loss: 66.6282, MinusLogProbMetric: 66.6282, val_loss: 66.7890, val_MinusLogProbMetric: 66.7890

Epoch 27: val_loss improved from 68.47415 to 66.78902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 66.6282 - MinusLogProbMetric: 66.6282 - val_loss: 66.7890 - val_MinusLogProbMetric: 66.7890 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 28/1000
2023-10-27 13:49:25.020 
Epoch 28/1000 
	 loss: 65.8304, MinusLogProbMetric: 65.8304, val_loss: 65.8905, val_MinusLogProbMetric: 65.8905

Epoch 28: val_loss improved from 66.78902 to 65.89046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 65.8304 - MinusLogProbMetric: 65.8304 - val_loss: 65.8905 - val_MinusLogProbMetric: 65.8905 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 29/1000
2023-10-27 13:49:58.614 
Epoch 29/1000 
	 loss: 65.0919, MinusLogProbMetric: 65.0919, val_loss: 64.4860, val_MinusLogProbMetric: 64.4860

Epoch 29: val_loss improved from 65.89046 to 64.48604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 65.0919 - MinusLogProbMetric: 65.0919 - val_loss: 64.4860 - val_MinusLogProbMetric: 64.4860 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 30/1000
2023-10-27 13:50:28.367 
Epoch 30/1000 
	 loss: 64.3403, MinusLogProbMetric: 64.3403, val_loss: 66.7663, val_MinusLogProbMetric: 66.7663

Epoch 30: val_loss did not improve from 64.48604
196/196 - 29s - loss: 64.3403 - MinusLogProbMetric: 64.3403 - val_loss: 66.7663 - val_MinusLogProbMetric: 66.7663 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 31/1000
2023-10-27 13:50:59.459 
Epoch 31/1000 
	 loss: 63.2650, MinusLogProbMetric: 63.2650, val_loss: 63.4000, val_MinusLogProbMetric: 63.4000

Epoch 31: val_loss improved from 64.48604 to 63.40005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 63.2650 - MinusLogProbMetric: 63.2650 - val_loss: 63.4000 - val_MinusLogProbMetric: 63.4000 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 32/1000
2023-10-27 13:51:32.700 
Epoch 32/1000 
	 loss: 62.7845, MinusLogProbMetric: 62.7845, val_loss: 62.3571, val_MinusLogProbMetric: 62.3571

Epoch 32: val_loss improved from 63.40005 to 62.35713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 62.7845 - MinusLogProbMetric: 62.7845 - val_loss: 62.3571 - val_MinusLogProbMetric: 62.3571 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 33/1000
2023-10-27 13:52:02.503 
Epoch 33/1000 
	 loss: 62.2847, MinusLogProbMetric: 62.2847, val_loss: 63.3220, val_MinusLogProbMetric: 63.3220

Epoch 33: val_loss did not improve from 62.35713
196/196 - 29s - loss: 62.2847 - MinusLogProbMetric: 62.2847 - val_loss: 63.3220 - val_MinusLogProbMetric: 63.3220 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 34/1000
2023-10-27 13:52:34.410 
Epoch 34/1000 
	 loss: 61.6752, MinusLogProbMetric: 61.6752, val_loss: 63.3441, val_MinusLogProbMetric: 63.3441

Epoch 34: val_loss did not improve from 62.35713
196/196 - 32s - loss: 61.6752 - MinusLogProbMetric: 61.6752 - val_loss: 63.3441 - val_MinusLogProbMetric: 63.3441 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 35/1000
2023-10-27 13:53:05.414 
Epoch 35/1000 
	 loss: 60.9348, MinusLogProbMetric: 60.9348, val_loss: 62.3599, val_MinusLogProbMetric: 62.3599

Epoch 35: val_loss did not improve from 62.35713
196/196 - 31s - loss: 60.9348 - MinusLogProbMetric: 60.9348 - val_loss: 62.3599 - val_MinusLogProbMetric: 62.3599 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 36/1000
2023-10-27 13:53:34.559 
Epoch 36/1000 
	 loss: 61.4265, MinusLogProbMetric: 61.4265, val_loss: 60.5857, val_MinusLogProbMetric: 60.5857

Epoch 36: val_loss improved from 62.35713 to 60.58573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 61.4265 - MinusLogProbMetric: 61.4265 - val_loss: 60.5857 - val_MinusLogProbMetric: 60.5857 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 37/1000
2023-10-27 13:54:08.571 
Epoch 37/1000 
	 loss: 60.2146, MinusLogProbMetric: 60.2146, val_loss: 61.4164, val_MinusLogProbMetric: 61.4164

Epoch 37: val_loss did not improve from 60.58573
196/196 - 34s - loss: 60.2146 - MinusLogProbMetric: 60.2146 - val_loss: 61.4164 - val_MinusLogProbMetric: 61.4164 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 38/1000
2023-10-27 13:54:38.128 
Epoch 38/1000 
	 loss: 59.9149, MinusLogProbMetric: 59.9149, val_loss: 60.5285, val_MinusLogProbMetric: 60.5285

Epoch 38: val_loss improved from 60.58573 to 60.52851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 59.9149 - MinusLogProbMetric: 59.9149 - val_loss: 60.5285 - val_MinusLogProbMetric: 60.5285 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 39/1000
2023-10-27 13:55:09.447 
Epoch 39/1000 
	 loss: 59.5399, MinusLogProbMetric: 59.5399, val_loss: 60.9524, val_MinusLogProbMetric: 60.9524

Epoch 39: val_loss did not improve from 60.52851
196/196 - 31s - loss: 59.5399 - MinusLogProbMetric: 59.5399 - val_loss: 60.9524 - val_MinusLogProbMetric: 60.9524 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 40/1000
2023-10-27 13:55:40.948 
Epoch 40/1000 
	 loss: 58.9250, MinusLogProbMetric: 58.9250, val_loss: 59.7156, val_MinusLogProbMetric: 59.7156

Epoch 40: val_loss improved from 60.52851 to 59.71557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 58.9250 - MinusLogProbMetric: 58.9250 - val_loss: 59.7156 - val_MinusLogProbMetric: 59.7156 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 41/1000
2023-10-27 13:56:10.636 
Epoch 41/1000 
	 loss: 58.5091, MinusLogProbMetric: 58.5091, val_loss: 59.2827, val_MinusLogProbMetric: 59.2827

Epoch 41: val_loss improved from 59.71557 to 59.28266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 58.5091 - MinusLogProbMetric: 58.5091 - val_loss: 59.2827 - val_MinusLogProbMetric: 59.2827 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 42/1000
2023-10-27 13:56:40.928 
Epoch 42/1000 
	 loss: 58.4378, MinusLogProbMetric: 58.4378, val_loss: 60.5818, val_MinusLogProbMetric: 60.5818

Epoch 42: val_loss did not improve from 59.28266
196/196 - 30s - loss: 58.4378 - MinusLogProbMetric: 58.4378 - val_loss: 60.5818 - val_MinusLogProbMetric: 60.5818 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 43/1000
2023-10-27 13:57:11.450 
Epoch 43/1000 
	 loss: 57.7810, MinusLogProbMetric: 57.7810, val_loss: 58.4882, val_MinusLogProbMetric: 58.4882

Epoch 43: val_loss improved from 59.28266 to 58.48820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 57.7810 - MinusLogProbMetric: 57.7810 - val_loss: 58.4882 - val_MinusLogProbMetric: 58.4882 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 44/1000
2023-10-27 13:57:41.204 
Epoch 44/1000 
	 loss: 57.4476, MinusLogProbMetric: 57.4476, val_loss: 59.0525, val_MinusLogProbMetric: 59.0525

Epoch 44: val_loss did not improve from 58.48820
196/196 - 29s - loss: 57.4476 - MinusLogProbMetric: 57.4476 - val_loss: 59.0525 - val_MinusLogProbMetric: 59.0525 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 45/1000
2023-10-27 13:58:12.998 
Epoch 45/1000 
	 loss: 57.4185, MinusLogProbMetric: 57.4185, val_loss: 60.3955, val_MinusLogProbMetric: 60.3955

Epoch 45: val_loss did not improve from 58.48820
196/196 - 32s - loss: 57.4185 - MinusLogProbMetric: 57.4185 - val_loss: 60.3955 - val_MinusLogProbMetric: 60.3955 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 46/1000
2023-10-27 13:58:42.601 
Epoch 46/1000 
	 loss: 56.9015, MinusLogProbMetric: 56.9015, val_loss: 57.5103, val_MinusLogProbMetric: 57.5103

Epoch 46: val_loss improved from 58.48820 to 57.51033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 56.9015 - MinusLogProbMetric: 56.9015 - val_loss: 57.5103 - val_MinusLogProbMetric: 57.5103 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 47/1000
2023-10-27 13:59:13.429 
Epoch 47/1000 
	 loss: 56.9152, MinusLogProbMetric: 56.9152, val_loss: 56.6852, val_MinusLogProbMetric: 56.6852

Epoch 47: val_loss improved from 57.51033 to 56.68521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 56.9152 - MinusLogProbMetric: 56.9152 - val_loss: 56.6852 - val_MinusLogProbMetric: 56.6852 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 48/1000
2023-10-27 13:59:45.872 
Epoch 48/1000 
	 loss: 56.5087, MinusLogProbMetric: 56.5087, val_loss: 57.1725, val_MinusLogProbMetric: 57.1725

Epoch 48: val_loss did not improve from 56.68521
196/196 - 32s - loss: 56.5087 - MinusLogProbMetric: 56.5087 - val_loss: 57.1725 - val_MinusLogProbMetric: 57.1725 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 49/1000
2023-10-27 14:00:15.266 
Epoch 49/1000 
	 loss: 56.1329, MinusLogProbMetric: 56.1329, val_loss: 55.5554, val_MinusLogProbMetric: 55.5554

Epoch 49: val_loss improved from 56.68521 to 55.55544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 56.1329 - MinusLogProbMetric: 56.1329 - val_loss: 55.5554 - val_MinusLogProbMetric: 55.5554 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 50/1000
2023-10-27 14:00:46.498 
Epoch 50/1000 
	 loss: 56.0259, MinusLogProbMetric: 56.0259, val_loss: 56.8538, val_MinusLogProbMetric: 56.8538

Epoch 50: val_loss did not improve from 55.55544
196/196 - 31s - loss: 56.0259 - MinusLogProbMetric: 56.0259 - val_loss: 56.8538 - val_MinusLogProbMetric: 56.8538 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 51/1000
2023-10-27 14:01:19.638 
Epoch 51/1000 
	 loss: 55.6690, MinusLogProbMetric: 55.6690, val_loss: 55.6827, val_MinusLogProbMetric: 55.6827

Epoch 51: val_loss did not improve from 55.55544
196/196 - 33s - loss: 55.6690 - MinusLogProbMetric: 55.6690 - val_loss: 55.6827 - val_MinusLogProbMetric: 55.6827 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 52/1000
2023-10-27 14:01:49.203 
Epoch 52/1000 
	 loss: 55.4448, MinusLogProbMetric: 55.4448, val_loss: 57.3189, val_MinusLogProbMetric: 57.3189

Epoch 52: val_loss did not improve from 55.55544
196/196 - 30s - loss: 55.4448 - MinusLogProbMetric: 55.4448 - val_loss: 57.3189 - val_MinusLogProbMetric: 57.3189 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 53/1000
2023-10-27 14:02:19.923 
Epoch 53/1000 
	 loss: 55.1847, MinusLogProbMetric: 55.1847, val_loss: 54.2547, val_MinusLogProbMetric: 54.2547

Epoch 53: val_loss improved from 55.55544 to 54.25471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 55.1847 - MinusLogProbMetric: 55.1847 - val_loss: 54.2547 - val_MinusLogProbMetric: 54.2547 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 54/1000
2023-10-27 14:02:54.155 
Epoch 54/1000 
	 loss: 54.9226, MinusLogProbMetric: 54.9226, val_loss: 54.4136, val_MinusLogProbMetric: 54.4136

Epoch 54: val_loss did not improve from 54.25471
196/196 - 34s - loss: 54.9226 - MinusLogProbMetric: 54.9226 - val_loss: 54.4136 - val_MinusLogProbMetric: 54.4136 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 55/1000
2023-10-27 14:03:27.917 
Epoch 55/1000 
	 loss: 54.6876, MinusLogProbMetric: 54.6876, val_loss: 56.7261, val_MinusLogProbMetric: 56.7261

Epoch 55: val_loss did not improve from 54.25471
196/196 - 34s - loss: 54.6876 - MinusLogProbMetric: 54.6876 - val_loss: 56.7261 - val_MinusLogProbMetric: 56.7261 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 56/1000
2023-10-27 14:04:01.159 
Epoch 56/1000 
	 loss: 55.0672, MinusLogProbMetric: 55.0672, val_loss: 56.4407, val_MinusLogProbMetric: 56.4407

Epoch 56: val_loss did not improve from 54.25471
196/196 - 33s - loss: 55.0672 - MinusLogProbMetric: 55.0672 - val_loss: 56.4407 - val_MinusLogProbMetric: 56.4407 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 57/1000
2023-10-27 14:04:34.497 
Epoch 57/1000 
	 loss: 54.3244, MinusLogProbMetric: 54.3244, val_loss: 54.8529, val_MinusLogProbMetric: 54.8529

Epoch 57: val_loss did not improve from 54.25471
196/196 - 33s - loss: 54.3244 - MinusLogProbMetric: 54.3244 - val_loss: 54.8529 - val_MinusLogProbMetric: 54.8529 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 58/1000
2023-10-27 14:05:08.289 
Epoch 58/1000 
	 loss: 54.3709, MinusLogProbMetric: 54.3709, val_loss: 56.2920, val_MinusLogProbMetric: 56.2920

Epoch 58: val_loss did not improve from 54.25471
196/196 - 34s - loss: 54.3709 - MinusLogProbMetric: 54.3709 - val_loss: 56.2920 - val_MinusLogProbMetric: 56.2920 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 59/1000
2023-10-27 14:05:41.966 
Epoch 59/1000 
	 loss: 53.8876, MinusLogProbMetric: 53.8876, val_loss: 53.6665, val_MinusLogProbMetric: 53.6665

Epoch 59: val_loss improved from 54.25471 to 53.66648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 53.8876 - MinusLogProbMetric: 53.8876 - val_loss: 53.6665 - val_MinusLogProbMetric: 53.6665 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 60/1000
2023-10-27 14:06:16.172 
Epoch 60/1000 
	 loss: 54.0039, MinusLogProbMetric: 54.0039, val_loss: 54.1272, val_MinusLogProbMetric: 54.1272

Epoch 60: val_loss did not improve from 53.66648
196/196 - 34s - loss: 54.0039 - MinusLogProbMetric: 54.0039 - val_loss: 54.1272 - val_MinusLogProbMetric: 54.1272 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 61/1000
2023-10-27 14:06:49.569 
Epoch 61/1000 
	 loss: 53.5483, MinusLogProbMetric: 53.5483, val_loss: 53.1490, val_MinusLogProbMetric: 53.1490

Epoch 61: val_loss improved from 53.66648 to 53.14904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 53.5483 - MinusLogProbMetric: 53.5483 - val_loss: 53.1490 - val_MinusLogProbMetric: 53.1490 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 62/1000
2023-10-27 14:07:24.022 
Epoch 62/1000 
	 loss: 53.7319, MinusLogProbMetric: 53.7319, val_loss: 56.8307, val_MinusLogProbMetric: 56.8307

Epoch 62: val_loss did not improve from 53.14904
196/196 - 34s - loss: 53.7319 - MinusLogProbMetric: 53.7319 - val_loss: 56.8307 - val_MinusLogProbMetric: 56.8307 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 63/1000
2023-10-27 14:07:57.816 
Epoch 63/1000 
	 loss: 53.6108, MinusLogProbMetric: 53.6108, val_loss: 55.1664, val_MinusLogProbMetric: 55.1664

Epoch 63: val_loss did not improve from 53.14904
196/196 - 34s - loss: 53.6108 - MinusLogProbMetric: 53.6108 - val_loss: 55.1664 - val_MinusLogProbMetric: 55.1664 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 64/1000
2023-10-27 14:08:31.522 
Epoch 64/1000 
	 loss: 53.8674, MinusLogProbMetric: 53.8674, val_loss: 52.7376, val_MinusLogProbMetric: 52.7376

Epoch 64: val_loss improved from 53.14904 to 52.73762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 53.8674 - MinusLogProbMetric: 53.8674 - val_loss: 52.7376 - val_MinusLogProbMetric: 52.7376 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 65/1000
2023-10-27 14:09:05.649 
Epoch 65/1000 
	 loss: 53.1619, MinusLogProbMetric: 53.1619, val_loss: 53.3966, val_MinusLogProbMetric: 53.3966

Epoch 65: val_loss did not improve from 52.73762
196/196 - 34s - loss: 53.1619 - MinusLogProbMetric: 53.1619 - val_loss: 53.3966 - val_MinusLogProbMetric: 53.3966 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 66/1000
2023-10-27 14:09:39.289 
Epoch 66/1000 
	 loss: 52.9584, MinusLogProbMetric: 52.9584, val_loss: 55.4493, val_MinusLogProbMetric: 55.4493

Epoch 66: val_loss did not improve from 52.73762
196/196 - 34s - loss: 52.9584 - MinusLogProbMetric: 52.9584 - val_loss: 55.4493 - val_MinusLogProbMetric: 55.4493 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 67/1000
2023-10-27 14:10:12.209 
Epoch 67/1000 
	 loss: 53.0369, MinusLogProbMetric: 53.0369, val_loss: 52.7019, val_MinusLogProbMetric: 52.7019

Epoch 67: val_loss improved from 52.73762 to 52.70194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 53.0369 - MinusLogProbMetric: 53.0369 - val_loss: 52.7019 - val_MinusLogProbMetric: 52.7019 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 68/1000
2023-10-27 14:10:46.904 
Epoch 68/1000 
	 loss: 52.5105, MinusLogProbMetric: 52.5105, val_loss: 52.7340, val_MinusLogProbMetric: 52.7340

Epoch 68: val_loss did not improve from 52.70194
196/196 - 34s - loss: 52.5105 - MinusLogProbMetric: 52.5105 - val_loss: 52.7340 - val_MinusLogProbMetric: 52.7340 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 69/1000
2023-10-27 14:11:20.516 
Epoch 69/1000 
	 loss: 52.5246, MinusLogProbMetric: 52.5246, val_loss: 52.6156, val_MinusLogProbMetric: 52.6156

Epoch 69: val_loss improved from 52.70194 to 52.61557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 52.5246 - MinusLogProbMetric: 52.5246 - val_loss: 52.6156 - val_MinusLogProbMetric: 52.6156 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 70/1000
2023-10-27 14:11:55.278 
Epoch 70/1000 
	 loss: 52.2556, MinusLogProbMetric: 52.2556, val_loss: 53.5263, val_MinusLogProbMetric: 53.5263

Epoch 70: val_loss did not improve from 52.61557
196/196 - 34s - loss: 52.2556 - MinusLogProbMetric: 52.2556 - val_loss: 53.5263 - val_MinusLogProbMetric: 53.5263 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 71/1000
2023-10-27 14:12:29.306 
Epoch 71/1000 
	 loss: 52.3019, MinusLogProbMetric: 52.3019, val_loss: 53.6311, val_MinusLogProbMetric: 53.6311

Epoch 71: val_loss did not improve from 52.61557
196/196 - 34s - loss: 52.3019 - MinusLogProbMetric: 52.3019 - val_loss: 53.6311 - val_MinusLogProbMetric: 53.6311 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 72/1000
2023-10-27 14:13:02.925 
Epoch 72/1000 
	 loss: 52.3593, MinusLogProbMetric: 52.3593, val_loss: 52.0178, val_MinusLogProbMetric: 52.0178

Epoch 72: val_loss improved from 52.61557 to 52.01779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 52.3593 - MinusLogProbMetric: 52.3593 - val_loss: 52.0178 - val_MinusLogProbMetric: 52.0178 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 73/1000
2023-10-27 14:13:36.944 
Epoch 73/1000 
	 loss: 52.7140, MinusLogProbMetric: 52.7140, val_loss: 53.8028, val_MinusLogProbMetric: 53.8028

Epoch 73: val_loss did not improve from 52.01779
196/196 - 33s - loss: 52.7140 - MinusLogProbMetric: 52.7140 - val_loss: 53.8028 - val_MinusLogProbMetric: 53.8028 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 74/1000
2023-10-27 14:14:10.669 
Epoch 74/1000 
	 loss: 51.9204, MinusLogProbMetric: 51.9204, val_loss: 52.9664, val_MinusLogProbMetric: 52.9664

Epoch 74: val_loss did not improve from 52.01779
196/196 - 34s - loss: 51.9204 - MinusLogProbMetric: 51.9204 - val_loss: 52.9664 - val_MinusLogProbMetric: 52.9664 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 75/1000
2023-10-27 14:14:44.433 
Epoch 75/1000 
	 loss: 51.6161, MinusLogProbMetric: 51.6161, val_loss: 52.8207, val_MinusLogProbMetric: 52.8207

Epoch 75: val_loss did not improve from 52.01779
196/196 - 34s - loss: 51.6161 - MinusLogProbMetric: 51.6161 - val_loss: 52.8207 - val_MinusLogProbMetric: 52.8207 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 76/1000
2023-10-27 14:15:18.014 
Epoch 76/1000 
	 loss: 51.8668, MinusLogProbMetric: 51.8668, val_loss: 54.5886, val_MinusLogProbMetric: 54.5886

Epoch 76: val_loss did not improve from 52.01779
196/196 - 34s - loss: 51.8668 - MinusLogProbMetric: 51.8668 - val_loss: 54.5886 - val_MinusLogProbMetric: 54.5886 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 77/1000
2023-10-27 14:15:51.310 
Epoch 77/1000 
	 loss: 51.4608, MinusLogProbMetric: 51.4608, val_loss: 51.6896, val_MinusLogProbMetric: 51.6896

Epoch 77: val_loss improved from 52.01779 to 51.68962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 51.4608 - MinusLogProbMetric: 51.4608 - val_loss: 51.6896 - val_MinusLogProbMetric: 51.6896 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 78/1000
2023-10-27 14:16:25.379 
Epoch 78/1000 
	 loss: 51.5630, MinusLogProbMetric: 51.5630, val_loss: 52.5793, val_MinusLogProbMetric: 52.5793

Epoch 78: val_loss did not improve from 51.68962
196/196 - 33s - loss: 51.5630 - MinusLogProbMetric: 51.5630 - val_loss: 52.5793 - val_MinusLogProbMetric: 52.5793 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 79/1000
2023-10-27 14:16:58.608 
Epoch 79/1000 
	 loss: 52.0102, MinusLogProbMetric: 52.0102, val_loss: 51.6912, val_MinusLogProbMetric: 51.6912

Epoch 79: val_loss did not improve from 51.68962
196/196 - 33s - loss: 52.0102 - MinusLogProbMetric: 52.0102 - val_loss: 51.6912 - val_MinusLogProbMetric: 51.6912 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 80/1000
2023-10-27 14:17:31.797 
Epoch 80/1000 
	 loss: 51.2047, MinusLogProbMetric: 51.2047, val_loss: 52.8243, val_MinusLogProbMetric: 52.8243

Epoch 80: val_loss did not improve from 51.68962
196/196 - 33s - loss: 51.2047 - MinusLogProbMetric: 51.2047 - val_loss: 52.8243 - val_MinusLogProbMetric: 52.8243 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 81/1000
2023-10-27 14:18:05.630 
Epoch 81/1000 
	 loss: 51.3325, MinusLogProbMetric: 51.3325, val_loss: 52.1594, val_MinusLogProbMetric: 52.1594

Epoch 81: val_loss did not improve from 51.68962
196/196 - 34s - loss: 51.3325 - MinusLogProbMetric: 51.3325 - val_loss: 52.1594 - val_MinusLogProbMetric: 52.1594 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 82/1000
2023-10-27 14:18:39.653 
Epoch 82/1000 
	 loss: 51.0445, MinusLogProbMetric: 51.0445, val_loss: 51.8303, val_MinusLogProbMetric: 51.8303

Epoch 82: val_loss did not improve from 51.68962
196/196 - 34s - loss: 51.0445 - MinusLogProbMetric: 51.0445 - val_loss: 51.8303 - val_MinusLogProbMetric: 51.8303 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 83/1000
2023-10-27 14:19:13.500 
Epoch 83/1000 
	 loss: 50.8958, MinusLogProbMetric: 50.8958, val_loss: 51.2306, val_MinusLogProbMetric: 51.2306

Epoch 83: val_loss improved from 51.68962 to 51.23062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 50.8958 - MinusLogProbMetric: 50.8958 - val_loss: 51.2306 - val_MinusLogProbMetric: 51.2306 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 84/1000
2023-10-27 14:19:47.509 
Epoch 84/1000 
	 loss: 50.7926, MinusLogProbMetric: 50.7926, val_loss: 50.7160, val_MinusLogProbMetric: 50.7160

Epoch 84: val_loss improved from 51.23062 to 50.71596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 50.7926 - MinusLogProbMetric: 50.7926 - val_loss: 50.7160 - val_MinusLogProbMetric: 50.7160 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 85/1000
2023-10-27 14:20:21.692 
Epoch 85/1000 
	 loss: 50.7766, MinusLogProbMetric: 50.7766, val_loss: 50.6687, val_MinusLogProbMetric: 50.6687

Epoch 85: val_loss improved from 50.71596 to 50.66872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 50.7766 - MinusLogProbMetric: 50.7766 - val_loss: 50.6687 - val_MinusLogProbMetric: 50.6687 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 86/1000
2023-10-27 14:20:56.325 
Epoch 86/1000 
	 loss: 50.7633, MinusLogProbMetric: 50.7633, val_loss: 50.9866, val_MinusLogProbMetric: 50.9866

Epoch 86: val_loss did not improve from 50.66872
196/196 - 34s - loss: 50.7633 - MinusLogProbMetric: 50.7633 - val_loss: 50.9866 - val_MinusLogProbMetric: 50.9866 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 87/1000
2023-10-27 14:21:29.892 
Epoch 87/1000 
	 loss: 50.6245, MinusLogProbMetric: 50.6245, val_loss: 51.2424, val_MinusLogProbMetric: 51.2424

Epoch 87: val_loss did not improve from 50.66872
196/196 - 34s - loss: 50.6245 - MinusLogProbMetric: 50.6245 - val_loss: 51.2424 - val_MinusLogProbMetric: 51.2424 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 88/1000
2023-10-27 14:22:03.865 
Epoch 88/1000 
	 loss: 51.1873, MinusLogProbMetric: 51.1873, val_loss: 50.1175, val_MinusLogProbMetric: 50.1175

Epoch 88: val_loss improved from 50.66872 to 50.11749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 35s - loss: 51.1873 - MinusLogProbMetric: 51.1873 - val_loss: 50.1175 - val_MinusLogProbMetric: 50.1175 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 89/1000
2023-10-27 14:22:38.041 
Epoch 89/1000 
	 loss: 50.3657, MinusLogProbMetric: 50.3657, val_loss: 50.2227, val_MinusLogProbMetric: 50.2227

Epoch 89: val_loss did not improve from 50.11749
196/196 - 34s - loss: 50.3657 - MinusLogProbMetric: 50.3657 - val_loss: 50.2227 - val_MinusLogProbMetric: 50.2227 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 90/1000
2023-10-27 14:23:11.623 
Epoch 90/1000 
	 loss: 50.7300, MinusLogProbMetric: 50.7300, val_loss: 52.2159, val_MinusLogProbMetric: 52.2159

Epoch 90: val_loss did not improve from 50.11749
196/196 - 34s - loss: 50.7300 - MinusLogProbMetric: 50.7300 - val_loss: 52.2159 - val_MinusLogProbMetric: 52.2159 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 91/1000
2023-10-27 14:23:45.291 
Epoch 91/1000 
	 loss: 50.0432, MinusLogProbMetric: 50.0432, val_loss: 50.1273, val_MinusLogProbMetric: 50.1273

Epoch 91: val_loss did not improve from 50.11749
196/196 - 34s - loss: 50.0432 - MinusLogProbMetric: 50.0432 - val_loss: 50.1273 - val_MinusLogProbMetric: 50.1273 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 92/1000
2023-10-27 14:24:18.340 
Epoch 92/1000 
	 loss: 50.0231, MinusLogProbMetric: 50.0231, val_loss: 51.9382, val_MinusLogProbMetric: 51.9382

Epoch 92: val_loss did not improve from 50.11749
196/196 - 33s - loss: 50.0231 - MinusLogProbMetric: 50.0231 - val_loss: 51.9382 - val_MinusLogProbMetric: 51.9382 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 93/1000
2023-10-27 14:24:51.674 
Epoch 93/1000 
	 loss: 49.9536, MinusLogProbMetric: 49.9536, val_loss: 50.3629, val_MinusLogProbMetric: 50.3629

Epoch 93: val_loss did not improve from 50.11749
196/196 - 33s - loss: 49.9536 - MinusLogProbMetric: 49.9536 - val_loss: 50.3629 - val_MinusLogProbMetric: 50.3629 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 94/1000
2023-10-27 14:25:24.898 
Epoch 94/1000 
	 loss: 50.4644, MinusLogProbMetric: 50.4644, val_loss: 50.9262, val_MinusLogProbMetric: 50.9262

Epoch 94: val_loss did not improve from 50.11749
196/196 - 33s - loss: 50.4644 - MinusLogProbMetric: 50.4644 - val_loss: 50.9262 - val_MinusLogProbMetric: 50.9262 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 95/1000
2023-10-27 14:25:58.648 
Epoch 95/1000 
	 loss: 49.9041, MinusLogProbMetric: 49.9041, val_loss: 49.7722, val_MinusLogProbMetric: 49.7722

Epoch 95: val_loss improved from 50.11749 to 49.77219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 49.9041 - MinusLogProbMetric: 49.9041 - val_loss: 49.7722 - val_MinusLogProbMetric: 49.7722 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 96/1000
2023-10-27 14:26:33.050 
Epoch 96/1000 
	 loss: 49.8438, MinusLogProbMetric: 49.8438, val_loss: 51.1274, val_MinusLogProbMetric: 51.1274

Epoch 96: val_loss did not improve from 49.77219
196/196 - 34s - loss: 49.8438 - MinusLogProbMetric: 49.8438 - val_loss: 51.1274 - val_MinusLogProbMetric: 51.1274 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 97/1000
2023-10-27 14:27:06.362 
Epoch 97/1000 
	 loss: 49.8164, MinusLogProbMetric: 49.8164, val_loss: 52.1029, val_MinusLogProbMetric: 52.1029

Epoch 97: val_loss did not improve from 49.77219
196/196 - 33s - loss: 49.8164 - MinusLogProbMetric: 49.8164 - val_loss: 52.1029 - val_MinusLogProbMetric: 52.1029 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 98/1000
2023-10-27 14:27:40.019 
Epoch 98/1000 
	 loss: 49.8513, MinusLogProbMetric: 49.8513, val_loss: 51.3091, val_MinusLogProbMetric: 51.3091

Epoch 98: val_loss did not improve from 49.77219
196/196 - 34s - loss: 49.8513 - MinusLogProbMetric: 49.8513 - val_loss: 51.3091 - val_MinusLogProbMetric: 51.3091 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 99/1000
2023-10-27 14:28:14.287 
Epoch 99/1000 
	 loss: 50.0962, MinusLogProbMetric: 50.0962, val_loss: 49.2231, val_MinusLogProbMetric: 49.2231

Epoch 99: val_loss improved from 49.77219 to 49.22314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 35s - loss: 50.0962 - MinusLogProbMetric: 50.0962 - val_loss: 49.2231 - val_MinusLogProbMetric: 49.2231 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 100/1000
2023-10-27 14:28:48.757 
Epoch 100/1000 
	 loss: 49.4360, MinusLogProbMetric: 49.4360, val_loss: 50.0203, val_MinusLogProbMetric: 50.0203

Epoch 100: val_loss did not improve from 49.22314
196/196 - 34s - loss: 49.4360 - MinusLogProbMetric: 49.4360 - val_loss: 50.0203 - val_MinusLogProbMetric: 50.0203 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 101/1000
2023-10-27 14:29:22.121 
Epoch 101/1000 
	 loss: 49.9162, MinusLogProbMetric: 49.9162, val_loss: 50.7968, val_MinusLogProbMetric: 50.7968

Epoch 101: val_loss did not improve from 49.22314
196/196 - 33s - loss: 49.9162 - MinusLogProbMetric: 49.9162 - val_loss: 50.7968 - val_MinusLogProbMetric: 50.7968 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 102/1000
2023-10-27 14:29:56.059 
Epoch 102/1000 
	 loss: 49.5670, MinusLogProbMetric: 49.5670, val_loss: 49.2625, val_MinusLogProbMetric: 49.2625

Epoch 102: val_loss did not improve from 49.22314
196/196 - 34s - loss: 49.5670 - MinusLogProbMetric: 49.5670 - val_loss: 49.2625 - val_MinusLogProbMetric: 49.2625 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 103/1000
2023-10-27 14:30:29.906 
Epoch 103/1000 
	 loss: 49.6137, MinusLogProbMetric: 49.6137, val_loss: 49.6524, val_MinusLogProbMetric: 49.6524

Epoch 103: val_loss did not improve from 49.22314
196/196 - 34s - loss: 49.6137 - MinusLogProbMetric: 49.6137 - val_loss: 49.6524 - val_MinusLogProbMetric: 49.6524 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 104/1000
2023-10-27 14:31:02.774 
Epoch 104/1000 
	 loss: 49.7907, MinusLogProbMetric: 49.7907, val_loss: 50.4319, val_MinusLogProbMetric: 50.4319

Epoch 104: val_loss did not improve from 49.22314
196/196 - 33s - loss: 49.7907 - MinusLogProbMetric: 49.7907 - val_loss: 50.4319 - val_MinusLogProbMetric: 50.4319 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 105/1000
2023-10-27 14:31:36.211 
Epoch 105/1000 
	 loss: 49.2515, MinusLogProbMetric: 49.2515, val_loss: 52.0556, val_MinusLogProbMetric: 52.0556

Epoch 105: val_loss did not improve from 49.22314
196/196 - 33s - loss: 49.2515 - MinusLogProbMetric: 49.2515 - val_loss: 52.0556 - val_MinusLogProbMetric: 52.0556 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 106/1000
2023-10-27 14:32:09.868 
Epoch 106/1000 
	 loss: 49.2449, MinusLogProbMetric: 49.2449, val_loss: 50.5351, val_MinusLogProbMetric: 50.5351

Epoch 106: val_loss did not improve from 49.22314
196/196 - 34s - loss: 49.2449 - MinusLogProbMetric: 49.2449 - val_loss: 50.5351 - val_MinusLogProbMetric: 50.5351 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 107/1000
2023-10-27 14:32:43.374 
Epoch 107/1000 
	 loss: 49.3921, MinusLogProbMetric: 49.3921, val_loss: 49.8276, val_MinusLogProbMetric: 49.8276

Epoch 107: val_loss did not improve from 49.22314
196/196 - 34s - loss: 49.3921 - MinusLogProbMetric: 49.3921 - val_loss: 49.8276 - val_MinusLogProbMetric: 49.8276 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 108/1000
2023-10-27 14:33:16.338 
Epoch 108/1000 
	 loss: 49.1046, MinusLogProbMetric: 49.1046, val_loss: 48.6280, val_MinusLogProbMetric: 48.6280

Epoch 108: val_loss improved from 49.22314 to 48.62804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 49.1046 - MinusLogProbMetric: 49.1046 - val_loss: 48.6280 - val_MinusLogProbMetric: 48.6280 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 109/1000
2023-10-27 14:33:50.677 
Epoch 109/1000 
	 loss: 49.0402, MinusLogProbMetric: 49.0402, val_loss: 49.8559, val_MinusLogProbMetric: 49.8559

Epoch 109: val_loss did not improve from 48.62804
196/196 - 34s - loss: 49.0402 - MinusLogProbMetric: 49.0402 - val_loss: 49.8559 - val_MinusLogProbMetric: 49.8559 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 110/1000
2023-10-27 14:34:23.197 
Epoch 110/1000 
	 loss: 49.0998, MinusLogProbMetric: 49.0998, val_loss: 49.5901, val_MinusLogProbMetric: 49.5901

Epoch 110: val_loss did not improve from 48.62804
196/196 - 33s - loss: 49.0998 - MinusLogProbMetric: 49.0998 - val_loss: 49.5901 - val_MinusLogProbMetric: 49.5901 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 111/1000
2023-10-27 14:34:56.871 
Epoch 111/1000 
	 loss: 49.5069, MinusLogProbMetric: 49.5069, val_loss: 49.7233, val_MinusLogProbMetric: 49.7233

Epoch 111: val_loss did not improve from 48.62804
196/196 - 34s - loss: 49.5069 - MinusLogProbMetric: 49.5069 - val_loss: 49.7233 - val_MinusLogProbMetric: 49.7233 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 112/1000
2023-10-27 14:35:30.281 
Epoch 112/1000 
	 loss: 49.9001, MinusLogProbMetric: 49.9001, val_loss: 51.9711, val_MinusLogProbMetric: 51.9711

Epoch 112: val_loss did not improve from 48.62804
196/196 - 33s - loss: 49.9001 - MinusLogProbMetric: 49.9001 - val_loss: 51.9711 - val_MinusLogProbMetric: 51.9711 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 113/1000
2023-10-27 14:36:03.761 
Epoch 113/1000 
	 loss: 49.1283, MinusLogProbMetric: 49.1283, val_loss: 50.6745, val_MinusLogProbMetric: 50.6745

Epoch 113: val_loss did not improve from 48.62804
196/196 - 33s - loss: 49.1283 - MinusLogProbMetric: 49.1283 - val_loss: 50.6745 - val_MinusLogProbMetric: 50.6745 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 114/1000
2023-10-27 14:36:37.577 
Epoch 114/1000 
	 loss: 49.5800, MinusLogProbMetric: 49.5800, val_loss: 51.2703, val_MinusLogProbMetric: 51.2703

Epoch 114: val_loss did not improve from 48.62804
196/196 - 34s - loss: 49.5800 - MinusLogProbMetric: 49.5800 - val_loss: 51.2703 - val_MinusLogProbMetric: 51.2703 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 115/1000
2023-10-27 14:37:11.255 
Epoch 115/1000 
	 loss: 48.9744, MinusLogProbMetric: 48.9744, val_loss: 50.1780, val_MinusLogProbMetric: 50.1780

Epoch 115: val_loss did not improve from 48.62804
196/196 - 34s - loss: 48.9744 - MinusLogProbMetric: 48.9744 - val_loss: 50.1780 - val_MinusLogProbMetric: 50.1780 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 116/1000
2023-10-27 14:37:42.523 
Epoch 116/1000 
	 loss: 49.0811, MinusLogProbMetric: 49.0811, val_loss: 52.2761, val_MinusLogProbMetric: 52.2761

Epoch 116: val_loss did not improve from 48.62804
196/196 - 31s - loss: 49.0811 - MinusLogProbMetric: 49.0811 - val_loss: 52.2761 - val_MinusLogProbMetric: 52.2761 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 117/1000
2023-10-27 14:38:12.109 
Epoch 117/1000 
	 loss: 49.0273, MinusLogProbMetric: 49.0273, val_loss: 48.9428, val_MinusLogProbMetric: 48.9428

Epoch 117: val_loss did not improve from 48.62804
196/196 - 30s - loss: 49.0273 - MinusLogProbMetric: 49.0273 - val_loss: 48.9428 - val_MinusLogProbMetric: 48.9428 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 118/1000
2023-10-27 14:38:41.989 
Epoch 118/1000 
	 loss: 48.4068, MinusLogProbMetric: 48.4068, val_loss: 48.8200, val_MinusLogProbMetric: 48.8200

Epoch 118: val_loss did not improve from 48.62804
196/196 - 30s - loss: 48.4068 - MinusLogProbMetric: 48.4068 - val_loss: 48.8200 - val_MinusLogProbMetric: 48.8200 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 119/1000
2023-10-27 14:39:14.553 
Epoch 119/1000 
	 loss: 48.9511, MinusLogProbMetric: 48.9511, val_loss: 49.1257, val_MinusLogProbMetric: 49.1257

Epoch 119: val_loss did not improve from 48.62804
196/196 - 33s - loss: 48.9511 - MinusLogProbMetric: 48.9511 - val_loss: 49.1257 - val_MinusLogProbMetric: 49.1257 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 120/1000
2023-10-27 14:39:47.882 
Epoch 120/1000 
	 loss: 48.4071, MinusLogProbMetric: 48.4071, val_loss: 49.8174, val_MinusLogProbMetric: 49.8174

Epoch 120: val_loss did not improve from 48.62804
196/196 - 33s - loss: 48.4071 - MinusLogProbMetric: 48.4071 - val_loss: 49.8174 - val_MinusLogProbMetric: 49.8174 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 121/1000
2023-10-27 14:40:21.294 
Epoch 121/1000 
	 loss: 48.6646, MinusLogProbMetric: 48.6646, val_loss: 48.3344, val_MinusLogProbMetric: 48.3344

Epoch 121: val_loss improved from 48.62804 to 48.33444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 48.6646 - MinusLogProbMetric: 48.6646 - val_loss: 48.3344 - val_MinusLogProbMetric: 48.3344 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 122/1000
2023-10-27 14:40:55.733 
Epoch 122/1000 
	 loss: 48.4229, MinusLogProbMetric: 48.4229, val_loss: 48.7793, val_MinusLogProbMetric: 48.7793

Epoch 122: val_loss did not improve from 48.33444
196/196 - 34s - loss: 48.4229 - MinusLogProbMetric: 48.4229 - val_loss: 48.7793 - val_MinusLogProbMetric: 48.7793 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 123/1000
2023-10-27 14:41:29.009 
Epoch 123/1000 
	 loss: 48.4797, MinusLogProbMetric: 48.4797, val_loss: 48.2960, val_MinusLogProbMetric: 48.2960

Epoch 123: val_loss improved from 48.33444 to 48.29604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 48.4797 - MinusLogProbMetric: 48.4797 - val_loss: 48.2960 - val_MinusLogProbMetric: 48.2960 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 124/1000
2023-10-27 14:42:03.498 
Epoch 124/1000 
	 loss: 48.6417, MinusLogProbMetric: 48.6417, val_loss: 48.9019, val_MinusLogProbMetric: 48.9019

Epoch 124: val_loss did not improve from 48.29604
196/196 - 34s - loss: 48.6417 - MinusLogProbMetric: 48.6417 - val_loss: 48.9019 - val_MinusLogProbMetric: 48.9019 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 125/1000
2023-10-27 14:42:37.171 
Epoch 125/1000 
	 loss: 48.5674, MinusLogProbMetric: 48.5674, val_loss: 48.2637, val_MinusLogProbMetric: 48.2637

Epoch 125: val_loss improved from 48.29604 to 48.26372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 48.5674 - MinusLogProbMetric: 48.5674 - val_loss: 48.2637 - val_MinusLogProbMetric: 48.2637 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 126/1000
2023-10-27 14:43:11.549 
Epoch 126/1000 
	 loss: 48.6045, MinusLogProbMetric: 48.6045, val_loss: 50.7634, val_MinusLogProbMetric: 50.7634

Epoch 126: val_loss did not improve from 48.26372
196/196 - 34s - loss: 48.6045 - MinusLogProbMetric: 48.6045 - val_loss: 50.7634 - val_MinusLogProbMetric: 50.7634 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 127/1000
2023-10-27 14:43:44.872 
Epoch 127/1000 
	 loss: 49.4627, MinusLogProbMetric: 49.4627, val_loss: 49.3179, val_MinusLogProbMetric: 49.3179

Epoch 127: val_loss did not improve from 48.26372
196/196 - 33s - loss: 49.4627 - MinusLogProbMetric: 49.4627 - val_loss: 49.3179 - val_MinusLogProbMetric: 49.3179 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 128/1000
2023-10-27 14:44:18.363 
Epoch 128/1000 
	 loss: 48.2976, MinusLogProbMetric: 48.2976, val_loss: 47.7975, val_MinusLogProbMetric: 47.7975

Epoch 128: val_loss improved from 48.26372 to 47.79750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 48.2976 - MinusLogProbMetric: 48.2976 - val_loss: 47.7975 - val_MinusLogProbMetric: 47.7975 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 129/1000
2023-10-27 14:44:52.720 
Epoch 129/1000 
	 loss: 48.3605, MinusLogProbMetric: 48.3605, val_loss: 49.9629, val_MinusLogProbMetric: 49.9629

Epoch 129: val_loss did not improve from 47.79750
196/196 - 34s - loss: 48.3605 - MinusLogProbMetric: 48.3605 - val_loss: 49.9629 - val_MinusLogProbMetric: 49.9629 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 130/1000
2023-10-27 14:45:26.059 
Epoch 130/1000 
	 loss: 48.2155, MinusLogProbMetric: 48.2155, val_loss: 49.3011, val_MinusLogProbMetric: 49.3011

Epoch 130: val_loss did not improve from 47.79750
196/196 - 33s - loss: 48.2155 - MinusLogProbMetric: 48.2155 - val_loss: 49.3011 - val_MinusLogProbMetric: 49.3011 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 131/1000
2023-10-27 14:45:59.862 
Epoch 131/1000 
	 loss: 48.3348, MinusLogProbMetric: 48.3348, val_loss: 51.0030, val_MinusLogProbMetric: 51.0030

Epoch 131: val_loss did not improve from 47.79750
196/196 - 34s - loss: 48.3348 - MinusLogProbMetric: 48.3348 - val_loss: 51.0030 - val_MinusLogProbMetric: 51.0030 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 132/1000
2023-10-27 14:46:33.430 
Epoch 132/1000 
	 loss: 48.6009, MinusLogProbMetric: 48.6009, val_loss: 48.5209, val_MinusLogProbMetric: 48.5209

Epoch 132: val_loss did not improve from 47.79750
196/196 - 34s - loss: 48.6009 - MinusLogProbMetric: 48.6009 - val_loss: 48.5209 - val_MinusLogProbMetric: 48.5209 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 133/1000
2023-10-27 14:47:07.310 
Epoch 133/1000 
	 loss: 47.8931, MinusLogProbMetric: 47.8931, val_loss: 48.4165, val_MinusLogProbMetric: 48.4165

Epoch 133: val_loss did not improve from 47.79750
196/196 - 34s - loss: 47.8931 - MinusLogProbMetric: 47.8931 - val_loss: 48.4165 - val_MinusLogProbMetric: 48.4165 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 134/1000
2023-10-27 14:47:41.226 
Epoch 134/1000 
	 loss: 47.9787, MinusLogProbMetric: 47.9787, val_loss: 48.7552, val_MinusLogProbMetric: 48.7552

Epoch 134: val_loss did not improve from 47.79750
196/196 - 34s - loss: 47.9787 - MinusLogProbMetric: 47.9787 - val_loss: 48.7552 - val_MinusLogProbMetric: 48.7552 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 135/1000
2023-10-27 14:48:14.922 
Epoch 135/1000 
	 loss: 48.1773, MinusLogProbMetric: 48.1773, val_loss: 50.9299, val_MinusLogProbMetric: 50.9299

Epoch 135: val_loss did not improve from 47.79750
196/196 - 34s - loss: 48.1773 - MinusLogProbMetric: 48.1773 - val_loss: 50.9299 - val_MinusLogProbMetric: 50.9299 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 136/1000
2023-10-27 14:48:48.358 
Epoch 136/1000 
	 loss: 47.8367, MinusLogProbMetric: 47.8367, val_loss: 49.0529, val_MinusLogProbMetric: 49.0529

Epoch 136: val_loss did not improve from 47.79750
196/196 - 33s - loss: 47.8367 - MinusLogProbMetric: 47.8367 - val_loss: 49.0529 - val_MinusLogProbMetric: 49.0529 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 137/1000
2023-10-27 14:49:21.950 
Epoch 137/1000 
	 loss: 48.4947, MinusLogProbMetric: 48.4947, val_loss: 47.6419, val_MinusLogProbMetric: 47.6419

Epoch 137: val_loss improved from 47.79750 to 47.64191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 48.4947 - MinusLogProbMetric: 48.4947 - val_loss: 47.6419 - val_MinusLogProbMetric: 47.6419 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 138/1000
2023-10-27 14:49:55.219 
Epoch 138/1000 
	 loss: 47.8896, MinusLogProbMetric: 47.8896, val_loss: 48.6979, val_MinusLogProbMetric: 48.6979

Epoch 138: val_loss did not improve from 47.64191
196/196 - 33s - loss: 47.8896 - MinusLogProbMetric: 47.8896 - val_loss: 48.6979 - val_MinusLogProbMetric: 48.6979 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 139/1000
2023-10-27 14:50:28.782 
Epoch 139/1000 
	 loss: 48.0908, MinusLogProbMetric: 48.0908, val_loss: 48.9462, val_MinusLogProbMetric: 48.9462

Epoch 139: val_loss did not improve from 47.64191
196/196 - 34s - loss: 48.0908 - MinusLogProbMetric: 48.0908 - val_loss: 48.9462 - val_MinusLogProbMetric: 48.9462 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 140/1000
2023-10-27 14:51:02.261 
Epoch 140/1000 
	 loss: 48.1460, MinusLogProbMetric: 48.1460, val_loss: 48.1854, val_MinusLogProbMetric: 48.1854

Epoch 140: val_loss did not improve from 47.64191
196/196 - 33s - loss: 48.1460 - MinusLogProbMetric: 48.1460 - val_loss: 48.1854 - val_MinusLogProbMetric: 48.1854 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 141/1000
2023-10-27 14:51:36.109 
Epoch 141/1000 
	 loss: 47.7546, MinusLogProbMetric: 47.7546, val_loss: 47.4837, val_MinusLogProbMetric: 47.4837

Epoch 141: val_loss improved from 47.64191 to 47.48372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 47.7546 - MinusLogProbMetric: 47.7546 - val_loss: 47.4837 - val_MinusLogProbMetric: 47.4837 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 142/1000
2023-10-27 14:52:10.352 
Epoch 142/1000 
	 loss: 48.5794, MinusLogProbMetric: 48.5794, val_loss: 51.8798, val_MinusLogProbMetric: 51.8798

Epoch 142: val_loss did not improve from 47.48372
196/196 - 34s - loss: 48.5794 - MinusLogProbMetric: 48.5794 - val_loss: 51.8798 - val_MinusLogProbMetric: 51.8798 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 143/1000
2023-10-27 14:52:43.965 
Epoch 143/1000 
	 loss: 48.1698, MinusLogProbMetric: 48.1698, val_loss: 48.0176, val_MinusLogProbMetric: 48.0176

Epoch 143: val_loss did not improve from 47.48372
196/196 - 34s - loss: 48.1698 - MinusLogProbMetric: 48.1698 - val_loss: 48.0176 - val_MinusLogProbMetric: 48.0176 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 144/1000
2023-10-27 14:53:17.573 
Epoch 144/1000 
	 loss: 47.6792, MinusLogProbMetric: 47.6792, val_loss: 48.8220, val_MinusLogProbMetric: 48.8220

Epoch 144: val_loss did not improve from 47.48372
196/196 - 34s - loss: 47.6792 - MinusLogProbMetric: 47.6792 - val_loss: 48.8220 - val_MinusLogProbMetric: 48.8220 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 145/1000
2023-10-27 14:53:51.043 
Epoch 145/1000 
	 loss: 47.7088, MinusLogProbMetric: 47.7088, val_loss: 48.1404, val_MinusLogProbMetric: 48.1404

Epoch 145: val_loss did not improve from 47.48372
196/196 - 33s - loss: 47.7088 - MinusLogProbMetric: 47.7088 - val_loss: 48.1404 - val_MinusLogProbMetric: 48.1404 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 146/1000
2023-10-27 14:54:24.351 
Epoch 146/1000 
	 loss: 47.8475, MinusLogProbMetric: 47.8475, val_loss: 48.5982, val_MinusLogProbMetric: 48.5982

Epoch 146: val_loss did not improve from 47.48372
196/196 - 33s - loss: 47.8475 - MinusLogProbMetric: 47.8475 - val_loss: 48.5982 - val_MinusLogProbMetric: 48.5982 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 147/1000
2023-10-27 14:54:58.160 
Epoch 147/1000 
	 loss: 47.8958, MinusLogProbMetric: 47.8958, val_loss: 48.4541, val_MinusLogProbMetric: 48.4541

Epoch 147: val_loss did not improve from 47.48372
196/196 - 34s - loss: 47.8958 - MinusLogProbMetric: 47.8958 - val_loss: 48.4541 - val_MinusLogProbMetric: 48.4541 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 148/1000
2023-10-27 14:55:31.615 
Epoch 148/1000 
	 loss: 47.9901, MinusLogProbMetric: 47.9901, val_loss: 47.7636, val_MinusLogProbMetric: 47.7636

Epoch 148: val_loss did not improve from 47.48372
196/196 - 33s - loss: 47.9901 - MinusLogProbMetric: 47.9901 - val_loss: 47.7636 - val_MinusLogProbMetric: 47.7636 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 149/1000
2023-10-27 14:56:05.647 
Epoch 149/1000 
	 loss: 47.7185, MinusLogProbMetric: 47.7185, val_loss: 47.9909, val_MinusLogProbMetric: 47.9909

Epoch 149: val_loss did not improve from 47.48372
196/196 - 34s - loss: 47.7185 - MinusLogProbMetric: 47.7185 - val_loss: 47.9909 - val_MinusLogProbMetric: 47.9909 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 150/1000
2023-10-27 14:56:39.038 
Epoch 150/1000 
	 loss: 48.5032, MinusLogProbMetric: 48.5032, val_loss: 49.6401, val_MinusLogProbMetric: 49.6401

Epoch 150: val_loss did not improve from 47.48372
196/196 - 33s - loss: 48.5032 - MinusLogProbMetric: 48.5032 - val_loss: 49.6401 - val_MinusLogProbMetric: 49.6401 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 151/1000
2023-10-27 14:57:09.361 
Epoch 151/1000 
	 loss: 48.5196, MinusLogProbMetric: 48.5196, val_loss: 48.0159, val_MinusLogProbMetric: 48.0159

Epoch 151: val_loss did not improve from 47.48372
196/196 - 30s - loss: 48.5196 - MinusLogProbMetric: 48.5196 - val_loss: 48.0159 - val_MinusLogProbMetric: 48.0159 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 152/1000
2023-10-27 14:57:38.819 
Epoch 152/1000 
	 loss: 47.9055, MinusLogProbMetric: 47.9055, val_loss: 49.1116, val_MinusLogProbMetric: 49.1116

Epoch 152: val_loss did not improve from 47.48372
196/196 - 29s - loss: 47.9055 - MinusLogProbMetric: 47.9055 - val_loss: 49.1116 - val_MinusLogProbMetric: 49.1116 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 153/1000
2023-10-27 14:58:08.353 
Epoch 153/1000 
	 loss: 47.7171, MinusLogProbMetric: 47.7171, val_loss: 49.4547, val_MinusLogProbMetric: 49.4547

Epoch 153: val_loss did not improve from 47.48372
196/196 - 30s - loss: 47.7171 - MinusLogProbMetric: 47.7171 - val_loss: 49.4547 - val_MinusLogProbMetric: 49.4547 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 154/1000
2023-10-27 14:58:41.897 
Epoch 154/1000 
	 loss: 47.3937, MinusLogProbMetric: 47.3937, val_loss: 47.9477, val_MinusLogProbMetric: 47.9477

Epoch 154: val_loss did not improve from 47.48372
196/196 - 34s - loss: 47.3937 - MinusLogProbMetric: 47.3937 - val_loss: 47.9477 - val_MinusLogProbMetric: 47.9477 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 155/1000
2023-10-27 14:59:14.809 
Epoch 155/1000 
	 loss: 47.5128, MinusLogProbMetric: 47.5128, val_loss: 47.6198, val_MinusLogProbMetric: 47.6198

Epoch 155: val_loss did not improve from 47.48372
196/196 - 33s - loss: 47.5128 - MinusLogProbMetric: 47.5128 - val_loss: 47.6198 - val_MinusLogProbMetric: 47.6198 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 156/1000
2023-10-27 14:59:45.063 
Epoch 156/1000 
	 loss: 47.2948, MinusLogProbMetric: 47.2948, val_loss: 48.0615, val_MinusLogProbMetric: 48.0615

Epoch 156: val_loss did not improve from 47.48372
196/196 - 30s - loss: 47.2948 - MinusLogProbMetric: 47.2948 - val_loss: 48.0615 - val_MinusLogProbMetric: 48.0615 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 157/1000
2023-10-27 15:00:17.881 
Epoch 157/1000 
	 loss: 48.0378, MinusLogProbMetric: 48.0378, val_loss: 47.7711, val_MinusLogProbMetric: 47.7711

Epoch 157: val_loss did not improve from 47.48372
196/196 - 33s - loss: 48.0378 - MinusLogProbMetric: 48.0378 - val_loss: 47.7711 - val_MinusLogProbMetric: 47.7711 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 158/1000
2023-10-27 15:00:47.870 
Epoch 158/1000 
	 loss: 47.4479, MinusLogProbMetric: 47.4479, val_loss: 48.1406, val_MinusLogProbMetric: 48.1406

Epoch 158: val_loss did not improve from 47.48372
196/196 - 30s - loss: 47.4479 - MinusLogProbMetric: 47.4479 - val_loss: 48.1406 - val_MinusLogProbMetric: 48.1406 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 159/1000
2023-10-27 15:01:20.157 
Epoch 159/1000 
	 loss: 47.2094, MinusLogProbMetric: 47.2094, val_loss: 47.3381, val_MinusLogProbMetric: 47.3381

Epoch 159: val_loss improved from 47.48372 to 47.33811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 47.2094 - MinusLogProbMetric: 47.2094 - val_loss: 47.3381 - val_MinusLogProbMetric: 47.3381 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 160/1000
2023-10-27 15:01:54.068 
Epoch 160/1000 
	 loss: 47.5974, MinusLogProbMetric: 47.5974, val_loss: 48.4007, val_MinusLogProbMetric: 48.4007

Epoch 160: val_loss did not improve from 47.33811
196/196 - 33s - loss: 47.5974 - MinusLogProbMetric: 47.5974 - val_loss: 48.4007 - val_MinusLogProbMetric: 48.4007 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 161/1000
2023-10-27 15:02:25.732 
Epoch 161/1000 
	 loss: 47.4439, MinusLogProbMetric: 47.4439, val_loss: 48.1599, val_MinusLogProbMetric: 48.1599

Epoch 161: val_loss did not improve from 47.33811
196/196 - 32s - loss: 47.4439 - MinusLogProbMetric: 47.4439 - val_loss: 48.1599 - val_MinusLogProbMetric: 48.1599 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 162/1000
2023-10-27 15:02:57.249 
Epoch 162/1000 
	 loss: 47.2132, MinusLogProbMetric: 47.2132, val_loss: 49.0204, val_MinusLogProbMetric: 49.0204

Epoch 162: val_loss did not improve from 47.33811
196/196 - 32s - loss: 47.2132 - MinusLogProbMetric: 47.2132 - val_loss: 49.0204 - val_MinusLogProbMetric: 49.0204 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 163/1000
2023-10-27 15:03:29.806 
Epoch 163/1000 
	 loss: 47.1105, MinusLogProbMetric: 47.1105, val_loss: 48.5833, val_MinusLogProbMetric: 48.5833

Epoch 163: val_loss did not improve from 47.33811
196/196 - 33s - loss: 47.1105 - MinusLogProbMetric: 47.1105 - val_loss: 48.5833 - val_MinusLogProbMetric: 48.5833 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 164/1000
2023-10-27 15:04:00.101 
Epoch 164/1000 
	 loss: 47.4280, MinusLogProbMetric: 47.4280, val_loss: 47.8239, val_MinusLogProbMetric: 47.8239

Epoch 164: val_loss did not improve from 47.33811
196/196 - 30s - loss: 47.4280 - MinusLogProbMetric: 47.4280 - val_loss: 47.8239 - val_MinusLogProbMetric: 47.8239 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 165/1000
2023-10-27 15:04:33.339 
Epoch 165/1000 
	 loss: 48.4966, MinusLogProbMetric: 48.4966, val_loss: 48.9508, val_MinusLogProbMetric: 48.9508

Epoch 165: val_loss did not improve from 47.33811
196/196 - 33s - loss: 48.4966 - MinusLogProbMetric: 48.4966 - val_loss: 48.9508 - val_MinusLogProbMetric: 48.9508 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 166/1000
2023-10-27 15:05:06.548 
Epoch 166/1000 
	 loss: 48.3393, MinusLogProbMetric: 48.3393, val_loss: 47.7267, val_MinusLogProbMetric: 47.7267

Epoch 166: val_loss did not improve from 47.33811
196/196 - 33s - loss: 48.3393 - MinusLogProbMetric: 48.3393 - val_loss: 47.7267 - val_MinusLogProbMetric: 47.7267 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 167/1000
2023-10-27 15:05:38.611 
Epoch 167/1000 
	 loss: 47.8685, MinusLogProbMetric: 47.8685, val_loss: 48.2229, val_MinusLogProbMetric: 48.2229

Epoch 167: val_loss did not improve from 47.33811
196/196 - 32s - loss: 47.8685 - MinusLogProbMetric: 47.8685 - val_loss: 48.2229 - val_MinusLogProbMetric: 48.2229 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 168/1000
2023-10-27 15:06:09.353 
Epoch 168/1000 
	 loss: 47.7999, MinusLogProbMetric: 47.7999, val_loss: 47.5420, val_MinusLogProbMetric: 47.5420

Epoch 168: val_loss did not improve from 47.33811
196/196 - 31s - loss: 47.7999 - MinusLogProbMetric: 47.7999 - val_loss: 47.5420 - val_MinusLogProbMetric: 47.5420 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 169/1000
2023-10-27 15:06:40.478 
Epoch 169/1000 
	 loss: 47.6288, MinusLogProbMetric: 47.6288, val_loss: 47.8730, val_MinusLogProbMetric: 47.8730

Epoch 169: val_loss did not improve from 47.33811
196/196 - 31s - loss: 47.6288 - MinusLogProbMetric: 47.6288 - val_loss: 47.8730 - val_MinusLogProbMetric: 47.8730 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 170/1000
2023-10-27 15:07:12.830 
Epoch 170/1000 
	 loss: 47.1311, MinusLogProbMetric: 47.1311, val_loss: 47.1331, val_MinusLogProbMetric: 47.1331

Epoch 170: val_loss improved from 47.33811 to 47.13311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 47.1311 - MinusLogProbMetric: 47.1311 - val_loss: 47.1331 - val_MinusLogProbMetric: 47.1331 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 171/1000
2023-10-27 15:07:46.566 
Epoch 171/1000 
	 loss: 47.4084, MinusLogProbMetric: 47.4084, val_loss: 50.0033, val_MinusLogProbMetric: 50.0033

Epoch 171: val_loss did not improve from 47.13311
196/196 - 33s - loss: 47.4084 - MinusLogProbMetric: 47.4084 - val_loss: 50.0033 - val_MinusLogProbMetric: 50.0033 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 172/1000
2023-10-27 15:08:17.011 
Epoch 172/1000 
	 loss: 47.4081, MinusLogProbMetric: 47.4081, val_loss: 46.6428, val_MinusLogProbMetric: 46.6428

Epoch 172: val_loss improved from 47.13311 to 46.64284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 47.4081 - MinusLogProbMetric: 47.4081 - val_loss: 46.6428 - val_MinusLogProbMetric: 46.6428 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 173/1000
2023-10-27 15:08:47.928 
Epoch 173/1000 
	 loss: 47.2262, MinusLogProbMetric: 47.2262, val_loss: 47.0102, val_MinusLogProbMetric: 47.0102

Epoch 173: val_loss did not improve from 46.64284
196/196 - 30s - loss: 47.2262 - MinusLogProbMetric: 47.2262 - val_loss: 47.0102 - val_MinusLogProbMetric: 47.0102 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 174/1000
2023-10-27 15:09:18.889 
Epoch 174/1000 
	 loss: 47.0577, MinusLogProbMetric: 47.0577, val_loss: 47.1831, val_MinusLogProbMetric: 47.1831

Epoch 174: val_loss did not improve from 46.64284
196/196 - 31s - loss: 47.0577 - MinusLogProbMetric: 47.0577 - val_loss: 47.1831 - val_MinusLogProbMetric: 47.1831 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 175/1000
2023-10-27 15:09:51.408 
Epoch 175/1000 
	 loss: 47.4448, MinusLogProbMetric: 47.4448, val_loss: 49.2014, val_MinusLogProbMetric: 49.2014

Epoch 175: val_loss did not improve from 46.64284
196/196 - 33s - loss: 47.4448 - MinusLogProbMetric: 47.4448 - val_loss: 49.2014 - val_MinusLogProbMetric: 49.2014 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 176/1000
2023-10-27 15:10:24.454 
Epoch 176/1000 
	 loss: 47.1248, MinusLogProbMetric: 47.1248, val_loss: 49.2878, val_MinusLogProbMetric: 49.2878

Epoch 176: val_loss did not improve from 46.64284
196/196 - 33s - loss: 47.1248 - MinusLogProbMetric: 47.1248 - val_loss: 49.2878 - val_MinusLogProbMetric: 49.2878 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 177/1000
2023-10-27 15:10:57.180 
Epoch 177/1000 
	 loss: 47.0656, MinusLogProbMetric: 47.0656, val_loss: 48.6860, val_MinusLogProbMetric: 48.6860

Epoch 177: val_loss did not improve from 46.64284
196/196 - 33s - loss: 47.0656 - MinusLogProbMetric: 47.0656 - val_loss: 48.6860 - val_MinusLogProbMetric: 48.6860 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 178/1000
2023-10-27 15:11:28.292 
Epoch 178/1000 
	 loss: 47.0391, MinusLogProbMetric: 47.0391, val_loss: 48.2792, val_MinusLogProbMetric: 48.2792

Epoch 178: val_loss did not improve from 46.64284
196/196 - 31s - loss: 47.0391 - MinusLogProbMetric: 47.0391 - val_loss: 48.2792 - val_MinusLogProbMetric: 48.2792 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 179/1000
2023-10-27 15:12:01.494 
Epoch 179/1000 
	 loss: 47.4379, MinusLogProbMetric: 47.4379, val_loss: 48.0010, val_MinusLogProbMetric: 48.0010

Epoch 179: val_loss did not improve from 46.64284
196/196 - 33s - loss: 47.4379 - MinusLogProbMetric: 47.4379 - val_loss: 48.0010 - val_MinusLogProbMetric: 48.0010 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 180/1000
2023-10-27 15:12:32.976 
Epoch 180/1000 
	 loss: 46.8300, MinusLogProbMetric: 46.8300, val_loss: 48.7125, val_MinusLogProbMetric: 48.7125

Epoch 180: val_loss did not improve from 46.64284
196/196 - 31s - loss: 46.8300 - MinusLogProbMetric: 46.8300 - val_loss: 48.7125 - val_MinusLogProbMetric: 48.7125 - lr: 3.3333e-04 - 31s/epoch - 161ms/step
Epoch 181/1000
2023-10-27 15:13:06.404 
Epoch 181/1000 
	 loss: 47.1846, MinusLogProbMetric: 47.1846, val_loss: 48.3782, val_MinusLogProbMetric: 48.3782

Epoch 181: val_loss did not improve from 46.64284
196/196 - 33s - loss: 47.1846 - MinusLogProbMetric: 47.1846 - val_loss: 48.3782 - val_MinusLogProbMetric: 48.3782 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 182/1000
2023-10-27 15:13:38.577 
Epoch 182/1000 
	 loss: 47.1751, MinusLogProbMetric: 47.1751, val_loss: 47.3755, val_MinusLogProbMetric: 47.3755

Epoch 182: val_loss did not improve from 46.64284
196/196 - 32s - loss: 47.1751 - MinusLogProbMetric: 47.1751 - val_loss: 47.3755 - val_MinusLogProbMetric: 47.3755 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 183/1000
2023-10-27 15:14:09.903 
Epoch 183/1000 
	 loss: 46.8406, MinusLogProbMetric: 46.8406, val_loss: 47.6149, val_MinusLogProbMetric: 47.6149

Epoch 183: val_loss did not improve from 46.64284
196/196 - 31s - loss: 46.8406 - MinusLogProbMetric: 46.8406 - val_loss: 47.6149 - val_MinusLogProbMetric: 47.6149 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 184/1000
2023-10-27 15:14:41.671 
Epoch 184/1000 
	 loss: 47.6005, MinusLogProbMetric: 47.6005, val_loss: 48.1923, val_MinusLogProbMetric: 48.1923

Epoch 184: val_loss did not improve from 46.64284
196/196 - 32s - loss: 47.6005 - MinusLogProbMetric: 47.6005 - val_loss: 48.1923 - val_MinusLogProbMetric: 48.1923 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 185/1000
2023-10-27 15:15:13.186 
Epoch 185/1000 
	 loss: 47.4702, MinusLogProbMetric: 47.4702, val_loss: 47.0087, val_MinusLogProbMetric: 47.0087

Epoch 185: val_loss did not improve from 46.64284
196/196 - 32s - loss: 47.4702 - MinusLogProbMetric: 47.4702 - val_loss: 47.0087 - val_MinusLogProbMetric: 47.0087 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 186/1000
2023-10-27 15:15:45.553 
Epoch 186/1000 
	 loss: 46.8810, MinusLogProbMetric: 46.8810, val_loss: 46.5718, val_MinusLogProbMetric: 46.5718

Epoch 186: val_loss improved from 46.64284 to 46.57185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 46.8810 - MinusLogProbMetric: 46.8810 - val_loss: 46.5718 - val_MinusLogProbMetric: 46.5718 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 187/1000
2023-10-27 15:16:18.845 
Epoch 187/1000 
	 loss: 46.6349, MinusLogProbMetric: 46.6349, val_loss: 49.5686, val_MinusLogProbMetric: 49.5686

Epoch 187: val_loss did not improve from 46.57185
196/196 - 33s - loss: 46.6349 - MinusLogProbMetric: 46.6349 - val_loss: 49.5686 - val_MinusLogProbMetric: 49.5686 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 188/1000
2023-10-27 15:16:50.140 
Epoch 188/1000 
	 loss: 46.6045, MinusLogProbMetric: 46.6045, val_loss: 48.3769, val_MinusLogProbMetric: 48.3769

Epoch 188: val_loss did not improve from 46.57185
196/196 - 31s - loss: 46.6045 - MinusLogProbMetric: 46.6045 - val_loss: 48.3769 - val_MinusLogProbMetric: 48.3769 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 189/1000
2023-10-27 15:17:20.982 
Epoch 189/1000 
	 loss: 47.0271, MinusLogProbMetric: 47.0271, val_loss: 48.4357, val_MinusLogProbMetric: 48.4357

Epoch 189: val_loss did not improve from 46.57185
196/196 - 31s - loss: 47.0271 - MinusLogProbMetric: 47.0271 - val_loss: 48.4357 - val_MinusLogProbMetric: 48.4357 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 190/1000
2023-10-27 15:17:51.812 
Epoch 190/1000 
	 loss: 46.8450, MinusLogProbMetric: 46.8450, val_loss: 46.8877, val_MinusLogProbMetric: 46.8877

Epoch 190: val_loss did not improve from 46.57185
196/196 - 31s - loss: 46.8450 - MinusLogProbMetric: 46.8450 - val_loss: 46.8877 - val_MinusLogProbMetric: 46.8877 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 191/1000
2023-10-27 15:18:23.715 
Epoch 191/1000 
	 loss: 46.6453, MinusLogProbMetric: 46.6453, val_loss: 47.3048, val_MinusLogProbMetric: 47.3048

Epoch 191: val_loss did not improve from 46.57185
196/196 - 32s - loss: 46.6453 - MinusLogProbMetric: 46.6453 - val_loss: 47.3048 - val_MinusLogProbMetric: 47.3048 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 192/1000
2023-10-27 15:18:57.338 
Epoch 192/1000 
	 loss: 46.4329, MinusLogProbMetric: 46.4329, val_loss: 48.7023, val_MinusLogProbMetric: 48.7023

Epoch 192: val_loss did not improve from 46.57185
196/196 - 34s - loss: 46.4329 - MinusLogProbMetric: 46.4329 - val_loss: 48.7023 - val_MinusLogProbMetric: 48.7023 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 193/1000
2023-10-27 15:19:28.404 
Epoch 193/1000 
	 loss: 46.6059, MinusLogProbMetric: 46.6059, val_loss: 46.6766, val_MinusLogProbMetric: 46.6766

Epoch 193: val_loss did not improve from 46.57185
196/196 - 31s - loss: 46.6059 - MinusLogProbMetric: 46.6059 - val_loss: 46.6766 - val_MinusLogProbMetric: 46.6766 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 194/1000
2023-10-27 15:19:58.538 
Epoch 194/1000 
	 loss: 46.6878, MinusLogProbMetric: 46.6878, val_loss: 47.2147, val_MinusLogProbMetric: 47.2147

Epoch 194: val_loss did not improve from 46.57185
196/196 - 30s - loss: 46.6878 - MinusLogProbMetric: 46.6878 - val_loss: 47.2147 - val_MinusLogProbMetric: 47.2147 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 195/1000
2023-10-27 15:20:29.673 
Epoch 195/1000 
	 loss: 47.2270, MinusLogProbMetric: 47.2270, val_loss: 47.2038, val_MinusLogProbMetric: 47.2038

Epoch 195: val_loss did not improve from 46.57185
196/196 - 31s - loss: 47.2270 - MinusLogProbMetric: 47.2270 - val_loss: 47.2038 - val_MinusLogProbMetric: 47.2038 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 196/1000
2023-10-27 15:21:03.414 
Epoch 196/1000 
	 loss: 46.9268, MinusLogProbMetric: 46.9268, val_loss: 49.9391, val_MinusLogProbMetric: 49.9391

Epoch 196: val_loss did not improve from 46.57185
196/196 - 34s - loss: 46.9268 - MinusLogProbMetric: 46.9268 - val_loss: 49.9391 - val_MinusLogProbMetric: 49.9391 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 197/1000
2023-10-27 15:21:36.585 
Epoch 197/1000 
	 loss: 48.0495, MinusLogProbMetric: 48.0495, val_loss: 47.5372, val_MinusLogProbMetric: 47.5372

Epoch 197: val_loss did not improve from 46.57185
196/196 - 33s - loss: 48.0495 - MinusLogProbMetric: 48.0495 - val_loss: 47.5372 - val_MinusLogProbMetric: 47.5372 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 198/1000
2023-10-27 15:22:06.463 
Epoch 198/1000 
	 loss: 47.0101, MinusLogProbMetric: 47.0101, val_loss: 49.3179, val_MinusLogProbMetric: 49.3179

Epoch 198: val_loss did not improve from 46.57185
196/196 - 30s - loss: 47.0101 - MinusLogProbMetric: 47.0101 - val_loss: 49.3179 - val_MinusLogProbMetric: 49.3179 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 199/1000
2023-10-27 15:22:38.674 
Epoch 199/1000 
	 loss: 47.7105, MinusLogProbMetric: 47.7105, val_loss: 48.1500, val_MinusLogProbMetric: 48.1500

Epoch 199: val_loss did not improve from 46.57185
196/196 - 32s - loss: 47.7105 - MinusLogProbMetric: 47.7105 - val_loss: 48.1500 - val_MinusLogProbMetric: 48.1500 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 200/1000
2023-10-27 15:23:09.014 
Epoch 200/1000 
	 loss: 46.9302, MinusLogProbMetric: 46.9302, val_loss: 47.2135, val_MinusLogProbMetric: 47.2135

Epoch 200: val_loss did not improve from 46.57185
196/196 - 30s - loss: 46.9302 - MinusLogProbMetric: 46.9302 - val_loss: 47.2135 - val_MinusLogProbMetric: 47.2135 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 201/1000
2023-10-27 15:23:39.903 
Epoch 201/1000 
	 loss: 47.2501, MinusLogProbMetric: 47.2501, val_loss: 46.3435, val_MinusLogProbMetric: 46.3435

Epoch 201: val_loss improved from 46.57185 to 46.34349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 47.2501 - MinusLogProbMetric: 47.2501 - val_loss: 46.3435 - val_MinusLogProbMetric: 46.3435 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 202/1000
2023-10-27 15:24:13.459 
Epoch 202/1000 
	 loss: 46.9279, MinusLogProbMetric: 46.9279, val_loss: 48.0491, val_MinusLogProbMetric: 48.0491

Epoch 202: val_loss did not improve from 46.34349
196/196 - 33s - loss: 46.9279 - MinusLogProbMetric: 46.9279 - val_loss: 48.0491 - val_MinusLogProbMetric: 48.0491 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 203/1000
2023-10-27 15:24:45.461 
Epoch 203/1000 
	 loss: 46.8230, MinusLogProbMetric: 46.8230, val_loss: 47.6851, val_MinusLogProbMetric: 47.6851

Epoch 203: val_loss did not improve from 46.34349
196/196 - 32s - loss: 46.8230 - MinusLogProbMetric: 46.8230 - val_loss: 47.6851 - val_MinusLogProbMetric: 47.6851 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 204/1000
2023-10-27 15:25:14.847 
Epoch 204/1000 
	 loss: 46.5784, MinusLogProbMetric: 46.5784, val_loss: 49.7743, val_MinusLogProbMetric: 49.7743

Epoch 204: val_loss did not improve from 46.34349
196/196 - 29s - loss: 46.5784 - MinusLogProbMetric: 46.5784 - val_loss: 49.7743 - val_MinusLogProbMetric: 49.7743 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 205/1000
2023-10-27 15:25:45.817 
Epoch 205/1000 
	 loss: 46.3833, MinusLogProbMetric: 46.3833, val_loss: 47.2444, val_MinusLogProbMetric: 47.2444

Epoch 205: val_loss did not improve from 46.34349
196/196 - 31s - loss: 46.3833 - MinusLogProbMetric: 46.3833 - val_loss: 47.2444 - val_MinusLogProbMetric: 47.2444 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 206/1000
2023-10-27 15:26:18.534 
Epoch 206/1000 
	 loss: 46.8387, MinusLogProbMetric: 46.8387, val_loss: 46.1611, val_MinusLogProbMetric: 46.1611

Epoch 206: val_loss improved from 46.34349 to 46.16108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 46.8387 - MinusLogProbMetric: 46.8387 - val_loss: 46.1611 - val_MinusLogProbMetric: 46.1611 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 207/1000
2023-10-27 15:26:52.805 
Epoch 207/1000 
	 loss: 46.3741, MinusLogProbMetric: 46.3741, val_loss: 47.2593, val_MinusLogProbMetric: 47.2593

Epoch 207: val_loss did not improve from 46.16108
196/196 - 34s - loss: 46.3741 - MinusLogProbMetric: 46.3741 - val_loss: 47.2593 - val_MinusLogProbMetric: 47.2593 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 208/1000
2023-10-27 15:27:24.640 
Epoch 208/1000 
	 loss: 46.3682, MinusLogProbMetric: 46.3682, val_loss: 48.6285, val_MinusLogProbMetric: 48.6285

Epoch 208: val_loss did not improve from 46.16108
196/196 - 32s - loss: 46.3682 - MinusLogProbMetric: 46.3682 - val_loss: 48.6285 - val_MinusLogProbMetric: 48.6285 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 209/1000
2023-10-27 15:27:55.278 
Epoch 209/1000 
	 loss: 46.4599, MinusLogProbMetric: 46.4599, val_loss: 46.8956, val_MinusLogProbMetric: 46.8956

Epoch 209: val_loss did not improve from 46.16108
196/196 - 31s - loss: 46.4599 - MinusLogProbMetric: 46.4599 - val_loss: 46.8956 - val_MinusLogProbMetric: 46.8956 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 210/1000
2023-10-27 15:28:26.029 
Epoch 210/1000 
	 loss: 46.1614, MinusLogProbMetric: 46.1614, val_loss: 46.7180, val_MinusLogProbMetric: 46.7180

Epoch 210: val_loss did not improve from 46.16108
196/196 - 31s - loss: 46.1614 - MinusLogProbMetric: 46.1614 - val_loss: 46.7180 - val_MinusLogProbMetric: 46.7180 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 211/1000
2023-10-27 15:28:57.866 
Epoch 211/1000 
	 loss: 46.3085, MinusLogProbMetric: 46.3085, val_loss: 47.0787, val_MinusLogProbMetric: 47.0787

Epoch 211: val_loss did not improve from 46.16108
196/196 - 32s - loss: 46.3085 - MinusLogProbMetric: 46.3085 - val_loss: 47.0787 - val_MinusLogProbMetric: 47.0787 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 212/1000
2023-10-27 15:29:31.127 
Epoch 212/1000 
	 loss: 46.5392, MinusLogProbMetric: 46.5392, val_loss: 47.5907, val_MinusLogProbMetric: 47.5907

Epoch 212: val_loss did not improve from 46.16108
196/196 - 33s - loss: 46.5392 - MinusLogProbMetric: 46.5392 - val_loss: 47.5907 - val_MinusLogProbMetric: 47.5907 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 213/1000
2023-10-27 15:30:02.712 
Epoch 213/1000 
	 loss: 46.9624, MinusLogProbMetric: 46.9624, val_loss: 45.9275, val_MinusLogProbMetric: 45.9275

Epoch 213: val_loss improved from 46.16108 to 45.92746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 46.9624 - MinusLogProbMetric: 46.9624 - val_loss: 45.9275 - val_MinusLogProbMetric: 45.9275 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 214/1000
2023-10-27 15:30:33.520 
Epoch 214/1000 
	 loss: 46.1809, MinusLogProbMetric: 46.1809, val_loss: 46.0559, val_MinusLogProbMetric: 46.0559

Epoch 214: val_loss did not improve from 45.92746
196/196 - 30s - loss: 46.1809 - MinusLogProbMetric: 46.1809 - val_loss: 46.0559 - val_MinusLogProbMetric: 46.0559 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 215/1000
2023-10-27 15:31:04.062 
Epoch 215/1000 
	 loss: 46.3775, MinusLogProbMetric: 46.3775, val_loss: 47.5412, val_MinusLogProbMetric: 47.5412

Epoch 215: val_loss did not improve from 45.92746
196/196 - 31s - loss: 46.3775 - MinusLogProbMetric: 46.3775 - val_loss: 47.5412 - val_MinusLogProbMetric: 47.5412 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 216/1000
2023-10-27 15:31:35.536 
Epoch 216/1000 
	 loss: 46.2493, MinusLogProbMetric: 46.2493, val_loss: 46.9793, val_MinusLogProbMetric: 46.9793

Epoch 216: val_loss did not improve from 45.92746
196/196 - 31s - loss: 46.2493 - MinusLogProbMetric: 46.2493 - val_loss: 46.9793 - val_MinusLogProbMetric: 46.9793 - lr: 3.3333e-04 - 31s/epoch - 161ms/step
Epoch 217/1000
2023-10-27 15:32:08.641 
Epoch 217/1000 
	 loss: 46.2198, MinusLogProbMetric: 46.2198, val_loss: 48.0142, val_MinusLogProbMetric: 48.0142

Epoch 217: val_loss did not improve from 45.92746
196/196 - 33s - loss: 46.2198 - MinusLogProbMetric: 46.2198 - val_loss: 48.0142 - val_MinusLogProbMetric: 48.0142 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 218/1000
2023-10-27 15:32:41.336 
Epoch 218/1000 
	 loss: 46.2975, MinusLogProbMetric: 46.2975, val_loss: 46.4927, val_MinusLogProbMetric: 46.4927

Epoch 218: val_loss did not improve from 45.92746
196/196 - 33s - loss: 46.2975 - MinusLogProbMetric: 46.2975 - val_loss: 46.4927 - val_MinusLogProbMetric: 46.4927 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 219/1000
2023-10-27 15:33:12.686 
Epoch 219/1000 
	 loss: 46.4582, MinusLogProbMetric: 46.4582, val_loss: 46.5173, val_MinusLogProbMetric: 46.5173

Epoch 219: val_loss did not improve from 45.92746
196/196 - 31s - loss: 46.4582 - MinusLogProbMetric: 46.4582 - val_loss: 46.5173 - val_MinusLogProbMetric: 46.5173 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 220/1000
2023-10-27 15:33:42.644 
Epoch 220/1000 
	 loss: 45.8443, MinusLogProbMetric: 45.8443, val_loss: 46.2043, val_MinusLogProbMetric: 46.2043

Epoch 220: val_loss did not improve from 45.92746
196/196 - 30s - loss: 45.8443 - MinusLogProbMetric: 45.8443 - val_loss: 46.2043 - val_MinusLogProbMetric: 46.2043 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 221/1000
2023-10-27 15:34:15.346 
Epoch 221/1000 
	 loss: 46.3161, MinusLogProbMetric: 46.3161, val_loss: 45.9475, val_MinusLogProbMetric: 45.9475

Epoch 221: val_loss did not improve from 45.92746
196/196 - 33s - loss: 46.3161 - MinusLogProbMetric: 46.3161 - val_loss: 45.9475 - val_MinusLogProbMetric: 45.9475 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 222/1000
2023-10-27 15:34:47.099 
Epoch 222/1000 
	 loss: 45.8674, MinusLogProbMetric: 45.8674, val_loss: 47.2812, val_MinusLogProbMetric: 47.2812

Epoch 222: val_loss did not improve from 45.92746
196/196 - 32s - loss: 45.8674 - MinusLogProbMetric: 45.8674 - val_loss: 47.2812 - val_MinusLogProbMetric: 47.2812 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 223/1000
2023-10-27 15:35:20.645 
Epoch 223/1000 
	 loss: 46.1456, MinusLogProbMetric: 46.1456, val_loss: 46.0552, val_MinusLogProbMetric: 46.0552

Epoch 223: val_loss did not improve from 45.92746
196/196 - 34s - loss: 46.1456 - MinusLogProbMetric: 46.1456 - val_loss: 46.0552 - val_MinusLogProbMetric: 46.0552 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 224/1000
2023-10-27 15:35:54.002 
Epoch 224/1000 
	 loss: 46.1852, MinusLogProbMetric: 46.1852, val_loss: 47.1743, val_MinusLogProbMetric: 47.1743

Epoch 224: val_loss did not improve from 45.92746
196/196 - 33s - loss: 46.1852 - MinusLogProbMetric: 46.1852 - val_loss: 47.1743 - val_MinusLogProbMetric: 47.1743 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 225/1000
2023-10-27 15:36:24.800 
Epoch 225/1000 
	 loss: 45.9156, MinusLogProbMetric: 45.9156, val_loss: 45.8269, val_MinusLogProbMetric: 45.8269

Epoch 225: val_loss improved from 45.92746 to 45.82685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 45.9156 - MinusLogProbMetric: 45.9156 - val_loss: 45.8269 - val_MinusLogProbMetric: 45.8269 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 226/1000
2023-10-27 15:36:56.246 
Epoch 226/1000 
	 loss: 46.0858, MinusLogProbMetric: 46.0858, val_loss: 46.5453, val_MinusLogProbMetric: 46.5453

Epoch 226: val_loss did not improve from 45.82685
196/196 - 31s - loss: 46.0858 - MinusLogProbMetric: 46.0858 - val_loss: 46.5453 - val_MinusLogProbMetric: 46.5453 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 227/1000
2023-10-27 15:37:27.508 
Epoch 227/1000 
	 loss: 46.3051, MinusLogProbMetric: 46.3051, val_loss: 45.8635, val_MinusLogProbMetric: 45.8635

Epoch 227: val_loss did not improve from 45.82685
196/196 - 31s - loss: 46.3051 - MinusLogProbMetric: 46.3051 - val_loss: 45.8635 - val_MinusLogProbMetric: 45.8635 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 228/1000
2023-10-27 15:37:59.346 
Epoch 228/1000 
	 loss: 46.7707, MinusLogProbMetric: 46.7707, val_loss: 46.3376, val_MinusLogProbMetric: 46.3376

Epoch 228: val_loss did not improve from 45.82685
196/196 - 32s - loss: 46.7707 - MinusLogProbMetric: 46.7707 - val_loss: 46.3376 - val_MinusLogProbMetric: 46.3376 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 229/1000
2023-10-27 15:38:33.181 
Epoch 229/1000 
	 loss: 46.2980, MinusLogProbMetric: 46.2980, val_loss: 47.7651, val_MinusLogProbMetric: 47.7651

Epoch 229: val_loss did not improve from 45.82685
196/196 - 34s - loss: 46.2980 - MinusLogProbMetric: 46.2980 - val_loss: 47.7651 - val_MinusLogProbMetric: 47.7651 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 230/1000
2023-10-27 15:39:06.606 
Epoch 230/1000 
	 loss: 46.1786, MinusLogProbMetric: 46.1786, val_loss: 46.9009, val_MinusLogProbMetric: 46.9009

Epoch 230: val_loss did not improve from 45.82685
196/196 - 33s - loss: 46.1786 - MinusLogProbMetric: 46.1786 - val_loss: 46.9009 - val_MinusLogProbMetric: 46.9009 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 231/1000
2023-10-27 15:39:38.911 
Epoch 231/1000 
	 loss: 45.9936, MinusLogProbMetric: 45.9936, val_loss: 46.5045, val_MinusLogProbMetric: 46.5045

Epoch 231: val_loss did not improve from 45.82685
196/196 - 32s - loss: 45.9936 - MinusLogProbMetric: 45.9936 - val_loss: 46.5045 - val_MinusLogProbMetric: 46.5045 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 232/1000
2023-10-27 15:40:10.344 
Epoch 232/1000 
	 loss: 46.5798, MinusLogProbMetric: 46.5798, val_loss: 47.6647, val_MinusLogProbMetric: 47.6647

Epoch 232: val_loss did not improve from 45.82685
196/196 - 31s - loss: 46.5798 - MinusLogProbMetric: 46.5798 - val_loss: 47.6647 - val_MinusLogProbMetric: 47.6647 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 233/1000
2023-10-27 15:40:40.866 
Epoch 233/1000 
	 loss: 45.7186, MinusLogProbMetric: 45.7186, val_loss: 48.3227, val_MinusLogProbMetric: 48.3227

Epoch 233: val_loss did not improve from 45.82685
196/196 - 31s - loss: 45.7186 - MinusLogProbMetric: 45.7186 - val_loss: 48.3227 - val_MinusLogProbMetric: 48.3227 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 234/1000
2023-10-27 15:41:14.167 
Epoch 234/1000 
	 loss: 45.9959, MinusLogProbMetric: 45.9959, val_loss: 46.8565, val_MinusLogProbMetric: 46.8565

Epoch 234: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.9959 - MinusLogProbMetric: 45.9959 - val_loss: 46.8565 - val_MinusLogProbMetric: 46.8565 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 235/1000
2023-10-27 15:41:47.296 
Epoch 235/1000 
	 loss: 45.8350, MinusLogProbMetric: 45.8350, val_loss: 47.4800, val_MinusLogProbMetric: 47.4800

Epoch 235: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.8350 - MinusLogProbMetric: 45.8350 - val_loss: 47.4800 - val_MinusLogProbMetric: 47.4800 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 236/1000
2023-10-27 15:42:18.458 
Epoch 236/1000 
	 loss: 46.3727, MinusLogProbMetric: 46.3727, val_loss: 46.7991, val_MinusLogProbMetric: 46.7991

Epoch 236: val_loss did not improve from 45.82685
196/196 - 31s - loss: 46.3727 - MinusLogProbMetric: 46.3727 - val_loss: 46.7991 - val_MinusLogProbMetric: 46.7991 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 237/1000
2023-10-27 15:42:49.085 
Epoch 237/1000 
	 loss: 45.9610, MinusLogProbMetric: 45.9610, val_loss: 45.8707, val_MinusLogProbMetric: 45.8707

Epoch 237: val_loss did not improve from 45.82685
196/196 - 31s - loss: 45.9610 - MinusLogProbMetric: 45.9610 - val_loss: 45.8707 - val_MinusLogProbMetric: 45.8707 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 238/1000
2023-10-27 15:43:21.896 
Epoch 238/1000 
	 loss: 45.9588, MinusLogProbMetric: 45.9588, val_loss: 47.2475, val_MinusLogProbMetric: 47.2475

Epoch 238: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.9588 - MinusLogProbMetric: 45.9588 - val_loss: 47.2475 - val_MinusLogProbMetric: 47.2475 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 239/1000
2023-10-27 15:43:54.691 
Epoch 239/1000 
	 loss: 46.5217, MinusLogProbMetric: 46.5217, val_loss: 48.8186, val_MinusLogProbMetric: 48.8186

Epoch 239: val_loss did not improve from 45.82685
196/196 - 33s - loss: 46.5217 - MinusLogProbMetric: 46.5217 - val_loss: 48.8186 - val_MinusLogProbMetric: 48.8186 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 240/1000
2023-10-27 15:44:27.197 
Epoch 240/1000 
	 loss: 45.9167, MinusLogProbMetric: 45.9167, val_loss: 46.7524, val_MinusLogProbMetric: 46.7524

Epoch 240: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.9167 - MinusLogProbMetric: 45.9167 - val_loss: 46.7524 - val_MinusLogProbMetric: 46.7524 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 241/1000
2023-10-27 15:45:00.523 
Epoch 241/1000 
	 loss: 46.2999, MinusLogProbMetric: 46.2999, val_loss: 48.1753, val_MinusLogProbMetric: 48.1753

Epoch 241: val_loss did not improve from 45.82685
196/196 - 33s - loss: 46.2999 - MinusLogProbMetric: 46.2999 - val_loss: 48.1753 - val_MinusLogProbMetric: 48.1753 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 242/1000
2023-10-27 15:45:34.083 
Epoch 242/1000 
	 loss: 46.1405, MinusLogProbMetric: 46.1405, val_loss: 46.0708, val_MinusLogProbMetric: 46.0708

Epoch 242: val_loss did not improve from 45.82685
196/196 - 34s - loss: 46.1405 - MinusLogProbMetric: 46.1405 - val_loss: 46.0708 - val_MinusLogProbMetric: 46.0708 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 243/1000
2023-10-27 15:46:07.022 
Epoch 243/1000 
	 loss: 46.3380, MinusLogProbMetric: 46.3380, val_loss: 48.3812, val_MinusLogProbMetric: 48.3812

Epoch 243: val_loss did not improve from 45.82685
196/196 - 33s - loss: 46.3380 - MinusLogProbMetric: 46.3380 - val_loss: 48.3812 - val_MinusLogProbMetric: 48.3812 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 244/1000
2023-10-27 15:46:39.073 
Epoch 244/1000 
	 loss: 45.8338, MinusLogProbMetric: 45.8338, val_loss: 45.9747, val_MinusLogProbMetric: 45.9747

Epoch 244: val_loss did not improve from 45.82685
196/196 - 32s - loss: 45.8338 - MinusLogProbMetric: 45.8338 - val_loss: 45.9747 - val_MinusLogProbMetric: 45.9747 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 245/1000
2023-10-27 15:47:09.905 
Epoch 245/1000 
	 loss: 45.6627, MinusLogProbMetric: 45.6627, val_loss: 46.4819, val_MinusLogProbMetric: 46.4819

Epoch 245: val_loss did not improve from 45.82685
196/196 - 31s - loss: 45.6627 - MinusLogProbMetric: 45.6627 - val_loss: 46.4819 - val_MinusLogProbMetric: 46.4819 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 246/1000
2023-10-27 15:47:43.518 
Epoch 246/1000 
	 loss: 45.7290, MinusLogProbMetric: 45.7290, val_loss: 46.0352, val_MinusLogProbMetric: 46.0352

Epoch 246: val_loss did not improve from 45.82685
196/196 - 34s - loss: 45.7290 - MinusLogProbMetric: 45.7290 - val_loss: 46.0352 - val_MinusLogProbMetric: 46.0352 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 247/1000
2023-10-27 15:48:16.529 
Epoch 247/1000 
	 loss: 45.3720, MinusLogProbMetric: 45.3720, val_loss: 46.5790, val_MinusLogProbMetric: 46.5790

Epoch 247: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.3720 - MinusLogProbMetric: 45.3720 - val_loss: 46.5790 - val_MinusLogProbMetric: 46.5790 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 248/1000
2023-10-27 15:48:48.767 
Epoch 248/1000 
	 loss: 45.9535, MinusLogProbMetric: 45.9535, val_loss: 48.0701, val_MinusLogProbMetric: 48.0701

Epoch 248: val_loss did not improve from 45.82685
196/196 - 32s - loss: 45.9535 - MinusLogProbMetric: 45.9535 - val_loss: 48.0701 - val_MinusLogProbMetric: 48.0701 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 249/1000
2023-10-27 15:49:21.759 
Epoch 249/1000 
	 loss: 45.7707, MinusLogProbMetric: 45.7707, val_loss: 46.4802, val_MinusLogProbMetric: 46.4802

Epoch 249: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.7707 - MinusLogProbMetric: 45.7707 - val_loss: 46.4802 - val_MinusLogProbMetric: 46.4802 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 250/1000
2023-10-27 15:49:54.959 
Epoch 250/1000 
	 loss: 45.9586, MinusLogProbMetric: 45.9586, val_loss: 45.8733, val_MinusLogProbMetric: 45.8733

Epoch 250: val_loss did not improve from 45.82685
196/196 - 33s - loss: 45.9586 - MinusLogProbMetric: 45.9586 - val_loss: 45.8733 - val_MinusLogProbMetric: 45.8733 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 251/1000
2023-10-27 15:50:25.845 
Epoch 251/1000 
	 loss: 46.1936, MinusLogProbMetric: 46.1936, val_loss: 46.1788, val_MinusLogProbMetric: 46.1788

Epoch 251: val_loss did not improve from 45.82685
196/196 - 31s - loss: 46.1936 - MinusLogProbMetric: 46.1936 - val_loss: 46.1788 - val_MinusLogProbMetric: 46.1788 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 252/1000
2023-10-27 15:50:59.473 
Epoch 252/1000 
	 loss: 45.8040, MinusLogProbMetric: 45.8040, val_loss: 50.7846, val_MinusLogProbMetric: 50.7846

Epoch 252: val_loss did not improve from 45.82685
196/196 - 34s - loss: 45.8040 - MinusLogProbMetric: 45.8040 - val_loss: 50.7846 - val_MinusLogProbMetric: 50.7846 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 253/1000
2023-10-27 15:51:33.086 
Epoch 253/1000 
	 loss: 45.5389, MinusLogProbMetric: 45.5389, val_loss: 45.8612, val_MinusLogProbMetric: 45.8612

Epoch 253: val_loss did not improve from 45.82685
196/196 - 34s - loss: 45.5389 - MinusLogProbMetric: 45.5389 - val_loss: 45.8612 - val_MinusLogProbMetric: 45.8612 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 254/1000
2023-10-27 15:52:05.165 
Epoch 254/1000 
	 loss: 45.4548, MinusLogProbMetric: 45.4548, val_loss: 46.8059, val_MinusLogProbMetric: 46.8059

Epoch 254: val_loss did not improve from 45.82685
196/196 - 32s - loss: 45.4548 - MinusLogProbMetric: 45.4548 - val_loss: 46.8059 - val_MinusLogProbMetric: 46.8059 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 255/1000
2023-10-27 15:52:36.026 
Epoch 255/1000 
	 loss: 45.7800, MinusLogProbMetric: 45.7800, val_loss: 45.4143, val_MinusLogProbMetric: 45.4143

Epoch 255: val_loss improved from 45.82685 to 45.41433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 31s - loss: 45.7800 - MinusLogProbMetric: 45.7800 - val_loss: 45.4143 - val_MinusLogProbMetric: 45.4143 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 256/1000
2023-10-27 15:53:06.462 
Epoch 256/1000 
	 loss: 45.5878, MinusLogProbMetric: 45.5878, val_loss: 46.9645, val_MinusLogProbMetric: 46.9645

Epoch 256: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.5878 - MinusLogProbMetric: 45.5878 - val_loss: 46.9645 - val_MinusLogProbMetric: 46.9645 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 257/1000
2023-10-27 15:53:39.091 
Epoch 257/1000 
	 loss: 45.6680, MinusLogProbMetric: 45.6680, val_loss: 47.1767, val_MinusLogProbMetric: 47.1767

Epoch 257: val_loss did not improve from 45.41433
196/196 - 33s - loss: 45.6680 - MinusLogProbMetric: 45.6680 - val_loss: 47.1767 - val_MinusLogProbMetric: 47.1767 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 258/1000
2023-10-27 15:54:12.489 
Epoch 258/1000 
	 loss: 46.0261, MinusLogProbMetric: 46.0261, val_loss: 47.1057, val_MinusLogProbMetric: 47.1057

Epoch 258: val_loss did not improve from 45.41433
196/196 - 33s - loss: 46.0261 - MinusLogProbMetric: 46.0261 - val_loss: 47.1057 - val_MinusLogProbMetric: 47.1057 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 259/1000
2023-10-27 15:54:42.295 
Epoch 259/1000 
	 loss: 45.8751, MinusLogProbMetric: 45.8751, val_loss: 46.5976, val_MinusLogProbMetric: 46.5976

Epoch 259: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.8751 - MinusLogProbMetric: 45.8751 - val_loss: 46.5976 - val_MinusLogProbMetric: 46.5976 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 260/1000
2023-10-27 15:55:12.776 
Epoch 260/1000 
	 loss: 45.5727, MinusLogProbMetric: 45.5727, val_loss: 45.7627, val_MinusLogProbMetric: 45.7627

Epoch 260: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.5727 - MinusLogProbMetric: 45.5727 - val_loss: 45.7627 - val_MinusLogProbMetric: 45.7627 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 261/1000
2023-10-27 15:55:40.215 
Epoch 261/1000 
	 loss: 45.6455, MinusLogProbMetric: 45.6455, val_loss: 45.8668, val_MinusLogProbMetric: 45.8668

Epoch 261: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.6455 - MinusLogProbMetric: 45.6455 - val_loss: 45.8668 - val_MinusLogProbMetric: 45.8668 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 262/1000
2023-10-27 15:56:07.811 
Epoch 262/1000 
	 loss: 45.8441, MinusLogProbMetric: 45.8441, val_loss: 47.0901, val_MinusLogProbMetric: 47.0901

Epoch 262: val_loss did not improve from 45.41433
196/196 - 28s - loss: 45.8441 - MinusLogProbMetric: 45.8441 - val_loss: 47.0901 - val_MinusLogProbMetric: 47.0901 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 263/1000
2023-10-27 15:56:41.018 
Epoch 263/1000 
	 loss: 45.6096, MinusLogProbMetric: 45.6096, val_loss: 48.0434, val_MinusLogProbMetric: 48.0434

Epoch 263: val_loss did not improve from 45.41433
196/196 - 33s - loss: 45.6096 - MinusLogProbMetric: 45.6096 - val_loss: 48.0434 - val_MinusLogProbMetric: 48.0434 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 264/1000
2023-10-27 15:57:13.172 
Epoch 264/1000 
	 loss: 45.6758, MinusLogProbMetric: 45.6758, val_loss: 46.0605, val_MinusLogProbMetric: 46.0605

Epoch 264: val_loss did not improve from 45.41433
196/196 - 32s - loss: 45.6758 - MinusLogProbMetric: 45.6758 - val_loss: 46.0605 - val_MinusLogProbMetric: 46.0605 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 265/1000
2023-10-27 15:57:43.685 
Epoch 265/1000 
	 loss: 45.9600, MinusLogProbMetric: 45.9600, val_loss: 45.9019, val_MinusLogProbMetric: 45.9019

Epoch 265: val_loss did not improve from 45.41433
196/196 - 31s - loss: 45.9600 - MinusLogProbMetric: 45.9600 - val_loss: 45.9019 - val_MinusLogProbMetric: 45.9019 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 266/1000
2023-10-27 15:58:11.744 
Epoch 266/1000 
	 loss: 45.9675, MinusLogProbMetric: 45.9675, val_loss: 46.2183, val_MinusLogProbMetric: 46.2183

Epoch 266: val_loss did not improve from 45.41433
196/196 - 28s - loss: 45.9675 - MinusLogProbMetric: 45.9675 - val_loss: 46.2183 - val_MinusLogProbMetric: 46.2183 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 267/1000
2023-10-27 15:58:39.592 
Epoch 267/1000 
	 loss: 45.4172, MinusLogProbMetric: 45.4172, val_loss: 46.0810, val_MinusLogProbMetric: 46.0810

Epoch 267: val_loss did not improve from 45.41433
196/196 - 28s - loss: 45.4172 - MinusLogProbMetric: 45.4172 - val_loss: 46.0810 - val_MinusLogProbMetric: 46.0810 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 268/1000
2023-10-27 15:59:12.825 
Epoch 268/1000 
	 loss: 45.7352, MinusLogProbMetric: 45.7352, val_loss: 45.5859, val_MinusLogProbMetric: 45.5859

Epoch 268: val_loss did not improve from 45.41433
196/196 - 33s - loss: 45.7352 - MinusLogProbMetric: 45.7352 - val_loss: 45.5859 - val_MinusLogProbMetric: 45.5859 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 269/1000
2023-10-27 15:59:41.506 
Epoch 269/1000 
	 loss: 45.4594, MinusLogProbMetric: 45.4594, val_loss: 46.5564, val_MinusLogProbMetric: 46.5564

Epoch 269: val_loss did not improve from 45.41433
196/196 - 29s - loss: 45.4594 - MinusLogProbMetric: 45.4594 - val_loss: 46.5564 - val_MinusLogProbMetric: 46.5564 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 270/1000
2023-10-27 16:00:08.979 
Epoch 270/1000 
	 loss: 45.4482, MinusLogProbMetric: 45.4482, val_loss: 47.1139, val_MinusLogProbMetric: 47.1139

Epoch 270: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.4482 - MinusLogProbMetric: 45.4482 - val_loss: 47.1139 - val_MinusLogProbMetric: 47.1139 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 271/1000
2023-10-27 16:00:38.091 
Epoch 271/1000 
	 loss: 45.3008, MinusLogProbMetric: 45.3008, val_loss: 48.2956, val_MinusLogProbMetric: 48.2956

Epoch 271: val_loss did not improve from 45.41433
196/196 - 29s - loss: 45.3008 - MinusLogProbMetric: 45.3008 - val_loss: 48.2956 - val_MinusLogProbMetric: 48.2956 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 272/1000
2023-10-27 16:01:08.167 
Epoch 272/1000 
	 loss: 45.8324, MinusLogProbMetric: 45.8324, val_loss: 47.7011, val_MinusLogProbMetric: 47.7011

Epoch 272: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.8324 - MinusLogProbMetric: 45.8324 - val_loss: 47.7011 - val_MinusLogProbMetric: 47.7011 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 273/1000
2023-10-27 16:01:37.262 
Epoch 273/1000 
	 loss: 45.7330, MinusLogProbMetric: 45.7330, val_loss: 46.4208, val_MinusLogProbMetric: 46.4208

Epoch 273: val_loss did not improve from 45.41433
196/196 - 29s - loss: 45.7330 - MinusLogProbMetric: 45.7330 - val_loss: 46.4208 - val_MinusLogProbMetric: 46.4208 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 274/1000
2023-10-27 16:02:07.331 
Epoch 274/1000 
	 loss: 45.4987, MinusLogProbMetric: 45.4987, val_loss: 46.0967, val_MinusLogProbMetric: 46.0967

Epoch 274: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.4987 - MinusLogProbMetric: 45.4987 - val_loss: 46.0967 - val_MinusLogProbMetric: 46.0967 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 275/1000
2023-10-27 16:02:36.220 
Epoch 275/1000 
	 loss: 45.2901, MinusLogProbMetric: 45.2901, val_loss: 46.0542, val_MinusLogProbMetric: 46.0542

Epoch 275: val_loss did not improve from 45.41433
196/196 - 29s - loss: 45.2901 - MinusLogProbMetric: 45.2901 - val_loss: 46.0542 - val_MinusLogProbMetric: 46.0542 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 276/1000
2023-10-27 16:03:07.847 
Epoch 276/1000 
	 loss: 45.6286, MinusLogProbMetric: 45.6286, val_loss: 45.4379, val_MinusLogProbMetric: 45.4379

Epoch 276: val_loss did not improve from 45.41433
196/196 - 32s - loss: 45.6286 - MinusLogProbMetric: 45.6286 - val_loss: 45.4379 - val_MinusLogProbMetric: 45.4379 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 277/1000
2023-10-27 16:03:39.062 
Epoch 277/1000 
	 loss: 45.8812, MinusLogProbMetric: 45.8812, val_loss: 45.7048, val_MinusLogProbMetric: 45.7048

Epoch 277: val_loss did not improve from 45.41433
196/196 - 31s - loss: 45.8812 - MinusLogProbMetric: 45.8812 - val_loss: 45.7048 - val_MinusLogProbMetric: 45.7048 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 278/1000
2023-10-27 16:04:08.989 
Epoch 278/1000 
	 loss: 45.4809, MinusLogProbMetric: 45.4809, val_loss: 47.5131, val_MinusLogProbMetric: 47.5131

Epoch 278: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.4809 - MinusLogProbMetric: 45.4809 - val_loss: 47.5131 - val_MinusLogProbMetric: 47.5131 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 279/1000
2023-10-27 16:04:38.703 
Epoch 279/1000 
	 loss: 45.2586, MinusLogProbMetric: 45.2586, val_loss: 45.9417, val_MinusLogProbMetric: 45.9417

Epoch 279: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.2586 - MinusLogProbMetric: 45.2586 - val_loss: 45.9417 - val_MinusLogProbMetric: 45.9417 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 280/1000
2023-10-27 16:05:06.188 
Epoch 280/1000 
	 loss: 45.8098, MinusLogProbMetric: 45.8098, val_loss: 45.7618, val_MinusLogProbMetric: 45.7618

Epoch 280: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.8098 - MinusLogProbMetric: 45.8098 - val_loss: 45.7618 - val_MinusLogProbMetric: 45.7618 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 281/1000
2023-10-27 16:05:32.955 
Epoch 281/1000 
	 loss: 45.6822, MinusLogProbMetric: 45.6822, val_loss: 45.6832, val_MinusLogProbMetric: 45.6832

Epoch 281: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.6822 - MinusLogProbMetric: 45.6822 - val_loss: 45.6832 - val_MinusLogProbMetric: 45.6832 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 282/1000
2023-10-27 16:05:59.657 
Epoch 282/1000 
	 loss: 45.5778, MinusLogProbMetric: 45.5778, val_loss: 45.7360, val_MinusLogProbMetric: 45.7360

Epoch 282: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.5778 - MinusLogProbMetric: 45.5778 - val_loss: 45.7360 - val_MinusLogProbMetric: 45.7360 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 283/1000
2023-10-27 16:06:26.560 
Epoch 283/1000 
	 loss: 45.3884, MinusLogProbMetric: 45.3884, val_loss: 45.6935, val_MinusLogProbMetric: 45.6935

Epoch 283: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.3884 - MinusLogProbMetric: 45.3884 - val_loss: 45.6935 - val_MinusLogProbMetric: 45.6935 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 284/1000
2023-10-27 16:06:54.223 
Epoch 284/1000 
	 loss: 45.5215, MinusLogProbMetric: 45.5215, val_loss: 45.7207, val_MinusLogProbMetric: 45.7207

Epoch 284: val_loss did not improve from 45.41433
196/196 - 28s - loss: 45.5215 - MinusLogProbMetric: 45.5215 - val_loss: 45.7207 - val_MinusLogProbMetric: 45.7207 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 285/1000
2023-10-27 16:07:23.844 
Epoch 285/1000 
	 loss: 45.2743, MinusLogProbMetric: 45.2743, val_loss: 46.0997, val_MinusLogProbMetric: 46.0997

Epoch 285: val_loss did not improve from 45.41433
196/196 - 30s - loss: 45.2743 - MinusLogProbMetric: 45.2743 - val_loss: 46.0997 - val_MinusLogProbMetric: 46.0997 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 286/1000
2023-10-27 16:07:52.336 
Epoch 286/1000 
	 loss: 45.3211, MinusLogProbMetric: 45.3211, val_loss: 46.3435, val_MinusLogProbMetric: 46.3435

Epoch 286: val_loss did not improve from 45.41433
196/196 - 28s - loss: 45.3211 - MinusLogProbMetric: 45.3211 - val_loss: 46.3435 - val_MinusLogProbMetric: 46.3435 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 287/1000
2023-10-27 16:08:19.230 
Epoch 287/1000 
	 loss: 45.2024, MinusLogProbMetric: 45.2024, val_loss: 45.4527, val_MinusLogProbMetric: 45.4527

Epoch 287: val_loss did not improve from 45.41433
196/196 - 27s - loss: 45.2024 - MinusLogProbMetric: 45.2024 - val_loss: 45.4527 - val_MinusLogProbMetric: 45.4527 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 288/1000
2023-10-27 16:08:46.003 
Epoch 288/1000 
	 loss: 45.1440, MinusLogProbMetric: 45.1440, val_loss: 45.2840, val_MinusLogProbMetric: 45.2840

Epoch 288: val_loss improved from 45.41433 to 45.28403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 27s - loss: 45.1440 - MinusLogProbMetric: 45.1440 - val_loss: 45.2840 - val_MinusLogProbMetric: 45.2840 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 289/1000
2023-10-27 16:09:13.442 
Epoch 289/1000 
	 loss: 45.7642, MinusLogProbMetric: 45.7642, val_loss: 46.7461, val_MinusLogProbMetric: 46.7461

Epoch 289: val_loss did not improve from 45.28403
196/196 - 27s - loss: 45.7642 - MinusLogProbMetric: 45.7642 - val_loss: 46.7461 - val_MinusLogProbMetric: 46.7461 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 290/1000
2023-10-27 16:09:41.424 
Epoch 290/1000 
	 loss: 45.6453, MinusLogProbMetric: 45.6453, val_loss: 45.3727, val_MinusLogProbMetric: 45.3727

Epoch 290: val_loss did not improve from 45.28403
196/196 - 28s - loss: 45.6453 - MinusLogProbMetric: 45.6453 - val_loss: 45.3727 - val_MinusLogProbMetric: 45.3727 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 291/1000
2023-10-27 16:10:09.591 
Epoch 291/1000 
	 loss: 45.1015, MinusLogProbMetric: 45.1015, val_loss: 45.4151, val_MinusLogProbMetric: 45.4151

Epoch 291: val_loss did not improve from 45.28403
196/196 - 28s - loss: 45.1015 - MinusLogProbMetric: 45.1015 - val_loss: 45.4151 - val_MinusLogProbMetric: 45.4151 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 292/1000
2023-10-27 16:10:40.723 
Epoch 292/1000 
	 loss: 45.3780, MinusLogProbMetric: 45.3780, val_loss: 46.1208, val_MinusLogProbMetric: 46.1208

Epoch 292: val_loss did not improve from 45.28403
196/196 - 31s - loss: 45.3780 - MinusLogProbMetric: 45.3780 - val_loss: 46.1208 - val_MinusLogProbMetric: 46.1208 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 293/1000
2023-10-27 16:11:08.751 
Epoch 293/1000 
	 loss: 45.0701, MinusLogProbMetric: 45.0701, val_loss: 47.5398, val_MinusLogProbMetric: 47.5398

Epoch 293: val_loss did not improve from 45.28403
196/196 - 28s - loss: 45.0701 - MinusLogProbMetric: 45.0701 - val_loss: 47.5398 - val_MinusLogProbMetric: 47.5398 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 294/1000
2023-10-27 16:11:35.244 
Epoch 294/1000 
	 loss: 45.3264, MinusLogProbMetric: 45.3264, val_loss: 45.7882, val_MinusLogProbMetric: 45.7882

Epoch 294: val_loss did not improve from 45.28403
196/196 - 26s - loss: 45.3264 - MinusLogProbMetric: 45.3264 - val_loss: 45.7882 - val_MinusLogProbMetric: 45.7882 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 295/1000
2023-10-27 16:12:02.038 
Epoch 295/1000 
	 loss: 45.0190, MinusLogProbMetric: 45.0190, val_loss: 46.0808, val_MinusLogProbMetric: 46.0808

Epoch 295: val_loss did not improve from 45.28403
196/196 - 27s - loss: 45.0190 - MinusLogProbMetric: 45.0190 - val_loss: 46.0808 - val_MinusLogProbMetric: 46.0808 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 296/1000
2023-10-27 16:12:29.217 
Epoch 296/1000 
	 loss: 45.1844, MinusLogProbMetric: 45.1844, val_loss: 47.1390, val_MinusLogProbMetric: 47.1390

Epoch 296: val_loss did not improve from 45.28403
196/196 - 27s - loss: 45.1844 - MinusLogProbMetric: 45.1844 - val_loss: 47.1390 - val_MinusLogProbMetric: 47.1390 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 297/1000
2023-10-27 16:12:58.551 
Epoch 297/1000 
	 loss: 45.0730, MinusLogProbMetric: 45.0730, val_loss: 45.9989, val_MinusLogProbMetric: 45.9989

Epoch 297: val_loss did not improve from 45.28403
196/196 - 29s - loss: 45.0730 - MinusLogProbMetric: 45.0730 - val_loss: 45.9989 - val_MinusLogProbMetric: 45.9989 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 298/1000
2023-10-27 16:13:31.169 
Epoch 298/1000 
	 loss: 45.1172, MinusLogProbMetric: 45.1172, val_loss: 46.5111, val_MinusLogProbMetric: 46.5111

Epoch 298: val_loss did not improve from 45.28403
196/196 - 33s - loss: 45.1172 - MinusLogProbMetric: 45.1172 - val_loss: 46.5111 - val_MinusLogProbMetric: 46.5111 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 299/1000
2023-10-27 16:13:59.781 
Epoch 299/1000 
	 loss: 45.0194, MinusLogProbMetric: 45.0194, val_loss: 46.0889, val_MinusLogProbMetric: 46.0889

Epoch 299: val_loss did not improve from 45.28403
196/196 - 29s - loss: 45.0194 - MinusLogProbMetric: 45.0194 - val_loss: 46.0889 - val_MinusLogProbMetric: 46.0889 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 300/1000
2023-10-27 16:14:26.612 
Epoch 300/1000 
	 loss: 45.0353, MinusLogProbMetric: 45.0353, val_loss: 45.2549, val_MinusLogProbMetric: 45.2549

Epoch 300: val_loss improved from 45.28403 to 45.25492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 27s - loss: 45.0353 - MinusLogProbMetric: 45.0353 - val_loss: 45.2549 - val_MinusLogProbMetric: 45.2549 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 301/1000
2023-10-27 16:14:54.359 
Epoch 301/1000 
	 loss: 45.3265, MinusLogProbMetric: 45.3265, val_loss: 45.6081, val_MinusLogProbMetric: 45.6081

Epoch 301: val_loss did not improve from 45.25492
196/196 - 27s - loss: 45.3265 - MinusLogProbMetric: 45.3265 - val_loss: 45.6081 - val_MinusLogProbMetric: 45.6081 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 302/1000
2023-10-27 16:15:21.434 
Epoch 302/1000 
	 loss: 44.8880, MinusLogProbMetric: 44.8880, val_loss: 47.5292, val_MinusLogProbMetric: 47.5292

Epoch 302: val_loss did not improve from 45.25492
196/196 - 27s - loss: 44.8880 - MinusLogProbMetric: 44.8880 - val_loss: 47.5292 - val_MinusLogProbMetric: 47.5292 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 303/1000
2023-10-27 16:15:48.771 
Epoch 303/1000 
	 loss: 45.0650, MinusLogProbMetric: 45.0650, val_loss: 45.2807, val_MinusLogProbMetric: 45.2807

Epoch 303: val_loss did not improve from 45.25492
196/196 - 27s - loss: 45.0650 - MinusLogProbMetric: 45.0650 - val_loss: 45.2807 - val_MinusLogProbMetric: 45.2807 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 304/1000
2023-10-27 16:16:16.152 
Epoch 304/1000 
	 loss: 44.9264, MinusLogProbMetric: 44.9264, val_loss: 45.0994, val_MinusLogProbMetric: 45.0994

Epoch 304: val_loss improved from 45.25492 to 45.09943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 28s - loss: 44.9264 - MinusLogProbMetric: 44.9264 - val_loss: 45.0994 - val_MinusLogProbMetric: 45.0994 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 305/1000
2023-10-27 16:16:44.166 
Epoch 305/1000 
	 loss: 45.1433, MinusLogProbMetric: 45.1433, val_loss: 45.6934, val_MinusLogProbMetric: 45.6934

Epoch 305: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.1433 - MinusLogProbMetric: 45.1433 - val_loss: 45.6934 - val_MinusLogProbMetric: 45.6934 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 306/1000
2023-10-27 16:17:13.455 
Epoch 306/1000 
	 loss: 44.8864, MinusLogProbMetric: 44.8864, val_loss: 45.6089, val_MinusLogProbMetric: 45.6089

Epoch 306: val_loss did not improve from 45.09943
196/196 - 29s - loss: 44.8864 - MinusLogProbMetric: 44.8864 - val_loss: 45.6089 - val_MinusLogProbMetric: 45.6089 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 307/1000
2023-10-27 16:17:40.330 
Epoch 307/1000 
	 loss: 45.4888, MinusLogProbMetric: 45.4888, val_loss: 45.2538, val_MinusLogProbMetric: 45.2538

Epoch 307: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.4888 - MinusLogProbMetric: 45.4888 - val_loss: 45.2538 - val_MinusLogProbMetric: 45.2538 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 308/1000
2023-10-27 16:18:07.580 
Epoch 308/1000 
	 loss: 44.9387, MinusLogProbMetric: 44.9387, val_loss: 45.2496, val_MinusLogProbMetric: 45.2496

Epoch 308: val_loss did not improve from 45.09943
196/196 - 27s - loss: 44.9387 - MinusLogProbMetric: 44.9387 - val_loss: 45.2496 - val_MinusLogProbMetric: 45.2496 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 309/1000
2023-10-27 16:18:34.461 
Epoch 309/1000 
	 loss: 44.9740, MinusLogProbMetric: 44.9740, val_loss: 45.7480, val_MinusLogProbMetric: 45.7480

Epoch 309: val_loss did not improve from 45.09943
196/196 - 27s - loss: 44.9740 - MinusLogProbMetric: 44.9740 - val_loss: 45.7480 - val_MinusLogProbMetric: 45.7480 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 310/1000
2023-10-27 16:19:02.114 
Epoch 310/1000 
	 loss: 44.7405, MinusLogProbMetric: 44.7405, val_loss: 45.1568, val_MinusLogProbMetric: 45.1568

Epoch 310: val_loss did not improve from 45.09943
196/196 - 28s - loss: 44.7405 - MinusLogProbMetric: 44.7405 - val_loss: 45.1568 - val_MinusLogProbMetric: 45.1568 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 311/1000
2023-10-27 16:19:34.732 
Epoch 311/1000 
	 loss: 45.1095, MinusLogProbMetric: 45.1095, val_loss: 45.9957, val_MinusLogProbMetric: 45.9957

Epoch 311: val_loss did not improve from 45.09943
196/196 - 33s - loss: 45.1095 - MinusLogProbMetric: 45.1095 - val_loss: 45.9957 - val_MinusLogProbMetric: 45.9957 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 312/1000
2023-10-27 16:20:03.104 
Epoch 312/1000 
	 loss: 44.8742, MinusLogProbMetric: 44.8742, val_loss: 46.2869, val_MinusLogProbMetric: 46.2869

Epoch 312: val_loss did not improve from 45.09943
196/196 - 28s - loss: 44.8742 - MinusLogProbMetric: 44.8742 - val_loss: 46.2869 - val_MinusLogProbMetric: 46.2869 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 313/1000
2023-10-27 16:20:29.803 
Epoch 313/1000 
	 loss: 44.9584, MinusLogProbMetric: 44.9584, val_loss: 46.1898, val_MinusLogProbMetric: 46.1898

Epoch 313: val_loss did not improve from 45.09943
196/196 - 27s - loss: 44.9584 - MinusLogProbMetric: 44.9584 - val_loss: 46.1898 - val_MinusLogProbMetric: 46.1898 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 314/1000
2023-10-27 16:20:56.008 
Epoch 314/1000 
	 loss: 44.9892, MinusLogProbMetric: 44.9892, val_loss: 45.1635, val_MinusLogProbMetric: 45.1635

Epoch 314: val_loss did not improve from 45.09943
196/196 - 26s - loss: 44.9892 - MinusLogProbMetric: 44.9892 - val_loss: 45.1635 - val_MinusLogProbMetric: 45.1635 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 315/1000
2023-10-27 16:21:23.140 
Epoch 315/1000 
	 loss: 44.8451, MinusLogProbMetric: 44.8451, val_loss: 45.8174, val_MinusLogProbMetric: 45.8174

Epoch 315: val_loss did not improve from 45.09943
196/196 - 27s - loss: 44.8451 - MinusLogProbMetric: 44.8451 - val_loss: 45.8174 - val_MinusLogProbMetric: 45.8174 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 316/1000
2023-10-27 16:21:50.016 
Epoch 316/1000 
	 loss: 45.0053, MinusLogProbMetric: 45.0053, val_loss: 47.6380, val_MinusLogProbMetric: 47.6380

Epoch 316: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.0053 - MinusLogProbMetric: 45.0053 - val_loss: 47.6380 - val_MinusLogProbMetric: 47.6380 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 317/1000
2023-10-27 16:22:19.153 
Epoch 317/1000 
	 loss: 45.2139, MinusLogProbMetric: 45.2139, val_loss: 46.9922, val_MinusLogProbMetric: 46.9922

Epoch 317: val_loss did not improve from 45.09943
196/196 - 29s - loss: 45.2139 - MinusLogProbMetric: 45.2139 - val_loss: 46.9922 - val_MinusLogProbMetric: 46.9922 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 318/1000
2023-10-27 16:22:49.399 
Epoch 318/1000 
	 loss: 45.1222, MinusLogProbMetric: 45.1222, val_loss: 45.2799, val_MinusLogProbMetric: 45.2799

Epoch 318: val_loss did not improve from 45.09943
196/196 - 30s - loss: 45.1222 - MinusLogProbMetric: 45.1222 - val_loss: 45.2799 - val_MinusLogProbMetric: 45.2799 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 319/1000
2023-10-27 16:23:17.868 
Epoch 319/1000 
	 loss: 46.0555, MinusLogProbMetric: 46.0555, val_loss: 50.6750, val_MinusLogProbMetric: 50.6750

Epoch 319: val_loss did not improve from 45.09943
196/196 - 28s - loss: 46.0555 - MinusLogProbMetric: 46.0555 - val_loss: 50.6750 - val_MinusLogProbMetric: 50.6750 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 320/1000
2023-10-27 16:23:44.708 
Epoch 320/1000 
	 loss: 46.0182, MinusLogProbMetric: 46.0182, val_loss: 45.4156, val_MinusLogProbMetric: 45.4156

Epoch 320: val_loss did not improve from 45.09943
196/196 - 27s - loss: 46.0182 - MinusLogProbMetric: 46.0182 - val_loss: 45.4156 - val_MinusLogProbMetric: 45.4156 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 321/1000
2023-10-27 16:24:11.615 
Epoch 321/1000 
	 loss: 45.6687, MinusLogProbMetric: 45.6687, val_loss: 45.7473, val_MinusLogProbMetric: 45.7473

Epoch 321: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.6687 - MinusLogProbMetric: 45.6687 - val_loss: 45.7473 - val_MinusLogProbMetric: 45.7473 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 322/1000
2023-10-27 16:24:38.509 
Epoch 322/1000 
	 loss: 45.0859, MinusLogProbMetric: 45.0859, val_loss: 45.9534, val_MinusLogProbMetric: 45.9534

Epoch 322: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.0859 - MinusLogProbMetric: 45.0859 - val_loss: 45.9534 - val_MinusLogProbMetric: 45.9534 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 323/1000
2023-10-27 16:25:05.497 
Epoch 323/1000 
	 loss: 45.3197, MinusLogProbMetric: 45.3197, val_loss: 45.9380, val_MinusLogProbMetric: 45.9380

Epoch 323: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.3197 - MinusLogProbMetric: 45.3197 - val_loss: 45.9380 - val_MinusLogProbMetric: 45.9380 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 324/1000
2023-10-27 16:25:34.695 
Epoch 324/1000 
	 loss: 44.9884, MinusLogProbMetric: 44.9884, val_loss: 45.2809, val_MinusLogProbMetric: 45.2809

Epoch 324: val_loss did not improve from 45.09943
196/196 - 29s - loss: 44.9884 - MinusLogProbMetric: 44.9884 - val_loss: 45.2809 - val_MinusLogProbMetric: 45.2809 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 325/1000
2023-10-27 16:26:04.138 
Epoch 325/1000 
	 loss: 45.1015, MinusLogProbMetric: 45.1015, val_loss: 45.5090, val_MinusLogProbMetric: 45.5090

Epoch 325: val_loss did not improve from 45.09943
196/196 - 29s - loss: 45.1015 - MinusLogProbMetric: 45.1015 - val_loss: 45.5090 - val_MinusLogProbMetric: 45.5090 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 326/1000
2023-10-27 16:26:31.298 
Epoch 326/1000 
	 loss: 45.2288, MinusLogProbMetric: 45.2288, val_loss: 45.5867, val_MinusLogProbMetric: 45.5867

Epoch 326: val_loss did not improve from 45.09943
196/196 - 27s - loss: 45.2288 - MinusLogProbMetric: 45.2288 - val_loss: 45.5867 - val_MinusLogProbMetric: 45.5867 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 327/1000
2023-10-27 16:26:58.371 
Epoch 327/1000 
	 loss: 44.7678, MinusLogProbMetric: 44.7678, val_loss: 44.9840, val_MinusLogProbMetric: 44.9840

Epoch 327: val_loss improved from 45.09943 to 44.98398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 28s - loss: 44.7678 - MinusLogProbMetric: 44.7678 - val_loss: 44.9840 - val_MinusLogProbMetric: 44.9840 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 328/1000
2023-10-27 16:27:25.494 
Epoch 328/1000 
	 loss: 45.0626, MinusLogProbMetric: 45.0626, val_loss: 44.7706, val_MinusLogProbMetric: 44.7706

Epoch 328: val_loss improved from 44.98398 to 44.77063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 27s - loss: 45.0626 - MinusLogProbMetric: 45.0626 - val_loss: 44.7706 - val_MinusLogProbMetric: 44.7706 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 329/1000
2023-10-27 16:27:52.649 
Epoch 329/1000 
	 loss: 44.9702, MinusLogProbMetric: 44.9702, val_loss: 47.1976, val_MinusLogProbMetric: 47.1976

Epoch 329: val_loss did not improve from 44.77063
196/196 - 27s - loss: 44.9702 - MinusLogProbMetric: 44.9702 - val_loss: 47.1976 - val_MinusLogProbMetric: 47.1976 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 330/1000
2023-10-27 16:28:19.237 
Epoch 330/1000 
	 loss: 44.8848, MinusLogProbMetric: 44.8848, val_loss: 45.2681, val_MinusLogProbMetric: 45.2681

Epoch 330: val_loss did not improve from 44.77063
196/196 - 27s - loss: 44.8848 - MinusLogProbMetric: 44.8848 - val_loss: 45.2681 - val_MinusLogProbMetric: 45.2681 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 331/1000
2023-10-27 16:28:45.472 
Epoch 331/1000 
	 loss: 44.7904, MinusLogProbMetric: 44.7904, val_loss: 45.2229, val_MinusLogProbMetric: 45.2229

Epoch 331: val_loss did not improve from 44.77063
196/196 - 26s - loss: 44.7904 - MinusLogProbMetric: 44.7904 - val_loss: 45.2229 - val_MinusLogProbMetric: 45.2229 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 332/1000
2023-10-27 16:29:11.875 
Epoch 332/1000 
	 loss: 45.0721, MinusLogProbMetric: 45.0721, val_loss: 46.2350, val_MinusLogProbMetric: 46.2350

Epoch 332: val_loss did not improve from 44.77063
196/196 - 26s - loss: 45.0721 - MinusLogProbMetric: 45.0721 - val_loss: 46.2350 - val_MinusLogProbMetric: 46.2350 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 333/1000
2023-10-27 16:29:39.682 
Epoch 333/1000 
	 loss: 45.0507, MinusLogProbMetric: 45.0507, val_loss: 46.3504, val_MinusLogProbMetric: 46.3504

Epoch 333: val_loss did not improve from 44.77063
196/196 - 28s - loss: 45.0507 - MinusLogProbMetric: 45.0507 - val_loss: 46.3504 - val_MinusLogProbMetric: 46.3504 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 334/1000
2023-10-27 16:30:06.199 
Epoch 334/1000 
	 loss: 45.0414, MinusLogProbMetric: 45.0414, val_loss: 46.0265, val_MinusLogProbMetric: 46.0265

Epoch 334: val_loss did not improve from 44.77063
196/196 - 27s - loss: 45.0414 - MinusLogProbMetric: 45.0414 - val_loss: 46.0265 - val_MinusLogProbMetric: 46.0265 - lr: 3.3333e-04 - 27s/epoch - 135ms/step
Epoch 335/1000
2023-10-27 16:30:33.375 
Epoch 335/1000 
	 loss: 45.1042, MinusLogProbMetric: 45.1042, val_loss: 45.8180, val_MinusLogProbMetric: 45.8180

Epoch 335: val_loss did not improve from 44.77063
196/196 - 27s - loss: 45.1042 - MinusLogProbMetric: 45.1042 - val_loss: 45.8180 - val_MinusLogProbMetric: 45.8180 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 336/1000
2023-10-27 16:31:01.701 
Epoch 336/1000 
	 loss: 44.8653, MinusLogProbMetric: 44.8653, val_loss: 45.3787, val_MinusLogProbMetric: 45.3787

Epoch 336: val_loss did not improve from 44.77063
196/196 - 28s - loss: 44.8653 - MinusLogProbMetric: 44.8653 - val_loss: 45.3787 - val_MinusLogProbMetric: 45.3787 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 337/1000
2023-10-27 16:31:35.171 
Epoch 337/1000 
	 loss: 44.9356, MinusLogProbMetric: 44.9356, val_loss: 46.2497, val_MinusLogProbMetric: 46.2497

Epoch 337: val_loss did not improve from 44.77063
196/196 - 33s - loss: 44.9356 - MinusLogProbMetric: 44.9356 - val_loss: 46.2497 - val_MinusLogProbMetric: 46.2497 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 338/1000
2023-10-27 16:32:03.176 
Epoch 338/1000 
	 loss: 45.0711, MinusLogProbMetric: 45.0711, val_loss: 45.4632, val_MinusLogProbMetric: 45.4632

Epoch 338: val_loss did not improve from 44.77063
196/196 - 28s - loss: 45.0711 - MinusLogProbMetric: 45.0711 - val_loss: 45.4632 - val_MinusLogProbMetric: 45.4632 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 339/1000
2023-10-27 16:32:29.885 
Epoch 339/1000 
	 loss: 45.0096, MinusLogProbMetric: 45.0096, val_loss: 45.0470, val_MinusLogProbMetric: 45.0470

Epoch 339: val_loss did not improve from 44.77063
196/196 - 27s - loss: 45.0096 - MinusLogProbMetric: 45.0096 - val_loss: 45.0470 - val_MinusLogProbMetric: 45.0470 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 340/1000
2023-10-27 16:32:56.065 
Epoch 340/1000 
	 loss: 44.7560, MinusLogProbMetric: 44.7560, val_loss: 45.0578, val_MinusLogProbMetric: 45.0578

Epoch 340: val_loss did not improve from 44.77063
196/196 - 26s - loss: 44.7560 - MinusLogProbMetric: 44.7560 - val_loss: 45.0578 - val_MinusLogProbMetric: 45.0578 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 341/1000
2023-10-27 16:33:22.900 
Epoch 341/1000 
	 loss: 44.8173, MinusLogProbMetric: 44.8173, val_loss: 44.6545, val_MinusLogProbMetric: 44.6545

Epoch 341: val_loss improved from 44.77063 to 44.65447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 27s - loss: 44.8173 - MinusLogProbMetric: 44.8173 - val_loss: 44.6545 - val_MinusLogProbMetric: 44.6545 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 342/1000
2023-10-27 16:33:50.496 
Epoch 342/1000 
	 loss: 44.8220, MinusLogProbMetric: 44.8220, val_loss: 46.4671, val_MinusLogProbMetric: 46.4671

Epoch 342: val_loss did not improve from 44.65447
196/196 - 27s - loss: 44.8220 - MinusLogProbMetric: 44.8220 - val_loss: 46.4671 - val_MinusLogProbMetric: 46.4671 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 343/1000
2023-10-27 16:34:18.739 
Epoch 343/1000 
	 loss: 44.9355, MinusLogProbMetric: 44.9355, val_loss: 45.0931, val_MinusLogProbMetric: 45.0931

Epoch 343: val_loss did not improve from 44.65447
196/196 - 28s - loss: 44.9355 - MinusLogProbMetric: 44.9355 - val_loss: 45.0931 - val_MinusLogProbMetric: 45.0931 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 344/1000
2023-10-27 16:34:46.748 
Epoch 344/1000 
	 loss: 44.9687, MinusLogProbMetric: 44.9687, val_loss: 45.6358, val_MinusLogProbMetric: 45.6358

Epoch 344: val_loss did not improve from 44.65447
196/196 - 28s - loss: 44.9687 - MinusLogProbMetric: 44.9687 - val_loss: 45.6358 - val_MinusLogProbMetric: 45.6358 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 345/1000
2023-10-27 16:35:18.492 
Epoch 345/1000 
	 loss: 44.8376, MinusLogProbMetric: 44.8376, val_loss: 45.8384, val_MinusLogProbMetric: 45.8384

Epoch 345: val_loss did not improve from 44.65447
196/196 - 32s - loss: 44.8376 - MinusLogProbMetric: 44.8376 - val_loss: 45.8384 - val_MinusLogProbMetric: 45.8384 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 346/1000
2023-10-27 16:35:51.405 
Epoch 346/1000 
	 loss: 44.6572, MinusLogProbMetric: 44.6572, val_loss: 44.8259, val_MinusLogProbMetric: 44.8259

Epoch 346: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.6572 - MinusLogProbMetric: 44.6572 - val_loss: 44.8259 - val_MinusLogProbMetric: 44.8259 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 347/1000
2023-10-27 16:36:24.189 
Epoch 347/1000 
	 loss: 44.7633, MinusLogProbMetric: 44.7633, val_loss: 45.8195, val_MinusLogProbMetric: 45.8195

Epoch 347: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.7633 - MinusLogProbMetric: 44.7633 - val_loss: 45.8195 - val_MinusLogProbMetric: 45.8195 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 348/1000
2023-10-27 16:36:56.826 
Epoch 348/1000 
	 loss: 44.6435, MinusLogProbMetric: 44.6435, val_loss: 46.6953, val_MinusLogProbMetric: 46.6953

Epoch 348: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.6435 - MinusLogProbMetric: 44.6435 - val_loss: 46.6953 - val_MinusLogProbMetric: 46.6953 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 349/1000
2023-10-27 16:37:28.782 
Epoch 349/1000 
	 loss: 45.0300, MinusLogProbMetric: 45.0300, val_loss: 46.8204, val_MinusLogProbMetric: 46.8204

Epoch 349: val_loss did not improve from 44.65447
196/196 - 32s - loss: 45.0300 - MinusLogProbMetric: 45.0300 - val_loss: 46.8204 - val_MinusLogProbMetric: 46.8204 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 350/1000
2023-10-27 16:38:01.840 
Epoch 350/1000 
	 loss: 44.7986, MinusLogProbMetric: 44.7986, val_loss: 45.8979, val_MinusLogProbMetric: 45.8979

Epoch 350: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.7986 - MinusLogProbMetric: 44.7986 - val_loss: 45.8979 - val_MinusLogProbMetric: 45.8979 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 351/1000
2023-10-27 16:38:34.451 
Epoch 351/1000 
	 loss: 44.5680, MinusLogProbMetric: 44.5680, val_loss: 44.7751, val_MinusLogProbMetric: 44.7751

Epoch 351: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.5680 - MinusLogProbMetric: 44.5680 - val_loss: 44.7751 - val_MinusLogProbMetric: 44.7751 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 352/1000
2023-10-27 16:39:07.279 
Epoch 352/1000 
	 loss: 44.9065, MinusLogProbMetric: 44.9065, val_loss: 45.1808, val_MinusLogProbMetric: 45.1808

Epoch 352: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.9065 - MinusLogProbMetric: 44.9065 - val_loss: 45.1808 - val_MinusLogProbMetric: 45.1808 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 353/1000
2023-10-27 16:39:40.102 
Epoch 353/1000 
	 loss: 44.7458, MinusLogProbMetric: 44.7458, val_loss: 45.8565, val_MinusLogProbMetric: 45.8565

Epoch 353: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.7458 - MinusLogProbMetric: 44.7458 - val_loss: 45.8565 - val_MinusLogProbMetric: 45.8565 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 354/1000
2023-10-27 16:40:12.520 
Epoch 354/1000 
	 loss: 44.7018, MinusLogProbMetric: 44.7018, val_loss: 46.7229, val_MinusLogProbMetric: 46.7229

Epoch 354: val_loss did not improve from 44.65447
196/196 - 32s - loss: 44.7018 - MinusLogProbMetric: 44.7018 - val_loss: 46.7229 - val_MinusLogProbMetric: 46.7229 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 355/1000
2023-10-27 16:40:45.364 
Epoch 355/1000 
	 loss: 44.9593, MinusLogProbMetric: 44.9593, val_loss: 44.9297, val_MinusLogProbMetric: 44.9297

Epoch 355: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.9593 - MinusLogProbMetric: 44.9593 - val_loss: 44.9297 - val_MinusLogProbMetric: 44.9297 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 356/1000
2023-10-27 16:41:17.888 
Epoch 356/1000 
	 loss: 44.6141, MinusLogProbMetric: 44.6141, val_loss: 47.2008, val_MinusLogProbMetric: 47.2008

Epoch 356: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.6141 - MinusLogProbMetric: 44.6141 - val_loss: 47.2008 - val_MinusLogProbMetric: 47.2008 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 357/1000
2023-10-27 16:41:50.491 
Epoch 357/1000 
	 loss: 44.9294, MinusLogProbMetric: 44.9294, val_loss: 45.1395, val_MinusLogProbMetric: 45.1395

Epoch 357: val_loss did not improve from 44.65447
196/196 - 33s - loss: 44.9294 - MinusLogProbMetric: 44.9294 - val_loss: 45.1395 - val_MinusLogProbMetric: 45.1395 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 358/1000
2023-10-27 16:42:17.663 
Epoch 358/1000 
	 loss: 44.5172, MinusLogProbMetric: 44.5172, val_loss: 44.5770, val_MinusLogProbMetric: 44.5770

Epoch 358: val_loss improved from 44.65447 to 44.57701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 28s - loss: 44.5172 - MinusLogProbMetric: 44.5172 - val_loss: 44.5770 - val_MinusLogProbMetric: 44.5770 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 359/1000
2023-10-27 16:42:44.779 
Epoch 359/1000 
	 loss: 44.4925, MinusLogProbMetric: 44.4925, val_loss: 45.2493, val_MinusLogProbMetric: 45.2493

Epoch 359: val_loss did not improve from 44.57701
196/196 - 27s - loss: 44.4925 - MinusLogProbMetric: 44.4925 - val_loss: 45.2493 - val_MinusLogProbMetric: 45.2493 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 360/1000
2023-10-27 16:43:11.622 
Epoch 360/1000 
	 loss: 44.9412, MinusLogProbMetric: 44.9412, val_loss: 46.0766, val_MinusLogProbMetric: 46.0766

Epoch 360: val_loss did not improve from 44.57701
196/196 - 27s - loss: 44.9412 - MinusLogProbMetric: 44.9412 - val_loss: 46.0766 - val_MinusLogProbMetric: 46.0766 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 361/1000
2023-10-27 16:43:38.204 
Epoch 361/1000 
	 loss: 44.6349, MinusLogProbMetric: 44.6349, val_loss: 44.7293, val_MinusLogProbMetric: 44.7293

Epoch 361: val_loss did not improve from 44.57701
196/196 - 27s - loss: 44.6349 - MinusLogProbMetric: 44.6349 - val_loss: 44.7293 - val_MinusLogProbMetric: 44.7293 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 362/1000
2023-10-27 16:44:05.308 
Epoch 362/1000 
	 loss: 44.5203, MinusLogProbMetric: 44.5203, val_loss: 46.1568, val_MinusLogProbMetric: 46.1568

Epoch 362: val_loss did not improve from 44.57701
196/196 - 27s - loss: 44.5203 - MinusLogProbMetric: 44.5203 - val_loss: 46.1568 - val_MinusLogProbMetric: 46.1568 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 363/1000
2023-10-27 16:44:37.700 
Epoch 363/1000 
	 loss: 44.6374, MinusLogProbMetric: 44.6374, val_loss: 45.0019, val_MinusLogProbMetric: 45.0019

Epoch 363: val_loss did not improve from 44.57701
196/196 - 32s - loss: 44.6374 - MinusLogProbMetric: 44.6374 - val_loss: 45.0019 - val_MinusLogProbMetric: 45.0019 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 364/1000
2023-10-27 16:45:09.681 
Epoch 364/1000 
	 loss: 44.7263, MinusLogProbMetric: 44.7263, val_loss: 45.3940, val_MinusLogProbMetric: 45.3940

Epoch 364: val_loss did not improve from 44.57701
196/196 - 32s - loss: 44.7263 - MinusLogProbMetric: 44.7263 - val_loss: 45.3940 - val_MinusLogProbMetric: 45.3940 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 365/1000
2023-10-27 16:45:42.005 
Epoch 365/1000 
	 loss: 44.6893, MinusLogProbMetric: 44.6893, val_loss: 44.2906, val_MinusLogProbMetric: 44.2906

Epoch 365: val_loss improved from 44.57701 to 44.29056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 44.6893 - MinusLogProbMetric: 44.6893 - val_loss: 44.2906 - val_MinusLogProbMetric: 44.2906 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 366/1000
2023-10-27 16:46:14.980 
Epoch 366/1000 
	 loss: 44.4617, MinusLogProbMetric: 44.4617, val_loss: 44.4582, val_MinusLogProbMetric: 44.4582

Epoch 366: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.4617 - MinusLogProbMetric: 44.4617 - val_loss: 44.4582 - val_MinusLogProbMetric: 44.4582 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 367/1000
2023-10-27 16:46:47.676 
Epoch 367/1000 
	 loss: 44.5274, MinusLogProbMetric: 44.5274, val_loss: 46.4396, val_MinusLogProbMetric: 46.4396

Epoch 367: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.5274 - MinusLogProbMetric: 44.5274 - val_loss: 46.4396 - val_MinusLogProbMetric: 46.4396 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 368/1000
2023-10-27 16:47:19.943 
Epoch 368/1000 
	 loss: 44.4179, MinusLogProbMetric: 44.4179, val_loss: 47.3238, val_MinusLogProbMetric: 47.3238

Epoch 368: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.4179 - MinusLogProbMetric: 44.4179 - val_loss: 47.3238 - val_MinusLogProbMetric: 47.3238 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 369/1000
2023-10-27 16:47:52.144 
Epoch 369/1000 
	 loss: 44.4537, MinusLogProbMetric: 44.4537, val_loss: 44.9492, val_MinusLogProbMetric: 44.9492

Epoch 369: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.4537 - MinusLogProbMetric: 44.4537 - val_loss: 44.9492 - val_MinusLogProbMetric: 44.9492 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 370/1000
2023-10-27 16:48:24.883 
Epoch 370/1000 
	 loss: 44.3503, MinusLogProbMetric: 44.3503, val_loss: 44.7719, val_MinusLogProbMetric: 44.7719

Epoch 370: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.3503 - MinusLogProbMetric: 44.3503 - val_loss: 44.7719 - val_MinusLogProbMetric: 44.7719 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 371/1000
2023-10-27 16:48:56.867 
Epoch 371/1000 
	 loss: 44.3099, MinusLogProbMetric: 44.3099, val_loss: 44.3215, val_MinusLogProbMetric: 44.3215

Epoch 371: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.3099 - MinusLogProbMetric: 44.3099 - val_loss: 44.3215 - val_MinusLogProbMetric: 44.3215 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 372/1000
2023-10-27 16:49:29.407 
Epoch 372/1000 
	 loss: 44.4797, MinusLogProbMetric: 44.4797, val_loss: 44.9104, val_MinusLogProbMetric: 44.9104

Epoch 372: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.4797 - MinusLogProbMetric: 44.4797 - val_loss: 44.9104 - val_MinusLogProbMetric: 44.9104 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 373/1000
2023-10-27 16:50:02.228 
Epoch 373/1000 
	 loss: 44.4452, MinusLogProbMetric: 44.4452, val_loss: 44.8260, val_MinusLogProbMetric: 44.8260

Epoch 373: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.4452 - MinusLogProbMetric: 44.4452 - val_loss: 44.8260 - val_MinusLogProbMetric: 44.8260 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 374/1000
2023-10-27 16:50:34.551 
Epoch 374/1000 
	 loss: 44.5093, MinusLogProbMetric: 44.5093, val_loss: 46.4096, val_MinusLogProbMetric: 46.4096

Epoch 374: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.5093 - MinusLogProbMetric: 44.5093 - val_loss: 46.4096 - val_MinusLogProbMetric: 46.4096 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 375/1000
2023-10-27 16:51:07.111 
Epoch 375/1000 
	 loss: 44.5760, MinusLogProbMetric: 44.5760, val_loss: 45.0981, val_MinusLogProbMetric: 45.0981

Epoch 375: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.5760 - MinusLogProbMetric: 44.5760 - val_loss: 45.0981 - val_MinusLogProbMetric: 45.0981 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 376/1000
2023-10-27 16:51:39.789 
Epoch 376/1000 
	 loss: 44.2727, MinusLogProbMetric: 44.2727, val_loss: 45.5519, val_MinusLogProbMetric: 45.5519

Epoch 376: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.2727 - MinusLogProbMetric: 44.2727 - val_loss: 45.5519 - val_MinusLogProbMetric: 45.5519 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 377/1000
2023-10-27 16:52:11.366 
Epoch 377/1000 
	 loss: 44.5264, MinusLogProbMetric: 44.5264, val_loss: 45.0155, val_MinusLogProbMetric: 45.0155

Epoch 377: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.5264 - MinusLogProbMetric: 44.5264 - val_loss: 45.0155 - val_MinusLogProbMetric: 45.0155 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 378/1000
2023-10-27 16:52:43.712 
Epoch 378/1000 
	 loss: 44.2061, MinusLogProbMetric: 44.2061, val_loss: 45.0110, val_MinusLogProbMetric: 45.0110

Epoch 378: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.2061 - MinusLogProbMetric: 44.2061 - val_loss: 45.0110 - val_MinusLogProbMetric: 45.0110 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 379/1000
2023-10-27 16:53:15.637 
Epoch 379/1000 
	 loss: 44.2424, MinusLogProbMetric: 44.2424, val_loss: 45.1624, val_MinusLogProbMetric: 45.1624

Epoch 379: val_loss did not improve from 44.29056
196/196 - 32s - loss: 44.2424 - MinusLogProbMetric: 44.2424 - val_loss: 45.1624 - val_MinusLogProbMetric: 45.1624 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 380/1000
2023-10-27 16:53:48.405 
Epoch 380/1000 
	 loss: 44.3219, MinusLogProbMetric: 44.3219, val_loss: 45.3482, val_MinusLogProbMetric: 45.3482

Epoch 380: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.3219 - MinusLogProbMetric: 44.3219 - val_loss: 45.3482 - val_MinusLogProbMetric: 45.3482 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 381/1000
2023-10-27 16:54:21.022 
Epoch 381/1000 
	 loss: 44.5686, MinusLogProbMetric: 44.5686, val_loss: 45.6742, val_MinusLogProbMetric: 45.6742

Epoch 381: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.5686 - MinusLogProbMetric: 44.5686 - val_loss: 45.6742 - val_MinusLogProbMetric: 45.6742 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 382/1000
2023-10-27 16:54:53.659 
Epoch 382/1000 
	 loss: 44.2282, MinusLogProbMetric: 44.2282, val_loss: 45.3700, val_MinusLogProbMetric: 45.3700

Epoch 382: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.2282 - MinusLogProbMetric: 44.2282 - val_loss: 45.3700 - val_MinusLogProbMetric: 45.3700 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 383/1000
2023-10-27 16:55:26.517 
Epoch 383/1000 
	 loss: 44.1601, MinusLogProbMetric: 44.1601, val_loss: 44.5468, val_MinusLogProbMetric: 44.5468

Epoch 383: val_loss did not improve from 44.29056
196/196 - 33s - loss: 44.1601 - MinusLogProbMetric: 44.1601 - val_loss: 44.5468 - val_MinusLogProbMetric: 44.5468 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 384/1000
2023-10-27 16:55:59.421 
Epoch 384/1000 
	 loss: 44.2827, MinusLogProbMetric: 44.2827, val_loss: 44.2855, val_MinusLogProbMetric: 44.2855

Epoch 384: val_loss improved from 44.29056 to 44.28548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 44.2827 - MinusLogProbMetric: 44.2827 - val_loss: 44.2855 - val_MinusLogProbMetric: 44.2855 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 385/1000
2023-10-27 16:56:32.821 
Epoch 385/1000 
	 loss: 44.3236, MinusLogProbMetric: 44.3236, val_loss: 45.3358, val_MinusLogProbMetric: 45.3358

Epoch 385: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.3236 - MinusLogProbMetric: 44.3236 - val_loss: 45.3358 - val_MinusLogProbMetric: 45.3358 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 386/1000
2023-10-27 16:57:05.960 
Epoch 386/1000 
	 loss: 44.1247, MinusLogProbMetric: 44.1247, val_loss: 46.1097, val_MinusLogProbMetric: 46.1097

Epoch 386: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.1247 - MinusLogProbMetric: 44.1247 - val_loss: 46.1097 - val_MinusLogProbMetric: 46.1097 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 387/1000
2023-10-27 16:57:38.436 
Epoch 387/1000 
	 loss: 44.3107, MinusLogProbMetric: 44.3107, val_loss: 45.4648, val_MinusLogProbMetric: 45.4648

Epoch 387: val_loss did not improve from 44.28548
196/196 - 32s - loss: 44.3107 - MinusLogProbMetric: 44.3107 - val_loss: 45.4648 - val_MinusLogProbMetric: 45.4648 - lr: 3.3333e-04 - 32s/epoch - 166ms/step
Epoch 388/1000
2023-10-27 16:58:11.288 
Epoch 388/1000 
	 loss: 44.1581, MinusLogProbMetric: 44.1581, val_loss: 45.2247, val_MinusLogProbMetric: 45.2247

Epoch 388: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.1581 - MinusLogProbMetric: 44.1581 - val_loss: 45.2247 - val_MinusLogProbMetric: 45.2247 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 389/1000
2023-10-27 16:58:43.952 
Epoch 389/1000 
	 loss: 44.2903, MinusLogProbMetric: 44.2903, val_loss: 45.1606, val_MinusLogProbMetric: 45.1606

Epoch 389: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.2903 - MinusLogProbMetric: 44.2903 - val_loss: 45.1606 - val_MinusLogProbMetric: 45.1606 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 390/1000
2023-10-27 16:59:16.780 
Epoch 390/1000 
	 loss: 44.2246, MinusLogProbMetric: 44.2246, val_loss: 44.6575, val_MinusLogProbMetric: 44.6575

Epoch 390: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.2246 - MinusLogProbMetric: 44.2246 - val_loss: 44.6575 - val_MinusLogProbMetric: 44.6575 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 391/1000
2023-10-27 16:59:49.103 
Epoch 391/1000 
	 loss: 44.4041, MinusLogProbMetric: 44.4041, val_loss: 44.3595, val_MinusLogProbMetric: 44.3595

Epoch 391: val_loss did not improve from 44.28548
196/196 - 32s - loss: 44.4041 - MinusLogProbMetric: 44.4041 - val_loss: 44.3595 - val_MinusLogProbMetric: 44.3595 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 392/1000
2023-10-27 17:00:20.717 
Epoch 392/1000 
	 loss: 44.2600, MinusLogProbMetric: 44.2600, val_loss: 45.1338, val_MinusLogProbMetric: 45.1338

Epoch 392: val_loss did not improve from 44.28548
196/196 - 32s - loss: 44.2600 - MinusLogProbMetric: 44.2600 - val_loss: 45.1338 - val_MinusLogProbMetric: 45.1338 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 393/1000
2023-10-27 17:00:53.323 
Epoch 393/1000 
	 loss: 44.1530, MinusLogProbMetric: 44.1530, val_loss: 44.8995, val_MinusLogProbMetric: 44.8995

Epoch 393: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.1530 - MinusLogProbMetric: 44.1530 - val_loss: 44.8995 - val_MinusLogProbMetric: 44.8995 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 394/1000
2023-10-27 17:01:26.073 
Epoch 394/1000 
	 loss: 44.2197, MinusLogProbMetric: 44.2197, val_loss: 44.5666, val_MinusLogProbMetric: 44.5666

Epoch 394: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.2197 - MinusLogProbMetric: 44.2197 - val_loss: 44.5666 - val_MinusLogProbMetric: 44.5666 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 395/1000
2023-10-27 17:01:58.898 
Epoch 395/1000 
	 loss: 44.2000, MinusLogProbMetric: 44.2000, val_loss: 45.4846, val_MinusLogProbMetric: 45.4846

Epoch 395: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.2000 - MinusLogProbMetric: 44.2000 - val_loss: 45.4846 - val_MinusLogProbMetric: 45.4846 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 396/1000
2023-10-27 17:02:31.976 
Epoch 396/1000 
	 loss: 44.2861, MinusLogProbMetric: 44.2861, val_loss: 44.7706, val_MinusLogProbMetric: 44.7706

Epoch 396: val_loss did not improve from 44.28548
196/196 - 33s - loss: 44.2861 - MinusLogProbMetric: 44.2861 - val_loss: 44.7706 - val_MinusLogProbMetric: 44.7706 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 397/1000
2023-10-27 17:03:04.385 
Epoch 397/1000 
	 loss: 44.0321, MinusLogProbMetric: 44.0321, val_loss: 44.2530, val_MinusLogProbMetric: 44.2530

Epoch 397: val_loss improved from 44.28548 to 44.25301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 44.0321 - MinusLogProbMetric: 44.0321 - val_loss: 44.2530 - val_MinusLogProbMetric: 44.2530 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 398/1000
2023-10-27 17:03:37.806 
Epoch 398/1000 
	 loss: 44.0546, MinusLogProbMetric: 44.0546, val_loss: 45.2517, val_MinusLogProbMetric: 45.2517

Epoch 398: val_loss did not improve from 44.25301
196/196 - 33s - loss: 44.0546 - MinusLogProbMetric: 44.0546 - val_loss: 45.2517 - val_MinusLogProbMetric: 45.2517 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 399/1000
2023-10-27 17:04:10.474 
Epoch 399/1000 
	 loss: 44.3015, MinusLogProbMetric: 44.3015, val_loss: 45.9739, val_MinusLogProbMetric: 45.9739

Epoch 399: val_loss did not improve from 44.25301
196/196 - 33s - loss: 44.3015 - MinusLogProbMetric: 44.3015 - val_loss: 45.9739 - val_MinusLogProbMetric: 45.9739 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 400/1000
2023-10-27 17:04:43.024 
Epoch 400/1000 
	 loss: 44.1223, MinusLogProbMetric: 44.1223, val_loss: 46.4223, val_MinusLogProbMetric: 46.4223

Epoch 400: val_loss did not improve from 44.25301
196/196 - 33s - loss: 44.1223 - MinusLogProbMetric: 44.1223 - val_loss: 46.4223 - val_MinusLogProbMetric: 46.4223 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 401/1000
2023-10-27 17:05:16.220 
Epoch 401/1000 
	 loss: 43.9350, MinusLogProbMetric: 43.9350, val_loss: 44.1939, val_MinusLogProbMetric: 44.1939

Epoch 401: val_loss improved from 44.25301 to 44.19394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 43.9350 - MinusLogProbMetric: 43.9350 - val_loss: 44.1939 - val_MinusLogProbMetric: 44.1939 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 402/1000
2023-10-27 17:05:45.219 
Epoch 402/1000 
	 loss: 44.1201, MinusLogProbMetric: 44.1201, val_loss: 45.9476, val_MinusLogProbMetric: 45.9476

Epoch 402: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.1201 - MinusLogProbMetric: 44.1201 - val_loss: 45.9476 - val_MinusLogProbMetric: 45.9476 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 403/1000
2023-10-27 17:06:12.114 
Epoch 403/1000 
	 loss: 44.1454, MinusLogProbMetric: 44.1454, val_loss: 44.7144, val_MinusLogProbMetric: 44.7144

Epoch 403: val_loss did not improve from 44.19394
196/196 - 27s - loss: 44.1454 - MinusLogProbMetric: 44.1454 - val_loss: 44.7144 - val_MinusLogProbMetric: 44.7144 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 404/1000
2023-10-27 17:06:41.545 
Epoch 404/1000 
	 loss: 44.1041, MinusLogProbMetric: 44.1041, val_loss: 44.2645, val_MinusLogProbMetric: 44.2645

Epoch 404: val_loss did not improve from 44.19394
196/196 - 29s - loss: 44.1041 - MinusLogProbMetric: 44.1041 - val_loss: 44.2645 - val_MinusLogProbMetric: 44.2645 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 405/1000
2023-10-27 17:07:07.981 
Epoch 405/1000 
	 loss: 44.0136, MinusLogProbMetric: 44.0136, val_loss: 44.3576, val_MinusLogProbMetric: 44.3576

Epoch 405: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0136 - MinusLogProbMetric: 44.0136 - val_loss: 44.3576 - val_MinusLogProbMetric: 44.3576 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 406/1000
2023-10-27 17:07:38.746 
Epoch 406/1000 
	 loss: 44.0414, MinusLogProbMetric: 44.0414, val_loss: 44.7626, val_MinusLogProbMetric: 44.7626

Epoch 406: val_loss did not improve from 44.19394
196/196 - 31s - loss: 44.0414 - MinusLogProbMetric: 44.0414 - val_loss: 44.7626 - val_MinusLogProbMetric: 44.7626 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 407/1000
2023-10-27 17:08:05.789 
Epoch 407/1000 
	 loss: 43.9873, MinusLogProbMetric: 43.9873, val_loss: 44.4291, val_MinusLogProbMetric: 44.4291

Epoch 407: val_loss did not improve from 44.19394
196/196 - 27s - loss: 43.9873 - MinusLogProbMetric: 43.9873 - val_loss: 44.4291 - val_MinusLogProbMetric: 44.4291 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 408/1000
2023-10-27 17:08:34.155 
Epoch 408/1000 
	 loss: 44.1876, MinusLogProbMetric: 44.1876, val_loss: 45.6178, val_MinusLogProbMetric: 45.6178

Epoch 408: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.1876 - MinusLogProbMetric: 44.1876 - val_loss: 45.6178 - val_MinusLogProbMetric: 45.6178 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 409/1000
2023-10-27 17:09:02.373 
Epoch 409/1000 
	 loss: 44.0457, MinusLogProbMetric: 44.0457, val_loss: 45.2489, val_MinusLogProbMetric: 45.2489

Epoch 409: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.0457 - MinusLogProbMetric: 44.0457 - val_loss: 45.2489 - val_MinusLogProbMetric: 45.2489 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 410/1000
2023-10-27 17:09:28.849 
Epoch 410/1000 
	 loss: 44.0063, MinusLogProbMetric: 44.0063, val_loss: 45.3002, val_MinusLogProbMetric: 45.3002

Epoch 410: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0063 - MinusLogProbMetric: 44.0063 - val_loss: 45.3002 - val_MinusLogProbMetric: 45.3002 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 411/1000
2023-10-27 17:09:56.926 
Epoch 411/1000 
	 loss: 44.0217, MinusLogProbMetric: 44.0217, val_loss: 48.5376, val_MinusLogProbMetric: 48.5376

Epoch 411: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.0217 - MinusLogProbMetric: 44.0217 - val_loss: 48.5376 - val_MinusLogProbMetric: 48.5376 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 412/1000
2023-10-27 17:10:23.289 
Epoch 412/1000 
	 loss: 44.0944, MinusLogProbMetric: 44.0944, val_loss: 44.6439, val_MinusLogProbMetric: 44.6439

Epoch 412: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0944 - MinusLogProbMetric: 44.0944 - val_loss: 44.6439 - val_MinusLogProbMetric: 44.6439 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 413/1000
2023-10-27 17:10:55.112 
Epoch 413/1000 
	 loss: 44.0310, MinusLogProbMetric: 44.0310, val_loss: 44.8104, val_MinusLogProbMetric: 44.8104

Epoch 413: val_loss did not improve from 44.19394
196/196 - 32s - loss: 44.0310 - MinusLogProbMetric: 44.0310 - val_loss: 44.8104 - val_MinusLogProbMetric: 44.8104 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 414/1000
2023-10-27 17:11:21.737 
Epoch 414/1000 
	 loss: 43.9861, MinusLogProbMetric: 43.9861, val_loss: 48.0591, val_MinusLogProbMetric: 48.0591

Epoch 414: val_loss did not improve from 44.19394
196/196 - 27s - loss: 43.9861 - MinusLogProbMetric: 43.9861 - val_loss: 48.0591 - val_MinusLogProbMetric: 48.0591 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 415/1000
2023-10-27 17:11:51.907 
Epoch 415/1000 
	 loss: 44.1034, MinusLogProbMetric: 44.1034, val_loss: 45.9027, val_MinusLogProbMetric: 45.9027

Epoch 415: val_loss did not improve from 44.19394
196/196 - 30s - loss: 44.1034 - MinusLogProbMetric: 44.1034 - val_loss: 45.9027 - val_MinusLogProbMetric: 45.9027 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 416/1000
2023-10-27 17:12:17.889 
Epoch 416/1000 
	 loss: 44.6660, MinusLogProbMetric: 44.6660, val_loss: 44.5675, val_MinusLogProbMetric: 44.5675

Epoch 416: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.6660 - MinusLogProbMetric: 44.6660 - val_loss: 44.5675 - val_MinusLogProbMetric: 44.5675 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 417/1000
2023-10-27 17:12:45.553 
Epoch 417/1000 
	 loss: 44.2033, MinusLogProbMetric: 44.2033, val_loss: 46.4212, val_MinusLogProbMetric: 46.4212

Epoch 417: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.2033 - MinusLogProbMetric: 44.2033 - val_loss: 46.4212 - val_MinusLogProbMetric: 46.4212 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 418/1000
2023-10-27 17:13:12.039 
Epoch 418/1000 
	 loss: 43.9404, MinusLogProbMetric: 43.9404, val_loss: 45.3221, val_MinusLogProbMetric: 45.3221

Epoch 418: val_loss did not improve from 44.19394
196/196 - 26s - loss: 43.9404 - MinusLogProbMetric: 43.9404 - val_loss: 45.3221 - val_MinusLogProbMetric: 45.3221 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 419/1000
2023-10-27 17:13:38.455 
Epoch 419/1000 
	 loss: 44.0797, MinusLogProbMetric: 44.0797, val_loss: 44.8585, val_MinusLogProbMetric: 44.8585

Epoch 419: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0797 - MinusLogProbMetric: 44.0797 - val_loss: 44.8585 - val_MinusLogProbMetric: 44.8585 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 420/1000
2023-10-27 17:14:04.550 
Epoch 420/1000 
	 loss: 44.0730, MinusLogProbMetric: 44.0730, val_loss: 44.5070, val_MinusLogProbMetric: 44.5070

Epoch 420: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0730 - MinusLogProbMetric: 44.0730 - val_loss: 44.5070 - val_MinusLogProbMetric: 44.5070 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 421/1000
2023-10-27 17:14:30.871 
Epoch 421/1000 
	 loss: 44.0047, MinusLogProbMetric: 44.0047, val_loss: 45.2262, val_MinusLogProbMetric: 45.2262

Epoch 421: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0047 - MinusLogProbMetric: 44.0047 - val_loss: 45.2262 - val_MinusLogProbMetric: 45.2262 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 422/1000
2023-10-27 17:14:56.822 
Epoch 422/1000 
	 loss: 43.9226, MinusLogProbMetric: 43.9226, val_loss: 44.8559, val_MinusLogProbMetric: 44.8559

Epoch 422: val_loss did not improve from 44.19394
196/196 - 26s - loss: 43.9226 - MinusLogProbMetric: 43.9226 - val_loss: 44.8559 - val_MinusLogProbMetric: 44.8559 - lr: 3.3333e-04 - 26s/epoch - 132ms/step
Epoch 423/1000
2023-10-27 17:15:22.747 
Epoch 423/1000 
	 loss: 43.8691, MinusLogProbMetric: 43.8691, val_loss: 44.9915, val_MinusLogProbMetric: 44.9915

Epoch 423: val_loss did not improve from 44.19394
196/196 - 26s - loss: 43.8691 - MinusLogProbMetric: 43.8691 - val_loss: 44.9915 - val_MinusLogProbMetric: 44.9915 - lr: 3.3333e-04 - 26s/epoch - 132ms/step
Epoch 424/1000
2023-10-27 17:15:50.974 
Epoch 424/1000 
	 loss: 43.8541, MinusLogProbMetric: 43.8541, val_loss: 44.5813, val_MinusLogProbMetric: 44.5813

Epoch 424: val_loss did not improve from 44.19394
196/196 - 28s - loss: 43.8541 - MinusLogProbMetric: 43.8541 - val_loss: 44.5813 - val_MinusLogProbMetric: 44.5813 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 425/1000
2023-10-27 17:16:19.066 
Epoch 425/1000 
	 loss: 44.0661, MinusLogProbMetric: 44.0661, val_loss: 44.9098, val_MinusLogProbMetric: 44.9098

Epoch 425: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.0661 - MinusLogProbMetric: 44.0661 - val_loss: 44.9098 - val_MinusLogProbMetric: 44.9098 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 426/1000
2023-10-27 17:16:46.581 
Epoch 426/1000 
	 loss: 44.0207, MinusLogProbMetric: 44.0207, val_loss: 44.9923, val_MinusLogProbMetric: 44.9923

Epoch 426: val_loss did not improve from 44.19394
196/196 - 28s - loss: 44.0207 - MinusLogProbMetric: 44.0207 - val_loss: 44.9923 - val_MinusLogProbMetric: 44.9923 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 427/1000
2023-10-27 17:17:12.769 
Epoch 427/1000 
	 loss: 44.0171, MinusLogProbMetric: 44.0171, val_loss: 44.3206, val_MinusLogProbMetric: 44.3206

Epoch 427: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0171 - MinusLogProbMetric: 44.0171 - val_loss: 44.3206 - val_MinusLogProbMetric: 44.3206 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 428/1000
2023-10-27 17:17:38.949 
Epoch 428/1000 
	 loss: 43.7636, MinusLogProbMetric: 43.7636, val_loss: 44.4848, val_MinusLogProbMetric: 44.4848

Epoch 428: val_loss did not improve from 44.19394
196/196 - 26s - loss: 43.7636 - MinusLogProbMetric: 43.7636 - val_loss: 44.4848 - val_MinusLogProbMetric: 44.4848 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 429/1000
2023-10-27 17:18:05.105 
Epoch 429/1000 
	 loss: 43.9185, MinusLogProbMetric: 43.9185, val_loss: 44.2840, val_MinusLogProbMetric: 44.2840

Epoch 429: val_loss did not improve from 44.19394
196/196 - 26s - loss: 43.9185 - MinusLogProbMetric: 43.9185 - val_loss: 44.2840 - val_MinusLogProbMetric: 44.2840 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 430/1000
2023-10-27 17:18:31.477 
Epoch 430/1000 
	 loss: 44.0221, MinusLogProbMetric: 44.0221, val_loss: 44.9942, val_MinusLogProbMetric: 44.9942

Epoch 430: val_loss did not improve from 44.19394
196/196 - 26s - loss: 44.0221 - MinusLogProbMetric: 44.0221 - val_loss: 44.9942 - val_MinusLogProbMetric: 44.9942 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 431/1000
2023-10-27 17:18:58.706 
Epoch 431/1000 
	 loss: 44.5057, MinusLogProbMetric: 44.5057, val_loss: 45.9493, val_MinusLogProbMetric: 45.9493

Epoch 431: val_loss did not improve from 44.19394
196/196 - 27s - loss: 44.5057 - MinusLogProbMetric: 44.5057 - val_loss: 45.9493 - val_MinusLogProbMetric: 45.9493 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 432/1000
2023-10-27 17:19:31.282 
Epoch 432/1000 
	 loss: 43.8520, MinusLogProbMetric: 43.8520, val_loss: 44.8008, val_MinusLogProbMetric: 44.8008

Epoch 432: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.8520 - MinusLogProbMetric: 43.8520 - val_loss: 44.8008 - val_MinusLogProbMetric: 44.8008 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 433/1000
2023-10-27 17:20:04.098 
Epoch 433/1000 
	 loss: 43.9271, MinusLogProbMetric: 43.9271, val_loss: 45.2459, val_MinusLogProbMetric: 45.2459

Epoch 433: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.9271 - MinusLogProbMetric: 43.9271 - val_loss: 45.2459 - val_MinusLogProbMetric: 45.2459 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 434/1000
2023-10-27 17:20:36.337 
Epoch 434/1000 
	 loss: 44.1829, MinusLogProbMetric: 44.1829, val_loss: 45.5139, val_MinusLogProbMetric: 45.5139

Epoch 434: val_loss did not improve from 44.19394
196/196 - 32s - loss: 44.1829 - MinusLogProbMetric: 44.1829 - val_loss: 45.5139 - val_MinusLogProbMetric: 45.5139 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 435/1000
2023-10-27 17:21:09.350 
Epoch 435/1000 
	 loss: 44.0578, MinusLogProbMetric: 44.0578, val_loss: 44.4370, val_MinusLogProbMetric: 44.4370

Epoch 435: val_loss did not improve from 44.19394
196/196 - 33s - loss: 44.0578 - MinusLogProbMetric: 44.0578 - val_loss: 44.4370 - val_MinusLogProbMetric: 44.4370 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 436/1000
2023-10-27 17:21:41.580 
Epoch 436/1000 
	 loss: 44.0644, MinusLogProbMetric: 44.0644, val_loss: 45.5389, val_MinusLogProbMetric: 45.5389

Epoch 436: val_loss did not improve from 44.19394
196/196 - 32s - loss: 44.0644 - MinusLogProbMetric: 44.0644 - val_loss: 45.5389 - val_MinusLogProbMetric: 45.5389 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 437/1000
2023-10-27 17:22:14.675 
Epoch 437/1000 
	 loss: 43.9346, MinusLogProbMetric: 43.9346, val_loss: 45.3516, val_MinusLogProbMetric: 45.3516

Epoch 437: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.9346 - MinusLogProbMetric: 43.9346 - val_loss: 45.3516 - val_MinusLogProbMetric: 45.3516 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 438/1000
2023-10-27 17:22:47.264 
Epoch 438/1000 
	 loss: 43.9211, MinusLogProbMetric: 43.9211, val_loss: 46.0505, val_MinusLogProbMetric: 46.0505

Epoch 438: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.9211 - MinusLogProbMetric: 43.9211 - val_loss: 46.0505 - val_MinusLogProbMetric: 46.0505 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 439/1000
2023-10-27 17:23:20.502 
Epoch 439/1000 
	 loss: 43.9855, MinusLogProbMetric: 43.9855, val_loss: 45.2023, val_MinusLogProbMetric: 45.2023

Epoch 439: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.9855 - MinusLogProbMetric: 43.9855 - val_loss: 45.2023 - val_MinusLogProbMetric: 45.2023 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 440/1000
2023-10-27 17:23:53.768 
Epoch 440/1000 
	 loss: 43.9095, MinusLogProbMetric: 43.9095, val_loss: 45.7717, val_MinusLogProbMetric: 45.7717

Epoch 440: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.9095 - MinusLogProbMetric: 43.9095 - val_loss: 45.7717 - val_MinusLogProbMetric: 45.7717 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 441/1000
2023-10-27 17:24:27.059 
Epoch 441/1000 
	 loss: 43.8274, MinusLogProbMetric: 43.8274, val_loss: 45.2145, val_MinusLogProbMetric: 45.2145

Epoch 441: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.8274 - MinusLogProbMetric: 43.8274 - val_loss: 45.2145 - val_MinusLogProbMetric: 45.2145 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 442/1000
2023-10-27 17:25:00.076 
Epoch 442/1000 
	 loss: 43.8248, MinusLogProbMetric: 43.8248, val_loss: 44.9008, val_MinusLogProbMetric: 44.9008

Epoch 442: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.8248 - MinusLogProbMetric: 43.8248 - val_loss: 44.9008 - val_MinusLogProbMetric: 44.9008 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 443/1000
2023-10-27 17:25:33.027 
Epoch 443/1000 
	 loss: 44.2078, MinusLogProbMetric: 44.2078, val_loss: 46.5277, val_MinusLogProbMetric: 46.5277

Epoch 443: val_loss did not improve from 44.19394
196/196 - 33s - loss: 44.2078 - MinusLogProbMetric: 44.2078 - val_loss: 46.5277 - val_MinusLogProbMetric: 46.5277 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 444/1000
2023-10-27 17:26:06.152 
Epoch 444/1000 
	 loss: 44.2491, MinusLogProbMetric: 44.2491, val_loss: 44.7184, val_MinusLogProbMetric: 44.7184

Epoch 444: val_loss did not improve from 44.19394
196/196 - 33s - loss: 44.2491 - MinusLogProbMetric: 44.2491 - val_loss: 44.7184 - val_MinusLogProbMetric: 44.7184 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 445/1000
2023-10-27 17:26:37.934 
Epoch 445/1000 
	 loss: 43.7133, MinusLogProbMetric: 43.7133, val_loss: 44.5775, val_MinusLogProbMetric: 44.5775

Epoch 445: val_loss did not improve from 44.19394
196/196 - 32s - loss: 43.7133 - MinusLogProbMetric: 43.7133 - val_loss: 44.5775 - val_MinusLogProbMetric: 44.5775 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 446/1000
2023-10-27 17:27:10.864 
Epoch 446/1000 
	 loss: 45.7651, MinusLogProbMetric: 45.7651, val_loss: 45.9593, val_MinusLogProbMetric: 45.9593

Epoch 446: val_loss did not improve from 44.19394
196/196 - 33s - loss: 45.7651 - MinusLogProbMetric: 45.7651 - val_loss: 45.9593 - val_MinusLogProbMetric: 45.9593 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 447/1000
2023-10-27 17:27:43.743 
Epoch 447/1000 
	 loss: 44.7553, MinusLogProbMetric: 44.7553, val_loss: 44.4821, val_MinusLogProbMetric: 44.4821

Epoch 447: val_loss did not improve from 44.19394
196/196 - 33s - loss: 44.7553 - MinusLogProbMetric: 44.7553 - val_loss: 44.4821 - val_MinusLogProbMetric: 44.4821 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 448/1000
2023-10-27 17:28:15.561 
Epoch 448/1000 
	 loss: 44.1003, MinusLogProbMetric: 44.1003, val_loss: 44.8629, val_MinusLogProbMetric: 44.8629

Epoch 448: val_loss did not improve from 44.19394
196/196 - 32s - loss: 44.1003 - MinusLogProbMetric: 44.1003 - val_loss: 44.8629 - val_MinusLogProbMetric: 44.8629 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 449/1000
2023-10-27 17:28:48.475 
Epoch 449/1000 
	 loss: 44.1121, MinusLogProbMetric: 44.1121, val_loss: 44.5159, val_MinusLogProbMetric: 44.5159

Epoch 449: val_loss did not improve from 44.19394
196/196 - 33s - loss: 44.1121 - MinusLogProbMetric: 44.1121 - val_loss: 44.5159 - val_MinusLogProbMetric: 44.5159 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 450/1000
2023-10-27 17:29:22.054 
Epoch 450/1000 
	 loss: 44.0264, MinusLogProbMetric: 44.0264, val_loss: 44.7117, val_MinusLogProbMetric: 44.7117

Epoch 450: val_loss did not improve from 44.19394
196/196 - 34s - loss: 44.0264 - MinusLogProbMetric: 44.0264 - val_loss: 44.7117 - val_MinusLogProbMetric: 44.7117 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 451/1000
2023-10-27 17:29:54.607 
Epoch 451/1000 
	 loss: 43.9600, MinusLogProbMetric: 43.9600, val_loss: 44.2682, val_MinusLogProbMetric: 44.2682

Epoch 451: val_loss did not improve from 44.19394
196/196 - 33s - loss: 43.9600 - MinusLogProbMetric: 43.9600 - val_loss: 44.2682 - val_MinusLogProbMetric: 44.2682 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 452/1000
2023-10-27 17:30:27.880 
Epoch 452/1000 
	 loss: 42.6249, MinusLogProbMetric: 42.6249, val_loss: 43.4796, val_MinusLogProbMetric: 43.4796

Epoch 452: val_loss improved from 44.19394 to 43.47960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 42.6249 - MinusLogProbMetric: 42.6249 - val_loss: 43.4796 - val_MinusLogProbMetric: 43.4796 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 453/1000
2023-10-27 17:31:02.151 
Epoch 453/1000 
	 loss: 42.7240, MinusLogProbMetric: 42.7240, val_loss: 43.6639, val_MinusLogProbMetric: 43.6639

Epoch 453: val_loss did not improve from 43.47960
196/196 - 34s - loss: 42.7240 - MinusLogProbMetric: 42.7240 - val_loss: 43.6639 - val_MinusLogProbMetric: 43.6639 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 454/1000
2023-10-27 17:31:35.283 
Epoch 454/1000 
	 loss: 42.7041, MinusLogProbMetric: 42.7041, val_loss: 43.6464, val_MinusLogProbMetric: 43.6464

Epoch 454: val_loss did not improve from 43.47960
196/196 - 33s - loss: 42.7041 - MinusLogProbMetric: 42.7041 - val_loss: 43.6464 - val_MinusLogProbMetric: 43.6464 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 455/1000
2023-10-27 17:32:08.592 
Epoch 455/1000 
	 loss: 42.7378, MinusLogProbMetric: 42.7378, val_loss: 43.2290, val_MinusLogProbMetric: 43.2290

Epoch 455: val_loss improved from 43.47960 to 43.22899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 42.7378 - MinusLogProbMetric: 42.7378 - val_loss: 43.2290 - val_MinusLogProbMetric: 43.2290 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 456/1000
2023-10-27 17:32:42.388 
Epoch 456/1000 
	 loss: 42.6861, MinusLogProbMetric: 42.6861, val_loss: 44.1984, val_MinusLogProbMetric: 44.1984

Epoch 456: val_loss did not improve from 43.22899
196/196 - 33s - loss: 42.6861 - MinusLogProbMetric: 42.6861 - val_loss: 44.1984 - val_MinusLogProbMetric: 44.1984 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 457/1000
2023-10-27 17:33:15.785 
Epoch 457/1000 
	 loss: 42.7274, MinusLogProbMetric: 42.7274, val_loss: 43.1495, val_MinusLogProbMetric: 43.1495

Epoch 457: val_loss improved from 43.22899 to 43.14949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 42.7274 - MinusLogProbMetric: 42.7274 - val_loss: 43.1495 - val_MinusLogProbMetric: 43.1495 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 458/1000
2023-10-27 17:33:49.676 
Epoch 458/1000 
	 loss: 42.6402, MinusLogProbMetric: 42.6402, val_loss: 43.9120, val_MinusLogProbMetric: 43.9120

Epoch 458: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.6402 - MinusLogProbMetric: 42.6402 - val_loss: 43.9120 - val_MinusLogProbMetric: 43.9120 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 459/1000
2023-10-27 17:34:22.898 
Epoch 459/1000 
	 loss: 42.6768, MinusLogProbMetric: 42.6768, val_loss: 44.1341, val_MinusLogProbMetric: 44.1341

Epoch 459: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.6768 - MinusLogProbMetric: 42.6768 - val_loss: 44.1341 - val_MinusLogProbMetric: 44.1341 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 460/1000
2023-10-27 17:34:56.128 
Epoch 460/1000 
	 loss: 42.7996, MinusLogProbMetric: 42.7996, val_loss: 44.7165, val_MinusLogProbMetric: 44.7165

Epoch 460: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.7996 - MinusLogProbMetric: 42.7996 - val_loss: 44.7165 - val_MinusLogProbMetric: 44.7165 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 461/1000
2023-10-27 17:35:28.955 
Epoch 461/1000 
	 loss: 42.6548, MinusLogProbMetric: 42.6548, val_loss: 43.3296, val_MinusLogProbMetric: 43.3296

Epoch 461: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.6548 - MinusLogProbMetric: 42.6548 - val_loss: 43.3296 - val_MinusLogProbMetric: 43.3296 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 462/1000
2023-10-27 17:36:02.263 
Epoch 462/1000 
	 loss: 42.9718, MinusLogProbMetric: 42.9718, val_loss: 43.7452, val_MinusLogProbMetric: 43.7452

Epoch 462: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.9718 - MinusLogProbMetric: 42.9718 - val_loss: 43.7452 - val_MinusLogProbMetric: 43.7452 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 463/1000
2023-10-27 17:36:34.658 
Epoch 463/1000 
	 loss: 42.5948, MinusLogProbMetric: 42.5948, val_loss: 43.3080, val_MinusLogProbMetric: 43.3080

Epoch 463: val_loss did not improve from 43.14949
196/196 - 32s - loss: 42.5948 - MinusLogProbMetric: 42.5948 - val_loss: 43.3080 - val_MinusLogProbMetric: 43.3080 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 464/1000
2023-10-27 17:37:07.398 
Epoch 464/1000 
	 loss: 42.6445, MinusLogProbMetric: 42.6445, val_loss: 43.5701, val_MinusLogProbMetric: 43.5701

Epoch 464: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.6445 - MinusLogProbMetric: 42.6445 - val_loss: 43.5701 - val_MinusLogProbMetric: 43.5701 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 465/1000
2023-10-27 17:37:39.980 
Epoch 465/1000 
	 loss: 42.6061, MinusLogProbMetric: 42.6061, val_loss: 44.4240, val_MinusLogProbMetric: 44.4240

Epoch 465: val_loss did not improve from 43.14949
196/196 - 33s - loss: 42.6061 - MinusLogProbMetric: 42.6061 - val_loss: 44.4240 - val_MinusLogProbMetric: 44.4240 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 466/1000
2023-10-27 17:38:12.599 
Epoch 466/1000 
	 loss: 42.5801, MinusLogProbMetric: 42.5801, val_loss: 43.0502, val_MinusLogProbMetric: 43.0502

Epoch 466: val_loss improved from 43.14949 to 43.05023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 42.5801 - MinusLogProbMetric: 42.5801 - val_loss: 43.0502 - val_MinusLogProbMetric: 43.0502 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 467/1000
2023-10-27 17:38:43.081 
Epoch 467/1000 
	 loss: 42.6572, MinusLogProbMetric: 42.6572, val_loss: 43.3195, val_MinusLogProbMetric: 43.3195

Epoch 467: val_loss did not improve from 43.05023
196/196 - 30s - loss: 42.6572 - MinusLogProbMetric: 42.6572 - val_loss: 43.3195 - val_MinusLogProbMetric: 43.3195 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 468/1000
2023-10-27 17:39:11.821 
Epoch 468/1000 
	 loss: 42.6428, MinusLogProbMetric: 42.6428, val_loss: 44.4100, val_MinusLogProbMetric: 44.4100

Epoch 468: val_loss did not improve from 43.05023
196/196 - 29s - loss: 42.6428 - MinusLogProbMetric: 42.6428 - val_loss: 44.4100 - val_MinusLogProbMetric: 44.4100 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 469/1000
2023-10-27 17:39:40.404 
Epoch 469/1000 
	 loss: 42.6224, MinusLogProbMetric: 42.6224, val_loss: 43.5129, val_MinusLogProbMetric: 43.5129

Epoch 469: val_loss did not improve from 43.05023
196/196 - 29s - loss: 42.6224 - MinusLogProbMetric: 42.6224 - val_loss: 43.5129 - val_MinusLogProbMetric: 43.5129 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 470/1000
2023-10-27 17:40:09.810 
Epoch 470/1000 
	 loss: 42.6296, MinusLogProbMetric: 42.6296, val_loss: 43.8136, val_MinusLogProbMetric: 43.8136

Epoch 470: val_loss did not improve from 43.05023
196/196 - 29s - loss: 42.6296 - MinusLogProbMetric: 42.6296 - val_loss: 43.8136 - val_MinusLogProbMetric: 43.8136 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 471/1000
2023-10-27 17:40:40.227 
Epoch 471/1000 
	 loss: 42.6127, MinusLogProbMetric: 42.6127, val_loss: 43.5896, val_MinusLogProbMetric: 43.5896

Epoch 471: val_loss did not improve from 43.05023
196/196 - 30s - loss: 42.6127 - MinusLogProbMetric: 42.6127 - val_loss: 43.5896 - val_MinusLogProbMetric: 43.5896 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 472/1000
2023-10-27 17:41:12.939 
Epoch 472/1000 
	 loss: 42.6966, MinusLogProbMetric: 42.6966, val_loss: 47.2792, val_MinusLogProbMetric: 47.2792

Epoch 472: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6966 - MinusLogProbMetric: 42.6966 - val_loss: 47.2792 - val_MinusLogProbMetric: 47.2792 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 473/1000
2023-10-27 17:41:45.713 
Epoch 473/1000 
	 loss: 42.6823, MinusLogProbMetric: 42.6823, val_loss: 43.5369, val_MinusLogProbMetric: 43.5369

Epoch 473: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6823 - MinusLogProbMetric: 42.6823 - val_loss: 43.5369 - val_MinusLogProbMetric: 43.5369 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 474/1000
2023-10-27 17:42:18.790 
Epoch 474/1000 
	 loss: 42.6400, MinusLogProbMetric: 42.6400, val_loss: 43.5614, val_MinusLogProbMetric: 43.5614

Epoch 474: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6400 - MinusLogProbMetric: 42.6400 - val_loss: 43.5614 - val_MinusLogProbMetric: 43.5614 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 475/1000
2023-10-27 17:42:51.703 
Epoch 475/1000 
	 loss: 42.5703, MinusLogProbMetric: 42.5703, val_loss: 43.2785, val_MinusLogProbMetric: 43.2785

Epoch 475: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5703 - MinusLogProbMetric: 42.5703 - val_loss: 43.2785 - val_MinusLogProbMetric: 43.2785 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 476/1000
2023-10-27 17:43:24.703 
Epoch 476/1000 
	 loss: 42.5877, MinusLogProbMetric: 42.5877, val_loss: 43.3568, val_MinusLogProbMetric: 43.3568

Epoch 476: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5877 - MinusLogProbMetric: 42.5877 - val_loss: 43.3568 - val_MinusLogProbMetric: 43.3568 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 477/1000
2023-10-27 17:43:57.151 
Epoch 477/1000 
	 loss: 42.6331, MinusLogProbMetric: 42.6331, val_loss: 43.6802, val_MinusLogProbMetric: 43.6802

Epoch 477: val_loss did not improve from 43.05023
196/196 - 32s - loss: 42.6331 - MinusLogProbMetric: 42.6331 - val_loss: 43.6802 - val_MinusLogProbMetric: 43.6802 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 478/1000
2023-10-27 17:44:29.900 
Epoch 478/1000 
	 loss: 42.5694, MinusLogProbMetric: 42.5694, val_loss: 43.7767, val_MinusLogProbMetric: 43.7767

Epoch 478: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5694 - MinusLogProbMetric: 42.5694 - val_loss: 43.7767 - val_MinusLogProbMetric: 43.7767 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 479/1000
2023-10-27 17:45:02.737 
Epoch 479/1000 
	 loss: 42.5808, MinusLogProbMetric: 42.5808, val_loss: 43.3629, val_MinusLogProbMetric: 43.3629

Epoch 479: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5808 - MinusLogProbMetric: 42.5808 - val_loss: 43.3629 - val_MinusLogProbMetric: 43.3629 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 480/1000
2023-10-27 17:45:35.735 
Epoch 480/1000 
	 loss: 42.6058, MinusLogProbMetric: 42.6058, val_loss: 43.5463, val_MinusLogProbMetric: 43.5463

Epoch 480: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6058 - MinusLogProbMetric: 42.6058 - val_loss: 43.5463 - val_MinusLogProbMetric: 43.5463 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 481/1000
2023-10-27 17:46:08.857 
Epoch 481/1000 
	 loss: 42.5306, MinusLogProbMetric: 42.5306, val_loss: 43.3470, val_MinusLogProbMetric: 43.3470

Epoch 481: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5306 - MinusLogProbMetric: 42.5306 - val_loss: 43.3470 - val_MinusLogProbMetric: 43.3470 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 482/1000
2023-10-27 17:46:41.713 
Epoch 482/1000 
	 loss: 42.5369, MinusLogProbMetric: 42.5369, val_loss: 43.5087, val_MinusLogProbMetric: 43.5087

Epoch 482: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5369 - MinusLogProbMetric: 42.5369 - val_loss: 43.5087 - val_MinusLogProbMetric: 43.5087 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 483/1000
2023-10-27 17:47:14.836 
Epoch 483/1000 
	 loss: 42.6440, MinusLogProbMetric: 42.6440, val_loss: 44.0201, val_MinusLogProbMetric: 44.0201

Epoch 483: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6440 - MinusLogProbMetric: 42.6440 - val_loss: 44.0201 - val_MinusLogProbMetric: 44.0201 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 484/1000
2023-10-27 17:47:47.614 
Epoch 484/1000 
	 loss: 42.5641, MinusLogProbMetric: 42.5641, val_loss: 43.4595, val_MinusLogProbMetric: 43.4595

Epoch 484: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5641 - MinusLogProbMetric: 42.5641 - val_loss: 43.4595 - val_MinusLogProbMetric: 43.4595 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 485/1000
2023-10-27 17:48:16.210 
Epoch 485/1000 
	 loss: 42.5128, MinusLogProbMetric: 42.5128, val_loss: 43.1963, val_MinusLogProbMetric: 43.1963

Epoch 485: val_loss did not improve from 43.05023
196/196 - 29s - loss: 42.5128 - MinusLogProbMetric: 42.5128 - val_loss: 43.1963 - val_MinusLogProbMetric: 43.1963 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 486/1000
2023-10-27 17:48:43.616 
Epoch 486/1000 
	 loss: 42.5758, MinusLogProbMetric: 42.5758, val_loss: 43.3569, val_MinusLogProbMetric: 43.3569

Epoch 486: val_loss did not improve from 43.05023
196/196 - 27s - loss: 42.5758 - MinusLogProbMetric: 42.5758 - val_loss: 43.3569 - val_MinusLogProbMetric: 43.3569 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 487/1000
2023-10-27 17:49:11.024 
Epoch 487/1000 
	 loss: 42.6143, MinusLogProbMetric: 42.6143, val_loss: 43.5063, val_MinusLogProbMetric: 43.5063

Epoch 487: val_loss did not improve from 43.05023
196/196 - 27s - loss: 42.6143 - MinusLogProbMetric: 42.6143 - val_loss: 43.5063 - val_MinusLogProbMetric: 43.5063 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 488/1000
2023-10-27 17:49:38.039 
Epoch 488/1000 
	 loss: 42.7026, MinusLogProbMetric: 42.7026, val_loss: 43.8211, val_MinusLogProbMetric: 43.8211

Epoch 488: val_loss did not improve from 43.05023
196/196 - 27s - loss: 42.7026 - MinusLogProbMetric: 42.7026 - val_loss: 43.8211 - val_MinusLogProbMetric: 43.8211 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 489/1000
2023-10-27 17:50:08.057 
Epoch 489/1000 
	 loss: 42.5356, MinusLogProbMetric: 42.5356, val_loss: 43.4613, val_MinusLogProbMetric: 43.4613

Epoch 489: val_loss did not improve from 43.05023
196/196 - 30s - loss: 42.5356 - MinusLogProbMetric: 42.5356 - val_loss: 43.4613 - val_MinusLogProbMetric: 43.4613 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 490/1000
2023-10-27 17:50:40.567 
Epoch 490/1000 
	 loss: 42.5351, MinusLogProbMetric: 42.5351, val_loss: 47.6360, val_MinusLogProbMetric: 47.6360

Epoch 490: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5351 - MinusLogProbMetric: 42.5351 - val_loss: 47.6360 - val_MinusLogProbMetric: 47.6360 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 491/1000
2023-10-27 17:51:12.931 
Epoch 491/1000 
	 loss: 42.7711, MinusLogProbMetric: 42.7711, val_loss: 45.3149, val_MinusLogProbMetric: 45.3149

Epoch 491: val_loss did not improve from 43.05023
196/196 - 32s - loss: 42.7711 - MinusLogProbMetric: 42.7711 - val_loss: 45.3149 - val_MinusLogProbMetric: 45.3149 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 492/1000
2023-10-27 17:51:45.790 
Epoch 492/1000 
	 loss: 42.5322, MinusLogProbMetric: 42.5322, val_loss: 43.2773, val_MinusLogProbMetric: 43.2773

Epoch 492: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5322 - MinusLogProbMetric: 42.5322 - val_loss: 43.2773 - val_MinusLogProbMetric: 43.2773 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 493/1000
2023-10-27 17:52:18.238 
Epoch 493/1000 
	 loss: 42.5760, MinusLogProbMetric: 42.5760, val_loss: 43.2180, val_MinusLogProbMetric: 43.2180

Epoch 493: val_loss did not improve from 43.05023
196/196 - 32s - loss: 42.5760 - MinusLogProbMetric: 42.5760 - val_loss: 43.2180 - val_MinusLogProbMetric: 43.2180 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 494/1000
2023-10-27 17:52:50.981 
Epoch 494/1000 
	 loss: 42.6091, MinusLogProbMetric: 42.6091, val_loss: 43.2347, val_MinusLogProbMetric: 43.2347

Epoch 494: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6091 - MinusLogProbMetric: 42.6091 - val_loss: 43.2347 - val_MinusLogProbMetric: 43.2347 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 495/1000
2023-10-27 17:53:23.802 
Epoch 495/1000 
	 loss: 42.5368, MinusLogProbMetric: 42.5368, val_loss: 43.7443, val_MinusLogProbMetric: 43.7443

Epoch 495: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.5368 - MinusLogProbMetric: 42.5368 - val_loss: 43.7443 - val_MinusLogProbMetric: 43.7443 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 496/1000
2023-10-27 17:53:56.478 
Epoch 496/1000 
	 loss: 42.4644, MinusLogProbMetric: 42.4644, val_loss: 43.6565, val_MinusLogProbMetric: 43.6565

Epoch 496: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.4644 - MinusLogProbMetric: 42.4644 - val_loss: 43.6565 - val_MinusLogProbMetric: 43.6565 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 497/1000
2023-10-27 17:54:28.960 
Epoch 497/1000 
	 loss: 42.5711, MinusLogProbMetric: 42.5711, val_loss: 43.2464, val_MinusLogProbMetric: 43.2464

Epoch 497: val_loss did not improve from 43.05023
196/196 - 32s - loss: 42.5711 - MinusLogProbMetric: 42.5711 - val_loss: 43.2464 - val_MinusLogProbMetric: 43.2464 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 498/1000
2023-10-27 17:55:01.804 
Epoch 498/1000 
	 loss: 42.6895, MinusLogProbMetric: 42.6895, val_loss: 43.2878, val_MinusLogProbMetric: 43.2878

Epoch 498: val_loss did not improve from 43.05023
196/196 - 33s - loss: 42.6895 - MinusLogProbMetric: 42.6895 - val_loss: 43.2878 - val_MinusLogProbMetric: 43.2878 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 499/1000
2023-10-27 17:55:33.664 
Epoch 499/1000 
	 loss: 42.4507, MinusLogProbMetric: 42.4507, val_loss: 43.1896, val_MinusLogProbMetric: 43.1896

Epoch 499: val_loss did not improve from 43.05023
196/196 - 32s - loss: 42.4507 - MinusLogProbMetric: 42.4507 - val_loss: 43.1896 - val_MinusLogProbMetric: 43.1896 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 500/1000
2023-10-27 17:56:06.350 
Epoch 500/1000 
	 loss: 42.4990, MinusLogProbMetric: 42.4990, val_loss: 42.9025, val_MinusLogProbMetric: 42.9025

Epoch 500: val_loss improved from 43.05023 to 42.90248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 42.4990 - MinusLogProbMetric: 42.4990 - val_loss: 42.9025 - val_MinusLogProbMetric: 42.9025 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 501/1000
2023-10-27 17:56:39.783 
Epoch 501/1000 
	 loss: 42.5919, MinusLogProbMetric: 42.5919, val_loss: 43.6491, val_MinusLogProbMetric: 43.6491

Epoch 501: val_loss did not improve from 42.90248
196/196 - 33s - loss: 42.5919 - MinusLogProbMetric: 42.5919 - val_loss: 43.6491 - val_MinusLogProbMetric: 43.6491 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 502/1000
2023-10-27 17:57:12.622 
Epoch 502/1000 
	 loss: 42.4451, MinusLogProbMetric: 42.4451, val_loss: 43.2304, val_MinusLogProbMetric: 43.2304

Epoch 502: val_loss did not improve from 42.90248
196/196 - 33s - loss: 42.4451 - MinusLogProbMetric: 42.4451 - val_loss: 43.2304 - val_MinusLogProbMetric: 43.2304 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 503/1000
2023-10-27 17:57:45.083 
Epoch 503/1000 
	 loss: 42.6419, MinusLogProbMetric: 42.6419, val_loss: 43.0900, val_MinusLogProbMetric: 43.0900

Epoch 503: val_loss did not improve from 42.90248
196/196 - 32s - loss: 42.6419 - MinusLogProbMetric: 42.6419 - val_loss: 43.0900 - val_MinusLogProbMetric: 43.0900 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 504/1000
2023-10-27 17:58:18.176 
Epoch 504/1000 
	 loss: 42.5044, MinusLogProbMetric: 42.5044, val_loss: 43.8419, val_MinusLogProbMetric: 43.8419

Epoch 504: val_loss did not improve from 42.90248
196/196 - 33s - loss: 42.5044 - MinusLogProbMetric: 42.5044 - val_loss: 43.8419 - val_MinusLogProbMetric: 43.8419 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 505/1000
2023-10-27 17:58:51.192 
Epoch 505/1000 
	 loss: 42.6197, MinusLogProbMetric: 42.6197, val_loss: 43.3238, val_MinusLogProbMetric: 43.3238

Epoch 505: val_loss did not improve from 42.90248
196/196 - 33s - loss: 42.6197 - MinusLogProbMetric: 42.6197 - val_loss: 43.3238 - val_MinusLogProbMetric: 43.3238 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 506/1000
2023-10-27 17:59:24.278 
Epoch 506/1000 
	 loss: 42.5376, MinusLogProbMetric: 42.5376, val_loss: 42.8978, val_MinusLogProbMetric: 42.8978

Epoch 506: val_loss improved from 42.90248 to 42.89782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 42.5376 - MinusLogProbMetric: 42.5376 - val_loss: 42.8978 - val_MinusLogProbMetric: 42.8978 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 507/1000
2023-10-27 17:59:57.794 
Epoch 507/1000 
	 loss: 42.5844, MinusLogProbMetric: 42.5844, val_loss: 44.1225, val_MinusLogProbMetric: 44.1225

Epoch 507: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5844 - MinusLogProbMetric: 42.5844 - val_loss: 44.1225 - val_MinusLogProbMetric: 44.1225 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 508/1000
2023-10-27 18:00:30.803 
Epoch 508/1000 
	 loss: 42.4558, MinusLogProbMetric: 42.4558, val_loss: 43.0713, val_MinusLogProbMetric: 43.0713

Epoch 508: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4558 - MinusLogProbMetric: 42.4558 - val_loss: 43.0713 - val_MinusLogProbMetric: 43.0713 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 509/1000
2023-10-27 18:01:03.458 
Epoch 509/1000 
	 loss: 42.4834, MinusLogProbMetric: 42.4834, val_loss: 44.3729, val_MinusLogProbMetric: 44.3729

Epoch 509: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4834 - MinusLogProbMetric: 42.4834 - val_loss: 44.3729 - val_MinusLogProbMetric: 44.3729 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 510/1000
2023-10-27 18:01:35.818 
Epoch 510/1000 
	 loss: 42.5058, MinusLogProbMetric: 42.5058, val_loss: 44.3394, val_MinusLogProbMetric: 44.3394

Epoch 510: val_loss did not improve from 42.89782
196/196 - 32s - loss: 42.5058 - MinusLogProbMetric: 42.5058 - val_loss: 44.3394 - val_MinusLogProbMetric: 44.3394 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 511/1000
2023-10-27 18:02:08.518 
Epoch 511/1000 
	 loss: 42.5806, MinusLogProbMetric: 42.5806, val_loss: 43.6249, val_MinusLogProbMetric: 43.6249

Epoch 511: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5806 - MinusLogProbMetric: 42.5806 - val_loss: 43.6249 - val_MinusLogProbMetric: 43.6249 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 512/1000
2023-10-27 18:02:41.463 
Epoch 512/1000 
	 loss: 42.4973, MinusLogProbMetric: 42.4973, val_loss: 43.8393, val_MinusLogProbMetric: 43.8393

Epoch 512: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4973 - MinusLogProbMetric: 42.4973 - val_loss: 43.8393 - val_MinusLogProbMetric: 43.8393 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 513/1000
2023-10-27 18:03:14.144 
Epoch 513/1000 
	 loss: 42.5026, MinusLogProbMetric: 42.5026, val_loss: 43.2309, val_MinusLogProbMetric: 43.2309

Epoch 513: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5026 - MinusLogProbMetric: 42.5026 - val_loss: 43.2309 - val_MinusLogProbMetric: 43.2309 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 514/1000
2023-10-27 18:03:46.721 
Epoch 514/1000 
	 loss: 43.2079, MinusLogProbMetric: 43.2079, val_loss: 43.6812, val_MinusLogProbMetric: 43.6812

Epoch 514: val_loss did not improve from 42.89782
196/196 - 33s - loss: 43.2079 - MinusLogProbMetric: 43.2079 - val_loss: 43.6812 - val_MinusLogProbMetric: 43.6812 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 515/1000
2023-10-27 18:04:19.178 
Epoch 515/1000 
	 loss: 43.0763, MinusLogProbMetric: 43.0763, val_loss: 43.5883, val_MinusLogProbMetric: 43.5883

Epoch 515: val_loss did not improve from 42.89782
196/196 - 32s - loss: 43.0763 - MinusLogProbMetric: 43.0763 - val_loss: 43.5883 - val_MinusLogProbMetric: 43.5883 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 516/1000
2023-10-27 18:04:51.527 
Epoch 516/1000 
	 loss: 42.7649, MinusLogProbMetric: 42.7649, val_loss: 44.0784, val_MinusLogProbMetric: 44.0784

Epoch 516: val_loss did not improve from 42.89782
196/196 - 32s - loss: 42.7649 - MinusLogProbMetric: 42.7649 - val_loss: 44.0784 - val_MinusLogProbMetric: 44.0784 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 517/1000
2023-10-27 18:05:24.158 
Epoch 517/1000 
	 loss: 42.6822, MinusLogProbMetric: 42.6822, val_loss: 43.0930, val_MinusLogProbMetric: 43.0930

Epoch 517: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.6822 - MinusLogProbMetric: 42.6822 - val_loss: 43.0930 - val_MinusLogProbMetric: 43.0930 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 518/1000
2023-10-27 18:05:56.892 
Epoch 518/1000 
	 loss: 42.6693, MinusLogProbMetric: 42.6693, val_loss: 43.1973, val_MinusLogProbMetric: 43.1973

Epoch 518: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.6693 - MinusLogProbMetric: 42.6693 - val_loss: 43.1973 - val_MinusLogProbMetric: 43.1973 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 519/1000
2023-10-27 18:06:29.810 
Epoch 519/1000 
	 loss: 42.6002, MinusLogProbMetric: 42.6002, val_loss: 43.2926, val_MinusLogProbMetric: 43.2926

Epoch 519: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.6002 - MinusLogProbMetric: 42.6002 - val_loss: 43.2926 - val_MinusLogProbMetric: 43.2926 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 520/1000
2023-10-27 18:07:02.079 
Epoch 520/1000 
	 loss: 42.6303, MinusLogProbMetric: 42.6303, val_loss: 43.7330, val_MinusLogProbMetric: 43.7330

Epoch 520: val_loss did not improve from 42.89782
196/196 - 32s - loss: 42.6303 - MinusLogProbMetric: 42.6303 - val_loss: 43.7330 - val_MinusLogProbMetric: 43.7330 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 521/1000
2023-10-27 18:07:34.713 
Epoch 521/1000 
	 loss: 42.5340, MinusLogProbMetric: 42.5340, val_loss: 44.0650, val_MinusLogProbMetric: 44.0650

Epoch 521: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5340 - MinusLogProbMetric: 42.5340 - val_loss: 44.0650 - val_MinusLogProbMetric: 44.0650 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 522/1000
2023-10-27 18:08:07.502 
Epoch 522/1000 
	 loss: 42.5196, MinusLogProbMetric: 42.5196, val_loss: 44.1011, val_MinusLogProbMetric: 44.1011

Epoch 522: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5196 - MinusLogProbMetric: 42.5196 - val_loss: 44.1011 - val_MinusLogProbMetric: 44.1011 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 523/1000
2023-10-27 18:08:40.798 
Epoch 523/1000 
	 loss: 42.5227, MinusLogProbMetric: 42.5227, val_loss: 44.1078, val_MinusLogProbMetric: 44.1078

Epoch 523: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5227 - MinusLogProbMetric: 42.5227 - val_loss: 44.1078 - val_MinusLogProbMetric: 44.1078 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 524/1000
2023-10-27 18:09:13.636 
Epoch 524/1000 
	 loss: 42.6049, MinusLogProbMetric: 42.6049, val_loss: 43.0336, val_MinusLogProbMetric: 43.0336

Epoch 524: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.6049 - MinusLogProbMetric: 42.6049 - val_loss: 43.0336 - val_MinusLogProbMetric: 43.0336 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 525/1000
2023-10-27 18:09:46.966 
Epoch 525/1000 
	 loss: 42.4835, MinusLogProbMetric: 42.4835, val_loss: 44.1148, val_MinusLogProbMetric: 44.1148

Epoch 525: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4835 - MinusLogProbMetric: 42.4835 - val_loss: 44.1148 - val_MinusLogProbMetric: 44.1148 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 526/1000
2023-10-27 18:10:19.752 
Epoch 526/1000 
	 loss: 42.4625, MinusLogProbMetric: 42.4625, val_loss: 44.2255, val_MinusLogProbMetric: 44.2255

Epoch 526: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4625 - MinusLogProbMetric: 42.4625 - val_loss: 44.2255 - val_MinusLogProbMetric: 44.2255 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 527/1000
2023-10-27 18:10:53.025 
Epoch 527/1000 
	 loss: 42.5458, MinusLogProbMetric: 42.5458, val_loss: 43.7821, val_MinusLogProbMetric: 43.7821

Epoch 527: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5458 - MinusLogProbMetric: 42.5458 - val_loss: 43.7821 - val_MinusLogProbMetric: 43.7821 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 528/1000
2023-10-27 18:11:26.014 
Epoch 528/1000 
	 loss: 42.5060, MinusLogProbMetric: 42.5060, val_loss: 43.5029, val_MinusLogProbMetric: 43.5029

Epoch 528: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5060 - MinusLogProbMetric: 42.5060 - val_loss: 43.5029 - val_MinusLogProbMetric: 43.5029 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 529/1000
2023-10-27 18:11:59.047 
Epoch 529/1000 
	 loss: 42.5154, MinusLogProbMetric: 42.5154, val_loss: 43.1666, val_MinusLogProbMetric: 43.1666

Epoch 529: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5154 - MinusLogProbMetric: 42.5154 - val_loss: 43.1666 - val_MinusLogProbMetric: 43.1666 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 530/1000
2023-10-27 18:12:31.889 
Epoch 530/1000 
	 loss: 42.4839, MinusLogProbMetric: 42.4839, val_loss: 43.1981, val_MinusLogProbMetric: 43.1981

Epoch 530: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4839 - MinusLogProbMetric: 42.4839 - val_loss: 43.1981 - val_MinusLogProbMetric: 43.1981 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 531/1000
2023-10-27 18:13:04.658 
Epoch 531/1000 
	 loss: 42.4221, MinusLogProbMetric: 42.4221, val_loss: 43.5624, val_MinusLogProbMetric: 43.5624

Epoch 531: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4221 - MinusLogProbMetric: 42.4221 - val_loss: 43.5624 - val_MinusLogProbMetric: 43.5624 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 532/1000
2023-10-27 18:13:37.666 
Epoch 532/1000 
	 loss: 42.4378, MinusLogProbMetric: 42.4378, val_loss: 43.7622, val_MinusLogProbMetric: 43.7622

Epoch 532: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4378 - MinusLogProbMetric: 42.4378 - val_loss: 43.7622 - val_MinusLogProbMetric: 43.7622 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 533/1000
2023-10-27 18:14:10.774 
Epoch 533/1000 
	 loss: 42.5223, MinusLogProbMetric: 42.5223, val_loss: 43.0997, val_MinusLogProbMetric: 43.0997

Epoch 533: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5223 - MinusLogProbMetric: 42.5223 - val_loss: 43.0997 - val_MinusLogProbMetric: 43.0997 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 534/1000
2023-10-27 18:14:44.190 
Epoch 534/1000 
	 loss: 42.4850, MinusLogProbMetric: 42.4850, val_loss: 43.0238, val_MinusLogProbMetric: 43.0238

Epoch 534: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4850 - MinusLogProbMetric: 42.4850 - val_loss: 43.0238 - val_MinusLogProbMetric: 43.0238 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 535/1000
2023-10-27 18:15:17.505 
Epoch 535/1000 
	 loss: 42.5046, MinusLogProbMetric: 42.5046, val_loss: 43.8301, val_MinusLogProbMetric: 43.8301

Epoch 535: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5046 - MinusLogProbMetric: 42.5046 - val_loss: 43.8301 - val_MinusLogProbMetric: 43.8301 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 536/1000
2023-10-27 18:15:51.124 
Epoch 536/1000 
	 loss: 42.4921, MinusLogProbMetric: 42.4921, val_loss: 43.4137, val_MinusLogProbMetric: 43.4137

Epoch 536: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.4921 - MinusLogProbMetric: 42.4921 - val_loss: 43.4137 - val_MinusLogProbMetric: 43.4137 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 537/1000
2023-10-27 18:16:25.048 
Epoch 537/1000 
	 loss: 42.5037, MinusLogProbMetric: 42.5037, val_loss: 43.0568, val_MinusLogProbMetric: 43.0568

Epoch 537: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.5037 - MinusLogProbMetric: 42.5037 - val_loss: 43.0568 - val_MinusLogProbMetric: 43.0568 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 538/1000
2023-10-27 18:17:00.236 
Epoch 538/1000 
	 loss: 42.4402, MinusLogProbMetric: 42.4402, val_loss: 43.7260, val_MinusLogProbMetric: 43.7260

Epoch 538: val_loss did not improve from 42.89782
196/196 - 35s - loss: 42.4402 - MinusLogProbMetric: 42.4402 - val_loss: 43.7260 - val_MinusLogProbMetric: 43.7260 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 539/1000
2023-10-27 18:17:35.231 
Epoch 539/1000 
	 loss: 42.4187, MinusLogProbMetric: 42.4187, val_loss: 43.2716, val_MinusLogProbMetric: 43.2716

Epoch 539: val_loss did not improve from 42.89782
196/196 - 35s - loss: 42.4187 - MinusLogProbMetric: 42.4187 - val_loss: 43.2716 - val_MinusLogProbMetric: 43.2716 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 540/1000
2023-10-27 18:18:10.021 
Epoch 540/1000 
	 loss: 42.6604, MinusLogProbMetric: 42.6604, val_loss: 44.4370, val_MinusLogProbMetric: 44.4370

Epoch 540: val_loss did not improve from 42.89782
196/196 - 35s - loss: 42.6604 - MinusLogProbMetric: 42.6604 - val_loss: 44.4370 - val_MinusLogProbMetric: 44.4370 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 541/1000
2023-10-27 18:18:43.911 
Epoch 541/1000 
	 loss: 42.4236, MinusLogProbMetric: 42.4236, val_loss: 43.4917, val_MinusLogProbMetric: 43.4917

Epoch 541: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.4236 - MinusLogProbMetric: 42.4236 - val_loss: 43.4917 - val_MinusLogProbMetric: 43.4917 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 542/1000
2023-10-27 18:19:17.795 
Epoch 542/1000 
	 loss: 42.3891, MinusLogProbMetric: 42.3891, val_loss: 43.1001, val_MinusLogProbMetric: 43.1001

Epoch 542: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.3891 - MinusLogProbMetric: 42.3891 - val_loss: 43.1001 - val_MinusLogProbMetric: 43.1001 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 543/1000
2023-10-27 18:19:52.007 
Epoch 543/1000 
	 loss: 42.4987, MinusLogProbMetric: 42.4987, val_loss: 43.3410, val_MinusLogProbMetric: 43.3410

Epoch 543: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.4987 - MinusLogProbMetric: 42.4987 - val_loss: 43.3410 - val_MinusLogProbMetric: 43.3410 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 544/1000
2023-10-27 18:20:27.309 
Epoch 544/1000 
	 loss: 42.5436, MinusLogProbMetric: 42.5436, val_loss: 43.7129, val_MinusLogProbMetric: 43.7129

Epoch 544: val_loss did not improve from 42.89782
196/196 - 35s - loss: 42.5436 - MinusLogProbMetric: 42.5436 - val_loss: 43.7129 - val_MinusLogProbMetric: 43.7129 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 545/1000
2023-10-27 18:21:01.391 
Epoch 545/1000 
	 loss: 42.5795, MinusLogProbMetric: 42.5795, val_loss: 43.3356, val_MinusLogProbMetric: 43.3356

Epoch 545: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.5795 - MinusLogProbMetric: 42.5795 - val_loss: 43.3356 - val_MinusLogProbMetric: 43.3356 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 546/1000
2023-10-27 18:21:35.008 
Epoch 546/1000 
	 loss: 42.4584, MinusLogProbMetric: 42.4584, val_loss: 44.2238, val_MinusLogProbMetric: 44.2238

Epoch 546: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.4584 - MinusLogProbMetric: 42.4584 - val_loss: 44.2238 - val_MinusLogProbMetric: 44.2238 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 547/1000
2023-10-27 18:22:08.232 
Epoch 547/1000 
	 loss: 42.4135, MinusLogProbMetric: 42.4135, val_loss: 43.2090, val_MinusLogProbMetric: 43.2090

Epoch 547: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4135 - MinusLogProbMetric: 42.4135 - val_loss: 43.2090 - val_MinusLogProbMetric: 43.2090 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 548/1000
2023-10-27 18:22:41.586 
Epoch 548/1000 
	 loss: 42.5093, MinusLogProbMetric: 42.5093, val_loss: 43.3531, val_MinusLogProbMetric: 43.3531

Epoch 548: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.5093 - MinusLogProbMetric: 42.5093 - val_loss: 43.3531 - val_MinusLogProbMetric: 43.3531 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 549/1000
2023-10-27 18:23:15.649 
Epoch 549/1000 
	 loss: 42.3882, MinusLogProbMetric: 42.3882, val_loss: 43.5171, val_MinusLogProbMetric: 43.5171

Epoch 549: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.3882 - MinusLogProbMetric: 42.3882 - val_loss: 43.5171 - val_MinusLogProbMetric: 43.5171 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 550/1000
2023-10-27 18:23:49.491 
Epoch 550/1000 
	 loss: 42.4385, MinusLogProbMetric: 42.4385, val_loss: 43.1604, val_MinusLogProbMetric: 43.1604

Epoch 550: val_loss did not improve from 42.89782
196/196 - 34s - loss: 42.4385 - MinusLogProbMetric: 42.4385 - val_loss: 43.1604 - val_MinusLogProbMetric: 43.1604 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 551/1000
2023-10-27 18:24:22.784 
Epoch 551/1000 
	 loss: 42.3590, MinusLogProbMetric: 42.3590, val_loss: 43.9548, val_MinusLogProbMetric: 43.9548

Epoch 551: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.3590 - MinusLogProbMetric: 42.3590 - val_loss: 43.9548 - val_MinusLogProbMetric: 43.9548 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 552/1000
2023-10-27 18:24:55.984 
Epoch 552/1000 
	 loss: 42.4765, MinusLogProbMetric: 42.4765, val_loss: 43.6757, val_MinusLogProbMetric: 43.6757

Epoch 552: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4765 - MinusLogProbMetric: 42.4765 - val_loss: 43.6757 - val_MinusLogProbMetric: 43.6757 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 553/1000
2023-10-27 18:25:29.483 
Epoch 553/1000 
	 loss: 42.4414, MinusLogProbMetric: 42.4414, val_loss: 43.1400, val_MinusLogProbMetric: 43.1400

Epoch 553: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4414 - MinusLogProbMetric: 42.4414 - val_loss: 43.1400 - val_MinusLogProbMetric: 43.1400 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 554/1000
2023-10-27 18:26:02.389 
Epoch 554/1000 
	 loss: 42.4004, MinusLogProbMetric: 42.4004, val_loss: 43.2575, val_MinusLogProbMetric: 43.2575

Epoch 554: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.4004 - MinusLogProbMetric: 42.4004 - val_loss: 43.2575 - val_MinusLogProbMetric: 43.2575 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 555/1000
2023-10-27 18:26:35.557 
Epoch 555/1000 
	 loss: 42.6183, MinusLogProbMetric: 42.6183, val_loss: 43.0258, val_MinusLogProbMetric: 43.0258

Epoch 555: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.6183 - MinusLogProbMetric: 42.6183 - val_loss: 43.0258 - val_MinusLogProbMetric: 43.0258 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 556/1000
2023-10-27 18:27:08.710 
Epoch 556/1000 
	 loss: 42.3834, MinusLogProbMetric: 42.3834, val_loss: 43.4661, val_MinusLogProbMetric: 43.4661

Epoch 556: val_loss did not improve from 42.89782
196/196 - 33s - loss: 42.3834 - MinusLogProbMetric: 42.3834 - val_loss: 43.4661 - val_MinusLogProbMetric: 43.4661 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 557/1000
2023-10-27 18:27:42.246 
Epoch 557/1000 
	 loss: 41.6921, MinusLogProbMetric: 41.6921, val_loss: 42.7235, val_MinusLogProbMetric: 42.7235

Epoch 557: val_loss improved from 42.89782 to 42.72353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.6921 - MinusLogProbMetric: 41.6921 - val_loss: 42.7235 - val_MinusLogProbMetric: 42.7235 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 558/1000
2023-10-27 18:28:15.760 
Epoch 558/1000 
	 loss: 41.7618, MinusLogProbMetric: 41.7618, val_loss: 42.6684, val_MinusLogProbMetric: 42.6684

Epoch 558: val_loss improved from 42.72353 to 42.66841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.7618 - MinusLogProbMetric: 41.7618 - val_loss: 42.6684 - val_MinusLogProbMetric: 42.6684 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 559/1000
2023-10-27 18:28:49.382 
Epoch 559/1000 
	 loss: 41.6604, MinusLogProbMetric: 41.6604, val_loss: 42.7528, val_MinusLogProbMetric: 42.7528

Epoch 559: val_loss did not improve from 42.66841
196/196 - 33s - loss: 41.6604 - MinusLogProbMetric: 41.6604 - val_loss: 42.7528 - val_MinusLogProbMetric: 42.7528 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 560/1000
2023-10-27 18:29:22.881 
Epoch 560/1000 
	 loss: 41.7079, MinusLogProbMetric: 41.7079, val_loss: 42.6345, val_MinusLogProbMetric: 42.6345

Epoch 560: val_loss improved from 42.66841 to 42.63452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.7079 - MinusLogProbMetric: 41.7079 - val_loss: 42.6345 - val_MinusLogProbMetric: 42.6345 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 561/1000
2023-10-27 18:29:56.792 
Epoch 561/1000 
	 loss: 41.7855, MinusLogProbMetric: 41.7855, val_loss: 42.5704, val_MinusLogProbMetric: 42.5704

Epoch 561: val_loss improved from 42.63452 to 42.57037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.7855 - MinusLogProbMetric: 41.7855 - val_loss: 42.5704 - val_MinusLogProbMetric: 42.5704 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 562/1000
2023-10-27 18:30:30.716 
Epoch 562/1000 
	 loss: 41.8147, MinusLogProbMetric: 41.8147, val_loss: 42.6397, val_MinusLogProbMetric: 42.6397

Epoch 562: val_loss did not improve from 42.57037
196/196 - 33s - loss: 41.8147 - MinusLogProbMetric: 41.8147 - val_loss: 42.6397 - val_MinusLogProbMetric: 42.6397 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 563/1000
2023-10-27 18:31:03.963 
Epoch 563/1000 
	 loss: 41.7139, MinusLogProbMetric: 41.7139, val_loss: 42.7918, val_MinusLogProbMetric: 42.7918

Epoch 563: val_loss did not improve from 42.57037
196/196 - 33s - loss: 41.7139 - MinusLogProbMetric: 41.7139 - val_loss: 42.7918 - val_MinusLogProbMetric: 42.7918 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 564/1000
2023-10-27 18:31:37.335 
Epoch 564/1000 
	 loss: 41.7303, MinusLogProbMetric: 41.7303, val_loss: 43.3202, val_MinusLogProbMetric: 43.3202

Epoch 564: val_loss did not improve from 42.57037
196/196 - 33s - loss: 41.7303 - MinusLogProbMetric: 41.7303 - val_loss: 43.3202 - val_MinusLogProbMetric: 43.3202 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 565/1000
2023-10-27 18:32:11.743 
Epoch 565/1000 
	 loss: 41.7574, MinusLogProbMetric: 41.7574, val_loss: 42.5737, val_MinusLogProbMetric: 42.5737

Epoch 565: val_loss did not improve from 42.57037
196/196 - 34s - loss: 41.7574 - MinusLogProbMetric: 41.7574 - val_loss: 42.5737 - val_MinusLogProbMetric: 42.5737 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 566/1000
2023-10-27 18:32:45.337 
Epoch 566/1000 
	 loss: 41.7255, MinusLogProbMetric: 41.7255, val_loss: 43.6365, val_MinusLogProbMetric: 43.6365

Epoch 566: val_loss did not improve from 42.57037
196/196 - 34s - loss: 41.7255 - MinusLogProbMetric: 41.7255 - val_loss: 43.6365 - val_MinusLogProbMetric: 43.6365 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 567/1000
2023-10-27 18:33:18.773 
Epoch 567/1000 
	 loss: 41.7989, MinusLogProbMetric: 41.7989, val_loss: 42.4496, val_MinusLogProbMetric: 42.4496

Epoch 567: val_loss improved from 42.57037 to 42.44958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.7989 - MinusLogProbMetric: 41.7989 - val_loss: 42.4496 - val_MinusLogProbMetric: 42.4496 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 568/1000
2023-10-27 18:33:52.942 
Epoch 568/1000 
	 loss: 41.7128, MinusLogProbMetric: 41.7128, val_loss: 42.5859, val_MinusLogProbMetric: 42.5859

Epoch 568: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.7128 - MinusLogProbMetric: 41.7128 - val_loss: 42.5859 - val_MinusLogProbMetric: 42.5859 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 569/1000
2023-10-27 18:34:25.709 
Epoch 569/1000 
	 loss: 41.7500, MinusLogProbMetric: 41.7500, val_loss: 42.5189, val_MinusLogProbMetric: 42.5189

Epoch 569: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7500 - MinusLogProbMetric: 41.7500 - val_loss: 42.5189 - val_MinusLogProbMetric: 42.5189 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 570/1000
2023-10-27 18:34:58.337 
Epoch 570/1000 
	 loss: 41.7154, MinusLogProbMetric: 41.7154, val_loss: 43.4794, val_MinusLogProbMetric: 43.4794

Epoch 570: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7154 - MinusLogProbMetric: 41.7154 - val_loss: 43.4794 - val_MinusLogProbMetric: 43.4794 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 571/1000
2023-10-27 18:35:31.199 
Epoch 571/1000 
	 loss: 41.7470, MinusLogProbMetric: 41.7470, val_loss: 42.9457, val_MinusLogProbMetric: 42.9457

Epoch 571: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7470 - MinusLogProbMetric: 41.7470 - val_loss: 42.9457 - val_MinusLogProbMetric: 42.9457 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 572/1000
2023-10-27 18:36:04.144 
Epoch 572/1000 
	 loss: 41.7628, MinusLogProbMetric: 41.7628, val_loss: 42.5491, val_MinusLogProbMetric: 42.5491

Epoch 572: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7628 - MinusLogProbMetric: 41.7628 - val_loss: 42.5491 - val_MinusLogProbMetric: 42.5491 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 573/1000
2023-10-27 18:36:37.040 
Epoch 573/1000 
	 loss: 41.7020, MinusLogProbMetric: 41.7020, val_loss: 42.6788, val_MinusLogProbMetric: 42.6788

Epoch 573: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7020 - MinusLogProbMetric: 41.7020 - val_loss: 42.6788 - val_MinusLogProbMetric: 42.6788 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 574/1000
2023-10-27 18:37:10.010 
Epoch 574/1000 
	 loss: 41.7386, MinusLogProbMetric: 41.7386, val_loss: 44.4337, val_MinusLogProbMetric: 44.4337

Epoch 574: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7386 - MinusLogProbMetric: 41.7386 - val_loss: 44.4337 - val_MinusLogProbMetric: 44.4337 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 575/1000
2023-10-27 18:37:42.942 
Epoch 575/1000 
	 loss: 41.6985, MinusLogProbMetric: 41.6985, val_loss: 42.7317, val_MinusLogProbMetric: 42.7317

Epoch 575: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6985 - MinusLogProbMetric: 41.6985 - val_loss: 42.7317 - val_MinusLogProbMetric: 42.7317 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 576/1000
2023-10-27 18:38:15.797 
Epoch 576/1000 
	 loss: 41.7433, MinusLogProbMetric: 41.7433, val_loss: 42.5166, val_MinusLogProbMetric: 42.5166

Epoch 576: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7433 - MinusLogProbMetric: 41.7433 - val_loss: 42.5166 - val_MinusLogProbMetric: 42.5166 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 577/1000
2023-10-27 18:38:48.654 
Epoch 577/1000 
	 loss: 41.7652, MinusLogProbMetric: 41.7652, val_loss: 42.9724, val_MinusLogProbMetric: 42.9724

Epoch 577: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7652 - MinusLogProbMetric: 41.7652 - val_loss: 42.9724 - val_MinusLogProbMetric: 42.9724 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 578/1000
2023-10-27 18:39:22.162 
Epoch 578/1000 
	 loss: 41.7108, MinusLogProbMetric: 41.7108, val_loss: 42.7011, val_MinusLogProbMetric: 42.7011

Epoch 578: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.7108 - MinusLogProbMetric: 41.7108 - val_loss: 42.7011 - val_MinusLogProbMetric: 42.7011 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 579/1000
2023-10-27 18:39:55.281 
Epoch 579/1000 
	 loss: 41.6986, MinusLogProbMetric: 41.6986, val_loss: 42.9111, val_MinusLogProbMetric: 42.9111

Epoch 579: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6986 - MinusLogProbMetric: 41.6986 - val_loss: 42.9111 - val_MinusLogProbMetric: 42.9111 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 580/1000
2023-10-27 18:40:28.242 
Epoch 580/1000 
	 loss: 41.7193, MinusLogProbMetric: 41.7193, val_loss: 42.6722, val_MinusLogProbMetric: 42.6722

Epoch 580: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7193 - MinusLogProbMetric: 41.7193 - val_loss: 42.6722 - val_MinusLogProbMetric: 42.6722 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 581/1000
2023-10-27 18:41:01.146 
Epoch 581/1000 
	 loss: 41.7779, MinusLogProbMetric: 41.7779, val_loss: 42.6910, val_MinusLogProbMetric: 42.6910

Epoch 581: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7779 - MinusLogProbMetric: 41.7779 - val_loss: 42.6910 - val_MinusLogProbMetric: 42.6910 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 582/1000
2023-10-27 18:41:33.923 
Epoch 582/1000 
	 loss: 41.7031, MinusLogProbMetric: 41.7031, val_loss: 43.1273, val_MinusLogProbMetric: 43.1273

Epoch 582: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7031 - MinusLogProbMetric: 41.7031 - val_loss: 43.1273 - val_MinusLogProbMetric: 43.1273 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 583/1000
2023-10-27 18:42:06.557 
Epoch 583/1000 
	 loss: 41.7470, MinusLogProbMetric: 41.7470, val_loss: 43.1142, val_MinusLogProbMetric: 43.1142

Epoch 583: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7470 - MinusLogProbMetric: 41.7470 - val_loss: 43.1142 - val_MinusLogProbMetric: 43.1142 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 584/1000
2023-10-27 18:42:39.862 
Epoch 584/1000 
	 loss: 41.6645, MinusLogProbMetric: 41.6645, val_loss: 42.6839, val_MinusLogProbMetric: 42.6839

Epoch 584: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6645 - MinusLogProbMetric: 41.6645 - val_loss: 42.6839 - val_MinusLogProbMetric: 42.6839 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 585/1000
2023-10-27 18:43:13.651 
Epoch 585/1000 
	 loss: 41.7409, MinusLogProbMetric: 41.7409, val_loss: 42.7026, val_MinusLogProbMetric: 42.7026

Epoch 585: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.7409 - MinusLogProbMetric: 41.7409 - val_loss: 42.7026 - val_MinusLogProbMetric: 42.7026 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 586/1000
2023-10-27 18:43:47.327 
Epoch 586/1000 
	 loss: 41.7496, MinusLogProbMetric: 41.7496, val_loss: 43.4317, val_MinusLogProbMetric: 43.4317

Epoch 586: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.7496 - MinusLogProbMetric: 41.7496 - val_loss: 43.4317 - val_MinusLogProbMetric: 43.4317 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 587/1000
2023-10-27 18:44:20.954 
Epoch 587/1000 
	 loss: 41.6984, MinusLogProbMetric: 41.6984, val_loss: 42.6196, val_MinusLogProbMetric: 42.6196

Epoch 587: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.6984 - MinusLogProbMetric: 41.6984 - val_loss: 42.6196 - val_MinusLogProbMetric: 42.6196 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 588/1000
2023-10-27 18:44:55.116 
Epoch 588/1000 
	 loss: 41.8125, MinusLogProbMetric: 41.8125, val_loss: 42.5884, val_MinusLogProbMetric: 42.5884

Epoch 588: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.8125 - MinusLogProbMetric: 41.8125 - val_loss: 42.5884 - val_MinusLogProbMetric: 42.5884 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 589/1000
2023-10-27 18:45:28.147 
Epoch 589/1000 
	 loss: 41.7042, MinusLogProbMetric: 41.7042, val_loss: 42.9499, val_MinusLogProbMetric: 42.9499

Epoch 589: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7042 - MinusLogProbMetric: 41.7042 - val_loss: 42.9499 - val_MinusLogProbMetric: 42.9499 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 590/1000
2023-10-27 18:46:01.945 
Epoch 590/1000 
	 loss: 41.6530, MinusLogProbMetric: 41.6530, val_loss: 42.7763, val_MinusLogProbMetric: 42.7763

Epoch 590: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.6530 - MinusLogProbMetric: 41.6530 - val_loss: 42.7763 - val_MinusLogProbMetric: 42.7763 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 591/1000
2023-10-27 18:46:35.492 
Epoch 591/1000 
	 loss: 41.7576, MinusLogProbMetric: 41.7576, val_loss: 42.8952, val_MinusLogProbMetric: 42.8952

Epoch 591: val_loss did not improve from 42.44958
196/196 - 34s - loss: 41.7576 - MinusLogProbMetric: 41.7576 - val_loss: 42.8952 - val_MinusLogProbMetric: 42.8952 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 592/1000
2023-10-27 18:47:08.448 
Epoch 592/1000 
	 loss: 41.6897, MinusLogProbMetric: 41.6897, val_loss: 42.4998, val_MinusLogProbMetric: 42.4998

Epoch 592: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6897 - MinusLogProbMetric: 41.6897 - val_loss: 42.4998 - val_MinusLogProbMetric: 42.4998 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 593/1000
2023-10-27 18:47:41.567 
Epoch 593/1000 
	 loss: 41.7527, MinusLogProbMetric: 41.7527, val_loss: 42.8091, val_MinusLogProbMetric: 42.8091

Epoch 593: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7527 - MinusLogProbMetric: 41.7527 - val_loss: 42.8091 - val_MinusLogProbMetric: 42.8091 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 594/1000
2023-10-27 18:48:14.238 
Epoch 594/1000 
	 loss: 41.6916, MinusLogProbMetric: 41.6916, val_loss: 42.4805, val_MinusLogProbMetric: 42.4805

Epoch 594: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6916 - MinusLogProbMetric: 41.6916 - val_loss: 42.4805 - val_MinusLogProbMetric: 42.4805 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 595/1000
2023-10-27 18:48:47.154 
Epoch 595/1000 
	 loss: 41.6720, MinusLogProbMetric: 41.6720, val_loss: 42.7508, val_MinusLogProbMetric: 42.7508

Epoch 595: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6720 - MinusLogProbMetric: 41.6720 - val_loss: 42.7508 - val_MinusLogProbMetric: 42.7508 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 596/1000
2023-10-27 18:49:20.003 
Epoch 596/1000 
	 loss: 41.6533, MinusLogProbMetric: 41.6533, val_loss: 42.8933, val_MinusLogProbMetric: 42.8933

Epoch 596: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6533 - MinusLogProbMetric: 41.6533 - val_loss: 42.8933 - val_MinusLogProbMetric: 42.8933 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 597/1000
2023-10-27 18:49:52.617 
Epoch 597/1000 
	 loss: 41.7245, MinusLogProbMetric: 41.7245, val_loss: 42.4908, val_MinusLogProbMetric: 42.4908

Epoch 597: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.7245 - MinusLogProbMetric: 41.7245 - val_loss: 42.4908 - val_MinusLogProbMetric: 42.4908 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 598/1000
2023-10-27 18:50:25.653 
Epoch 598/1000 
	 loss: 41.6480, MinusLogProbMetric: 41.6480, val_loss: 43.1073, val_MinusLogProbMetric: 43.1073

Epoch 598: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6480 - MinusLogProbMetric: 41.6480 - val_loss: 43.1073 - val_MinusLogProbMetric: 43.1073 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 599/1000
2023-10-27 18:50:58.933 
Epoch 599/1000 
	 loss: 42.1596, MinusLogProbMetric: 42.1596, val_loss: 42.6254, val_MinusLogProbMetric: 42.6254

Epoch 599: val_loss did not improve from 42.44958
196/196 - 33s - loss: 42.1596 - MinusLogProbMetric: 42.1596 - val_loss: 42.6254 - val_MinusLogProbMetric: 42.6254 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 600/1000
2023-10-27 18:51:31.981 
Epoch 600/1000 
	 loss: 41.5524, MinusLogProbMetric: 41.5524, val_loss: 42.4924, val_MinusLogProbMetric: 42.4924

Epoch 600: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.5524 - MinusLogProbMetric: 41.5524 - val_loss: 42.4924 - val_MinusLogProbMetric: 42.4924 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 601/1000
2023-10-27 18:52:05.246 
Epoch 601/1000 
	 loss: 41.6371, MinusLogProbMetric: 41.6371, val_loss: 42.8204, val_MinusLogProbMetric: 42.8204

Epoch 601: val_loss did not improve from 42.44958
196/196 - 33s - loss: 41.6371 - MinusLogProbMetric: 41.6371 - val_loss: 42.8204 - val_MinusLogProbMetric: 42.8204 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 602/1000
2023-10-27 18:52:38.800 
Epoch 602/1000 
	 loss: 41.6287, MinusLogProbMetric: 41.6287, val_loss: 42.3224, val_MinusLogProbMetric: 42.3224

Epoch 602: val_loss improved from 42.44958 to 42.32242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.6287 - MinusLogProbMetric: 41.6287 - val_loss: 42.3224 - val_MinusLogProbMetric: 42.3224 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 603/1000
2023-10-27 18:53:12.613 
Epoch 603/1000 
	 loss: 41.7742, MinusLogProbMetric: 41.7742, val_loss: 42.6939, val_MinusLogProbMetric: 42.6939

Epoch 603: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7742 - MinusLogProbMetric: 41.7742 - val_loss: 42.6939 - val_MinusLogProbMetric: 42.6939 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 604/1000
2023-10-27 18:53:45.663 
Epoch 604/1000 
	 loss: 41.6782, MinusLogProbMetric: 41.6782, val_loss: 42.6738, val_MinusLogProbMetric: 42.6738

Epoch 604: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6782 - MinusLogProbMetric: 41.6782 - val_loss: 42.6738 - val_MinusLogProbMetric: 42.6738 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 605/1000
2023-10-27 18:54:18.746 
Epoch 605/1000 
	 loss: 41.6960, MinusLogProbMetric: 41.6960, val_loss: 43.1085, val_MinusLogProbMetric: 43.1085

Epoch 605: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6960 - MinusLogProbMetric: 41.6960 - val_loss: 43.1085 - val_MinusLogProbMetric: 43.1085 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 606/1000
2023-10-27 18:54:51.323 
Epoch 606/1000 
	 loss: 41.7065, MinusLogProbMetric: 41.7065, val_loss: 42.6329, val_MinusLogProbMetric: 42.6329

Epoch 606: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7065 - MinusLogProbMetric: 41.7065 - val_loss: 42.6329 - val_MinusLogProbMetric: 42.6329 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 607/1000
2023-10-27 18:55:24.237 
Epoch 607/1000 
	 loss: 41.7363, MinusLogProbMetric: 41.7363, val_loss: 43.3722, val_MinusLogProbMetric: 43.3722

Epoch 607: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7363 - MinusLogProbMetric: 41.7363 - val_loss: 43.3722 - val_MinusLogProbMetric: 43.3722 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 608/1000
2023-10-27 18:55:56.534 
Epoch 608/1000 
	 loss: 41.6754, MinusLogProbMetric: 41.6754, val_loss: 42.9672, val_MinusLogProbMetric: 42.9672

Epoch 608: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.6754 - MinusLogProbMetric: 41.6754 - val_loss: 42.9672 - val_MinusLogProbMetric: 42.9672 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 609/1000
2023-10-27 18:56:29.765 
Epoch 609/1000 
	 loss: 41.7180, MinusLogProbMetric: 41.7180, val_loss: 42.6602, val_MinusLogProbMetric: 42.6602

Epoch 609: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7180 - MinusLogProbMetric: 41.7180 - val_loss: 42.6602 - val_MinusLogProbMetric: 42.6602 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 610/1000
2023-10-27 18:57:02.112 
Epoch 610/1000 
	 loss: 41.6253, MinusLogProbMetric: 41.6253, val_loss: 42.5794, val_MinusLogProbMetric: 42.5794

Epoch 610: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.6253 - MinusLogProbMetric: 41.6253 - val_loss: 42.5794 - val_MinusLogProbMetric: 42.5794 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 611/1000
2023-10-27 18:57:34.913 
Epoch 611/1000 
	 loss: 41.6650, MinusLogProbMetric: 41.6650, val_loss: 42.4167, val_MinusLogProbMetric: 42.4167

Epoch 611: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6650 - MinusLogProbMetric: 41.6650 - val_loss: 42.4167 - val_MinusLogProbMetric: 42.4167 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 612/1000
2023-10-27 18:58:07.996 
Epoch 612/1000 
	 loss: 41.7213, MinusLogProbMetric: 41.7213, val_loss: 42.8666, val_MinusLogProbMetric: 42.8666

Epoch 612: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7213 - MinusLogProbMetric: 41.7213 - val_loss: 42.8666 - val_MinusLogProbMetric: 42.8666 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 613/1000
2023-10-27 18:58:41.282 
Epoch 613/1000 
	 loss: 41.6725, MinusLogProbMetric: 41.6725, val_loss: 42.6048, val_MinusLogProbMetric: 42.6048

Epoch 613: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6725 - MinusLogProbMetric: 41.6725 - val_loss: 42.6048 - val_MinusLogProbMetric: 42.6048 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 614/1000
2023-10-27 18:59:14.278 
Epoch 614/1000 
	 loss: 41.7122, MinusLogProbMetric: 41.7122, val_loss: 42.4811, val_MinusLogProbMetric: 42.4811

Epoch 614: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7122 - MinusLogProbMetric: 41.7122 - val_loss: 42.4811 - val_MinusLogProbMetric: 42.4811 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 615/1000
2023-10-27 18:59:47.135 
Epoch 615/1000 
	 loss: 41.6948, MinusLogProbMetric: 41.6948, val_loss: 42.6598, val_MinusLogProbMetric: 42.6598

Epoch 615: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6948 - MinusLogProbMetric: 41.6948 - val_loss: 42.6598 - val_MinusLogProbMetric: 42.6598 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 616/1000
2023-10-27 19:00:19.633 
Epoch 616/1000 
	 loss: 41.9116, MinusLogProbMetric: 41.9116, val_loss: 42.5972, val_MinusLogProbMetric: 42.5972

Epoch 616: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.9116 - MinusLogProbMetric: 41.9116 - val_loss: 42.5972 - val_MinusLogProbMetric: 42.5972 - lr: 8.3333e-05 - 32s/epoch - 166ms/step
Epoch 617/1000
2023-10-27 19:00:52.806 
Epoch 617/1000 
	 loss: 41.5784, MinusLogProbMetric: 41.5784, val_loss: 42.5188, val_MinusLogProbMetric: 42.5188

Epoch 617: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.5784 - MinusLogProbMetric: 41.5784 - val_loss: 42.5188 - val_MinusLogProbMetric: 42.5188 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 618/1000
2023-10-27 19:01:24.862 
Epoch 618/1000 
	 loss: 41.7054, MinusLogProbMetric: 41.7054, val_loss: 42.6324, val_MinusLogProbMetric: 42.6324

Epoch 618: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.7054 - MinusLogProbMetric: 41.7054 - val_loss: 42.6324 - val_MinusLogProbMetric: 42.6324 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 619/1000
2023-10-27 19:01:58.486 
Epoch 619/1000 
	 loss: 41.6735, MinusLogProbMetric: 41.6735, val_loss: 42.9220, val_MinusLogProbMetric: 42.9220

Epoch 619: val_loss did not improve from 42.32242
196/196 - 34s - loss: 41.6735 - MinusLogProbMetric: 41.6735 - val_loss: 42.9220 - val_MinusLogProbMetric: 42.9220 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 620/1000
2023-10-27 19:02:31.805 
Epoch 620/1000 
	 loss: 41.7192, MinusLogProbMetric: 41.7192, val_loss: 43.0528, val_MinusLogProbMetric: 43.0528

Epoch 620: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7192 - MinusLogProbMetric: 41.7192 - val_loss: 43.0528 - val_MinusLogProbMetric: 43.0528 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 621/1000
2023-10-27 19:03:05.331 
Epoch 621/1000 
	 loss: 41.6199, MinusLogProbMetric: 41.6199, val_loss: 43.5086, val_MinusLogProbMetric: 43.5086

Epoch 621: val_loss did not improve from 42.32242
196/196 - 34s - loss: 41.6199 - MinusLogProbMetric: 41.6199 - val_loss: 43.5086 - val_MinusLogProbMetric: 43.5086 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 622/1000
2023-10-27 19:03:38.590 
Epoch 622/1000 
	 loss: 41.6869, MinusLogProbMetric: 41.6869, val_loss: 43.0741, val_MinusLogProbMetric: 43.0741

Epoch 622: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6869 - MinusLogProbMetric: 41.6869 - val_loss: 43.0741 - val_MinusLogProbMetric: 43.0741 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 623/1000
2023-10-27 19:04:11.602 
Epoch 623/1000 
	 loss: 41.6518, MinusLogProbMetric: 41.6518, val_loss: 42.5391, val_MinusLogProbMetric: 42.5391

Epoch 623: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6518 - MinusLogProbMetric: 41.6518 - val_loss: 42.5391 - val_MinusLogProbMetric: 42.5391 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 624/1000
2023-10-27 19:04:44.911 
Epoch 624/1000 
	 loss: 41.6323, MinusLogProbMetric: 41.6323, val_loss: 42.4947, val_MinusLogProbMetric: 42.4947

Epoch 624: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6323 - MinusLogProbMetric: 41.6323 - val_loss: 42.4947 - val_MinusLogProbMetric: 42.4947 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 625/1000
2023-10-27 19:05:17.933 
Epoch 625/1000 
	 loss: 41.6579, MinusLogProbMetric: 41.6579, val_loss: 42.9364, val_MinusLogProbMetric: 42.9364

Epoch 625: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6579 - MinusLogProbMetric: 41.6579 - val_loss: 42.9364 - val_MinusLogProbMetric: 42.9364 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 626/1000
2023-10-27 19:05:51.809 
Epoch 626/1000 
	 loss: 41.6122, MinusLogProbMetric: 41.6122, val_loss: 42.6390, val_MinusLogProbMetric: 42.6390

Epoch 626: val_loss did not improve from 42.32242
196/196 - 34s - loss: 41.6122 - MinusLogProbMetric: 41.6122 - val_loss: 42.6390 - val_MinusLogProbMetric: 42.6390 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 627/1000
2023-10-27 19:06:25.079 
Epoch 627/1000 
	 loss: 41.6641, MinusLogProbMetric: 41.6641, val_loss: 42.6318, val_MinusLogProbMetric: 42.6318

Epoch 627: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6641 - MinusLogProbMetric: 41.6641 - val_loss: 42.6318 - val_MinusLogProbMetric: 42.6318 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 628/1000
2023-10-27 19:06:57.813 
Epoch 628/1000 
	 loss: 41.6884, MinusLogProbMetric: 41.6884, val_loss: 42.5157, val_MinusLogProbMetric: 42.5157

Epoch 628: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6884 - MinusLogProbMetric: 41.6884 - val_loss: 42.5157 - val_MinusLogProbMetric: 42.5157 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 629/1000
2023-10-27 19:07:30.734 
Epoch 629/1000 
	 loss: 41.7344, MinusLogProbMetric: 41.7344, val_loss: 43.0200, val_MinusLogProbMetric: 43.0200

Epoch 629: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7344 - MinusLogProbMetric: 41.7344 - val_loss: 43.0200 - val_MinusLogProbMetric: 43.0200 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 630/1000
2023-10-27 19:08:03.253 
Epoch 630/1000 
	 loss: 41.6836, MinusLogProbMetric: 41.6836, val_loss: 42.6078, val_MinusLogProbMetric: 42.6078

Epoch 630: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6836 - MinusLogProbMetric: 41.6836 - val_loss: 42.6078 - val_MinusLogProbMetric: 42.6078 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 631/1000
2023-10-27 19:08:36.567 
Epoch 631/1000 
	 loss: 41.6766, MinusLogProbMetric: 41.6766, val_loss: 43.1341, val_MinusLogProbMetric: 43.1341

Epoch 631: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6766 - MinusLogProbMetric: 41.6766 - val_loss: 43.1341 - val_MinusLogProbMetric: 43.1341 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 632/1000
2023-10-27 19:09:09.002 
Epoch 632/1000 
	 loss: 41.6174, MinusLogProbMetric: 41.6174, val_loss: 43.7912, val_MinusLogProbMetric: 43.7912

Epoch 632: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.6174 - MinusLogProbMetric: 41.6174 - val_loss: 43.7912 - val_MinusLogProbMetric: 43.7912 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 633/1000
2023-10-27 19:09:41.622 
Epoch 633/1000 
	 loss: 41.6716, MinusLogProbMetric: 41.6716, val_loss: 42.9762, val_MinusLogProbMetric: 42.9762

Epoch 633: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6716 - MinusLogProbMetric: 41.6716 - val_loss: 42.9762 - val_MinusLogProbMetric: 42.9762 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 634/1000
2023-10-27 19:10:14.380 
Epoch 634/1000 
	 loss: 41.7107, MinusLogProbMetric: 41.7107, val_loss: 42.7042, val_MinusLogProbMetric: 42.7042

Epoch 634: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7107 - MinusLogProbMetric: 41.7107 - val_loss: 42.7042 - val_MinusLogProbMetric: 42.7042 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 635/1000
2023-10-27 19:10:47.025 
Epoch 635/1000 
	 loss: 41.6520, MinusLogProbMetric: 41.6520, val_loss: 42.4858, val_MinusLogProbMetric: 42.4858

Epoch 635: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6520 - MinusLogProbMetric: 41.6520 - val_loss: 42.4858 - val_MinusLogProbMetric: 42.4858 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 636/1000
2023-10-27 19:11:20.365 
Epoch 636/1000 
	 loss: 41.6885, MinusLogProbMetric: 41.6885, val_loss: 42.5852, val_MinusLogProbMetric: 42.5852

Epoch 636: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6885 - MinusLogProbMetric: 41.6885 - val_loss: 42.5852 - val_MinusLogProbMetric: 42.5852 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 637/1000
2023-10-27 19:11:53.566 
Epoch 637/1000 
	 loss: 41.7048, MinusLogProbMetric: 41.7048, val_loss: 42.7297, val_MinusLogProbMetric: 42.7297

Epoch 637: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7048 - MinusLogProbMetric: 41.7048 - val_loss: 42.7297 - val_MinusLogProbMetric: 42.7297 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 638/1000
2023-10-27 19:12:26.468 
Epoch 638/1000 
	 loss: 41.6403, MinusLogProbMetric: 41.6403, val_loss: 42.6496, val_MinusLogProbMetric: 42.6496

Epoch 638: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6403 - MinusLogProbMetric: 41.6403 - val_loss: 42.6496 - val_MinusLogProbMetric: 42.6496 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 639/1000
2023-10-27 19:13:00.245 
Epoch 639/1000 
	 loss: 41.7697, MinusLogProbMetric: 41.7697, val_loss: 43.0658, val_MinusLogProbMetric: 43.0658

Epoch 639: val_loss did not improve from 42.32242
196/196 - 34s - loss: 41.7697 - MinusLogProbMetric: 41.7697 - val_loss: 43.0658 - val_MinusLogProbMetric: 43.0658 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 640/1000
2023-10-27 19:13:33.446 
Epoch 640/1000 
	 loss: 41.6305, MinusLogProbMetric: 41.6305, val_loss: 43.0427, val_MinusLogProbMetric: 43.0427

Epoch 640: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6305 - MinusLogProbMetric: 41.6305 - val_loss: 43.0427 - val_MinusLogProbMetric: 43.0427 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 641/1000
2023-10-27 19:14:06.470 
Epoch 641/1000 
	 loss: 41.6347, MinusLogProbMetric: 41.6347, val_loss: 42.6412, val_MinusLogProbMetric: 42.6412

Epoch 641: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6347 - MinusLogProbMetric: 41.6347 - val_loss: 42.6412 - val_MinusLogProbMetric: 42.6412 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 642/1000
2023-10-27 19:14:38.466 
Epoch 642/1000 
	 loss: 41.7036, MinusLogProbMetric: 41.7036, val_loss: 42.7690, val_MinusLogProbMetric: 42.7690

Epoch 642: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.7036 - MinusLogProbMetric: 41.7036 - val_loss: 42.7690 - val_MinusLogProbMetric: 42.7690 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 643/1000
2023-10-27 19:15:10.840 
Epoch 643/1000 
	 loss: 41.6283, MinusLogProbMetric: 41.6283, val_loss: 42.4670, val_MinusLogProbMetric: 42.4670

Epoch 643: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.6283 - MinusLogProbMetric: 41.6283 - val_loss: 42.4670 - val_MinusLogProbMetric: 42.4670 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 644/1000
2023-10-27 19:15:44.302 
Epoch 644/1000 
	 loss: 41.6498, MinusLogProbMetric: 41.6498, val_loss: 42.5537, val_MinusLogProbMetric: 42.5537

Epoch 644: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6498 - MinusLogProbMetric: 41.6498 - val_loss: 42.5537 - val_MinusLogProbMetric: 42.5537 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 645/1000
2023-10-27 19:16:17.237 
Epoch 645/1000 
	 loss: 41.6537, MinusLogProbMetric: 41.6537, val_loss: 43.1390, val_MinusLogProbMetric: 43.1390

Epoch 645: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.6537 - MinusLogProbMetric: 41.6537 - val_loss: 43.1390 - val_MinusLogProbMetric: 43.1390 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 646/1000
2023-10-27 19:16:50.218 
Epoch 646/1000 
	 loss: 41.5975, MinusLogProbMetric: 41.5975, val_loss: 42.5400, val_MinusLogProbMetric: 42.5400

Epoch 646: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.5975 - MinusLogProbMetric: 41.5975 - val_loss: 42.5400 - val_MinusLogProbMetric: 42.5400 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 647/1000
2023-10-27 19:17:23.391 
Epoch 647/1000 
	 loss: 41.7317, MinusLogProbMetric: 41.7317, val_loss: 43.4197, val_MinusLogProbMetric: 43.4197

Epoch 647: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.7317 - MinusLogProbMetric: 41.7317 - val_loss: 43.4197 - val_MinusLogProbMetric: 43.4197 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 648/1000
2023-10-27 19:17:56.351 
Epoch 648/1000 
	 loss: 41.8798, MinusLogProbMetric: 41.8798, val_loss: 42.5592, val_MinusLogProbMetric: 42.5592

Epoch 648: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.8798 - MinusLogProbMetric: 41.8798 - val_loss: 42.5592 - val_MinusLogProbMetric: 42.5592 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 649/1000
2023-10-27 19:18:29.252 
Epoch 649/1000 
	 loss: 41.5961, MinusLogProbMetric: 41.5961, val_loss: 42.5404, val_MinusLogProbMetric: 42.5404

Epoch 649: val_loss did not improve from 42.32242
196/196 - 33s - loss: 41.5961 - MinusLogProbMetric: 41.5961 - val_loss: 42.5404 - val_MinusLogProbMetric: 42.5404 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 650/1000
2023-10-27 19:19:01.737 
Epoch 650/1000 
	 loss: 41.6099, MinusLogProbMetric: 41.6099, val_loss: 42.4529, val_MinusLogProbMetric: 42.4529

Epoch 650: val_loss did not improve from 42.32242
196/196 - 32s - loss: 41.6099 - MinusLogProbMetric: 41.6099 - val_loss: 42.4529 - val_MinusLogProbMetric: 42.4529 - lr: 8.3333e-05 - 32s/epoch - 166ms/step
Epoch 651/1000
2023-10-27 19:19:35.649 
Epoch 651/1000 
	 loss: 41.6873, MinusLogProbMetric: 41.6873, val_loss: 43.0193, val_MinusLogProbMetric: 43.0193

Epoch 651: val_loss did not improve from 42.32242
196/196 - 34s - loss: 41.6873 - MinusLogProbMetric: 41.6873 - val_loss: 43.0193 - val_MinusLogProbMetric: 43.0193 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 652/1000
2023-10-27 19:20:09.849 
Epoch 652/1000 
	 loss: 41.5671, MinusLogProbMetric: 41.5671, val_loss: 42.4058, val_MinusLogProbMetric: 42.4058

Epoch 652: val_loss did not improve from 42.32242
196/196 - 34s - loss: 41.5671 - MinusLogProbMetric: 41.5671 - val_loss: 42.4058 - val_MinusLogProbMetric: 42.4058 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 653/1000
2023-10-27 19:20:44.912 
Epoch 653/1000 
	 loss: 41.2537, MinusLogProbMetric: 41.2537, val_loss: 42.5146, val_MinusLogProbMetric: 42.5146

Epoch 653: val_loss did not improve from 42.32242
196/196 - 35s - loss: 41.2537 - MinusLogProbMetric: 41.2537 - val_loss: 42.5146 - val_MinusLogProbMetric: 42.5146 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 654/1000
2023-10-27 19:21:19.400 
Epoch 654/1000 
	 loss: 41.2400, MinusLogProbMetric: 41.2400, val_loss: 42.3110, val_MinusLogProbMetric: 42.3110

Epoch 654: val_loss improved from 42.32242 to 42.31099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 35s - loss: 41.2400 - MinusLogProbMetric: 41.2400 - val_loss: 42.3110 - val_MinusLogProbMetric: 42.3110 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 655/1000
2023-10-27 19:21:53.133 
Epoch 655/1000 
	 loss: 41.2185, MinusLogProbMetric: 41.2185, val_loss: 42.3499, val_MinusLogProbMetric: 42.3499

Epoch 655: val_loss did not improve from 42.31099
196/196 - 33s - loss: 41.2185 - MinusLogProbMetric: 41.2185 - val_loss: 42.3499 - val_MinusLogProbMetric: 42.3499 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 656/1000
2023-10-27 19:22:26.540 
Epoch 656/1000 
	 loss: 41.2070, MinusLogProbMetric: 41.2070, val_loss: 42.3744, val_MinusLogProbMetric: 42.3744

Epoch 656: val_loss did not improve from 42.31099
196/196 - 33s - loss: 41.2070 - MinusLogProbMetric: 41.2070 - val_loss: 42.3744 - val_MinusLogProbMetric: 42.3744 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 657/1000
2023-10-27 19:23:00.027 
Epoch 657/1000 
	 loss: 41.2232, MinusLogProbMetric: 41.2232, val_loss: 42.2655, val_MinusLogProbMetric: 42.2655

Epoch 657: val_loss improved from 42.31099 to 42.26554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.2232 - MinusLogProbMetric: 41.2232 - val_loss: 42.2655 - val_MinusLogProbMetric: 42.2655 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 658/1000
2023-10-27 19:23:35.375 
Epoch 658/1000 
	 loss: 41.2389, MinusLogProbMetric: 41.2389, val_loss: 42.2097, val_MinusLogProbMetric: 42.2097

Epoch 658: val_loss improved from 42.26554 to 42.20971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 35s - loss: 41.2389 - MinusLogProbMetric: 41.2389 - val_loss: 42.2097 - val_MinusLogProbMetric: 42.2097 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 659/1000
2023-10-27 19:24:09.238 
Epoch 659/1000 
	 loss: 41.2214, MinusLogProbMetric: 41.2214, val_loss: 42.3986, val_MinusLogProbMetric: 42.3986

Epoch 659: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2214 - MinusLogProbMetric: 41.2214 - val_loss: 42.3986 - val_MinusLogProbMetric: 42.3986 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 660/1000
2023-10-27 19:24:42.385 
Epoch 660/1000 
	 loss: 41.2303, MinusLogProbMetric: 41.2303, val_loss: 42.4667, val_MinusLogProbMetric: 42.4667

Epoch 660: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2303 - MinusLogProbMetric: 41.2303 - val_loss: 42.4667 - val_MinusLogProbMetric: 42.4667 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 661/1000
2023-10-27 19:25:15.496 
Epoch 661/1000 
	 loss: 41.3145, MinusLogProbMetric: 41.3145, val_loss: 42.2799, val_MinusLogProbMetric: 42.2799

Epoch 661: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.3145 - MinusLogProbMetric: 41.3145 - val_loss: 42.2799 - val_MinusLogProbMetric: 42.2799 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 662/1000
2023-10-27 19:25:48.945 
Epoch 662/1000 
	 loss: 41.2399, MinusLogProbMetric: 41.2399, val_loss: 42.3713, val_MinusLogProbMetric: 42.3713

Epoch 662: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2399 - MinusLogProbMetric: 41.2399 - val_loss: 42.3713 - val_MinusLogProbMetric: 42.3713 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 663/1000
2023-10-27 19:26:21.636 
Epoch 663/1000 
	 loss: 41.2579, MinusLogProbMetric: 41.2579, val_loss: 42.2565, val_MinusLogProbMetric: 42.2565

Epoch 663: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2579 - MinusLogProbMetric: 41.2579 - val_loss: 42.2565 - val_MinusLogProbMetric: 42.2565 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 664/1000
2023-10-27 19:26:54.844 
Epoch 664/1000 
	 loss: 41.2061, MinusLogProbMetric: 41.2061, val_loss: 42.3547, val_MinusLogProbMetric: 42.3547

Epoch 664: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2061 - MinusLogProbMetric: 41.2061 - val_loss: 42.3547 - val_MinusLogProbMetric: 42.3547 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 665/1000
2023-10-27 19:27:27.916 
Epoch 665/1000 
	 loss: 41.2225, MinusLogProbMetric: 41.2225, val_loss: 42.2744, val_MinusLogProbMetric: 42.2744

Epoch 665: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2225 - MinusLogProbMetric: 41.2225 - val_loss: 42.2744 - val_MinusLogProbMetric: 42.2744 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 666/1000
2023-10-27 19:28:01.031 
Epoch 666/1000 
	 loss: 41.1951, MinusLogProbMetric: 41.1951, val_loss: 42.3541, val_MinusLogProbMetric: 42.3541

Epoch 666: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.1951 - MinusLogProbMetric: 41.1951 - val_loss: 42.3541 - val_MinusLogProbMetric: 42.3541 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 667/1000
2023-10-27 19:28:34.114 
Epoch 667/1000 
	 loss: 41.2417, MinusLogProbMetric: 41.2417, val_loss: 42.2626, val_MinusLogProbMetric: 42.2626

Epoch 667: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2417 - MinusLogProbMetric: 41.2417 - val_loss: 42.2626 - val_MinusLogProbMetric: 42.2626 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 668/1000
2023-10-27 19:29:07.264 
Epoch 668/1000 
	 loss: 41.2365, MinusLogProbMetric: 41.2365, val_loss: 42.8775, val_MinusLogProbMetric: 42.8775

Epoch 668: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2365 - MinusLogProbMetric: 41.2365 - val_loss: 42.8775 - val_MinusLogProbMetric: 42.8775 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 669/1000
2023-10-27 19:29:40.096 
Epoch 669/1000 
	 loss: 41.3180, MinusLogProbMetric: 41.3180, val_loss: 42.2365, val_MinusLogProbMetric: 42.2365

Epoch 669: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.3180 - MinusLogProbMetric: 41.3180 - val_loss: 42.2365 - val_MinusLogProbMetric: 42.2365 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 670/1000
2023-10-27 19:30:13.396 
Epoch 670/1000 
	 loss: 41.2819, MinusLogProbMetric: 41.2819, val_loss: 42.3770, val_MinusLogProbMetric: 42.3770

Epoch 670: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2819 - MinusLogProbMetric: 41.2819 - val_loss: 42.3770 - val_MinusLogProbMetric: 42.3770 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 671/1000
2023-10-27 19:30:46.493 
Epoch 671/1000 
	 loss: 41.2504, MinusLogProbMetric: 41.2504, val_loss: 42.2823, val_MinusLogProbMetric: 42.2823

Epoch 671: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2504 - MinusLogProbMetric: 41.2504 - val_loss: 42.2823 - val_MinusLogProbMetric: 42.2823 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 672/1000
2023-10-27 19:31:19.362 
Epoch 672/1000 
	 loss: 41.2429, MinusLogProbMetric: 41.2429, val_loss: 42.3989, val_MinusLogProbMetric: 42.3989

Epoch 672: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2429 - MinusLogProbMetric: 41.2429 - val_loss: 42.3989 - val_MinusLogProbMetric: 42.3989 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 673/1000
2023-10-27 19:31:52.559 
Epoch 673/1000 
	 loss: 41.2941, MinusLogProbMetric: 41.2941, val_loss: 42.2929, val_MinusLogProbMetric: 42.2929

Epoch 673: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2941 - MinusLogProbMetric: 41.2941 - val_loss: 42.2929 - val_MinusLogProbMetric: 42.2929 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 674/1000
2023-10-27 19:32:26.657 
Epoch 674/1000 
	 loss: 41.2179, MinusLogProbMetric: 41.2179, val_loss: 42.2469, val_MinusLogProbMetric: 42.2469

Epoch 674: val_loss did not improve from 42.20971
196/196 - 34s - loss: 41.2179 - MinusLogProbMetric: 41.2179 - val_loss: 42.2469 - val_MinusLogProbMetric: 42.2469 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 675/1000
2023-10-27 19:33:00.311 
Epoch 675/1000 
	 loss: 41.2243, MinusLogProbMetric: 41.2243, val_loss: 42.3114, val_MinusLogProbMetric: 42.3114

Epoch 675: val_loss did not improve from 42.20971
196/196 - 34s - loss: 41.2243 - MinusLogProbMetric: 41.2243 - val_loss: 42.3114 - val_MinusLogProbMetric: 42.3114 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 676/1000
2023-10-27 19:33:33.956 
Epoch 676/1000 
	 loss: 41.2084, MinusLogProbMetric: 41.2084, val_loss: 42.2781, val_MinusLogProbMetric: 42.2781

Epoch 676: val_loss did not improve from 42.20971
196/196 - 34s - loss: 41.2084 - MinusLogProbMetric: 41.2084 - val_loss: 42.2781 - val_MinusLogProbMetric: 42.2781 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 677/1000
2023-10-27 19:34:07.075 
Epoch 677/1000 
	 loss: 41.3128, MinusLogProbMetric: 41.3128, val_loss: 42.3611, val_MinusLogProbMetric: 42.3611

Epoch 677: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.3128 - MinusLogProbMetric: 41.3128 - val_loss: 42.3611 - val_MinusLogProbMetric: 42.3611 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 678/1000
2023-10-27 19:34:40.430 
Epoch 678/1000 
	 loss: 41.2140, MinusLogProbMetric: 41.2140, val_loss: 42.3760, val_MinusLogProbMetric: 42.3760

Epoch 678: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2140 - MinusLogProbMetric: 41.2140 - val_loss: 42.3760 - val_MinusLogProbMetric: 42.3760 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 679/1000
2023-10-27 19:35:13.688 
Epoch 679/1000 
	 loss: 41.1994, MinusLogProbMetric: 41.1994, val_loss: 42.3555, val_MinusLogProbMetric: 42.3555

Epoch 679: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.1994 - MinusLogProbMetric: 41.1994 - val_loss: 42.3555 - val_MinusLogProbMetric: 42.3555 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 680/1000
2023-10-27 19:35:46.777 
Epoch 680/1000 
	 loss: 41.2170, MinusLogProbMetric: 41.2170, val_loss: 42.3296, val_MinusLogProbMetric: 42.3296

Epoch 680: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2170 - MinusLogProbMetric: 41.2170 - val_loss: 42.3296 - val_MinusLogProbMetric: 42.3296 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 681/1000
2023-10-27 19:36:19.431 
Epoch 681/1000 
	 loss: 41.2429, MinusLogProbMetric: 41.2429, val_loss: 42.5221, val_MinusLogProbMetric: 42.5221

Epoch 681: val_loss did not improve from 42.20971
196/196 - 33s - loss: 41.2429 - MinusLogProbMetric: 41.2429 - val_loss: 42.5221 - val_MinusLogProbMetric: 42.5221 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 682/1000
2023-10-27 19:36:52.442 
Epoch 682/1000 
	 loss: 41.2393, MinusLogProbMetric: 41.2393, val_loss: 42.2047, val_MinusLogProbMetric: 42.2047

Epoch 682: val_loss improved from 42.20971 to 42.20470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.2393 - MinusLogProbMetric: 41.2393 - val_loss: 42.2047 - val_MinusLogProbMetric: 42.2047 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 683/1000
2023-10-27 19:37:26.000 
Epoch 683/1000 
	 loss: 41.2091, MinusLogProbMetric: 41.2091, val_loss: 42.3933, val_MinusLogProbMetric: 42.3933

Epoch 683: val_loss did not improve from 42.20470
196/196 - 33s - loss: 41.2091 - MinusLogProbMetric: 41.2091 - val_loss: 42.3933 - val_MinusLogProbMetric: 42.3933 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 684/1000
2023-10-27 19:37:58.803 
Epoch 684/1000 
	 loss: 41.3150, MinusLogProbMetric: 41.3150, val_loss: 42.2921, val_MinusLogProbMetric: 42.2921

Epoch 684: val_loss did not improve from 42.20470
196/196 - 33s - loss: 41.3150 - MinusLogProbMetric: 41.3150 - val_loss: 42.2921 - val_MinusLogProbMetric: 42.2921 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 685/1000
2023-10-27 19:38:31.704 
Epoch 685/1000 
	 loss: 41.2944, MinusLogProbMetric: 41.2944, val_loss: 42.2720, val_MinusLogProbMetric: 42.2720

Epoch 685: val_loss did not improve from 42.20470
196/196 - 33s - loss: 41.2944 - MinusLogProbMetric: 41.2944 - val_loss: 42.2720 - val_MinusLogProbMetric: 42.2720 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 686/1000
2023-10-27 19:39:03.943 
Epoch 686/1000 
	 loss: 41.2076, MinusLogProbMetric: 41.2076, val_loss: 42.3734, val_MinusLogProbMetric: 42.3734

Epoch 686: val_loss did not improve from 42.20470
196/196 - 32s - loss: 41.2076 - MinusLogProbMetric: 41.2076 - val_loss: 42.3734 - val_MinusLogProbMetric: 42.3734 - lr: 4.1667e-05 - 32s/epoch - 164ms/step
Epoch 687/1000
2023-10-27 19:39:36.714 
Epoch 687/1000 
	 loss: 41.2104, MinusLogProbMetric: 41.2104, val_loss: 42.2836, val_MinusLogProbMetric: 42.2836

Epoch 687: val_loss did not improve from 42.20470
196/196 - 33s - loss: 41.2104 - MinusLogProbMetric: 41.2104 - val_loss: 42.2836 - val_MinusLogProbMetric: 42.2836 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 688/1000
2023-10-27 19:40:09.708 
Epoch 688/1000 
	 loss: 41.2116, MinusLogProbMetric: 41.2116, val_loss: 42.2020, val_MinusLogProbMetric: 42.2020

Epoch 688: val_loss improved from 42.20470 to 42.20195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.2116 - MinusLogProbMetric: 41.2116 - val_loss: 42.2020 - val_MinusLogProbMetric: 42.2020 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 689/1000
2023-10-27 19:40:43.645 
Epoch 689/1000 
	 loss: 41.3836, MinusLogProbMetric: 41.3836, val_loss: 42.6072, val_MinusLogProbMetric: 42.6072

Epoch 689: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.3836 - MinusLogProbMetric: 41.3836 - val_loss: 42.6072 - val_MinusLogProbMetric: 42.6072 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 690/1000
2023-10-27 19:41:16.851 
Epoch 690/1000 
	 loss: 41.1887, MinusLogProbMetric: 41.1887, val_loss: 42.3088, val_MinusLogProbMetric: 42.3088

Epoch 690: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1887 - MinusLogProbMetric: 41.1887 - val_loss: 42.3088 - val_MinusLogProbMetric: 42.3088 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 691/1000
2023-10-27 19:41:50.352 
Epoch 691/1000 
	 loss: 41.1925, MinusLogProbMetric: 41.1925, val_loss: 42.2939, val_MinusLogProbMetric: 42.2939

Epoch 691: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1925 - MinusLogProbMetric: 41.1925 - val_loss: 42.2939 - val_MinusLogProbMetric: 42.2939 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 692/1000
2023-10-27 19:42:23.862 
Epoch 692/1000 
	 loss: 41.2219, MinusLogProbMetric: 41.2219, val_loss: 42.2856, val_MinusLogProbMetric: 42.2856

Epoch 692: val_loss did not improve from 42.20195
196/196 - 34s - loss: 41.2219 - MinusLogProbMetric: 41.2219 - val_loss: 42.2856 - val_MinusLogProbMetric: 42.2856 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 693/1000
2023-10-27 19:42:56.679 
Epoch 693/1000 
	 loss: 41.2454, MinusLogProbMetric: 41.2454, val_loss: 42.2753, val_MinusLogProbMetric: 42.2753

Epoch 693: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2454 - MinusLogProbMetric: 41.2454 - val_loss: 42.2753 - val_MinusLogProbMetric: 42.2753 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 694/1000
2023-10-27 19:43:30.334 
Epoch 694/1000 
	 loss: 41.2361, MinusLogProbMetric: 41.2361, val_loss: 42.2404, val_MinusLogProbMetric: 42.2404

Epoch 694: val_loss did not improve from 42.20195
196/196 - 34s - loss: 41.2361 - MinusLogProbMetric: 41.2361 - val_loss: 42.2404 - val_MinusLogProbMetric: 42.2404 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 695/1000
2023-10-27 19:44:03.291 
Epoch 695/1000 
	 loss: 41.2184, MinusLogProbMetric: 41.2184, val_loss: 42.2572, val_MinusLogProbMetric: 42.2572

Epoch 695: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2184 - MinusLogProbMetric: 41.2184 - val_loss: 42.2572 - val_MinusLogProbMetric: 42.2572 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 696/1000
2023-10-27 19:44:36.496 
Epoch 696/1000 
	 loss: 41.2101, MinusLogProbMetric: 41.2101, val_loss: 42.2549, val_MinusLogProbMetric: 42.2549

Epoch 696: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2101 - MinusLogProbMetric: 41.2101 - val_loss: 42.2549 - val_MinusLogProbMetric: 42.2549 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 697/1000
2023-10-27 19:45:10.100 
Epoch 697/1000 
	 loss: 41.3572, MinusLogProbMetric: 41.3572, val_loss: 42.7644, val_MinusLogProbMetric: 42.7644

Epoch 697: val_loss did not improve from 42.20195
196/196 - 34s - loss: 41.3572 - MinusLogProbMetric: 41.3572 - val_loss: 42.7644 - val_MinusLogProbMetric: 42.7644 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 698/1000
2023-10-27 19:45:43.878 
Epoch 698/1000 
	 loss: 41.2480, MinusLogProbMetric: 41.2480, val_loss: 42.2740, val_MinusLogProbMetric: 42.2740

Epoch 698: val_loss did not improve from 42.20195
196/196 - 34s - loss: 41.2480 - MinusLogProbMetric: 41.2480 - val_loss: 42.2740 - val_MinusLogProbMetric: 42.2740 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 699/1000
2023-10-27 19:46:18.019 
Epoch 699/1000 
	 loss: 41.2042, MinusLogProbMetric: 41.2042, val_loss: 42.2228, val_MinusLogProbMetric: 42.2228

Epoch 699: val_loss did not improve from 42.20195
196/196 - 34s - loss: 41.2042 - MinusLogProbMetric: 41.2042 - val_loss: 42.2228 - val_MinusLogProbMetric: 42.2228 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 700/1000
2023-10-27 19:46:51.235 
Epoch 700/1000 
	 loss: 41.1954, MinusLogProbMetric: 41.1954, val_loss: 42.3481, val_MinusLogProbMetric: 42.3481

Epoch 700: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1954 - MinusLogProbMetric: 41.1954 - val_loss: 42.3481 - val_MinusLogProbMetric: 42.3481 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 701/1000
2023-10-27 19:47:24.145 
Epoch 701/1000 
	 loss: 41.1896, MinusLogProbMetric: 41.1896, val_loss: 42.4131, val_MinusLogProbMetric: 42.4131

Epoch 701: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1896 - MinusLogProbMetric: 41.1896 - val_loss: 42.4131 - val_MinusLogProbMetric: 42.4131 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 702/1000
2023-10-27 19:47:57.183 
Epoch 702/1000 
	 loss: 41.2572, MinusLogProbMetric: 41.2572, val_loss: 42.2741, val_MinusLogProbMetric: 42.2741

Epoch 702: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2572 - MinusLogProbMetric: 41.2572 - val_loss: 42.2741 - val_MinusLogProbMetric: 42.2741 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 703/1000
2023-10-27 19:48:30.410 
Epoch 703/1000 
	 loss: 41.2049, MinusLogProbMetric: 41.2049, val_loss: 42.2825, val_MinusLogProbMetric: 42.2825

Epoch 703: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2049 - MinusLogProbMetric: 41.2049 - val_loss: 42.2825 - val_MinusLogProbMetric: 42.2825 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 704/1000
2023-10-27 19:49:03.066 
Epoch 704/1000 
	 loss: 41.1849, MinusLogProbMetric: 41.1849, val_loss: 42.6938, val_MinusLogProbMetric: 42.6938

Epoch 704: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1849 - MinusLogProbMetric: 41.1849 - val_loss: 42.6938 - val_MinusLogProbMetric: 42.6938 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 705/1000
2023-10-27 19:49:35.571 
Epoch 705/1000 
	 loss: 41.2667, MinusLogProbMetric: 41.2667, val_loss: 43.5853, val_MinusLogProbMetric: 43.5853

Epoch 705: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2667 - MinusLogProbMetric: 41.2667 - val_loss: 43.5853 - val_MinusLogProbMetric: 43.5853 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 706/1000
2023-10-27 19:50:08.650 
Epoch 706/1000 
	 loss: 41.3526, MinusLogProbMetric: 41.3526, val_loss: 42.3266, val_MinusLogProbMetric: 42.3266

Epoch 706: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.3526 - MinusLogProbMetric: 41.3526 - val_loss: 42.3266 - val_MinusLogProbMetric: 42.3266 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 707/1000
2023-10-27 19:50:41.833 
Epoch 707/1000 
	 loss: 41.1906, MinusLogProbMetric: 41.1906, val_loss: 42.2192, val_MinusLogProbMetric: 42.2192

Epoch 707: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1906 - MinusLogProbMetric: 41.1906 - val_loss: 42.2192 - val_MinusLogProbMetric: 42.2192 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 708/1000
2023-10-27 19:51:14.556 
Epoch 708/1000 
	 loss: 41.1762, MinusLogProbMetric: 41.1762, val_loss: 42.2653, val_MinusLogProbMetric: 42.2653

Epoch 708: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1762 - MinusLogProbMetric: 41.1762 - val_loss: 42.2653 - val_MinusLogProbMetric: 42.2653 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 709/1000
2023-10-27 19:51:47.710 
Epoch 709/1000 
	 loss: 41.2024, MinusLogProbMetric: 41.2024, val_loss: 42.3249, val_MinusLogProbMetric: 42.3249

Epoch 709: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2024 - MinusLogProbMetric: 41.2024 - val_loss: 42.3249 - val_MinusLogProbMetric: 42.3249 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 710/1000
2023-10-27 19:52:20.488 
Epoch 710/1000 
	 loss: 41.1917, MinusLogProbMetric: 41.1917, val_loss: 42.5104, val_MinusLogProbMetric: 42.5104

Epoch 710: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.1917 - MinusLogProbMetric: 41.1917 - val_loss: 42.5104 - val_MinusLogProbMetric: 42.5104 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 711/1000
2023-10-27 19:52:53.287 
Epoch 711/1000 
	 loss: 41.2342, MinusLogProbMetric: 41.2342, val_loss: 42.2793, val_MinusLogProbMetric: 42.2793

Epoch 711: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2342 - MinusLogProbMetric: 41.2342 - val_loss: 42.2793 - val_MinusLogProbMetric: 42.2793 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 712/1000
2023-10-27 19:53:25.961 
Epoch 712/1000 
	 loss: 41.2360, MinusLogProbMetric: 41.2360, val_loss: 42.4579, val_MinusLogProbMetric: 42.4579

Epoch 712: val_loss did not improve from 42.20195
196/196 - 33s - loss: 41.2360 - MinusLogProbMetric: 41.2360 - val_loss: 42.4579 - val_MinusLogProbMetric: 42.4579 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 713/1000
2023-10-27 19:53:59.162 
Epoch 713/1000 
	 loss: 41.2129, MinusLogProbMetric: 41.2129, val_loss: 42.1925, val_MinusLogProbMetric: 42.1925

Epoch 713: val_loss improved from 42.20195 to 42.19245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 41.2129 - MinusLogProbMetric: 41.2129 - val_loss: 42.1925 - val_MinusLogProbMetric: 42.1925 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 714/1000
2023-10-27 19:54:32.853 
Epoch 714/1000 
	 loss: 41.2019, MinusLogProbMetric: 41.2019, val_loss: 42.3662, val_MinusLogProbMetric: 42.3662

Epoch 714: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2019 - MinusLogProbMetric: 41.2019 - val_loss: 42.3662 - val_MinusLogProbMetric: 42.3662 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 715/1000
2023-10-27 19:55:05.854 
Epoch 715/1000 
	 loss: 41.2588, MinusLogProbMetric: 41.2588, val_loss: 42.2307, val_MinusLogProbMetric: 42.2307

Epoch 715: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2588 - MinusLogProbMetric: 41.2588 - val_loss: 42.2307 - val_MinusLogProbMetric: 42.2307 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 716/1000
2023-10-27 19:55:37.632 
Epoch 716/1000 
	 loss: 41.2129, MinusLogProbMetric: 41.2129, val_loss: 42.2076, val_MinusLogProbMetric: 42.2076

Epoch 716: val_loss did not improve from 42.19245
196/196 - 32s - loss: 41.2129 - MinusLogProbMetric: 41.2129 - val_loss: 42.2076 - val_MinusLogProbMetric: 42.2076 - lr: 4.1667e-05 - 32s/epoch - 162ms/step
Epoch 717/1000
2023-10-27 19:56:04.743 
Epoch 717/1000 
	 loss: 41.1791, MinusLogProbMetric: 41.1791, val_loss: 42.2968, val_MinusLogProbMetric: 42.2968

Epoch 717: val_loss did not improve from 42.19245
196/196 - 27s - loss: 41.1791 - MinusLogProbMetric: 41.1791 - val_loss: 42.2968 - val_MinusLogProbMetric: 42.2968 - lr: 4.1667e-05 - 27s/epoch - 138ms/step
Epoch 718/1000
2023-10-27 19:56:31.690 
Epoch 718/1000 
	 loss: 41.4070, MinusLogProbMetric: 41.4070, val_loss: 42.4764, val_MinusLogProbMetric: 42.4764

Epoch 718: val_loss did not improve from 42.19245
196/196 - 27s - loss: 41.4070 - MinusLogProbMetric: 41.4070 - val_loss: 42.4764 - val_MinusLogProbMetric: 42.4764 - lr: 4.1667e-05 - 27s/epoch - 137ms/step
Epoch 719/1000
2023-10-27 19:56:58.427 
Epoch 719/1000 
	 loss: 41.1850, MinusLogProbMetric: 41.1850, val_loss: 42.8155, val_MinusLogProbMetric: 42.8155

Epoch 719: val_loss did not improve from 42.19245
196/196 - 27s - loss: 41.1850 - MinusLogProbMetric: 41.1850 - val_loss: 42.8155 - val_MinusLogProbMetric: 42.8155 - lr: 4.1667e-05 - 27s/epoch - 136ms/step
Epoch 720/1000
2023-10-27 19:57:25.499 
Epoch 720/1000 
	 loss: 41.2262, MinusLogProbMetric: 41.2262, val_loss: 42.2430, val_MinusLogProbMetric: 42.2430

Epoch 720: val_loss did not improve from 42.19245
196/196 - 27s - loss: 41.2262 - MinusLogProbMetric: 41.2262 - val_loss: 42.2430 - val_MinusLogProbMetric: 42.2430 - lr: 4.1667e-05 - 27s/epoch - 138ms/step
Epoch 721/1000
2023-10-27 19:57:53.320 
Epoch 721/1000 
	 loss: 41.1799, MinusLogProbMetric: 41.1799, val_loss: 42.2799, val_MinusLogProbMetric: 42.2799

Epoch 721: val_loss did not improve from 42.19245
196/196 - 28s - loss: 41.1799 - MinusLogProbMetric: 41.1799 - val_loss: 42.2799 - val_MinusLogProbMetric: 42.2799 - lr: 4.1667e-05 - 28s/epoch - 142ms/step
Epoch 722/1000
2023-10-27 19:58:25.927 
Epoch 722/1000 
	 loss: 41.1621, MinusLogProbMetric: 41.1621, val_loss: 42.2104, val_MinusLogProbMetric: 42.2104

Epoch 722: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1621 - MinusLogProbMetric: 41.1621 - val_loss: 42.2104 - val_MinusLogProbMetric: 42.2104 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 723/1000
2023-10-27 19:58:58.769 
Epoch 723/1000 
	 loss: 41.2050, MinusLogProbMetric: 41.2050, val_loss: 42.3659, val_MinusLogProbMetric: 42.3659

Epoch 723: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2050 - MinusLogProbMetric: 41.2050 - val_loss: 42.3659 - val_MinusLogProbMetric: 42.3659 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 724/1000
2023-10-27 19:59:32.210 
Epoch 724/1000 
	 loss: 41.2160, MinusLogProbMetric: 41.2160, val_loss: 42.4288, val_MinusLogProbMetric: 42.4288

Epoch 724: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2160 - MinusLogProbMetric: 41.2160 - val_loss: 42.4288 - val_MinusLogProbMetric: 42.4288 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 725/1000
2023-10-27 20:00:05.747 
Epoch 725/1000 
	 loss: 41.1656, MinusLogProbMetric: 41.1656, val_loss: 42.3022, val_MinusLogProbMetric: 42.3022

Epoch 725: val_loss did not improve from 42.19245
196/196 - 34s - loss: 41.1656 - MinusLogProbMetric: 41.1656 - val_loss: 42.3022 - val_MinusLogProbMetric: 42.3022 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 726/1000
2023-10-27 20:00:36.879 
Epoch 726/1000 
	 loss: 41.3746, MinusLogProbMetric: 41.3746, val_loss: 42.4418, val_MinusLogProbMetric: 42.4418

Epoch 726: val_loss did not improve from 42.19245
196/196 - 31s - loss: 41.3746 - MinusLogProbMetric: 41.3746 - val_loss: 42.4418 - val_MinusLogProbMetric: 42.4418 - lr: 4.1667e-05 - 31s/epoch - 159ms/step
Epoch 727/1000
2023-10-27 20:01:04.906 
Epoch 727/1000 
	 loss: 41.1582, MinusLogProbMetric: 41.1582, val_loss: 42.2384, val_MinusLogProbMetric: 42.2384

Epoch 727: val_loss did not improve from 42.19245
196/196 - 28s - loss: 41.1582 - MinusLogProbMetric: 41.1582 - val_loss: 42.2384 - val_MinusLogProbMetric: 42.2384 - lr: 4.1667e-05 - 28s/epoch - 143ms/step
Epoch 728/1000
2023-10-27 20:01:31.680 
Epoch 728/1000 
	 loss: 41.1962, MinusLogProbMetric: 41.1962, val_loss: 42.5593, val_MinusLogProbMetric: 42.5593

Epoch 728: val_loss did not improve from 42.19245
196/196 - 27s - loss: 41.1962 - MinusLogProbMetric: 41.1962 - val_loss: 42.5593 - val_MinusLogProbMetric: 42.5593 - lr: 4.1667e-05 - 27s/epoch - 137ms/step
Epoch 729/1000
2023-10-27 20:01:58.556 
Epoch 729/1000 
	 loss: 41.2454, MinusLogProbMetric: 41.2454, val_loss: 42.3946, val_MinusLogProbMetric: 42.3946

Epoch 729: val_loss did not improve from 42.19245
196/196 - 27s - loss: 41.2454 - MinusLogProbMetric: 41.2454 - val_loss: 42.3946 - val_MinusLogProbMetric: 42.3946 - lr: 4.1667e-05 - 27s/epoch - 137ms/step
Epoch 730/1000
2023-10-27 20:02:26.297 
Epoch 730/1000 
	 loss: 41.3564, MinusLogProbMetric: 41.3564, val_loss: 42.2171, val_MinusLogProbMetric: 42.2171

Epoch 730: val_loss did not improve from 42.19245
196/196 - 28s - loss: 41.3564 - MinusLogProbMetric: 41.3564 - val_loss: 42.2171 - val_MinusLogProbMetric: 42.2171 - lr: 4.1667e-05 - 28s/epoch - 142ms/step
Epoch 731/1000
2023-10-27 20:02:56.783 
Epoch 731/1000 
	 loss: 41.1507, MinusLogProbMetric: 41.1507, val_loss: 42.2581, val_MinusLogProbMetric: 42.2581

Epoch 731: val_loss did not improve from 42.19245
196/196 - 30s - loss: 41.1507 - MinusLogProbMetric: 41.1507 - val_loss: 42.2581 - val_MinusLogProbMetric: 42.2581 - lr: 4.1667e-05 - 30s/epoch - 156ms/step
Epoch 732/1000
2023-10-27 20:03:30.087 
Epoch 732/1000 
	 loss: 41.3852, MinusLogProbMetric: 41.3852, val_loss: 42.4802, val_MinusLogProbMetric: 42.4802

Epoch 732: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.3852 - MinusLogProbMetric: 41.3852 - val_loss: 42.4802 - val_MinusLogProbMetric: 42.4802 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 733/1000
2023-10-27 20:04:03.086 
Epoch 733/1000 
	 loss: 41.1817, MinusLogProbMetric: 41.1817, val_loss: 42.3166, val_MinusLogProbMetric: 42.3166

Epoch 733: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1817 - MinusLogProbMetric: 41.1817 - val_loss: 42.3166 - val_MinusLogProbMetric: 42.3166 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 734/1000
2023-10-27 20:04:36.446 
Epoch 734/1000 
	 loss: 41.2182, MinusLogProbMetric: 41.2182, val_loss: 42.5988, val_MinusLogProbMetric: 42.5988

Epoch 734: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2182 - MinusLogProbMetric: 41.2182 - val_loss: 42.5988 - val_MinusLogProbMetric: 42.5988 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 735/1000
2023-10-27 20:05:10.431 
Epoch 735/1000 
	 loss: 41.2017, MinusLogProbMetric: 41.2017, val_loss: 42.2712, val_MinusLogProbMetric: 42.2712

Epoch 735: val_loss did not improve from 42.19245
196/196 - 34s - loss: 41.2017 - MinusLogProbMetric: 41.2017 - val_loss: 42.2712 - val_MinusLogProbMetric: 42.2712 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 736/1000
2023-10-27 20:05:43.985 
Epoch 736/1000 
	 loss: 41.2209, MinusLogProbMetric: 41.2209, val_loss: 42.2951, val_MinusLogProbMetric: 42.2951

Epoch 736: val_loss did not improve from 42.19245
196/196 - 34s - loss: 41.2209 - MinusLogProbMetric: 41.2209 - val_loss: 42.2951 - val_MinusLogProbMetric: 42.2951 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 737/1000
2023-10-27 20:06:17.037 
Epoch 737/1000 
	 loss: 41.2556, MinusLogProbMetric: 41.2556, val_loss: 42.3550, val_MinusLogProbMetric: 42.3550

Epoch 737: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2556 - MinusLogProbMetric: 41.2556 - val_loss: 42.3550 - val_MinusLogProbMetric: 42.3550 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 738/1000
2023-10-27 20:06:49.852 
Epoch 738/1000 
	 loss: 41.1622, MinusLogProbMetric: 41.1622, val_loss: 42.2583, val_MinusLogProbMetric: 42.2583

Epoch 738: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1622 - MinusLogProbMetric: 41.1622 - val_loss: 42.2583 - val_MinusLogProbMetric: 42.2583 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 739/1000
2023-10-27 20:07:22.875 
Epoch 739/1000 
	 loss: 41.3761, MinusLogProbMetric: 41.3761, val_loss: 42.3250, val_MinusLogProbMetric: 42.3250

Epoch 739: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.3761 - MinusLogProbMetric: 41.3761 - val_loss: 42.3250 - val_MinusLogProbMetric: 42.3250 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 740/1000
2023-10-27 20:07:55.884 
Epoch 740/1000 
	 loss: 41.1865, MinusLogProbMetric: 41.1865, val_loss: 42.2527, val_MinusLogProbMetric: 42.2527

Epoch 740: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1865 - MinusLogProbMetric: 41.1865 - val_loss: 42.2527 - val_MinusLogProbMetric: 42.2527 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 741/1000
2023-10-27 20:08:29.019 
Epoch 741/1000 
	 loss: 41.3206, MinusLogProbMetric: 41.3206, val_loss: 42.3261, val_MinusLogProbMetric: 42.3261

Epoch 741: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.3206 - MinusLogProbMetric: 41.3206 - val_loss: 42.3261 - val_MinusLogProbMetric: 42.3261 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 742/1000
2023-10-27 20:09:01.895 
Epoch 742/1000 
	 loss: 41.1698, MinusLogProbMetric: 41.1698, val_loss: 42.4967, val_MinusLogProbMetric: 42.4967

Epoch 742: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1698 - MinusLogProbMetric: 41.1698 - val_loss: 42.4967 - val_MinusLogProbMetric: 42.4967 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 743/1000
2023-10-27 20:09:34.368 
Epoch 743/1000 
	 loss: 41.1903, MinusLogProbMetric: 41.1903, val_loss: 42.5684, val_MinusLogProbMetric: 42.5684

Epoch 743: val_loss did not improve from 42.19245
196/196 - 32s - loss: 41.1903 - MinusLogProbMetric: 41.1903 - val_loss: 42.5684 - val_MinusLogProbMetric: 42.5684 - lr: 4.1667e-05 - 32s/epoch - 166ms/step
Epoch 744/1000
2023-10-27 20:10:06.521 
Epoch 744/1000 
	 loss: 41.2161, MinusLogProbMetric: 41.2161, val_loss: 42.3110, val_MinusLogProbMetric: 42.3110

Epoch 744: val_loss did not improve from 42.19245
196/196 - 32s - loss: 41.2161 - MinusLogProbMetric: 41.2161 - val_loss: 42.3110 - val_MinusLogProbMetric: 42.3110 - lr: 4.1667e-05 - 32s/epoch - 164ms/step
Epoch 745/1000
2023-10-27 20:10:39.696 
Epoch 745/1000 
	 loss: 41.2226, MinusLogProbMetric: 41.2226, val_loss: 42.3008, val_MinusLogProbMetric: 42.3008

Epoch 745: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2226 - MinusLogProbMetric: 41.2226 - val_loss: 42.3008 - val_MinusLogProbMetric: 42.3008 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 746/1000
2023-10-27 20:11:13.152 
Epoch 746/1000 
	 loss: 41.1908, MinusLogProbMetric: 41.1908, val_loss: 42.2832, val_MinusLogProbMetric: 42.2832

Epoch 746: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1908 - MinusLogProbMetric: 41.1908 - val_loss: 42.2832 - val_MinusLogProbMetric: 42.2832 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 747/1000
2023-10-27 20:11:46.544 
Epoch 747/1000 
	 loss: 41.3594, MinusLogProbMetric: 41.3594, val_loss: 42.3230, val_MinusLogProbMetric: 42.3230

Epoch 747: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.3594 - MinusLogProbMetric: 41.3594 - val_loss: 42.3230 - val_MinusLogProbMetric: 42.3230 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 748/1000
2023-10-27 20:12:19.582 
Epoch 748/1000 
	 loss: 41.2360, MinusLogProbMetric: 41.2360, val_loss: 42.2688, val_MinusLogProbMetric: 42.2688

Epoch 748: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2360 - MinusLogProbMetric: 41.2360 - val_loss: 42.2688 - val_MinusLogProbMetric: 42.2688 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 749/1000
2023-10-27 20:12:52.944 
Epoch 749/1000 
	 loss: 41.3421, MinusLogProbMetric: 41.3421, val_loss: 42.2050, val_MinusLogProbMetric: 42.2050

Epoch 749: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.3421 - MinusLogProbMetric: 41.3421 - val_loss: 42.2050 - val_MinusLogProbMetric: 42.2050 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 750/1000
2023-10-27 20:13:25.881 
Epoch 750/1000 
	 loss: 41.1446, MinusLogProbMetric: 41.1446, val_loss: 42.4125, val_MinusLogProbMetric: 42.4125

Epoch 750: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.1446 - MinusLogProbMetric: 41.1446 - val_loss: 42.4125 - val_MinusLogProbMetric: 42.4125 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 751/1000
2023-10-27 20:13:59.309 
Epoch 751/1000 
	 loss: 41.2223, MinusLogProbMetric: 41.2223, val_loss: 42.2878, val_MinusLogProbMetric: 42.2878

Epoch 751: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.2223 - MinusLogProbMetric: 41.2223 - val_loss: 42.2878 - val_MinusLogProbMetric: 42.2878 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 752/1000
2023-10-27 20:14:32.256 
Epoch 752/1000 
	 loss: 41.3044, MinusLogProbMetric: 41.3044, val_loss: 42.2946, val_MinusLogProbMetric: 42.2946

Epoch 752: val_loss did not improve from 42.19245
196/196 - 33s - loss: 41.3044 - MinusLogProbMetric: 41.3044 - val_loss: 42.2946 - val_MinusLogProbMetric: 42.2946 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 753/1000
2023-10-27 20:15:04.981 
Epoch 753/1000 
	 loss: 41.1432, MinusLogProbMetric: 41.1432, val_loss: 42.1614, val_MinusLogProbMetric: 42.1614

Epoch 753: val_loss improved from 42.19245 to 42.16135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 41.1432 - MinusLogProbMetric: 41.1432 - val_loss: 42.1614 - val_MinusLogProbMetric: 42.1614 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 754/1000
2023-10-27 20:15:38.438 
Epoch 754/1000 
	 loss: 41.2002, MinusLogProbMetric: 41.2002, val_loss: 42.1864, val_MinusLogProbMetric: 42.1864

Epoch 754: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.2002 - MinusLogProbMetric: 41.2002 - val_loss: 42.1864 - val_MinusLogProbMetric: 42.1864 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 755/1000
2023-10-27 20:16:11.394 
Epoch 755/1000 
	 loss: 41.1929, MinusLogProbMetric: 41.1929, val_loss: 42.7520, val_MinusLogProbMetric: 42.7520

Epoch 755: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.1929 - MinusLogProbMetric: 41.1929 - val_loss: 42.7520 - val_MinusLogProbMetric: 42.7520 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 756/1000
2023-10-27 20:16:44.578 
Epoch 756/1000 
	 loss: 41.1778, MinusLogProbMetric: 41.1778, val_loss: 42.4440, val_MinusLogProbMetric: 42.4440

Epoch 756: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.1778 - MinusLogProbMetric: 41.1778 - val_loss: 42.4440 - val_MinusLogProbMetric: 42.4440 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 757/1000
2023-10-27 20:17:18.169 
Epoch 757/1000 
	 loss: 41.2037, MinusLogProbMetric: 41.2037, val_loss: 42.2155, val_MinusLogProbMetric: 42.2155

Epoch 757: val_loss did not improve from 42.16135
196/196 - 34s - loss: 41.2037 - MinusLogProbMetric: 41.2037 - val_loss: 42.2155 - val_MinusLogProbMetric: 42.2155 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 758/1000
2023-10-27 20:17:51.884 
Epoch 758/1000 
	 loss: 41.1500, MinusLogProbMetric: 41.1500, val_loss: 42.3375, val_MinusLogProbMetric: 42.3375

Epoch 758: val_loss did not improve from 42.16135
196/196 - 34s - loss: 41.1500 - MinusLogProbMetric: 41.1500 - val_loss: 42.3375 - val_MinusLogProbMetric: 42.3375 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 759/1000
2023-10-27 20:18:25.551 
Epoch 759/1000 
	 loss: 41.2156, MinusLogProbMetric: 41.2156, val_loss: 42.7292, val_MinusLogProbMetric: 42.7292

Epoch 759: val_loss did not improve from 42.16135
196/196 - 34s - loss: 41.2156 - MinusLogProbMetric: 41.2156 - val_loss: 42.7292 - val_MinusLogProbMetric: 42.7292 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 760/1000
2023-10-27 20:18:58.897 
Epoch 760/1000 
	 loss: 41.2580, MinusLogProbMetric: 41.2580, val_loss: 42.1794, val_MinusLogProbMetric: 42.1794

Epoch 760: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.2580 - MinusLogProbMetric: 41.2580 - val_loss: 42.1794 - val_MinusLogProbMetric: 42.1794 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 761/1000
2023-10-27 20:19:32.307 
Epoch 761/1000 
	 loss: 41.2052, MinusLogProbMetric: 41.2052, val_loss: 42.5224, val_MinusLogProbMetric: 42.5224

Epoch 761: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.2052 - MinusLogProbMetric: 41.2052 - val_loss: 42.5224 - val_MinusLogProbMetric: 42.5224 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 762/1000
2023-10-27 20:20:05.452 
Epoch 762/1000 
	 loss: 41.2980, MinusLogProbMetric: 41.2980, val_loss: 42.2865, val_MinusLogProbMetric: 42.2865

Epoch 762: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.2980 - MinusLogProbMetric: 41.2980 - val_loss: 42.2865 - val_MinusLogProbMetric: 42.2865 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 763/1000
2023-10-27 20:20:38.295 
Epoch 763/1000 
	 loss: 41.1261, MinusLogProbMetric: 41.1261, val_loss: 42.5431, val_MinusLogProbMetric: 42.5431

Epoch 763: val_loss did not improve from 42.16135
196/196 - 33s - loss: 41.1261 - MinusLogProbMetric: 41.1261 - val_loss: 42.5431 - val_MinusLogProbMetric: 42.5431 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 764/1000
2023-10-27 20:21:10.226 
Epoch 764/1000 
	 loss: 41.2085, MinusLogProbMetric: 41.2085, val_loss: 42.1561, val_MinusLogProbMetric: 42.1561

Epoch 764: val_loss improved from 42.16135 to 42.15607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 41.2085 - MinusLogProbMetric: 41.2085 - val_loss: 42.1561 - val_MinusLogProbMetric: 42.1561 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 765/1000
2023-10-27 20:21:43.868 
Epoch 765/1000 
	 loss: 41.1635, MinusLogProbMetric: 41.1635, val_loss: 42.4025, val_MinusLogProbMetric: 42.4025

Epoch 765: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1635 - MinusLogProbMetric: 41.1635 - val_loss: 42.4025 - val_MinusLogProbMetric: 42.4025 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 766/1000
2023-10-27 20:22:16.916 
Epoch 766/1000 
	 loss: 41.2422, MinusLogProbMetric: 41.2422, val_loss: 42.2878, val_MinusLogProbMetric: 42.2878

Epoch 766: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2422 - MinusLogProbMetric: 41.2422 - val_loss: 42.2878 - val_MinusLogProbMetric: 42.2878 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 767/1000
2023-10-27 20:22:49.767 
Epoch 767/1000 
	 loss: 41.1498, MinusLogProbMetric: 41.1498, val_loss: 42.2510, val_MinusLogProbMetric: 42.2510

Epoch 767: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1498 - MinusLogProbMetric: 41.1498 - val_loss: 42.2510 - val_MinusLogProbMetric: 42.2510 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 768/1000
2023-10-27 20:23:23.162 
Epoch 768/1000 
	 loss: 41.3018, MinusLogProbMetric: 41.3018, val_loss: 42.3753, val_MinusLogProbMetric: 42.3753

Epoch 768: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.3018 - MinusLogProbMetric: 41.3018 - val_loss: 42.3753 - val_MinusLogProbMetric: 42.3753 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 769/1000
2023-10-27 20:23:56.081 
Epoch 769/1000 
	 loss: 41.1712, MinusLogProbMetric: 41.1712, val_loss: 42.5130, val_MinusLogProbMetric: 42.5130

Epoch 769: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1712 - MinusLogProbMetric: 41.1712 - val_loss: 42.5130 - val_MinusLogProbMetric: 42.5130 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 770/1000
2023-10-27 20:24:29.296 
Epoch 770/1000 
	 loss: 41.1620, MinusLogProbMetric: 41.1620, val_loss: 42.2303, val_MinusLogProbMetric: 42.2303

Epoch 770: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1620 - MinusLogProbMetric: 41.1620 - val_loss: 42.2303 - val_MinusLogProbMetric: 42.2303 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 771/1000
2023-10-27 20:25:02.675 
Epoch 771/1000 
	 loss: 41.1367, MinusLogProbMetric: 41.1367, val_loss: 42.4435, val_MinusLogProbMetric: 42.4435

Epoch 771: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1367 - MinusLogProbMetric: 41.1367 - val_loss: 42.4435 - val_MinusLogProbMetric: 42.4435 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 772/1000
2023-10-27 20:25:35.968 
Epoch 772/1000 
	 loss: 41.3681, MinusLogProbMetric: 41.3681, val_loss: 42.4002, val_MinusLogProbMetric: 42.4002

Epoch 772: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.3681 - MinusLogProbMetric: 41.3681 - val_loss: 42.4002 - val_MinusLogProbMetric: 42.4002 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 773/1000
2023-10-27 20:26:08.729 
Epoch 773/1000 
	 loss: 41.1209, MinusLogProbMetric: 41.1209, val_loss: 42.2134, val_MinusLogProbMetric: 42.2134

Epoch 773: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1209 - MinusLogProbMetric: 41.1209 - val_loss: 42.2134 - val_MinusLogProbMetric: 42.2134 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 774/1000
2023-10-27 20:26:41.679 
Epoch 774/1000 
	 loss: 41.3931, MinusLogProbMetric: 41.3931, val_loss: 42.3332, val_MinusLogProbMetric: 42.3332

Epoch 774: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.3931 - MinusLogProbMetric: 41.3931 - val_loss: 42.3332 - val_MinusLogProbMetric: 42.3332 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 775/1000
2023-10-27 20:27:14.843 
Epoch 775/1000 
	 loss: 41.1444, MinusLogProbMetric: 41.1444, val_loss: 42.4379, val_MinusLogProbMetric: 42.4379

Epoch 775: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1444 - MinusLogProbMetric: 41.1444 - val_loss: 42.4379 - val_MinusLogProbMetric: 42.4379 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 776/1000
2023-10-27 20:27:47.713 
Epoch 776/1000 
	 loss: 41.1435, MinusLogProbMetric: 41.1435, val_loss: 42.2242, val_MinusLogProbMetric: 42.2242

Epoch 776: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1435 - MinusLogProbMetric: 41.1435 - val_loss: 42.2242 - val_MinusLogProbMetric: 42.2242 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 777/1000
2023-10-27 20:28:20.450 
Epoch 777/1000 
	 loss: 41.1359, MinusLogProbMetric: 41.1359, val_loss: 42.3099, val_MinusLogProbMetric: 42.3099

Epoch 777: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1359 - MinusLogProbMetric: 41.1359 - val_loss: 42.3099 - val_MinusLogProbMetric: 42.3099 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 778/1000
2023-10-27 20:28:52.892 
Epoch 778/1000 
	 loss: 41.2101, MinusLogProbMetric: 41.2101, val_loss: 42.2245, val_MinusLogProbMetric: 42.2245

Epoch 778: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.2101 - MinusLogProbMetric: 41.2101 - val_loss: 42.2245 - val_MinusLogProbMetric: 42.2245 - lr: 4.1667e-05 - 32s/epoch - 166ms/step
Epoch 779/1000
2023-10-27 20:29:26.175 
Epoch 779/1000 
	 loss: 41.1652, MinusLogProbMetric: 41.1652, val_loss: 42.2278, val_MinusLogProbMetric: 42.2278

Epoch 779: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1652 - MinusLogProbMetric: 41.1652 - val_loss: 42.2278 - val_MinusLogProbMetric: 42.2278 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 780/1000
2023-10-27 20:29:59.680 
Epoch 780/1000 
	 loss: 41.1742, MinusLogProbMetric: 41.1742, val_loss: 42.3021, val_MinusLogProbMetric: 42.3021

Epoch 780: val_loss did not improve from 42.15607
196/196 - 34s - loss: 41.1742 - MinusLogProbMetric: 41.1742 - val_loss: 42.3021 - val_MinusLogProbMetric: 42.3021 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 781/1000
2023-10-27 20:30:32.553 
Epoch 781/1000 
	 loss: 41.1936, MinusLogProbMetric: 41.1936, val_loss: 42.2885, val_MinusLogProbMetric: 42.2885

Epoch 781: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1936 - MinusLogProbMetric: 41.1936 - val_loss: 42.2885 - val_MinusLogProbMetric: 42.2885 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 782/1000
2023-10-27 20:31:04.975 
Epoch 782/1000 
	 loss: 41.1594, MinusLogProbMetric: 41.1594, val_loss: 42.2055, val_MinusLogProbMetric: 42.2055

Epoch 782: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.1594 - MinusLogProbMetric: 41.1594 - val_loss: 42.2055 - val_MinusLogProbMetric: 42.2055 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 783/1000
2023-10-27 20:31:38.016 
Epoch 783/1000 
	 loss: 41.1616, MinusLogProbMetric: 41.1616, val_loss: 42.2041, val_MinusLogProbMetric: 42.2041

Epoch 783: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1616 - MinusLogProbMetric: 41.1616 - val_loss: 42.2041 - val_MinusLogProbMetric: 42.2041 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 784/1000
2023-10-27 20:32:10.975 
Epoch 784/1000 
	 loss: 41.4398, MinusLogProbMetric: 41.4398, val_loss: 42.2180, val_MinusLogProbMetric: 42.2180

Epoch 784: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.4398 - MinusLogProbMetric: 41.4398 - val_loss: 42.2180 - val_MinusLogProbMetric: 42.2180 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 785/1000
2023-10-27 20:32:43.726 
Epoch 785/1000 
	 loss: 41.1908, MinusLogProbMetric: 41.1908, val_loss: 42.3035, val_MinusLogProbMetric: 42.3035

Epoch 785: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1908 - MinusLogProbMetric: 41.1908 - val_loss: 42.3035 - val_MinusLogProbMetric: 42.3035 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 786/1000
2023-10-27 20:33:16.773 
Epoch 786/1000 
	 loss: 41.2264, MinusLogProbMetric: 41.2264, val_loss: 42.5336, val_MinusLogProbMetric: 42.5336

Epoch 786: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2264 - MinusLogProbMetric: 41.2264 - val_loss: 42.5336 - val_MinusLogProbMetric: 42.5336 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 787/1000
2023-10-27 20:33:48.943 
Epoch 787/1000 
	 loss: 41.1404, MinusLogProbMetric: 41.1404, val_loss: 42.3855, val_MinusLogProbMetric: 42.3855

Epoch 787: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.1404 - MinusLogProbMetric: 41.1404 - val_loss: 42.3855 - val_MinusLogProbMetric: 42.3855 - lr: 4.1667e-05 - 32s/epoch - 164ms/step
Epoch 788/1000
2023-10-27 20:34:21.242 
Epoch 788/1000 
	 loss: 41.1206, MinusLogProbMetric: 41.1206, val_loss: 42.2798, val_MinusLogProbMetric: 42.2798

Epoch 788: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.1206 - MinusLogProbMetric: 41.1206 - val_loss: 42.2798 - val_MinusLogProbMetric: 42.2798 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 789/1000
2023-10-27 20:34:54.358 
Epoch 789/1000 
	 loss: 41.1403, MinusLogProbMetric: 41.1403, val_loss: 42.5136, val_MinusLogProbMetric: 42.5136

Epoch 789: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1403 - MinusLogProbMetric: 41.1403 - val_loss: 42.5136 - val_MinusLogProbMetric: 42.5136 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 790/1000
2023-10-27 20:35:27.289 
Epoch 790/1000 
	 loss: 41.2297, MinusLogProbMetric: 41.2297, val_loss: 42.5905, val_MinusLogProbMetric: 42.5905

Epoch 790: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2297 - MinusLogProbMetric: 41.2297 - val_loss: 42.5905 - val_MinusLogProbMetric: 42.5905 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 791/1000
2023-10-27 20:36:00.115 
Epoch 791/1000 
	 loss: 41.2634, MinusLogProbMetric: 41.2634, val_loss: 42.2075, val_MinusLogProbMetric: 42.2075

Epoch 791: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2634 - MinusLogProbMetric: 41.2634 - val_loss: 42.2075 - val_MinusLogProbMetric: 42.2075 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 792/1000
2023-10-27 20:36:33.376 
Epoch 792/1000 
	 loss: 41.1564, MinusLogProbMetric: 41.1564, val_loss: 42.3231, val_MinusLogProbMetric: 42.3231

Epoch 792: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1564 - MinusLogProbMetric: 41.1564 - val_loss: 42.3231 - val_MinusLogProbMetric: 42.3231 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 793/1000
2023-10-27 20:37:06.271 
Epoch 793/1000 
	 loss: 41.1744, MinusLogProbMetric: 41.1744, val_loss: 42.4417, val_MinusLogProbMetric: 42.4417

Epoch 793: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1744 - MinusLogProbMetric: 41.1744 - val_loss: 42.4417 - val_MinusLogProbMetric: 42.4417 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 794/1000
2023-10-27 20:37:39.711 
Epoch 794/1000 
	 loss: 41.1446, MinusLogProbMetric: 41.1446, val_loss: 42.2402, val_MinusLogProbMetric: 42.2402

Epoch 794: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1446 - MinusLogProbMetric: 41.1446 - val_loss: 42.2402 - val_MinusLogProbMetric: 42.2402 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 795/1000
2023-10-27 20:38:12.933 
Epoch 795/1000 
	 loss: 41.1806, MinusLogProbMetric: 41.1806, val_loss: 42.3062, val_MinusLogProbMetric: 42.3062

Epoch 795: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1806 - MinusLogProbMetric: 41.1806 - val_loss: 42.3062 - val_MinusLogProbMetric: 42.3062 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 796/1000
2023-10-27 20:38:45.901 
Epoch 796/1000 
	 loss: 41.1560, MinusLogProbMetric: 41.1560, val_loss: 42.3822, val_MinusLogProbMetric: 42.3822

Epoch 796: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1560 - MinusLogProbMetric: 41.1560 - val_loss: 42.3822 - val_MinusLogProbMetric: 42.3822 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 797/1000
2023-10-27 20:39:19.101 
Epoch 797/1000 
	 loss: 41.2248, MinusLogProbMetric: 41.2248, val_loss: 42.5641, val_MinusLogProbMetric: 42.5641

Epoch 797: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2248 - MinusLogProbMetric: 41.2248 - val_loss: 42.5641 - val_MinusLogProbMetric: 42.5641 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 798/1000
2023-10-27 20:39:52.154 
Epoch 798/1000 
	 loss: 41.1560, MinusLogProbMetric: 41.1560, val_loss: 42.3191, val_MinusLogProbMetric: 42.3191

Epoch 798: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1560 - MinusLogProbMetric: 41.1560 - val_loss: 42.3191 - val_MinusLogProbMetric: 42.3191 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 799/1000
2023-10-27 20:40:24.910 
Epoch 799/1000 
	 loss: 41.1747, MinusLogProbMetric: 41.1747, val_loss: 42.3333, val_MinusLogProbMetric: 42.3333

Epoch 799: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1747 - MinusLogProbMetric: 41.1747 - val_loss: 42.3333 - val_MinusLogProbMetric: 42.3333 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 800/1000
2023-10-27 20:40:57.758 
Epoch 800/1000 
	 loss: 41.2220, MinusLogProbMetric: 41.2220, val_loss: 42.6062, val_MinusLogProbMetric: 42.6062

Epoch 800: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2220 - MinusLogProbMetric: 41.2220 - val_loss: 42.6062 - val_MinusLogProbMetric: 42.6062 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 801/1000
2023-10-27 20:41:30.536 
Epoch 801/1000 
	 loss: 41.1743, MinusLogProbMetric: 41.1743, val_loss: 42.2078, val_MinusLogProbMetric: 42.2078

Epoch 801: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1743 - MinusLogProbMetric: 41.1743 - val_loss: 42.2078 - val_MinusLogProbMetric: 42.2078 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 802/1000
2023-10-27 20:42:03.359 
Epoch 802/1000 
	 loss: 41.1729, MinusLogProbMetric: 41.1729, val_loss: 42.3451, val_MinusLogProbMetric: 42.3451

Epoch 802: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1729 - MinusLogProbMetric: 41.1729 - val_loss: 42.3451 - val_MinusLogProbMetric: 42.3451 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 803/1000
2023-10-27 20:42:36.323 
Epoch 803/1000 
	 loss: 41.2018, MinusLogProbMetric: 41.2018, val_loss: 42.2325, val_MinusLogProbMetric: 42.2325

Epoch 803: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.2018 - MinusLogProbMetric: 41.2018 - val_loss: 42.2325 - val_MinusLogProbMetric: 42.2325 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 804/1000
2023-10-27 20:43:09.139 
Epoch 804/1000 
	 loss: 41.1439, MinusLogProbMetric: 41.1439, val_loss: 42.3781, val_MinusLogProbMetric: 42.3781

Epoch 804: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1439 - MinusLogProbMetric: 41.1439 - val_loss: 42.3781 - val_MinusLogProbMetric: 42.3781 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 805/1000
2023-10-27 20:43:41.695 
Epoch 805/1000 
	 loss: 41.1995, MinusLogProbMetric: 41.1995, val_loss: 42.2076, val_MinusLogProbMetric: 42.2076

Epoch 805: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1995 - MinusLogProbMetric: 41.1995 - val_loss: 42.2076 - val_MinusLogProbMetric: 42.2076 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 806/1000
2023-10-27 20:44:14.302 
Epoch 806/1000 
	 loss: 41.1682, MinusLogProbMetric: 41.1682, val_loss: 42.2090, val_MinusLogProbMetric: 42.2090

Epoch 806: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1682 - MinusLogProbMetric: 41.1682 - val_loss: 42.2090 - val_MinusLogProbMetric: 42.2090 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 807/1000
2023-10-27 20:44:47.284 
Epoch 807/1000 
	 loss: 41.1435, MinusLogProbMetric: 41.1435, val_loss: 42.3037, val_MinusLogProbMetric: 42.3037

Epoch 807: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1435 - MinusLogProbMetric: 41.1435 - val_loss: 42.3037 - val_MinusLogProbMetric: 42.3037 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 808/1000
2023-10-27 20:45:19.609 
Epoch 808/1000 
	 loss: 41.1315, MinusLogProbMetric: 41.1315, val_loss: 42.1768, val_MinusLogProbMetric: 42.1768

Epoch 808: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.1315 - MinusLogProbMetric: 41.1315 - val_loss: 42.1768 - val_MinusLogProbMetric: 42.1768 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 809/1000
2023-10-27 20:45:51.915 
Epoch 809/1000 
	 loss: 41.1268, MinusLogProbMetric: 41.1268, val_loss: 42.3092, val_MinusLogProbMetric: 42.3092

Epoch 809: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.1268 - MinusLogProbMetric: 41.1268 - val_loss: 42.3092 - val_MinusLogProbMetric: 42.3092 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 810/1000
2023-10-27 20:46:25.019 
Epoch 810/1000 
	 loss: 41.1883, MinusLogProbMetric: 41.1883, val_loss: 43.2273, val_MinusLogProbMetric: 43.2273

Epoch 810: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1883 - MinusLogProbMetric: 41.1883 - val_loss: 43.2273 - val_MinusLogProbMetric: 43.2273 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 811/1000
2023-10-27 20:46:57.622 
Epoch 811/1000 
	 loss: 41.1317, MinusLogProbMetric: 41.1317, val_loss: 42.2470, val_MinusLogProbMetric: 42.2470

Epoch 811: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1317 - MinusLogProbMetric: 41.1317 - val_loss: 42.2470 - val_MinusLogProbMetric: 42.2470 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 812/1000
2023-10-27 20:47:30.771 
Epoch 812/1000 
	 loss: 41.1733, MinusLogProbMetric: 41.1733, val_loss: 42.4026, val_MinusLogProbMetric: 42.4026

Epoch 812: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1733 - MinusLogProbMetric: 41.1733 - val_loss: 42.4026 - val_MinusLogProbMetric: 42.4026 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 813/1000
2023-10-27 20:48:03.760 
Epoch 813/1000 
	 loss: 41.6183, MinusLogProbMetric: 41.6183, val_loss: 42.5663, val_MinusLogProbMetric: 42.5663

Epoch 813: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.6183 - MinusLogProbMetric: 41.6183 - val_loss: 42.5663 - val_MinusLogProbMetric: 42.5663 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 814/1000
2023-10-27 20:48:36.746 
Epoch 814/1000 
	 loss: 41.4889, MinusLogProbMetric: 41.4889, val_loss: 43.4038, val_MinusLogProbMetric: 43.4038

Epoch 814: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.4889 - MinusLogProbMetric: 41.4889 - val_loss: 43.4038 - val_MinusLogProbMetric: 43.4038 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 815/1000
2023-10-27 20:49:09.657 
Epoch 815/1000 
	 loss: 41.1775, MinusLogProbMetric: 41.1775, val_loss: 42.3044, val_MinusLogProbMetric: 42.3044

Epoch 815: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1775 - MinusLogProbMetric: 41.1775 - val_loss: 42.3044 - val_MinusLogProbMetric: 42.3044 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 816/1000
2023-10-27 20:49:42.630 
Epoch 816/1000 
	 loss: 41.1044, MinusLogProbMetric: 41.1044, val_loss: 42.2605, val_MinusLogProbMetric: 42.2605

Epoch 816: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.1044 - MinusLogProbMetric: 41.1044 - val_loss: 42.2605 - val_MinusLogProbMetric: 42.2605 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 817/1000
2023-10-27 20:50:15.897 
Epoch 817/1000 
	 loss: 41.0817, MinusLogProbMetric: 41.0817, val_loss: 42.3859, val_MinusLogProbMetric: 42.3859

Epoch 817: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.0817 - MinusLogProbMetric: 41.0817 - val_loss: 42.3859 - val_MinusLogProbMetric: 42.3859 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 818/1000
2023-10-27 20:50:48.981 
Epoch 818/1000 
	 loss: 41.0717, MinusLogProbMetric: 41.0717, val_loss: 42.2954, val_MinusLogProbMetric: 42.2954

Epoch 818: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.0717 - MinusLogProbMetric: 41.0717 - val_loss: 42.2954 - val_MinusLogProbMetric: 42.2954 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 819/1000
2023-10-27 20:51:21.913 
Epoch 819/1000 
	 loss: 41.0458, MinusLogProbMetric: 41.0458, val_loss: 42.3588, val_MinusLogProbMetric: 42.3588

Epoch 819: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.0458 - MinusLogProbMetric: 41.0458 - val_loss: 42.3588 - val_MinusLogProbMetric: 42.3588 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 820/1000
2023-10-27 20:51:54.917 
Epoch 820/1000 
	 loss: 41.0251, MinusLogProbMetric: 41.0251, val_loss: 42.2238, val_MinusLogProbMetric: 42.2238

Epoch 820: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.0251 - MinusLogProbMetric: 41.0251 - val_loss: 42.2238 - val_MinusLogProbMetric: 42.2238 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 821/1000
2023-10-27 20:52:27.620 
Epoch 821/1000 
	 loss: 41.0174, MinusLogProbMetric: 41.0174, val_loss: 42.2682, val_MinusLogProbMetric: 42.2682

Epoch 821: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.0174 - MinusLogProbMetric: 41.0174 - val_loss: 42.2682 - val_MinusLogProbMetric: 42.2682 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 822/1000
2023-10-27 20:53:00.096 
Epoch 822/1000 
	 loss: 41.0025, MinusLogProbMetric: 41.0025, val_loss: 42.1610, val_MinusLogProbMetric: 42.1610

Epoch 822: val_loss did not improve from 42.15607
196/196 - 32s - loss: 41.0025 - MinusLogProbMetric: 41.0025 - val_loss: 42.1610 - val_MinusLogProbMetric: 42.1610 - lr: 2.0833e-05 - 32s/epoch - 166ms/step
Epoch 823/1000
2023-10-27 20:53:32.829 
Epoch 823/1000 
	 loss: 40.9940, MinusLogProbMetric: 40.9940, val_loss: 42.1844, val_MinusLogProbMetric: 42.1844

Epoch 823: val_loss did not improve from 42.15607
196/196 - 33s - loss: 40.9940 - MinusLogProbMetric: 40.9940 - val_loss: 42.1844 - val_MinusLogProbMetric: 42.1844 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 824/1000
2023-10-27 20:54:05.653 
Epoch 824/1000 
	 loss: 41.0036, MinusLogProbMetric: 41.0036, val_loss: 42.1926, val_MinusLogProbMetric: 42.1926

Epoch 824: val_loss did not improve from 42.15607
196/196 - 33s - loss: 41.0036 - MinusLogProbMetric: 41.0036 - val_loss: 42.1926 - val_MinusLogProbMetric: 42.1926 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 825/1000
2023-10-27 20:54:38.317 
Epoch 825/1000 
	 loss: 40.9847, MinusLogProbMetric: 40.9847, val_loss: 42.2236, val_MinusLogProbMetric: 42.2236

Epoch 825: val_loss did not improve from 42.15607
196/196 - 33s - loss: 40.9847 - MinusLogProbMetric: 40.9847 - val_loss: 42.2236 - val_MinusLogProbMetric: 42.2236 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 826/1000
2023-10-27 20:55:11.473 
Epoch 826/1000 
	 loss: 40.9739, MinusLogProbMetric: 40.9739, val_loss: 42.1567, val_MinusLogProbMetric: 42.1567

Epoch 826: val_loss did not improve from 42.15607
196/196 - 33s - loss: 40.9739 - MinusLogProbMetric: 40.9739 - val_loss: 42.1567 - val_MinusLogProbMetric: 42.1567 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 827/1000
2023-10-27 20:55:44.484 
Epoch 827/1000 
	 loss: 40.9890, MinusLogProbMetric: 40.9890, val_loss: 42.3462, val_MinusLogProbMetric: 42.3462

Epoch 827: val_loss did not improve from 42.15607
196/196 - 33s - loss: 40.9890 - MinusLogProbMetric: 40.9890 - val_loss: 42.3462 - val_MinusLogProbMetric: 42.3462 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 828/1000
2023-10-27 20:56:17.286 
Epoch 828/1000 
	 loss: 40.9722, MinusLogProbMetric: 40.9722, val_loss: 42.2909, val_MinusLogProbMetric: 42.2909

Epoch 828: val_loss did not improve from 42.15607
196/196 - 33s - loss: 40.9722 - MinusLogProbMetric: 40.9722 - val_loss: 42.2909 - val_MinusLogProbMetric: 42.2909 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 829/1000
2023-10-27 20:56:50.232 
Epoch 829/1000 
	 loss: 40.9895, MinusLogProbMetric: 40.9895, val_loss: 42.2260, val_MinusLogProbMetric: 42.2260

Epoch 829: val_loss did not improve from 42.15607
196/196 - 33s - loss: 40.9895 - MinusLogProbMetric: 40.9895 - val_loss: 42.2260 - val_MinusLogProbMetric: 42.2260 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 830/1000
2023-10-27 20:57:23.582 
Epoch 830/1000 
	 loss: 40.9703, MinusLogProbMetric: 40.9703, val_loss: 42.1422, val_MinusLogProbMetric: 42.1422

Epoch 830: val_loss improved from 42.15607 to 42.14219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 40.9703 - MinusLogProbMetric: 40.9703 - val_loss: 42.1422 - val_MinusLogProbMetric: 42.1422 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 831/1000
2023-10-27 20:57:57.222 
Epoch 831/1000 
	 loss: 40.9749, MinusLogProbMetric: 40.9749, val_loss: 42.1862, val_MinusLogProbMetric: 42.1862

Epoch 831: val_loss did not improve from 42.14219
196/196 - 33s - loss: 40.9749 - MinusLogProbMetric: 40.9749 - val_loss: 42.1862 - val_MinusLogProbMetric: 42.1862 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 832/1000
2023-10-27 20:58:29.998 
Epoch 832/1000 
	 loss: 40.9762, MinusLogProbMetric: 40.9762, val_loss: 42.1255, val_MinusLogProbMetric: 42.1255

Epoch 832: val_loss improved from 42.14219 to 42.12550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 40.9762 - MinusLogProbMetric: 40.9762 - val_loss: 42.1255 - val_MinusLogProbMetric: 42.1255 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 833/1000
2023-10-27 20:59:03.426 
Epoch 833/1000 
	 loss: 40.9649, MinusLogProbMetric: 40.9649, val_loss: 42.1485, val_MinusLogProbMetric: 42.1485

Epoch 833: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9649 - MinusLogProbMetric: 40.9649 - val_loss: 42.1485 - val_MinusLogProbMetric: 42.1485 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 834/1000
2023-10-27 20:59:36.446 
Epoch 834/1000 
	 loss: 40.9706, MinusLogProbMetric: 40.9706, val_loss: 42.2028, val_MinusLogProbMetric: 42.2028

Epoch 834: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9706 - MinusLogProbMetric: 40.9706 - val_loss: 42.2028 - val_MinusLogProbMetric: 42.2028 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 835/1000
2023-10-27 21:00:09.472 
Epoch 835/1000 
	 loss: 40.9873, MinusLogProbMetric: 40.9873, val_loss: 42.1731, val_MinusLogProbMetric: 42.1731

Epoch 835: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9873 - MinusLogProbMetric: 40.9873 - val_loss: 42.1731 - val_MinusLogProbMetric: 42.1731 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 836/1000
2023-10-27 21:00:43.899 
Epoch 836/1000 
	 loss: 40.9771, MinusLogProbMetric: 40.9771, val_loss: 42.1567, val_MinusLogProbMetric: 42.1567

Epoch 836: val_loss did not improve from 42.12550
196/196 - 34s - loss: 40.9771 - MinusLogProbMetric: 40.9771 - val_loss: 42.1567 - val_MinusLogProbMetric: 42.1567 - lr: 2.0833e-05 - 34s/epoch - 176ms/step
Epoch 837/1000
2023-10-27 21:01:17.331 
Epoch 837/1000 
	 loss: 40.9716, MinusLogProbMetric: 40.9716, val_loss: 42.1763, val_MinusLogProbMetric: 42.1763

Epoch 837: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9716 - MinusLogProbMetric: 40.9716 - val_loss: 42.1763 - val_MinusLogProbMetric: 42.1763 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 838/1000
2023-10-27 21:01:50.043 
Epoch 838/1000 
	 loss: 40.9884, MinusLogProbMetric: 40.9884, val_loss: 42.2749, val_MinusLogProbMetric: 42.2749

Epoch 838: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9884 - MinusLogProbMetric: 40.9884 - val_loss: 42.2749 - val_MinusLogProbMetric: 42.2749 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 839/1000
2023-10-27 21:02:23.072 
Epoch 839/1000 
	 loss: 40.9698, MinusLogProbMetric: 40.9698, val_loss: 42.1961, val_MinusLogProbMetric: 42.1961

Epoch 839: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9698 - MinusLogProbMetric: 40.9698 - val_loss: 42.1961 - val_MinusLogProbMetric: 42.1961 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 840/1000
2023-10-27 21:02:55.972 
Epoch 840/1000 
	 loss: 40.9672, MinusLogProbMetric: 40.9672, val_loss: 42.2252, val_MinusLogProbMetric: 42.2252

Epoch 840: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9672 - MinusLogProbMetric: 40.9672 - val_loss: 42.2252 - val_MinusLogProbMetric: 42.2252 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 841/1000
2023-10-27 21:03:28.996 
Epoch 841/1000 
	 loss: 40.9741, MinusLogProbMetric: 40.9741, val_loss: 42.1627, val_MinusLogProbMetric: 42.1627

Epoch 841: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9741 - MinusLogProbMetric: 40.9741 - val_loss: 42.1627 - val_MinusLogProbMetric: 42.1627 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 842/1000
2023-10-27 21:04:01.828 
Epoch 842/1000 
	 loss: 40.9675, MinusLogProbMetric: 40.9675, val_loss: 42.1551, val_MinusLogProbMetric: 42.1551

Epoch 842: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9675 - MinusLogProbMetric: 40.9675 - val_loss: 42.1551 - val_MinusLogProbMetric: 42.1551 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 843/1000
2023-10-27 21:04:34.828 
Epoch 843/1000 
	 loss: 40.9851, MinusLogProbMetric: 40.9851, val_loss: 42.1770, val_MinusLogProbMetric: 42.1770

Epoch 843: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9851 - MinusLogProbMetric: 40.9851 - val_loss: 42.1770 - val_MinusLogProbMetric: 42.1770 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 844/1000
2023-10-27 21:05:07.603 
Epoch 844/1000 
	 loss: 40.9769, MinusLogProbMetric: 40.9769, val_loss: 42.1654, val_MinusLogProbMetric: 42.1654

Epoch 844: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9769 - MinusLogProbMetric: 40.9769 - val_loss: 42.1654 - val_MinusLogProbMetric: 42.1654 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 845/1000
2023-10-27 21:05:40.300 
Epoch 845/1000 
	 loss: 40.9776, MinusLogProbMetric: 40.9776, val_loss: 42.1547, val_MinusLogProbMetric: 42.1547

Epoch 845: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9776 - MinusLogProbMetric: 40.9776 - val_loss: 42.1547 - val_MinusLogProbMetric: 42.1547 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 846/1000
2023-10-27 21:06:13.155 
Epoch 846/1000 
	 loss: 40.9729, MinusLogProbMetric: 40.9729, val_loss: 42.1943, val_MinusLogProbMetric: 42.1943

Epoch 846: val_loss did not improve from 42.12550
196/196 - 33s - loss: 40.9729 - MinusLogProbMetric: 40.9729 - val_loss: 42.1943 - val_MinusLogProbMetric: 42.1943 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 847/1000
2023-10-27 21:06:46.871 
Epoch 847/1000 
	 loss: 40.9819, MinusLogProbMetric: 40.9819, val_loss: 42.1039, val_MinusLogProbMetric: 42.1039

Epoch 847: val_loss improved from 42.12550 to 42.10387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 40.9819 - MinusLogProbMetric: 40.9819 - val_loss: 42.1039 - val_MinusLogProbMetric: 42.1039 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 848/1000
2023-10-27 21:07:20.776 
Epoch 848/1000 
	 loss: 40.9602, MinusLogProbMetric: 40.9602, val_loss: 42.1010, val_MinusLogProbMetric: 42.1010

Epoch 848: val_loss improved from 42.10387 to 42.10103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 40.9602 - MinusLogProbMetric: 40.9602 - val_loss: 42.1010 - val_MinusLogProbMetric: 42.1010 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 849/1000
2023-10-27 21:07:54.844 
Epoch 849/1000 
	 loss: 40.9624, MinusLogProbMetric: 40.9624, val_loss: 42.1719, val_MinusLogProbMetric: 42.1719

Epoch 849: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9624 - MinusLogProbMetric: 40.9624 - val_loss: 42.1719 - val_MinusLogProbMetric: 42.1719 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 850/1000
2023-10-27 21:08:27.825 
Epoch 850/1000 
	 loss: 40.9708, MinusLogProbMetric: 40.9708, val_loss: 42.2059, val_MinusLogProbMetric: 42.2059

Epoch 850: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9708 - MinusLogProbMetric: 40.9708 - val_loss: 42.2059 - val_MinusLogProbMetric: 42.2059 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 851/1000
2023-10-27 21:09:00.930 
Epoch 851/1000 
	 loss: 40.9652, MinusLogProbMetric: 40.9652, val_loss: 42.2052, val_MinusLogProbMetric: 42.2052

Epoch 851: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9652 - MinusLogProbMetric: 40.9652 - val_loss: 42.2052 - val_MinusLogProbMetric: 42.2052 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 852/1000
2023-10-27 21:09:33.941 
Epoch 852/1000 
	 loss: 40.9629, MinusLogProbMetric: 40.9629, val_loss: 42.1025, val_MinusLogProbMetric: 42.1025

Epoch 852: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9629 - MinusLogProbMetric: 40.9629 - val_loss: 42.1025 - val_MinusLogProbMetric: 42.1025 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 853/1000
2023-10-27 21:10:06.821 
Epoch 853/1000 
	 loss: 40.9684, MinusLogProbMetric: 40.9684, val_loss: 42.2730, val_MinusLogProbMetric: 42.2730

Epoch 853: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9684 - MinusLogProbMetric: 40.9684 - val_loss: 42.2730 - val_MinusLogProbMetric: 42.2730 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 854/1000
2023-10-27 21:10:39.844 
Epoch 854/1000 
	 loss: 40.9604, MinusLogProbMetric: 40.9604, val_loss: 42.2645, val_MinusLogProbMetric: 42.2645

Epoch 854: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9604 - MinusLogProbMetric: 40.9604 - val_loss: 42.2645 - val_MinusLogProbMetric: 42.2645 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 855/1000
2023-10-27 21:11:12.902 
Epoch 855/1000 
	 loss: 40.9697, MinusLogProbMetric: 40.9697, val_loss: 42.2700, val_MinusLogProbMetric: 42.2700

Epoch 855: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9697 - MinusLogProbMetric: 40.9697 - val_loss: 42.2700 - val_MinusLogProbMetric: 42.2700 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 856/1000
2023-10-27 21:11:45.919 
Epoch 856/1000 
	 loss: 40.9670, MinusLogProbMetric: 40.9670, val_loss: 42.2476, val_MinusLogProbMetric: 42.2476

Epoch 856: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9670 - MinusLogProbMetric: 40.9670 - val_loss: 42.2476 - val_MinusLogProbMetric: 42.2476 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 857/1000
2023-10-27 21:12:18.410 
Epoch 857/1000 
	 loss: 40.9638, MinusLogProbMetric: 40.9638, val_loss: 42.1468, val_MinusLogProbMetric: 42.1468

Epoch 857: val_loss did not improve from 42.10103
196/196 - 32s - loss: 40.9638 - MinusLogProbMetric: 40.9638 - val_loss: 42.1468 - val_MinusLogProbMetric: 42.1468 - lr: 2.0833e-05 - 32s/epoch - 166ms/step
Epoch 858/1000
2023-10-27 21:12:51.033 
Epoch 858/1000 
	 loss: 40.9693, MinusLogProbMetric: 40.9693, val_loss: 42.1479, val_MinusLogProbMetric: 42.1479

Epoch 858: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9693 - MinusLogProbMetric: 40.9693 - val_loss: 42.1479 - val_MinusLogProbMetric: 42.1479 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 859/1000
2023-10-27 21:13:23.257 
Epoch 859/1000 
	 loss: 40.9769, MinusLogProbMetric: 40.9769, val_loss: 42.2449, val_MinusLogProbMetric: 42.2449

Epoch 859: val_loss did not improve from 42.10103
196/196 - 32s - loss: 40.9769 - MinusLogProbMetric: 40.9769 - val_loss: 42.2449 - val_MinusLogProbMetric: 42.2449 - lr: 2.0833e-05 - 32s/epoch - 164ms/step
Epoch 860/1000
2023-10-27 21:13:56.315 
Epoch 860/1000 
	 loss: 40.9645, MinusLogProbMetric: 40.9645, val_loss: 42.1996, val_MinusLogProbMetric: 42.1996

Epoch 860: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9645 - MinusLogProbMetric: 40.9645 - val_loss: 42.1996 - val_MinusLogProbMetric: 42.1996 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 861/1000
2023-10-27 21:14:29.162 
Epoch 861/1000 
	 loss: 40.9839, MinusLogProbMetric: 40.9839, val_loss: 42.1576, val_MinusLogProbMetric: 42.1576

Epoch 861: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9839 - MinusLogProbMetric: 40.9839 - val_loss: 42.1576 - val_MinusLogProbMetric: 42.1576 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 862/1000
2023-10-27 21:15:02.373 
Epoch 862/1000 
	 loss: 40.9629, MinusLogProbMetric: 40.9629, val_loss: 42.1351, val_MinusLogProbMetric: 42.1351

Epoch 862: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9629 - MinusLogProbMetric: 40.9629 - val_loss: 42.1351 - val_MinusLogProbMetric: 42.1351 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 863/1000
2023-10-27 21:15:35.200 
Epoch 863/1000 
	 loss: 40.9530, MinusLogProbMetric: 40.9530, val_loss: 42.1296, val_MinusLogProbMetric: 42.1296

Epoch 863: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9530 - MinusLogProbMetric: 40.9530 - val_loss: 42.1296 - val_MinusLogProbMetric: 42.1296 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 864/1000
2023-10-27 21:16:08.789 
Epoch 864/1000 
	 loss: 40.9540, MinusLogProbMetric: 40.9540, val_loss: 42.1583, val_MinusLogProbMetric: 42.1583

Epoch 864: val_loss did not improve from 42.10103
196/196 - 34s - loss: 40.9540 - MinusLogProbMetric: 40.9540 - val_loss: 42.1583 - val_MinusLogProbMetric: 42.1583 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 865/1000
2023-10-27 21:16:42.630 
Epoch 865/1000 
	 loss: 40.9562, MinusLogProbMetric: 40.9562, val_loss: 42.1054, val_MinusLogProbMetric: 42.1054

Epoch 865: val_loss did not improve from 42.10103
196/196 - 34s - loss: 40.9562 - MinusLogProbMetric: 40.9562 - val_loss: 42.1054 - val_MinusLogProbMetric: 42.1054 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 866/1000
2023-10-27 21:17:15.829 
Epoch 866/1000 
	 loss: 40.9600, MinusLogProbMetric: 40.9600, val_loss: 42.1528, val_MinusLogProbMetric: 42.1528

Epoch 866: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9600 - MinusLogProbMetric: 40.9600 - val_loss: 42.1528 - val_MinusLogProbMetric: 42.1528 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 867/1000
2023-10-27 21:17:48.832 
Epoch 867/1000 
	 loss: 40.9586, MinusLogProbMetric: 40.9586, val_loss: 42.1849, val_MinusLogProbMetric: 42.1849

Epoch 867: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9586 - MinusLogProbMetric: 40.9586 - val_loss: 42.1849 - val_MinusLogProbMetric: 42.1849 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 868/1000
2023-10-27 21:18:21.699 
Epoch 868/1000 
	 loss: 40.9547, MinusLogProbMetric: 40.9547, val_loss: 42.1169, val_MinusLogProbMetric: 42.1169

Epoch 868: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9547 - MinusLogProbMetric: 40.9547 - val_loss: 42.1169 - val_MinusLogProbMetric: 42.1169 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 869/1000
2023-10-27 21:18:55.044 
Epoch 869/1000 
	 loss: 40.9534, MinusLogProbMetric: 40.9534, val_loss: 42.2023, val_MinusLogProbMetric: 42.2023

Epoch 869: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9534 - MinusLogProbMetric: 40.9534 - val_loss: 42.2023 - val_MinusLogProbMetric: 42.2023 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 870/1000
2023-10-27 21:19:28.413 
Epoch 870/1000 
	 loss: 40.9635, MinusLogProbMetric: 40.9635, val_loss: 42.2657, val_MinusLogProbMetric: 42.2657

Epoch 870: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9635 - MinusLogProbMetric: 40.9635 - val_loss: 42.2657 - val_MinusLogProbMetric: 42.2657 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 871/1000
2023-10-27 21:20:01.794 
Epoch 871/1000 
	 loss: 40.9675, MinusLogProbMetric: 40.9675, val_loss: 42.1908, val_MinusLogProbMetric: 42.1908

Epoch 871: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9675 - MinusLogProbMetric: 40.9675 - val_loss: 42.1908 - val_MinusLogProbMetric: 42.1908 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 872/1000
2023-10-27 21:20:34.904 
Epoch 872/1000 
	 loss: 40.9508, MinusLogProbMetric: 40.9508, val_loss: 42.1669, val_MinusLogProbMetric: 42.1669

Epoch 872: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9508 - MinusLogProbMetric: 40.9508 - val_loss: 42.1669 - val_MinusLogProbMetric: 42.1669 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 873/1000
2023-10-27 21:21:08.315 
Epoch 873/1000 
	 loss: 40.9542, MinusLogProbMetric: 40.9542, val_loss: 42.2080, val_MinusLogProbMetric: 42.2080

Epoch 873: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9542 - MinusLogProbMetric: 40.9542 - val_loss: 42.2080 - val_MinusLogProbMetric: 42.2080 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 874/1000
2023-10-27 21:21:41.537 
Epoch 874/1000 
	 loss: 40.9535, MinusLogProbMetric: 40.9535, val_loss: 42.1306, val_MinusLogProbMetric: 42.1306

Epoch 874: val_loss did not improve from 42.10103
196/196 - 33s - loss: 40.9535 - MinusLogProbMetric: 40.9535 - val_loss: 42.1306 - val_MinusLogProbMetric: 42.1306 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 875/1000
2023-10-27 21:22:14.159 
Epoch 875/1000 
	 loss: 40.9498, MinusLogProbMetric: 40.9498, val_loss: 42.0976, val_MinusLogProbMetric: 42.0976

Epoch 875: val_loss improved from 42.10103 to 42.09757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 40.9498 - MinusLogProbMetric: 40.9498 - val_loss: 42.0976 - val_MinusLogProbMetric: 42.0976 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 876/1000
2023-10-27 21:22:47.540 
Epoch 876/1000 
	 loss: 40.9524, MinusLogProbMetric: 40.9524, val_loss: 42.2268, val_MinusLogProbMetric: 42.2268

Epoch 876: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9524 - MinusLogProbMetric: 40.9524 - val_loss: 42.2268 - val_MinusLogProbMetric: 42.2268 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 877/1000
2023-10-27 21:23:20.022 
Epoch 877/1000 
	 loss: 40.9539, MinusLogProbMetric: 40.9539, val_loss: 42.3654, val_MinusLogProbMetric: 42.3654

Epoch 877: val_loss did not improve from 42.09757
196/196 - 32s - loss: 40.9539 - MinusLogProbMetric: 40.9539 - val_loss: 42.3654 - val_MinusLogProbMetric: 42.3654 - lr: 2.0833e-05 - 32s/epoch - 166ms/step
Epoch 878/1000
2023-10-27 21:23:52.751 
Epoch 878/1000 
	 loss: 40.9622, MinusLogProbMetric: 40.9622, val_loss: 42.1337, val_MinusLogProbMetric: 42.1337

Epoch 878: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9622 - MinusLogProbMetric: 40.9622 - val_loss: 42.1337 - val_MinusLogProbMetric: 42.1337 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 879/1000
2023-10-27 21:24:25.413 
Epoch 879/1000 
	 loss: 40.9460, MinusLogProbMetric: 40.9460, val_loss: 42.1283, val_MinusLogProbMetric: 42.1283

Epoch 879: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9460 - MinusLogProbMetric: 40.9460 - val_loss: 42.1283 - val_MinusLogProbMetric: 42.1283 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 880/1000
2023-10-27 21:24:58.151 
Epoch 880/1000 
	 loss: 40.9514, MinusLogProbMetric: 40.9514, val_loss: 42.1578, val_MinusLogProbMetric: 42.1578

Epoch 880: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9514 - MinusLogProbMetric: 40.9514 - val_loss: 42.1578 - val_MinusLogProbMetric: 42.1578 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 881/1000
2023-10-27 21:25:31.280 
Epoch 881/1000 
	 loss: 40.9556, MinusLogProbMetric: 40.9556, val_loss: 42.2422, val_MinusLogProbMetric: 42.2422

Epoch 881: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9556 - MinusLogProbMetric: 40.9556 - val_loss: 42.2422 - val_MinusLogProbMetric: 42.2422 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 882/1000
2023-10-27 21:26:04.044 
Epoch 882/1000 
	 loss: 40.9745, MinusLogProbMetric: 40.9745, val_loss: 42.3789, val_MinusLogProbMetric: 42.3789

Epoch 882: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9745 - MinusLogProbMetric: 40.9745 - val_loss: 42.3789 - val_MinusLogProbMetric: 42.3789 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 883/1000
2023-10-27 21:26:37.091 
Epoch 883/1000 
	 loss: 40.9470, MinusLogProbMetric: 40.9470, val_loss: 42.1445, val_MinusLogProbMetric: 42.1445

Epoch 883: val_loss did not improve from 42.09757
196/196 - 33s - loss: 40.9470 - MinusLogProbMetric: 40.9470 - val_loss: 42.1445 - val_MinusLogProbMetric: 42.1445 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 884/1000
2023-10-27 21:27:10.178 
Epoch 884/1000 
	 loss: 40.9627, MinusLogProbMetric: 40.9627, val_loss: 42.0958, val_MinusLogProbMetric: 42.0958

Epoch 884: val_loss improved from 42.09757 to 42.09580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 40.9627 - MinusLogProbMetric: 40.9627 - val_loss: 42.0958 - val_MinusLogProbMetric: 42.0958 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 885/1000
2023-10-27 21:27:43.283 
Epoch 885/1000 
	 loss: 40.9579, MinusLogProbMetric: 40.9579, val_loss: 42.2261, val_MinusLogProbMetric: 42.2261

Epoch 885: val_loss did not improve from 42.09580
196/196 - 32s - loss: 40.9579 - MinusLogProbMetric: 40.9579 - val_loss: 42.2261 - val_MinusLogProbMetric: 42.2261 - lr: 2.0833e-05 - 32s/epoch - 166ms/step
Epoch 886/1000
2023-10-27 21:28:16.629 
Epoch 886/1000 
	 loss: 40.9599, MinusLogProbMetric: 40.9599, val_loss: 42.2358, val_MinusLogProbMetric: 42.2358

Epoch 886: val_loss did not improve from 42.09580
196/196 - 33s - loss: 40.9599 - MinusLogProbMetric: 40.9599 - val_loss: 42.2358 - val_MinusLogProbMetric: 42.2358 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 887/1000
2023-10-27 21:28:49.598 
Epoch 887/1000 
	 loss: 40.9602, MinusLogProbMetric: 40.9602, val_loss: 42.1432, val_MinusLogProbMetric: 42.1432

Epoch 887: val_loss did not improve from 42.09580
196/196 - 33s - loss: 40.9602 - MinusLogProbMetric: 40.9602 - val_loss: 42.1432 - val_MinusLogProbMetric: 42.1432 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 888/1000
2023-10-27 21:29:22.377 
Epoch 888/1000 
	 loss: 40.9547, MinusLogProbMetric: 40.9547, val_loss: 42.1526, val_MinusLogProbMetric: 42.1526

Epoch 888: val_loss did not improve from 42.09580
196/196 - 33s - loss: 40.9547 - MinusLogProbMetric: 40.9547 - val_loss: 42.1526 - val_MinusLogProbMetric: 42.1526 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 889/1000
2023-10-27 21:29:55.284 
Epoch 889/1000 
	 loss: 40.9430, MinusLogProbMetric: 40.9430, val_loss: 42.1214, val_MinusLogProbMetric: 42.1214

Epoch 889: val_loss did not improve from 42.09580
196/196 - 33s - loss: 40.9430 - MinusLogProbMetric: 40.9430 - val_loss: 42.1214 - val_MinusLogProbMetric: 42.1214 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 890/1000
2023-10-27 21:30:28.365 
Epoch 890/1000 
	 loss: 40.9587, MinusLogProbMetric: 40.9587, val_loss: 42.1323, val_MinusLogProbMetric: 42.1323

Epoch 890: val_loss did not improve from 42.09580
196/196 - 33s - loss: 40.9587 - MinusLogProbMetric: 40.9587 - val_loss: 42.1323 - val_MinusLogProbMetric: 42.1323 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 891/1000
2023-10-27 21:31:01.269 
Epoch 891/1000 
	 loss: 40.9528, MinusLogProbMetric: 40.9528, val_loss: 42.1410, val_MinusLogProbMetric: 42.1410

Epoch 891: val_loss did not improve from 42.09580
196/196 - 33s - loss: 40.9528 - MinusLogProbMetric: 40.9528 - val_loss: 42.1410 - val_MinusLogProbMetric: 42.1410 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 892/1000
2023-10-27 21:31:34.144 
Epoch 892/1000 
	 loss: 40.9423, MinusLogProbMetric: 40.9423, val_loss: 42.0885, val_MinusLogProbMetric: 42.0885

Epoch 892: val_loss improved from 42.09580 to 42.08853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 33s - loss: 40.9423 - MinusLogProbMetric: 40.9423 - val_loss: 42.0885 - val_MinusLogProbMetric: 42.0885 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 893/1000
2023-10-27 21:32:07.749 
Epoch 893/1000 
	 loss: 40.9395, MinusLogProbMetric: 40.9395, val_loss: 42.3694, val_MinusLogProbMetric: 42.3694

Epoch 893: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9395 - MinusLogProbMetric: 40.9395 - val_loss: 42.3694 - val_MinusLogProbMetric: 42.3694 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 894/1000
2023-10-27 21:32:40.840 
Epoch 894/1000 
	 loss: 40.9381, MinusLogProbMetric: 40.9381, val_loss: 42.1598, val_MinusLogProbMetric: 42.1598

Epoch 894: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9381 - MinusLogProbMetric: 40.9381 - val_loss: 42.1598 - val_MinusLogProbMetric: 42.1598 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 895/1000
2023-10-27 21:33:14.018 
Epoch 895/1000 
	 loss: 40.9703, MinusLogProbMetric: 40.9703, val_loss: 42.1376, val_MinusLogProbMetric: 42.1376

Epoch 895: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9703 - MinusLogProbMetric: 40.9703 - val_loss: 42.1376 - val_MinusLogProbMetric: 42.1376 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 896/1000
2023-10-27 21:33:47.011 
Epoch 896/1000 
	 loss: 40.9528, MinusLogProbMetric: 40.9528, val_loss: 42.1248, val_MinusLogProbMetric: 42.1248

Epoch 896: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9528 - MinusLogProbMetric: 40.9528 - val_loss: 42.1248 - val_MinusLogProbMetric: 42.1248 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 897/1000
2023-10-27 21:34:20.429 
Epoch 897/1000 
	 loss: 40.9736, MinusLogProbMetric: 40.9736, val_loss: 42.0975, val_MinusLogProbMetric: 42.0975

Epoch 897: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9736 - MinusLogProbMetric: 40.9736 - val_loss: 42.0975 - val_MinusLogProbMetric: 42.0975 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 898/1000
2023-10-27 21:34:53.715 
Epoch 898/1000 
	 loss: 40.9763, MinusLogProbMetric: 40.9763, val_loss: 42.1489, val_MinusLogProbMetric: 42.1489

Epoch 898: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9763 - MinusLogProbMetric: 40.9763 - val_loss: 42.1489 - val_MinusLogProbMetric: 42.1489 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 899/1000
2023-10-27 21:35:27.124 
Epoch 899/1000 
	 loss: 40.9416, MinusLogProbMetric: 40.9416, val_loss: 42.1796, val_MinusLogProbMetric: 42.1796

Epoch 899: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9416 - MinusLogProbMetric: 40.9416 - val_loss: 42.1796 - val_MinusLogProbMetric: 42.1796 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 900/1000
2023-10-27 21:36:00.891 
Epoch 900/1000 
	 loss: 40.9442, MinusLogProbMetric: 40.9442, val_loss: 42.1547, val_MinusLogProbMetric: 42.1547

Epoch 900: val_loss did not improve from 42.08853
196/196 - 34s - loss: 40.9442 - MinusLogProbMetric: 40.9442 - val_loss: 42.1547 - val_MinusLogProbMetric: 42.1547 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 901/1000
2023-10-27 21:36:34.185 
Epoch 901/1000 
	 loss: 40.9513, MinusLogProbMetric: 40.9513, val_loss: 42.1677, val_MinusLogProbMetric: 42.1677

Epoch 901: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9513 - MinusLogProbMetric: 40.9513 - val_loss: 42.1677 - val_MinusLogProbMetric: 42.1677 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 902/1000
2023-10-27 21:37:06.839 
Epoch 902/1000 
	 loss: 40.9542, MinusLogProbMetric: 40.9542, val_loss: 42.1488, val_MinusLogProbMetric: 42.1488

Epoch 902: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9542 - MinusLogProbMetric: 40.9542 - val_loss: 42.1488 - val_MinusLogProbMetric: 42.1488 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 903/1000
2023-10-27 21:37:39.869 
Epoch 903/1000 
	 loss: 40.9520, MinusLogProbMetric: 40.9520, val_loss: 42.2565, val_MinusLogProbMetric: 42.2565

Epoch 903: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9520 - MinusLogProbMetric: 40.9520 - val_loss: 42.2565 - val_MinusLogProbMetric: 42.2565 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 904/1000
2023-10-27 21:38:13.098 
Epoch 904/1000 
	 loss: 40.9480, MinusLogProbMetric: 40.9480, val_loss: 42.4650, val_MinusLogProbMetric: 42.4650

Epoch 904: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9480 - MinusLogProbMetric: 40.9480 - val_loss: 42.4650 - val_MinusLogProbMetric: 42.4650 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 905/1000
2023-10-27 21:38:46.433 
Epoch 905/1000 
	 loss: 40.9467, MinusLogProbMetric: 40.9467, val_loss: 42.1516, val_MinusLogProbMetric: 42.1516

Epoch 905: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9467 - MinusLogProbMetric: 40.9467 - val_loss: 42.1516 - val_MinusLogProbMetric: 42.1516 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 906/1000
2023-10-27 21:39:19.579 
Epoch 906/1000 
	 loss: 40.9317, MinusLogProbMetric: 40.9317, val_loss: 42.1353, val_MinusLogProbMetric: 42.1353

Epoch 906: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9317 - MinusLogProbMetric: 40.9317 - val_loss: 42.1353 - val_MinusLogProbMetric: 42.1353 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 907/1000
2023-10-27 21:39:53.298 
Epoch 907/1000 
	 loss: 40.9430, MinusLogProbMetric: 40.9430, val_loss: 42.1222, val_MinusLogProbMetric: 42.1222

Epoch 907: val_loss did not improve from 42.08853
196/196 - 34s - loss: 40.9430 - MinusLogProbMetric: 40.9430 - val_loss: 42.1222 - val_MinusLogProbMetric: 42.1222 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 908/1000
2023-10-27 21:40:26.752 
Epoch 908/1000 
	 loss: 40.9481, MinusLogProbMetric: 40.9481, val_loss: 42.1438, val_MinusLogProbMetric: 42.1438

Epoch 908: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9481 - MinusLogProbMetric: 40.9481 - val_loss: 42.1438 - val_MinusLogProbMetric: 42.1438 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 909/1000
2023-10-27 21:40:59.963 
Epoch 909/1000 
	 loss: 40.9618, MinusLogProbMetric: 40.9618, val_loss: 42.1181, val_MinusLogProbMetric: 42.1181

Epoch 909: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9618 - MinusLogProbMetric: 40.9618 - val_loss: 42.1181 - val_MinusLogProbMetric: 42.1181 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 910/1000
2023-10-27 21:41:33.361 
Epoch 910/1000 
	 loss: 40.9639, MinusLogProbMetric: 40.9639, val_loss: 42.2301, val_MinusLogProbMetric: 42.2301

Epoch 910: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9639 - MinusLogProbMetric: 40.9639 - val_loss: 42.2301 - val_MinusLogProbMetric: 42.2301 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 911/1000
2023-10-27 21:42:06.835 
Epoch 911/1000 
	 loss: 40.9478, MinusLogProbMetric: 40.9478, val_loss: 42.1756, val_MinusLogProbMetric: 42.1756

Epoch 911: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9478 - MinusLogProbMetric: 40.9478 - val_loss: 42.1756 - val_MinusLogProbMetric: 42.1756 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 912/1000
2023-10-27 21:42:40.300 
Epoch 912/1000 
	 loss: 40.9339, MinusLogProbMetric: 40.9339, val_loss: 42.1712, val_MinusLogProbMetric: 42.1712

Epoch 912: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9339 - MinusLogProbMetric: 40.9339 - val_loss: 42.1712 - val_MinusLogProbMetric: 42.1712 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 913/1000
2023-10-27 21:43:14.344 
Epoch 913/1000 
	 loss: 40.9449, MinusLogProbMetric: 40.9449, val_loss: 42.3765, val_MinusLogProbMetric: 42.3765

Epoch 913: val_loss did not improve from 42.08853
196/196 - 34s - loss: 40.9449 - MinusLogProbMetric: 40.9449 - val_loss: 42.3765 - val_MinusLogProbMetric: 42.3765 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 914/1000
2023-10-27 21:43:47.426 
Epoch 914/1000 
	 loss: 40.9678, MinusLogProbMetric: 40.9678, val_loss: 42.2136, val_MinusLogProbMetric: 42.2136

Epoch 914: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9678 - MinusLogProbMetric: 40.9678 - val_loss: 42.2136 - val_MinusLogProbMetric: 42.2136 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 915/1000
2023-10-27 21:44:20.614 
Epoch 915/1000 
	 loss: 40.9475, MinusLogProbMetric: 40.9475, val_loss: 42.1431, val_MinusLogProbMetric: 42.1431

Epoch 915: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9475 - MinusLogProbMetric: 40.9475 - val_loss: 42.1431 - val_MinusLogProbMetric: 42.1431 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 916/1000
2023-10-27 21:44:53.538 
Epoch 916/1000 
	 loss: 40.9362, MinusLogProbMetric: 40.9362, val_loss: 42.1290, val_MinusLogProbMetric: 42.1290

Epoch 916: val_loss did not improve from 42.08853
196/196 - 33s - loss: 40.9362 - MinusLogProbMetric: 40.9362 - val_loss: 42.1290 - val_MinusLogProbMetric: 42.1290 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 917/1000
2023-10-27 21:45:25.651 
Epoch 917/1000 
	 loss: 40.9445, MinusLogProbMetric: 40.9445, val_loss: 42.1274, val_MinusLogProbMetric: 42.1274

Epoch 917: val_loss did not improve from 42.08853
196/196 - 32s - loss: 40.9445 - MinusLogProbMetric: 40.9445 - val_loss: 42.1274 - val_MinusLogProbMetric: 42.1274 - lr: 2.0833e-05 - 32s/epoch - 164ms/step
Epoch 918/1000
2023-10-27 21:45:54.326 
Epoch 918/1000 
	 loss: 40.9463, MinusLogProbMetric: 40.9463, val_loss: 42.1245, val_MinusLogProbMetric: 42.1245

Epoch 918: val_loss did not improve from 42.08853
196/196 - 29s - loss: 40.9463 - MinusLogProbMetric: 40.9463 - val_loss: 42.1245 - val_MinusLogProbMetric: 42.1245 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 919/1000
2023-10-27 21:46:23.821 
Epoch 919/1000 
	 loss: 40.9373, MinusLogProbMetric: 40.9373, val_loss: 42.0898, val_MinusLogProbMetric: 42.0898

Epoch 919: val_loss did not improve from 42.08853
196/196 - 29s - loss: 40.9373 - MinusLogProbMetric: 40.9373 - val_loss: 42.0898 - val_MinusLogProbMetric: 42.0898 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 920/1000
2023-10-27 21:46:55.601 
Epoch 920/1000 
	 loss: 40.9513, MinusLogProbMetric: 40.9513, val_loss: 42.1306, val_MinusLogProbMetric: 42.1306

Epoch 920: val_loss did not improve from 42.08853
196/196 - 32s - loss: 40.9513 - MinusLogProbMetric: 40.9513 - val_loss: 42.1306 - val_MinusLogProbMetric: 42.1306 - lr: 2.0833e-05 - 32s/epoch - 162ms/step
Epoch 921/1000
2023-10-27 21:47:23.880 
Epoch 921/1000 
	 loss: 40.9376, MinusLogProbMetric: 40.9376, val_loss: 42.1827, val_MinusLogProbMetric: 42.1827

Epoch 921: val_loss did not improve from 42.08853
196/196 - 28s - loss: 40.9376 - MinusLogProbMetric: 40.9376 - val_loss: 42.1827 - val_MinusLogProbMetric: 42.1827 - lr: 2.0833e-05 - 28s/epoch - 144ms/step
Epoch 922/1000
2023-10-27 21:47:55.066 
Epoch 922/1000 
	 loss: 40.9379, MinusLogProbMetric: 40.9379, val_loss: 42.1434, val_MinusLogProbMetric: 42.1434

Epoch 922: val_loss did not improve from 42.08853
196/196 - 31s - loss: 40.9379 - MinusLogProbMetric: 40.9379 - val_loss: 42.1434 - val_MinusLogProbMetric: 42.1434 - lr: 2.0833e-05 - 31s/epoch - 159ms/step
Epoch 923/1000
2023-10-27 21:48:25.383 
Epoch 923/1000 
	 loss: 40.9330, MinusLogProbMetric: 40.9330, val_loss: 42.1166, val_MinusLogProbMetric: 42.1166

Epoch 923: val_loss did not improve from 42.08853
196/196 - 30s - loss: 40.9330 - MinusLogProbMetric: 40.9330 - val_loss: 42.1166 - val_MinusLogProbMetric: 42.1166 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 924/1000
2023-10-27 21:48:54.880 
Epoch 924/1000 
	 loss: 40.9393, MinusLogProbMetric: 40.9393, val_loss: 42.1476, val_MinusLogProbMetric: 42.1476

Epoch 924: val_loss did not improve from 42.08853
196/196 - 29s - loss: 40.9393 - MinusLogProbMetric: 40.9393 - val_loss: 42.1476 - val_MinusLogProbMetric: 42.1476 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 925/1000
2023-10-27 21:49:27.173 
Epoch 925/1000 
	 loss: 40.9301, MinusLogProbMetric: 40.9301, val_loss: 42.1362, val_MinusLogProbMetric: 42.1362

Epoch 925: val_loss did not improve from 42.08853
196/196 - 32s - loss: 40.9301 - MinusLogProbMetric: 40.9301 - val_loss: 42.1362 - val_MinusLogProbMetric: 42.1362 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 926/1000
2023-10-27 21:49:57.110 
Epoch 926/1000 
	 loss: 40.9267, MinusLogProbMetric: 40.9267, val_loss: 42.2170, val_MinusLogProbMetric: 42.2170

Epoch 926: val_loss did not improve from 42.08853
196/196 - 30s - loss: 40.9267 - MinusLogProbMetric: 40.9267 - val_loss: 42.2170 - val_MinusLogProbMetric: 42.2170 - lr: 2.0833e-05 - 30s/epoch - 153ms/step
Epoch 927/1000
2023-10-27 21:50:27.202 
Epoch 927/1000 
	 loss: 40.9641, MinusLogProbMetric: 40.9641, val_loss: 42.1370, val_MinusLogProbMetric: 42.1370

Epoch 927: val_loss did not improve from 42.08853
196/196 - 30s - loss: 40.9641 - MinusLogProbMetric: 40.9641 - val_loss: 42.1370 - val_MinusLogProbMetric: 42.1370 - lr: 2.0833e-05 - 30s/epoch - 154ms/step
Epoch 928/1000
2023-10-27 21:50:59.607 
Epoch 928/1000 
	 loss: 40.9520, MinusLogProbMetric: 40.9520, val_loss: 42.2882, val_MinusLogProbMetric: 42.2882

Epoch 928: val_loss did not improve from 42.08853
196/196 - 32s - loss: 40.9520 - MinusLogProbMetric: 40.9520 - val_loss: 42.2882 - val_MinusLogProbMetric: 42.2882 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 929/1000
2023-10-27 21:51:28.026 
Epoch 929/1000 
	 loss: 40.9529, MinusLogProbMetric: 40.9529, val_loss: 42.1225, val_MinusLogProbMetric: 42.1225

Epoch 929: val_loss did not improve from 42.08853
196/196 - 28s - loss: 40.9529 - MinusLogProbMetric: 40.9529 - val_loss: 42.1225 - val_MinusLogProbMetric: 42.1225 - lr: 2.0833e-05 - 28s/epoch - 145ms/step
Epoch 930/1000
2023-10-27 21:51:57.437 
Epoch 930/1000 
	 loss: 40.9316, MinusLogProbMetric: 40.9316, val_loss: 42.1301, val_MinusLogProbMetric: 42.1301

Epoch 930: val_loss did not improve from 42.08853
196/196 - 29s - loss: 40.9316 - MinusLogProbMetric: 40.9316 - val_loss: 42.1301 - val_MinusLogProbMetric: 42.1301 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 931/1000
2023-10-27 21:52:28.134 
Epoch 931/1000 
	 loss: 40.9264, MinusLogProbMetric: 40.9264, val_loss: 42.1565, val_MinusLogProbMetric: 42.1565

Epoch 931: val_loss did not improve from 42.08853
196/196 - 31s - loss: 40.9264 - MinusLogProbMetric: 40.9264 - val_loss: 42.1565 - val_MinusLogProbMetric: 42.1565 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 932/1000
2023-10-27 21:52:56.763 
Epoch 932/1000 
	 loss: 40.9434, MinusLogProbMetric: 40.9434, val_loss: 42.2013, val_MinusLogProbMetric: 42.2013

Epoch 932: val_loss did not improve from 42.08853
196/196 - 29s - loss: 40.9434 - MinusLogProbMetric: 40.9434 - val_loss: 42.2013 - val_MinusLogProbMetric: 42.2013 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 933/1000
2023-10-27 21:53:27.820 
Epoch 933/1000 
	 loss: 40.9479, MinusLogProbMetric: 40.9479, val_loss: 42.2762, val_MinusLogProbMetric: 42.2762

Epoch 933: val_loss did not improve from 42.08853
196/196 - 31s - loss: 40.9479 - MinusLogProbMetric: 40.9479 - val_loss: 42.2762 - val_MinusLogProbMetric: 42.2762 - lr: 2.0833e-05 - 31s/epoch - 158ms/step
Epoch 934/1000
2023-10-27 21:53:58.404 
Epoch 934/1000 
	 loss: 40.9403, MinusLogProbMetric: 40.9403, val_loss: 42.2937, val_MinusLogProbMetric: 42.2937

Epoch 934: val_loss did not improve from 42.08853
196/196 - 31s - loss: 40.9403 - MinusLogProbMetric: 40.9403 - val_loss: 42.2937 - val_MinusLogProbMetric: 42.2937 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 935/1000
2023-10-27 21:54:27.491 
Epoch 935/1000 
	 loss: 40.9451, MinusLogProbMetric: 40.9451, val_loss: 42.1927, val_MinusLogProbMetric: 42.1927

Epoch 935: val_loss did not improve from 42.08853
196/196 - 29s - loss: 40.9451 - MinusLogProbMetric: 40.9451 - val_loss: 42.1927 - val_MinusLogProbMetric: 42.1927 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 936/1000
2023-10-27 21:54:59.283 
Epoch 936/1000 
	 loss: 40.9335, MinusLogProbMetric: 40.9335, val_loss: 42.0874, val_MinusLogProbMetric: 42.0874

Epoch 936: val_loss improved from 42.08853 to 42.08743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 32s - loss: 40.9335 - MinusLogProbMetric: 40.9335 - val_loss: 42.0874 - val_MinusLogProbMetric: 42.0874 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 937/1000
2023-10-27 21:55:28.822 
Epoch 937/1000 
	 loss: 40.9189, MinusLogProbMetric: 40.9189, val_loss: 42.1618, val_MinusLogProbMetric: 42.1618

Epoch 937: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9189 - MinusLogProbMetric: 40.9189 - val_loss: 42.1618 - val_MinusLogProbMetric: 42.1618 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 938/1000
2023-10-27 21:55:59.673 
Epoch 938/1000 
	 loss: 40.9214, MinusLogProbMetric: 40.9214, val_loss: 42.1176, val_MinusLogProbMetric: 42.1176

Epoch 938: val_loss did not improve from 42.08743
196/196 - 31s - loss: 40.9214 - MinusLogProbMetric: 40.9214 - val_loss: 42.1176 - val_MinusLogProbMetric: 42.1176 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 939/1000
2023-10-27 21:56:30.162 
Epoch 939/1000 
	 loss: 40.9330, MinusLogProbMetric: 40.9330, val_loss: 42.2041, val_MinusLogProbMetric: 42.2041

Epoch 939: val_loss did not improve from 42.08743
196/196 - 30s - loss: 40.9330 - MinusLogProbMetric: 40.9330 - val_loss: 42.2041 - val_MinusLogProbMetric: 42.2041 - lr: 2.0833e-05 - 30s/epoch - 156ms/step
Epoch 940/1000
2023-10-27 21:56:59.327 
Epoch 940/1000 
	 loss: 40.9325, MinusLogProbMetric: 40.9325, val_loss: 42.1549, val_MinusLogProbMetric: 42.1549

Epoch 940: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9325 - MinusLogProbMetric: 40.9325 - val_loss: 42.1549 - val_MinusLogProbMetric: 42.1549 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 941/1000
2023-10-27 21:57:28.897 
Epoch 941/1000 
	 loss: 40.9460, MinusLogProbMetric: 40.9460, val_loss: 42.1446, val_MinusLogProbMetric: 42.1446

Epoch 941: val_loss did not improve from 42.08743
196/196 - 30s - loss: 40.9460 - MinusLogProbMetric: 40.9460 - val_loss: 42.1446 - val_MinusLogProbMetric: 42.1446 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 942/1000
2023-10-27 21:57:58.305 
Epoch 942/1000 
	 loss: 40.9251, MinusLogProbMetric: 40.9251, val_loss: 42.1177, val_MinusLogProbMetric: 42.1177

Epoch 942: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9251 - MinusLogProbMetric: 40.9251 - val_loss: 42.1177 - val_MinusLogProbMetric: 42.1177 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 943/1000
2023-10-27 21:58:27.475 
Epoch 943/1000 
	 loss: 40.9392, MinusLogProbMetric: 40.9392, val_loss: 42.2545, val_MinusLogProbMetric: 42.2545

Epoch 943: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9392 - MinusLogProbMetric: 40.9392 - val_loss: 42.2545 - val_MinusLogProbMetric: 42.2545 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 944/1000
2023-10-27 21:58:58.328 
Epoch 944/1000 
	 loss: 40.9697, MinusLogProbMetric: 40.9697, val_loss: 42.1788, val_MinusLogProbMetric: 42.1788

Epoch 944: val_loss did not improve from 42.08743
196/196 - 31s - loss: 40.9697 - MinusLogProbMetric: 40.9697 - val_loss: 42.1788 - val_MinusLogProbMetric: 42.1788 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 945/1000
2023-10-27 21:59:27.421 
Epoch 945/1000 
	 loss: 40.9433, MinusLogProbMetric: 40.9433, val_loss: 42.2115, val_MinusLogProbMetric: 42.2115

Epoch 945: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9433 - MinusLogProbMetric: 40.9433 - val_loss: 42.2115 - val_MinusLogProbMetric: 42.2115 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 946/1000
2023-10-27 21:59:56.656 
Epoch 946/1000 
	 loss: 40.9516, MinusLogProbMetric: 40.9516, val_loss: 42.1371, val_MinusLogProbMetric: 42.1371

Epoch 946: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9516 - MinusLogProbMetric: 40.9516 - val_loss: 42.1371 - val_MinusLogProbMetric: 42.1371 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 947/1000
2023-10-27 22:00:28.746 
Epoch 947/1000 
	 loss: 40.9188, MinusLogProbMetric: 40.9188, val_loss: 42.2149, val_MinusLogProbMetric: 42.2149

Epoch 947: val_loss did not improve from 42.08743
196/196 - 32s - loss: 40.9188 - MinusLogProbMetric: 40.9188 - val_loss: 42.2149 - val_MinusLogProbMetric: 42.2149 - lr: 2.0833e-05 - 32s/epoch - 164ms/step
Epoch 948/1000
2023-10-27 22:00:58.151 
Epoch 948/1000 
	 loss: 40.9352, MinusLogProbMetric: 40.9352, val_loss: 42.2344, val_MinusLogProbMetric: 42.2344

Epoch 948: val_loss did not improve from 42.08743
196/196 - 29s - loss: 40.9352 - MinusLogProbMetric: 40.9352 - val_loss: 42.2344 - val_MinusLogProbMetric: 42.2344 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 949/1000
2023-10-27 22:01:28.073 
Epoch 949/1000 
	 loss: 40.9320, MinusLogProbMetric: 40.9320, val_loss: 42.0587, val_MinusLogProbMetric: 42.0587

Epoch 949: val_loss improved from 42.08743 to 42.05871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 30s - loss: 40.9320 - MinusLogProbMetric: 40.9320 - val_loss: 42.0587 - val_MinusLogProbMetric: 42.0587 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 950/1000
2023-10-27 22:01:58.637 
Epoch 950/1000 
	 loss: 40.9409, MinusLogProbMetric: 40.9409, val_loss: 42.1585, val_MinusLogProbMetric: 42.1585

Epoch 950: val_loss did not improve from 42.05871
196/196 - 30s - loss: 40.9409 - MinusLogProbMetric: 40.9409 - val_loss: 42.1585 - val_MinusLogProbMetric: 42.1585 - lr: 2.0833e-05 - 30s/epoch - 153ms/step
Epoch 951/1000
2023-10-27 22:02:27.631 
Epoch 951/1000 
	 loss: 40.9324, MinusLogProbMetric: 40.9324, val_loss: 42.1479, val_MinusLogProbMetric: 42.1479

Epoch 951: val_loss did not improve from 42.05871
196/196 - 29s - loss: 40.9324 - MinusLogProbMetric: 40.9324 - val_loss: 42.1479 - val_MinusLogProbMetric: 42.1479 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 952/1000
2023-10-27 22:02:59.368 
Epoch 952/1000 
	 loss: 40.9369, MinusLogProbMetric: 40.9369, val_loss: 42.2693, val_MinusLogProbMetric: 42.2693

Epoch 952: val_loss did not improve from 42.05871
196/196 - 32s - loss: 40.9369 - MinusLogProbMetric: 40.9369 - val_loss: 42.2693 - val_MinusLogProbMetric: 42.2693 - lr: 2.0833e-05 - 32s/epoch - 162ms/step
Epoch 953/1000
2023-10-27 22:03:29.847 
Epoch 953/1000 
	 loss: 40.9424, MinusLogProbMetric: 40.9424, val_loss: 42.1620, val_MinusLogProbMetric: 42.1620

Epoch 953: val_loss did not improve from 42.05871
196/196 - 30s - loss: 40.9424 - MinusLogProbMetric: 40.9424 - val_loss: 42.1620 - val_MinusLogProbMetric: 42.1620 - lr: 2.0833e-05 - 30s/epoch - 156ms/step
Epoch 954/1000
2023-10-27 22:03:58.676 
Epoch 954/1000 
	 loss: 40.9295, MinusLogProbMetric: 40.9295, val_loss: 42.1461, val_MinusLogProbMetric: 42.1461

Epoch 954: val_loss did not improve from 42.05871
196/196 - 29s - loss: 40.9295 - MinusLogProbMetric: 40.9295 - val_loss: 42.1461 - val_MinusLogProbMetric: 42.1461 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 955/1000
2023-10-27 22:04:30.331 
Epoch 955/1000 
	 loss: 40.9339, MinusLogProbMetric: 40.9339, val_loss: 42.1612, val_MinusLogProbMetric: 42.1612

Epoch 955: val_loss did not improve from 42.05871
196/196 - 32s - loss: 40.9339 - MinusLogProbMetric: 40.9339 - val_loss: 42.1612 - val_MinusLogProbMetric: 42.1612 - lr: 2.0833e-05 - 32s/epoch - 161ms/step
Epoch 956/1000
2023-10-27 22:05:03.469 
Epoch 956/1000 
	 loss: 40.9337, MinusLogProbMetric: 40.9337, val_loss: 42.1778, val_MinusLogProbMetric: 42.1778

Epoch 956: val_loss did not improve from 42.05871
196/196 - 33s - loss: 40.9337 - MinusLogProbMetric: 40.9337 - val_loss: 42.1778 - val_MinusLogProbMetric: 42.1778 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 957/1000
2023-10-27 22:05:36.052 
Epoch 957/1000 
	 loss: 40.9680, MinusLogProbMetric: 40.9680, val_loss: 42.2004, val_MinusLogProbMetric: 42.2004

Epoch 957: val_loss did not improve from 42.05871
196/196 - 33s - loss: 40.9680 - MinusLogProbMetric: 40.9680 - val_loss: 42.2004 - val_MinusLogProbMetric: 42.2004 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 958/1000
2023-10-27 22:06:08.980 
Epoch 958/1000 
	 loss: 40.9291, MinusLogProbMetric: 40.9291, val_loss: 42.0518, val_MinusLogProbMetric: 42.0518

Epoch 958: val_loss improved from 42.05871 to 42.05185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_418/weights/best_weights.h5
196/196 - 34s - loss: 40.9291 - MinusLogProbMetric: 40.9291 - val_loss: 42.0518 - val_MinusLogProbMetric: 42.0518 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 959/1000
2023-10-27 22:06:42.013 
Epoch 959/1000 
	 loss: 40.9291, MinusLogProbMetric: 40.9291, val_loss: 42.1584, val_MinusLogProbMetric: 42.1584

Epoch 959: val_loss did not improve from 42.05185
196/196 - 32s - loss: 40.9291 - MinusLogProbMetric: 40.9291 - val_loss: 42.1584 - val_MinusLogProbMetric: 42.1584 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 960/1000
2023-10-27 22:07:14.713 
Epoch 960/1000 
	 loss: 40.9277, MinusLogProbMetric: 40.9277, val_loss: 42.2254, val_MinusLogProbMetric: 42.2254

Epoch 960: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9277 - MinusLogProbMetric: 40.9277 - val_loss: 42.2254 - val_MinusLogProbMetric: 42.2254 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 961/1000
2023-10-27 22:07:47.402 
Epoch 961/1000 
	 loss: 40.9410, MinusLogProbMetric: 40.9410, val_loss: 42.2388, val_MinusLogProbMetric: 42.2388

Epoch 961: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9410 - MinusLogProbMetric: 40.9410 - val_loss: 42.2388 - val_MinusLogProbMetric: 42.2388 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 962/1000
2023-10-27 22:08:20.406 
Epoch 962/1000 
	 loss: 40.9256, MinusLogProbMetric: 40.9256, val_loss: 42.1910, val_MinusLogProbMetric: 42.1910

Epoch 962: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9256 - MinusLogProbMetric: 40.9256 - val_loss: 42.1910 - val_MinusLogProbMetric: 42.1910 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 963/1000
2023-10-27 22:08:53.447 
Epoch 963/1000 
	 loss: 40.9460, MinusLogProbMetric: 40.9460, val_loss: 42.2822, val_MinusLogProbMetric: 42.2822

Epoch 963: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9460 - MinusLogProbMetric: 40.9460 - val_loss: 42.2822 - val_MinusLogProbMetric: 42.2822 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 964/1000
2023-10-27 22:09:26.617 
Epoch 964/1000 
	 loss: 40.9253, MinusLogProbMetric: 40.9253, val_loss: 42.1958, val_MinusLogProbMetric: 42.1958

Epoch 964: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9253 - MinusLogProbMetric: 40.9253 - val_loss: 42.1958 - val_MinusLogProbMetric: 42.1958 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 965/1000
2023-10-27 22:09:59.474 
Epoch 965/1000 
	 loss: 40.9275, MinusLogProbMetric: 40.9275, val_loss: 42.1290, val_MinusLogProbMetric: 42.1290

Epoch 965: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9275 - MinusLogProbMetric: 40.9275 - val_loss: 42.1290 - val_MinusLogProbMetric: 42.1290 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 966/1000
2023-10-27 22:10:32.185 
Epoch 966/1000 
	 loss: 40.9219, MinusLogProbMetric: 40.9219, val_loss: 42.1420, val_MinusLogProbMetric: 42.1420

Epoch 966: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9219 - MinusLogProbMetric: 40.9219 - val_loss: 42.1420 - val_MinusLogProbMetric: 42.1420 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 967/1000
2023-10-27 22:11:05.083 
Epoch 967/1000 
	 loss: 40.9255, MinusLogProbMetric: 40.9255, val_loss: 42.3522, val_MinusLogProbMetric: 42.3522

Epoch 967: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9255 - MinusLogProbMetric: 40.9255 - val_loss: 42.3522 - val_MinusLogProbMetric: 42.3522 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 968/1000
2023-10-27 22:11:37.330 
Epoch 968/1000 
	 loss: 40.9181, MinusLogProbMetric: 40.9181, val_loss: 42.1674, val_MinusLogProbMetric: 42.1674

Epoch 968: val_loss did not improve from 42.05185
196/196 - 32s - loss: 40.9181 - MinusLogProbMetric: 40.9181 - val_loss: 42.1674 - val_MinusLogProbMetric: 42.1674 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 969/1000
2023-10-27 22:12:10.524 
Epoch 969/1000 
	 loss: 40.9530, MinusLogProbMetric: 40.9530, val_loss: 42.0756, val_MinusLogProbMetric: 42.0756

Epoch 969: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9530 - MinusLogProbMetric: 40.9530 - val_loss: 42.0756 - val_MinusLogProbMetric: 42.0756 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 970/1000
2023-10-27 22:12:43.794 
Epoch 970/1000 
	 loss: 40.9315, MinusLogProbMetric: 40.9315, val_loss: 42.0678, val_MinusLogProbMetric: 42.0678

Epoch 970: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9315 - MinusLogProbMetric: 40.9315 - val_loss: 42.0678 - val_MinusLogProbMetric: 42.0678 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 971/1000
2023-10-27 22:13:16.523 
Epoch 971/1000 
	 loss: 40.9451, MinusLogProbMetric: 40.9451, val_loss: 42.2169, val_MinusLogProbMetric: 42.2169

Epoch 971: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9451 - MinusLogProbMetric: 40.9451 - val_loss: 42.2169 - val_MinusLogProbMetric: 42.2169 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 972/1000
2023-10-27 22:13:49.122 
Epoch 972/1000 
	 loss: 40.9248, MinusLogProbMetric: 40.9248, val_loss: 42.0985, val_MinusLogProbMetric: 42.0985

Epoch 972: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9248 - MinusLogProbMetric: 40.9248 - val_loss: 42.0985 - val_MinusLogProbMetric: 42.0985 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 973/1000
2023-10-27 22:14:22.034 
Epoch 973/1000 
	 loss: 40.9431, MinusLogProbMetric: 40.9431, val_loss: 42.1982, val_MinusLogProbMetric: 42.1982

Epoch 973: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9431 - MinusLogProbMetric: 40.9431 - val_loss: 42.1982 - val_MinusLogProbMetric: 42.1982 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 974/1000
2023-10-27 22:14:54.877 
Epoch 974/1000 
	 loss: 40.9502, MinusLogProbMetric: 40.9502, val_loss: 42.1727, val_MinusLogProbMetric: 42.1727

Epoch 974: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9502 - MinusLogProbMetric: 40.9502 - val_loss: 42.1727 - val_MinusLogProbMetric: 42.1727 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 975/1000
2023-10-27 22:15:27.973 
Epoch 975/1000 
	 loss: 40.9515, MinusLogProbMetric: 40.9515, val_loss: 42.1079, val_MinusLogProbMetric: 42.1079

Epoch 975: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9515 - MinusLogProbMetric: 40.9515 - val_loss: 42.1079 - val_MinusLogProbMetric: 42.1079 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 976/1000
2023-10-27 22:16:01.290 
Epoch 976/1000 
	 loss: 40.9364, MinusLogProbMetric: 40.9364, val_loss: 42.1405, val_MinusLogProbMetric: 42.1405

Epoch 976: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9364 - MinusLogProbMetric: 40.9364 - val_loss: 42.1405 - val_MinusLogProbMetric: 42.1405 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 977/1000
2023-10-27 22:16:34.423 
Epoch 977/1000 
	 loss: 40.9382, MinusLogProbMetric: 40.9382, val_loss: 42.1297, val_MinusLogProbMetric: 42.1297

Epoch 977: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9382 - MinusLogProbMetric: 40.9382 - val_loss: 42.1297 - val_MinusLogProbMetric: 42.1297 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 978/1000
2023-10-27 22:17:07.293 
Epoch 978/1000 
	 loss: 40.9345, MinusLogProbMetric: 40.9345, val_loss: 42.2263, val_MinusLogProbMetric: 42.2263

Epoch 978: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9345 - MinusLogProbMetric: 40.9345 - val_loss: 42.2263 - val_MinusLogProbMetric: 42.2263 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 979/1000
2023-10-27 22:17:39.960 
Epoch 979/1000 
	 loss: 40.9198, MinusLogProbMetric: 40.9198, val_loss: 42.0667, val_MinusLogProbMetric: 42.0667

Epoch 979: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9198 - MinusLogProbMetric: 40.9198 - val_loss: 42.0667 - val_MinusLogProbMetric: 42.0667 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 980/1000
2023-10-27 22:18:12.732 
Epoch 980/1000 
	 loss: 40.9103, MinusLogProbMetric: 40.9103, val_loss: 42.1437, val_MinusLogProbMetric: 42.1437

Epoch 980: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9103 - MinusLogProbMetric: 40.9103 - val_loss: 42.1437 - val_MinusLogProbMetric: 42.1437 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 981/1000
2023-10-27 22:18:45.523 
Epoch 981/1000 
	 loss: 40.9102, MinusLogProbMetric: 40.9102, val_loss: 42.1340, val_MinusLogProbMetric: 42.1340

Epoch 981: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9102 - MinusLogProbMetric: 40.9102 - val_loss: 42.1340 - val_MinusLogProbMetric: 42.1340 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 982/1000
2023-10-27 22:19:18.668 
Epoch 982/1000 
	 loss: 40.9089, MinusLogProbMetric: 40.9089, val_loss: 42.1532, val_MinusLogProbMetric: 42.1532

Epoch 982: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9089 - MinusLogProbMetric: 40.9089 - val_loss: 42.1532 - val_MinusLogProbMetric: 42.1532 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 983/1000
2023-10-27 22:19:51.895 
Epoch 983/1000 
	 loss: 40.9234, MinusLogProbMetric: 40.9234, val_loss: 42.0908, val_MinusLogProbMetric: 42.0908

Epoch 983: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9234 - MinusLogProbMetric: 40.9234 - val_loss: 42.0908 - val_MinusLogProbMetric: 42.0908 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 984/1000
2023-10-27 22:20:25.132 
Epoch 984/1000 
	 loss: 40.9047, MinusLogProbMetric: 40.9047, val_loss: 42.1116, val_MinusLogProbMetric: 42.1116

Epoch 984: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9047 - MinusLogProbMetric: 40.9047 - val_loss: 42.1116 - val_MinusLogProbMetric: 42.1116 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 985/1000
2023-10-27 22:20:58.066 
Epoch 985/1000 
	 loss: 40.9142, MinusLogProbMetric: 40.9142, val_loss: 42.1809, val_MinusLogProbMetric: 42.1809

Epoch 985: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9142 - MinusLogProbMetric: 40.9142 - val_loss: 42.1809 - val_MinusLogProbMetric: 42.1809 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 986/1000
2023-10-27 22:21:30.789 
Epoch 986/1000 
	 loss: 40.9305, MinusLogProbMetric: 40.9305, val_loss: 42.2058, val_MinusLogProbMetric: 42.2058

Epoch 986: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9305 - MinusLogProbMetric: 40.9305 - val_loss: 42.2058 - val_MinusLogProbMetric: 42.2058 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 987/1000
2023-10-27 22:22:03.611 
Epoch 987/1000 
	 loss: 40.9234, MinusLogProbMetric: 40.9234, val_loss: 42.4887, val_MinusLogProbMetric: 42.4887

Epoch 987: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9234 - MinusLogProbMetric: 40.9234 - val_loss: 42.4887 - val_MinusLogProbMetric: 42.4887 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 988/1000
2023-10-27 22:22:36.279 
Epoch 988/1000 
	 loss: 40.9263, MinusLogProbMetric: 40.9263, val_loss: 42.2466, val_MinusLogProbMetric: 42.2466

Epoch 988: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9263 - MinusLogProbMetric: 40.9263 - val_loss: 42.2466 - val_MinusLogProbMetric: 42.2466 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 989/1000
2023-10-27 22:23:08.851 
Epoch 989/1000 
	 loss: 40.9197, MinusLogProbMetric: 40.9197, val_loss: 42.4904, val_MinusLogProbMetric: 42.4904

Epoch 989: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9197 - MinusLogProbMetric: 40.9197 - val_loss: 42.4904 - val_MinusLogProbMetric: 42.4904 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 990/1000
2023-10-27 22:23:41.700 
Epoch 990/1000 
	 loss: 40.9094, MinusLogProbMetric: 40.9094, val_loss: 42.1894, val_MinusLogProbMetric: 42.1894

Epoch 990: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9094 - MinusLogProbMetric: 40.9094 - val_loss: 42.1894 - val_MinusLogProbMetric: 42.1894 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 991/1000
2023-10-27 22:24:14.509 
Epoch 991/1000 
	 loss: 40.9272, MinusLogProbMetric: 40.9272, val_loss: 42.0959, val_MinusLogProbMetric: 42.0959

Epoch 991: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9272 - MinusLogProbMetric: 40.9272 - val_loss: 42.0959 - val_MinusLogProbMetric: 42.0959 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 992/1000
2023-10-27 22:24:47.256 
Epoch 992/1000 
	 loss: 40.9262, MinusLogProbMetric: 40.9262, val_loss: 42.1674, val_MinusLogProbMetric: 42.1674

Epoch 992: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9262 - MinusLogProbMetric: 40.9262 - val_loss: 42.1674 - val_MinusLogProbMetric: 42.1674 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 993/1000
2023-10-27 22:25:20.023 
Epoch 993/1000 
	 loss: 40.9414, MinusLogProbMetric: 40.9414, val_loss: 42.1926, val_MinusLogProbMetric: 42.1926

Epoch 993: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9414 - MinusLogProbMetric: 40.9414 - val_loss: 42.1926 - val_MinusLogProbMetric: 42.1926 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 994/1000
2023-10-27 22:25:52.759 
Epoch 994/1000 
	 loss: 40.9408, MinusLogProbMetric: 40.9408, val_loss: 42.1682, val_MinusLogProbMetric: 42.1682

Epoch 994: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9408 - MinusLogProbMetric: 40.9408 - val_loss: 42.1682 - val_MinusLogProbMetric: 42.1682 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 995/1000
2023-10-27 22:26:25.735 
Epoch 995/1000 
	 loss: 40.9274, MinusLogProbMetric: 40.9274, val_loss: 42.2409, val_MinusLogProbMetric: 42.2409

Epoch 995: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9274 - MinusLogProbMetric: 40.9274 - val_loss: 42.2409 - val_MinusLogProbMetric: 42.2409 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 996/1000
2023-10-27 22:26:58.783 
Epoch 996/1000 
	 loss: 40.9413, MinusLogProbMetric: 40.9413, val_loss: 42.1265, val_MinusLogProbMetric: 42.1265

Epoch 996: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9413 - MinusLogProbMetric: 40.9413 - val_loss: 42.1265 - val_MinusLogProbMetric: 42.1265 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 997/1000
2023-10-27 22:27:31.567 
Epoch 997/1000 
	 loss: 40.9059, MinusLogProbMetric: 40.9059, val_loss: 42.1305, val_MinusLogProbMetric: 42.1305

Epoch 997: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9059 - MinusLogProbMetric: 40.9059 - val_loss: 42.1305 - val_MinusLogProbMetric: 42.1305 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 998/1000
2023-10-27 22:28:04.460 
Epoch 998/1000 
	 loss: 40.9295, MinusLogProbMetric: 40.9295, val_loss: 42.2384, val_MinusLogProbMetric: 42.2384

Epoch 998: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9295 - MinusLogProbMetric: 40.9295 - val_loss: 42.2384 - val_MinusLogProbMetric: 42.2384 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 999/1000
2023-10-27 22:28:37.394 
Epoch 999/1000 
	 loss: 40.9371, MinusLogProbMetric: 40.9371, val_loss: 42.1889, val_MinusLogProbMetric: 42.1889

Epoch 999: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9371 - MinusLogProbMetric: 40.9371 - val_loss: 42.1889 - val_MinusLogProbMetric: 42.1889 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 1000/1000
2023-10-27 22:29:10.458 
Epoch 1000/1000 
	 loss: 40.9088, MinusLogProbMetric: 40.9088, val_loss: 42.1510, val_MinusLogProbMetric: 42.1510

Epoch 1000: val_loss did not improve from 42.05185
196/196 - 33s - loss: 40.9088 - MinusLogProbMetric: 40.9088 - val_loss: 42.1510 - val_MinusLogProbMetric: 42.1510 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 377.
Model trained in 32131.05 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.14 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.40 s.
===========
Run 418/720 done in 32200.74 s.
===========

Directory ../../results/CsplineN_new/run_419/ already exists.
Skipping it.
===========
Run 419/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_420/ already exists.
Skipping it.
===========
Run 420/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_421/ already exists.
Skipping it.
===========
Run 421/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_422/ already exists.
Skipping it.
===========
Run 422/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_423/ already exists.
Skipping it.
===========
Run 423/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_424/ already exists.
Skipping it.
===========
Run 424/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_425/ already exists.
Skipping it.
===========
Run 425/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_426/ already exists.
Skipping it.
===========
Run 426/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_427/ already exists.
Skipping it.
===========
Run 427/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_428/ already exists.
Skipping it.
===========
Run 428/720 already exists. Skipping it.
===========

===========
Generating train data for run 429.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_448"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_449 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7feccfb07fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec1d15c040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec1d15c040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fecd061cd60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feeaa6b9ff0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feeaa6b8130>, <keras.callbacks.ModelCheckpoint object at 0x7feeaa6b9c60>, <keras.callbacks.EarlyStopping object at 0x7feeaa6bb9a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feeaa6b83a0>, <keras.callbacks.TerminateOnNaN object at 0x7feeaa6bb520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:29:19.350845
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:31:25.997 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 127s/epoch - 645ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 429.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_459"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_460 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7fea40d33a90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feaf5d4fe50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feaf5d4fe50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febd8279090>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feaf5d8b7f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feaf5d8bd60>, <keras.callbacks.ModelCheckpoint object at 0x7feaf5d8be20>, <keras.callbacks.EarlyStopping object at 0x7feaf5d8bfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feaf5d8bd30>, <keras.callbacks.TerminateOnNaN object at 0x7feaf5d8bf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:31:34.752606
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:33:59.192 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 144s/epoch - 736ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 429.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_470"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_471 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7fec259a9090>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec85b4f460>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec85b4f460>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febd827bb20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecac4f5b40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecac4f7af0>, <keras.callbacks.ModelCheckpoint object at 0x7fecac4f47c0>, <keras.callbacks.EarlyStopping object at 0x7fecac4f4310>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecac4f63b0>, <keras.callbacks.TerminateOnNaN object at 0x7fecac4f4880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:34:09.390733
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:36:09.836 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 120s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 120s/epoch - 614ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 429.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_481"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_482 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7fea338e07f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea308b2b60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea308b2b60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea30fc4ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea20409c90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea2040a200>, <keras.callbacks.ModelCheckpoint object at 0x7fea2040a2c0>, <keras.callbacks.EarlyStopping object at 0x7fea2040a530>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea2040a560>, <keras.callbacks.TerminateOnNaN object at 0x7fea2040a1a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:36:19.031903
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:38:38.328 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 139s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 139s/epoch - 709ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 429.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_492"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_493 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7fea304ede70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee802e8f10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee802e8f10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4c3036d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec4c9490f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec4c949660>, <keras.callbacks.ModelCheckpoint object at 0x7fec4c949720>, <keras.callbacks.EarlyStopping object at 0x7fec4c949990>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec4c9499c0>, <keras.callbacks.TerminateOnNaN object at 0x7fec4c949600>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:38:46.090922
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:40:53.297 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 127s/epoch - 648ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 429.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_503"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_504 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7fec2624d870>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea107a9ab0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea107a9ab0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea1294e770>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fec874f0be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fec874f24a0>, <keras.callbacks.ModelCheckpoint object at 0x7fec874f05e0>, <keras.callbacks.EarlyStopping object at 0x7fec874f0730>, <keras.callbacks.ReduceLROnPlateau object at 0x7fec874f25f0>, <keras.callbacks.TerminateOnNaN object at 0x7fec874f0820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:40:59.692802
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:43:03.215 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 123s/epoch - 630ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 429.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_514"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_515 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7fec1d818070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feba6052380>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feba6052380>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea325dec20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea3b052350>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea3b0528c0>, <keras.callbacks.ModelCheckpoint object at 0x7fea3b052980>, <keras.callbacks.EarlyStopping object at 0x7fea3b052bf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea3b052c20>, <keras.callbacks.TerminateOnNaN object at 0x7fea3b052860>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:43:11.910294
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:45:29.456 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 137s/epoch - 701ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 429.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_525"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_526 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7fea403c8a00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea807b22c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea807b22c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb185ba1d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea940d3a60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea940d0250>, <keras.callbacks.ModelCheckpoint object at 0x7fea940d2d40>, <keras.callbacks.EarlyStopping object at 0x7fea940d0910>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea940d2380>, <keras.callbacks.TerminateOnNaN object at 0x7fea940d14b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:45:38.653533
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:47:39.092 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 120s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 120s/epoch - 614ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 429.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_536"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_537 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7feb507419c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feaec25b310>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feaec25b310>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec25c7ed70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feae04a1930>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feae04a1ea0>, <keras.callbacks.ModelCheckpoint object at 0x7feae04a1f60>, <keras.callbacks.EarlyStopping object at 0x7feae04a21d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feae04a2200>, <keras.callbacks.TerminateOnNaN object at 0x7feae04a1e40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:47:48.235328
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:50:03.599 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 135s/epoch - 690ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 429.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_547"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_548 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7feb017e39d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec878c7610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec878c7610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb19fe8b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb5e78a350>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb5e788910>, <keras.callbacks.ModelCheckpoint object at 0x7feb5e788730>, <keras.callbacks.EarlyStopping object at 0x7feb5e78ad70>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb5e78abc0>, <keras.callbacks.TerminateOnNaN object at 0x7feb5e78a650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:50:11.115556
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:52:08.275 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 117s/epoch - 597ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 429.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_429/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_429
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_558"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_559 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7feaf4833e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea3af0e3e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea3af0e3e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feb18ad98a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb19ba1db0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_429/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb19ba2320>, <keras.callbacks.ModelCheckpoint object at 0x7feb19ba23e0>, <keras.callbacks.EarlyStopping object at 0x7feb19ba2650>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb19ba2680>, <keras.callbacks.TerminateOnNaN object at 0x7feb19ba22c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_429/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 429/720 with hyperparameters:
timestamp = 2023-10-27 22:52:17.498717
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:54:32.670 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12453.4385, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 12453.4385 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 135s/epoch - 689ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 429/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 430.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_569"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_570 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7fec25c57df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee047b3df0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee047b3df0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea40ec3160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb20d06110>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb20d06680>, <keras.callbacks.ModelCheckpoint object at 0x7feb20d06740>, <keras.callbacks.EarlyStopping object at 0x7feb20d069b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb20d069e0>, <keras.callbacks.TerminateOnNaN object at 0x7feb20d06620>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 22:54:40.889794
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:56:45.843 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11565.8438, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 11565.8438 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 125s/epoch - 636ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 430.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_580"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_581 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7fea1354ac50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fec4f747820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fec4f747820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec4ec19540>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fecae185b70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fecae1860e0>, <keras.callbacks.ModelCheckpoint object at 0x7fecae1861a0>, <keras.callbacks.EarlyStopping object at 0x7fecae186410>, <keras.callbacks.ReduceLROnPlateau object at 0x7fecae186440>, <keras.callbacks.TerminateOnNaN object at 0x7fecae186080>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 22:56:53.418745
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:59:00.483 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11552.3057, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 11552.3057 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 127s/epoch - 648ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 430.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_591"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_592 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7feb9c2373a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff36e6e3580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff36e6e3580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febdb5f5600>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea8046df00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea8046e470>, <keras.callbacks.ModelCheckpoint object at 0x7fea8046e530>, <keras.callbacks.EarlyStopping object at 0x7fea8046e7a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea8046e7d0>, <keras.callbacks.TerminateOnNaN object at 0x7fea8046e410>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 22:59:08.957973
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:01:18.555 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11532.7197, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 11532.7197 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 130s/epoch - 661ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 430.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_602"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_603 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7feb9c2c43d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feccf4f9750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feccf4f9750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea2bc67280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb088fb460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb088fbaf0>, <keras.callbacks.ModelCheckpoint object at 0x7feb088f9840>, <keras.callbacks.EarlyStopping object at 0x7feb088f89d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb088fada0>, <keras.callbacks.TerminateOnNaN object at 0x7feb088fb790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 23:01:27.755701
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:03:30.786 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11602.7793, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 11602.7793 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 123s/epoch - 627ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 430.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_613"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_614 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7feb01b37c40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb01c796f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb01c796f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe8b2ecf250>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb01b6ec50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb01b6f1c0>, <keras.callbacks.ModelCheckpoint object at 0x7feb01b6f280>, <keras.callbacks.EarlyStopping object at 0x7feb01b6f4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb01b6f520>, <keras.callbacks.TerminateOnNaN object at 0x7feb01b6f160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 23:03:39.797935
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:05:57.672 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11658.8506, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 138s - loss: nan - MinusLogProbMetric: 11658.8506 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 138s/epoch - 703ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 430.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_624"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_625 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7feba5c5c640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecccb11d50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecccb11d50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feba6209c00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea11addf00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea11add990>, <keras.callbacks.ModelCheckpoint object at 0x7fea11add8a0>, <keras.callbacks.EarlyStopping object at 0x7fea11add660>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea11add5a0>, <keras.callbacks.TerminateOnNaN object at 0x7fea11add9f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 23:06:05.330377
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:08:03.555 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11673.3994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 118s - loss: nan - MinusLogProbMetric: 11673.3994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 118s/epoch - 603ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 430.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_635"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_636 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7fec74ece680>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feb196e9ba0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feb196e9ba0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea38689750>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feba4f01f00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feba4f02470>, <keras.callbacks.ModelCheckpoint object at 0x7feba4f02530>, <keras.callbacks.EarlyStopping object at 0x7feba4f027a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feba4f027d0>, <keras.callbacks.TerminateOnNaN object at 0x7feba4f02410>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 23:08:12.671552
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:10:22.620 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11689.2168, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 11689.2168 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 130s/epoch - 663ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 430.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_430/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_430
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_646"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_647 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_66 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_66/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_66'")
self.model: <keras.engine.functional.Functional object at 0x7feb9fffa920>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fecad6aeda0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fecad6aeda0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fec76d87790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff355c2de10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_430/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff355c2eb90>, <keras.callbacks.ModelCheckpoint object at 0x7ff355c2e7a0>, <keras.callbacks.EarlyStopping object at 0x7ff355c2fca0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff355c2e0b0>, <keras.callbacks.TerminateOnNaN object at 0x7ff355c2f9d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_430/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 430/720 with hyperparameters:
timestamp = 2023-10-27 23:10:31.944067
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
LLVM ERROR: Unable to allocate section memory!
