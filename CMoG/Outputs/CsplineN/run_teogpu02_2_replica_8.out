2023-10-30 14:03:40.405008: Importing os...
2023-10-30 14:03:40.405087: Importing sys...
2023-10-30 14:03:40.405105: Importing and initializing argparse...
Visible devices: [2]
2023-10-30 14:03:40.422740: Importing timer from timeit...
2023-10-30 14:03:40.423337: Setting env variables for tf import (only device [2] will be available)...
2023-10-30 14:03:40.423385: Importing numpy...
2023-10-30 14:03:40.606996: Importing pandas...
2023-10-30 14:03:40.796889: Importing shutil...
2023-10-30 14:03:40.796918: Importing subprocess...
2023-10-30 14:03:40.796925: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-30 14:03:42.964091: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-30 14:03:43.317750: Importing textwrap...
2023-10-30 14:03:43.317777: Importing timeit...
2023-10-30 14:03:43.317786: Importing traceback...
2023-10-30 14:03:43.317792: Importing typing...
2023-10-30 14:03:43.317802: Setting tf configs...
2023-10-30 14:03:43.427312: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-30 14:03:44.431447: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

===========
Generating train data for run 341.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1321920   
 r)                                                              
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f4ace897190>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4a9c1dbf40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4a9c1dbf40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4a946a9ff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4a94503b80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4a9444c130>, <keras.callbacks.ModelCheckpoint object at 0x7f4a9444c280>, <keras.callbacks.EarlyStopping object at 0x7f4a9444c490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4a9444c4c0>, <keras.callbacks.TerminateOnNaN object at 0x7f4a9444c1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:03:50.179349
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:05:45.163 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 115s/epoch - 586ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 341.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f4e3e332b30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e666aeb90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e666aeb90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e3df7e290>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e3dde6530>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e3dde6aa0>, <keras.callbacks.ModelCheckpoint object at 0x7f4e3dde6b60>, <keras.callbacks.EarlyStopping object at 0x7f4e3dde6dd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e3dde6e00>, <keras.callbacks.TerminateOnNaN object at 0x7f4e3dde6a40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:05:52.189005
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:07:59.212 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 127s/epoch - 648ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 341.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f4e68578730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e681db850>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e681db850>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f494027c310>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e6810cca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e6810d210>, <keras.callbacks.ModelCheckpoint object at 0x7f4e6810d2d0>, <keras.callbacks.EarlyStopping object at 0x7f4e6810d540>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e6810d570>, <keras.callbacks.TerminateOnNaN object at 0x7f4e6810d1b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:08:05.836744
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:10:11.692 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 126s/epoch - 642ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 341.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f4e67ef7760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47b14a1630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47b14a1630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e67a05c90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e6786dd50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e6786e2c0>, <keras.callbacks.ModelCheckpoint object at 0x7f4e6786e380>, <keras.callbacks.EarlyStopping object at 0x7f4e6786e5f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e6786e620>, <keras.callbacks.TerminateOnNaN object at 0x7f4e6786e260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:10:18.657011
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:12:18.083 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 119s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 119s/epoch - 609ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 341.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f4e676dbd30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47b17e8be0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47b17e8be0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e6736f430>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e35619db0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e3561a320>, <keras.callbacks.ModelCheckpoint object at 0x7f4e3561a3e0>, <keras.callbacks.EarlyStopping object at 0x7f4e3561a650>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e3561a680>, <keras.callbacks.TerminateOnNaN object at 0x7f4e3561a2c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:12:24.687822
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f4858382dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:14:25.602 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 121s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 121s/epoch - 616ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 341.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f4820433c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e666a4d30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e666a4d30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f482045f0d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47b16dac80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f47b16db1f0>, <keras.callbacks.ModelCheckpoint object at 0x7f47b16db2b0>, <keras.callbacks.EarlyStopping object at 0x7f47b16db520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47b16db550>, <keras.callbacks.TerminateOnNaN object at 0x7f47b16db190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:14:34.023189
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7f4880610ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:16:40.312 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 126s/epoch - 643ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 341.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f48e0666f50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47b02222c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47b02222c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f48c064b3a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f48c0348850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f48c0349000>, <keras.callbacks.ModelCheckpoint object at 0x7f48c03497e0>, <keras.callbacks.EarlyStopping object at 0x7f48c03499f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f48c0349cf0>, <keras.callbacks.TerminateOnNaN object at 0x7f48c0349390>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:16:47.988661
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:18:58.790 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 130s/epoch - 666ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 341.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f4784d5a200>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f478c71da80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f478c71da80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f478d207d00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47841664d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4784166a40>, <keras.callbacks.ModelCheckpoint object at 0x7f4784166b00>, <keras.callbacks.EarlyStopping object at 0x7f4784166d70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4784166da0>, <keras.callbacks.TerminateOnNaN object at 0x7f47841669e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:19:06.138512
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:21:23.488 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 137s/epoch - 700ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 341.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f48e045f6a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f478c292620>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f478c292620>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47f87921d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47b0de2290>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f47b0de2800>, <keras.callbacks.ModelCheckpoint object at 0x7f47b0de28c0>, <keras.callbacks.EarlyStopping object at 0x7f47b0de2b30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47b0de2b60>, <keras.callbacks.TerminateOnNaN object at 0x7f47b0de27a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:21:30.077388
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:23:24.891 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 115s/epoch - 585ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 341.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f4e352ee740>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e34dfd1b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e34dfd1b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e3515da50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e34e2c670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e34e2cbe0>, <keras.callbacks.ModelCheckpoint object at 0x7f4e34e2cca0>, <keras.callbacks.EarlyStopping object at 0x7f4e34e2cf10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e34e2cf40>, <keras.callbacks.TerminateOnNaN object at 0x7f4e34e2cb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:23:33.238784
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:25:46.612 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 133s/epoch - 680ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 341.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f499879b7f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e67533520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e67533520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47845a6ce0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4784569420>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4784569990>, <keras.callbacks.ModelCheckpoint object at 0x7f4784569a50>, <keras.callbacks.EarlyStopping object at 0x7f4784569cc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4784569cf0>, <keras.callbacks.TerminateOnNaN object at 0x7f4784569930>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-30 14:25:54.149014
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:28:10.937 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 137s/epoch - 697ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 341/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

===========
Generating train data for run 357.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_131"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_132 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f47591357e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f475a1acca0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f475a1acca0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e33d51f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f475981a7d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f475981ad40>, <keras.callbacks.ModelCheckpoint object at 0x7f475981ae00>, <keras.callbacks.EarlyStopping object at 0x7f475981b070>, <keras.callbacks.ReduceLROnPlateau object at 0x7f475981b0a0>, <keras.callbacks.TerminateOnNaN object at 0x7f475981ace0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:28:17.424786
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:30:32.437 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 135s/epoch - 688ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 357.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_142"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_143 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f4e349fb4c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46e0125ab0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46e0125ab0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e340ef730>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e341811e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e34181750>, <keras.callbacks.ModelCheckpoint object at 0x7f4e34181810>, <keras.callbacks.EarlyStopping object at 0x7f4e34181a80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e34181ab0>, <keras.callbacks.TerminateOnNaN object at 0x7f4e341816f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:30:40.237877
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:32:53.422 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 133s/epoch - 678ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 357.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_153"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_154 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f46fe3af9d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4753c91090>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4753c91090>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4731e97280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4731f689a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4731f68f10>, <keras.callbacks.ModelCheckpoint object at 0x7f4731f68fd0>, <keras.callbacks.EarlyStopping object at 0x7f4731f69240>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4731f69270>, <keras.callbacks.TerminateOnNaN object at 0x7f4731f68eb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:33:01.056465
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:35:08.669 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 127s/epoch - 650ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 357.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_164"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_165 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f46e0580820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46e0a1fd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46e0a1fd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47582c35b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46c7051d20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46c7052290>, <keras.callbacks.ModelCheckpoint object at 0x7f46c7052350>, <keras.callbacks.EarlyStopping object at 0x7f46c70525c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46c70525f0>, <keras.callbacks.TerminateOnNaN object at 0x7f46c7052230>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:35:16.056410
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:37:24.526 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 128s/epoch - 654ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 357.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_175"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_176 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f46e0ebec80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47503b04f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47503b04f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4750bcb4f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4750b9b8e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4750b9be50>, <keras.callbacks.ModelCheckpoint object at 0x7f4750b9bf10>, <keras.callbacks.EarlyStopping object at 0x7f4750b9be20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4750b9bfd0>, <keras.callbacks.TerminateOnNaN object at 0x7f4750c44040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:37:33.838217
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:39:43.374 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 129s/epoch - 660ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 357.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f47585ef010>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4759421510>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4759421510>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46e0127e50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46ff67cd90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46ff67d300>, <keras.callbacks.ModelCheckpoint object at 0x7f46ff67d3c0>, <keras.callbacks.EarlyStopping object at 0x7f46ff67d630>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46ff67d660>, <keras.callbacks.TerminateOnNaN object at 0x7f46ff67d2a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:39:50.062146
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:42:04.284 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 134s/epoch - 684ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 357.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f46c6583df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46fff7beb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46fff7beb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f469c8f01f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f469c8b8fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f469c8b9510>, <keras.callbacks.ModelCheckpoint object at 0x7f469c8b95d0>, <keras.callbacks.EarlyStopping object at 0x7f469c8b9840>, <keras.callbacks.ReduceLROnPlateau object at 0x7f469c8b9870>, <keras.callbacks.TerminateOnNaN object at 0x7f469c8b94b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:42:11.994586
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:44:23.079 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 131s/epoch - 668ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 357.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f4731f69a20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f473017ffd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f473017ffd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4e341a2080>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46816c3100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46816c3670>, <keras.callbacks.ModelCheckpoint object at 0x7f46816c3730>, <keras.callbacks.EarlyStopping object at 0x7f46816c39a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46816c39d0>, <keras.callbacks.TerminateOnNaN object at 0x7f46816c3610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:44:30.862972
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:46:35.654 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 125s/epoch - 636ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 357.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f4753c10df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4758cc8ac0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4758cc8ac0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47535b7550>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46c47acbe0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46c47ac580>, <keras.callbacks.ModelCheckpoint object at 0x7f46c47ac2b0>, <keras.callbacks.EarlyStopping object at 0x7f46c47af4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46c47aef50>, <keras.callbacks.TerminateOnNaN object at 0x7f46c47ae290>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:46:47.386122
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:48:50.674 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 123s/epoch - 628ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 357.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f4e2ad77ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e2ae53460>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e2ae53460>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46185b97e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e2adaf520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e2adafa90>, <keras.callbacks.ModelCheckpoint object at 0x7f4e2adafb50>, <keras.callbacks.EarlyStopping object at 0x7f4e2adafdc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e2adafdf0>, <keras.callbacks.TerminateOnNaN object at 0x7f4e2adafa30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:48:58.251079
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:51:05.732 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 127s/epoch - 649ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 357.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f46c62b73d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f475947f520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f475947f520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46e3f935b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46e36ca440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46e36ca890>, <keras.callbacks.ModelCheckpoint object at 0x7f46e36cab00>, <keras.callbacks.EarlyStopping object at 0x7f46e36cb700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46e36cb7c0>, <keras.callbacks.TerminateOnNaN object at 0x7f46e36caef0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-30 14:51:13.260844
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:53:26.907 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 134s/epoch - 681ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 357/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

===========
Generating train data for run 377.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_247"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_248 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f4785afb760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f473055de10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f473055de10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46e132e440>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f469ca17f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f469ca17b50>, <keras.callbacks.ModelCheckpoint object at 0x7f469ca17160>, <keras.callbacks.EarlyStopping object at 0x7f469ca15420>, <keras.callbacks.ReduceLROnPlateau object at 0x7f469ca15720>, <keras.callbacks.TerminateOnNaN object at 0x7f469ca144f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 14:53:30.958383
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:54:36.992 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 66s/epoch - 336ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 377.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_253"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_254 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f47845a8340>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f49203fe4d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f49203fe4d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4880462860>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47b02d7670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f47b02d6b30>, <keras.callbacks.ModelCheckpoint object at 0x7f47b02d68c0>, <keras.callbacks.EarlyStopping object at 0x7f47b02d4490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47b02d4940>, <keras.callbacks.TerminateOnNaN object at 0x7f47b02d43a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 14:54:40.751358
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:55:41.880 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 61s/epoch - 311ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 377.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_259"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_260 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f46c51e9300>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4681337a30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4681337a30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4682787b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f468290e320>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f468290e890>, <keras.callbacks.ModelCheckpoint object at 0x7f468290e950>, <keras.callbacks.EarlyStopping object at 0x7f468290ebc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f468290ebf0>, <keras.callbacks.TerminateOnNaN object at 0x7f468290e830>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 14:55:45.874786
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:56:51.765 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 66s/epoch - 335ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 377.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_265"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_266 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f47b0648310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f49fc6d7e80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f49fc6d7e80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4a4c1d8880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47b0bc4130>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f47b0bc4370>, <keras.callbacks.ModelCheckpoint object at 0x7f47b0bc5450>, <keras.callbacks.EarlyStopping object at 0x7f47b0bc6620>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47b0bc5f00>, <keras.callbacks.TerminateOnNaN object at 0x7f47b0bc46d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 14:56:55.899716
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:57:58.249 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 62s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 62s/epoch - 317ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 377.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_271"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_272 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f465c518400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46807de950>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46807de950>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46c4a4a980>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46c57fc340>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46c57fc8b0>, <keras.callbacks.ModelCheckpoint object at 0x7f46c57fc970>, <keras.callbacks.EarlyStopping object at 0x7f46c57fcbe0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46c57fcc10>, <keras.callbacks.TerminateOnNaN object at 0x7f46c57fc850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 14:58:02.205450
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:59:09.713 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 67s/epoch - 344ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 377.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_277"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_278 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f488053b4f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47d85d2ce0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47d85d2ce0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47b0be8c10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e67f6ec50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f48e0218400>, <keras.callbacks.ModelCheckpoint object at 0x7f48e021b160>, <keras.callbacks.EarlyStopping object at 0x7f48e0219f30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f48e02189a0>, <keras.callbacks.TerminateOnNaN object at 0x7f48e021a170>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 14:59:13.984368
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:00:11.385 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 57s/epoch - 292ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 377.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_283"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_284 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f4680a92aa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f469cc59660>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f469cc59660>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f475acacd60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46806482b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f468064a560>, <keras.callbacks.ModelCheckpoint object at 0x7f468064aaa0>, <keras.callbacks.EarlyStopping object at 0x7f468064a620>, <keras.callbacks.ReduceLROnPlateau object at 0x7f468064a4a0>, <keras.callbacks.TerminateOnNaN object at 0x7f468064a3e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 15:00:14.543984
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:01:22.684 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 68s/epoch - 347ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 377.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_289"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_290 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f4990702050>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f49d823b460>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f49d823b460>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47b1384610>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47b03122f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f47b0310670>, <keras.callbacks.ModelCheckpoint object at 0x7f47b03112a0>, <keras.callbacks.EarlyStopping object at 0x7f47b0310430>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47b03100d0>, <keras.callbacks.TerminateOnNaN object at 0x7f47b0311ed0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 15:01:26.839472
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:02:31.651 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 65s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 65s/epoch - 330ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 377.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_295"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_296 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f46e25170d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46835473a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46835473a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f475b1e2a10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46558bd540>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46558bdab0>, <keras.callbacks.ModelCheckpoint object at 0x7f46558bdb70>, <keras.callbacks.EarlyStopping object at 0x7f46558bdde0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46558bde10>, <keras.callbacks.TerminateOnNaN object at 0x7f46558bda50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 15:02:35.386438
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:03:36.506 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 61s/epoch - 311ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 377.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_301"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_302 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f4e121dc640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e01af3be0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e01af3be0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f461c4f1cf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e019e5390>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e019e5900>, <keras.callbacks.ModelCheckpoint object at 0x7f4e019e59c0>, <keras.callbacks.EarlyStopping object at 0x7f4e019e5c30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e019e5c60>, <keras.callbacks.TerminateOnNaN object at 0x7f4e019e58a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 15:03:40.737237
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:04:47.506 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 67s/epoch - 340ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 377.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f4e2ab3e5c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e2ab3ebf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e2ab3ebf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46807c9360>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f478c5d3d90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4785534340>, <keras.callbacks.ModelCheckpoint object at 0x7f4785534400>, <keras.callbacks.EarlyStopping object at 0x7f4785534670>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47855346a0>, <keras.callbacks.TerminateOnNaN object at 0x7f47855342e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-30 15:04:51.738507
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:05:48.291 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 56s/epoch - 288ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 377/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

===========
Generating train data for run 382.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_318"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_319 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f4625763d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f462568f880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f462568f880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4655113400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f462574aef0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f462574b460>, <keras.callbacks.ModelCheckpoint object at 0x7f462574b520>, <keras.callbacks.EarlyStopping object at 0x7f462574b790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f462574b7c0>, <keras.callbacks.TerminateOnNaN object at 0x7f462574b400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_382/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-30 15:05:53.850968
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:07:55.900 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6532.8247, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 122s - loss: nan - MinusLogProbMetric: 6532.8247 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 122s/epoch - 622ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 382.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_329"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_330 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f46c58b7fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46e02d3e80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46e02d3e80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47524fa680>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f482015c7c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f482015cd30>, <keras.callbacks.ModelCheckpoint object at 0x7f482015cdf0>, <keras.callbacks.EarlyStopping object at 0x7f482015d060>, <keras.callbacks.ReduceLROnPlateau object at 0x7f482015d090>, <keras.callbacks.TerminateOnNaN object at 0x7f482015ccd0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_382/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-30 15:08:03.165152
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:10:24.775 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6568.1631, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 6568.1631 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 141s/epoch - 722ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 382.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_340"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_341 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f4786036320>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47582851b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47582851b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47313fe920>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f488058c1f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f488058c760>, <keras.callbacks.ModelCheckpoint object at 0x7f488058c820>, <keras.callbacks.EarlyStopping object at 0x7f488058ca90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f488058cac0>, <keras.callbacks.TerminateOnNaN object at 0x7f488058c700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_382/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-30 15:10:32.705670
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-30 15:13:39.897 
Epoch 1/1000 
	 loss: 3382.7764, MinusLogProbMetric: 3382.7764, val_loss: 1099.7390, val_MinusLogProbMetric: 1099.7390

Epoch 1: val_loss improved from inf to 1099.73901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 187s - loss: 3382.7764 - MinusLogProbMetric: 3382.7764 - val_loss: 1099.7390 - val_MinusLogProbMetric: 1099.7390 - lr: 1.1111e-04 - 187s/epoch - 956ms/step
Epoch 2/1000
2023-10-30 15:14:34.444 
Epoch 2/1000 
	 loss: 910.6746, MinusLogProbMetric: 910.6746, val_loss: 881.2718, val_MinusLogProbMetric: 881.2718

Epoch 2: val_loss improved from 1099.73901 to 881.27179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 910.6746 - MinusLogProbMetric: 910.6746 - val_loss: 881.2718 - val_MinusLogProbMetric: 881.2718 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 3/1000
2023-10-30 15:15:29.103 
Epoch 3/1000 
	 loss: 819.0172, MinusLogProbMetric: 819.0172, val_loss: 758.4839, val_MinusLogProbMetric: 758.4839

Epoch 3: val_loss improved from 881.27179 to 758.48395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 819.0172 - MinusLogProbMetric: 819.0172 - val_loss: 758.4839 - val_MinusLogProbMetric: 758.4839 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 4/1000
2023-10-30 15:16:29.787 
Epoch 4/1000 
	 loss: 701.7128, MinusLogProbMetric: 701.7128, val_loss: 659.4790, val_MinusLogProbMetric: 659.4790

Epoch 4: val_loss improved from 758.48395 to 659.47900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 701.7128 - MinusLogProbMetric: 701.7128 - val_loss: 659.4790 - val_MinusLogProbMetric: 659.4790 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 5/1000
2023-10-30 15:17:24.967 
Epoch 5/1000 
	 loss: 895.4629, MinusLogProbMetric: 895.4629, val_loss: 624.9402, val_MinusLogProbMetric: 624.9402

Epoch 5: val_loss improved from 659.47900 to 624.94025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 895.4629 - MinusLogProbMetric: 895.4629 - val_loss: 624.9402 - val_MinusLogProbMetric: 624.9402 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 6/1000
2023-10-30 15:18:19.869 
Epoch 6/1000 
	 loss: 589.5397, MinusLogProbMetric: 589.5397, val_loss: 567.5828, val_MinusLogProbMetric: 567.5828

Epoch 6: val_loss improved from 624.94025 to 567.58276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 589.5397 - MinusLogProbMetric: 589.5397 - val_loss: 567.5828 - val_MinusLogProbMetric: 567.5828 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 7/1000
2023-10-30 15:19:19.486 
Epoch 7/1000 
	 loss: 523.4056, MinusLogProbMetric: 523.4056, val_loss: 492.0386, val_MinusLogProbMetric: 492.0386

Epoch 7: val_loss improved from 567.58276 to 492.03864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 523.4056 - MinusLogProbMetric: 523.4056 - val_loss: 492.0386 - val_MinusLogProbMetric: 492.0386 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 8/1000
2023-10-30 15:20:16.201 
Epoch 8/1000 
	 loss: 477.3380, MinusLogProbMetric: 477.3380, val_loss: 457.3850, val_MinusLogProbMetric: 457.3850

Epoch 8: val_loss improved from 492.03864 to 457.38498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 477.3380 - MinusLogProbMetric: 477.3380 - val_loss: 457.3850 - val_MinusLogProbMetric: 457.3850 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 9/1000
2023-10-30 15:21:12.107 
Epoch 9/1000 
	 loss: 460.4307, MinusLogProbMetric: 460.4307, val_loss: 437.6817, val_MinusLogProbMetric: 437.6817

Epoch 9: val_loss improved from 457.38498 to 437.68167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 460.4307 - MinusLogProbMetric: 460.4307 - val_loss: 437.6817 - val_MinusLogProbMetric: 437.6817 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 10/1000
2023-10-30 15:22:13.419 
Epoch 10/1000 
	 loss: 411.3450, MinusLogProbMetric: 411.3450, val_loss: 369.2110, val_MinusLogProbMetric: 369.2110

Epoch 10: val_loss improved from 437.68167 to 369.21100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 411.3450 - MinusLogProbMetric: 411.3450 - val_loss: 369.2110 - val_MinusLogProbMetric: 369.2110 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 11/1000
2023-10-30 15:23:08.369 
Epoch 11/1000 
	 loss: 356.4005, MinusLogProbMetric: 356.4005, val_loss: 334.0054, val_MinusLogProbMetric: 334.0054

Epoch 11: val_loss improved from 369.21100 to 334.00537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 356.4005 - MinusLogProbMetric: 356.4005 - val_loss: 334.0054 - val_MinusLogProbMetric: 334.0054 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 12/1000
2023-10-30 15:24:03.142 
Epoch 12/1000 
	 loss: 348.3936, MinusLogProbMetric: 348.3936, val_loss: 337.8861, val_MinusLogProbMetric: 337.8861

Epoch 12: val_loss did not improve from 334.00537
196/196 - 54s - loss: 348.3936 - MinusLogProbMetric: 348.3936 - val_loss: 337.8861 - val_MinusLogProbMetric: 337.8861 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 13/1000
2023-10-30 15:25:00.657 
Epoch 13/1000 
	 loss: 316.6255, MinusLogProbMetric: 316.6255, val_loss: 302.1169, val_MinusLogProbMetric: 302.1169

Epoch 13: val_loss improved from 334.00537 to 302.11685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 316.6255 - MinusLogProbMetric: 316.6255 - val_loss: 302.1169 - val_MinusLogProbMetric: 302.1169 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 14/1000
2023-10-30 15:25:56.953 
Epoch 14/1000 
	 loss: 297.9207, MinusLogProbMetric: 297.9207, val_loss: 285.6458, val_MinusLogProbMetric: 285.6458

Epoch 14: val_loss improved from 302.11685 to 285.64584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 297.9207 - MinusLogProbMetric: 297.9207 - val_loss: 285.6458 - val_MinusLogProbMetric: 285.6458 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 15/1000
2023-10-30 15:26:52.494 
Epoch 15/1000 
	 loss: 286.4697, MinusLogProbMetric: 286.4697, val_loss: 274.8484, val_MinusLogProbMetric: 274.8484

Epoch 15: val_loss improved from 285.64584 to 274.84842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 286.4697 - MinusLogProbMetric: 286.4697 - val_loss: 274.8484 - val_MinusLogProbMetric: 274.8484 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 16/1000
2023-10-30 15:27:51.506 
Epoch 16/1000 
	 loss: 273.1932, MinusLogProbMetric: 273.1932, val_loss: 264.4449, val_MinusLogProbMetric: 264.4449

Epoch 16: val_loss improved from 274.84842 to 264.44485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 273.1932 - MinusLogProbMetric: 273.1932 - val_loss: 264.4449 - val_MinusLogProbMetric: 264.4449 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 17/1000
2023-10-30 15:28:48.658 
Epoch 17/1000 
	 loss: 262.8680, MinusLogProbMetric: 262.8680, val_loss: 259.8044, val_MinusLogProbMetric: 259.8044

Epoch 17: val_loss improved from 264.44485 to 259.80441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 262.8680 - MinusLogProbMetric: 262.8680 - val_loss: 259.8044 - val_MinusLogProbMetric: 259.8044 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 18/1000
2023-10-30 15:29:44.610 
Epoch 18/1000 
	 loss: 301.9688, MinusLogProbMetric: 301.9688, val_loss: 295.1500, val_MinusLogProbMetric: 295.1500

Epoch 18: val_loss did not improve from 259.80441
196/196 - 55s - loss: 301.9688 - MinusLogProbMetric: 301.9688 - val_loss: 295.1500 - val_MinusLogProbMetric: 295.1500 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 19/1000
2023-10-30 15:30:42.958 
Epoch 19/1000 
	 loss: 274.3346, MinusLogProbMetric: 274.3346, val_loss: 257.9668, val_MinusLogProbMetric: 257.9668

Epoch 19: val_loss improved from 259.80441 to 257.96677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 274.3346 - MinusLogProbMetric: 274.3346 - val_loss: 257.9668 - val_MinusLogProbMetric: 257.9668 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 20/1000
2023-10-30 15:31:41.213 
Epoch 20/1000 
	 loss: 252.8752, MinusLogProbMetric: 252.8752, val_loss: 247.8318, val_MinusLogProbMetric: 247.8318

Epoch 20: val_loss improved from 257.96677 to 247.83182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 252.8752 - MinusLogProbMetric: 252.8752 - val_loss: 247.8318 - val_MinusLogProbMetric: 247.8318 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 21/1000
2023-10-30 15:32:36.770 
Epoch 21/1000 
	 loss: 246.9210, MinusLogProbMetric: 246.9210, val_loss: 239.0794, val_MinusLogProbMetric: 239.0794

Epoch 21: val_loss improved from 247.83182 to 239.07938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 246.9210 - MinusLogProbMetric: 246.9210 - val_loss: 239.0794 - val_MinusLogProbMetric: 239.0794 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 22/1000
2023-10-30 15:33:33.581 
Epoch 22/1000 
	 loss: 247.0297, MinusLogProbMetric: 247.0297, val_loss: 235.6079, val_MinusLogProbMetric: 235.6079

Epoch 22: val_loss improved from 239.07938 to 235.60794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 247.0297 - MinusLogProbMetric: 247.0297 - val_loss: 235.6079 - val_MinusLogProbMetric: 235.6079 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 23/1000
2023-10-30 15:34:31.312 
Epoch 23/1000 
	 loss: 231.0615, MinusLogProbMetric: 231.0615, val_loss: 243.3970, val_MinusLogProbMetric: 243.3970

Epoch 23: val_loss did not improve from 235.60794
196/196 - 57s - loss: 231.0615 - MinusLogProbMetric: 231.0615 - val_loss: 243.3970 - val_MinusLogProbMetric: 243.3970 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 24/1000
2023-10-30 15:35:26.523 
Epoch 24/1000 
	 loss: 221.3139, MinusLogProbMetric: 221.3139, val_loss: 215.2531, val_MinusLogProbMetric: 215.2531

Epoch 24: val_loss improved from 235.60794 to 215.25310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 221.3139 - MinusLogProbMetric: 221.3139 - val_loss: 215.2531 - val_MinusLogProbMetric: 215.2531 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 25/1000
2023-10-30 15:36:23.039 
Epoch 25/1000 
	 loss: 213.6747, MinusLogProbMetric: 213.6747, val_loss: 211.7150, val_MinusLogProbMetric: 211.7150

Epoch 25: val_loss improved from 215.25310 to 211.71500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 213.6747 - MinusLogProbMetric: 213.6747 - val_loss: 211.7150 - val_MinusLogProbMetric: 211.7150 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 26/1000
2023-10-30 15:37:15.473 
Epoch 26/1000 
	 loss: 210.0126, MinusLogProbMetric: 210.0126, val_loss: 238.4670, val_MinusLogProbMetric: 238.4670

Epoch 26: val_loss did not improve from 211.71500
196/196 - 52s - loss: 210.0126 - MinusLogProbMetric: 210.0126 - val_loss: 238.4670 - val_MinusLogProbMetric: 238.4670 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 27/1000
2023-10-30 15:38:06.382 
Epoch 27/1000 
	 loss: 209.5265, MinusLogProbMetric: 209.5265, val_loss: 202.4401, val_MinusLogProbMetric: 202.4401

Epoch 27: val_loss improved from 211.71500 to 202.44006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 209.5265 - MinusLogProbMetric: 209.5265 - val_loss: 202.4401 - val_MinusLogProbMetric: 202.4401 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 28/1000
2023-10-30 15:39:01.014 
Epoch 28/1000 
	 loss: 199.6996, MinusLogProbMetric: 199.6996, val_loss: 195.8448, val_MinusLogProbMetric: 195.8448

Epoch 28: val_loss improved from 202.44006 to 195.84482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 199.6996 - MinusLogProbMetric: 199.6996 - val_loss: 195.8448 - val_MinusLogProbMetric: 195.8448 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 29/1000
2023-10-30 15:39:59.924 
Epoch 29/1000 
	 loss: 285.0086, MinusLogProbMetric: 285.0086, val_loss: 281.6585, val_MinusLogProbMetric: 281.6585

Epoch 29: val_loss did not improve from 195.84482
196/196 - 58s - loss: 285.0086 - MinusLogProbMetric: 285.0086 - val_loss: 281.6585 - val_MinusLogProbMetric: 281.6585 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 30/1000
2023-10-30 15:40:54.811 
Epoch 30/1000 
	 loss: 262.1654, MinusLogProbMetric: 262.1654, val_loss: 237.7202, val_MinusLogProbMetric: 237.7202

Epoch 30: val_loss did not improve from 195.84482
196/196 - 55s - loss: 262.1654 - MinusLogProbMetric: 262.1654 - val_loss: 237.7202 - val_MinusLogProbMetric: 237.7202 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 31/1000
2023-10-30 15:41:49.307 
Epoch 31/1000 
	 loss: 229.3842, MinusLogProbMetric: 229.3842, val_loss: 229.0427, val_MinusLogProbMetric: 229.0427

Epoch 31: val_loss did not improve from 195.84482
196/196 - 54s - loss: 229.3842 - MinusLogProbMetric: 229.3842 - val_loss: 229.0427 - val_MinusLogProbMetric: 229.0427 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 32/1000
2023-10-30 15:42:48.157 
Epoch 32/1000 
	 loss: 218.9246, MinusLogProbMetric: 218.9246, val_loss: 215.6954, val_MinusLogProbMetric: 215.6954

Epoch 32: val_loss did not improve from 195.84482
196/196 - 59s - loss: 218.9246 - MinusLogProbMetric: 218.9246 - val_loss: 215.6954 - val_MinusLogProbMetric: 215.6954 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 33/1000
2023-10-30 15:43:43.024 
Epoch 33/1000 
	 loss: 207.1502, MinusLogProbMetric: 207.1502, val_loss: 200.0699, val_MinusLogProbMetric: 200.0699

Epoch 33: val_loss did not improve from 195.84482
196/196 - 55s - loss: 207.1502 - MinusLogProbMetric: 207.1502 - val_loss: 200.0699 - val_MinusLogProbMetric: 200.0699 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 34/1000
2023-10-30 15:44:41.005 
Epoch 34/1000 
	 loss: 198.5002, MinusLogProbMetric: 198.5002, val_loss: 191.8871, val_MinusLogProbMetric: 191.8871

Epoch 34: val_loss improved from 195.84482 to 191.88712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 198.5002 - MinusLogProbMetric: 198.5002 - val_loss: 191.8871 - val_MinusLogProbMetric: 191.8871 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 35/1000
2023-10-30 15:45:36.721 
Epoch 35/1000 
	 loss: 188.8129, MinusLogProbMetric: 188.8129, val_loss: 185.4072, val_MinusLogProbMetric: 185.4072

Epoch 35: val_loss improved from 191.88712 to 185.40720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 188.8129 - MinusLogProbMetric: 188.8129 - val_loss: 185.4072 - val_MinusLogProbMetric: 185.4072 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 36/1000
2023-10-30 15:46:32.354 
Epoch 36/1000 
	 loss: 181.2882, MinusLogProbMetric: 181.2882, val_loss: 180.4252, val_MinusLogProbMetric: 180.4252

Epoch 36: val_loss improved from 185.40720 to 180.42523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 181.2882 - MinusLogProbMetric: 181.2882 - val_loss: 180.4252 - val_MinusLogProbMetric: 180.4252 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 37/1000
2023-10-30 15:47:25.613 
Epoch 37/1000 
	 loss: 177.1864, MinusLogProbMetric: 177.1864, val_loss: 176.4276, val_MinusLogProbMetric: 176.4276

Epoch 37: val_loss improved from 180.42523 to 176.42758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 177.1864 - MinusLogProbMetric: 177.1864 - val_loss: 176.4276 - val_MinusLogProbMetric: 176.4276 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 38/1000
2023-10-30 15:48:16.934 
Epoch 38/1000 
	 loss: 173.2792, MinusLogProbMetric: 173.2792, val_loss: 172.9472, val_MinusLogProbMetric: 172.9472

Epoch 38: val_loss improved from 176.42758 to 172.94719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 173.2792 - MinusLogProbMetric: 173.2792 - val_loss: 172.9472 - val_MinusLogProbMetric: 172.9472 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 39/1000
2023-10-30 15:49:12.032 
Epoch 39/1000 
	 loss: 168.3916, MinusLogProbMetric: 168.3916, val_loss: 176.8726, val_MinusLogProbMetric: 176.8726

Epoch 39: val_loss did not improve from 172.94719
196/196 - 54s - loss: 168.3916 - MinusLogProbMetric: 168.3916 - val_loss: 176.8726 - val_MinusLogProbMetric: 176.8726 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 40/1000
2023-10-30 15:50:06.118 
Epoch 40/1000 
	 loss: 168.9889, MinusLogProbMetric: 168.9889, val_loss: 170.2875, val_MinusLogProbMetric: 170.2875

Epoch 40: val_loss improved from 172.94719 to 170.28749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 168.9889 - MinusLogProbMetric: 168.9889 - val_loss: 170.2875 - val_MinusLogProbMetric: 170.2875 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 41/1000
2023-10-30 15:51:02.330 
Epoch 41/1000 
	 loss: 166.2998, MinusLogProbMetric: 166.2998, val_loss: 164.7641, val_MinusLogProbMetric: 164.7641

Epoch 41: val_loss improved from 170.28749 to 164.76414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 166.2998 - MinusLogProbMetric: 166.2998 - val_loss: 164.7641 - val_MinusLogProbMetric: 164.7641 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 42/1000
2023-10-30 15:51:58.025 
Epoch 42/1000 
	 loss: 161.6003, MinusLogProbMetric: 161.6003, val_loss: 158.3525, val_MinusLogProbMetric: 158.3525

Epoch 42: val_loss improved from 164.76414 to 158.35254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 161.6003 - MinusLogProbMetric: 161.6003 - val_loss: 158.3525 - val_MinusLogProbMetric: 158.3525 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 43/1000
2023-10-30 15:52:49.913 
Epoch 43/1000 
	 loss: 157.1049, MinusLogProbMetric: 157.1049, val_loss: 155.3404, val_MinusLogProbMetric: 155.3404

Epoch 43: val_loss improved from 158.35254 to 155.34039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 157.1049 - MinusLogProbMetric: 157.1049 - val_loss: 155.3404 - val_MinusLogProbMetric: 155.3404 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 44/1000
2023-10-30 15:53:44.156 
Epoch 44/1000 
	 loss: 153.5300, MinusLogProbMetric: 153.5300, val_loss: 151.2834, val_MinusLogProbMetric: 151.2834

Epoch 44: val_loss improved from 155.34039 to 151.28342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 153.5300 - MinusLogProbMetric: 153.5300 - val_loss: 151.2834 - val_MinusLogProbMetric: 151.2834 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 45/1000
2023-10-30 15:54:42.494 
Epoch 45/1000 
	 loss: 150.9566, MinusLogProbMetric: 150.9566, val_loss: 149.3199, val_MinusLogProbMetric: 149.3199

Epoch 45: val_loss improved from 151.28342 to 149.31987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 150.9566 - MinusLogProbMetric: 150.9566 - val_loss: 149.3199 - val_MinusLogProbMetric: 149.3199 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 46/1000
2023-10-30 15:55:36.175 
Epoch 46/1000 
	 loss: 147.4621, MinusLogProbMetric: 147.4621, val_loss: 145.0248, val_MinusLogProbMetric: 145.0248

Epoch 46: val_loss improved from 149.31987 to 145.02481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 147.4621 - MinusLogProbMetric: 147.4621 - val_loss: 145.0248 - val_MinusLogProbMetric: 145.0248 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 47/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 189: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:56:26.065 
Epoch 47/1000 
	 loss: nan, MinusLogProbMetric: 145.2165, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 47: val_loss did not improve from 145.02481
196/196 - 49s - loss: nan - MinusLogProbMetric: 145.2165 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 49s/epoch - 250ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 382.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_351"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_352 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f4752374850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f49401b3f40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f49401b3f40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f48e07c4b80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47863dc2b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f47863dc820>, <keras.callbacks.ModelCheckpoint object at 0x7f47863dc8e0>, <keras.callbacks.EarlyStopping object at 0x7f47863dcb50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f47863dcb80>, <keras.callbacks.TerminateOnNaN object at 0x7f47863dc7c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-30 15:56:33.896172
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-30 15:59:32.464 
Epoch 1/1000 
	 loss: 191.3179, MinusLogProbMetric: 191.3179, val_loss: 143.9341, val_MinusLogProbMetric: 143.9341

Epoch 1: val_loss improved from inf to 143.93405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 179s - loss: 191.3179 - MinusLogProbMetric: 191.3179 - val_loss: 143.9341 - val_MinusLogProbMetric: 143.9341 - lr: 3.7037e-05 - 179s/epoch - 913ms/step
Epoch 2/1000
2023-10-30 16:00:23.701 
Epoch 2/1000 
	 loss: 136.5647, MinusLogProbMetric: 136.5647, val_loss: 123.7861, val_MinusLogProbMetric: 123.7861

Epoch 2: val_loss improved from 143.93405 to 123.78607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 136.5647 - MinusLogProbMetric: 136.5647 - val_loss: 123.7861 - val_MinusLogProbMetric: 123.7861 - lr: 3.7037e-05 - 51s/epoch - 260ms/step
Epoch 3/1000
2023-10-30 16:01:18.582 
Epoch 3/1000 
	 loss: 127.4404, MinusLogProbMetric: 127.4404, val_loss: 118.2735, val_MinusLogProbMetric: 118.2735

Epoch 3: val_loss improved from 123.78607 to 118.27354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 127.4404 - MinusLogProbMetric: 127.4404 - val_loss: 118.2735 - val_MinusLogProbMetric: 118.2735 - lr: 3.7037e-05 - 55s/epoch - 280ms/step
Epoch 4/1000
2023-10-30 16:02:09.767 
Epoch 4/1000 
	 loss: 183.3967, MinusLogProbMetric: 183.3967, val_loss: 153.9140, val_MinusLogProbMetric: 153.9140

Epoch 4: val_loss did not improve from 118.27354
196/196 - 50s - loss: 183.3967 - MinusLogProbMetric: 183.3967 - val_loss: 153.9140 - val_MinusLogProbMetric: 153.9140 - lr: 3.7037e-05 - 50s/epoch - 257ms/step
Epoch 5/1000
2023-10-30 16:03:00.939 
Epoch 5/1000 
	 loss: 125.7466, MinusLogProbMetric: 125.7466, val_loss: 112.0614, val_MinusLogProbMetric: 112.0614

Epoch 5: val_loss improved from 118.27354 to 112.06142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 125.7466 - MinusLogProbMetric: 125.7466 - val_loss: 112.0614 - val_MinusLogProbMetric: 112.0614 - lr: 3.7037e-05 - 53s/epoch - 272ms/step
Epoch 6/1000
2023-10-30 16:03:57.033 
Epoch 6/1000 
	 loss: 139.9924, MinusLogProbMetric: 139.9924, val_loss: 142.6492, val_MinusLogProbMetric: 142.6492

Epoch 6: val_loss did not improve from 112.06142
196/196 - 54s - loss: 139.9924 - MinusLogProbMetric: 139.9924 - val_loss: 142.6492 - val_MinusLogProbMetric: 142.6492 - lr: 3.7037e-05 - 54s/epoch - 275ms/step
Epoch 7/1000
2023-10-30 16:04:48.490 
Epoch 7/1000 
	 loss: 119.4635, MinusLogProbMetric: 119.4635, val_loss: 109.3275, val_MinusLogProbMetric: 109.3275

Epoch 7: val_loss improved from 112.06142 to 109.32755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 119.4635 - MinusLogProbMetric: 119.4635 - val_loss: 109.3275 - val_MinusLogProbMetric: 109.3275 - lr: 3.7037e-05 - 52s/epoch - 267ms/step
Epoch 8/1000
2023-10-30 16:05:40.633 
Epoch 8/1000 
	 loss: 107.9065, MinusLogProbMetric: 107.9065, val_loss: 98.3928, val_MinusLogProbMetric: 98.3928

Epoch 8: val_loss improved from 109.32755 to 98.39283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 107.9065 - MinusLogProbMetric: 107.9065 - val_loss: 98.3928 - val_MinusLogProbMetric: 98.3928 - lr: 3.7037e-05 - 52s/epoch - 265ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 140: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:06:18.690 
Epoch 9/1000 
	 loss: nan, MinusLogProbMetric: 202.6080, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 9: val_loss did not improve from 98.39283
196/196 - 37s - loss: nan - MinusLogProbMetric: 202.6080 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 37s/epoch - 190ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 382.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_362"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_363 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f4752426650>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47b03ebd00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47b03ebd00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46e2d9a350>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46ff800430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46ff8009a0>, <keras.callbacks.ModelCheckpoint object at 0x7f46ff800a60>, <keras.callbacks.EarlyStopping object at 0x7f46ff800cd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46ff800d00>, <keras.callbacks.TerminateOnNaN object at 0x7f46ff800940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-30 16:06:25.781779
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-30 16:09:26.643 
Epoch 1/1000 
	 loss: 97.9714, MinusLogProbMetric: 97.9714, val_loss: 91.0580, val_MinusLogProbMetric: 91.0580

Epoch 1: val_loss improved from inf to 91.05800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 181s - loss: 97.9714 - MinusLogProbMetric: 97.9714 - val_loss: 91.0580 - val_MinusLogProbMetric: 91.0580 - lr: 1.2346e-05 - 181s/epoch - 925ms/step
Epoch 2/1000
2023-10-30 16:10:24.534 
Epoch 2/1000 
	 loss: 90.2704, MinusLogProbMetric: 90.2704, val_loss: 88.4990, val_MinusLogProbMetric: 88.4990

Epoch 2: val_loss improved from 91.05800 to 88.49903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 90.2704 - MinusLogProbMetric: 90.2704 - val_loss: 88.4990 - val_MinusLogProbMetric: 88.4990 - lr: 1.2346e-05 - 58s/epoch - 294ms/step
Epoch 3/1000
2023-10-30 16:11:18.611 
Epoch 3/1000 
	 loss: 87.2077, MinusLogProbMetric: 87.2077, val_loss: 84.1926, val_MinusLogProbMetric: 84.1926

Epoch 3: val_loss improved from 88.49903 to 84.19258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 87.2077 - MinusLogProbMetric: 87.2077 - val_loss: 84.1926 - val_MinusLogProbMetric: 84.1926 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 4/1000
2023-10-30 16:12:15.225 
Epoch 4/1000 
	 loss: 82.1669, MinusLogProbMetric: 82.1669, val_loss: 81.0800, val_MinusLogProbMetric: 81.0800

Epoch 4: val_loss improved from 84.19258 to 81.07999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 82.1669 - MinusLogProbMetric: 82.1669 - val_loss: 81.0800 - val_MinusLogProbMetric: 81.0800 - lr: 1.2346e-05 - 57s/epoch - 289ms/step
Epoch 5/1000
2023-10-30 16:13:11.028 
Epoch 5/1000 
	 loss: 81.3796, MinusLogProbMetric: 81.3796, val_loss: 87.2172, val_MinusLogProbMetric: 87.2172

Epoch 5: val_loss did not improve from 81.07999
196/196 - 55s - loss: 81.3796 - MinusLogProbMetric: 81.3796 - val_loss: 87.2172 - val_MinusLogProbMetric: 87.2172 - lr: 1.2346e-05 - 55s/epoch - 281ms/step
Epoch 6/1000
2023-10-30 16:14:08.193 
Epoch 6/1000 
	 loss: 80.0316, MinusLogProbMetric: 80.0316, val_loss: 81.5688, val_MinusLogProbMetric: 81.5688

Epoch 6: val_loss did not improve from 81.07999
196/196 - 57s - loss: 80.0316 - MinusLogProbMetric: 80.0316 - val_loss: 81.5688 - val_MinusLogProbMetric: 81.5688 - lr: 1.2346e-05 - 57s/epoch - 292ms/step
Epoch 7/1000
2023-10-30 16:15:04.146 
Epoch 7/1000 
	 loss: 75.7527, MinusLogProbMetric: 75.7527, val_loss: 75.3128, val_MinusLogProbMetric: 75.3128

Epoch 7: val_loss improved from 81.07999 to 75.31281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 75.7527 - MinusLogProbMetric: 75.7527 - val_loss: 75.3128 - val_MinusLogProbMetric: 75.3128 - lr: 1.2346e-05 - 57s/epoch - 289ms/step
Epoch 8/1000
2023-10-30 16:15:59.954 
Epoch 8/1000 
	 loss: 73.3240, MinusLogProbMetric: 73.3240, val_loss: 72.4581, val_MinusLogProbMetric: 72.4581

Epoch 8: val_loss improved from 75.31281 to 72.45810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 73.3240 - MinusLogProbMetric: 73.3240 - val_loss: 72.4581 - val_MinusLogProbMetric: 72.4581 - lr: 1.2346e-05 - 56s/epoch - 285ms/step
Epoch 9/1000
2023-10-30 16:16:52.236 
Epoch 9/1000 
	 loss: 71.3588, MinusLogProbMetric: 71.3588, val_loss: 71.7829, val_MinusLogProbMetric: 71.7829

Epoch 9: val_loss improved from 72.45810 to 71.78288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 71.3588 - MinusLogProbMetric: 71.3588 - val_loss: 71.7829 - val_MinusLogProbMetric: 71.7829 - lr: 1.2346e-05 - 53s/epoch - 268ms/step
Epoch 10/1000
2023-10-30 16:17:43.916 
Epoch 10/1000 
	 loss: 70.1308, MinusLogProbMetric: 70.1308, val_loss: 68.4129, val_MinusLogProbMetric: 68.4129

Epoch 10: val_loss improved from 71.78288 to 68.41287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 70.1308 - MinusLogProbMetric: 70.1308 - val_loss: 68.4129 - val_MinusLogProbMetric: 68.4129 - lr: 1.2346e-05 - 52s/epoch - 263ms/step
Epoch 11/1000
2023-10-30 16:18:40.813 
Epoch 11/1000 
	 loss: 68.7177, MinusLogProbMetric: 68.7177, val_loss: 69.5705, val_MinusLogProbMetric: 69.5705

Epoch 11: val_loss did not improve from 68.41287
196/196 - 56s - loss: 68.7177 - MinusLogProbMetric: 68.7177 - val_loss: 69.5705 - val_MinusLogProbMetric: 69.5705 - lr: 1.2346e-05 - 56s/epoch - 286ms/step
Epoch 12/1000
2023-10-30 16:19:41.159 
Epoch 12/1000 
	 loss: 67.4484, MinusLogProbMetric: 67.4484, val_loss: 67.0486, val_MinusLogProbMetric: 67.0486

Epoch 12: val_loss improved from 68.41287 to 67.04863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 67.4484 - MinusLogProbMetric: 67.4484 - val_loss: 67.0486 - val_MinusLogProbMetric: 67.0486 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 13/1000
2023-10-30 16:20:36.510 
Epoch 13/1000 
	 loss: 65.8219, MinusLogProbMetric: 65.8219, val_loss: 64.9141, val_MinusLogProbMetric: 64.9141

Epoch 13: val_loss improved from 67.04863 to 64.91415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 65.8219 - MinusLogProbMetric: 65.8219 - val_loss: 64.9141 - val_MinusLogProbMetric: 64.9141 - lr: 1.2346e-05 - 55s/epoch - 283ms/step
Epoch 14/1000
2023-10-30 16:21:39.339 
Epoch 14/1000 
	 loss: 64.6175, MinusLogProbMetric: 64.6175, val_loss: 63.8349, val_MinusLogProbMetric: 63.8349

Epoch 14: val_loss improved from 64.91415 to 63.83489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 64.6175 - MinusLogProbMetric: 64.6175 - val_loss: 63.8349 - val_MinusLogProbMetric: 63.8349 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 15/1000
2023-10-30 16:22:33.834 
Epoch 15/1000 
	 loss: 63.4022, MinusLogProbMetric: 63.4022, val_loss: 62.5478, val_MinusLogProbMetric: 62.5478

Epoch 15: val_loss improved from 63.83489 to 62.54782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 63.4022 - MinusLogProbMetric: 63.4022 - val_loss: 62.5478 - val_MinusLogProbMetric: 62.5478 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 16/1000
2023-10-30 16:23:25.912 
Epoch 16/1000 
	 loss: 62.2837, MinusLogProbMetric: 62.2837, val_loss: 61.8043, val_MinusLogProbMetric: 61.8043

Epoch 16: val_loss improved from 62.54782 to 61.80426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 62.2837 - MinusLogProbMetric: 62.2837 - val_loss: 61.8043 - val_MinusLogProbMetric: 61.8043 - lr: 1.2346e-05 - 52s/epoch - 265ms/step
Epoch 17/1000
2023-10-30 16:24:27.320 
Epoch 17/1000 
	 loss: 61.9010, MinusLogProbMetric: 61.9010, val_loss: 61.9347, val_MinusLogProbMetric: 61.9347

Epoch 17: val_loss did not improve from 61.80426
196/196 - 61s - loss: 61.9010 - MinusLogProbMetric: 61.9010 - val_loss: 61.9347 - val_MinusLogProbMetric: 61.9347 - lr: 1.2346e-05 - 61s/epoch - 309ms/step
Epoch 18/1000
2023-10-30 16:25:31.721 
Epoch 18/1000 
	 loss: 60.8274, MinusLogProbMetric: 60.8274, val_loss: 60.1338, val_MinusLogProbMetric: 60.1338

Epoch 18: val_loss improved from 61.80426 to 60.13381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 60.8274 - MinusLogProbMetric: 60.8274 - val_loss: 60.1338 - val_MinusLogProbMetric: 60.1338 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 19/1000
2023-10-30 16:26:36.257 
Epoch 19/1000 
	 loss: 60.2554, MinusLogProbMetric: 60.2554, val_loss: 60.1631, val_MinusLogProbMetric: 60.1631

Epoch 19: val_loss did not improve from 60.13381
196/196 - 64s - loss: 60.2554 - MinusLogProbMetric: 60.2554 - val_loss: 60.1631 - val_MinusLogProbMetric: 60.1631 - lr: 1.2346e-05 - 64s/epoch - 324ms/step
Epoch 20/1000
2023-10-30 16:27:30.787 
Epoch 20/1000 
	 loss: 59.0462, MinusLogProbMetric: 59.0462, val_loss: 62.5137, val_MinusLogProbMetric: 62.5137

Epoch 20: val_loss did not improve from 60.13381
196/196 - 55s - loss: 59.0462 - MinusLogProbMetric: 59.0462 - val_loss: 62.5137 - val_MinusLogProbMetric: 62.5137 - lr: 1.2346e-05 - 55s/epoch - 278ms/step
Epoch 21/1000
2023-10-30 16:28:33.481 
Epoch 21/1000 
	 loss: 79.6822, MinusLogProbMetric: 79.6822, val_loss: 136.7783, val_MinusLogProbMetric: 136.7783

Epoch 21: val_loss did not improve from 60.13381
196/196 - 63s - loss: 79.6822 - MinusLogProbMetric: 79.6822 - val_loss: 136.7783 - val_MinusLogProbMetric: 136.7783 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 22/1000
2023-10-30 16:29:37.816 
Epoch 22/1000 
	 loss: 89.7312, MinusLogProbMetric: 89.7312, val_loss: 76.3168, val_MinusLogProbMetric: 76.3168

Epoch 22: val_loss did not improve from 60.13381
196/196 - 64s - loss: 89.7312 - MinusLogProbMetric: 89.7312 - val_loss: 76.3168 - val_MinusLogProbMetric: 76.3168 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 23/1000
2023-10-30 16:30:38.390 
Epoch 23/1000 
	 loss: 69.9004, MinusLogProbMetric: 69.9004, val_loss: 66.5369, val_MinusLogProbMetric: 66.5369

Epoch 23: val_loss did not improve from 60.13381
196/196 - 61s - loss: 69.9004 - MinusLogProbMetric: 69.9004 - val_loss: 66.5369 - val_MinusLogProbMetric: 66.5369 - lr: 1.2346e-05 - 61s/epoch - 309ms/step
Epoch 24/1000
2023-10-30 16:31:41.851 
Epoch 24/1000 
	 loss: 76.4476, MinusLogProbMetric: 76.4476, val_loss: 79.9759, val_MinusLogProbMetric: 79.9759

Epoch 24: val_loss did not improve from 60.13381
196/196 - 63s - loss: 76.4476 - MinusLogProbMetric: 76.4476 - val_loss: 79.9759 - val_MinusLogProbMetric: 79.9759 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 25/1000
2023-10-30 16:32:42.042 
Epoch 25/1000 
	 loss: 77.8434, MinusLogProbMetric: 77.8434, val_loss: 112.1017, val_MinusLogProbMetric: 112.1017

Epoch 25: val_loss did not improve from 60.13381
196/196 - 60s - loss: 77.8434 - MinusLogProbMetric: 77.8434 - val_loss: 112.1017 - val_MinusLogProbMetric: 112.1017 - lr: 1.2346e-05 - 60s/epoch - 307ms/step
Epoch 26/1000
2023-10-30 16:33:43.749 
Epoch 26/1000 
	 loss: 71.0006, MinusLogProbMetric: 71.0006, val_loss: 65.6022, val_MinusLogProbMetric: 65.6022

Epoch 26: val_loss did not improve from 60.13381
196/196 - 62s - loss: 71.0006 - MinusLogProbMetric: 71.0006 - val_loss: 65.6022 - val_MinusLogProbMetric: 65.6022 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 27/1000
2023-10-30 16:34:42.908 
Epoch 27/1000 
	 loss: 64.1951, MinusLogProbMetric: 64.1951, val_loss: 88.1930, val_MinusLogProbMetric: 88.1930

Epoch 27: val_loss did not improve from 60.13381
196/196 - 59s - loss: 64.1951 - MinusLogProbMetric: 64.1951 - val_loss: 88.1930 - val_MinusLogProbMetric: 88.1930 - lr: 1.2346e-05 - 59s/epoch - 302ms/step
Epoch 28/1000
2023-10-30 16:35:33.725 
Epoch 28/1000 
	 loss: 65.9359, MinusLogProbMetric: 65.9359, val_loss: 61.9620, val_MinusLogProbMetric: 61.9620

Epoch 28: val_loss did not improve from 60.13381
196/196 - 51s - loss: 65.9359 - MinusLogProbMetric: 65.9359 - val_loss: 61.9620 - val_MinusLogProbMetric: 61.9620 - lr: 1.2346e-05 - 51s/epoch - 259ms/step
Epoch 29/1000
2023-10-30 16:36:26.115 
Epoch 29/1000 
	 loss: 60.3357, MinusLogProbMetric: 60.3357, val_loss: 61.5097, val_MinusLogProbMetric: 61.5097

Epoch 29: val_loss did not improve from 60.13381
196/196 - 52s - loss: 60.3357 - MinusLogProbMetric: 60.3357 - val_loss: 61.5097 - val_MinusLogProbMetric: 61.5097 - lr: 1.2346e-05 - 52s/epoch - 267ms/step
Epoch 30/1000
2023-10-30 16:37:16.430 
Epoch 30/1000 
	 loss: 58.4496, MinusLogProbMetric: 58.4496, val_loss: 58.1965, val_MinusLogProbMetric: 58.1965

Epoch 30: val_loss improved from 60.13381 to 58.19651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 58.4496 - MinusLogProbMetric: 58.4496 - val_loss: 58.1965 - val_MinusLogProbMetric: 58.1965 - lr: 1.2346e-05 - 51s/epoch - 261ms/step
Epoch 31/1000
2023-10-30 16:38:07.394 
Epoch 31/1000 
	 loss: 57.6406, MinusLogProbMetric: 57.6406, val_loss: 57.5771, val_MinusLogProbMetric: 57.5771

Epoch 31: val_loss improved from 58.19651 to 57.57706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 57.6406 - MinusLogProbMetric: 57.6406 - val_loss: 57.5771 - val_MinusLogProbMetric: 57.5771 - lr: 1.2346e-05 - 51s/epoch - 260ms/step
Epoch 32/1000
2023-10-30 16:39:01.197 
Epoch 32/1000 
	 loss: 56.5346, MinusLogProbMetric: 56.5346, val_loss: 56.3194, val_MinusLogProbMetric: 56.3194

Epoch 32: val_loss improved from 57.57706 to 56.31937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 56.5346 - MinusLogProbMetric: 56.5346 - val_loss: 56.3194 - val_MinusLogProbMetric: 56.3194 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 33/1000
2023-10-30 16:39:57.583 
Epoch 33/1000 
	 loss: 55.7290, MinusLogProbMetric: 55.7290, val_loss: 55.4568, val_MinusLogProbMetric: 55.4568

Epoch 33: val_loss improved from 56.31937 to 55.45681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 55.7290 - MinusLogProbMetric: 55.7290 - val_loss: 55.4568 - val_MinusLogProbMetric: 55.4568 - lr: 1.2346e-05 - 57s/epoch - 288ms/step
Epoch 34/1000
2023-10-30 16:40:50.618 
Epoch 34/1000 
	 loss: 55.2871, MinusLogProbMetric: 55.2871, val_loss: 55.3375, val_MinusLogProbMetric: 55.3375

Epoch 34: val_loss improved from 55.45681 to 55.33752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 55.2871 - MinusLogProbMetric: 55.2871 - val_loss: 55.3375 - val_MinusLogProbMetric: 55.3375 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 35/1000
2023-10-30 16:41:50.750 
Epoch 35/1000 
	 loss: 54.5823, MinusLogProbMetric: 54.5823, val_loss: 54.5548, val_MinusLogProbMetric: 54.5548

Epoch 35: val_loss improved from 55.33752 to 54.55482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 54.5823 - MinusLogProbMetric: 54.5823 - val_loss: 54.5548 - val_MinusLogProbMetric: 54.5548 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 36/1000
2023-10-30 16:42:57.328 
Epoch 36/1000 
	 loss: 54.2289, MinusLogProbMetric: 54.2289, val_loss: 53.6865, val_MinusLogProbMetric: 53.6865

Epoch 36: val_loss improved from 54.55482 to 53.68652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 54.2289 - MinusLogProbMetric: 54.2289 - val_loss: 53.6865 - val_MinusLogProbMetric: 53.6865 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 37/1000
2023-10-30 16:44:01.773 
Epoch 37/1000 
	 loss: 53.9788, MinusLogProbMetric: 53.9788, val_loss: 54.5349, val_MinusLogProbMetric: 54.5349

Epoch 37: val_loss did not improve from 53.68652
196/196 - 63s - loss: 53.9788 - MinusLogProbMetric: 53.9788 - val_loss: 54.5349 - val_MinusLogProbMetric: 54.5349 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 38/1000
2023-10-30 16:45:01.461 
Epoch 38/1000 
	 loss: 53.3696, MinusLogProbMetric: 53.3696, val_loss: 53.1249, val_MinusLogProbMetric: 53.1249

Epoch 38: val_loss improved from 53.68652 to 53.12488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 53.3696 - MinusLogProbMetric: 53.3696 - val_loss: 53.1249 - val_MinusLogProbMetric: 53.1249 - lr: 1.2346e-05 - 60s/epoch - 308ms/step
Epoch 39/1000
2023-10-30 16:45:53.835 
Epoch 39/1000 
	 loss: 52.9776, MinusLogProbMetric: 52.9776, val_loss: 52.7672, val_MinusLogProbMetric: 52.7672

Epoch 39: val_loss improved from 53.12488 to 52.76716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 52.9776 - MinusLogProbMetric: 52.9776 - val_loss: 52.7672 - val_MinusLogProbMetric: 52.7672 - lr: 1.2346e-05 - 52s/epoch - 268ms/step
Epoch 40/1000
2023-10-30 16:46:51.474 
Epoch 40/1000 
	 loss: 52.5874, MinusLogProbMetric: 52.5874, val_loss: 52.3165, val_MinusLogProbMetric: 52.3165

Epoch 40: val_loss improved from 52.76716 to 52.31650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 52.5874 - MinusLogProbMetric: 52.5874 - val_loss: 52.3165 - val_MinusLogProbMetric: 52.3165 - lr: 1.2346e-05 - 58s/epoch - 294ms/step
Epoch 41/1000
2023-10-30 16:47:55.854 
Epoch 41/1000 
	 loss: 52.4039, MinusLogProbMetric: 52.4039, val_loss: 52.7800, val_MinusLogProbMetric: 52.7800

Epoch 41: val_loss did not improve from 52.31650
196/196 - 63s - loss: 52.4039 - MinusLogProbMetric: 52.4039 - val_loss: 52.7800 - val_MinusLogProbMetric: 52.7800 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 42/1000
2023-10-30 16:49:00.661 
Epoch 42/1000 
	 loss: 51.7260, MinusLogProbMetric: 51.7260, val_loss: 51.6588, val_MinusLogProbMetric: 51.6588

Epoch 42: val_loss improved from 52.31650 to 51.65883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 51.7260 - MinusLogProbMetric: 51.7260 - val_loss: 51.6588 - val_MinusLogProbMetric: 51.6588 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 43/1000
2023-10-30 16:50:05.960 
Epoch 43/1000 
	 loss: 51.3186, MinusLogProbMetric: 51.3186, val_loss: 51.4546, val_MinusLogProbMetric: 51.4546

Epoch 43: val_loss improved from 51.65883 to 51.45461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 51.3186 - MinusLogProbMetric: 51.3186 - val_loss: 51.4546 - val_MinusLogProbMetric: 51.4546 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 44/1000
2023-10-30 16:51:06.230 
Epoch 44/1000 
	 loss: 50.9932, MinusLogProbMetric: 50.9932, val_loss: 51.1367, val_MinusLogProbMetric: 51.1367

Epoch 44: val_loss improved from 51.45461 to 51.13667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 50.9932 - MinusLogProbMetric: 50.9932 - val_loss: 51.1367 - val_MinusLogProbMetric: 51.1367 - lr: 1.2346e-05 - 61s/epoch - 310ms/step
Epoch 45/1000
2023-10-30 16:52:08.612 
Epoch 45/1000 
	 loss: 51.2070, MinusLogProbMetric: 51.2070, val_loss: 184.9047, val_MinusLogProbMetric: 184.9047

Epoch 45: val_loss did not improve from 51.13667
196/196 - 61s - loss: 51.2070 - MinusLogProbMetric: 51.2070 - val_loss: 184.9047 - val_MinusLogProbMetric: 184.9047 - lr: 1.2346e-05 - 61s/epoch - 310ms/step
Epoch 46/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:52:14.802 
Epoch 46/1000 
	 loss: nan, MinusLogProbMetric: 232.9649, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 46: val_loss did not improve from 51.13667
196/196 - 6s - loss: nan - MinusLogProbMetric: 232.9649 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 6s/epoch - 32ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 382.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f46feaf3d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4a4c61f580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4a4c61f580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f47d04650f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f48fc52b700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f48fc529120>, <keras.callbacks.ModelCheckpoint object at 0x7f48fc52a290>, <keras.callbacks.EarlyStopping object at 0x7f48fc528dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f48fc52b010>, <keras.callbacks.TerminateOnNaN object at 0x7f48fc52ac80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-30 16:52:23.944943
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-30 16:55:36.476 
Epoch 1/1000 
	 loss: 52.5376, MinusLogProbMetric: 52.5376, val_loss: 50.3029, val_MinusLogProbMetric: 50.3029

Epoch 1: val_loss improved from inf to 50.30292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 193s - loss: 52.5376 - MinusLogProbMetric: 52.5376 - val_loss: 50.3029 - val_MinusLogProbMetric: 50.3029 - lr: 4.1152e-06 - 193s/epoch - 986ms/step
Epoch 2/1000
2023-10-30 16:56:40.693 
Epoch 2/1000 
	 loss: 49.9411, MinusLogProbMetric: 49.9411, val_loss: 50.0800, val_MinusLogProbMetric: 50.0800

Epoch 2: val_loss improved from 50.30292 to 50.08003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 49.9411 - MinusLogProbMetric: 49.9411 - val_loss: 50.0800 - val_MinusLogProbMetric: 50.0800 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 3/1000
2023-10-30 16:57:42.805 
Epoch 3/1000 
	 loss: 49.7226, MinusLogProbMetric: 49.7226, val_loss: 49.8655, val_MinusLogProbMetric: 49.8655

Epoch 3: val_loss improved from 50.08003 to 49.86547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 49.7226 - MinusLogProbMetric: 49.7226 - val_loss: 49.8655 - val_MinusLogProbMetric: 49.8655 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 4/1000
2023-10-30 16:58:46.127 
Epoch 4/1000 
	 loss: 49.5637, MinusLogProbMetric: 49.5637, val_loss: 49.8132, val_MinusLogProbMetric: 49.8132

Epoch 4: val_loss improved from 49.86547 to 49.81319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 49.5637 - MinusLogProbMetric: 49.5637 - val_loss: 49.8132 - val_MinusLogProbMetric: 49.8132 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 5/1000
2023-10-30 16:59:50.104 
Epoch 5/1000 
	 loss: 49.3405, MinusLogProbMetric: 49.3405, val_loss: 49.4276, val_MinusLogProbMetric: 49.4276

Epoch 5: val_loss improved from 49.81319 to 49.42762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 49.3405 - MinusLogProbMetric: 49.3405 - val_loss: 49.4276 - val_MinusLogProbMetric: 49.4276 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 6/1000
2023-10-30 17:00:54.232 
Epoch 6/1000 
	 loss: 49.8349, MinusLogProbMetric: 49.8349, val_loss: 49.3654, val_MinusLogProbMetric: 49.3654

Epoch 6: val_loss improved from 49.42762 to 49.36543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 49.8349 - MinusLogProbMetric: 49.8349 - val_loss: 49.3654 - val_MinusLogProbMetric: 49.3654 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 7/1000
2023-10-30 17:01:57.232 
Epoch 7/1000 
	 loss: 48.9765, MinusLogProbMetric: 48.9765, val_loss: 49.8689, val_MinusLogProbMetric: 49.8689

Epoch 7: val_loss did not improve from 49.36543
196/196 - 62s - loss: 48.9765 - MinusLogProbMetric: 48.9765 - val_loss: 49.8689 - val_MinusLogProbMetric: 49.8689 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 8/1000
2023-10-30 17:03:01.154 
Epoch 8/1000 
	 loss: 48.8386, MinusLogProbMetric: 48.8386, val_loss: 49.3128, val_MinusLogProbMetric: 49.3128

Epoch 8: val_loss improved from 49.36543 to 49.31284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 48.8386 - MinusLogProbMetric: 48.8386 - val_loss: 49.3128 - val_MinusLogProbMetric: 49.3128 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 9/1000
2023-10-30 17:04:07.522 
Epoch 9/1000 
	 loss: 48.6113, MinusLogProbMetric: 48.6113, val_loss: 48.6206, val_MinusLogProbMetric: 48.6206

Epoch 9: val_loss improved from 49.31284 to 48.62065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 48.6113 - MinusLogProbMetric: 48.6113 - val_loss: 48.6206 - val_MinusLogProbMetric: 48.6206 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 10/1000
2023-10-30 17:05:10.240 
Epoch 10/1000 
	 loss: 50.2544, MinusLogProbMetric: 50.2544, val_loss: 48.7137, val_MinusLogProbMetric: 48.7137

Epoch 10: val_loss did not improve from 48.62065
196/196 - 62s - loss: 50.2544 - MinusLogProbMetric: 50.2544 - val_loss: 48.7137 - val_MinusLogProbMetric: 48.7137 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 11/1000
2023-10-30 17:06:15.305 
Epoch 11/1000 
	 loss: 48.3802, MinusLogProbMetric: 48.3802, val_loss: 48.2863, val_MinusLogProbMetric: 48.2863

Epoch 11: val_loss improved from 48.62065 to 48.28625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 48.3802 - MinusLogProbMetric: 48.3802 - val_loss: 48.2863 - val_MinusLogProbMetric: 48.2863 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 12/1000
2023-10-30 17:07:20.124 
Epoch 12/1000 
	 loss: 47.9950, MinusLogProbMetric: 47.9950, val_loss: 48.0965, val_MinusLogProbMetric: 48.0965

Epoch 12: val_loss improved from 48.28625 to 48.09646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 47.9950 - MinusLogProbMetric: 47.9950 - val_loss: 48.0965 - val_MinusLogProbMetric: 48.0965 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 13/1000
2023-10-30 17:08:25.943 
Epoch 13/1000 
	 loss: 47.7766, MinusLogProbMetric: 47.7766, val_loss: 47.6959, val_MinusLogProbMetric: 47.6959

Epoch 13: val_loss improved from 48.09646 to 47.69593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 47.7766 - MinusLogProbMetric: 47.7766 - val_loss: 47.6959 - val_MinusLogProbMetric: 47.6959 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 14/1000
2023-10-30 17:09:28.993 
Epoch 14/1000 
	 loss: 47.6659, MinusLogProbMetric: 47.6659, val_loss: 47.8733, val_MinusLogProbMetric: 47.8733

Epoch 14: val_loss did not improve from 47.69593
196/196 - 62s - loss: 47.6659 - MinusLogProbMetric: 47.6659 - val_loss: 47.8733 - val_MinusLogProbMetric: 47.8733 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 15/1000
2023-10-30 17:10:32.266 
Epoch 15/1000 
	 loss: 47.4396, MinusLogProbMetric: 47.4396, val_loss: 47.6577, val_MinusLogProbMetric: 47.6577

Epoch 15: val_loss improved from 47.69593 to 47.65770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 47.4396 - MinusLogProbMetric: 47.4396 - val_loss: 47.6577 - val_MinusLogProbMetric: 47.6577 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 16/1000
2023-10-30 17:11:34.343 
Epoch 16/1000 
	 loss: 47.2677, MinusLogProbMetric: 47.2677, val_loss: 47.4893, val_MinusLogProbMetric: 47.4893

Epoch 16: val_loss improved from 47.65770 to 47.48928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 47.2677 - MinusLogProbMetric: 47.2677 - val_loss: 47.4893 - val_MinusLogProbMetric: 47.4893 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 17/1000
2023-10-30 17:12:28.485 
Epoch 17/1000 
	 loss: 47.1271, MinusLogProbMetric: 47.1271, val_loss: 51.7694, val_MinusLogProbMetric: 51.7694

Epoch 17: val_loss did not improve from 47.48928
196/196 - 53s - loss: 47.1271 - MinusLogProbMetric: 47.1271 - val_loss: 51.7694 - val_MinusLogProbMetric: 51.7694 - lr: 4.1152e-06 - 53s/epoch - 272ms/step
Epoch 18/1000
2023-10-30 17:13:29.928 
Epoch 18/1000 
	 loss: 47.9626, MinusLogProbMetric: 47.9626, val_loss: 47.2173, val_MinusLogProbMetric: 47.2173

Epoch 18: val_loss improved from 47.48928 to 47.21727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 47.9626 - MinusLogProbMetric: 47.9626 - val_loss: 47.2173 - val_MinusLogProbMetric: 47.2173 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 19/1000
2023-10-30 17:14:34.664 
Epoch 19/1000 
	 loss: 46.7915, MinusLogProbMetric: 46.7915, val_loss: 47.1246, val_MinusLogProbMetric: 47.1246

Epoch 19: val_loss improved from 47.21727 to 47.12463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 46.7915 - MinusLogProbMetric: 46.7915 - val_loss: 47.1246 - val_MinusLogProbMetric: 47.1246 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 20/1000
2023-10-30 17:15:39.353 
Epoch 20/1000 
	 loss: 46.5682, MinusLogProbMetric: 46.5682, val_loss: 46.6783, val_MinusLogProbMetric: 46.6783

Epoch 20: val_loss improved from 47.12463 to 46.67834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 46.5682 - MinusLogProbMetric: 46.5682 - val_loss: 46.6783 - val_MinusLogProbMetric: 46.6783 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 21/1000
2023-10-30 17:16:43.615 
Epoch 21/1000 
	 loss: 46.4357, MinusLogProbMetric: 46.4357, val_loss: 46.6760, val_MinusLogProbMetric: 46.6760

Epoch 21: val_loss improved from 46.67834 to 46.67605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 46.4357 - MinusLogProbMetric: 46.4357 - val_loss: 46.6760 - val_MinusLogProbMetric: 46.6760 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 22/1000
2023-10-30 17:17:46.416 
Epoch 22/1000 
	 loss: 46.3210, MinusLogProbMetric: 46.3210, val_loss: 46.5899, val_MinusLogProbMetric: 46.5899

Epoch 22: val_loss improved from 46.67605 to 46.58990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 46.3210 - MinusLogProbMetric: 46.3210 - val_loss: 46.5899 - val_MinusLogProbMetric: 46.5899 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 23/1000
2023-10-30 17:18:49.214 
Epoch 23/1000 
	 loss: 46.1578, MinusLogProbMetric: 46.1578, val_loss: 46.3201, val_MinusLogProbMetric: 46.3201

Epoch 23: val_loss improved from 46.58990 to 46.32013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 46.1578 - MinusLogProbMetric: 46.1578 - val_loss: 46.3201 - val_MinusLogProbMetric: 46.3201 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 24/1000
2023-10-30 17:19:53.966 
Epoch 24/1000 
	 loss: 46.0641, MinusLogProbMetric: 46.0641, val_loss: 46.1186, val_MinusLogProbMetric: 46.1186

Epoch 24: val_loss improved from 46.32013 to 46.11855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 46.0641 - MinusLogProbMetric: 46.0641 - val_loss: 46.1186 - val_MinusLogProbMetric: 46.1186 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 25/1000
2023-10-30 17:20:56.423 
Epoch 25/1000 
	 loss: 45.7988, MinusLogProbMetric: 45.7988, val_loss: 46.1851, val_MinusLogProbMetric: 46.1851

Epoch 25: val_loss did not improve from 46.11855
196/196 - 61s - loss: 45.7988 - MinusLogProbMetric: 45.7988 - val_loss: 46.1851 - val_MinusLogProbMetric: 46.1851 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 26/1000
2023-10-30 17:21:55.630 
Epoch 26/1000 
	 loss: 45.8281, MinusLogProbMetric: 45.8281, val_loss: 45.7576, val_MinusLogProbMetric: 45.7576

Epoch 26: val_loss improved from 46.11855 to 45.75760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 45.8281 - MinusLogProbMetric: 45.8281 - val_loss: 45.7576 - val_MinusLogProbMetric: 45.7576 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 27/1000
2023-10-30 17:22:45.997 
Epoch 27/1000 
	 loss: 45.5626, MinusLogProbMetric: 45.5626, val_loss: 45.5691, val_MinusLogProbMetric: 45.5691

Epoch 27: val_loss improved from 45.75760 to 45.56913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 45.5626 - MinusLogProbMetric: 45.5626 - val_loss: 45.5691 - val_MinusLogProbMetric: 45.5691 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 28/1000
2023-10-30 17:23:36.475 
Epoch 28/1000 
	 loss: 45.4111, MinusLogProbMetric: 45.4111, val_loss: 45.4244, val_MinusLogProbMetric: 45.4244

Epoch 28: val_loss improved from 45.56913 to 45.42438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 45.4111 - MinusLogProbMetric: 45.4111 - val_loss: 45.4244 - val_MinusLogProbMetric: 45.4244 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 29/1000
2023-10-30 17:24:36.066 
Epoch 29/1000 
	 loss: 45.6681, MinusLogProbMetric: 45.6681, val_loss: 45.5365, val_MinusLogProbMetric: 45.5365

Epoch 29: val_loss did not improve from 45.42438
196/196 - 59s - loss: 45.6681 - MinusLogProbMetric: 45.6681 - val_loss: 45.5365 - val_MinusLogProbMetric: 45.5365 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 30/1000
2023-10-30 17:25:36.732 
Epoch 30/1000 
	 loss: 45.1986, MinusLogProbMetric: 45.1986, val_loss: 45.3839, val_MinusLogProbMetric: 45.3839

Epoch 30: val_loss improved from 45.42438 to 45.38390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 45.1986 - MinusLogProbMetric: 45.1986 - val_loss: 45.3839 - val_MinusLogProbMetric: 45.3839 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 31/1000
2023-10-30 17:26:39.201 
Epoch 31/1000 
	 loss: 45.0781, MinusLogProbMetric: 45.0781, val_loss: 45.4616, val_MinusLogProbMetric: 45.4616

Epoch 31: val_loss did not improve from 45.38390
196/196 - 61s - loss: 45.0781 - MinusLogProbMetric: 45.0781 - val_loss: 45.4616 - val_MinusLogProbMetric: 45.4616 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 32/1000
2023-10-30 17:27:42.294 
Epoch 32/1000 
	 loss: 45.1047, MinusLogProbMetric: 45.1047, val_loss: 45.3845, val_MinusLogProbMetric: 45.3845

Epoch 32: val_loss did not improve from 45.38390
196/196 - 63s - loss: 45.1047 - MinusLogProbMetric: 45.1047 - val_loss: 45.3845 - val_MinusLogProbMetric: 45.3845 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 33/1000
2023-10-30 17:28:43.337 
Epoch 33/1000 
	 loss: 45.1463, MinusLogProbMetric: 45.1463, val_loss: 44.8669, val_MinusLogProbMetric: 44.8669

Epoch 33: val_loss improved from 45.38390 to 44.86686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 45.1463 - MinusLogProbMetric: 45.1463 - val_loss: 44.8669 - val_MinusLogProbMetric: 44.8669 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 34/1000
2023-10-30 17:29:47.828 
Epoch 34/1000 
	 loss: 44.7933, MinusLogProbMetric: 44.7933, val_loss: 44.7625, val_MinusLogProbMetric: 44.7625

Epoch 34: val_loss improved from 44.86686 to 44.76247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 44.7933 - MinusLogProbMetric: 44.7933 - val_loss: 44.7625 - val_MinusLogProbMetric: 44.7625 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 35/1000
2023-10-30 17:30:50.404 
Epoch 35/1000 
	 loss: 44.6328, MinusLogProbMetric: 44.6328, val_loss: 45.0094, val_MinusLogProbMetric: 45.0094

Epoch 35: val_loss did not improve from 44.76247
196/196 - 62s - loss: 44.6328 - MinusLogProbMetric: 44.6328 - val_loss: 45.0094 - val_MinusLogProbMetric: 45.0094 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 36/1000
2023-10-30 17:31:51.998 
Epoch 36/1000 
	 loss: 44.7202, MinusLogProbMetric: 44.7202, val_loss: 44.6320, val_MinusLogProbMetric: 44.6320

Epoch 36: val_loss improved from 44.76247 to 44.63198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 44.7202 - MinusLogProbMetric: 44.7202 - val_loss: 44.6320 - val_MinusLogProbMetric: 44.6320 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 37/1000
2023-10-30 17:32:55.412 
Epoch 37/1000 
	 loss: 47.7623, MinusLogProbMetric: 47.7623, val_loss: 46.9578, val_MinusLogProbMetric: 46.9578

Epoch 37: val_loss did not improve from 44.63198
196/196 - 63s - loss: 47.7623 - MinusLogProbMetric: 47.7623 - val_loss: 46.9578 - val_MinusLogProbMetric: 46.9578 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 38/1000
2023-10-30 17:33:56.198 
Epoch 38/1000 
	 loss: 45.7325, MinusLogProbMetric: 45.7325, val_loss: 45.4248, val_MinusLogProbMetric: 45.4248

Epoch 38: val_loss did not improve from 44.63198
196/196 - 61s - loss: 45.7325 - MinusLogProbMetric: 45.7325 - val_loss: 45.4248 - val_MinusLogProbMetric: 45.4248 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 39/1000
2023-10-30 17:34:49.512 
Epoch 39/1000 
	 loss: 44.6921, MinusLogProbMetric: 44.6921, val_loss: 44.8596, val_MinusLogProbMetric: 44.8596

Epoch 39: val_loss did not improve from 44.63198
196/196 - 53s - loss: 44.6921 - MinusLogProbMetric: 44.6921 - val_loss: 44.8596 - val_MinusLogProbMetric: 44.8596 - lr: 4.1152e-06 - 53s/epoch - 272ms/step
Epoch 40/1000
2023-10-30 17:35:43.559 
Epoch 40/1000 
	 loss: 44.3271, MinusLogProbMetric: 44.3271, val_loss: 44.6394, val_MinusLogProbMetric: 44.6394

Epoch 40: val_loss did not improve from 44.63198
196/196 - 54s - loss: 44.3271 - MinusLogProbMetric: 44.3271 - val_loss: 44.6394 - val_MinusLogProbMetric: 44.6394 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 41/1000
2023-10-30 17:36:45.511 
Epoch 41/1000 
	 loss: 44.1542, MinusLogProbMetric: 44.1542, val_loss: 44.4257, val_MinusLogProbMetric: 44.4257

Epoch 41: val_loss improved from 44.63198 to 44.42573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 44.1542 - MinusLogProbMetric: 44.1542 - val_loss: 44.4257 - val_MinusLogProbMetric: 44.4257 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 42/1000
2023-10-30 17:37:49.063 
Epoch 42/1000 
	 loss: 44.0576, MinusLogProbMetric: 44.0576, val_loss: 44.1821, val_MinusLogProbMetric: 44.1821

Epoch 42: val_loss improved from 44.42573 to 44.18212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 44.0576 - MinusLogProbMetric: 44.0576 - val_loss: 44.1821 - val_MinusLogProbMetric: 44.1821 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 43/1000
2023-10-30 17:38:54.294 
Epoch 43/1000 
	 loss: 44.0392, MinusLogProbMetric: 44.0392, val_loss: 44.2031, val_MinusLogProbMetric: 44.2031

Epoch 43: val_loss did not improve from 44.18212
196/196 - 64s - loss: 44.0392 - MinusLogProbMetric: 44.0392 - val_loss: 44.2031 - val_MinusLogProbMetric: 44.2031 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 44/1000
2023-10-30 17:39:58.467 
Epoch 44/1000 
	 loss: 43.7982, MinusLogProbMetric: 43.7982, val_loss: 44.1178, val_MinusLogProbMetric: 44.1178

Epoch 44: val_loss improved from 44.18212 to 44.11777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 43.7982 - MinusLogProbMetric: 43.7982 - val_loss: 44.1178 - val_MinusLogProbMetric: 44.1178 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 45/1000
2023-10-30 17:41:03.418 
Epoch 45/1000 
	 loss: 43.7091, MinusLogProbMetric: 43.7091, val_loss: 44.2015, val_MinusLogProbMetric: 44.2015

Epoch 45: val_loss did not improve from 44.11777
196/196 - 64s - loss: 43.7091 - MinusLogProbMetric: 43.7091 - val_loss: 44.2015 - val_MinusLogProbMetric: 44.2015 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 46/1000
2023-10-30 17:42:07.383 
Epoch 46/1000 
	 loss: 43.6831, MinusLogProbMetric: 43.6831, val_loss: 43.8761, val_MinusLogProbMetric: 43.8761

Epoch 46: val_loss improved from 44.11777 to 43.87609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 43.6831 - MinusLogProbMetric: 43.6831 - val_loss: 43.8761 - val_MinusLogProbMetric: 43.8761 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 47/1000
2023-10-30 17:43:12.054 
Epoch 47/1000 
	 loss: 43.5030, MinusLogProbMetric: 43.5030, val_loss: 43.5403, val_MinusLogProbMetric: 43.5403

Epoch 47: val_loss improved from 43.87609 to 43.54033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 43.5030 - MinusLogProbMetric: 43.5030 - val_loss: 43.5403 - val_MinusLogProbMetric: 43.5403 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 48/1000
2023-10-30 17:44:09.046 
Epoch 48/1000 
	 loss: 43.3810, MinusLogProbMetric: 43.3810, val_loss: 43.4547, val_MinusLogProbMetric: 43.4547

Epoch 48: val_loss improved from 43.54033 to 43.45473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 43.3810 - MinusLogProbMetric: 43.3810 - val_loss: 43.4547 - val_MinusLogProbMetric: 43.4547 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 49/1000
2023-10-30 17:45:12.053 
Epoch 49/1000 
	 loss: 44.2115, MinusLogProbMetric: 44.2115, val_loss: 43.8158, val_MinusLogProbMetric: 43.8158

Epoch 49: val_loss did not improve from 43.45473
196/196 - 62s - loss: 44.2115 - MinusLogProbMetric: 44.2115 - val_loss: 43.8158 - val_MinusLogProbMetric: 43.8158 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 50/1000
2023-10-30 17:46:17.158 
Epoch 50/1000 
	 loss: 43.3144, MinusLogProbMetric: 43.3144, val_loss: 43.3608, val_MinusLogProbMetric: 43.3608

Epoch 50: val_loss improved from 43.45473 to 43.36084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 43.3144 - MinusLogProbMetric: 43.3144 - val_loss: 43.3608 - val_MinusLogProbMetric: 43.3608 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 51/1000
2023-10-30 17:47:21.631 
Epoch 51/1000 
	 loss: 43.2257, MinusLogProbMetric: 43.2257, val_loss: 43.9576, val_MinusLogProbMetric: 43.9576

Epoch 51: val_loss did not improve from 43.36084
196/196 - 64s - loss: 43.2257 - MinusLogProbMetric: 43.2257 - val_loss: 43.9576 - val_MinusLogProbMetric: 43.9576 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 52/1000
2023-10-30 17:48:24.515 
Epoch 52/1000 
	 loss: 43.1075, MinusLogProbMetric: 43.1075, val_loss: 43.2596, val_MinusLogProbMetric: 43.2596

Epoch 52: val_loss improved from 43.36084 to 43.25960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 43.1075 - MinusLogProbMetric: 43.1075 - val_loss: 43.2596 - val_MinusLogProbMetric: 43.2596 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 53/1000
2023-10-30 17:49:26.893 
Epoch 53/1000 
	 loss: 42.9917, MinusLogProbMetric: 42.9917, val_loss: 43.4392, val_MinusLogProbMetric: 43.4392

Epoch 53: val_loss did not improve from 43.25960
196/196 - 61s - loss: 42.9917 - MinusLogProbMetric: 42.9917 - val_loss: 43.4392 - val_MinusLogProbMetric: 43.4392 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 54/1000
2023-10-30 17:50:30.215 
Epoch 54/1000 
	 loss: 42.9285, MinusLogProbMetric: 42.9285, val_loss: 43.0243, val_MinusLogProbMetric: 43.0243

Epoch 54: val_loss improved from 43.25960 to 43.02431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 42.9285 - MinusLogProbMetric: 42.9285 - val_loss: 43.0243 - val_MinusLogProbMetric: 43.0243 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 55/1000
2023-10-30 17:51:26.445 
Epoch 55/1000 
	 loss: 42.7850, MinusLogProbMetric: 42.7850, val_loss: 43.0352, val_MinusLogProbMetric: 43.0352

Epoch 55: val_loss did not improve from 43.02431
196/196 - 55s - loss: 42.7850 - MinusLogProbMetric: 42.7850 - val_loss: 43.0352 - val_MinusLogProbMetric: 43.0352 - lr: 4.1152e-06 - 55s/epoch - 283ms/step
Epoch 56/1000
2023-10-30 17:52:30.442 
Epoch 56/1000 
	 loss: 42.7154, MinusLogProbMetric: 42.7154, val_loss: 43.2182, val_MinusLogProbMetric: 43.2182

Epoch 56: val_loss did not improve from 43.02431
196/196 - 64s - loss: 42.7154 - MinusLogProbMetric: 42.7154 - val_loss: 43.2182 - val_MinusLogProbMetric: 43.2182 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 57/1000
2023-10-30 17:53:32.509 
Epoch 57/1000 
	 loss: 44.2641, MinusLogProbMetric: 44.2641, val_loss: 44.6153, val_MinusLogProbMetric: 44.6153

Epoch 57: val_loss did not improve from 43.02431
196/196 - 62s - loss: 44.2641 - MinusLogProbMetric: 44.2641 - val_loss: 44.6153 - val_MinusLogProbMetric: 44.6153 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 58/1000
2023-10-30 17:54:36.969 
Epoch 58/1000 
	 loss: 43.3513, MinusLogProbMetric: 43.3513, val_loss: 43.1638, val_MinusLogProbMetric: 43.1638

Epoch 58: val_loss did not improve from 43.02431
196/196 - 64s - loss: 43.3513 - MinusLogProbMetric: 43.3513 - val_loss: 43.1638 - val_MinusLogProbMetric: 43.1638 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 59/1000
2023-10-30 17:55:41.078 
Epoch 59/1000 
	 loss: 42.7459, MinusLogProbMetric: 42.7459, val_loss: 42.7484, val_MinusLogProbMetric: 42.7484

Epoch 59: val_loss improved from 43.02431 to 42.74838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 42.7459 - MinusLogProbMetric: 42.7459 - val_loss: 42.7484 - val_MinusLogProbMetric: 42.7484 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 60/1000
2023-10-30 17:56:44.974 
Epoch 60/1000 
	 loss: 42.5465, MinusLogProbMetric: 42.5465, val_loss: 42.8143, val_MinusLogProbMetric: 42.8143

Epoch 60: val_loss did not improve from 42.74838
196/196 - 63s - loss: 42.5465 - MinusLogProbMetric: 42.5465 - val_loss: 42.8143 - val_MinusLogProbMetric: 42.8143 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 61/1000
2023-10-30 17:57:47.117 
Epoch 61/1000 
	 loss: 42.4671, MinusLogProbMetric: 42.4671, val_loss: 42.6929, val_MinusLogProbMetric: 42.6929

Epoch 61: val_loss improved from 42.74838 to 42.69287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 42.4671 - MinusLogProbMetric: 42.4671 - val_loss: 42.6929 - val_MinusLogProbMetric: 42.6929 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 62/1000
2023-10-30 17:58:49.722 
Epoch 62/1000 
	 loss: 42.6086, MinusLogProbMetric: 42.6086, val_loss: 43.3412, val_MinusLogProbMetric: 43.3412

Epoch 62: val_loss did not improve from 42.69287
196/196 - 62s - loss: 42.6086 - MinusLogProbMetric: 42.6086 - val_loss: 43.3412 - val_MinusLogProbMetric: 43.3412 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 63/1000
2023-10-30 17:59:51.572 
Epoch 63/1000 
	 loss: 42.3759, MinusLogProbMetric: 42.3759, val_loss: 42.6433, val_MinusLogProbMetric: 42.6433

Epoch 63: val_loss improved from 42.69287 to 42.64328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 42.3759 - MinusLogProbMetric: 42.3759 - val_loss: 42.6433 - val_MinusLogProbMetric: 42.6433 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 64/1000
2023-10-30 18:00:55.831 
Epoch 64/1000 
	 loss: 42.2703, MinusLogProbMetric: 42.2703, val_loss: 42.5216, val_MinusLogProbMetric: 42.5216

Epoch 64: val_loss improved from 42.64328 to 42.52163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 42.2703 - MinusLogProbMetric: 42.2703 - val_loss: 42.5216 - val_MinusLogProbMetric: 42.5216 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 65/1000
2023-10-30 18:01:59.223 
Epoch 65/1000 
	 loss: 42.1518, MinusLogProbMetric: 42.1518, val_loss: 42.4170, val_MinusLogProbMetric: 42.4170

Epoch 65: val_loss improved from 42.52163 to 42.41697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 42.1518 - MinusLogProbMetric: 42.1518 - val_loss: 42.4170 - val_MinusLogProbMetric: 42.4170 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 66/1000
2023-10-30 18:03:03.829 
Epoch 66/1000 
	 loss: 42.0748, MinusLogProbMetric: 42.0748, val_loss: 42.2015, val_MinusLogProbMetric: 42.2015

Epoch 66: val_loss improved from 42.41697 to 42.20152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 42.0748 - MinusLogProbMetric: 42.0748 - val_loss: 42.2015 - val_MinusLogProbMetric: 42.2015 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 67/1000
2023-10-30 18:04:09.039 
Epoch 67/1000 
	 loss: 42.0097, MinusLogProbMetric: 42.0097, val_loss: 42.3300, val_MinusLogProbMetric: 42.3300

Epoch 67: val_loss did not improve from 42.20152
196/196 - 64s - loss: 42.0097 - MinusLogProbMetric: 42.0097 - val_loss: 42.3300 - val_MinusLogProbMetric: 42.3300 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 68/1000
2023-10-30 18:05:09.357 
Epoch 68/1000 
	 loss: 42.0330, MinusLogProbMetric: 42.0330, val_loss: 42.2603, val_MinusLogProbMetric: 42.2603

Epoch 68: val_loss did not improve from 42.20152
196/196 - 60s - loss: 42.0330 - MinusLogProbMetric: 42.0330 - val_loss: 42.2603 - val_MinusLogProbMetric: 42.2603 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 69/1000
2023-10-30 18:06:12.555 
Epoch 69/1000 
	 loss: 41.8961, MinusLogProbMetric: 41.8961, val_loss: 42.2656, val_MinusLogProbMetric: 42.2656

Epoch 69: val_loss did not improve from 42.20152
196/196 - 63s - loss: 41.8961 - MinusLogProbMetric: 41.8961 - val_loss: 42.2656 - val_MinusLogProbMetric: 42.2656 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 70/1000
2023-10-30 18:07:13.412 
Epoch 70/1000 
	 loss: 41.8662, MinusLogProbMetric: 41.8662, val_loss: 42.3689, val_MinusLogProbMetric: 42.3689

Epoch 70: val_loss did not improve from 42.20152
196/196 - 61s - loss: 41.8662 - MinusLogProbMetric: 41.8662 - val_loss: 42.3689 - val_MinusLogProbMetric: 42.3689 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 71/1000
2023-10-30 18:08:16.377 
Epoch 71/1000 
	 loss: 41.7517, MinusLogProbMetric: 41.7517, val_loss: 42.1037, val_MinusLogProbMetric: 42.1037

Epoch 71: val_loss improved from 42.20152 to 42.10371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 41.7517 - MinusLogProbMetric: 41.7517 - val_loss: 42.1037 - val_MinusLogProbMetric: 42.1037 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 72/1000
2023-10-30 18:09:20.140 
Epoch 72/1000 
	 loss: 41.7697, MinusLogProbMetric: 41.7697, val_loss: 41.9707, val_MinusLogProbMetric: 41.9707

Epoch 72: val_loss improved from 42.10371 to 41.97069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 41.7697 - MinusLogProbMetric: 41.7697 - val_loss: 41.9707 - val_MinusLogProbMetric: 41.9707 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 73/1000
2023-10-30 18:10:24.481 
Epoch 73/1000 
	 loss: 41.5734, MinusLogProbMetric: 41.5734, val_loss: 41.8009, val_MinusLogProbMetric: 41.8009

Epoch 73: val_loss improved from 41.97069 to 41.80086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 41.5734 - MinusLogProbMetric: 41.5734 - val_loss: 41.8009 - val_MinusLogProbMetric: 41.8009 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 74/1000
2023-10-30 18:11:28.019 
Epoch 74/1000 
	 loss: 45.6274, MinusLogProbMetric: 45.6274, val_loss: 49.5088, val_MinusLogProbMetric: 49.5088

Epoch 74: val_loss did not improve from 41.80086
196/196 - 63s - loss: 45.6274 - MinusLogProbMetric: 45.6274 - val_loss: 49.5088 - val_MinusLogProbMetric: 49.5088 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 75/1000
2023-10-30 18:12:31.636 
Epoch 75/1000 
	 loss: 45.3298, MinusLogProbMetric: 45.3298, val_loss: 44.4543, val_MinusLogProbMetric: 44.4543

Epoch 75: val_loss did not improve from 41.80086
196/196 - 64s - loss: 45.3298 - MinusLogProbMetric: 45.3298 - val_loss: 44.4543 - val_MinusLogProbMetric: 44.4543 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 76/1000
2023-10-30 18:13:35.163 
Epoch 76/1000 
	 loss: 44.0407, MinusLogProbMetric: 44.0407, val_loss: 44.5013, val_MinusLogProbMetric: 44.5013

Epoch 76: val_loss did not improve from 41.80086
196/196 - 64s - loss: 44.0407 - MinusLogProbMetric: 44.0407 - val_loss: 44.5013 - val_MinusLogProbMetric: 44.5013 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 77/1000
2023-10-30 18:14:40.385 
Epoch 77/1000 
	 loss: 43.9220, MinusLogProbMetric: 43.9220, val_loss: 43.9153, val_MinusLogProbMetric: 43.9153

Epoch 77: val_loss did not improve from 41.80086
196/196 - 65s - loss: 43.9220 - MinusLogProbMetric: 43.9220 - val_loss: 43.9153 - val_MinusLogProbMetric: 43.9153 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 78/1000
2023-10-30 18:15:42.569 
Epoch 78/1000 
	 loss: 43.3923, MinusLogProbMetric: 43.3923, val_loss: 43.8512, val_MinusLogProbMetric: 43.8512

Epoch 78: val_loss did not improve from 41.80086
196/196 - 62s - loss: 43.3923 - MinusLogProbMetric: 43.3923 - val_loss: 43.8512 - val_MinusLogProbMetric: 43.8512 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 79/1000
2023-10-30 18:16:48.542 
Epoch 79/1000 
	 loss: 43.2495, MinusLogProbMetric: 43.2495, val_loss: 43.1220, val_MinusLogProbMetric: 43.1220

Epoch 79: val_loss did not improve from 41.80086
196/196 - 66s - loss: 43.2495 - MinusLogProbMetric: 43.2495 - val_loss: 43.1220 - val_MinusLogProbMetric: 43.1220 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 80/1000
2023-10-30 18:17:52.623 
Epoch 80/1000 
	 loss: 42.7003, MinusLogProbMetric: 42.7003, val_loss: 43.1448, val_MinusLogProbMetric: 43.1448

Epoch 80: val_loss did not improve from 41.80086
196/196 - 64s - loss: 42.7003 - MinusLogProbMetric: 42.7003 - val_loss: 43.1448 - val_MinusLogProbMetric: 43.1448 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 81/1000
2023-10-30 18:18:56.401 
Epoch 81/1000 
	 loss: 42.3369, MinusLogProbMetric: 42.3369, val_loss: 42.4406, val_MinusLogProbMetric: 42.4406

Epoch 81: val_loss did not improve from 41.80086
196/196 - 64s - loss: 42.3369 - MinusLogProbMetric: 42.3369 - val_loss: 42.4406 - val_MinusLogProbMetric: 42.4406 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 82/1000
2023-10-30 18:20:00.147 
Epoch 82/1000 
	 loss: 42.1161, MinusLogProbMetric: 42.1161, val_loss: 42.7107, val_MinusLogProbMetric: 42.7107

Epoch 82: val_loss did not improve from 41.80086
196/196 - 64s - loss: 42.1161 - MinusLogProbMetric: 42.1161 - val_loss: 42.7107 - val_MinusLogProbMetric: 42.7107 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 83/1000
2023-10-30 18:21:01.316 
Epoch 83/1000 
	 loss: 42.0692, MinusLogProbMetric: 42.0692, val_loss: 42.3156, val_MinusLogProbMetric: 42.3156

Epoch 83: val_loss did not improve from 41.80086
196/196 - 61s - loss: 42.0692 - MinusLogProbMetric: 42.0692 - val_loss: 42.3156 - val_MinusLogProbMetric: 42.3156 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 84/1000
2023-10-30 18:22:06.724 
Epoch 84/1000 
	 loss: 41.8521, MinusLogProbMetric: 41.8521, val_loss: 42.0136, val_MinusLogProbMetric: 42.0136

Epoch 84: val_loss did not improve from 41.80086
196/196 - 65s - loss: 41.8521 - MinusLogProbMetric: 41.8521 - val_loss: 42.0136 - val_MinusLogProbMetric: 42.0136 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 85/1000
2023-10-30 18:23:09.552 
Epoch 85/1000 
	 loss: 41.6508, MinusLogProbMetric: 41.6508, val_loss: 42.2762, val_MinusLogProbMetric: 42.2762

Epoch 85: val_loss did not improve from 41.80086
196/196 - 63s - loss: 41.6508 - MinusLogProbMetric: 41.6508 - val_loss: 42.2762 - val_MinusLogProbMetric: 42.2762 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 86/1000
2023-10-30 18:24:14.237 
Epoch 86/1000 
	 loss: 41.6135, MinusLogProbMetric: 41.6135, val_loss: 41.8008, val_MinusLogProbMetric: 41.8008

Epoch 86: val_loss improved from 41.80086 to 41.80081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 41.6135 - MinusLogProbMetric: 41.6135 - val_loss: 41.8008 - val_MinusLogProbMetric: 41.8008 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 87/1000
2023-10-30 18:25:20.001 
Epoch 87/1000 
	 loss: 41.5544, MinusLogProbMetric: 41.5544, val_loss: 42.3537, val_MinusLogProbMetric: 42.3537

Epoch 87: val_loss did not improve from 41.80081
196/196 - 65s - loss: 41.5544 - MinusLogProbMetric: 41.5544 - val_loss: 42.3537 - val_MinusLogProbMetric: 42.3537 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 88/1000
2023-10-30 18:26:24.132 
Epoch 88/1000 
	 loss: 41.4661, MinusLogProbMetric: 41.4661, val_loss: 41.4301, val_MinusLogProbMetric: 41.4301

Epoch 88: val_loss improved from 41.80081 to 41.43009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 41.4661 - MinusLogProbMetric: 41.4661 - val_loss: 41.4301 - val_MinusLogProbMetric: 41.4301 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 89/1000
2023-10-30 18:27:28.382 
Epoch 89/1000 
	 loss: 41.3701, MinusLogProbMetric: 41.3701, val_loss: 44.7073, val_MinusLogProbMetric: 44.7073

Epoch 89: val_loss did not improve from 41.43009
196/196 - 63s - loss: 41.3701 - MinusLogProbMetric: 41.3701 - val_loss: 44.7073 - val_MinusLogProbMetric: 44.7073 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 90/1000
2023-10-30 18:28:32.193 
Epoch 90/1000 
	 loss: 41.5993, MinusLogProbMetric: 41.5993, val_loss: 41.6378, val_MinusLogProbMetric: 41.6378

Epoch 90: val_loss did not improve from 41.43009
196/196 - 64s - loss: 41.5993 - MinusLogProbMetric: 41.5993 - val_loss: 41.6378 - val_MinusLogProbMetric: 41.6378 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 91/1000
2023-10-30 18:29:34.718 
Epoch 91/1000 
	 loss: 41.2041, MinusLogProbMetric: 41.2041, val_loss: 41.4564, val_MinusLogProbMetric: 41.4564

Epoch 91: val_loss did not improve from 41.43009
196/196 - 63s - loss: 41.2041 - MinusLogProbMetric: 41.2041 - val_loss: 41.4564 - val_MinusLogProbMetric: 41.4564 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 92/1000
2023-10-30 18:30:35.230 
Epoch 92/1000 
	 loss: 41.0904, MinusLogProbMetric: 41.0904, val_loss: 41.5606, val_MinusLogProbMetric: 41.5606

Epoch 92: val_loss did not improve from 41.43009
196/196 - 61s - loss: 41.0904 - MinusLogProbMetric: 41.0904 - val_loss: 41.5606 - val_MinusLogProbMetric: 41.5606 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 93/1000
2023-10-30 18:31:36.543 
Epoch 93/1000 
	 loss: 41.0207, MinusLogProbMetric: 41.0207, val_loss: 41.2617, val_MinusLogProbMetric: 41.2617

Epoch 93: val_loss improved from 41.43009 to 41.26169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 41.0207 - MinusLogProbMetric: 41.0207 - val_loss: 41.2617 - val_MinusLogProbMetric: 41.2617 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 94/1000
2023-10-30 18:32:30.310 
Epoch 94/1000 
	 loss: 40.9210, MinusLogProbMetric: 40.9210, val_loss: 41.0562, val_MinusLogProbMetric: 41.0562

Epoch 94: val_loss improved from 41.26169 to 41.05619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 40.9210 - MinusLogProbMetric: 40.9210 - val_loss: 41.0562 - val_MinusLogProbMetric: 41.0562 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 95/1000
2023-10-30 18:33:21.980 
Epoch 95/1000 
	 loss: 40.8825, MinusLogProbMetric: 40.8825, val_loss: 41.0820, val_MinusLogProbMetric: 41.0820

Epoch 95: val_loss did not improve from 41.05619
196/196 - 51s - loss: 40.8825 - MinusLogProbMetric: 40.8825 - val_loss: 41.0820 - val_MinusLogProbMetric: 41.0820 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 96/1000
2023-10-30 18:34:13.147 
Epoch 96/1000 
	 loss: 40.8344, MinusLogProbMetric: 40.8344, val_loss: 41.4878, val_MinusLogProbMetric: 41.4878

Epoch 96: val_loss did not improve from 41.05619
196/196 - 51s - loss: 40.8344 - MinusLogProbMetric: 40.8344 - val_loss: 41.4878 - val_MinusLogProbMetric: 41.4878 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 97/1000
2023-10-30 18:35:06.966 
Epoch 97/1000 
	 loss: 40.7750, MinusLogProbMetric: 40.7750, val_loss: 41.6113, val_MinusLogProbMetric: 41.6113

Epoch 97: val_loss did not improve from 41.05619
196/196 - 54s - loss: 40.7750 - MinusLogProbMetric: 40.7750 - val_loss: 41.6113 - val_MinusLogProbMetric: 41.6113 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 98/1000
2023-10-30 18:36:02.164 
Epoch 98/1000 
	 loss: 40.7890, MinusLogProbMetric: 40.7890, val_loss: 41.0140, val_MinusLogProbMetric: 41.0140

Epoch 98: val_loss improved from 41.05619 to 41.01400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 40.7890 - MinusLogProbMetric: 40.7890 - val_loss: 41.0140 - val_MinusLogProbMetric: 41.0140 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 99/1000
2023-10-30 18:36:57.762 
Epoch 99/1000 
	 loss: 42.4125, MinusLogProbMetric: 42.4125, val_loss: 59.1224, val_MinusLogProbMetric: 59.1224

Epoch 99: val_loss did not improve from 41.01400
196/196 - 55s - loss: 42.4125 - MinusLogProbMetric: 42.4125 - val_loss: 59.1224 - val_MinusLogProbMetric: 59.1224 - lr: 4.1152e-06 - 55s/epoch - 279ms/step
Epoch 100/1000
2023-10-30 18:37:48.624 
Epoch 100/1000 
	 loss: 42.4584, MinusLogProbMetric: 42.4584, val_loss: 40.8027, val_MinusLogProbMetric: 40.8027

Epoch 100: val_loss improved from 41.01400 to 40.80269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 42.4584 - MinusLogProbMetric: 42.4584 - val_loss: 40.8027 - val_MinusLogProbMetric: 40.8027 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 101/1000
2023-10-30 18:38:40.609 
Epoch 101/1000 
	 loss: 40.5518, MinusLogProbMetric: 40.5518, val_loss: 40.6696, val_MinusLogProbMetric: 40.6696

Epoch 101: val_loss improved from 40.80269 to 40.66955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 40.5518 - MinusLogProbMetric: 40.5518 - val_loss: 40.6696 - val_MinusLogProbMetric: 40.6696 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 102/1000
2023-10-30 18:39:36.416 
Epoch 102/1000 
	 loss: 40.6457, MinusLogProbMetric: 40.6457, val_loss: 40.9579, val_MinusLogProbMetric: 40.9579

Epoch 102: val_loss did not improve from 40.66955
196/196 - 55s - loss: 40.6457 - MinusLogProbMetric: 40.6457 - val_loss: 40.9579 - val_MinusLogProbMetric: 40.9579 - lr: 4.1152e-06 - 55s/epoch - 279ms/step
Epoch 103/1000
2023-10-30 18:40:29.128 
Epoch 103/1000 
	 loss: 40.4646, MinusLogProbMetric: 40.4646, val_loss: 40.7333, val_MinusLogProbMetric: 40.7333

Epoch 103: val_loss did not improve from 40.66955
196/196 - 53s - loss: 40.4646 - MinusLogProbMetric: 40.4646 - val_loss: 40.7333 - val_MinusLogProbMetric: 40.7333 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 104/1000
2023-10-30 18:41:23.125 
Epoch 104/1000 
	 loss: 40.3651, MinusLogProbMetric: 40.3651, val_loss: 40.7364, val_MinusLogProbMetric: 40.7364

Epoch 104: val_loss did not improve from 40.66955
196/196 - 54s - loss: 40.3651 - MinusLogProbMetric: 40.3651 - val_loss: 40.7364 - val_MinusLogProbMetric: 40.7364 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 105/1000
2023-10-30 18:42:21.069 
Epoch 105/1000 
	 loss: 40.2690, MinusLogProbMetric: 40.2690, val_loss: 40.8972, val_MinusLogProbMetric: 40.8972

Epoch 105: val_loss did not improve from 40.66955
196/196 - 58s - loss: 40.2690 - MinusLogProbMetric: 40.2690 - val_loss: 40.8972 - val_MinusLogProbMetric: 40.8972 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 106/1000
2023-10-30 18:43:15.144 
Epoch 106/1000 
	 loss: 40.2879, MinusLogProbMetric: 40.2879, val_loss: 40.6983, val_MinusLogProbMetric: 40.6983

Epoch 106: val_loss did not improve from 40.66955
196/196 - 54s - loss: 40.2879 - MinusLogProbMetric: 40.2879 - val_loss: 40.6983 - val_MinusLogProbMetric: 40.6983 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 107/1000
2023-10-30 18:44:06.438 
Epoch 107/1000 
	 loss: 40.2089, MinusLogProbMetric: 40.2089, val_loss: 42.1275, val_MinusLogProbMetric: 42.1275

Epoch 107: val_loss did not improve from 40.66955
196/196 - 51s - loss: 40.2089 - MinusLogProbMetric: 40.2089 - val_loss: 42.1275 - val_MinusLogProbMetric: 42.1275 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 108/1000
2023-10-30 18:44:57.466 
Epoch 108/1000 
	 loss: 40.3942, MinusLogProbMetric: 40.3942, val_loss: 40.5800, val_MinusLogProbMetric: 40.5800

Epoch 108: val_loss improved from 40.66955 to 40.57999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 40.3942 - MinusLogProbMetric: 40.3942 - val_loss: 40.5800 - val_MinusLogProbMetric: 40.5800 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 109/1000
2023-10-30 18:45:49.128 
Epoch 109/1000 
	 loss: 40.2329, MinusLogProbMetric: 40.2329, val_loss: 40.4758, val_MinusLogProbMetric: 40.4758

Epoch 109: val_loss improved from 40.57999 to 40.47581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 40.2329 - MinusLogProbMetric: 40.2329 - val_loss: 40.4758 - val_MinusLogProbMetric: 40.4758 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 110/1000
2023-10-30 18:46:43.418 
Epoch 110/1000 
	 loss: 40.1104, MinusLogProbMetric: 40.1104, val_loss: 40.4888, val_MinusLogProbMetric: 40.4888

Epoch 110: val_loss did not improve from 40.47581
196/196 - 53s - loss: 40.1104 - MinusLogProbMetric: 40.1104 - val_loss: 40.4888 - val_MinusLogProbMetric: 40.4888 - lr: 4.1152e-06 - 53s/epoch - 273ms/step
Epoch 111/1000
2023-10-30 18:47:39.100 
Epoch 111/1000 
	 loss: 40.0785, MinusLogProbMetric: 40.0785, val_loss: 40.2808, val_MinusLogProbMetric: 40.2808

Epoch 111: val_loss improved from 40.47581 to 40.28082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 40.0785 - MinusLogProbMetric: 40.0785 - val_loss: 40.2808 - val_MinusLogProbMetric: 40.2808 - lr: 4.1152e-06 - 56s/epoch - 288ms/step
Epoch 112/1000
2023-10-30 18:48:34.024 
Epoch 112/1000 
	 loss: 39.9714, MinusLogProbMetric: 39.9714, val_loss: 40.4577, val_MinusLogProbMetric: 40.4577

Epoch 112: val_loss did not improve from 40.28082
196/196 - 54s - loss: 39.9714 - MinusLogProbMetric: 39.9714 - val_loss: 40.4577 - val_MinusLogProbMetric: 40.4577 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 113/1000
2023-10-30 18:49:27.866 
Epoch 113/1000 
	 loss: 39.9471, MinusLogProbMetric: 39.9471, val_loss: 40.3891, val_MinusLogProbMetric: 40.3891

Epoch 113: val_loss did not improve from 40.28082
196/196 - 54s - loss: 39.9471 - MinusLogProbMetric: 39.9471 - val_loss: 40.3891 - val_MinusLogProbMetric: 40.3891 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 114/1000
2023-10-30 18:50:25.659 
Epoch 114/1000 
	 loss: 39.9396, MinusLogProbMetric: 39.9396, val_loss: 40.1731, val_MinusLogProbMetric: 40.1731

Epoch 114: val_loss improved from 40.28082 to 40.17315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 39.9396 - MinusLogProbMetric: 39.9396 - val_loss: 40.1731 - val_MinusLogProbMetric: 40.1731 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 115/1000
2023-10-30 18:51:24.550 
Epoch 115/1000 
	 loss: 39.8497, MinusLogProbMetric: 39.8497, val_loss: 40.0308, val_MinusLogProbMetric: 40.0308

Epoch 115: val_loss improved from 40.17315 to 40.03076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 39.8497 - MinusLogProbMetric: 39.8497 - val_loss: 40.0308 - val_MinusLogProbMetric: 40.0308 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 116/1000
2023-10-30 18:52:25.721 
Epoch 116/1000 
	 loss: 39.8000, MinusLogProbMetric: 39.8000, val_loss: 40.1403, val_MinusLogProbMetric: 40.1403

Epoch 116: val_loss did not improve from 40.03076
196/196 - 60s - loss: 39.8000 - MinusLogProbMetric: 39.8000 - val_loss: 40.1403 - val_MinusLogProbMetric: 40.1403 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 117/1000
2023-10-30 18:53:16.812 
Epoch 117/1000 
	 loss: 39.8081, MinusLogProbMetric: 39.8081, val_loss: 40.1506, val_MinusLogProbMetric: 40.1506

Epoch 117: val_loss did not improve from 40.03076
196/196 - 51s - loss: 39.8081 - MinusLogProbMetric: 39.8081 - val_loss: 40.1506 - val_MinusLogProbMetric: 40.1506 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 118/1000
2023-10-30 18:54:07.321 
Epoch 118/1000 
	 loss: 39.7575, MinusLogProbMetric: 39.7575, val_loss: 40.2315, val_MinusLogProbMetric: 40.2315

Epoch 118: val_loss did not improve from 40.03076
196/196 - 51s - loss: 39.7575 - MinusLogProbMetric: 39.7575 - val_loss: 40.2315 - val_MinusLogProbMetric: 40.2315 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 119/1000
2023-10-30 18:55:00.007 
Epoch 119/1000 
	 loss: 39.6983, MinusLogProbMetric: 39.6983, val_loss: 40.2503, val_MinusLogProbMetric: 40.2503

Epoch 119: val_loss did not improve from 40.03076
196/196 - 53s - loss: 39.6983 - MinusLogProbMetric: 39.6983 - val_loss: 40.2503 - val_MinusLogProbMetric: 40.2503 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 120/1000
2023-10-30 18:55:51.263 
Epoch 120/1000 
	 loss: 39.6520, MinusLogProbMetric: 39.6520, val_loss: 39.9304, val_MinusLogProbMetric: 39.9304

Epoch 120: val_loss improved from 40.03076 to 39.93036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 39.6520 - MinusLogProbMetric: 39.6520 - val_loss: 39.9304 - val_MinusLogProbMetric: 39.9304 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 121/1000
2023-10-30 18:56:43.352 
Epoch 121/1000 
	 loss: 39.6821, MinusLogProbMetric: 39.6821, val_loss: 39.8889, val_MinusLogProbMetric: 39.8889

Epoch 121: val_loss improved from 39.93036 to 39.88895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 39.6821 - MinusLogProbMetric: 39.6821 - val_loss: 39.8889 - val_MinusLogProbMetric: 39.8889 - lr: 4.1152e-06 - 52s/epoch - 266ms/step
Epoch 122/1000
2023-10-30 18:57:34.886 
Epoch 122/1000 
	 loss: 39.6079, MinusLogProbMetric: 39.6079, val_loss: 40.1873, val_MinusLogProbMetric: 40.1873

Epoch 122: val_loss did not improve from 39.88895
196/196 - 51s - loss: 39.6079 - MinusLogProbMetric: 39.6079 - val_loss: 40.1873 - val_MinusLogProbMetric: 40.1873 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 123/1000
2023-10-30 18:58:27.491 
Epoch 123/1000 
	 loss: 39.5383, MinusLogProbMetric: 39.5383, val_loss: 40.0124, val_MinusLogProbMetric: 40.0124

Epoch 123: val_loss did not improve from 39.88895
196/196 - 53s - loss: 39.5383 - MinusLogProbMetric: 39.5383 - val_loss: 40.0124 - val_MinusLogProbMetric: 40.0124 - lr: 4.1152e-06 - 53s/epoch - 268ms/step
Epoch 124/1000
2023-10-30 18:59:20.112 
Epoch 124/1000 
	 loss: 39.5644, MinusLogProbMetric: 39.5644, val_loss: 40.0871, val_MinusLogProbMetric: 40.0871

Epoch 124: val_loss did not improve from 39.88895
196/196 - 53s - loss: 39.5644 - MinusLogProbMetric: 39.5644 - val_loss: 40.0871 - val_MinusLogProbMetric: 40.0871 - lr: 4.1152e-06 - 53s/epoch - 268ms/step
Epoch 125/1000
2023-10-30 19:00:14.772 
Epoch 125/1000 
	 loss: 39.5269, MinusLogProbMetric: 39.5269, val_loss: 39.8199, val_MinusLogProbMetric: 39.8199

Epoch 125: val_loss improved from 39.88895 to 39.81987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 39.5269 - MinusLogProbMetric: 39.5269 - val_loss: 39.8199 - val_MinusLogProbMetric: 39.8199 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 126/1000
2023-10-30 19:01:13.454 
Epoch 126/1000 
	 loss: 39.4501, MinusLogProbMetric: 39.4501, val_loss: 39.7850, val_MinusLogProbMetric: 39.7850

Epoch 126: val_loss improved from 39.81987 to 39.78497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 39.4501 - MinusLogProbMetric: 39.4501 - val_loss: 39.7850 - val_MinusLogProbMetric: 39.7850 - lr: 4.1152e-06 - 57s/epoch - 293ms/step
Epoch 127/1000
2023-10-30 19:02:15.270 
Epoch 127/1000 
	 loss: 39.4520, MinusLogProbMetric: 39.4520, val_loss: 40.2135, val_MinusLogProbMetric: 40.2135

Epoch 127: val_loss did not improve from 39.78497
196/196 - 61s - loss: 39.4520 - MinusLogProbMetric: 39.4520 - val_loss: 40.2135 - val_MinusLogProbMetric: 40.2135 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 128/1000
2023-10-30 19:03:07.656 
Epoch 128/1000 
	 loss: 39.4415, MinusLogProbMetric: 39.4415, val_loss: 39.9915, val_MinusLogProbMetric: 39.9915

Epoch 128: val_loss did not improve from 39.78497
196/196 - 52s - loss: 39.4415 - MinusLogProbMetric: 39.4415 - val_loss: 39.9915 - val_MinusLogProbMetric: 39.9915 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 129/1000
2023-10-30 19:03:58.501 
Epoch 129/1000 
	 loss: 39.3778, MinusLogProbMetric: 39.3778, val_loss: 39.5515, val_MinusLogProbMetric: 39.5515

Epoch 129: val_loss improved from 39.78497 to 39.55151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 39.3778 - MinusLogProbMetric: 39.3778 - val_loss: 39.5515 - val_MinusLogProbMetric: 39.5515 - lr: 4.1152e-06 - 52s/epoch - 263ms/step
Epoch 130/1000
2023-10-30 19:04:50.663 
Epoch 130/1000 
	 loss: 39.2850, MinusLogProbMetric: 39.2850, val_loss: 39.6823, val_MinusLogProbMetric: 39.6823

Epoch 130: val_loss did not improve from 39.55151
196/196 - 51s - loss: 39.2850 - MinusLogProbMetric: 39.2850 - val_loss: 39.6823 - val_MinusLogProbMetric: 39.6823 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 131/1000
2023-10-30 19:05:41.055 
Epoch 131/1000 
	 loss: 39.5358, MinusLogProbMetric: 39.5358, val_loss: 39.9854, val_MinusLogProbMetric: 39.9854

Epoch 131: val_loss did not improve from 39.55151
196/196 - 50s - loss: 39.5358 - MinusLogProbMetric: 39.5358 - val_loss: 39.9854 - val_MinusLogProbMetric: 39.9854 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 132/1000
2023-10-30 19:06:31.845 
Epoch 132/1000 
	 loss: 39.3371, MinusLogProbMetric: 39.3371, val_loss: 39.7857, val_MinusLogProbMetric: 39.7857

Epoch 132: val_loss did not improve from 39.55151
196/196 - 51s - loss: 39.3371 - MinusLogProbMetric: 39.3371 - val_loss: 39.7857 - val_MinusLogProbMetric: 39.7857 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 133/1000
2023-10-30 19:07:22.554 
Epoch 133/1000 
	 loss: 39.1380, MinusLogProbMetric: 39.1380, val_loss: 39.3910, val_MinusLogProbMetric: 39.3910

Epoch 133: val_loss improved from 39.55151 to 39.39101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 39.1380 - MinusLogProbMetric: 39.1380 - val_loss: 39.3910 - val_MinusLogProbMetric: 39.3910 - lr: 4.1152e-06 - 51s/epoch - 263ms/step
Epoch 134/1000
2023-10-30 19:08:14.461 
Epoch 134/1000 
	 loss: 39.2012, MinusLogProbMetric: 39.2012, val_loss: 39.9394, val_MinusLogProbMetric: 39.9394

Epoch 134: val_loss did not improve from 39.39101
196/196 - 51s - loss: 39.2012 - MinusLogProbMetric: 39.2012 - val_loss: 39.9394 - val_MinusLogProbMetric: 39.9394 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 135/1000
2023-10-30 19:09:06.446 
Epoch 135/1000 
	 loss: 39.1022, MinusLogProbMetric: 39.1022, val_loss: 39.2640, val_MinusLogProbMetric: 39.2640

Epoch 135: val_loss improved from 39.39101 to 39.26403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 39.1022 - MinusLogProbMetric: 39.1022 - val_loss: 39.2640 - val_MinusLogProbMetric: 39.2640 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 136/1000
2023-10-30 19:09:59.154 
Epoch 136/1000 
	 loss: 39.0668, MinusLogProbMetric: 39.0668, val_loss: 39.5653, val_MinusLogProbMetric: 39.5653

Epoch 136: val_loss did not improve from 39.26403
196/196 - 52s - loss: 39.0668 - MinusLogProbMetric: 39.0668 - val_loss: 39.5653 - val_MinusLogProbMetric: 39.5653 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 137/1000
2023-10-30 19:10:51.317 
Epoch 137/1000 
	 loss: 39.0915, MinusLogProbMetric: 39.0915, val_loss: 39.4337, val_MinusLogProbMetric: 39.4337

Epoch 137: val_loss did not improve from 39.26403
196/196 - 52s - loss: 39.0915 - MinusLogProbMetric: 39.0915 - val_loss: 39.4337 - val_MinusLogProbMetric: 39.4337 - lr: 4.1152e-06 - 52s/epoch - 266ms/step
Epoch 138/1000
2023-10-30 19:11:46.387 
Epoch 138/1000 
	 loss: 39.0031, MinusLogProbMetric: 39.0031, val_loss: 39.2622, val_MinusLogProbMetric: 39.2622

Epoch 138: val_loss improved from 39.26403 to 39.26215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 39.0031 - MinusLogProbMetric: 39.0031 - val_loss: 39.2622 - val_MinusLogProbMetric: 39.2622 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 139/1000
2023-10-30 19:12:39.988 
Epoch 139/1000 
	 loss: 39.0240, MinusLogProbMetric: 39.0240, val_loss: 39.7918, val_MinusLogProbMetric: 39.7918

Epoch 139: val_loss did not improve from 39.26215
196/196 - 52s - loss: 39.0240 - MinusLogProbMetric: 39.0240 - val_loss: 39.7918 - val_MinusLogProbMetric: 39.7918 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 140/1000
2023-10-30 19:13:32.260 
Epoch 140/1000 
	 loss: 39.0190, MinusLogProbMetric: 39.0190, val_loss: 39.2964, val_MinusLogProbMetric: 39.2964

Epoch 140: val_loss did not improve from 39.26215
196/196 - 52s - loss: 39.0190 - MinusLogProbMetric: 39.0190 - val_loss: 39.2964 - val_MinusLogProbMetric: 39.2964 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 141/1000
2023-10-30 19:14:22.677 
Epoch 141/1000 
	 loss: 39.0092, MinusLogProbMetric: 39.0092, val_loss: 39.2112, val_MinusLogProbMetric: 39.2112

Epoch 141: val_loss improved from 39.26215 to 39.21117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 39.0092 - MinusLogProbMetric: 39.0092 - val_loss: 39.2112 - val_MinusLogProbMetric: 39.2112 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 142/1000
2023-10-30 19:15:15.756 
Epoch 142/1000 
	 loss: 38.8736, MinusLogProbMetric: 38.8736, val_loss: 39.2902, val_MinusLogProbMetric: 39.2902

Epoch 142: val_loss did not improve from 39.21117
196/196 - 52s - loss: 38.8736 - MinusLogProbMetric: 38.8736 - val_loss: 39.2902 - val_MinusLogProbMetric: 39.2902 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 143/1000
2023-10-30 19:16:07.219 
Epoch 143/1000 
	 loss: 38.8525, MinusLogProbMetric: 38.8525, val_loss: 39.3389, val_MinusLogProbMetric: 39.3389

Epoch 143: val_loss did not improve from 39.21117
196/196 - 51s - loss: 38.8525 - MinusLogProbMetric: 38.8525 - val_loss: 39.3389 - val_MinusLogProbMetric: 39.3389 - lr: 4.1152e-06 - 51s/epoch - 263ms/step
Epoch 144/1000
2023-10-30 19:16:58.436 
Epoch 144/1000 
	 loss: 65.5324, MinusLogProbMetric: 65.5324, val_loss: 51.1494, val_MinusLogProbMetric: 51.1494

Epoch 144: val_loss did not improve from 39.21117
196/196 - 51s - loss: 65.5324 - MinusLogProbMetric: 65.5324 - val_loss: 51.1494 - val_MinusLogProbMetric: 51.1494 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 145/1000
2023-10-30 19:17:52.014 
Epoch 145/1000 
	 loss: 48.7377, MinusLogProbMetric: 48.7377, val_loss: 47.4618, val_MinusLogProbMetric: 47.4618

Epoch 145: val_loss did not improve from 39.21117
196/196 - 54s - loss: 48.7377 - MinusLogProbMetric: 48.7377 - val_loss: 47.4618 - val_MinusLogProbMetric: 47.4618 - lr: 4.1152e-06 - 54s/epoch - 273ms/step
Epoch 146/1000
2023-10-30 19:18:43.753 
Epoch 146/1000 
	 loss: 46.1419, MinusLogProbMetric: 46.1419, val_loss: 45.4836, val_MinusLogProbMetric: 45.4836

Epoch 146: val_loss did not improve from 39.21117
196/196 - 52s - loss: 46.1419 - MinusLogProbMetric: 46.1419 - val_loss: 45.4836 - val_MinusLogProbMetric: 45.4836 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 147/1000
2023-10-30 19:19:34.865 
Epoch 147/1000 
	 loss: 44.8185, MinusLogProbMetric: 44.8185, val_loss: 44.6016, val_MinusLogProbMetric: 44.6016

Epoch 147: val_loss did not improve from 39.21117
196/196 - 51s - loss: 44.8185 - MinusLogProbMetric: 44.8185 - val_loss: 44.6016 - val_MinusLogProbMetric: 44.6016 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 148/1000
2023-10-30 19:20:28.783 
Epoch 148/1000 
	 loss: 43.9990, MinusLogProbMetric: 43.9990, val_loss: 43.9309, val_MinusLogProbMetric: 43.9309

Epoch 148: val_loss did not improve from 39.21117
196/196 - 54s - loss: 43.9990 - MinusLogProbMetric: 43.9990 - val_loss: 43.9309 - val_MinusLogProbMetric: 43.9309 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 149/1000
2023-10-30 19:21:19.393 
Epoch 149/1000 
	 loss: 42.6105, MinusLogProbMetric: 42.6105, val_loss: 42.4609, val_MinusLogProbMetric: 42.4609

Epoch 149: val_loss did not improve from 39.21117
196/196 - 51s - loss: 42.6105 - MinusLogProbMetric: 42.6105 - val_loss: 42.4609 - val_MinusLogProbMetric: 42.4609 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 150/1000
2023-10-30 19:22:18.117 
Epoch 150/1000 
	 loss: 41.5594, MinusLogProbMetric: 41.5594, val_loss: 40.8635, val_MinusLogProbMetric: 40.8635

Epoch 150: val_loss did not improve from 39.21117
196/196 - 59s - loss: 41.5594 - MinusLogProbMetric: 41.5594 - val_loss: 40.8635 - val_MinusLogProbMetric: 40.8635 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 151/1000
2023-10-30 19:23:08.778 
Epoch 151/1000 
	 loss: 40.5178, MinusLogProbMetric: 40.5178, val_loss: 40.7852, val_MinusLogProbMetric: 40.7852

Epoch 151: val_loss did not improve from 39.21117
196/196 - 51s - loss: 40.5178 - MinusLogProbMetric: 40.5178 - val_loss: 40.7852 - val_MinusLogProbMetric: 40.7852 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 152/1000
2023-10-30 19:24:03.328 
Epoch 152/1000 
	 loss: 40.2965, MinusLogProbMetric: 40.2965, val_loss: 40.7588, val_MinusLogProbMetric: 40.7588

Epoch 152: val_loss did not improve from 39.21117
196/196 - 55s - loss: 40.2965 - MinusLogProbMetric: 40.2965 - val_loss: 40.7588 - val_MinusLogProbMetric: 40.7588 - lr: 4.1152e-06 - 55s/epoch - 278ms/step
Epoch 153/1000
2023-10-30 19:25:03.388 
Epoch 153/1000 
	 loss: 40.0564, MinusLogProbMetric: 40.0564, val_loss: 40.2436, val_MinusLogProbMetric: 40.2436

Epoch 153: val_loss did not improve from 39.21117
196/196 - 60s - loss: 40.0564 - MinusLogProbMetric: 40.0564 - val_loss: 40.2436 - val_MinusLogProbMetric: 40.2436 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 154/1000
2023-10-30 19:26:04.228 
Epoch 154/1000 
	 loss: 39.9155, MinusLogProbMetric: 39.9155, val_loss: 40.4098, val_MinusLogProbMetric: 40.4098

Epoch 154: val_loss did not improve from 39.21117
196/196 - 61s - loss: 39.9155 - MinusLogProbMetric: 39.9155 - val_loss: 40.4098 - val_MinusLogProbMetric: 40.4098 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 155/1000
2023-10-30 19:26:59.145 
Epoch 155/1000 
	 loss: 39.8024, MinusLogProbMetric: 39.8024, val_loss: 40.0439, val_MinusLogProbMetric: 40.0439

Epoch 155: val_loss did not improve from 39.21117
196/196 - 55s - loss: 39.8024 - MinusLogProbMetric: 39.8024 - val_loss: 40.0439 - val_MinusLogProbMetric: 40.0439 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 156/1000
2023-10-30 19:27:53.060 
Epoch 156/1000 
	 loss: 39.6959, MinusLogProbMetric: 39.6959, val_loss: 40.1067, val_MinusLogProbMetric: 40.1067

Epoch 156: val_loss did not improve from 39.21117
196/196 - 54s - loss: 39.6959 - MinusLogProbMetric: 39.6959 - val_loss: 40.1067 - val_MinusLogProbMetric: 40.1067 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 157/1000
2023-10-30 19:28:44.392 
Epoch 157/1000 
	 loss: 39.6307, MinusLogProbMetric: 39.6307, val_loss: 39.8798, val_MinusLogProbMetric: 39.8798

Epoch 157: val_loss did not improve from 39.21117
196/196 - 51s - loss: 39.6307 - MinusLogProbMetric: 39.6307 - val_loss: 39.8798 - val_MinusLogProbMetric: 39.8798 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 158/1000
2023-10-30 19:29:39.192 
Epoch 158/1000 
	 loss: 39.6683, MinusLogProbMetric: 39.6683, val_loss: 40.7671, val_MinusLogProbMetric: 40.7671

Epoch 158: val_loss did not improve from 39.21117
196/196 - 55s - loss: 39.6683 - MinusLogProbMetric: 39.6683 - val_loss: 40.7671 - val_MinusLogProbMetric: 40.7671 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 159/1000
2023-10-30 19:30:40.579 
Epoch 159/1000 
	 loss: 39.6240, MinusLogProbMetric: 39.6240, val_loss: 39.9078, val_MinusLogProbMetric: 39.9078

Epoch 159: val_loss did not improve from 39.21117
196/196 - 61s - loss: 39.6240 - MinusLogProbMetric: 39.6240 - val_loss: 39.9078 - val_MinusLogProbMetric: 39.9078 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 160/1000
2023-10-30 19:31:43.275 
Epoch 160/1000 
	 loss: 39.4853, MinusLogProbMetric: 39.4853, val_loss: 39.7440, val_MinusLogProbMetric: 39.7440

Epoch 160: val_loss did not improve from 39.21117
196/196 - 63s - loss: 39.4853 - MinusLogProbMetric: 39.4853 - val_loss: 39.7440 - val_MinusLogProbMetric: 39.7440 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 161/1000
2023-10-30 19:32:47.166 
Epoch 161/1000 
	 loss: 39.3799, MinusLogProbMetric: 39.3799, val_loss: 39.6949, val_MinusLogProbMetric: 39.6949

Epoch 161: val_loss did not improve from 39.21117
196/196 - 64s - loss: 39.3799 - MinusLogProbMetric: 39.3799 - val_loss: 39.6949 - val_MinusLogProbMetric: 39.6949 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 162/1000
2023-10-30 19:33:49.318 
Epoch 162/1000 
	 loss: 39.3090, MinusLogProbMetric: 39.3090, val_loss: 39.5104, val_MinusLogProbMetric: 39.5104

Epoch 162: val_loss did not improve from 39.21117
196/196 - 62s - loss: 39.3090 - MinusLogProbMetric: 39.3090 - val_loss: 39.5104 - val_MinusLogProbMetric: 39.5104 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 163/1000
2023-10-30 19:34:49.827 
Epoch 163/1000 
	 loss: 39.2529, MinusLogProbMetric: 39.2529, val_loss: 39.6764, val_MinusLogProbMetric: 39.6764

Epoch 163: val_loss did not improve from 39.21117
196/196 - 61s - loss: 39.2529 - MinusLogProbMetric: 39.2529 - val_loss: 39.6764 - val_MinusLogProbMetric: 39.6764 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 164/1000
2023-10-30 19:35:44.974 
Epoch 164/1000 
	 loss: 39.1985, MinusLogProbMetric: 39.1985, val_loss: 39.4513, val_MinusLogProbMetric: 39.4513

Epoch 164: val_loss did not improve from 39.21117
196/196 - 55s - loss: 39.1985 - MinusLogProbMetric: 39.1985 - val_loss: 39.4513 - val_MinusLogProbMetric: 39.4513 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 165/1000
2023-10-30 19:36:48.259 
Epoch 165/1000 
	 loss: 39.1161, MinusLogProbMetric: 39.1161, val_loss: 39.3373, val_MinusLogProbMetric: 39.3373

Epoch 165: val_loss did not improve from 39.21117
196/196 - 63s - loss: 39.1161 - MinusLogProbMetric: 39.1161 - val_loss: 39.3373 - val_MinusLogProbMetric: 39.3373 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 166/1000
2023-10-30 19:37:52.969 
Epoch 166/1000 
	 loss: 39.0022, MinusLogProbMetric: 39.0022, val_loss: 39.4082, val_MinusLogProbMetric: 39.4082

Epoch 166: val_loss did not improve from 39.21117
196/196 - 65s - loss: 39.0022 - MinusLogProbMetric: 39.0022 - val_loss: 39.4082 - val_MinusLogProbMetric: 39.4082 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 167/1000
2023-10-30 19:38:57.830 
Epoch 167/1000 
	 loss: 38.9876, MinusLogProbMetric: 38.9876, val_loss: 39.2655, val_MinusLogProbMetric: 39.2655

Epoch 167: val_loss did not improve from 39.21117
196/196 - 65s - loss: 38.9876 - MinusLogProbMetric: 38.9876 - val_loss: 39.2655 - val_MinusLogProbMetric: 39.2655 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 168/1000
2023-10-30 19:40:01.891 
Epoch 168/1000 
	 loss: 38.9356, MinusLogProbMetric: 38.9356, val_loss: 39.3026, val_MinusLogProbMetric: 39.3026

Epoch 168: val_loss did not improve from 39.21117
196/196 - 64s - loss: 38.9356 - MinusLogProbMetric: 38.9356 - val_loss: 39.3026 - val_MinusLogProbMetric: 39.3026 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 169/1000
2023-10-30 19:41:04.712 
Epoch 169/1000 
	 loss: 38.9022, MinusLogProbMetric: 38.9022, val_loss: 39.3817, val_MinusLogProbMetric: 39.3817

Epoch 169: val_loss did not improve from 39.21117
196/196 - 63s - loss: 38.9022 - MinusLogProbMetric: 38.9022 - val_loss: 39.3817 - val_MinusLogProbMetric: 39.3817 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 170/1000
2023-10-30 19:42:04.852 
Epoch 170/1000 
	 loss: 38.8949, MinusLogProbMetric: 38.8949, val_loss: 39.1891, val_MinusLogProbMetric: 39.1891

Epoch 170: val_loss improved from 39.21117 to 39.18907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 38.8949 - MinusLogProbMetric: 38.8949 - val_loss: 39.1891 - val_MinusLogProbMetric: 39.1891 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 171/1000
2023-10-30 19:43:08.112 
Epoch 171/1000 
	 loss: 38.8278, MinusLogProbMetric: 38.8278, val_loss: 39.0821, val_MinusLogProbMetric: 39.0821

Epoch 171: val_loss improved from 39.18907 to 39.08207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 38.8278 - MinusLogProbMetric: 38.8278 - val_loss: 39.0821 - val_MinusLogProbMetric: 39.0821 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 172/1000
2023-10-30 19:44:11.803 
Epoch 172/1000 
	 loss: 38.7667, MinusLogProbMetric: 38.7667, val_loss: 38.9400, val_MinusLogProbMetric: 38.9400

Epoch 172: val_loss improved from 39.08207 to 38.94003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 38.7667 - MinusLogProbMetric: 38.7667 - val_loss: 38.9400 - val_MinusLogProbMetric: 38.9400 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 173/1000
2023-10-30 19:45:16.502 
Epoch 173/1000 
	 loss: 38.8830, MinusLogProbMetric: 38.8830, val_loss: 39.0797, val_MinusLogProbMetric: 39.0797

Epoch 173: val_loss did not improve from 38.94003
196/196 - 63s - loss: 38.8830 - MinusLogProbMetric: 38.8830 - val_loss: 39.0797 - val_MinusLogProbMetric: 39.0797 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 174/1000
2023-10-30 19:46:20.031 
Epoch 174/1000 
	 loss: 38.7622, MinusLogProbMetric: 38.7622, val_loss: 39.1929, val_MinusLogProbMetric: 39.1929

Epoch 174: val_loss did not improve from 38.94003
196/196 - 64s - loss: 38.7622 - MinusLogProbMetric: 38.7622 - val_loss: 39.1929 - val_MinusLogProbMetric: 39.1929 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 175/1000
2023-10-30 19:47:23.920 
Epoch 175/1000 
	 loss: 38.6774, MinusLogProbMetric: 38.6774, val_loss: 39.0260, val_MinusLogProbMetric: 39.0260

Epoch 175: val_loss did not improve from 38.94003
196/196 - 64s - loss: 38.6774 - MinusLogProbMetric: 38.6774 - val_loss: 39.0260 - val_MinusLogProbMetric: 39.0260 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 176/1000
2023-10-30 19:48:29.112 
Epoch 176/1000 
	 loss: 38.6249, MinusLogProbMetric: 38.6249, val_loss: 38.9194, val_MinusLogProbMetric: 38.9194

Epoch 176: val_loss improved from 38.94003 to 38.91941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 38.6249 - MinusLogProbMetric: 38.6249 - val_loss: 38.9194 - val_MinusLogProbMetric: 38.9194 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 177/1000
2023-10-30 19:49:34.187 
Epoch 177/1000 
	 loss: 38.6047, MinusLogProbMetric: 38.6047, val_loss: 38.8552, val_MinusLogProbMetric: 38.8552

Epoch 177: val_loss improved from 38.91941 to 38.85518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 38.6047 - MinusLogProbMetric: 38.6047 - val_loss: 38.8552 - val_MinusLogProbMetric: 38.8552 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 178/1000
2023-10-30 19:50:40.839 
Epoch 178/1000 
	 loss: 38.5771, MinusLogProbMetric: 38.5771, val_loss: 38.9905, val_MinusLogProbMetric: 38.9905

Epoch 178: val_loss did not improve from 38.85518
196/196 - 66s - loss: 38.5771 - MinusLogProbMetric: 38.5771 - val_loss: 38.9905 - val_MinusLogProbMetric: 38.9905 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 179/1000
2023-10-30 19:51:45.579 
Epoch 179/1000 
	 loss: 38.5406, MinusLogProbMetric: 38.5406, val_loss: 38.9441, val_MinusLogProbMetric: 38.9441

Epoch 179: val_loss did not improve from 38.85518
196/196 - 65s - loss: 38.5406 - MinusLogProbMetric: 38.5406 - val_loss: 38.9441 - val_MinusLogProbMetric: 38.9441 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 180/1000
2023-10-30 19:52:50.255 
Epoch 180/1000 
	 loss: 38.4954, MinusLogProbMetric: 38.4954, val_loss: 39.0378, val_MinusLogProbMetric: 39.0378

Epoch 180: val_loss did not improve from 38.85518
196/196 - 65s - loss: 38.4954 - MinusLogProbMetric: 38.4954 - val_loss: 39.0378 - val_MinusLogProbMetric: 39.0378 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 181/1000
2023-10-30 19:53:52.054 
Epoch 181/1000 
	 loss: 39.2251, MinusLogProbMetric: 39.2251, val_loss: 38.6814, val_MinusLogProbMetric: 38.6814

Epoch 181: val_loss improved from 38.85518 to 38.68139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 39.2251 - MinusLogProbMetric: 39.2251 - val_loss: 38.6814 - val_MinusLogProbMetric: 38.6814 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 182/1000
2023-10-30 19:54:57.371 
Epoch 182/1000 
	 loss: 38.4841, MinusLogProbMetric: 38.4841, val_loss: 38.7154, val_MinusLogProbMetric: 38.7154

Epoch 182: val_loss did not improve from 38.68139
196/196 - 64s - loss: 38.4841 - MinusLogProbMetric: 38.4841 - val_loss: 38.7154 - val_MinusLogProbMetric: 38.7154 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 183/1000
2023-10-30 19:55:59.746 
Epoch 183/1000 
	 loss: 38.4184, MinusLogProbMetric: 38.4184, val_loss: 38.6401, val_MinusLogProbMetric: 38.6401

Epoch 183: val_loss improved from 38.68139 to 38.64006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 38.4184 - MinusLogProbMetric: 38.4184 - val_loss: 38.6401 - val_MinusLogProbMetric: 38.6401 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 184/1000
2023-10-30 19:57:03.594 
Epoch 184/1000 
	 loss: 38.4083, MinusLogProbMetric: 38.4083, val_loss: 38.6279, val_MinusLogProbMetric: 38.6279

Epoch 184: val_loss improved from 38.64006 to 38.62792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 38.4083 - MinusLogProbMetric: 38.4083 - val_loss: 38.6279 - val_MinusLogProbMetric: 38.6279 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 185/1000
2023-10-30 19:58:07.286 
Epoch 185/1000 
	 loss: 38.3422, MinusLogProbMetric: 38.3422, val_loss: 38.5949, val_MinusLogProbMetric: 38.5949

Epoch 185: val_loss improved from 38.62792 to 38.59491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 38.3422 - MinusLogProbMetric: 38.3422 - val_loss: 38.5949 - val_MinusLogProbMetric: 38.5949 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 186/1000
2023-10-30 19:59:07.288 
Epoch 186/1000 
	 loss: 38.3002, MinusLogProbMetric: 38.3002, val_loss: 38.6258, val_MinusLogProbMetric: 38.6258

Epoch 186: val_loss did not improve from 38.59491
196/196 - 58s - loss: 38.3002 - MinusLogProbMetric: 38.3002 - val_loss: 38.6258 - val_MinusLogProbMetric: 38.6258 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 187/1000
2023-10-30 19:59:58.010 
Epoch 187/1000 
	 loss: 38.3357, MinusLogProbMetric: 38.3357, val_loss: 38.6424, val_MinusLogProbMetric: 38.6424

Epoch 187: val_loss did not improve from 38.59491
196/196 - 51s - loss: 38.3357 - MinusLogProbMetric: 38.3357 - val_loss: 38.6424 - val_MinusLogProbMetric: 38.6424 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 188/1000
2023-10-30 20:00:48.980 
Epoch 188/1000 
	 loss: 38.2629, MinusLogProbMetric: 38.2629, val_loss: 38.4949, val_MinusLogProbMetric: 38.4949

Epoch 188: val_loss improved from 38.59491 to 38.49492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 38.2629 - MinusLogProbMetric: 38.2629 - val_loss: 38.4949 - val_MinusLogProbMetric: 38.4949 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 189/1000
2023-10-30 20:01:45.033 
Epoch 189/1000 
	 loss: 38.2532, MinusLogProbMetric: 38.2532, val_loss: 38.5397, val_MinusLogProbMetric: 38.5397

Epoch 189: val_loss did not improve from 38.49492
196/196 - 55s - loss: 38.2532 - MinusLogProbMetric: 38.2532 - val_loss: 38.5397 - val_MinusLogProbMetric: 38.5397 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 190/1000
2023-10-30 20:02:34.259 
Epoch 190/1000 
	 loss: 38.1936, MinusLogProbMetric: 38.1936, val_loss: 38.4312, val_MinusLogProbMetric: 38.4312

Epoch 190: val_loss improved from 38.49492 to 38.43118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 38.1936 - MinusLogProbMetric: 38.1936 - val_loss: 38.4312 - val_MinusLogProbMetric: 38.4312 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 191/1000
2023-10-30 20:03:24.730 
Epoch 191/1000 
	 loss: 38.1687, MinusLogProbMetric: 38.1687, val_loss: 38.5966, val_MinusLogProbMetric: 38.5966

Epoch 191: val_loss did not improve from 38.43118
196/196 - 50s - loss: 38.1687 - MinusLogProbMetric: 38.1687 - val_loss: 38.5966 - val_MinusLogProbMetric: 38.5966 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 192/1000
2023-10-30 20:04:21.003 
Epoch 192/1000 
	 loss: 38.1389, MinusLogProbMetric: 38.1389, val_loss: 38.5109, val_MinusLogProbMetric: 38.5109

Epoch 192: val_loss did not improve from 38.43118
196/196 - 56s - loss: 38.1389 - MinusLogProbMetric: 38.1389 - val_loss: 38.5109 - val_MinusLogProbMetric: 38.5109 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 193/1000
2023-10-30 20:05:10.291 
Epoch 193/1000 
	 loss: 38.1566, MinusLogProbMetric: 38.1566, val_loss: 38.5042, val_MinusLogProbMetric: 38.5042

Epoch 193: val_loss did not improve from 38.43118
196/196 - 49s - loss: 38.1566 - MinusLogProbMetric: 38.1566 - val_loss: 38.5042 - val_MinusLogProbMetric: 38.5042 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 194/1000
2023-10-30 20:05:59.603 
Epoch 194/1000 
	 loss: 38.0913, MinusLogProbMetric: 38.0913, val_loss: 38.3832, val_MinusLogProbMetric: 38.3832

Epoch 194: val_loss improved from 38.43118 to 38.38321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 38.0913 - MinusLogProbMetric: 38.0913 - val_loss: 38.3832 - val_MinusLogProbMetric: 38.3832 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 195/1000
2023-10-30 20:06:59.686 
Epoch 195/1000 
	 loss: 38.2380, MinusLogProbMetric: 38.2380, val_loss: 38.3534, val_MinusLogProbMetric: 38.3534

Epoch 195: val_loss improved from 38.38321 to 38.35339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 38.2380 - MinusLogProbMetric: 38.2380 - val_loss: 38.3534 - val_MinusLogProbMetric: 38.3534 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 196/1000
2023-10-30 20:07:51.075 
Epoch 196/1000 
	 loss: 38.0511, MinusLogProbMetric: 38.0511, val_loss: 38.5641, val_MinusLogProbMetric: 38.5641

Epoch 196: val_loss did not improve from 38.35339
196/196 - 50s - loss: 38.0511 - MinusLogProbMetric: 38.0511 - val_loss: 38.5641 - val_MinusLogProbMetric: 38.5641 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 197/1000
2023-10-30 20:08:40.632 
Epoch 197/1000 
	 loss: 38.0301, MinusLogProbMetric: 38.0301, val_loss: 38.4157, val_MinusLogProbMetric: 38.4157

Epoch 197: val_loss did not improve from 38.35339
196/196 - 50s - loss: 38.0301 - MinusLogProbMetric: 38.0301 - val_loss: 38.4157 - val_MinusLogProbMetric: 38.4157 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 198/1000
2023-10-30 20:09:37.846 
Epoch 198/1000 
	 loss: 38.0015, MinusLogProbMetric: 38.0015, val_loss: 38.2627, val_MinusLogProbMetric: 38.2627

Epoch 198: val_loss improved from 38.35339 to 38.26274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 38.0015 - MinusLogProbMetric: 38.0015 - val_loss: 38.2627 - val_MinusLogProbMetric: 38.2627 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 199/1000
2023-10-30 20:10:29.330 
Epoch 199/1000 
	 loss: 37.9637, MinusLogProbMetric: 37.9637, val_loss: 38.2770, val_MinusLogProbMetric: 38.2770

Epoch 199: val_loss did not improve from 38.26274
196/196 - 50s - loss: 37.9637 - MinusLogProbMetric: 37.9637 - val_loss: 38.2770 - val_MinusLogProbMetric: 38.2770 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 200/1000
2023-10-30 20:11:19.183 
Epoch 200/1000 
	 loss: 37.9685, MinusLogProbMetric: 37.9685, val_loss: 38.1791, val_MinusLogProbMetric: 38.1791

Epoch 200: val_loss improved from 38.26274 to 38.17915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 37.9685 - MinusLogProbMetric: 37.9685 - val_loss: 38.1791 - val_MinusLogProbMetric: 38.1791 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 201/1000
2023-10-30 20:12:14.165 
Epoch 201/1000 
	 loss: 37.9351, MinusLogProbMetric: 37.9351, val_loss: 38.2237, val_MinusLogProbMetric: 38.2237

Epoch 201: val_loss did not improve from 38.17915
196/196 - 54s - loss: 37.9351 - MinusLogProbMetric: 37.9351 - val_loss: 38.2237 - val_MinusLogProbMetric: 38.2237 - lr: 4.1152e-06 - 54s/epoch - 277ms/step
Epoch 202/1000
2023-10-30 20:13:07.240 
Epoch 202/1000 
	 loss: 37.9082, MinusLogProbMetric: 37.9082, val_loss: 38.2684, val_MinusLogProbMetric: 38.2684

Epoch 202: val_loss did not improve from 38.17915
196/196 - 53s - loss: 37.9082 - MinusLogProbMetric: 37.9082 - val_loss: 38.2684 - val_MinusLogProbMetric: 38.2684 - lr: 4.1152e-06 - 53s/epoch - 271ms/step
Epoch 203/1000
2023-10-30 20:13:57.087 
Epoch 203/1000 
	 loss: 37.8946, MinusLogProbMetric: 37.8946, val_loss: 38.1634, val_MinusLogProbMetric: 38.1634

Epoch 203: val_loss improved from 38.17915 to 38.16342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 37.8946 - MinusLogProbMetric: 37.8946 - val_loss: 38.1634 - val_MinusLogProbMetric: 38.1634 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 204/1000
2023-10-30 20:14:51.097 
Epoch 204/1000 
	 loss: 37.8668, MinusLogProbMetric: 37.8668, val_loss: 38.0751, val_MinusLogProbMetric: 38.0751

Epoch 204: val_loss improved from 38.16342 to 38.07509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 37.8668 - MinusLogProbMetric: 37.8668 - val_loss: 38.0751 - val_MinusLogProbMetric: 38.0751 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 205/1000
2023-10-30 20:15:47.083 
Epoch 205/1000 
	 loss: 37.8462, MinusLogProbMetric: 37.8462, val_loss: 38.1149, val_MinusLogProbMetric: 38.1149

Epoch 205: val_loss did not improve from 38.07509
196/196 - 55s - loss: 37.8462 - MinusLogProbMetric: 37.8462 - val_loss: 38.1149 - val_MinusLogProbMetric: 38.1149 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 206/1000
2023-10-30 20:16:36.501 
Epoch 206/1000 
	 loss: 37.8028, MinusLogProbMetric: 37.8028, val_loss: 38.1638, val_MinusLogProbMetric: 38.1638

Epoch 206: val_loss did not improve from 38.07509
196/196 - 49s - loss: 37.8028 - MinusLogProbMetric: 37.8028 - val_loss: 38.1638 - val_MinusLogProbMetric: 38.1638 - lr: 4.1152e-06 - 49s/epoch - 252ms/step
Epoch 207/1000
2023-10-30 20:17:27.529 
Epoch 207/1000 
	 loss: 37.7687, MinusLogProbMetric: 37.7687, val_loss: 38.3918, val_MinusLogProbMetric: 38.3918

Epoch 207: val_loss did not improve from 38.07509
196/196 - 51s - loss: 37.7687 - MinusLogProbMetric: 37.7687 - val_loss: 38.3918 - val_MinusLogProbMetric: 38.3918 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 208/1000
2023-10-30 20:18:21.487 
Epoch 208/1000 
	 loss: 37.7970, MinusLogProbMetric: 37.7970, val_loss: 38.3228, val_MinusLogProbMetric: 38.3228

Epoch 208: val_loss did not improve from 38.07509
196/196 - 54s - loss: 37.7970 - MinusLogProbMetric: 37.7970 - val_loss: 38.3228 - val_MinusLogProbMetric: 38.3228 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 209/1000
2023-10-30 20:19:11.465 
Epoch 209/1000 
	 loss: 37.7492, MinusLogProbMetric: 37.7492, val_loss: 38.0478, val_MinusLogProbMetric: 38.0478

Epoch 209: val_loss improved from 38.07509 to 38.04779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 37.7492 - MinusLogProbMetric: 37.7492 - val_loss: 38.0478 - val_MinusLogProbMetric: 38.0478 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 210/1000
2023-10-30 20:20:01.706 
Epoch 210/1000 
	 loss: 37.7324, MinusLogProbMetric: 37.7324, val_loss: 38.1574, val_MinusLogProbMetric: 38.1574

Epoch 210: val_loss did not improve from 38.04779
196/196 - 49s - loss: 37.7324 - MinusLogProbMetric: 37.7324 - val_loss: 38.1574 - val_MinusLogProbMetric: 38.1574 - lr: 4.1152e-06 - 49s/epoch - 253ms/step
Epoch 211/1000
2023-10-30 20:21:03.054 
Epoch 211/1000 
	 loss: 37.7153, MinusLogProbMetric: 37.7153, val_loss: 37.9225, val_MinusLogProbMetric: 37.9225

Epoch 211: val_loss improved from 38.04779 to 37.92252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 37.7153 - MinusLogProbMetric: 37.7153 - val_loss: 37.9225 - val_MinusLogProbMetric: 37.9225 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 212/1000
2023-10-30 20:21:55.152 
Epoch 212/1000 
	 loss: 37.7072, MinusLogProbMetric: 37.7072, val_loss: 37.9727, val_MinusLogProbMetric: 37.9727

Epoch 212: val_loss did not improve from 37.92252
196/196 - 51s - loss: 37.7072 - MinusLogProbMetric: 37.7072 - val_loss: 37.9727 - val_MinusLogProbMetric: 37.9727 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 213/1000
2023-10-30 20:22:45.094 
Epoch 213/1000 
	 loss: 37.6452, MinusLogProbMetric: 37.6452, val_loss: 37.9533, val_MinusLogProbMetric: 37.9533

Epoch 213: val_loss did not improve from 37.92252
196/196 - 50s - loss: 37.6452 - MinusLogProbMetric: 37.6452 - val_loss: 37.9533 - val_MinusLogProbMetric: 37.9533 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 214/1000
2023-10-30 20:23:44.951 
Epoch 214/1000 
	 loss: 37.6180, MinusLogProbMetric: 37.6180, val_loss: 37.9459, val_MinusLogProbMetric: 37.9459

Epoch 214: val_loss did not improve from 37.92252
196/196 - 60s - loss: 37.6180 - MinusLogProbMetric: 37.6180 - val_loss: 37.9459 - val_MinusLogProbMetric: 37.9459 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 215/1000
2023-10-30 20:24:37.608 
Epoch 215/1000 
	 loss: 37.6315, MinusLogProbMetric: 37.6315, val_loss: 37.7870, val_MinusLogProbMetric: 37.7870

Epoch 215: val_loss improved from 37.92252 to 37.78704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 37.6315 - MinusLogProbMetric: 37.6315 - val_loss: 37.7870 - val_MinusLogProbMetric: 37.7870 - lr: 4.1152e-06 - 53s/epoch - 272ms/step
Epoch 216/1000
2023-10-30 20:25:27.950 
Epoch 216/1000 
	 loss: 37.6034, MinusLogProbMetric: 37.6034, val_loss: 38.0056, val_MinusLogProbMetric: 38.0056

Epoch 216: val_loss did not improve from 37.78704
196/196 - 50s - loss: 37.6034 - MinusLogProbMetric: 37.6034 - val_loss: 38.0056 - val_MinusLogProbMetric: 38.0056 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 217/1000
2023-10-30 20:26:24.009 
Epoch 217/1000 
	 loss: 37.6058, MinusLogProbMetric: 37.6058, val_loss: 37.8921, val_MinusLogProbMetric: 37.8921

Epoch 217: val_loss did not improve from 37.78704
196/196 - 56s - loss: 37.6058 - MinusLogProbMetric: 37.6058 - val_loss: 37.8921 - val_MinusLogProbMetric: 37.8921 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 218/1000
2023-10-30 20:27:17.220 
Epoch 218/1000 
	 loss: 37.6120, MinusLogProbMetric: 37.6120, val_loss: 37.7600, val_MinusLogProbMetric: 37.7600

Epoch 218: val_loss improved from 37.78704 to 37.75996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 37.6120 - MinusLogProbMetric: 37.6120 - val_loss: 37.7600 - val_MinusLogProbMetric: 37.7600 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 219/1000
2023-10-30 20:28:08.263 
Epoch 219/1000 
	 loss: 37.5128, MinusLogProbMetric: 37.5128, val_loss: 37.8197, val_MinusLogProbMetric: 37.8197

Epoch 219: val_loss did not improve from 37.75996
196/196 - 50s - loss: 37.5128 - MinusLogProbMetric: 37.5128 - val_loss: 37.8197 - val_MinusLogProbMetric: 37.8197 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 220/1000
2023-10-30 20:29:02.636 
Epoch 220/1000 
	 loss: 37.5524, MinusLogProbMetric: 37.5524, val_loss: 37.7519, val_MinusLogProbMetric: 37.7519

Epoch 220: val_loss improved from 37.75996 to 37.75191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 37.5524 - MinusLogProbMetric: 37.5524 - val_loss: 37.7519 - val_MinusLogProbMetric: 37.7519 - lr: 4.1152e-06 - 55s/epoch - 283ms/step
Epoch 221/1000
2023-10-30 20:29:59.399 
Epoch 221/1000 
	 loss: 37.5006, MinusLogProbMetric: 37.5006, val_loss: 37.8701, val_MinusLogProbMetric: 37.8701

Epoch 221: val_loss did not improve from 37.75191
196/196 - 56s - loss: 37.5006 - MinusLogProbMetric: 37.5006 - val_loss: 37.8701 - val_MinusLogProbMetric: 37.8701 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 222/1000
2023-10-30 20:30:54.691 
Epoch 222/1000 
	 loss: 37.4495, MinusLogProbMetric: 37.4495, val_loss: 37.9115, val_MinusLogProbMetric: 37.9115

Epoch 222: val_loss did not improve from 37.75191
196/196 - 55s - loss: 37.4495 - MinusLogProbMetric: 37.4495 - val_loss: 37.9115 - val_MinusLogProbMetric: 37.9115 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 223/1000
2023-10-30 20:31:47.212 
Epoch 223/1000 
	 loss: 37.4643, MinusLogProbMetric: 37.4643, val_loss: 37.8432, val_MinusLogProbMetric: 37.8432

Epoch 223: val_loss did not improve from 37.75191
196/196 - 53s - loss: 37.4643 - MinusLogProbMetric: 37.4643 - val_loss: 37.8432 - val_MinusLogProbMetric: 37.8432 - lr: 4.1152e-06 - 53s/epoch - 268ms/step
Epoch 224/1000
2023-10-30 20:32:37.627 
Epoch 224/1000 
	 loss: 37.4198, MinusLogProbMetric: 37.4198, val_loss: 37.8269, val_MinusLogProbMetric: 37.8269

Epoch 224: val_loss did not improve from 37.75191
196/196 - 50s - loss: 37.4198 - MinusLogProbMetric: 37.4198 - val_loss: 37.8269 - val_MinusLogProbMetric: 37.8269 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 225/1000
2023-10-30 20:33:32.282 
Epoch 225/1000 
	 loss: 37.4342, MinusLogProbMetric: 37.4342, val_loss: 37.7725, val_MinusLogProbMetric: 37.7725

Epoch 225: val_loss did not improve from 37.75191
196/196 - 55s - loss: 37.4342 - MinusLogProbMetric: 37.4342 - val_loss: 37.7725 - val_MinusLogProbMetric: 37.7725 - lr: 4.1152e-06 - 55s/epoch - 279ms/step
Epoch 226/1000
2023-10-30 20:34:27.420 
Epoch 226/1000 
	 loss: 37.4219, MinusLogProbMetric: 37.4219, val_loss: 37.6035, val_MinusLogProbMetric: 37.6035

Epoch 226: val_loss improved from 37.75191 to 37.60351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 37.4219 - MinusLogProbMetric: 37.4219 - val_loss: 37.6035 - val_MinusLogProbMetric: 37.6035 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 227/1000
2023-10-30 20:35:24.384 
Epoch 227/1000 
	 loss: 37.3588, MinusLogProbMetric: 37.3588, val_loss: 37.8830, val_MinusLogProbMetric: 37.8830

Epoch 227: val_loss did not improve from 37.60351
196/196 - 56s - loss: 37.3588 - MinusLogProbMetric: 37.3588 - val_loss: 37.8830 - val_MinusLogProbMetric: 37.8830 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 228/1000
2023-10-30 20:36:20.544 
Epoch 228/1000 
	 loss: 37.3492, MinusLogProbMetric: 37.3492, val_loss: 37.5855, val_MinusLogProbMetric: 37.5855

Epoch 228: val_loss improved from 37.60351 to 37.58554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 37.3492 - MinusLogProbMetric: 37.3492 - val_loss: 37.5855 - val_MinusLogProbMetric: 37.5855 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 229/1000
2023-10-30 20:37:11.599 
Epoch 229/1000 
	 loss: 37.3394, MinusLogProbMetric: 37.3394, val_loss: 37.5465, val_MinusLogProbMetric: 37.5465

Epoch 229: val_loss improved from 37.58554 to 37.54648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 37.3394 - MinusLogProbMetric: 37.3394 - val_loss: 37.5465 - val_MinusLogProbMetric: 37.5465 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 230/1000
2023-10-30 20:38:04.403 
Epoch 230/1000 
	 loss: 37.3038, MinusLogProbMetric: 37.3038, val_loss: 37.7485, val_MinusLogProbMetric: 37.7485

Epoch 230: val_loss did not improve from 37.54648
196/196 - 52s - loss: 37.3038 - MinusLogProbMetric: 37.3038 - val_loss: 37.7485 - val_MinusLogProbMetric: 37.7485 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 231/1000
2023-10-30 20:38:56.964 
Epoch 231/1000 
	 loss: 37.3068, MinusLogProbMetric: 37.3068, val_loss: 37.5464, val_MinusLogProbMetric: 37.5464

Epoch 231: val_loss improved from 37.54648 to 37.54637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 37.3068 - MinusLogProbMetric: 37.3068 - val_loss: 37.5464 - val_MinusLogProbMetric: 37.5464 - lr: 4.1152e-06 - 54s/epoch - 273ms/step
Epoch 232/1000
2023-10-30 20:39:49.151 
Epoch 232/1000 
	 loss: 37.2714, MinusLogProbMetric: 37.2714, val_loss: 37.6845, val_MinusLogProbMetric: 37.6845

Epoch 232: val_loss did not improve from 37.54637
196/196 - 51s - loss: 37.2714 - MinusLogProbMetric: 37.2714 - val_loss: 37.6845 - val_MinusLogProbMetric: 37.6845 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 233/1000
2023-10-30 20:40:44.444 
Epoch 233/1000 
	 loss: 37.2784, MinusLogProbMetric: 37.2784, val_loss: 37.6270, val_MinusLogProbMetric: 37.6270

Epoch 233: val_loss did not improve from 37.54637
196/196 - 55s - loss: 37.2784 - MinusLogProbMetric: 37.2784 - val_loss: 37.6270 - val_MinusLogProbMetric: 37.6270 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 234/1000
2023-10-30 20:41:36.805 
Epoch 234/1000 
	 loss: 37.2220, MinusLogProbMetric: 37.2220, val_loss: 37.5588, val_MinusLogProbMetric: 37.5588

Epoch 234: val_loss did not improve from 37.54637
196/196 - 52s - loss: 37.2220 - MinusLogProbMetric: 37.2220 - val_loss: 37.5588 - val_MinusLogProbMetric: 37.5588 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 235/1000
2023-10-30 20:42:28.671 
Epoch 235/1000 
	 loss: 37.2576, MinusLogProbMetric: 37.2576, val_loss: 37.5591, val_MinusLogProbMetric: 37.5591

Epoch 235: val_loss did not improve from 37.54637
196/196 - 52s - loss: 37.2576 - MinusLogProbMetric: 37.2576 - val_loss: 37.5591 - val_MinusLogProbMetric: 37.5591 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 236/1000
2023-10-30 20:43:23.532 
Epoch 236/1000 
	 loss: 37.1981, MinusLogProbMetric: 37.1981, val_loss: 37.4473, val_MinusLogProbMetric: 37.4473

Epoch 236: val_loss improved from 37.54637 to 37.44733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 37.1981 - MinusLogProbMetric: 37.1981 - val_loss: 37.4473 - val_MinusLogProbMetric: 37.4473 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 237/1000
2023-10-30 20:44:22.019 
Epoch 237/1000 
	 loss: 37.1898, MinusLogProbMetric: 37.1898, val_loss: 37.5142, val_MinusLogProbMetric: 37.5142

Epoch 237: val_loss did not improve from 37.44733
196/196 - 57s - loss: 37.1898 - MinusLogProbMetric: 37.1898 - val_loss: 37.5142 - val_MinusLogProbMetric: 37.5142 - lr: 4.1152e-06 - 57s/epoch - 293ms/step
Epoch 238/1000
2023-10-30 20:45:12.401 
Epoch 238/1000 
	 loss: 37.1730, MinusLogProbMetric: 37.1730, val_loss: 37.5428, val_MinusLogProbMetric: 37.5428

Epoch 238: val_loss did not improve from 37.44733
196/196 - 50s - loss: 37.1730 - MinusLogProbMetric: 37.1730 - val_loss: 37.5428 - val_MinusLogProbMetric: 37.5428 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 239/1000
2023-10-30 20:46:05.026 
Epoch 239/1000 
	 loss: 37.1979, MinusLogProbMetric: 37.1979, val_loss: 37.3564, val_MinusLogProbMetric: 37.3564

Epoch 239: val_loss improved from 37.44733 to 37.35638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 37.1979 - MinusLogProbMetric: 37.1979 - val_loss: 37.3564 - val_MinusLogProbMetric: 37.3564 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 240/1000
2023-10-30 20:46:58.620 
Epoch 240/1000 
	 loss: 37.1377, MinusLogProbMetric: 37.1377, val_loss: 37.4023, val_MinusLogProbMetric: 37.4023

Epoch 240: val_loss did not improve from 37.35638
196/196 - 53s - loss: 37.1377 - MinusLogProbMetric: 37.1377 - val_loss: 37.4023 - val_MinusLogProbMetric: 37.4023 - lr: 4.1152e-06 - 53s/epoch - 268ms/step
Epoch 241/1000
2023-10-30 20:47:49.828 
Epoch 241/1000 
	 loss: 37.0935, MinusLogProbMetric: 37.0935, val_loss: 37.4648, val_MinusLogProbMetric: 37.4648

Epoch 241: val_loss did not improve from 37.35638
196/196 - 51s - loss: 37.0935 - MinusLogProbMetric: 37.0935 - val_loss: 37.4648 - val_MinusLogProbMetric: 37.4648 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 242/1000
2023-10-30 20:48:45.286 
Epoch 242/1000 
	 loss: 37.2167, MinusLogProbMetric: 37.2167, val_loss: 37.4107, val_MinusLogProbMetric: 37.4107

Epoch 242: val_loss did not improve from 37.35638
196/196 - 55s - loss: 37.2167 - MinusLogProbMetric: 37.2167 - val_loss: 37.4107 - val_MinusLogProbMetric: 37.4107 - lr: 4.1152e-06 - 55s/epoch - 283ms/step
Epoch 243/1000
2023-10-30 20:49:38.573 
Epoch 243/1000 
	 loss: 37.0767, MinusLogProbMetric: 37.0767, val_loss: 37.3211, val_MinusLogProbMetric: 37.3211

Epoch 243: val_loss improved from 37.35638 to 37.32114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 37.0767 - MinusLogProbMetric: 37.0767 - val_loss: 37.3211 - val_MinusLogProbMetric: 37.3211 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 244/1000
2023-10-30 20:50:32.230 
Epoch 244/1000 
	 loss: 37.0799, MinusLogProbMetric: 37.0799, val_loss: 37.4487, val_MinusLogProbMetric: 37.4487

Epoch 244: val_loss did not improve from 37.32114
196/196 - 52s - loss: 37.0799 - MinusLogProbMetric: 37.0799 - val_loss: 37.4487 - val_MinusLogProbMetric: 37.4487 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 245/1000
2023-10-30 20:51:30.258 
Epoch 245/1000 
	 loss: 38.0504, MinusLogProbMetric: 38.0504, val_loss: 37.5337, val_MinusLogProbMetric: 37.5337

Epoch 245: val_loss did not improve from 37.32114
196/196 - 58s - loss: 38.0504 - MinusLogProbMetric: 38.0504 - val_loss: 37.5337 - val_MinusLogProbMetric: 37.5337 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 246/1000
2023-10-30 20:52:27.229 
Epoch 246/1000 
	 loss: 37.0894, MinusLogProbMetric: 37.0894, val_loss: 37.2732, val_MinusLogProbMetric: 37.2732

Epoch 246: val_loss improved from 37.32114 to 37.27324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 37.0894 - MinusLogProbMetric: 37.0894 - val_loss: 37.2732 - val_MinusLogProbMetric: 37.2732 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 247/1000
2023-10-30 20:53:19.023 
Epoch 247/1000 
	 loss: 37.0021, MinusLogProbMetric: 37.0021, val_loss: 37.4053, val_MinusLogProbMetric: 37.4053

Epoch 247: val_loss did not improve from 37.27324
196/196 - 51s - loss: 37.0021 - MinusLogProbMetric: 37.0021 - val_loss: 37.4053 - val_MinusLogProbMetric: 37.4053 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 248/1000
2023-10-30 20:54:14.969 
Epoch 248/1000 
	 loss: 37.0155, MinusLogProbMetric: 37.0155, val_loss: 37.1906, val_MinusLogProbMetric: 37.1906

Epoch 248: val_loss improved from 37.27324 to 37.19061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 37.0155 - MinusLogProbMetric: 37.0155 - val_loss: 37.1906 - val_MinusLogProbMetric: 37.1906 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 249/1000
2023-10-30 20:55:08.549 
Epoch 249/1000 
	 loss: 37.0031, MinusLogProbMetric: 37.0031, val_loss: 37.2795, val_MinusLogProbMetric: 37.2795

Epoch 249: val_loss did not improve from 37.19061
196/196 - 53s - loss: 37.0031 - MinusLogProbMetric: 37.0031 - val_loss: 37.2795 - val_MinusLogProbMetric: 37.2795 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 250/1000
2023-10-30 20:56:00.737 
Epoch 250/1000 
	 loss: 36.9783, MinusLogProbMetric: 36.9783, val_loss: 37.2741, val_MinusLogProbMetric: 37.2741

Epoch 250: val_loss did not improve from 37.19061
196/196 - 52s - loss: 36.9783 - MinusLogProbMetric: 36.9783 - val_loss: 37.2741 - val_MinusLogProbMetric: 37.2741 - lr: 4.1152e-06 - 52s/epoch - 266ms/step
Epoch 251/1000
2023-10-30 20:56:57.004 
Epoch 251/1000 
	 loss: 36.9425, MinusLogProbMetric: 36.9425, val_loss: 37.2114, val_MinusLogProbMetric: 37.2114

Epoch 251: val_loss did not improve from 37.19061
196/196 - 56s - loss: 36.9425 - MinusLogProbMetric: 36.9425 - val_loss: 37.2114 - val_MinusLogProbMetric: 37.2114 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 252/1000
2023-10-30 20:57:47.583 
Epoch 252/1000 
	 loss: 36.9331, MinusLogProbMetric: 36.9331, val_loss: 37.3196, val_MinusLogProbMetric: 37.3196

Epoch 252: val_loss did not improve from 37.19061
196/196 - 51s - loss: 36.9331 - MinusLogProbMetric: 36.9331 - val_loss: 37.3196 - val_MinusLogProbMetric: 37.3196 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 253/1000
2023-10-30 20:58:44.893 
Epoch 253/1000 
	 loss: 36.9289, MinusLogProbMetric: 36.9289, val_loss: 37.2713, val_MinusLogProbMetric: 37.2713

Epoch 253: val_loss did not improve from 37.19061
196/196 - 57s - loss: 36.9289 - MinusLogProbMetric: 36.9289 - val_loss: 37.2713 - val_MinusLogProbMetric: 37.2713 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 254/1000
2023-10-30 20:59:39.788 
Epoch 254/1000 
	 loss: 36.9002, MinusLogProbMetric: 36.9002, val_loss: 37.1394, val_MinusLogProbMetric: 37.1394

Epoch 254: val_loss improved from 37.19061 to 37.13939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 36.9002 - MinusLogProbMetric: 36.9002 - val_loss: 37.1394 - val_MinusLogProbMetric: 37.1394 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 255/1000
2023-10-30 21:00:30.090 
Epoch 255/1000 
	 loss: 36.9186, MinusLogProbMetric: 36.9186, val_loss: 37.1227, val_MinusLogProbMetric: 37.1227

Epoch 255: val_loss improved from 37.13939 to 37.12275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 36.9186 - MinusLogProbMetric: 36.9186 - val_loss: 37.1227 - val_MinusLogProbMetric: 37.1227 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 256/1000
2023-10-30 21:01:24.326 
Epoch 256/1000 
	 loss: 36.8478, MinusLogProbMetric: 36.8478, val_loss: 37.2638, val_MinusLogProbMetric: 37.2638

Epoch 256: val_loss did not improve from 37.12275
196/196 - 53s - loss: 36.8478 - MinusLogProbMetric: 36.8478 - val_loss: 37.2638 - val_MinusLogProbMetric: 37.2638 - lr: 4.1152e-06 - 53s/epoch - 273ms/step
Epoch 257/1000
2023-10-30 21:02:15.813 
Epoch 257/1000 
	 loss: 36.8926, MinusLogProbMetric: 36.8926, val_loss: 37.0315, val_MinusLogProbMetric: 37.0315

Epoch 257: val_loss improved from 37.12275 to 37.03150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 36.8926 - MinusLogProbMetric: 36.8926 - val_loss: 37.0315 - val_MinusLogProbMetric: 37.0315 - lr: 4.1152e-06 - 52s/epoch - 266ms/step
Epoch 258/1000
2023-10-30 21:03:07.139 
Epoch 258/1000 
	 loss: 36.8448, MinusLogProbMetric: 36.8448, val_loss: 37.1905, val_MinusLogProbMetric: 37.1905

Epoch 258: val_loss did not improve from 37.03150
196/196 - 51s - loss: 36.8448 - MinusLogProbMetric: 36.8448 - val_loss: 37.1905 - val_MinusLogProbMetric: 37.1905 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 259/1000
2023-10-30 21:04:00.744 
Epoch 259/1000 
	 loss: 36.8792, MinusLogProbMetric: 36.8792, val_loss: 37.0948, val_MinusLogProbMetric: 37.0948

Epoch 259: val_loss did not improve from 37.03150
196/196 - 54s - loss: 36.8792 - MinusLogProbMetric: 36.8792 - val_loss: 37.0948 - val_MinusLogProbMetric: 37.0948 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 260/1000
2023-10-30 21:04:50.806 
Epoch 260/1000 
	 loss: 36.7687, MinusLogProbMetric: 36.7687, val_loss: 36.9782, val_MinusLogProbMetric: 36.9782

Epoch 260: val_loss improved from 37.03150 to 36.97815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 36.7687 - MinusLogProbMetric: 36.7687 - val_loss: 36.9782 - val_MinusLogProbMetric: 36.9782 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 261/1000
2023-10-30 21:05:46.290 
Epoch 261/1000 
	 loss: 36.7736, MinusLogProbMetric: 36.7736, val_loss: 37.2260, val_MinusLogProbMetric: 37.2260

Epoch 261: val_loss did not improve from 36.97815
196/196 - 55s - loss: 36.7736 - MinusLogProbMetric: 36.7736 - val_loss: 37.2260 - val_MinusLogProbMetric: 37.2260 - lr: 4.1152e-06 - 55s/epoch - 279ms/step
Epoch 262/1000
2023-10-30 21:06:39.918 
Epoch 262/1000 
	 loss: 36.7804, MinusLogProbMetric: 36.7804, val_loss: 37.4779, val_MinusLogProbMetric: 37.4779

Epoch 262: val_loss did not improve from 36.97815
196/196 - 54s - loss: 36.7804 - MinusLogProbMetric: 36.7804 - val_loss: 37.4779 - val_MinusLogProbMetric: 37.4779 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 263/1000
2023-10-30 21:07:30.710 
Epoch 263/1000 
	 loss: 36.7855, MinusLogProbMetric: 36.7855, val_loss: 37.0625, val_MinusLogProbMetric: 37.0625

Epoch 263: val_loss did not improve from 36.97815
196/196 - 51s - loss: 36.7855 - MinusLogProbMetric: 36.7855 - val_loss: 37.0625 - val_MinusLogProbMetric: 37.0625 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 264/1000
2023-10-30 21:08:28.910 
Epoch 264/1000 
	 loss: 36.7482, MinusLogProbMetric: 36.7482, val_loss: 37.2592, val_MinusLogProbMetric: 37.2592

Epoch 264: val_loss did not improve from 36.97815
196/196 - 58s - loss: 36.7482 - MinusLogProbMetric: 36.7482 - val_loss: 37.2592 - val_MinusLogProbMetric: 37.2592 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 265/1000
2023-10-30 21:09:19.240 
Epoch 265/1000 
	 loss: 36.7342, MinusLogProbMetric: 36.7342, val_loss: 37.0203, val_MinusLogProbMetric: 37.0203

Epoch 265: val_loss did not improve from 36.97815
196/196 - 50s - loss: 36.7342 - MinusLogProbMetric: 36.7342 - val_loss: 37.0203 - val_MinusLogProbMetric: 37.0203 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 266/1000
2023-10-30 21:10:09.357 
Epoch 266/1000 
	 loss: 36.7023, MinusLogProbMetric: 36.7023, val_loss: 36.9556, val_MinusLogProbMetric: 36.9556

Epoch 266: val_loss improved from 36.97815 to 36.95558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 36.7023 - MinusLogProbMetric: 36.7023 - val_loss: 36.9556 - val_MinusLogProbMetric: 36.9556 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 267/1000
2023-10-30 21:11:06.301 
Epoch 267/1000 
	 loss: 36.6767, MinusLogProbMetric: 36.6767, val_loss: 37.1094, val_MinusLogProbMetric: 37.1094

Epoch 267: val_loss did not improve from 36.95558
196/196 - 56s - loss: 36.6767 - MinusLogProbMetric: 36.6767 - val_loss: 37.1094 - val_MinusLogProbMetric: 37.1094 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 268/1000
2023-10-30 21:11:57.304 
Epoch 268/1000 
	 loss: 36.6533, MinusLogProbMetric: 36.6533, val_loss: 37.0628, val_MinusLogProbMetric: 37.0628

Epoch 268: val_loss did not improve from 36.95558
196/196 - 51s - loss: 36.6533 - MinusLogProbMetric: 36.6533 - val_loss: 37.0628 - val_MinusLogProbMetric: 37.0628 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 269/1000
2023-10-30 21:12:54.738 
Epoch 269/1000 
	 loss: 36.7188, MinusLogProbMetric: 36.7188, val_loss: 36.9328, val_MinusLogProbMetric: 36.9328

Epoch 269: val_loss improved from 36.95558 to 36.93281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 36.7188 - MinusLogProbMetric: 36.7188 - val_loss: 36.9328 - val_MinusLogProbMetric: 36.9328 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 270/1000
2023-10-30 21:13:45.243 
Epoch 270/1000 
	 loss: 36.6487, MinusLogProbMetric: 36.6487, val_loss: 36.8444, val_MinusLogProbMetric: 36.8444

Epoch 270: val_loss improved from 36.93281 to 36.84440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 36.6487 - MinusLogProbMetric: 36.6487 - val_loss: 36.8444 - val_MinusLogProbMetric: 36.8444 - lr: 4.1152e-06 - 50s/epoch - 258ms/step
Epoch 271/1000
2023-10-30 21:14:37.714 
Epoch 271/1000 
	 loss: 36.6360, MinusLogProbMetric: 36.6360, val_loss: 36.9347, val_MinusLogProbMetric: 36.9347

Epoch 271: val_loss did not improve from 36.84440
196/196 - 52s - loss: 36.6360 - MinusLogProbMetric: 36.6360 - val_loss: 36.9347 - val_MinusLogProbMetric: 36.9347 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 272/1000
2023-10-30 21:15:32.880 
Epoch 272/1000 
	 loss: 36.6084, MinusLogProbMetric: 36.6084, val_loss: 37.2669, val_MinusLogProbMetric: 37.2669

Epoch 272: val_loss did not improve from 36.84440
196/196 - 55s - loss: 36.6084 - MinusLogProbMetric: 36.6084 - val_loss: 37.2669 - val_MinusLogProbMetric: 37.2669 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 273/1000
2023-10-30 21:16:22.232 
Epoch 273/1000 
	 loss: 36.6179, MinusLogProbMetric: 36.6179, val_loss: 36.8932, val_MinusLogProbMetric: 36.8932

Epoch 273: val_loss did not improve from 36.84440
196/196 - 49s - loss: 36.6179 - MinusLogProbMetric: 36.6179 - val_loss: 36.8932 - val_MinusLogProbMetric: 36.8932 - lr: 4.1152e-06 - 49s/epoch - 252ms/step
Epoch 274/1000
2023-10-30 21:17:13.189 
Epoch 274/1000 
	 loss: 36.5981, MinusLogProbMetric: 36.5981, val_loss: 37.0547, val_MinusLogProbMetric: 37.0547

Epoch 274: val_loss did not improve from 36.84440
196/196 - 51s - loss: 36.5981 - MinusLogProbMetric: 36.5981 - val_loss: 37.0547 - val_MinusLogProbMetric: 37.0547 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 275/1000
2023-10-30 21:18:07.887 
Epoch 275/1000 
	 loss: 36.6130, MinusLogProbMetric: 36.6130, val_loss: 36.8755, val_MinusLogProbMetric: 36.8755

Epoch 275: val_loss did not improve from 36.84440
196/196 - 55s - loss: 36.6130 - MinusLogProbMetric: 36.6130 - val_loss: 36.8755 - val_MinusLogProbMetric: 36.8755 - lr: 4.1152e-06 - 55s/epoch - 279ms/step
Epoch 276/1000
2023-10-30 21:18:59.339 
Epoch 276/1000 
	 loss: 36.5868, MinusLogProbMetric: 36.5868, val_loss: 37.2849, val_MinusLogProbMetric: 37.2849

Epoch 276: val_loss did not improve from 36.84440
196/196 - 51s - loss: 36.5868 - MinusLogProbMetric: 36.5868 - val_loss: 37.2849 - val_MinusLogProbMetric: 37.2849 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 277/1000
2023-10-30 21:19:52.094 
Epoch 277/1000 
	 loss: 36.6149, MinusLogProbMetric: 36.6149, val_loss: 36.9384, val_MinusLogProbMetric: 36.9384

Epoch 277: val_loss did not improve from 36.84440
196/196 - 53s - loss: 36.6149 - MinusLogProbMetric: 36.6149 - val_loss: 36.9384 - val_MinusLogProbMetric: 36.9384 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 278/1000
2023-10-30 21:20:41.767 
Epoch 278/1000 
	 loss: 36.5669, MinusLogProbMetric: 36.5669, val_loss: 36.9897, val_MinusLogProbMetric: 36.9897

Epoch 278: val_loss did not improve from 36.84440
196/196 - 50s - loss: 36.5669 - MinusLogProbMetric: 36.5669 - val_loss: 36.9897 - val_MinusLogProbMetric: 36.9897 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 279/1000
2023-10-30 21:21:33.505 
Epoch 279/1000 
	 loss: 36.5652, MinusLogProbMetric: 36.5652, val_loss: 37.0294, val_MinusLogProbMetric: 37.0294

Epoch 279: val_loss did not improve from 36.84440
196/196 - 52s - loss: 36.5652 - MinusLogProbMetric: 36.5652 - val_loss: 37.0294 - val_MinusLogProbMetric: 37.0294 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 280/1000
2023-10-30 21:22:27.695 
Epoch 280/1000 
	 loss: 36.5097, MinusLogProbMetric: 36.5097, val_loss: 36.7676, val_MinusLogProbMetric: 36.7676

Epoch 280: val_loss improved from 36.84440 to 36.76757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 36.5097 - MinusLogProbMetric: 36.5097 - val_loss: 36.7676 - val_MinusLogProbMetric: 36.7676 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 281/1000
2023-10-30 21:23:18.131 
Epoch 281/1000 
	 loss: 36.5009, MinusLogProbMetric: 36.5009, val_loss: 37.3326, val_MinusLogProbMetric: 37.3326

Epoch 281: val_loss did not improve from 36.76757
196/196 - 50s - loss: 36.5009 - MinusLogProbMetric: 36.5009 - val_loss: 37.3326 - val_MinusLogProbMetric: 37.3326 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 282/1000
2023-10-30 21:24:10.215 
Epoch 282/1000 
	 loss: 36.5446, MinusLogProbMetric: 36.5446, val_loss: 36.7595, val_MinusLogProbMetric: 36.7595

Epoch 282: val_loss improved from 36.76757 to 36.75950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 36.5446 - MinusLogProbMetric: 36.5446 - val_loss: 36.7595 - val_MinusLogProbMetric: 36.7595 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 283/1000
2023-10-30 21:25:09.988 
Epoch 283/1000 
	 loss: 36.4504, MinusLogProbMetric: 36.4504, val_loss: 36.6958, val_MinusLogProbMetric: 36.6958

Epoch 283: val_loss improved from 36.75950 to 36.69579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 36.4504 - MinusLogProbMetric: 36.4504 - val_loss: 36.6958 - val_MinusLogProbMetric: 36.6958 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 284/1000
2023-10-30 21:26:01.092 
Epoch 284/1000 
	 loss: 36.4279, MinusLogProbMetric: 36.4279, val_loss: 36.8138, val_MinusLogProbMetric: 36.8138

Epoch 284: val_loss did not improve from 36.69579
196/196 - 50s - loss: 36.4279 - MinusLogProbMetric: 36.4279 - val_loss: 36.8138 - val_MinusLogProbMetric: 36.8138 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 285/1000
2023-10-30 21:26:52.913 
Epoch 285/1000 
	 loss: 36.4371, MinusLogProbMetric: 36.4371, val_loss: 36.8437, val_MinusLogProbMetric: 36.8437

Epoch 285: val_loss did not improve from 36.69579
196/196 - 52s - loss: 36.4371 - MinusLogProbMetric: 36.4371 - val_loss: 36.8437 - val_MinusLogProbMetric: 36.8437 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 286/1000
2023-10-30 21:27:50.109 
Epoch 286/1000 
	 loss: 36.5973, MinusLogProbMetric: 36.5973, val_loss: 36.8133, val_MinusLogProbMetric: 36.8133

Epoch 286: val_loss did not improve from 36.69579
196/196 - 57s - loss: 36.5973 - MinusLogProbMetric: 36.5973 - val_loss: 36.8133 - val_MinusLogProbMetric: 36.8133 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 287/1000
2023-10-30 21:28:39.986 
Epoch 287/1000 
	 loss: 36.4239, MinusLogProbMetric: 36.4239, val_loss: 36.7854, val_MinusLogProbMetric: 36.7854

Epoch 287: val_loss did not improve from 36.69579
196/196 - 50s - loss: 36.4239 - MinusLogProbMetric: 36.4239 - val_loss: 36.7854 - val_MinusLogProbMetric: 36.7854 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 288/1000
2023-10-30 21:29:30.303 
Epoch 288/1000 
	 loss: 36.4280, MinusLogProbMetric: 36.4280, val_loss: 36.9293, val_MinusLogProbMetric: 36.9293

Epoch 288: val_loss did not improve from 36.69579
196/196 - 50s - loss: 36.4280 - MinusLogProbMetric: 36.4280 - val_loss: 36.9293 - val_MinusLogProbMetric: 36.9293 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 289/1000
2023-10-30 21:30:25.829 
Epoch 289/1000 
	 loss: 36.4225, MinusLogProbMetric: 36.4225, val_loss: 36.5952, val_MinusLogProbMetric: 36.5952

Epoch 289: val_loss improved from 36.69579 to 36.59519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 36.4225 - MinusLogProbMetric: 36.4225 - val_loss: 36.5952 - val_MinusLogProbMetric: 36.5952 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 290/1000
2023-10-30 21:31:27.931 
Epoch 290/1000 
	 loss: 36.3456, MinusLogProbMetric: 36.3456, val_loss: 36.8123, val_MinusLogProbMetric: 36.8123

Epoch 290: val_loss did not improve from 36.59519
196/196 - 60s - loss: 36.3456 - MinusLogProbMetric: 36.3456 - val_loss: 36.8123 - val_MinusLogProbMetric: 36.8123 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 291/1000
2023-10-30 21:32:31.434 
Epoch 291/1000 
	 loss: 36.3387, MinusLogProbMetric: 36.3387, val_loss: 36.5923, val_MinusLogProbMetric: 36.5923

Epoch 291: val_loss improved from 36.59519 to 36.59229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 36.3387 - MinusLogProbMetric: 36.3387 - val_loss: 36.5923 - val_MinusLogProbMetric: 36.5923 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 292/1000
2023-10-30 21:33:36.931 
Epoch 292/1000 
	 loss: 36.3421, MinusLogProbMetric: 36.3421, val_loss: 36.8065, val_MinusLogProbMetric: 36.8065

Epoch 292: val_loss did not improve from 36.59229
196/196 - 64s - loss: 36.3421 - MinusLogProbMetric: 36.3421 - val_loss: 36.8065 - val_MinusLogProbMetric: 36.8065 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 293/1000
2023-10-30 21:34:36.455 
Epoch 293/1000 
	 loss: 36.3275, MinusLogProbMetric: 36.3275, val_loss: 36.6591, val_MinusLogProbMetric: 36.6591

Epoch 293: val_loss did not improve from 36.59229
196/196 - 60s - loss: 36.3275 - MinusLogProbMetric: 36.3275 - val_loss: 36.6591 - val_MinusLogProbMetric: 36.6591 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 294/1000
2023-10-30 21:35:40.219 
Epoch 294/1000 
	 loss: 36.3391, MinusLogProbMetric: 36.3391, val_loss: 36.6512, val_MinusLogProbMetric: 36.6512

Epoch 294: val_loss did not improve from 36.59229
196/196 - 64s - loss: 36.3391 - MinusLogProbMetric: 36.3391 - val_loss: 36.6512 - val_MinusLogProbMetric: 36.6512 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 295/1000
2023-10-30 21:36:45.895 
Epoch 295/1000 
	 loss: 44.5387, MinusLogProbMetric: 44.5387, val_loss: 42.5430, val_MinusLogProbMetric: 42.5430

Epoch 295: val_loss did not improve from 36.59229
196/196 - 66s - loss: 44.5387 - MinusLogProbMetric: 44.5387 - val_loss: 42.5430 - val_MinusLogProbMetric: 42.5430 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 296/1000
2023-10-30 21:37:50.332 
Epoch 296/1000 
	 loss: 40.9937, MinusLogProbMetric: 40.9937, val_loss: 40.5705, val_MinusLogProbMetric: 40.5705

Epoch 296: val_loss did not improve from 36.59229
196/196 - 64s - loss: 40.9937 - MinusLogProbMetric: 40.9937 - val_loss: 40.5705 - val_MinusLogProbMetric: 40.5705 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 297/1000
2023-10-30 21:38:53.748 
Epoch 297/1000 
	 loss: 39.5418, MinusLogProbMetric: 39.5418, val_loss: 39.2485, val_MinusLogProbMetric: 39.2485

Epoch 297: val_loss did not improve from 36.59229
196/196 - 63s - loss: 39.5418 - MinusLogProbMetric: 39.5418 - val_loss: 39.2485 - val_MinusLogProbMetric: 39.2485 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 298/1000
2023-10-30 21:39:57.705 
Epoch 298/1000 
	 loss: 38.7036, MinusLogProbMetric: 38.7036, val_loss: 38.7842, val_MinusLogProbMetric: 38.7842

Epoch 298: val_loss did not improve from 36.59229
196/196 - 64s - loss: 38.7036 - MinusLogProbMetric: 38.7036 - val_loss: 38.7842 - val_MinusLogProbMetric: 38.7842 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 299/1000
2023-10-30 21:41:01.552 
Epoch 299/1000 
	 loss: 38.3648, MinusLogProbMetric: 38.3648, val_loss: 38.5838, val_MinusLogProbMetric: 38.5838

Epoch 299: val_loss did not improve from 36.59229
196/196 - 64s - loss: 38.3648 - MinusLogProbMetric: 38.3648 - val_loss: 38.5838 - val_MinusLogProbMetric: 38.5838 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 300/1000
2023-10-30 21:42:05.317 
Epoch 300/1000 
	 loss: 37.9509, MinusLogProbMetric: 37.9509, val_loss: 37.7559, val_MinusLogProbMetric: 37.7559

Epoch 300: val_loss did not improve from 36.59229
196/196 - 64s - loss: 37.9509 - MinusLogProbMetric: 37.9509 - val_loss: 37.7559 - val_MinusLogProbMetric: 37.7559 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 301/1000
2023-10-30 21:43:07.471 
Epoch 301/1000 
	 loss: 37.4535, MinusLogProbMetric: 37.4535, val_loss: 37.7007, val_MinusLogProbMetric: 37.7007

Epoch 301: val_loss did not improve from 36.59229
196/196 - 62s - loss: 37.4535 - MinusLogProbMetric: 37.4535 - val_loss: 37.7007 - val_MinusLogProbMetric: 37.7007 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 302/1000
2023-10-30 21:44:11.721 
Epoch 302/1000 
	 loss: 37.3040, MinusLogProbMetric: 37.3040, val_loss: 37.4906, val_MinusLogProbMetric: 37.4906

Epoch 302: val_loss did not improve from 36.59229
196/196 - 64s - loss: 37.3040 - MinusLogProbMetric: 37.3040 - val_loss: 37.4906 - val_MinusLogProbMetric: 37.4906 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 303/1000
2023-10-30 21:45:15.143 
Epoch 303/1000 
	 loss: 37.1895, MinusLogProbMetric: 37.1895, val_loss: 37.4675, val_MinusLogProbMetric: 37.4675

Epoch 303: val_loss did not improve from 36.59229
196/196 - 63s - loss: 37.1895 - MinusLogProbMetric: 37.1895 - val_loss: 37.4675 - val_MinusLogProbMetric: 37.4675 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 304/1000
2023-10-30 21:46:20.782 
Epoch 304/1000 
	 loss: 37.1355, MinusLogProbMetric: 37.1355, val_loss: 37.5052, val_MinusLogProbMetric: 37.5052

Epoch 304: val_loss did not improve from 36.59229
196/196 - 66s - loss: 37.1355 - MinusLogProbMetric: 37.1355 - val_loss: 37.5052 - val_MinusLogProbMetric: 37.5052 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 305/1000
2023-10-30 21:47:24.754 
Epoch 305/1000 
	 loss: 37.0394, MinusLogProbMetric: 37.0394, val_loss: 37.3338, val_MinusLogProbMetric: 37.3338

Epoch 305: val_loss did not improve from 36.59229
196/196 - 64s - loss: 37.0394 - MinusLogProbMetric: 37.0394 - val_loss: 37.3338 - val_MinusLogProbMetric: 37.3338 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 306/1000
2023-10-30 21:48:27.412 
Epoch 306/1000 
	 loss: 36.9823, MinusLogProbMetric: 36.9823, val_loss: 37.2472, val_MinusLogProbMetric: 37.2472

Epoch 306: val_loss did not improve from 36.59229
196/196 - 63s - loss: 36.9823 - MinusLogProbMetric: 36.9823 - val_loss: 37.2472 - val_MinusLogProbMetric: 37.2472 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 307/1000
2023-10-30 21:49:29.121 
Epoch 307/1000 
	 loss: 36.9448, MinusLogProbMetric: 36.9448, val_loss: 37.1194, val_MinusLogProbMetric: 37.1194

Epoch 307: val_loss did not improve from 36.59229
196/196 - 62s - loss: 36.9448 - MinusLogProbMetric: 36.9448 - val_loss: 37.1194 - val_MinusLogProbMetric: 37.1194 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 308/1000
2023-10-30 21:50:31.755 
Epoch 308/1000 
	 loss: 36.8884, MinusLogProbMetric: 36.8884, val_loss: 37.3426, val_MinusLogProbMetric: 37.3426

Epoch 308: val_loss did not improve from 36.59229
196/196 - 63s - loss: 36.8884 - MinusLogProbMetric: 36.8884 - val_loss: 37.3426 - val_MinusLogProbMetric: 37.3426 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 309/1000
2023-10-30 21:51:35.965 
Epoch 309/1000 
	 loss: 36.5754, MinusLogProbMetric: 36.5754, val_loss: 36.7942, val_MinusLogProbMetric: 36.7942

Epoch 309: val_loss did not improve from 36.59229
196/196 - 64s - loss: 36.5754 - MinusLogProbMetric: 36.5754 - val_loss: 36.7942 - val_MinusLogProbMetric: 36.7942 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 310/1000
2023-10-30 21:52:41.489 
Epoch 310/1000 
	 loss: 36.3080, MinusLogProbMetric: 36.3080, val_loss: 36.5655, val_MinusLogProbMetric: 36.5655

Epoch 310: val_loss improved from 36.59229 to 36.56546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 36.3080 - MinusLogProbMetric: 36.3080 - val_loss: 36.5655 - val_MinusLogProbMetric: 36.5655 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 311/1000
2023-10-30 21:53:44.749 
Epoch 311/1000 
	 loss: 36.2400, MinusLogProbMetric: 36.2400, val_loss: 36.5513, val_MinusLogProbMetric: 36.5513

Epoch 311: val_loss improved from 36.56546 to 36.55135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 36.2400 - MinusLogProbMetric: 36.2400 - val_loss: 36.5513 - val_MinusLogProbMetric: 36.5513 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 312/1000
2023-10-30 21:54:46.206 
Epoch 312/1000 
	 loss: 36.2530, MinusLogProbMetric: 36.2530, val_loss: 36.4394, val_MinusLogProbMetric: 36.4394

Epoch 312: val_loss improved from 36.55135 to 36.43938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 36.2530 - MinusLogProbMetric: 36.2530 - val_loss: 36.4394 - val_MinusLogProbMetric: 36.4394 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 313/1000
2023-10-30 21:55:50.959 
Epoch 313/1000 
	 loss: 36.2057, MinusLogProbMetric: 36.2057, val_loss: 36.6261, val_MinusLogProbMetric: 36.6261

Epoch 313: val_loss did not improve from 36.43938
196/196 - 64s - loss: 36.2057 - MinusLogProbMetric: 36.2057 - val_loss: 36.6261 - val_MinusLogProbMetric: 36.6261 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 314/1000
2023-10-30 21:56:55.471 
Epoch 314/1000 
	 loss: 36.1837, MinusLogProbMetric: 36.1837, val_loss: 36.6607, val_MinusLogProbMetric: 36.6607

Epoch 314: val_loss did not improve from 36.43938
196/196 - 65s - loss: 36.1837 - MinusLogProbMetric: 36.1837 - val_loss: 36.6607 - val_MinusLogProbMetric: 36.6607 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 315/1000
2023-10-30 21:57:59.087 
Epoch 315/1000 
	 loss: 36.2590, MinusLogProbMetric: 36.2590, val_loss: 36.6441, val_MinusLogProbMetric: 36.6441

Epoch 315: val_loss did not improve from 36.43938
196/196 - 64s - loss: 36.2590 - MinusLogProbMetric: 36.2590 - val_loss: 36.6441 - val_MinusLogProbMetric: 36.6441 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 316/1000
2023-10-30 21:59:02.572 
Epoch 316/1000 
	 loss: 36.1678, MinusLogProbMetric: 36.1678, val_loss: 36.5005, val_MinusLogProbMetric: 36.5005

Epoch 316: val_loss did not improve from 36.43938
196/196 - 63s - loss: 36.1678 - MinusLogProbMetric: 36.1678 - val_loss: 36.5005 - val_MinusLogProbMetric: 36.5005 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 317/1000
2023-10-30 22:00:06.523 
Epoch 317/1000 
	 loss: 36.1416, MinusLogProbMetric: 36.1416, val_loss: 36.5423, val_MinusLogProbMetric: 36.5423

Epoch 317: val_loss did not improve from 36.43938
196/196 - 64s - loss: 36.1416 - MinusLogProbMetric: 36.1416 - val_loss: 36.5423 - val_MinusLogProbMetric: 36.5423 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 318/1000
2023-10-30 22:01:11.034 
Epoch 318/1000 
	 loss: 36.1018, MinusLogProbMetric: 36.1018, val_loss: 36.9132, val_MinusLogProbMetric: 36.9132

Epoch 318: val_loss did not improve from 36.43938
196/196 - 65s - loss: 36.1018 - MinusLogProbMetric: 36.1018 - val_loss: 36.9132 - val_MinusLogProbMetric: 36.9132 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 319/1000
2023-10-30 22:02:16.010 
Epoch 319/1000 
	 loss: 36.1180, MinusLogProbMetric: 36.1180, val_loss: 36.4192, val_MinusLogProbMetric: 36.4192

Epoch 319: val_loss improved from 36.43938 to 36.41916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 36.1180 - MinusLogProbMetric: 36.1180 - val_loss: 36.4192 - val_MinusLogProbMetric: 36.4192 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 320/1000
2023-10-30 22:03:21.415 
Epoch 320/1000 
	 loss: 36.0694, MinusLogProbMetric: 36.0694, val_loss: 36.2931, val_MinusLogProbMetric: 36.2931

Epoch 320: val_loss improved from 36.41916 to 36.29310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 36.0694 - MinusLogProbMetric: 36.0694 - val_loss: 36.2931 - val_MinusLogProbMetric: 36.2931 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 321/1000
2023-10-30 22:04:26.469 
Epoch 321/1000 
	 loss: 36.0637, MinusLogProbMetric: 36.0637, val_loss: 36.6852, val_MinusLogProbMetric: 36.6852

Epoch 321: val_loss did not improve from 36.29310
196/196 - 64s - loss: 36.0637 - MinusLogProbMetric: 36.0637 - val_loss: 36.6852 - val_MinusLogProbMetric: 36.6852 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 322/1000
2023-10-30 22:05:31.076 
Epoch 322/1000 
	 loss: 36.0609, MinusLogProbMetric: 36.0609, val_loss: 36.3939, val_MinusLogProbMetric: 36.3939

Epoch 322: val_loss did not improve from 36.29310
196/196 - 65s - loss: 36.0609 - MinusLogProbMetric: 36.0609 - val_loss: 36.3939 - val_MinusLogProbMetric: 36.3939 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 323/1000
2023-10-30 22:06:35.184 
Epoch 323/1000 
	 loss: 35.9671, MinusLogProbMetric: 35.9671, val_loss: 36.3789, val_MinusLogProbMetric: 36.3789

Epoch 323: val_loss did not improve from 36.29310
196/196 - 64s - loss: 35.9671 - MinusLogProbMetric: 35.9671 - val_loss: 36.3789 - val_MinusLogProbMetric: 36.3789 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 324/1000
2023-10-30 22:07:40.409 
Epoch 324/1000 
	 loss: 36.0159, MinusLogProbMetric: 36.0159, val_loss: 36.4801, val_MinusLogProbMetric: 36.4801

Epoch 324: val_loss did not improve from 36.29310
196/196 - 65s - loss: 36.0159 - MinusLogProbMetric: 36.0159 - val_loss: 36.4801 - val_MinusLogProbMetric: 36.4801 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 325/1000
2023-10-30 22:08:45.632 
Epoch 325/1000 
	 loss: 36.0055, MinusLogProbMetric: 36.0055, val_loss: 36.3734, val_MinusLogProbMetric: 36.3734

Epoch 325: val_loss did not improve from 36.29310
196/196 - 65s - loss: 36.0055 - MinusLogProbMetric: 36.0055 - val_loss: 36.3734 - val_MinusLogProbMetric: 36.3734 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 326/1000
2023-10-30 22:09:47.644 
Epoch 326/1000 
	 loss: 35.9728, MinusLogProbMetric: 35.9728, val_loss: 36.3429, val_MinusLogProbMetric: 36.3429

Epoch 326: val_loss did not improve from 36.29310
196/196 - 62s - loss: 35.9728 - MinusLogProbMetric: 35.9728 - val_loss: 36.3429 - val_MinusLogProbMetric: 36.3429 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 327/1000
2023-10-30 22:10:52.000 
Epoch 327/1000 
	 loss: 35.9581, MinusLogProbMetric: 35.9581, val_loss: 36.2770, val_MinusLogProbMetric: 36.2770

Epoch 327: val_loss improved from 36.29310 to 36.27701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.9581 - MinusLogProbMetric: 35.9581 - val_loss: 36.2770 - val_MinusLogProbMetric: 36.2770 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 328/1000
2023-10-30 22:11:57.315 
Epoch 328/1000 
	 loss: 35.9678, MinusLogProbMetric: 35.9678, val_loss: 36.2455, val_MinusLogProbMetric: 36.2455

Epoch 328: val_loss improved from 36.27701 to 36.24552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.9678 - MinusLogProbMetric: 35.9678 - val_loss: 36.2455 - val_MinusLogProbMetric: 36.2455 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 329/1000
2023-10-30 22:13:02.722 
Epoch 329/1000 
	 loss: 35.9253, MinusLogProbMetric: 35.9253, val_loss: 36.2480, val_MinusLogProbMetric: 36.2480

Epoch 329: val_loss did not improve from 36.24552
196/196 - 64s - loss: 35.9253 - MinusLogProbMetric: 35.9253 - val_loss: 36.2480 - val_MinusLogProbMetric: 36.2480 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 330/1000
2023-10-30 22:14:06.861 
Epoch 330/1000 
	 loss: 35.8982, MinusLogProbMetric: 35.8982, val_loss: 36.3943, val_MinusLogProbMetric: 36.3943

Epoch 330: val_loss did not improve from 36.24552
196/196 - 64s - loss: 35.8982 - MinusLogProbMetric: 35.8982 - val_loss: 36.3943 - val_MinusLogProbMetric: 36.3943 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 331/1000
2023-10-30 22:15:11.280 
Epoch 331/1000 
	 loss: 35.9414, MinusLogProbMetric: 35.9414, val_loss: 36.1672, val_MinusLogProbMetric: 36.1672

Epoch 331: val_loss improved from 36.24552 to 36.16720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.9414 - MinusLogProbMetric: 35.9414 - val_loss: 36.1672 - val_MinusLogProbMetric: 36.1672 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 332/1000
2023-10-30 22:16:16.192 
Epoch 332/1000 
	 loss: 35.8724, MinusLogProbMetric: 35.8724, val_loss: 36.4735, val_MinusLogProbMetric: 36.4735

Epoch 332: val_loss did not improve from 36.16720
196/196 - 64s - loss: 35.8724 - MinusLogProbMetric: 35.8724 - val_loss: 36.4735 - val_MinusLogProbMetric: 36.4735 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 333/1000
2023-10-30 22:17:18.163 
Epoch 333/1000 
	 loss: 35.8678, MinusLogProbMetric: 35.8678, val_loss: 36.3039, val_MinusLogProbMetric: 36.3039

Epoch 333: val_loss did not improve from 36.16720
196/196 - 62s - loss: 35.8678 - MinusLogProbMetric: 35.8678 - val_loss: 36.3039 - val_MinusLogProbMetric: 36.3039 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 334/1000
2023-10-30 22:18:21.746 
Epoch 334/1000 
	 loss: 35.8654, MinusLogProbMetric: 35.8654, val_loss: 36.2909, val_MinusLogProbMetric: 36.2909

Epoch 334: val_loss did not improve from 36.16720
196/196 - 64s - loss: 35.8654 - MinusLogProbMetric: 35.8654 - val_loss: 36.2909 - val_MinusLogProbMetric: 36.2909 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 335/1000
2023-10-30 22:19:26.766 
Epoch 335/1000 
	 loss: 35.8405, MinusLogProbMetric: 35.8405, val_loss: 36.1919, val_MinusLogProbMetric: 36.1919

Epoch 335: val_loss did not improve from 36.16720
196/196 - 65s - loss: 35.8405 - MinusLogProbMetric: 35.8405 - val_loss: 36.1919 - val_MinusLogProbMetric: 36.1919 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 336/1000
2023-10-30 22:20:29.029 
Epoch 336/1000 
	 loss: 35.8228, MinusLogProbMetric: 35.8228, val_loss: 36.3116, val_MinusLogProbMetric: 36.3116

Epoch 336: val_loss did not improve from 36.16720
196/196 - 62s - loss: 35.8228 - MinusLogProbMetric: 35.8228 - val_loss: 36.3116 - val_MinusLogProbMetric: 36.3116 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 337/1000
2023-10-30 22:21:33.955 
Epoch 337/1000 
	 loss: 35.8166, MinusLogProbMetric: 35.8166, val_loss: 36.2623, val_MinusLogProbMetric: 36.2623

Epoch 337: val_loss did not improve from 36.16720
196/196 - 65s - loss: 35.8166 - MinusLogProbMetric: 35.8166 - val_loss: 36.2623 - val_MinusLogProbMetric: 36.2623 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 338/1000
2023-10-30 22:22:37.158 
Epoch 338/1000 
	 loss: 35.7907, MinusLogProbMetric: 35.7907, val_loss: 36.0461, val_MinusLogProbMetric: 36.0461

Epoch 338: val_loss improved from 36.16720 to 36.04613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.7907 - MinusLogProbMetric: 35.7907 - val_loss: 36.0461 - val_MinusLogProbMetric: 36.0461 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 339/1000
2023-10-30 22:23:41.605 
Epoch 339/1000 
	 loss: 35.8082, MinusLogProbMetric: 35.8082, val_loss: 36.3076, val_MinusLogProbMetric: 36.3076

Epoch 339: val_loss did not improve from 36.04613
196/196 - 63s - loss: 35.8082 - MinusLogProbMetric: 35.8082 - val_loss: 36.3076 - val_MinusLogProbMetric: 36.3076 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 340/1000
2023-10-30 22:24:46.016 
Epoch 340/1000 
	 loss: 35.7970, MinusLogProbMetric: 35.7970, val_loss: 36.2411, val_MinusLogProbMetric: 36.2411

Epoch 340: val_loss did not improve from 36.04613
196/196 - 64s - loss: 35.7970 - MinusLogProbMetric: 35.7970 - val_loss: 36.2411 - val_MinusLogProbMetric: 36.2411 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 341/1000
2023-10-30 22:25:50.073 
Epoch 341/1000 
	 loss: 35.7720, MinusLogProbMetric: 35.7720, val_loss: 36.0690, val_MinusLogProbMetric: 36.0690

Epoch 341: val_loss did not improve from 36.04613
196/196 - 64s - loss: 35.7720 - MinusLogProbMetric: 35.7720 - val_loss: 36.0690 - val_MinusLogProbMetric: 36.0690 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 342/1000
2023-10-30 22:26:54.169 
Epoch 342/1000 
	 loss: 35.7539, MinusLogProbMetric: 35.7539, val_loss: 36.7792, val_MinusLogProbMetric: 36.7792

Epoch 342: val_loss did not improve from 36.04613
196/196 - 64s - loss: 35.7539 - MinusLogProbMetric: 35.7539 - val_loss: 36.7792 - val_MinusLogProbMetric: 36.7792 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 343/1000
2023-10-30 22:27:58.022 
Epoch 343/1000 
	 loss: 35.8041, MinusLogProbMetric: 35.8041, val_loss: 36.1127, val_MinusLogProbMetric: 36.1127

Epoch 343: val_loss did not improve from 36.04613
196/196 - 64s - loss: 35.8041 - MinusLogProbMetric: 35.8041 - val_loss: 36.1127 - val_MinusLogProbMetric: 36.1127 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 344/1000
2023-10-30 22:29:00.420 
Epoch 344/1000 
	 loss: 35.7398, MinusLogProbMetric: 35.7398, val_loss: 36.1582, val_MinusLogProbMetric: 36.1582

Epoch 344: val_loss did not improve from 36.04613
196/196 - 62s - loss: 35.7398 - MinusLogProbMetric: 35.7398 - val_loss: 36.1582 - val_MinusLogProbMetric: 36.1582 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 345/1000
2023-10-30 22:30:02.011 
Epoch 345/1000 
	 loss: 35.7397, MinusLogProbMetric: 35.7397, val_loss: 36.4300, val_MinusLogProbMetric: 36.4300

Epoch 345: val_loss did not improve from 36.04613
196/196 - 62s - loss: 35.7397 - MinusLogProbMetric: 35.7397 - val_loss: 36.4300 - val_MinusLogProbMetric: 36.4300 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 346/1000
2023-10-30 22:31:04.191 
Epoch 346/1000 
	 loss: 35.7500, MinusLogProbMetric: 35.7500, val_loss: 36.0178, val_MinusLogProbMetric: 36.0178

Epoch 346: val_loss improved from 36.04613 to 36.01778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 35.7500 - MinusLogProbMetric: 35.7500 - val_loss: 36.0178 - val_MinusLogProbMetric: 36.0178 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 347/1000
2023-10-30 22:32:10.468 
Epoch 347/1000 
	 loss: 35.6850, MinusLogProbMetric: 35.6850, val_loss: 36.1793, val_MinusLogProbMetric: 36.1793

Epoch 347: val_loss did not improve from 36.01778
196/196 - 65s - loss: 35.6850 - MinusLogProbMetric: 35.6850 - val_loss: 36.1793 - val_MinusLogProbMetric: 36.1793 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 348/1000
2023-10-30 22:33:13.734 
Epoch 348/1000 
	 loss: 35.7015, MinusLogProbMetric: 35.7015, val_loss: 36.0816, val_MinusLogProbMetric: 36.0816

Epoch 348: val_loss did not improve from 36.01778
196/196 - 63s - loss: 35.7015 - MinusLogProbMetric: 35.7015 - val_loss: 36.0816 - val_MinusLogProbMetric: 36.0816 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 349/1000
2023-10-30 22:34:17.149 
Epoch 349/1000 
	 loss: 35.6390, MinusLogProbMetric: 35.6390, val_loss: 36.1038, val_MinusLogProbMetric: 36.1038

Epoch 349: val_loss did not improve from 36.01778
196/196 - 63s - loss: 35.6390 - MinusLogProbMetric: 35.6390 - val_loss: 36.1038 - val_MinusLogProbMetric: 36.1038 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 350/1000
2023-10-30 22:35:21.305 
Epoch 350/1000 
	 loss: 35.6002, MinusLogProbMetric: 35.6002, val_loss: 35.9907, val_MinusLogProbMetric: 35.9907

Epoch 350: val_loss improved from 36.01778 to 35.99073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.6002 - MinusLogProbMetric: 35.6002 - val_loss: 35.9907 - val_MinusLogProbMetric: 35.9907 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 351/1000
2023-10-30 22:36:25.803 
Epoch 351/1000 
	 loss: 35.6721, MinusLogProbMetric: 35.6721, val_loss: 36.0775, val_MinusLogProbMetric: 36.0775

Epoch 351: val_loss did not improve from 35.99073
196/196 - 63s - loss: 35.6721 - MinusLogProbMetric: 35.6721 - val_loss: 36.0775 - val_MinusLogProbMetric: 36.0775 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 352/1000
2023-10-30 22:37:27.575 
Epoch 352/1000 
	 loss: 35.6384, MinusLogProbMetric: 35.6384, val_loss: 36.2090, val_MinusLogProbMetric: 36.2090

Epoch 352: val_loss did not improve from 35.99073
196/196 - 62s - loss: 35.6384 - MinusLogProbMetric: 35.6384 - val_loss: 36.2090 - val_MinusLogProbMetric: 36.2090 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 353/1000
2023-10-30 22:38:29.717 
Epoch 353/1000 
	 loss: 35.6206, MinusLogProbMetric: 35.6206, val_loss: 36.1221, val_MinusLogProbMetric: 36.1221

Epoch 353: val_loss did not improve from 35.99073
196/196 - 62s - loss: 35.6206 - MinusLogProbMetric: 35.6206 - val_loss: 36.1221 - val_MinusLogProbMetric: 36.1221 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 354/1000
2023-10-30 22:39:34.333 
Epoch 354/1000 
	 loss: 35.6293, MinusLogProbMetric: 35.6293, val_loss: 36.0266, val_MinusLogProbMetric: 36.0266

Epoch 354: val_loss did not improve from 35.99073
196/196 - 65s - loss: 35.6293 - MinusLogProbMetric: 35.6293 - val_loss: 36.0266 - val_MinusLogProbMetric: 36.0266 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 355/1000
2023-10-30 22:40:39.240 
Epoch 355/1000 
	 loss: 35.5710, MinusLogProbMetric: 35.5710, val_loss: 35.9904, val_MinusLogProbMetric: 35.9904

Epoch 355: val_loss improved from 35.99073 to 35.99043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 35.5710 - MinusLogProbMetric: 35.5710 - val_loss: 35.9904 - val_MinusLogProbMetric: 35.9904 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 356/1000
2023-10-30 22:41:42.397 
Epoch 356/1000 
	 loss: 35.5696, MinusLogProbMetric: 35.5696, val_loss: 35.8234, val_MinusLogProbMetric: 35.8234

Epoch 356: val_loss improved from 35.99043 to 35.82337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 35.5696 - MinusLogProbMetric: 35.5696 - val_loss: 35.8234 - val_MinusLogProbMetric: 35.8234 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 357/1000
2023-10-30 22:42:48.702 
Epoch 357/1000 
	 loss: 35.5787, MinusLogProbMetric: 35.5787, val_loss: 36.3117, val_MinusLogProbMetric: 36.3117

Epoch 357: val_loss did not improve from 35.82337
196/196 - 64s - loss: 35.5787 - MinusLogProbMetric: 35.5787 - val_loss: 36.3117 - val_MinusLogProbMetric: 36.3117 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 358/1000
2023-10-30 22:43:53.267 
Epoch 358/1000 
	 loss: 35.5686, MinusLogProbMetric: 35.5686, val_loss: 35.8508, val_MinusLogProbMetric: 35.8508

Epoch 358: val_loss did not improve from 35.82337
196/196 - 65s - loss: 35.5686 - MinusLogProbMetric: 35.5686 - val_loss: 35.8508 - val_MinusLogProbMetric: 35.8508 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 359/1000
2023-10-30 22:44:57.033 
Epoch 359/1000 
	 loss: 35.5408, MinusLogProbMetric: 35.5408, val_loss: 36.0919, val_MinusLogProbMetric: 36.0919

Epoch 359: val_loss did not improve from 35.82337
196/196 - 64s - loss: 35.5408 - MinusLogProbMetric: 35.5408 - val_loss: 36.0919 - val_MinusLogProbMetric: 36.0919 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 360/1000
2023-10-30 22:46:02.375 
Epoch 360/1000 
	 loss: 35.5338, MinusLogProbMetric: 35.5338, val_loss: 35.9687, val_MinusLogProbMetric: 35.9687

Epoch 360: val_loss did not improve from 35.82337
196/196 - 65s - loss: 35.5338 - MinusLogProbMetric: 35.5338 - val_loss: 35.9687 - val_MinusLogProbMetric: 35.9687 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 361/1000
2023-10-30 22:47:04.215 
Epoch 361/1000 
	 loss: 35.5767, MinusLogProbMetric: 35.5767, val_loss: 36.0020, val_MinusLogProbMetric: 36.0020

Epoch 361: val_loss did not improve from 35.82337
196/196 - 62s - loss: 35.5767 - MinusLogProbMetric: 35.5767 - val_loss: 36.0020 - val_MinusLogProbMetric: 36.0020 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 362/1000
2023-10-30 22:48:09.187 
Epoch 362/1000 
	 loss: 35.5507, MinusLogProbMetric: 35.5507, val_loss: 35.9217, val_MinusLogProbMetric: 35.9217

Epoch 362: val_loss did not improve from 35.82337
196/196 - 65s - loss: 35.5507 - MinusLogProbMetric: 35.5507 - val_loss: 35.9217 - val_MinusLogProbMetric: 35.9217 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 363/1000
2023-10-30 22:49:09.994 
Epoch 363/1000 
	 loss: 35.5270, MinusLogProbMetric: 35.5270, val_loss: 36.0301, val_MinusLogProbMetric: 36.0301

Epoch 363: val_loss did not improve from 35.82337
196/196 - 61s - loss: 35.5270 - MinusLogProbMetric: 35.5270 - val_loss: 36.0301 - val_MinusLogProbMetric: 36.0301 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 364/1000
2023-10-30 22:50:13.482 
Epoch 364/1000 
	 loss: 35.5273, MinusLogProbMetric: 35.5273, val_loss: 36.3097, val_MinusLogProbMetric: 36.3097

Epoch 364: val_loss did not improve from 35.82337
196/196 - 63s - loss: 35.5273 - MinusLogProbMetric: 35.5273 - val_loss: 36.3097 - val_MinusLogProbMetric: 36.3097 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 365/1000
2023-10-30 22:51:16.756 
Epoch 365/1000 
	 loss: 35.5181, MinusLogProbMetric: 35.5181, val_loss: 35.8699, val_MinusLogProbMetric: 35.8699

Epoch 365: val_loss did not improve from 35.82337
196/196 - 63s - loss: 35.5181 - MinusLogProbMetric: 35.5181 - val_loss: 35.8699 - val_MinusLogProbMetric: 35.8699 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 366/1000
2023-10-30 22:52:23.033 
Epoch 366/1000 
	 loss: 35.4725, MinusLogProbMetric: 35.4725, val_loss: 35.8384, val_MinusLogProbMetric: 35.8384

Epoch 366: val_loss did not improve from 35.82337
196/196 - 66s - loss: 35.4725 - MinusLogProbMetric: 35.4725 - val_loss: 35.8384 - val_MinusLogProbMetric: 35.8384 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 367/1000
2023-10-30 22:53:26.350 
Epoch 367/1000 
	 loss: 35.4431, MinusLogProbMetric: 35.4431, val_loss: 36.0314, val_MinusLogProbMetric: 36.0314

Epoch 367: val_loss did not improve from 35.82337
196/196 - 63s - loss: 35.4431 - MinusLogProbMetric: 35.4431 - val_loss: 36.0314 - val_MinusLogProbMetric: 36.0314 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 368/1000
2023-10-30 22:54:29.951 
Epoch 368/1000 
	 loss: 35.4544, MinusLogProbMetric: 35.4544, val_loss: 35.7590, val_MinusLogProbMetric: 35.7590

Epoch 368: val_loss improved from 35.82337 to 35.75903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.4544 - MinusLogProbMetric: 35.4544 - val_loss: 35.7590 - val_MinusLogProbMetric: 35.7590 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 369/1000
2023-10-30 22:55:35.294 
Epoch 369/1000 
	 loss: 35.4536, MinusLogProbMetric: 35.4536, val_loss: 35.7422, val_MinusLogProbMetric: 35.7422

Epoch 369: val_loss improved from 35.75903 to 35.74218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.4536 - MinusLogProbMetric: 35.4536 - val_loss: 35.7422 - val_MinusLogProbMetric: 35.7422 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 370/1000
2023-10-30 22:56:36.485 
Epoch 370/1000 
	 loss: 35.4507, MinusLogProbMetric: 35.4507, val_loss: 35.9207, val_MinusLogProbMetric: 35.9207

Epoch 370: val_loss did not improve from 35.74218
196/196 - 60s - loss: 35.4507 - MinusLogProbMetric: 35.4507 - val_loss: 35.9207 - val_MinusLogProbMetric: 35.9207 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 371/1000
2023-10-30 22:57:39.070 
Epoch 371/1000 
	 loss: 35.4323, MinusLogProbMetric: 35.4323, val_loss: 35.7537, val_MinusLogProbMetric: 35.7537

Epoch 371: val_loss did not improve from 35.74218
196/196 - 63s - loss: 35.4323 - MinusLogProbMetric: 35.4323 - val_loss: 35.7537 - val_MinusLogProbMetric: 35.7537 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 372/1000
2023-10-30 22:58:42.264 
Epoch 372/1000 
	 loss: 35.4131, MinusLogProbMetric: 35.4131, val_loss: 35.8274, val_MinusLogProbMetric: 35.8274

Epoch 372: val_loss did not improve from 35.74218
196/196 - 63s - loss: 35.4131 - MinusLogProbMetric: 35.4131 - val_loss: 35.8274 - val_MinusLogProbMetric: 35.8274 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 373/1000
2023-10-30 22:59:45.981 
Epoch 373/1000 
	 loss: 35.3842, MinusLogProbMetric: 35.3842, val_loss: 35.7242, val_MinusLogProbMetric: 35.7242

Epoch 373: val_loss improved from 35.74218 to 35.72425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.3842 - MinusLogProbMetric: 35.3842 - val_loss: 35.7242 - val_MinusLogProbMetric: 35.7242 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 374/1000
2023-10-30 23:00:51.385 
Epoch 374/1000 
	 loss: 35.3697, MinusLogProbMetric: 35.3697, val_loss: 35.8111, val_MinusLogProbMetric: 35.8111

Epoch 374: val_loss did not improve from 35.72425
196/196 - 65s - loss: 35.3697 - MinusLogProbMetric: 35.3697 - val_loss: 35.8111 - val_MinusLogProbMetric: 35.8111 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 375/1000
2023-10-30 23:01:55.869 
Epoch 375/1000 
	 loss: 35.3617, MinusLogProbMetric: 35.3617, val_loss: 35.7163, val_MinusLogProbMetric: 35.7163

Epoch 375: val_loss improved from 35.72425 to 35.71629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.3617 - MinusLogProbMetric: 35.3617 - val_loss: 35.7163 - val_MinusLogProbMetric: 35.7163 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 376/1000
2023-10-30 23:02:59.264 
Epoch 376/1000 
	 loss: 35.3611, MinusLogProbMetric: 35.3611, val_loss: 35.7603, val_MinusLogProbMetric: 35.7603

Epoch 376: val_loss did not improve from 35.71629
196/196 - 62s - loss: 35.3611 - MinusLogProbMetric: 35.3611 - val_loss: 35.7603 - val_MinusLogProbMetric: 35.7603 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 377/1000
2023-10-30 23:04:03.500 
Epoch 377/1000 
	 loss: 35.3303, MinusLogProbMetric: 35.3303, val_loss: 35.6804, val_MinusLogProbMetric: 35.6804

Epoch 377: val_loss improved from 35.71629 to 35.68042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.3303 - MinusLogProbMetric: 35.3303 - val_loss: 35.6804 - val_MinusLogProbMetric: 35.6804 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 378/1000
2023-10-30 23:05:08.807 
Epoch 378/1000 
	 loss: 35.3275, MinusLogProbMetric: 35.3275, val_loss: 35.8116, val_MinusLogProbMetric: 35.8116

Epoch 378: val_loss did not improve from 35.68042
196/196 - 64s - loss: 35.3275 - MinusLogProbMetric: 35.3275 - val_loss: 35.8116 - val_MinusLogProbMetric: 35.8116 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 379/1000
2023-10-30 23:06:14.701 
Epoch 379/1000 
	 loss: 35.3592, MinusLogProbMetric: 35.3592, val_loss: 35.6152, val_MinusLogProbMetric: 35.6152

Epoch 379: val_loss improved from 35.68042 to 35.61523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 35.3592 - MinusLogProbMetric: 35.3592 - val_loss: 35.6152 - val_MinusLogProbMetric: 35.6152 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 380/1000
2023-10-30 23:07:20.664 
Epoch 380/1000 
	 loss: 35.3157, MinusLogProbMetric: 35.3157, val_loss: 35.6162, val_MinusLogProbMetric: 35.6162

Epoch 380: val_loss did not improve from 35.61523
196/196 - 65s - loss: 35.3157 - MinusLogProbMetric: 35.3157 - val_loss: 35.6162 - val_MinusLogProbMetric: 35.6162 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 381/1000
2023-10-30 23:08:25.630 
Epoch 381/1000 
	 loss: 35.4016, MinusLogProbMetric: 35.4016, val_loss: 35.6672, val_MinusLogProbMetric: 35.6672

Epoch 381: val_loss did not improve from 35.61523
196/196 - 65s - loss: 35.4016 - MinusLogProbMetric: 35.4016 - val_loss: 35.6672 - val_MinusLogProbMetric: 35.6672 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 382/1000
2023-10-30 23:09:28.985 
Epoch 382/1000 
	 loss: 35.2923, MinusLogProbMetric: 35.2923, val_loss: 35.5821, val_MinusLogProbMetric: 35.5821

Epoch 382: val_loss improved from 35.61523 to 35.58207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 35.2923 - MinusLogProbMetric: 35.2923 - val_loss: 35.5821 - val_MinusLogProbMetric: 35.5821 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 383/1000
2023-10-30 23:10:30.030 
Epoch 383/1000 
	 loss: 35.2761, MinusLogProbMetric: 35.2761, val_loss: 35.6552, val_MinusLogProbMetric: 35.6552

Epoch 383: val_loss did not improve from 35.58207
196/196 - 60s - loss: 35.2761 - MinusLogProbMetric: 35.2761 - val_loss: 35.6552 - val_MinusLogProbMetric: 35.6552 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 384/1000
2023-10-30 23:11:36.482 
Epoch 384/1000 
	 loss: 35.2793, MinusLogProbMetric: 35.2793, val_loss: 35.7514, val_MinusLogProbMetric: 35.7514

Epoch 384: val_loss did not improve from 35.58207
196/196 - 66s - loss: 35.2793 - MinusLogProbMetric: 35.2793 - val_loss: 35.7514 - val_MinusLogProbMetric: 35.7514 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 385/1000
2023-10-30 23:12:36.968 
Epoch 385/1000 
	 loss: 35.2803, MinusLogProbMetric: 35.2803, val_loss: 35.6815, val_MinusLogProbMetric: 35.6815

Epoch 385: val_loss did not improve from 35.58207
196/196 - 60s - loss: 35.2803 - MinusLogProbMetric: 35.2803 - val_loss: 35.6815 - val_MinusLogProbMetric: 35.6815 - lr: 4.1152e-06 - 60s/epoch - 309ms/step
Epoch 386/1000
2023-10-30 23:13:38.482 
Epoch 386/1000 
	 loss: 35.2471, MinusLogProbMetric: 35.2471, val_loss: 35.5704, val_MinusLogProbMetric: 35.5704

Epoch 386: val_loss improved from 35.58207 to 35.57036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 35.2471 - MinusLogProbMetric: 35.2471 - val_loss: 35.5704 - val_MinusLogProbMetric: 35.5704 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 387/1000
2023-10-30 23:14:43.435 
Epoch 387/1000 
	 loss: 35.2346, MinusLogProbMetric: 35.2346, val_loss: 35.6117, val_MinusLogProbMetric: 35.6117

Epoch 387: val_loss did not improve from 35.57036
196/196 - 64s - loss: 35.2346 - MinusLogProbMetric: 35.2346 - val_loss: 35.6117 - val_MinusLogProbMetric: 35.6117 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 388/1000
2023-10-30 23:15:47.259 
Epoch 388/1000 
	 loss: 35.3981, MinusLogProbMetric: 35.3981, val_loss: 35.7330, val_MinusLogProbMetric: 35.7330

Epoch 388: val_loss did not improve from 35.57036
196/196 - 64s - loss: 35.3981 - MinusLogProbMetric: 35.3981 - val_loss: 35.7330 - val_MinusLogProbMetric: 35.7330 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 389/1000
2023-10-30 23:16:52.234 
Epoch 389/1000 
	 loss: 35.2868, MinusLogProbMetric: 35.2868, val_loss: 35.8986, val_MinusLogProbMetric: 35.8986

Epoch 389: val_loss did not improve from 35.57036
196/196 - 65s - loss: 35.2868 - MinusLogProbMetric: 35.2868 - val_loss: 35.8986 - val_MinusLogProbMetric: 35.8986 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 390/1000
2023-10-30 23:17:56.944 
Epoch 390/1000 
	 loss: 35.2502, MinusLogProbMetric: 35.2502, val_loss: 35.6984, val_MinusLogProbMetric: 35.6984

Epoch 390: val_loss did not improve from 35.57036
196/196 - 65s - loss: 35.2502 - MinusLogProbMetric: 35.2502 - val_loss: 35.6984 - val_MinusLogProbMetric: 35.6984 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 391/1000
2023-10-30 23:19:02.182 
Epoch 391/1000 
	 loss: 35.2426, MinusLogProbMetric: 35.2426, val_loss: 35.5740, val_MinusLogProbMetric: 35.5740

Epoch 391: val_loss did not improve from 35.57036
196/196 - 65s - loss: 35.2426 - MinusLogProbMetric: 35.2426 - val_loss: 35.5740 - val_MinusLogProbMetric: 35.5740 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 392/1000
2023-10-30 23:20:03.379 
Epoch 392/1000 
	 loss: 35.1962, MinusLogProbMetric: 35.1962, val_loss: 35.4790, val_MinusLogProbMetric: 35.4790

Epoch 392: val_loss improved from 35.57036 to 35.47900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 35.1962 - MinusLogProbMetric: 35.1962 - val_loss: 35.4790 - val_MinusLogProbMetric: 35.4790 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 393/1000
2023-10-30 23:21:08.264 
Epoch 393/1000 
	 loss: 35.1787, MinusLogProbMetric: 35.1787, val_loss: 35.5390, val_MinusLogProbMetric: 35.5390

Epoch 393: val_loss did not improve from 35.47900
196/196 - 64s - loss: 35.1787 - MinusLogProbMetric: 35.1787 - val_loss: 35.5390 - val_MinusLogProbMetric: 35.5390 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 394/1000
2023-10-30 23:22:10.899 
Epoch 394/1000 
	 loss: 35.1506, MinusLogProbMetric: 35.1506, val_loss: 35.6883, val_MinusLogProbMetric: 35.6883

Epoch 394: val_loss did not improve from 35.47900
196/196 - 63s - loss: 35.1506 - MinusLogProbMetric: 35.1506 - val_loss: 35.6883 - val_MinusLogProbMetric: 35.6883 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 395/1000
2023-10-30 23:23:13.667 
Epoch 395/1000 
	 loss: 35.1760, MinusLogProbMetric: 35.1760, val_loss: 35.4842, val_MinusLogProbMetric: 35.4842

Epoch 395: val_loss did not improve from 35.47900
196/196 - 63s - loss: 35.1760 - MinusLogProbMetric: 35.1760 - val_loss: 35.4842 - val_MinusLogProbMetric: 35.4842 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 396/1000
2023-10-30 23:24:18.233 
Epoch 396/1000 
	 loss: 35.1604, MinusLogProbMetric: 35.1604, val_loss: 35.6068, val_MinusLogProbMetric: 35.6068

Epoch 396: val_loss did not improve from 35.47900
196/196 - 65s - loss: 35.1604 - MinusLogProbMetric: 35.1604 - val_loss: 35.6068 - val_MinusLogProbMetric: 35.6068 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 397/1000
2023-10-30 23:25:21.696 
Epoch 397/1000 
	 loss: 35.1277, MinusLogProbMetric: 35.1277, val_loss: 35.6034, val_MinusLogProbMetric: 35.6034

Epoch 397: val_loss did not improve from 35.47900
196/196 - 63s - loss: 35.1277 - MinusLogProbMetric: 35.1277 - val_loss: 35.6034 - val_MinusLogProbMetric: 35.6034 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 398/1000
2023-10-30 23:26:26.545 
Epoch 398/1000 
	 loss: 35.2440, MinusLogProbMetric: 35.2440, val_loss: 35.5159, val_MinusLogProbMetric: 35.5159

Epoch 398: val_loss did not improve from 35.47900
196/196 - 65s - loss: 35.2440 - MinusLogProbMetric: 35.2440 - val_loss: 35.5159 - val_MinusLogProbMetric: 35.5159 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 399/1000
2023-10-30 23:27:31.272 
Epoch 399/1000 
	 loss: 35.1277, MinusLogProbMetric: 35.1277, val_loss: 35.5603, val_MinusLogProbMetric: 35.5603

Epoch 399: val_loss did not improve from 35.47900
196/196 - 65s - loss: 35.1277 - MinusLogProbMetric: 35.1277 - val_loss: 35.5603 - val_MinusLogProbMetric: 35.5603 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 400/1000
2023-10-30 23:28:36.905 
Epoch 400/1000 
	 loss: 35.1122, MinusLogProbMetric: 35.1122, val_loss: 35.4545, val_MinusLogProbMetric: 35.4545

Epoch 400: val_loss improved from 35.47900 to 35.45454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 35.1122 - MinusLogProbMetric: 35.1122 - val_loss: 35.4545 - val_MinusLogProbMetric: 35.4545 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 401/1000
2023-10-30 23:29:40.333 
Epoch 401/1000 
	 loss: 35.1246, MinusLogProbMetric: 35.1246, val_loss: 35.4726, val_MinusLogProbMetric: 35.4726

Epoch 401: val_loss did not improve from 35.45454
196/196 - 62s - loss: 35.1246 - MinusLogProbMetric: 35.1246 - val_loss: 35.4726 - val_MinusLogProbMetric: 35.4726 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 402/1000
2023-10-30 23:30:42.881 
Epoch 402/1000 
	 loss: 35.0917, MinusLogProbMetric: 35.0917, val_loss: 35.5299, val_MinusLogProbMetric: 35.5299

Epoch 402: val_loss did not improve from 35.45454
196/196 - 63s - loss: 35.0917 - MinusLogProbMetric: 35.0917 - val_loss: 35.5299 - val_MinusLogProbMetric: 35.5299 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 403/1000
2023-10-30 23:31:47.022 
Epoch 403/1000 
	 loss: 35.2197, MinusLogProbMetric: 35.2197, val_loss: 35.4312, val_MinusLogProbMetric: 35.4312

Epoch 403: val_loss improved from 35.45454 to 35.43125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.2197 - MinusLogProbMetric: 35.2197 - val_loss: 35.4312 - val_MinusLogProbMetric: 35.4312 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 404/1000
2023-10-30 23:32:52.380 
Epoch 404/1000 
	 loss: 35.0706, MinusLogProbMetric: 35.0706, val_loss: 35.4577, val_MinusLogProbMetric: 35.4577

Epoch 404: val_loss did not improve from 35.43125
196/196 - 64s - loss: 35.0706 - MinusLogProbMetric: 35.0706 - val_loss: 35.4577 - val_MinusLogProbMetric: 35.4577 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 405/1000
2023-10-30 23:33:56.350 
Epoch 405/1000 
	 loss: 35.0519, MinusLogProbMetric: 35.0519, val_loss: 35.5314, val_MinusLogProbMetric: 35.5314

Epoch 405: val_loss did not improve from 35.43125
196/196 - 64s - loss: 35.0519 - MinusLogProbMetric: 35.0519 - val_loss: 35.5314 - val_MinusLogProbMetric: 35.5314 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 406/1000
2023-10-30 23:35:01.097 
Epoch 406/1000 
	 loss: 35.0572, MinusLogProbMetric: 35.0572, val_loss: 35.6408, val_MinusLogProbMetric: 35.6408

Epoch 406: val_loss did not improve from 35.43125
196/196 - 65s - loss: 35.0572 - MinusLogProbMetric: 35.0572 - val_loss: 35.6408 - val_MinusLogProbMetric: 35.6408 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 407/1000
2023-10-30 23:36:05.264 
Epoch 407/1000 
	 loss: 35.0236, MinusLogProbMetric: 35.0236, val_loss: 35.3547, val_MinusLogProbMetric: 35.3547

Epoch 407: val_loss improved from 35.43125 to 35.35474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 35.0236 - MinusLogProbMetric: 35.0236 - val_loss: 35.3547 - val_MinusLogProbMetric: 35.3547 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 408/1000
2023-10-30 23:37:08.837 
Epoch 408/1000 
	 loss: 35.0554, MinusLogProbMetric: 35.0554, val_loss: 35.4122, val_MinusLogProbMetric: 35.4122

Epoch 408: val_loss did not improve from 35.35474
196/196 - 63s - loss: 35.0554 - MinusLogProbMetric: 35.0554 - val_loss: 35.4122 - val_MinusLogProbMetric: 35.4122 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 409/1000
2023-10-30 23:38:11.272 
Epoch 409/1000 
	 loss: 35.0251, MinusLogProbMetric: 35.0251, val_loss: 35.4228, val_MinusLogProbMetric: 35.4228

Epoch 409: val_loss did not improve from 35.35474
196/196 - 62s - loss: 35.0251 - MinusLogProbMetric: 35.0251 - val_loss: 35.4228 - val_MinusLogProbMetric: 35.4228 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 410/1000
2023-10-30 23:39:13.559 
Epoch 410/1000 
	 loss: 35.0128, MinusLogProbMetric: 35.0128, val_loss: 35.5629, val_MinusLogProbMetric: 35.5629

Epoch 410: val_loss did not improve from 35.35474
196/196 - 62s - loss: 35.0128 - MinusLogProbMetric: 35.0128 - val_loss: 35.5629 - val_MinusLogProbMetric: 35.5629 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 411/1000
2023-10-30 23:40:15.242 
Epoch 411/1000 
	 loss: 35.0165, MinusLogProbMetric: 35.0165, val_loss: 35.4456, val_MinusLogProbMetric: 35.4456

Epoch 411: val_loss did not improve from 35.35474
196/196 - 62s - loss: 35.0165 - MinusLogProbMetric: 35.0165 - val_loss: 35.4456 - val_MinusLogProbMetric: 35.4456 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 412/1000
2023-10-30 23:41:18.251 
Epoch 412/1000 
	 loss: 34.9773, MinusLogProbMetric: 34.9773, val_loss: 35.3350, val_MinusLogProbMetric: 35.3350

Epoch 412: val_loss improved from 35.35474 to 35.33505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 34.9773 - MinusLogProbMetric: 34.9773 - val_loss: 35.3350 - val_MinusLogProbMetric: 35.3350 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 413/1000
2023-10-30 23:42:23.181 
Epoch 413/1000 
	 loss: 34.9971, MinusLogProbMetric: 34.9971, val_loss: 35.4132, val_MinusLogProbMetric: 35.4132

Epoch 413: val_loss did not improve from 35.33505
196/196 - 63s - loss: 34.9971 - MinusLogProbMetric: 34.9971 - val_loss: 35.4132 - val_MinusLogProbMetric: 35.4132 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 414/1000
2023-10-30 23:43:22.992 
Epoch 414/1000 
	 loss: 35.0160, MinusLogProbMetric: 35.0160, val_loss: 35.5911, val_MinusLogProbMetric: 35.5911

Epoch 414: val_loss did not improve from 35.33505
196/196 - 60s - loss: 35.0160 - MinusLogProbMetric: 35.0160 - val_loss: 35.5911 - val_MinusLogProbMetric: 35.5911 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 415/1000
2023-10-30 23:44:26.712 
Epoch 415/1000 
	 loss: 35.0021, MinusLogProbMetric: 35.0021, val_loss: 35.5675, val_MinusLogProbMetric: 35.5675

Epoch 415: val_loss did not improve from 35.33505
196/196 - 64s - loss: 35.0021 - MinusLogProbMetric: 35.0021 - val_loss: 35.5675 - val_MinusLogProbMetric: 35.5675 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 416/1000
2023-10-30 23:45:32.980 
Epoch 416/1000 
	 loss: 34.9667, MinusLogProbMetric: 34.9667, val_loss: 35.3507, val_MinusLogProbMetric: 35.3507

Epoch 416: val_loss did not improve from 35.33505
196/196 - 66s - loss: 34.9667 - MinusLogProbMetric: 34.9667 - val_loss: 35.3507 - val_MinusLogProbMetric: 35.3507 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 417/1000
2023-10-30 23:46:38.732 
Epoch 417/1000 
	 loss: 34.9286, MinusLogProbMetric: 34.9286, val_loss: 35.2953, val_MinusLogProbMetric: 35.2953

Epoch 417: val_loss improved from 35.33505 to 35.29526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 34.9286 - MinusLogProbMetric: 34.9286 - val_loss: 35.2953 - val_MinusLogProbMetric: 35.2953 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 418/1000
2023-10-30 23:47:42.801 
Epoch 418/1000 
	 loss: 34.9232, MinusLogProbMetric: 34.9232, val_loss: 35.3341, val_MinusLogProbMetric: 35.3341

Epoch 418: val_loss did not improve from 35.29526
196/196 - 63s - loss: 34.9232 - MinusLogProbMetric: 34.9232 - val_loss: 35.3341 - val_MinusLogProbMetric: 35.3341 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 419/1000
2023-10-30 23:48:45.955 
Epoch 419/1000 
	 loss: 34.9468, MinusLogProbMetric: 34.9468, val_loss: 35.3372, val_MinusLogProbMetric: 35.3372

Epoch 419: val_loss did not improve from 35.29526
196/196 - 63s - loss: 34.9468 - MinusLogProbMetric: 34.9468 - val_loss: 35.3372 - val_MinusLogProbMetric: 35.3372 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 420/1000
2023-10-30 23:49:50.336 
Epoch 420/1000 
	 loss: 34.9189, MinusLogProbMetric: 34.9189, val_loss: 35.2850, val_MinusLogProbMetric: 35.2850

Epoch 420: val_loss improved from 35.29526 to 35.28498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 34.9189 - MinusLogProbMetric: 34.9189 - val_loss: 35.2850 - val_MinusLogProbMetric: 35.2850 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 421/1000
2023-10-30 23:50:53.785 
Epoch 421/1000 
	 loss: 34.9035, MinusLogProbMetric: 34.9035, val_loss: 35.2381, val_MinusLogProbMetric: 35.2381

Epoch 421: val_loss improved from 35.28498 to 35.23808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 34.9035 - MinusLogProbMetric: 34.9035 - val_loss: 35.2381 - val_MinusLogProbMetric: 35.2381 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 422/1000
2023-10-30 23:51:58.978 
Epoch 422/1000 
	 loss: 34.9117, MinusLogProbMetric: 34.9117, val_loss: 35.2521, val_MinusLogProbMetric: 35.2521

Epoch 422: val_loss did not improve from 35.23808
196/196 - 64s - loss: 34.9117 - MinusLogProbMetric: 34.9117 - val_loss: 35.2521 - val_MinusLogProbMetric: 35.2521 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 423/1000
2023-10-30 23:53:00.411 
Epoch 423/1000 
	 loss: 34.9187, MinusLogProbMetric: 34.9187, val_loss: 35.4303, val_MinusLogProbMetric: 35.4303

Epoch 423: val_loss did not improve from 35.23808
196/196 - 61s - loss: 34.9187 - MinusLogProbMetric: 34.9187 - val_loss: 35.4303 - val_MinusLogProbMetric: 35.4303 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 424/1000
2023-10-30 23:54:06.182 
Epoch 424/1000 
	 loss: 34.8932, MinusLogProbMetric: 34.8932, val_loss: 35.6040, val_MinusLogProbMetric: 35.6040

Epoch 424: val_loss did not improve from 35.23808
196/196 - 66s - loss: 34.8932 - MinusLogProbMetric: 34.8932 - val_loss: 35.6040 - val_MinusLogProbMetric: 35.6040 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 425/1000
2023-10-30 23:55:09.266 
Epoch 425/1000 
	 loss: 34.8764, MinusLogProbMetric: 34.8764, val_loss: 35.1715, val_MinusLogProbMetric: 35.1715

Epoch 425: val_loss improved from 35.23808 to 35.17149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 34.8764 - MinusLogProbMetric: 34.8764 - val_loss: 35.1715 - val_MinusLogProbMetric: 35.1715 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 426/1000
2023-10-30 23:56:15.359 
Epoch 426/1000 
	 loss: 34.8521, MinusLogProbMetric: 34.8521, val_loss: 35.1543, val_MinusLogProbMetric: 35.1543

Epoch 426: val_loss improved from 35.17149 to 35.15435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 34.8521 - MinusLogProbMetric: 34.8521 - val_loss: 35.1543 - val_MinusLogProbMetric: 35.1543 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 427/1000
2023-10-30 23:57:18.168 
Epoch 427/1000 
	 loss: 34.8869, MinusLogProbMetric: 34.8869, val_loss: 35.3053, val_MinusLogProbMetric: 35.3053

Epoch 427: val_loss did not improve from 35.15435
196/196 - 61s - loss: 34.8869 - MinusLogProbMetric: 34.8869 - val_loss: 35.3053 - val_MinusLogProbMetric: 35.3053 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 428/1000
2023-10-30 23:58:18.145 
Epoch 428/1000 
	 loss: 34.8376, MinusLogProbMetric: 34.8376, val_loss: 35.1859, val_MinusLogProbMetric: 35.1859

Epoch 428: val_loss did not improve from 35.15435
196/196 - 60s - loss: 34.8376 - MinusLogProbMetric: 34.8376 - val_loss: 35.1859 - val_MinusLogProbMetric: 35.1859 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 429/1000
2023-10-30 23:59:06.924 
Epoch 429/1000 
	 loss: 34.9951, MinusLogProbMetric: 34.9951, val_loss: 35.6667, val_MinusLogProbMetric: 35.6667

Epoch 429: val_loss did not improve from 35.15435
196/196 - 49s - loss: 34.9951 - MinusLogProbMetric: 34.9951 - val_loss: 35.6667 - val_MinusLogProbMetric: 35.6667 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 430/1000
2023-10-30 23:59:55.372 
Epoch 430/1000 
	 loss: 34.9118, MinusLogProbMetric: 34.9118, val_loss: 35.2277, val_MinusLogProbMetric: 35.2277

Epoch 430: val_loss did not improve from 35.15435
196/196 - 48s - loss: 34.9118 - MinusLogProbMetric: 34.9118 - val_loss: 35.2277 - val_MinusLogProbMetric: 35.2277 - lr: 4.1152e-06 - 48s/epoch - 247ms/step
Epoch 431/1000
2023-10-31 00:00:45.107 
Epoch 431/1000 
	 loss: 34.8323, MinusLogProbMetric: 34.8323, val_loss: 35.2605, val_MinusLogProbMetric: 35.2605

Epoch 431: val_loss did not improve from 35.15435
196/196 - 50s - loss: 34.8323 - MinusLogProbMetric: 34.8323 - val_loss: 35.2605 - val_MinusLogProbMetric: 35.2605 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 432/1000
2023-10-31 00:01:38.449 
Epoch 432/1000 
	 loss: 34.8320, MinusLogProbMetric: 34.8320, val_loss: 35.2428, val_MinusLogProbMetric: 35.2428

Epoch 432: val_loss did not improve from 35.15435
196/196 - 53s - loss: 34.8320 - MinusLogProbMetric: 34.8320 - val_loss: 35.2428 - val_MinusLogProbMetric: 35.2428 - lr: 4.1152e-06 - 53s/epoch - 272ms/step
Epoch 433/1000
2023-10-31 00:02:26.597 
Epoch 433/1000 
	 loss: 34.8316, MinusLogProbMetric: 34.8316, val_loss: 35.3838, val_MinusLogProbMetric: 35.3838

Epoch 433: val_loss did not improve from 35.15435
196/196 - 48s - loss: 34.8316 - MinusLogProbMetric: 34.8316 - val_loss: 35.3838 - val_MinusLogProbMetric: 35.3838 - lr: 4.1152e-06 - 48s/epoch - 246ms/step
Epoch 434/1000
2023-10-31 00:03:14.864 
Epoch 434/1000 
	 loss: 34.7799, MinusLogProbMetric: 34.7799, val_loss: 35.2145, val_MinusLogProbMetric: 35.2145

Epoch 434: val_loss did not improve from 35.15435
196/196 - 48s - loss: 34.7799 - MinusLogProbMetric: 34.7799 - val_loss: 35.2145 - val_MinusLogProbMetric: 35.2145 - lr: 4.1152e-06 - 48s/epoch - 246ms/step
Epoch 435/1000
2023-10-31 00:04:05.414 
Epoch 435/1000 
	 loss: 34.7770, MinusLogProbMetric: 34.7770, val_loss: 35.3282, val_MinusLogProbMetric: 35.3282

Epoch 435: val_loss did not improve from 35.15435
196/196 - 51s - loss: 34.7770 - MinusLogProbMetric: 34.7770 - val_loss: 35.3282 - val_MinusLogProbMetric: 35.3282 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 436/1000
2023-10-31 00:04:56.160 
Epoch 436/1000 
	 loss: 34.7555, MinusLogProbMetric: 34.7555, val_loss: 35.1327, val_MinusLogProbMetric: 35.1327

Epoch 436: val_loss improved from 35.15435 to 35.13267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 52s - loss: 34.7555 - MinusLogProbMetric: 34.7555 - val_loss: 35.1327 - val_MinusLogProbMetric: 35.1327 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 437/1000
2023-10-31 00:05:45.195 
Epoch 437/1000 
	 loss: 34.7831, MinusLogProbMetric: 34.7831, val_loss: 35.2885, val_MinusLogProbMetric: 35.2885

Epoch 437: val_loss did not improve from 35.13267
196/196 - 48s - loss: 34.7831 - MinusLogProbMetric: 34.7831 - val_loss: 35.2885 - val_MinusLogProbMetric: 35.2885 - lr: 4.1152e-06 - 48s/epoch - 245ms/step
Epoch 438/1000
2023-10-31 00:06:33.935 
Epoch 438/1000 
	 loss: 34.7615, MinusLogProbMetric: 34.7615, val_loss: 35.1206, val_MinusLogProbMetric: 35.1206

Epoch 438: val_loss improved from 35.13267 to 35.12062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 34.7615 - MinusLogProbMetric: 34.7615 - val_loss: 35.1206 - val_MinusLogProbMetric: 35.1206 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 439/1000
2023-10-31 00:07:24.942 
Epoch 439/1000 
	 loss: 39.0022, MinusLogProbMetric: 39.0022, val_loss: 38.9176, val_MinusLogProbMetric: 38.9176

Epoch 439: val_loss did not improve from 35.12062
196/196 - 50s - loss: 39.0022 - MinusLogProbMetric: 39.0022 - val_loss: 38.9176 - val_MinusLogProbMetric: 38.9176 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 440/1000
2023-10-31 00:08:13.517 
Epoch 440/1000 
	 loss: 37.1376, MinusLogProbMetric: 37.1376, val_loss: 36.6393, val_MinusLogProbMetric: 36.6393

Epoch 440: val_loss did not improve from 35.12062
196/196 - 49s - loss: 37.1376 - MinusLogProbMetric: 37.1376 - val_loss: 36.6393 - val_MinusLogProbMetric: 36.6393 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 441/1000
2023-10-31 00:09:02.309 
Epoch 441/1000 
	 loss: 36.1350, MinusLogProbMetric: 36.1350, val_loss: 36.4788, val_MinusLogProbMetric: 36.4788

Epoch 441: val_loss did not improve from 35.12062
196/196 - 49s - loss: 36.1350 - MinusLogProbMetric: 36.1350 - val_loss: 36.4788 - val_MinusLogProbMetric: 36.4788 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 442/1000
2023-10-31 00:09:59.021 
Epoch 442/1000 
	 loss: 35.7656, MinusLogProbMetric: 35.7656, val_loss: 35.9218, val_MinusLogProbMetric: 35.9218

Epoch 442: val_loss did not improve from 35.12062
196/196 - 57s - loss: 35.7656 - MinusLogProbMetric: 35.7656 - val_loss: 35.9218 - val_MinusLogProbMetric: 35.9218 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 443/1000
2023-10-31 00:10:49.164 
Epoch 443/1000 
	 loss: 35.4512, MinusLogProbMetric: 35.4512, val_loss: 35.7614, val_MinusLogProbMetric: 35.7614

Epoch 443: val_loss did not improve from 35.12062
196/196 - 50s - loss: 35.4512 - MinusLogProbMetric: 35.4512 - val_loss: 35.7614 - val_MinusLogProbMetric: 35.7614 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 444/1000
2023-10-31 00:11:37.860 
Epoch 444/1000 
	 loss: 35.3924, MinusLogProbMetric: 35.3924, val_loss: 35.8839, val_MinusLogProbMetric: 35.8839

Epoch 444: val_loss did not improve from 35.12062
196/196 - 49s - loss: 35.3924 - MinusLogProbMetric: 35.3924 - val_loss: 35.8839 - val_MinusLogProbMetric: 35.8839 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 445/1000
2023-10-31 00:12:27.008 
Epoch 445/1000 
	 loss: 35.3443, MinusLogProbMetric: 35.3443, val_loss: 35.5512, val_MinusLogProbMetric: 35.5512

Epoch 445: val_loss did not improve from 35.12062
196/196 - 49s - loss: 35.3443 - MinusLogProbMetric: 35.3443 - val_loss: 35.5512 - val_MinusLogProbMetric: 35.5512 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 446/1000
2023-10-31 00:13:19.890 
Epoch 446/1000 
	 loss: 35.0610, MinusLogProbMetric: 35.0610, val_loss: 35.3072, val_MinusLogProbMetric: 35.3072

Epoch 446: val_loss did not improve from 35.12062
196/196 - 53s - loss: 35.0610 - MinusLogProbMetric: 35.0610 - val_loss: 35.3072 - val_MinusLogProbMetric: 35.3072 - lr: 4.1152e-06 - 53s/epoch - 270ms/step
Epoch 447/1000
2023-10-31 00:14:09.103 
Epoch 447/1000 
	 loss: 34.7490, MinusLogProbMetric: 34.7490, val_loss: 35.1355, val_MinusLogProbMetric: 35.1355

Epoch 447: val_loss did not improve from 35.12062
196/196 - 49s - loss: 34.7490 - MinusLogProbMetric: 34.7490 - val_loss: 35.1355 - val_MinusLogProbMetric: 35.1355 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 448/1000
2023-10-31 00:15:01.194 
Epoch 448/1000 
	 loss: 34.7177, MinusLogProbMetric: 34.7177, val_loss: 35.0964, val_MinusLogProbMetric: 35.0964

Epoch 448: val_loss improved from 35.12062 to 35.09640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 34.7177 - MinusLogProbMetric: 34.7177 - val_loss: 35.0964 - val_MinusLogProbMetric: 35.0964 - lr: 4.1152e-06 - 53s/epoch - 271ms/step
Epoch 449/1000
2023-10-31 00:15:59.863 
Epoch 449/1000 
	 loss: 34.7668, MinusLogProbMetric: 34.7668, val_loss: 36.0366, val_MinusLogProbMetric: 36.0366

Epoch 449: val_loss did not improve from 35.09640
196/196 - 58s - loss: 34.7668 - MinusLogProbMetric: 34.7668 - val_loss: 36.0366 - val_MinusLogProbMetric: 36.0366 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 450/1000
2023-10-31 00:16:48.290 
Epoch 450/1000 
	 loss: 35.3346, MinusLogProbMetric: 35.3346, val_loss: 35.6256, val_MinusLogProbMetric: 35.6256

Epoch 450: val_loss did not improve from 35.09640
196/196 - 48s - loss: 35.3346 - MinusLogProbMetric: 35.3346 - val_loss: 35.6256 - val_MinusLogProbMetric: 35.6256 - lr: 4.1152e-06 - 48s/epoch - 247ms/step
Epoch 451/1000
2023-10-31 00:17:37.069 
Epoch 451/1000 
	 loss: 35.1545, MinusLogProbMetric: 35.1545, val_loss: 36.0586, val_MinusLogProbMetric: 36.0586

Epoch 451: val_loss did not improve from 35.09640
196/196 - 49s - loss: 35.1545 - MinusLogProbMetric: 35.1545 - val_loss: 36.0586 - val_MinusLogProbMetric: 36.0586 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 452/1000
2023-10-31 00:18:32.881 
Epoch 452/1000 
	 loss: 34.8259, MinusLogProbMetric: 34.8259, val_loss: 35.0023, val_MinusLogProbMetric: 35.0023

Epoch 452: val_loss improved from 35.09640 to 35.00227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 34.8259 - MinusLogProbMetric: 34.8259 - val_loss: 35.0023 - val_MinusLogProbMetric: 35.0023 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 453/1000
2023-10-31 00:19:30.700 
Epoch 453/1000 
	 loss: 34.6439, MinusLogProbMetric: 34.6439, val_loss: 35.1208, val_MinusLogProbMetric: 35.1208

Epoch 453: val_loss did not improve from 35.00227
196/196 - 57s - loss: 34.6439 - MinusLogProbMetric: 34.6439 - val_loss: 35.1208 - val_MinusLogProbMetric: 35.1208 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 454/1000
2023-10-31 00:20:20.457 
Epoch 454/1000 
	 loss: 34.6968, MinusLogProbMetric: 34.6968, val_loss: 34.9926, val_MinusLogProbMetric: 34.9926

Epoch 454: val_loss improved from 35.00227 to 34.99259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 34.6968 - MinusLogProbMetric: 34.6968 - val_loss: 34.9926 - val_MinusLogProbMetric: 34.9926 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 455/1000
2023-10-31 00:21:10.921 
Epoch 455/1000 
	 loss: 34.6437, MinusLogProbMetric: 34.6437, val_loss: 35.0104, val_MinusLogProbMetric: 35.0104

Epoch 455: val_loss did not improve from 34.99259
196/196 - 50s - loss: 34.6437 - MinusLogProbMetric: 34.6437 - val_loss: 35.0104 - val_MinusLogProbMetric: 35.0104 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 456/1000
2023-10-31 00:22:06.661 
Epoch 456/1000 
	 loss: 34.6119, MinusLogProbMetric: 34.6119, val_loss: 34.9693, val_MinusLogProbMetric: 34.9693

Epoch 456: val_loss improved from 34.99259 to 34.96934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 34.6119 - MinusLogProbMetric: 34.6119 - val_loss: 34.9693 - val_MinusLogProbMetric: 34.9693 - lr: 4.1152e-06 - 56s/epoch - 288ms/step
Epoch 457/1000
2023-10-31 00:22:56.083 
Epoch 457/1000 
	 loss: 34.5973, MinusLogProbMetric: 34.5973, val_loss: 35.0706, val_MinusLogProbMetric: 35.0706

Epoch 457: val_loss did not improve from 34.96934
196/196 - 49s - loss: 34.5973 - MinusLogProbMetric: 34.5973 - val_loss: 35.0706 - val_MinusLogProbMetric: 35.0706 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 458/1000
2023-10-31 00:23:44.617 
Epoch 458/1000 
	 loss: 34.5576, MinusLogProbMetric: 34.5576, val_loss: 35.0429, val_MinusLogProbMetric: 35.0429

Epoch 458: val_loss did not improve from 34.96934
196/196 - 49s - loss: 34.5576 - MinusLogProbMetric: 34.5576 - val_loss: 35.0429 - val_MinusLogProbMetric: 35.0429 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 459/1000
2023-10-31 00:24:37.439 
Epoch 459/1000 
	 loss: 34.5478, MinusLogProbMetric: 34.5478, val_loss: 35.0334, val_MinusLogProbMetric: 35.0334

Epoch 459: val_loss did not improve from 34.96934
196/196 - 53s - loss: 34.5478 - MinusLogProbMetric: 34.5478 - val_loss: 35.0334 - val_MinusLogProbMetric: 35.0334 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 460/1000
2023-10-31 00:25:27.534 
Epoch 460/1000 
	 loss: 34.5940, MinusLogProbMetric: 34.5940, val_loss: 35.1016, val_MinusLogProbMetric: 35.1016

Epoch 460: val_loss did not improve from 34.96934
196/196 - 50s - loss: 34.5940 - MinusLogProbMetric: 34.5940 - val_loss: 35.1016 - val_MinusLogProbMetric: 35.1016 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 461/1000
2023-10-31 00:26:15.678 
Epoch 461/1000 
	 loss: 34.5464, MinusLogProbMetric: 34.5464, val_loss: 34.9296, val_MinusLogProbMetric: 34.9296

Epoch 461: val_loss improved from 34.96934 to 34.92960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 49s - loss: 34.5464 - MinusLogProbMetric: 34.5464 - val_loss: 34.9296 - val_MinusLogProbMetric: 34.9296 - lr: 4.1152e-06 - 49s/epoch - 250ms/step
Epoch 462/1000
2023-10-31 00:27:06.808 
Epoch 462/1000 
	 loss: 34.5536, MinusLogProbMetric: 34.5536, val_loss: 34.9837, val_MinusLogProbMetric: 34.9837

Epoch 462: val_loss did not improve from 34.92960
196/196 - 50s - loss: 34.5536 - MinusLogProbMetric: 34.5536 - val_loss: 34.9837 - val_MinusLogProbMetric: 34.9837 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 463/1000
2023-10-31 00:28:01.970 
Epoch 463/1000 
	 loss: 34.5096, MinusLogProbMetric: 34.5096, val_loss: 34.9225, val_MinusLogProbMetric: 34.9225

Epoch 463: val_loss improved from 34.92960 to 34.92247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 34.5096 - MinusLogProbMetric: 34.5096 - val_loss: 34.9225 - val_MinusLogProbMetric: 34.9225 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 464/1000
2023-10-31 00:28:50.936 
Epoch 464/1000 
	 loss: 34.5212, MinusLogProbMetric: 34.5212, val_loss: 34.8340, val_MinusLogProbMetric: 34.8340

Epoch 464: val_loss improved from 34.92247 to 34.83398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 49s - loss: 34.5212 - MinusLogProbMetric: 34.5212 - val_loss: 34.8340 - val_MinusLogProbMetric: 34.8340 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 465/1000
2023-10-31 00:29:40.903 
Epoch 465/1000 
	 loss: 34.5314, MinusLogProbMetric: 34.5314, val_loss: 34.8568, val_MinusLogProbMetric: 34.8568

Epoch 465: val_loss did not improve from 34.83398
196/196 - 49s - loss: 34.5314 - MinusLogProbMetric: 34.5314 - val_loss: 34.8568 - val_MinusLogProbMetric: 34.8568 - lr: 4.1152e-06 - 49s/epoch - 252ms/step
Epoch 466/1000
2023-10-31 00:30:32.713 
Epoch 466/1000 
	 loss: 34.5312, MinusLogProbMetric: 34.5312, val_loss: 35.0099, val_MinusLogProbMetric: 35.0099

Epoch 466: val_loss did not improve from 34.83398
196/196 - 52s - loss: 34.5312 - MinusLogProbMetric: 34.5312 - val_loss: 35.0099 - val_MinusLogProbMetric: 35.0099 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 467/1000
2023-10-31 00:31:22.886 
Epoch 467/1000 
	 loss: 34.5247, MinusLogProbMetric: 34.5247, val_loss: 35.0338, val_MinusLogProbMetric: 35.0338

Epoch 467: val_loss did not improve from 34.83398
196/196 - 50s - loss: 34.5247 - MinusLogProbMetric: 34.5247 - val_loss: 35.0338 - val_MinusLogProbMetric: 35.0338 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 468/1000
2023-10-31 00:32:14.803 
Epoch 468/1000 
	 loss: 34.5087, MinusLogProbMetric: 34.5087, val_loss: 35.0132, val_MinusLogProbMetric: 35.0132

Epoch 468: val_loss did not improve from 34.83398
196/196 - 52s - loss: 34.5087 - MinusLogProbMetric: 34.5087 - val_loss: 35.0132 - val_MinusLogProbMetric: 35.0132 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 469/1000
2023-10-31 00:33:09.689 
Epoch 469/1000 
	 loss: 34.5034, MinusLogProbMetric: 34.5034, val_loss: 35.1307, val_MinusLogProbMetric: 35.1307

Epoch 469: val_loss did not improve from 34.83398
196/196 - 55s - loss: 34.5034 - MinusLogProbMetric: 34.5034 - val_loss: 35.1307 - val_MinusLogProbMetric: 35.1307 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 470/1000
2023-10-31 00:34:02.303 
Epoch 470/1000 
	 loss: 34.5007, MinusLogProbMetric: 34.5007, val_loss: 34.8044, val_MinusLogProbMetric: 34.8044

Epoch 470: val_loss improved from 34.83398 to 34.80435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 34.5007 - MinusLogProbMetric: 34.5007 - val_loss: 34.8044 - val_MinusLogProbMetric: 34.8044 - lr: 4.1152e-06 - 53s/epoch - 272ms/step
Epoch 471/1000
2023-10-31 00:34:53.106 
Epoch 471/1000 
	 loss: 34.4878, MinusLogProbMetric: 34.4878, val_loss: 35.1904, val_MinusLogProbMetric: 35.1904

Epoch 471: val_loss did not improve from 34.80435
196/196 - 50s - loss: 34.4878 - MinusLogProbMetric: 34.4878 - val_loss: 35.1904 - val_MinusLogProbMetric: 35.1904 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 472/1000
2023-10-31 00:35:46.833 
Epoch 472/1000 
	 loss: 34.4814, MinusLogProbMetric: 34.4814, val_loss: 34.8767, val_MinusLogProbMetric: 34.8767

Epoch 472: val_loss did not improve from 34.80435
196/196 - 54s - loss: 34.4814 - MinusLogProbMetric: 34.4814 - val_loss: 34.8767 - val_MinusLogProbMetric: 34.8767 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 473/1000
2023-10-31 00:36:36.761 
Epoch 473/1000 
	 loss: 34.4596, MinusLogProbMetric: 34.4596, val_loss: 34.9632, val_MinusLogProbMetric: 34.9632

Epoch 473: val_loss did not improve from 34.80435
196/196 - 50s - loss: 34.4596 - MinusLogProbMetric: 34.4596 - val_loss: 34.9632 - val_MinusLogProbMetric: 34.9632 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 474/1000
2023-10-31 00:37:26.816 
Epoch 474/1000 
	 loss: 34.4639, MinusLogProbMetric: 34.4639, val_loss: 34.7995, val_MinusLogProbMetric: 34.7995

Epoch 474: val_loss improved from 34.80435 to 34.79953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 34.4639 - MinusLogProbMetric: 34.4639 - val_loss: 34.7995 - val_MinusLogProbMetric: 34.7995 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 475/1000
2023-10-31 00:38:20.034 
Epoch 475/1000 
	 loss: 34.4449, MinusLogProbMetric: 34.4449, val_loss: 34.8679, val_MinusLogProbMetric: 34.8679

Epoch 475: val_loss did not improve from 34.79953
196/196 - 52s - loss: 34.4449 - MinusLogProbMetric: 34.4449 - val_loss: 34.8679 - val_MinusLogProbMetric: 34.8679 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 476/1000
2023-10-31 00:39:13.645 
Epoch 476/1000 
	 loss: 34.4367, MinusLogProbMetric: 34.4367, val_loss: 34.8277, val_MinusLogProbMetric: 34.8277

Epoch 476: val_loss did not improve from 34.79953
196/196 - 54s - loss: 34.4367 - MinusLogProbMetric: 34.4367 - val_loss: 34.8277 - val_MinusLogProbMetric: 34.8277 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 477/1000
2023-10-31 00:40:02.295 
Epoch 477/1000 
	 loss: 34.4376, MinusLogProbMetric: 34.4376, val_loss: 34.9675, val_MinusLogProbMetric: 34.9675

Epoch 477: val_loss did not improve from 34.79953
196/196 - 49s - loss: 34.4376 - MinusLogProbMetric: 34.4376 - val_loss: 34.9675 - val_MinusLogProbMetric: 34.9675 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 478/1000
2023-10-31 00:40:50.826 
Epoch 478/1000 
	 loss: 34.4108, MinusLogProbMetric: 34.4108, val_loss: 34.8669, val_MinusLogProbMetric: 34.8669

Epoch 478: val_loss did not improve from 34.79953
196/196 - 49s - loss: 34.4108 - MinusLogProbMetric: 34.4108 - val_loss: 34.8669 - val_MinusLogProbMetric: 34.8669 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 479/1000
2023-10-31 00:41:39.903 
Epoch 479/1000 
	 loss: 34.4125, MinusLogProbMetric: 34.4125, val_loss: 34.9867, val_MinusLogProbMetric: 34.9867

Epoch 479: val_loss did not improve from 34.79953
196/196 - 49s - loss: 34.4125 - MinusLogProbMetric: 34.4125 - val_loss: 34.9867 - val_MinusLogProbMetric: 34.9867 - lr: 4.1152e-06 - 49s/epoch - 250ms/step
Epoch 480/1000
2023-10-31 00:42:31.186 
Epoch 480/1000 
	 loss: 34.3919, MinusLogProbMetric: 34.3919, val_loss: 34.8762, val_MinusLogProbMetric: 34.8762

Epoch 480: val_loss did not improve from 34.79953
196/196 - 51s - loss: 34.3919 - MinusLogProbMetric: 34.3919 - val_loss: 34.8762 - val_MinusLogProbMetric: 34.8762 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 481/1000
2023-10-31 00:43:24.811 
Epoch 481/1000 
	 loss: 34.3678, MinusLogProbMetric: 34.3678, val_loss: 34.7467, val_MinusLogProbMetric: 34.7467

Epoch 481: val_loss improved from 34.79953 to 34.74672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 34.3678 - MinusLogProbMetric: 34.3678 - val_loss: 34.7467 - val_MinusLogProbMetric: 34.7467 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 482/1000
2023-10-31 00:44:16.508 
Epoch 482/1000 
	 loss: 34.3892, MinusLogProbMetric: 34.3892, val_loss: 34.9198, val_MinusLogProbMetric: 34.9198

Epoch 482: val_loss did not improve from 34.74672
196/196 - 50s - loss: 34.3892 - MinusLogProbMetric: 34.3892 - val_loss: 34.9198 - val_MinusLogProbMetric: 34.9198 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 483/1000
2023-10-31 00:45:08.341 
Epoch 483/1000 
	 loss: 34.3808, MinusLogProbMetric: 34.3808, val_loss: 34.7681, val_MinusLogProbMetric: 34.7681

Epoch 483: val_loss did not improve from 34.74672
196/196 - 52s - loss: 34.3808 - MinusLogProbMetric: 34.3808 - val_loss: 34.7681 - val_MinusLogProbMetric: 34.7681 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 484/1000
2023-10-31 00:46:00.332 
Epoch 484/1000 
	 loss: 34.3928, MinusLogProbMetric: 34.3928, val_loss: 35.0877, val_MinusLogProbMetric: 35.0877

Epoch 484: val_loss did not improve from 34.74672
196/196 - 52s - loss: 34.3928 - MinusLogProbMetric: 34.3928 - val_loss: 35.0877 - val_MinusLogProbMetric: 35.0877 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 485/1000
2023-10-31 00:46:53.581 
Epoch 485/1000 
	 loss: 34.4618, MinusLogProbMetric: 34.4618, val_loss: 34.7417, val_MinusLogProbMetric: 34.7417

Epoch 485: val_loss improved from 34.74672 to 34.74166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 34.4618 - MinusLogProbMetric: 34.4618 - val_loss: 34.7417 - val_MinusLogProbMetric: 34.7417 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 486/1000
2023-10-31 00:47:44.741 
Epoch 486/1000 
	 loss: 34.3505, MinusLogProbMetric: 34.3505, val_loss: 34.8867, val_MinusLogProbMetric: 34.8867

Epoch 486: val_loss did not improve from 34.74166
196/196 - 50s - loss: 34.3505 - MinusLogProbMetric: 34.3505 - val_loss: 34.8867 - val_MinusLogProbMetric: 34.8867 - lr: 4.1152e-06 - 50s/epoch - 257ms/step
Epoch 487/1000
2023-10-31 00:48:34.892 
Epoch 487/1000 
	 loss: 34.3579, MinusLogProbMetric: 34.3579, val_loss: 34.7819, val_MinusLogProbMetric: 34.7819

Epoch 487: val_loss did not improve from 34.74166
196/196 - 50s - loss: 34.3579 - MinusLogProbMetric: 34.3579 - val_loss: 34.7819 - val_MinusLogProbMetric: 34.7819 - lr: 4.1152e-06 - 50s/epoch - 256ms/step
Epoch 488/1000
2023-10-31 00:49:27.758 
Epoch 488/1000 
	 loss: 34.3412, MinusLogProbMetric: 34.3412, val_loss: 35.0165, val_MinusLogProbMetric: 35.0165

Epoch 488: val_loss did not improve from 34.74166
196/196 - 53s - loss: 34.3412 - MinusLogProbMetric: 34.3412 - val_loss: 35.0165 - val_MinusLogProbMetric: 35.0165 - lr: 4.1152e-06 - 53s/epoch - 270ms/step
Epoch 489/1000
2023-10-31 00:50:20.934 
Epoch 489/1000 
	 loss: 34.3406, MinusLogProbMetric: 34.3406, val_loss: 34.6674, val_MinusLogProbMetric: 34.6674

Epoch 489: val_loss improved from 34.74166 to 34.66737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 34.3406 - MinusLogProbMetric: 34.3406 - val_loss: 34.6674 - val_MinusLogProbMetric: 34.6674 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 490/1000
2023-10-31 00:51:11.377 
Epoch 490/1000 
	 loss: 34.3125, MinusLogProbMetric: 34.3125, val_loss: 34.6849, val_MinusLogProbMetric: 34.6849

Epoch 490: val_loss did not improve from 34.66737
196/196 - 49s - loss: 34.3125 - MinusLogProbMetric: 34.3125 - val_loss: 34.6849 - val_MinusLogProbMetric: 34.6849 - lr: 4.1152e-06 - 49s/epoch - 252ms/step
Epoch 491/1000
2023-10-31 00:52:06.188 
Epoch 491/1000 
	 loss: 34.3245, MinusLogProbMetric: 34.3245, val_loss: 34.6491, val_MinusLogProbMetric: 34.6491

Epoch 491: val_loss improved from 34.66737 to 34.64912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 34.3245 - MinusLogProbMetric: 34.3245 - val_loss: 34.6491 - val_MinusLogProbMetric: 34.6491 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 492/1000
2023-10-31 00:52:56.845 
Epoch 492/1000 
	 loss: 34.3061, MinusLogProbMetric: 34.3061, val_loss: 34.8466, val_MinusLogProbMetric: 34.8466

Epoch 492: val_loss did not improve from 34.64912
196/196 - 50s - loss: 34.3061 - MinusLogProbMetric: 34.3061 - val_loss: 34.8466 - val_MinusLogProbMetric: 34.8466 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 493/1000
2023-10-31 00:53:46.862 
Epoch 493/1000 
	 loss: 34.2902, MinusLogProbMetric: 34.2902, val_loss: 34.6804, val_MinusLogProbMetric: 34.6804

Epoch 493: val_loss did not improve from 34.64912
196/196 - 50s - loss: 34.2902 - MinusLogProbMetric: 34.2902 - val_loss: 34.6804 - val_MinusLogProbMetric: 34.6804 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 494/1000
2023-10-31 00:54:39.057 
Epoch 494/1000 
	 loss: 34.2824, MinusLogProbMetric: 34.2824, val_loss: 34.8120, val_MinusLogProbMetric: 34.8120

Epoch 494: val_loss did not improve from 34.64912
196/196 - 52s - loss: 34.2824 - MinusLogProbMetric: 34.2824 - val_loss: 34.8120 - val_MinusLogProbMetric: 34.8120 - lr: 4.1152e-06 - 52s/epoch - 266ms/step
Epoch 495/1000
2023-10-31 00:55:28.066 
Epoch 495/1000 
	 loss: 34.2633, MinusLogProbMetric: 34.2633, val_loss: 34.6691, val_MinusLogProbMetric: 34.6691

Epoch 495: val_loss did not improve from 34.64912
196/196 - 49s - loss: 34.2633 - MinusLogProbMetric: 34.2633 - val_loss: 34.6691 - val_MinusLogProbMetric: 34.6691 - lr: 4.1152e-06 - 49s/epoch - 250ms/step
Epoch 496/1000
2023-10-31 00:56:17.069 
Epoch 496/1000 
	 loss: 34.2798, MinusLogProbMetric: 34.2798, val_loss: 34.5838, val_MinusLogProbMetric: 34.5838

Epoch 496: val_loss improved from 34.64912 to 34.58375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 34.2798 - MinusLogProbMetric: 34.2798 - val_loss: 34.5838 - val_MinusLogProbMetric: 34.5838 - lr: 4.1152e-06 - 53s/epoch - 268ms/step
Epoch 497/1000
2023-10-31 00:57:10.145 
Epoch 497/1000 
	 loss: 34.2753, MinusLogProbMetric: 34.2753, val_loss: 34.6884, val_MinusLogProbMetric: 34.6884

Epoch 497: val_loss did not improve from 34.58375
196/196 - 49s - loss: 34.2753 - MinusLogProbMetric: 34.2753 - val_loss: 34.6884 - val_MinusLogProbMetric: 34.6884 - lr: 4.1152e-06 - 49s/epoch - 252ms/step
Epoch 498/1000
2023-10-31 00:57:58.427 
Epoch 498/1000 
	 loss: 34.2772, MinusLogProbMetric: 34.2772, val_loss: 34.6487, val_MinusLogProbMetric: 34.6487

Epoch 498: val_loss did not improve from 34.58375
196/196 - 48s - loss: 34.2772 - MinusLogProbMetric: 34.2772 - val_loss: 34.6487 - val_MinusLogProbMetric: 34.6487 - lr: 4.1152e-06 - 48s/epoch - 246ms/step
Epoch 499/1000
2023-10-31 00:58:46.780 
Epoch 499/1000 
	 loss: 34.2391, MinusLogProbMetric: 34.2391, val_loss: 34.7492, val_MinusLogProbMetric: 34.7492

Epoch 499: val_loss did not improve from 34.58375
196/196 - 48s - loss: 34.2391 - MinusLogProbMetric: 34.2391 - val_loss: 34.7492 - val_MinusLogProbMetric: 34.7492 - lr: 4.1152e-06 - 48s/epoch - 247ms/step
Epoch 500/1000
2023-10-31 00:59:36.080 
Epoch 500/1000 
	 loss: 34.2119, MinusLogProbMetric: 34.2119, val_loss: 34.9868, val_MinusLogProbMetric: 34.9868

Epoch 500: val_loss did not improve from 34.58375
196/196 - 49s - loss: 34.2119 - MinusLogProbMetric: 34.2119 - val_loss: 34.9868 - val_MinusLogProbMetric: 34.9868 - lr: 4.1152e-06 - 49s/epoch - 252ms/step
Epoch 501/1000
2023-10-31 01:00:24.109 
Epoch 501/1000 
	 loss: 34.2423, MinusLogProbMetric: 34.2423, val_loss: 34.6648, val_MinusLogProbMetric: 34.6648

Epoch 501: val_loss did not improve from 34.58375
196/196 - 48s - loss: 34.2423 - MinusLogProbMetric: 34.2423 - val_loss: 34.6648 - val_MinusLogProbMetric: 34.6648 - lr: 4.1152e-06 - 48s/epoch - 245ms/step
Epoch 502/1000
2023-10-31 01:01:13.269 
Epoch 502/1000 
	 loss: 34.2097, MinusLogProbMetric: 34.2097, val_loss: 34.7861, val_MinusLogProbMetric: 34.7861

Epoch 502: val_loss did not improve from 34.58375
196/196 - 49s - loss: 34.2097 - MinusLogProbMetric: 34.2097 - val_loss: 34.7861 - val_MinusLogProbMetric: 34.7861 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 503/1000
2023-10-31 01:02:13.540 
Epoch 503/1000 
	 loss: 34.2138, MinusLogProbMetric: 34.2138, val_loss: 34.5812, val_MinusLogProbMetric: 34.5812

Epoch 503: val_loss improved from 34.58375 to 34.58121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 34.2138 - MinusLogProbMetric: 34.2138 - val_loss: 34.5812 - val_MinusLogProbMetric: 34.5812 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 504/1000
2023-10-31 01:03:12.740 
Epoch 504/1000 
	 loss: 34.1922, MinusLogProbMetric: 34.1922, val_loss: 34.7212, val_MinusLogProbMetric: 34.7212

Epoch 504: val_loss did not improve from 34.58121
196/196 - 58s - loss: 34.1922 - MinusLogProbMetric: 34.1922 - val_loss: 34.7212 - val_MinusLogProbMetric: 34.7212 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 505/1000
2023-10-31 01:04:13.649 
Epoch 505/1000 
	 loss: 34.2071, MinusLogProbMetric: 34.2071, val_loss: 34.6034, val_MinusLogProbMetric: 34.6034

Epoch 505: val_loss did not improve from 34.58121
196/196 - 61s - loss: 34.2071 - MinusLogProbMetric: 34.2071 - val_loss: 34.6034 - val_MinusLogProbMetric: 34.6034 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 506/1000
2023-10-31 01:05:14.931 
Epoch 506/1000 
	 loss: 34.1962, MinusLogProbMetric: 34.1962, val_loss: 34.5769, val_MinusLogProbMetric: 34.5769

Epoch 506: val_loss improved from 34.58121 to 34.57690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 34.1962 - MinusLogProbMetric: 34.1962 - val_loss: 34.5769 - val_MinusLogProbMetric: 34.5769 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 507/1000
2023-10-31 01:06:16.546 
Epoch 507/1000 
	 loss: 34.1839, MinusLogProbMetric: 34.1839, val_loss: 34.6935, val_MinusLogProbMetric: 34.6935

Epoch 507: val_loss did not improve from 34.57690
196/196 - 61s - loss: 34.1839 - MinusLogProbMetric: 34.1839 - val_loss: 34.6935 - val_MinusLogProbMetric: 34.6935 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 508/1000
2023-10-31 01:07:14.232 
Epoch 508/1000 
	 loss: 34.1728, MinusLogProbMetric: 34.1728, val_loss: 34.6121, val_MinusLogProbMetric: 34.6121

Epoch 508: val_loss did not improve from 34.57690
196/196 - 58s - loss: 34.1728 - MinusLogProbMetric: 34.1728 - val_loss: 34.6121 - val_MinusLogProbMetric: 34.6121 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 509/1000
2023-10-31 01:08:08.572 
Epoch 509/1000 
	 loss: 34.1872, MinusLogProbMetric: 34.1872, val_loss: 34.7626, val_MinusLogProbMetric: 34.7626

Epoch 509: val_loss did not improve from 34.57690
196/196 - 54s - loss: 34.1872 - MinusLogProbMetric: 34.1872 - val_loss: 34.7626 - val_MinusLogProbMetric: 34.7626 - lr: 4.1152e-06 - 54s/epoch - 277ms/step
Epoch 510/1000
2023-10-31 01:09:05.594 
Epoch 510/1000 
	 loss: 34.1644, MinusLogProbMetric: 34.1644, val_loss: 34.7281, val_MinusLogProbMetric: 34.7281

Epoch 510: val_loss did not improve from 34.57690
196/196 - 57s - loss: 34.1644 - MinusLogProbMetric: 34.1644 - val_loss: 34.7281 - val_MinusLogProbMetric: 34.7281 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 511/1000
2023-10-31 01:10:02.630 
Epoch 511/1000 
	 loss: 34.1432, MinusLogProbMetric: 34.1432, val_loss: 34.6610, val_MinusLogProbMetric: 34.6610

Epoch 511: val_loss did not improve from 34.57690
196/196 - 57s - loss: 34.1432 - MinusLogProbMetric: 34.1432 - val_loss: 34.6610 - val_MinusLogProbMetric: 34.6610 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 512/1000
2023-10-31 01:11:01.206 
Epoch 512/1000 
	 loss: 34.1613, MinusLogProbMetric: 34.1613, val_loss: 34.6199, val_MinusLogProbMetric: 34.6199

Epoch 512: val_loss did not improve from 34.57690
196/196 - 59s - loss: 34.1613 - MinusLogProbMetric: 34.1613 - val_loss: 34.6199 - val_MinusLogProbMetric: 34.6199 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 513/1000
2023-10-31 01:11:59.320 
Epoch 513/1000 
	 loss: 34.1916, MinusLogProbMetric: 34.1916, val_loss: 34.5920, val_MinusLogProbMetric: 34.5920

Epoch 513: val_loss did not improve from 34.57690
196/196 - 58s - loss: 34.1916 - MinusLogProbMetric: 34.1916 - val_loss: 34.5920 - val_MinusLogProbMetric: 34.5920 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 514/1000
2023-10-31 01:12:57.194 
Epoch 514/1000 
	 loss: 34.1348, MinusLogProbMetric: 34.1348, val_loss: 34.6179, val_MinusLogProbMetric: 34.6179

Epoch 514: val_loss did not improve from 34.57690
196/196 - 58s - loss: 34.1348 - MinusLogProbMetric: 34.1348 - val_loss: 34.6179 - val_MinusLogProbMetric: 34.6179 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 515/1000
2023-10-31 01:14:00.815 
Epoch 515/1000 
	 loss: 34.1408, MinusLogProbMetric: 34.1408, val_loss: 34.4819, val_MinusLogProbMetric: 34.4819

Epoch 515: val_loss improved from 34.57690 to 34.48191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 34.1408 - MinusLogProbMetric: 34.1408 - val_loss: 34.4819 - val_MinusLogProbMetric: 34.4819 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 516/1000
2023-10-31 01:15:03.043 
Epoch 516/1000 
	 loss: 34.1112, MinusLogProbMetric: 34.1112, val_loss: 34.6781, val_MinusLogProbMetric: 34.6781

Epoch 516: val_loss did not improve from 34.48191
196/196 - 61s - loss: 34.1112 - MinusLogProbMetric: 34.1112 - val_loss: 34.6781 - val_MinusLogProbMetric: 34.6781 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 517/1000
2023-10-31 01:16:04.005 
Epoch 517/1000 
	 loss: 34.1092, MinusLogProbMetric: 34.1092, val_loss: 34.6268, val_MinusLogProbMetric: 34.6268

Epoch 517: val_loss did not improve from 34.48191
196/196 - 61s - loss: 34.1092 - MinusLogProbMetric: 34.1092 - val_loss: 34.6268 - val_MinusLogProbMetric: 34.6268 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 518/1000
2023-10-31 01:17:04.232 
Epoch 518/1000 
	 loss: 34.1363, MinusLogProbMetric: 34.1363, val_loss: 34.6827, val_MinusLogProbMetric: 34.6827

Epoch 518: val_loss did not improve from 34.48191
196/196 - 60s - loss: 34.1363 - MinusLogProbMetric: 34.1363 - val_loss: 34.6827 - val_MinusLogProbMetric: 34.6827 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 519/1000
2023-10-31 01:18:05.073 
Epoch 519/1000 
	 loss: 34.1064, MinusLogProbMetric: 34.1064, val_loss: 34.5505, val_MinusLogProbMetric: 34.5505

Epoch 519: val_loss did not improve from 34.48191
196/196 - 61s - loss: 34.1064 - MinusLogProbMetric: 34.1064 - val_loss: 34.5505 - val_MinusLogProbMetric: 34.5505 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 520/1000
2023-10-31 01:19:07.405 
Epoch 520/1000 
	 loss: 34.0934, MinusLogProbMetric: 34.0934, val_loss: 34.5090, val_MinusLogProbMetric: 34.5090

Epoch 520: val_loss did not improve from 34.48191
196/196 - 62s - loss: 34.0934 - MinusLogProbMetric: 34.0934 - val_loss: 34.5090 - val_MinusLogProbMetric: 34.5090 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 521/1000
2023-10-31 01:20:08.690 
Epoch 521/1000 
	 loss: 34.0699, MinusLogProbMetric: 34.0699, val_loss: 34.4924, val_MinusLogProbMetric: 34.4924

Epoch 521: val_loss did not improve from 34.48191
196/196 - 61s - loss: 34.0699 - MinusLogProbMetric: 34.0699 - val_loss: 34.4924 - val_MinusLogProbMetric: 34.4924 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 522/1000
2023-10-31 01:21:11.772 
Epoch 522/1000 
	 loss: 34.0884, MinusLogProbMetric: 34.0884, val_loss: 34.4236, val_MinusLogProbMetric: 34.4236

Epoch 522: val_loss improved from 34.48191 to 34.42358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 34.0884 - MinusLogProbMetric: 34.0884 - val_loss: 34.4236 - val_MinusLogProbMetric: 34.4236 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 523/1000
2023-10-31 01:22:15.640 
Epoch 523/1000 
	 loss: 34.0932, MinusLogProbMetric: 34.0932, val_loss: 34.4006, val_MinusLogProbMetric: 34.4006

Epoch 523: val_loss improved from 34.42358 to 34.40063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 34.0932 - MinusLogProbMetric: 34.0932 - val_loss: 34.4006 - val_MinusLogProbMetric: 34.4006 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 524/1000
2023-10-31 01:23:16.470 
Epoch 524/1000 
	 loss: 34.0443, MinusLogProbMetric: 34.0443, val_loss: 34.4464, val_MinusLogProbMetric: 34.4464

Epoch 524: val_loss did not improve from 34.40063
196/196 - 60s - loss: 34.0443 - MinusLogProbMetric: 34.0443 - val_loss: 34.4464 - val_MinusLogProbMetric: 34.4464 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 525/1000
2023-10-31 01:24:12.638 
Epoch 525/1000 
	 loss: 34.0698, MinusLogProbMetric: 34.0698, val_loss: 34.5109, val_MinusLogProbMetric: 34.5109

Epoch 525: val_loss did not improve from 34.40063
196/196 - 56s - loss: 34.0698 - MinusLogProbMetric: 34.0698 - val_loss: 34.5109 - val_MinusLogProbMetric: 34.5109 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 526/1000
2023-10-31 01:25:09.654 
Epoch 526/1000 
	 loss: 34.0404, MinusLogProbMetric: 34.0404, val_loss: 34.6406, val_MinusLogProbMetric: 34.6406

Epoch 526: val_loss did not improve from 34.40063
196/196 - 57s - loss: 34.0404 - MinusLogProbMetric: 34.0404 - val_loss: 34.6406 - val_MinusLogProbMetric: 34.6406 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 527/1000
2023-10-31 01:26:10.522 
Epoch 527/1000 
	 loss: 34.0567, MinusLogProbMetric: 34.0567, val_loss: 34.4338, val_MinusLogProbMetric: 34.4338

Epoch 527: val_loss did not improve from 34.40063
196/196 - 61s - loss: 34.0567 - MinusLogProbMetric: 34.0567 - val_loss: 34.4338 - val_MinusLogProbMetric: 34.4338 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 528/1000
2023-10-31 01:27:11.131 
Epoch 528/1000 
	 loss: 34.0696, MinusLogProbMetric: 34.0696, val_loss: 34.5264, val_MinusLogProbMetric: 34.5264

Epoch 528: val_loss did not improve from 34.40063
196/196 - 61s - loss: 34.0696 - MinusLogProbMetric: 34.0696 - val_loss: 34.5264 - val_MinusLogProbMetric: 34.5264 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 529/1000
2023-10-31 01:28:12.108 
Epoch 529/1000 
	 loss: 34.0519, MinusLogProbMetric: 34.0519, val_loss: 34.4056, val_MinusLogProbMetric: 34.4056

Epoch 529: val_loss did not improve from 34.40063
196/196 - 61s - loss: 34.0519 - MinusLogProbMetric: 34.0519 - val_loss: 34.4056 - val_MinusLogProbMetric: 34.4056 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 530/1000
2023-10-31 01:29:13.038 
Epoch 530/1000 
	 loss: 33.9877, MinusLogProbMetric: 33.9877, val_loss: 34.3833, val_MinusLogProbMetric: 34.3833

Epoch 530: val_loss improved from 34.40063 to 34.38326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.9877 - MinusLogProbMetric: 33.9877 - val_loss: 34.3833 - val_MinusLogProbMetric: 34.3833 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 531/1000
2023-10-31 01:30:18.072 
Epoch 531/1000 
	 loss: 34.0424, MinusLogProbMetric: 34.0424, val_loss: 34.4113, val_MinusLogProbMetric: 34.4113

Epoch 531: val_loss did not improve from 34.38326
196/196 - 64s - loss: 34.0424 - MinusLogProbMetric: 34.0424 - val_loss: 34.4113 - val_MinusLogProbMetric: 34.4113 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 532/1000
2023-10-31 01:31:19.310 
Epoch 532/1000 
	 loss: 34.0208, MinusLogProbMetric: 34.0208, val_loss: 34.4623, val_MinusLogProbMetric: 34.4623

Epoch 532: val_loss did not improve from 34.38326
196/196 - 61s - loss: 34.0208 - MinusLogProbMetric: 34.0208 - val_loss: 34.4623 - val_MinusLogProbMetric: 34.4623 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 533/1000
2023-10-31 01:32:16.840 
Epoch 533/1000 
	 loss: 34.0315, MinusLogProbMetric: 34.0315, val_loss: 34.6027, val_MinusLogProbMetric: 34.6027

Epoch 533: val_loss did not improve from 34.38326
196/196 - 58s - loss: 34.0315 - MinusLogProbMetric: 34.0315 - val_loss: 34.6027 - val_MinusLogProbMetric: 34.6027 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 534/1000
2023-10-31 01:33:14.375 
Epoch 534/1000 
	 loss: 34.0807, MinusLogProbMetric: 34.0807, val_loss: 34.5866, val_MinusLogProbMetric: 34.5866

Epoch 534: val_loss did not improve from 34.38326
196/196 - 58s - loss: 34.0807 - MinusLogProbMetric: 34.0807 - val_loss: 34.5866 - val_MinusLogProbMetric: 34.5866 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 535/1000
2023-10-31 01:34:15.854 
Epoch 535/1000 
	 loss: 33.9932, MinusLogProbMetric: 33.9932, val_loss: 34.4258, val_MinusLogProbMetric: 34.4258

Epoch 535: val_loss did not improve from 34.38326
196/196 - 61s - loss: 33.9932 - MinusLogProbMetric: 33.9932 - val_loss: 34.4258 - val_MinusLogProbMetric: 34.4258 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 536/1000
2023-10-31 01:35:18.600 
Epoch 536/1000 
	 loss: 33.9978, MinusLogProbMetric: 33.9978, val_loss: 34.2398, val_MinusLogProbMetric: 34.2398

Epoch 536: val_loss improved from 34.38326 to 34.23976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 33.9978 - MinusLogProbMetric: 33.9978 - val_loss: 34.2398 - val_MinusLogProbMetric: 34.2398 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 537/1000
2023-10-31 01:36:22.427 
Epoch 537/1000 
	 loss: 33.9601, MinusLogProbMetric: 33.9601, val_loss: 34.3316, val_MinusLogProbMetric: 34.3316

Epoch 537: val_loss did not improve from 34.23976
196/196 - 63s - loss: 33.9601 - MinusLogProbMetric: 33.9601 - val_loss: 34.3316 - val_MinusLogProbMetric: 34.3316 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 538/1000
2023-10-31 01:37:24.926 
Epoch 538/1000 
	 loss: 33.9401, MinusLogProbMetric: 33.9401, val_loss: 34.3269, val_MinusLogProbMetric: 34.3269

Epoch 538: val_loss did not improve from 34.23976
196/196 - 62s - loss: 33.9401 - MinusLogProbMetric: 33.9401 - val_loss: 34.3269 - val_MinusLogProbMetric: 34.3269 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 539/1000
2023-10-31 01:38:26.946 
Epoch 539/1000 
	 loss: 33.9850, MinusLogProbMetric: 33.9850, val_loss: 34.4488, val_MinusLogProbMetric: 34.4488

Epoch 539: val_loss did not improve from 34.23976
196/196 - 62s - loss: 33.9850 - MinusLogProbMetric: 33.9850 - val_loss: 34.4488 - val_MinusLogProbMetric: 34.4488 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 540/1000
2023-10-31 01:39:28.499 
Epoch 540/1000 
	 loss: 33.9800, MinusLogProbMetric: 33.9800, val_loss: 34.4848, val_MinusLogProbMetric: 34.4848

Epoch 540: val_loss did not improve from 34.23976
196/196 - 62s - loss: 33.9800 - MinusLogProbMetric: 33.9800 - val_loss: 34.4848 - val_MinusLogProbMetric: 34.4848 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 541/1000
2023-10-31 01:40:32.500 
Epoch 541/1000 
	 loss: 33.9645, MinusLogProbMetric: 33.9645, val_loss: 34.4989, val_MinusLogProbMetric: 34.4989

Epoch 541: val_loss did not improve from 34.23976
196/196 - 64s - loss: 33.9645 - MinusLogProbMetric: 33.9645 - val_loss: 34.4989 - val_MinusLogProbMetric: 34.4989 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 542/1000
2023-10-31 01:41:30.645 
Epoch 542/1000 
	 loss: 33.9673, MinusLogProbMetric: 33.9673, val_loss: 34.2547, val_MinusLogProbMetric: 34.2547

Epoch 542: val_loss did not improve from 34.23976
196/196 - 58s - loss: 33.9673 - MinusLogProbMetric: 33.9673 - val_loss: 34.2547 - val_MinusLogProbMetric: 34.2547 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 543/1000
2023-10-31 01:42:33.240 
Epoch 543/1000 
	 loss: 33.9206, MinusLogProbMetric: 33.9206, val_loss: 34.3977, val_MinusLogProbMetric: 34.3977

Epoch 543: val_loss did not improve from 34.23976
196/196 - 63s - loss: 33.9206 - MinusLogProbMetric: 33.9206 - val_loss: 34.3977 - val_MinusLogProbMetric: 34.3977 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 544/1000
2023-10-31 01:43:35.654 
Epoch 544/1000 
	 loss: 33.9235, MinusLogProbMetric: 33.9235, val_loss: 34.3269, val_MinusLogProbMetric: 34.3269

Epoch 544: val_loss did not improve from 34.23976
196/196 - 62s - loss: 33.9235 - MinusLogProbMetric: 33.9235 - val_loss: 34.3269 - val_MinusLogProbMetric: 34.3269 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 545/1000
2023-10-31 01:44:37.935 
Epoch 545/1000 
	 loss: 33.9210, MinusLogProbMetric: 33.9210, val_loss: 34.2967, val_MinusLogProbMetric: 34.2967

Epoch 545: val_loss did not improve from 34.23976
196/196 - 62s - loss: 33.9210 - MinusLogProbMetric: 33.9210 - val_loss: 34.2967 - val_MinusLogProbMetric: 34.2967 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 546/1000
2023-10-31 01:45:38.559 
Epoch 546/1000 
	 loss: 33.9277, MinusLogProbMetric: 33.9277, val_loss: 34.4857, val_MinusLogProbMetric: 34.4857

Epoch 546: val_loss did not improve from 34.23976
196/196 - 61s - loss: 33.9277 - MinusLogProbMetric: 33.9277 - val_loss: 34.4857 - val_MinusLogProbMetric: 34.4857 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 547/1000
2023-10-31 01:46:36.307 
Epoch 547/1000 
	 loss: 33.9078, MinusLogProbMetric: 33.9078, val_loss: 34.3983, val_MinusLogProbMetric: 34.3983

Epoch 547: val_loss did not improve from 34.23976
196/196 - 58s - loss: 33.9078 - MinusLogProbMetric: 33.9078 - val_loss: 34.3983 - val_MinusLogProbMetric: 34.3983 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 548/1000
2023-10-31 01:47:34.940 
Epoch 548/1000 
	 loss: 33.9100, MinusLogProbMetric: 33.9100, val_loss: 34.4416, val_MinusLogProbMetric: 34.4416

Epoch 548: val_loss did not improve from 34.23976
196/196 - 59s - loss: 33.9100 - MinusLogProbMetric: 33.9100 - val_loss: 34.4416 - val_MinusLogProbMetric: 34.4416 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 549/1000
2023-10-31 01:48:33.544 
Epoch 549/1000 
	 loss: 33.9167, MinusLogProbMetric: 33.9167, val_loss: 34.2471, val_MinusLogProbMetric: 34.2471

Epoch 549: val_loss did not improve from 34.23976
196/196 - 59s - loss: 33.9167 - MinusLogProbMetric: 33.9167 - val_loss: 34.2471 - val_MinusLogProbMetric: 34.2471 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 550/1000
2023-10-31 01:49:29.977 
Epoch 550/1000 
	 loss: 33.9030, MinusLogProbMetric: 33.9030, val_loss: 34.3009, val_MinusLogProbMetric: 34.3009

Epoch 550: val_loss did not improve from 34.23976
196/196 - 56s - loss: 33.9030 - MinusLogProbMetric: 33.9030 - val_loss: 34.3009 - val_MinusLogProbMetric: 34.3009 - lr: 4.1152e-06 - 56s/epoch - 288ms/step
Epoch 551/1000
2023-10-31 01:50:32.251 
Epoch 551/1000 
	 loss: 33.8874, MinusLogProbMetric: 33.8874, val_loss: 34.2791, val_MinusLogProbMetric: 34.2791

Epoch 551: val_loss did not improve from 34.23976
196/196 - 62s - loss: 33.8874 - MinusLogProbMetric: 33.8874 - val_loss: 34.2791 - val_MinusLogProbMetric: 34.2791 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 552/1000
2023-10-31 01:51:29.175 
Epoch 552/1000 
	 loss: 33.8826, MinusLogProbMetric: 33.8826, val_loss: 34.3454, val_MinusLogProbMetric: 34.3454

Epoch 552: val_loss did not improve from 34.23976
196/196 - 57s - loss: 33.8826 - MinusLogProbMetric: 33.8826 - val_loss: 34.3454 - val_MinusLogProbMetric: 34.3454 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 553/1000
2023-10-31 01:52:30.564 
Epoch 553/1000 
	 loss: 33.8990, MinusLogProbMetric: 33.8990, val_loss: 34.3439, val_MinusLogProbMetric: 34.3439

Epoch 553: val_loss did not improve from 34.23976
196/196 - 61s - loss: 33.8990 - MinusLogProbMetric: 33.8990 - val_loss: 34.3439 - val_MinusLogProbMetric: 34.3439 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 554/1000
2023-10-31 01:53:31.804 
Epoch 554/1000 
	 loss: 33.8769, MinusLogProbMetric: 33.8769, val_loss: 34.2213, val_MinusLogProbMetric: 34.2213

Epoch 554: val_loss improved from 34.23976 to 34.22134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.8769 - MinusLogProbMetric: 33.8769 - val_loss: 34.2213 - val_MinusLogProbMetric: 34.2213 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 555/1000
2023-10-31 01:54:32.654 
Epoch 555/1000 
	 loss: 33.8465, MinusLogProbMetric: 33.8465, val_loss: 34.5002, val_MinusLogProbMetric: 34.5002

Epoch 555: val_loss did not improve from 34.22134
196/196 - 60s - loss: 33.8465 - MinusLogProbMetric: 33.8465 - val_loss: 34.5002 - val_MinusLogProbMetric: 34.5002 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 556/1000
2023-10-31 01:55:33.871 
Epoch 556/1000 
	 loss: 33.8673, MinusLogProbMetric: 33.8673, val_loss: 34.1906, val_MinusLogProbMetric: 34.1906

Epoch 556: val_loss improved from 34.22134 to 34.19055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.8673 - MinusLogProbMetric: 33.8673 - val_loss: 34.1906 - val_MinusLogProbMetric: 34.1906 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 557/1000
2023-10-31 01:56:30.811 
Epoch 557/1000 
	 loss: 33.8333, MinusLogProbMetric: 33.8333, val_loss: 34.2559, val_MinusLogProbMetric: 34.2559

Epoch 557: val_loss did not improve from 34.19055
196/196 - 56s - loss: 33.8333 - MinusLogProbMetric: 33.8333 - val_loss: 34.2559 - val_MinusLogProbMetric: 34.2559 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 558/1000
2023-10-31 01:57:27.135 
Epoch 558/1000 
	 loss: 38.3850, MinusLogProbMetric: 38.3850, val_loss: 36.8072, val_MinusLogProbMetric: 36.8072

Epoch 558: val_loss did not improve from 34.19055
196/196 - 56s - loss: 38.3850 - MinusLogProbMetric: 38.3850 - val_loss: 36.8072 - val_MinusLogProbMetric: 36.8072 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 559/1000
2023-10-31 01:58:23.156 
Epoch 559/1000 
	 loss: 35.5492, MinusLogProbMetric: 35.5492, val_loss: 35.4739, val_MinusLogProbMetric: 35.4739

Epoch 559: val_loss did not improve from 34.19055
196/196 - 56s - loss: 35.5492 - MinusLogProbMetric: 35.5492 - val_loss: 35.4739 - val_MinusLogProbMetric: 35.4739 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 560/1000
2023-10-31 01:59:18.212 
Epoch 560/1000 
	 loss: 34.7408, MinusLogProbMetric: 34.7408, val_loss: 34.9878, val_MinusLogProbMetric: 34.9878

Epoch 560: val_loss did not improve from 34.19055
196/196 - 55s - loss: 34.7408 - MinusLogProbMetric: 34.7408 - val_loss: 34.9878 - val_MinusLogProbMetric: 34.9878 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 561/1000
2023-10-31 02:00:14.043 
Epoch 561/1000 
	 loss: 34.5804, MinusLogProbMetric: 34.5804, val_loss: 35.0047, val_MinusLogProbMetric: 35.0047

Epoch 561: val_loss did not improve from 34.19055
196/196 - 56s - loss: 34.5804 - MinusLogProbMetric: 34.5804 - val_loss: 35.0047 - val_MinusLogProbMetric: 35.0047 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 562/1000
2023-10-31 02:01:14.472 
Epoch 562/1000 
	 loss: 34.5290, MinusLogProbMetric: 34.5290, val_loss: 35.0837, val_MinusLogProbMetric: 35.0837

Epoch 562: val_loss did not improve from 34.19055
196/196 - 60s - loss: 34.5290 - MinusLogProbMetric: 34.5290 - val_loss: 35.0837 - val_MinusLogProbMetric: 35.0837 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 563/1000
2023-10-31 02:02:14.865 
Epoch 563/1000 
	 loss: 34.2011, MinusLogProbMetric: 34.2011, val_loss: 34.3482, val_MinusLogProbMetric: 34.3482

Epoch 563: val_loss did not improve from 34.19055
196/196 - 60s - loss: 34.2011 - MinusLogProbMetric: 34.2011 - val_loss: 34.3482 - val_MinusLogProbMetric: 34.3482 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 564/1000
2023-10-31 02:03:11.815 
Epoch 564/1000 
	 loss: 33.9347, MinusLogProbMetric: 33.9347, val_loss: 34.6303, val_MinusLogProbMetric: 34.6303

Epoch 564: val_loss did not improve from 34.19055
196/196 - 57s - loss: 33.9347 - MinusLogProbMetric: 33.9347 - val_loss: 34.6303 - val_MinusLogProbMetric: 34.6303 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 565/1000
2023-10-31 02:04:09.805 
Epoch 565/1000 
	 loss: 33.9139, MinusLogProbMetric: 33.9139, val_loss: 34.2468, val_MinusLogProbMetric: 34.2468

Epoch 565: val_loss did not improve from 34.19055
196/196 - 58s - loss: 33.9139 - MinusLogProbMetric: 33.9139 - val_loss: 34.2468 - val_MinusLogProbMetric: 34.2468 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 566/1000
2023-10-31 02:05:06.680 
Epoch 566/1000 
	 loss: 33.8698, MinusLogProbMetric: 33.8698, val_loss: 34.2849, val_MinusLogProbMetric: 34.2849

Epoch 566: val_loss did not improve from 34.19055
196/196 - 57s - loss: 33.8698 - MinusLogProbMetric: 33.8698 - val_loss: 34.2849 - val_MinusLogProbMetric: 34.2849 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 567/1000
2023-10-31 02:06:02.818 
Epoch 567/1000 
	 loss: 33.8515, MinusLogProbMetric: 33.8515, val_loss: 34.2103, val_MinusLogProbMetric: 34.2103

Epoch 567: val_loss did not improve from 34.19055
196/196 - 56s - loss: 33.8515 - MinusLogProbMetric: 33.8515 - val_loss: 34.2103 - val_MinusLogProbMetric: 34.2103 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 568/1000
2023-10-31 02:06:59.665 
Epoch 568/1000 
	 loss: 33.8170, MinusLogProbMetric: 33.8170, val_loss: 34.1934, val_MinusLogProbMetric: 34.1934

Epoch 568: val_loss did not improve from 34.19055
196/196 - 57s - loss: 33.8170 - MinusLogProbMetric: 33.8170 - val_loss: 34.1934 - val_MinusLogProbMetric: 34.1934 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 569/1000
2023-10-31 02:07:56.216 
Epoch 569/1000 
	 loss: 33.8291, MinusLogProbMetric: 33.8291, val_loss: 34.3325, val_MinusLogProbMetric: 34.3325

Epoch 569: val_loss did not improve from 34.19055
196/196 - 57s - loss: 33.8291 - MinusLogProbMetric: 33.8291 - val_loss: 34.3325 - val_MinusLogProbMetric: 34.3325 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 570/1000
2023-10-31 02:08:52.874 
Epoch 570/1000 
	 loss: 33.8373, MinusLogProbMetric: 33.8373, val_loss: 34.2490, val_MinusLogProbMetric: 34.2490

Epoch 570: val_loss did not improve from 34.19055
196/196 - 57s - loss: 33.8373 - MinusLogProbMetric: 33.8373 - val_loss: 34.2490 - val_MinusLogProbMetric: 34.2490 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 571/1000
2023-10-31 02:09:49.463 
Epoch 571/1000 
	 loss: 33.8101, MinusLogProbMetric: 33.8101, val_loss: 34.3718, val_MinusLogProbMetric: 34.3718

Epoch 571: val_loss did not improve from 34.19055
196/196 - 57s - loss: 33.8101 - MinusLogProbMetric: 33.8101 - val_loss: 34.3718 - val_MinusLogProbMetric: 34.3718 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 572/1000
2023-10-31 02:10:47.151 
Epoch 572/1000 
	 loss: 33.8285, MinusLogProbMetric: 33.8285, val_loss: 34.3170, val_MinusLogProbMetric: 34.3170

Epoch 572: val_loss did not improve from 34.19055
196/196 - 58s - loss: 33.8285 - MinusLogProbMetric: 33.8285 - val_loss: 34.3170 - val_MinusLogProbMetric: 34.3170 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 573/1000
2023-10-31 02:11:47.381 
Epoch 573/1000 
	 loss: 33.8100, MinusLogProbMetric: 33.8100, val_loss: 34.2430, val_MinusLogProbMetric: 34.2430

Epoch 573: val_loss did not improve from 34.19055
196/196 - 60s - loss: 33.8100 - MinusLogProbMetric: 33.8100 - val_loss: 34.2430 - val_MinusLogProbMetric: 34.2430 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 574/1000
2023-10-31 02:12:45.841 
Epoch 574/1000 
	 loss: 33.7828, MinusLogProbMetric: 33.7828, val_loss: 34.1447, val_MinusLogProbMetric: 34.1447

Epoch 574: val_loss improved from 34.19055 to 34.14467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 33.7828 - MinusLogProbMetric: 33.7828 - val_loss: 34.1447 - val_MinusLogProbMetric: 34.1447 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 575/1000
2023-10-31 02:13:47.832 
Epoch 575/1000 
	 loss: 33.7466, MinusLogProbMetric: 33.7466, val_loss: 34.1102, val_MinusLogProbMetric: 34.1102

Epoch 575: val_loss improved from 34.14467 to 34.11020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 33.7466 - MinusLogProbMetric: 33.7466 - val_loss: 34.1102 - val_MinusLogProbMetric: 34.1102 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 576/1000
2023-10-31 02:14:49.696 
Epoch 576/1000 
	 loss: 33.7277, MinusLogProbMetric: 33.7277, val_loss: 34.2570, val_MinusLogProbMetric: 34.2570

Epoch 576: val_loss did not improve from 34.11020
196/196 - 61s - loss: 33.7277 - MinusLogProbMetric: 33.7277 - val_loss: 34.2570 - val_MinusLogProbMetric: 34.2570 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 577/1000
2023-10-31 02:15:51.071 
Epoch 577/1000 
	 loss: 33.7448, MinusLogProbMetric: 33.7448, val_loss: 34.1218, val_MinusLogProbMetric: 34.1218

Epoch 577: val_loss did not improve from 34.11020
196/196 - 61s - loss: 33.7448 - MinusLogProbMetric: 33.7448 - val_loss: 34.1218 - val_MinusLogProbMetric: 34.1218 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 578/1000
2023-10-31 02:16:51.188 
Epoch 578/1000 
	 loss: 33.6806, MinusLogProbMetric: 33.6806, val_loss: 34.1712, val_MinusLogProbMetric: 34.1712

Epoch 578: val_loss did not improve from 34.11020
196/196 - 60s - loss: 33.6806 - MinusLogProbMetric: 33.6806 - val_loss: 34.1712 - val_MinusLogProbMetric: 34.1712 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 579/1000
2023-10-31 02:17:51.327 
Epoch 579/1000 
	 loss: 33.7211, MinusLogProbMetric: 33.7211, val_loss: 34.0512, val_MinusLogProbMetric: 34.0512

Epoch 579: val_loss improved from 34.11020 to 34.05123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 33.7211 - MinusLogProbMetric: 33.7211 - val_loss: 34.0512 - val_MinusLogProbMetric: 34.0512 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 580/1000
2023-10-31 02:18:52.119 
Epoch 580/1000 
	 loss: 33.6891, MinusLogProbMetric: 33.6891, val_loss: 34.1102, val_MinusLogProbMetric: 34.1102

Epoch 580: val_loss did not improve from 34.05123
196/196 - 60s - loss: 33.6891 - MinusLogProbMetric: 33.6891 - val_loss: 34.1102 - val_MinusLogProbMetric: 34.1102 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 581/1000
2023-10-31 02:19:51.016 
Epoch 581/1000 
	 loss: 33.8226, MinusLogProbMetric: 33.8226, val_loss: 34.0659, val_MinusLogProbMetric: 34.0659

Epoch 581: val_loss did not improve from 34.05123
196/196 - 59s - loss: 33.8226 - MinusLogProbMetric: 33.8226 - val_loss: 34.0659 - val_MinusLogProbMetric: 34.0659 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 582/1000
2023-10-31 02:20:53.073 
Epoch 582/1000 
	 loss: 33.7496, MinusLogProbMetric: 33.7496, val_loss: 34.2224, val_MinusLogProbMetric: 34.2224

Epoch 582: val_loss did not improve from 34.05123
196/196 - 62s - loss: 33.7496 - MinusLogProbMetric: 33.7496 - val_loss: 34.2224 - val_MinusLogProbMetric: 34.2224 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 583/1000
2023-10-31 02:21:52.046 
Epoch 583/1000 
	 loss: 33.7045, MinusLogProbMetric: 33.7045, val_loss: 34.1359, val_MinusLogProbMetric: 34.1359

Epoch 583: val_loss did not improve from 34.05123
196/196 - 59s - loss: 33.7045 - MinusLogProbMetric: 33.7045 - val_loss: 34.1359 - val_MinusLogProbMetric: 34.1359 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 584/1000
2023-10-31 02:22:48.759 
Epoch 584/1000 
	 loss: 33.7022, MinusLogProbMetric: 33.7022, val_loss: 34.0937, val_MinusLogProbMetric: 34.0937

Epoch 584: val_loss did not improve from 34.05123
196/196 - 57s - loss: 33.7022 - MinusLogProbMetric: 33.7022 - val_loss: 34.0937 - val_MinusLogProbMetric: 34.0937 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 585/1000
2023-10-31 02:23:46.504 
Epoch 585/1000 
	 loss: 33.6950, MinusLogProbMetric: 33.6950, val_loss: 33.9976, val_MinusLogProbMetric: 33.9976

Epoch 585: val_loss improved from 34.05123 to 33.99757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 33.6950 - MinusLogProbMetric: 33.6950 - val_loss: 33.9976 - val_MinusLogProbMetric: 33.9976 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 586/1000
2023-10-31 02:24:44.876 
Epoch 586/1000 
	 loss: 33.6946, MinusLogProbMetric: 33.6946, val_loss: 34.0426, val_MinusLogProbMetric: 34.0426

Epoch 586: val_loss did not improve from 33.99757
196/196 - 57s - loss: 33.6946 - MinusLogProbMetric: 33.6946 - val_loss: 34.0426 - val_MinusLogProbMetric: 34.0426 - lr: 4.1152e-06 - 57s/epoch - 293ms/step
Epoch 587/1000
2023-10-31 02:25:42.450 
Epoch 587/1000 
	 loss: 33.6995, MinusLogProbMetric: 33.6995, val_loss: 34.0527, val_MinusLogProbMetric: 34.0527

Epoch 587: val_loss did not improve from 33.99757
196/196 - 58s - loss: 33.6995 - MinusLogProbMetric: 33.6995 - val_loss: 34.0527 - val_MinusLogProbMetric: 34.0527 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 588/1000
2023-10-31 02:26:39.612 
Epoch 588/1000 
	 loss: 33.6603, MinusLogProbMetric: 33.6603, val_loss: 34.2167, val_MinusLogProbMetric: 34.2167

Epoch 588: val_loss did not improve from 33.99757
196/196 - 57s - loss: 33.6603 - MinusLogProbMetric: 33.6603 - val_loss: 34.2167 - val_MinusLogProbMetric: 34.2167 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 589/1000
2023-10-31 02:27:36.491 
Epoch 589/1000 
	 loss: 33.6512, MinusLogProbMetric: 33.6512, val_loss: 34.0123, val_MinusLogProbMetric: 34.0123

Epoch 589: val_loss did not improve from 33.99757
196/196 - 57s - loss: 33.6512 - MinusLogProbMetric: 33.6512 - val_loss: 34.0123 - val_MinusLogProbMetric: 34.0123 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 590/1000
2023-10-31 02:28:36.228 
Epoch 590/1000 
	 loss: 33.6492, MinusLogProbMetric: 33.6492, val_loss: 34.0587, val_MinusLogProbMetric: 34.0587

Epoch 590: val_loss did not improve from 33.99757
196/196 - 60s - loss: 33.6492 - MinusLogProbMetric: 33.6492 - val_loss: 34.0587 - val_MinusLogProbMetric: 34.0587 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 591/1000
2023-10-31 02:29:35.392 
Epoch 591/1000 
	 loss: 33.6519, MinusLogProbMetric: 33.6519, val_loss: 34.0708, val_MinusLogProbMetric: 34.0708

Epoch 591: val_loss did not improve from 33.99757
196/196 - 59s - loss: 33.6519 - MinusLogProbMetric: 33.6519 - val_loss: 34.0708 - val_MinusLogProbMetric: 34.0708 - lr: 4.1152e-06 - 59s/epoch - 302ms/step
Epoch 592/1000
2023-10-31 02:30:36.641 
Epoch 592/1000 
	 loss: 33.6246, MinusLogProbMetric: 33.6246, val_loss: 33.9279, val_MinusLogProbMetric: 33.9279

Epoch 592: val_loss improved from 33.99757 to 33.92792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.6246 - MinusLogProbMetric: 33.6246 - val_loss: 33.9279 - val_MinusLogProbMetric: 33.9279 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 593/1000
2023-10-31 02:31:34.631 
Epoch 593/1000 
	 loss: 33.6384, MinusLogProbMetric: 33.6384, val_loss: 34.1785, val_MinusLogProbMetric: 34.1785

Epoch 593: val_loss did not improve from 33.92792
196/196 - 57s - loss: 33.6384 - MinusLogProbMetric: 33.6384 - val_loss: 34.1785 - val_MinusLogProbMetric: 34.1785 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 594/1000
2023-10-31 02:32:36.343 
Epoch 594/1000 
	 loss: 33.6267, MinusLogProbMetric: 33.6267, val_loss: 34.0615, val_MinusLogProbMetric: 34.0615

Epoch 594: val_loss did not improve from 33.92792
196/196 - 62s - loss: 33.6267 - MinusLogProbMetric: 33.6267 - val_loss: 34.0615 - val_MinusLogProbMetric: 34.0615 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 595/1000
2023-10-31 02:33:38.774 
Epoch 595/1000 
	 loss: 33.6420, MinusLogProbMetric: 33.6420, val_loss: 34.0960, val_MinusLogProbMetric: 34.0960

Epoch 595: val_loss did not improve from 33.92792
196/196 - 62s - loss: 33.6420 - MinusLogProbMetric: 33.6420 - val_loss: 34.0960 - val_MinusLogProbMetric: 34.0960 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 596/1000
2023-10-31 02:34:33.762 
Epoch 596/1000 
	 loss: 33.6074, MinusLogProbMetric: 33.6074, val_loss: 33.9866, val_MinusLogProbMetric: 33.9866

Epoch 596: val_loss did not improve from 33.92792
196/196 - 55s - loss: 33.6074 - MinusLogProbMetric: 33.6074 - val_loss: 33.9866 - val_MinusLogProbMetric: 33.9866 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 597/1000
2023-10-31 02:35:30.711 
Epoch 597/1000 
	 loss: 33.6168, MinusLogProbMetric: 33.6168, val_loss: 33.9949, val_MinusLogProbMetric: 33.9949

Epoch 597: val_loss did not improve from 33.92792
196/196 - 57s - loss: 33.6168 - MinusLogProbMetric: 33.6168 - val_loss: 33.9949 - val_MinusLogProbMetric: 33.9949 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 598/1000
2023-10-31 02:36:28.277 
Epoch 598/1000 
	 loss: 33.6105, MinusLogProbMetric: 33.6105, val_loss: 34.6158, val_MinusLogProbMetric: 34.6158

Epoch 598: val_loss did not improve from 33.92792
196/196 - 58s - loss: 33.6105 - MinusLogProbMetric: 33.6105 - val_loss: 34.6158 - val_MinusLogProbMetric: 34.6158 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 599/1000
2023-10-31 02:37:26.408 
Epoch 599/1000 
	 loss: 33.5957, MinusLogProbMetric: 33.5957, val_loss: 33.9260, val_MinusLogProbMetric: 33.9260

Epoch 599: val_loss improved from 33.92792 to 33.92596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 33.5957 - MinusLogProbMetric: 33.5957 - val_loss: 33.9260 - val_MinusLogProbMetric: 33.9260 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 600/1000
2023-10-31 02:38:25.811 
Epoch 600/1000 
	 loss: 33.5602, MinusLogProbMetric: 33.5602, val_loss: 34.0257, val_MinusLogProbMetric: 34.0257

Epoch 600: val_loss did not improve from 33.92596
196/196 - 59s - loss: 33.5602 - MinusLogProbMetric: 33.5602 - val_loss: 34.0257 - val_MinusLogProbMetric: 34.0257 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 601/1000
2023-10-31 02:39:27.063 
Epoch 601/1000 
	 loss: 33.5872, MinusLogProbMetric: 33.5872, val_loss: 33.9356, val_MinusLogProbMetric: 33.9356

Epoch 601: val_loss did not improve from 33.92596
196/196 - 61s - loss: 33.5872 - MinusLogProbMetric: 33.5872 - val_loss: 33.9356 - val_MinusLogProbMetric: 33.9356 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 602/1000
2023-10-31 02:40:30.826 
Epoch 602/1000 
	 loss: 33.5802, MinusLogProbMetric: 33.5802, val_loss: 34.1105, val_MinusLogProbMetric: 34.1105

Epoch 602: val_loss did not improve from 33.92596
196/196 - 64s - loss: 33.5802 - MinusLogProbMetric: 33.5802 - val_loss: 34.1105 - val_MinusLogProbMetric: 34.1105 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 603/1000
2023-10-31 02:41:33.235 
Epoch 603/1000 
	 loss: 33.6056, MinusLogProbMetric: 33.6056, val_loss: 34.0649, val_MinusLogProbMetric: 34.0649

Epoch 603: val_loss did not improve from 33.92596
196/196 - 62s - loss: 33.6056 - MinusLogProbMetric: 33.6056 - val_loss: 34.0649 - val_MinusLogProbMetric: 34.0649 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 604/1000
2023-10-31 02:42:38.153 
Epoch 604/1000 
	 loss: 33.5633, MinusLogProbMetric: 33.5633, val_loss: 34.0340, val_MinusLogProbMetric: 34.0340

Epoch 604: val_loss did not improve from 33.92596
196/196 - 65s - loss: 33.5633 - MinusLogProbMetric: 33.5633 - val_loss: 34.0340 - val_MinusLogProbMetric: 34.0340 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 605/1000
2023-10-31 02:43:40.255 
Epoch 605/1000 
	 loss: 33.5637, MinusLogProbMetric: 33.5637, val_loss: 33.9563, val_MinusLogProbMetric: 33.9563

Epoch 605: val_loss did not improve from 33.92596
196/196 - 62s - loss: 33.5637 - MinusLogProbMetric: 33.5637 - val_loss: 33.9563 - val_MinusLogProbMetric: 33.9563 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 606/1000
2023-10-31 02:44:40.670 
Epoch 606/1000 
	 loss: 33.5120, MinusLogProbMetric: 33.5120, val_loss: 33.9254, val_MinusLogProbMetric: 33.9254

Epoch 606: val_loss improved from 33.92596 to 33.92537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 33.5120 - MinusLogProbMetric: 33.5120 - val_loss: 33.9254 - val_MinusLogProbMetric: 33.9254 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 607/1000
2023-10-31 02:45:41.818 
Epoch 607/1000 
	 loss: 33.5396, MinusLogProbMetric: 33.5396, val_loss: 34.2410, val_MinusLogProbMetric: 34.2410

Epoch 607: val_loss did not improve from 33.92537
196/196 - 60s - loss: 33.5396 - MinusLogProbMetric: 33.5396 - val_loss: 34.2410 - val_MinusLogProbMetric: 34.2410 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 608/1000
2023-10-31 02:46:42.486 
Epoch 608/1000 
	 loss: 33.5474, MinusLogProbMetric: 33.5474, val_loss: 33.9932, val_MinusLogProbMetric: 33.9932

Epoch 608: val_loss did not improve from 33.92537
196/196 - 61s - loss: 33.5474 - MinusLogProbMetric: 33.5474 - val_loss: 33.9932 - val_MinusLogProbMetric: 33.9932 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 609/1000
2023-10-31 02:47:44.836 
Epoch 609/1000 
	 loss: 33.5131, MinusLogProbMetric: 33.5131, val_loss: 33.9124, val_MinusLogProbMetric: 33.9124

Epoch 609: val_loss improved from 33.92537 to 33.91240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 33.5131 - MinusLogProbMetric: 33.5131 - val_loss: 33.9124 - val_MinusLogProbMetric: 33.9124 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 610/1000
2023-10-31 02:48:49.885 
Epoch 610/1000 
	 loss: 33.5177, MinusLogProbMetric: 33.5177, val_loss: 33.9170, val_MinusLogProbMetric: 33.9170

Epoch 610: val_loss did not improve from 33.91240
196/196 - 63s - loss: 33.5177 - MinusLogProbMetric: 33.5177 - val_loss: 33.9170 - val_MinusLogProbMetric: 33.9170 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 611/1000
2023-10-31 02:49:52.354 
Epoch 611/1000 
	 loss: 33.5292, MinusLogProbMetric: 33.5292, val_loss: 34.0329, val_MinusLogProbMetric: 34.0329

Epoch 611: val_loss did not improve from 33.91240
196/196 - 62s - loss: 33.5292 - MinusLogProbMetric: 33.5292 - val_loss: 34.0329 - val_MinusLogProbMetric: 34.0329 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 612/1000
2023-10-31 02:50:53.762 
Epoch 612/1000 
	 loss: 33.5078, MinusLogProbMetric: 33.5078, val_loss: 34.0893, val_MinusLogProbMetric: 34.0893

Epoch 612: val_loss did not improve from 33.91240
196/196 - 61s - loss: 33.5078 - MinusLogProbMetric: 33.5078 - val_loss: 34.0893 - val_MinusLogProbMetric: 34.0893 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 613/1000
2023-10-31 02:51:55.219 
Epoch 613/1000 
	 loss: 33.5116, MinusLogProbMetric: 33.5116, val_loss: 33.9225, val_MinusLogProbMetric: 33.9225

Epoch 613: val_loss did not improve from 33.91240
196/196 - 61s - loss: 33.5116 - MinusLogProbMetric: 33.5116 - val_loss: 33.9225 - val_MinusLogProbMetric: 33.9225 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 614/1000
2023-10-31 02:52:57.642 
Epoch 614/1000 
	 loss: 33.4830, MinusLogProbMetric: 33.4830, val_loss: 33.8958, val_MinusLogProbMetric: 33.8958

Epoch 614: val_loss improved from 33.91240 to 33.89583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 33.4830 - MinusLogProbMetric: 33.4830 - val_loss: 33.8958 - val_MinusLogProbMetric: 33.8958 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 615/1000
2023-10-31 02:53:59.301 
Epoch 615/1000 
	 loss: 33.4718, MinusLogProbMetric: 33.4718, val_loss: 34.0728, val_MinusLogProbMetric: 34.0728

Epoch 615: val_loss did not improve from 33.89583
196/196 - 60s - loss: 33.4718 - MinusLogProbMetric: 33.4718 - val_loss: 34.0728 - val_MinusLogProbMetric: 34.0728 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 616/1000
2023-10-31 02:54:57.430 
Epoch 616/1000 
	 loss: 33.4857, MinusLogProbMetric: 33.4857, val_loss: 33.8850, val_MinusLogProbMetric: 33.8850

Epoch 616: val_loss improved from 33.89583 to 33.88498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 33.4857 - MinusLogProbMetric: 33.4857 - val_loss: 33.8850 - val_MinusLogProbMetric: 33.8850 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 617/1000
2023-10-31 02:55:57.121 
Epoch 617/1000 
	 loss: 33.4762, MinusLogProbMetric: 33.4762, val_loss: 34.0182, val_MinusLogProbMetric: 34.0182

Epoch 617: val_loss did not improve from 33.88498
196/196 - 59s - loss: 33.4762 - MinusLogProbMetric: 33.4762 - val_loss: 34.0182 - val_MinusLogProbMetric: 34.0182 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 618/1000
2023-10-31 02:56:56.911 
Epoch 618/1000 
	 loss: 33.4649, MinusLogProbMetric: 33.4649, val_loss: 33.8498, val_MinusLogProbMetric: 33.8498

Epoch 618: val_loss improved from 33.88498 to 33.84980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 33.4649 - MinusLogProbMetric: 33.4649 - val_loss: 33.8498 - val_MinusLogProbMetric: 33.8498 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 619/1000
2023-10-31 02:58:01.477 
Epoch 619/1000 
	 loss: 33.4405, MinusLogProbMetric: 33.4405, val_loss: 33.9268, val_MinusLogProbMetric: 33.9268

Epoch 619: val_loss did not improve from 33.84980
196/196 - 64s - loss: 33.4405 - MinusLogProbMetric: 33.4405 - val_loss: 33.9268 - val_MinusLogProbMetric: 33.9268 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 620/1000
2023-10-31 02:59:04.849 
Epoch 620/1000 
	 loss: 33.4603, MinusLogProbMetric: 33.4603, val_loss: 33.8886, val_MinusLogProbMetric: 33.8886

Epoch 620: val_loss did not improve from 33.84980
196/196 - 63s - loss: 33.4603 - MinusLogProbMetric: 33.4603 - val_loss: 33.8886 - val_MinusLogProbMetric: 33.8886 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 621/1000
2023-10-31 03:00:07.463 
Epoch 621/1000 
	 loss: 33.4695, MinusLogProbMetric: 33.4695, val_loss: 33.9216, val_MinusLogProbMetric: 33.9216

Epoch 621: val_loss did not improve from 33.84980
196/196 - 63s - loss: 33.4695 - MinusLogProbMetric: 33.4695 - val_loss: 33.9216 - val_MinusLogProbMetric: 33.9216 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 622/1000
2023-10-31 03:01:10.355 
Epoch 622/1000 
	 loss: 33.4412, MinusLogProbMetric: 33.4412, val_loss: 34.1473, val_MinusLogProbMetric: 34.1473

Epoch 622: val_loss did not improve from 33.84980
196/196 - 63s - loss: 33.4412 - MinusLogProbMetric: 33.4412 - val_loss: 34.1473 - val_MinusLogProbMetric: 34.1473 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 623/1000
2023-10-31 03:02:11.055 
Epoch 623/1000 
	 loss: 33.4493, MinusLogProbMetric: 33.4493, val_loss: 33.8665, val_MinusLogProbMetric: 33.8665

Epoch 623: val_loss did not improve from 33.84980
196/196 - 61s - loss: 33.4493 - MinusLogProbMetric: 33.4493 - val_loss: 33.8665 - val_MinusLogProbMetric: 33.8665 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 624/1000
2023-10-31 03:03:14.263 
Epoch 624/1000 
	 loss: 33.4653, MinusLogProbMetric: 33.4653, val_loss: 34.1787, val_MinusLogProbMetric: 34.1787

Epoch 624: val_loss did not improve from 33.84980
196/196 - 63s - loss: 33.4653 - MinusLogProbMetric: 33.4653 - val_loss: 34.1787 - val_MinusLogProbMetric: 34.1787 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 625/1000
2023-10-31 03:04:16.778 
Epoch 625/1000 
	 loss: 33.4537, MinusLogProbMetric: 33.4537, val_loss: 33.8399, val_MinusLogProbMetric: 33.8399

Epoch 625: val_loss improved from 33.84980 to 33.83994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 33.4537 - MinusLogProbMetric: 33.4537 - val_loss: 33.8399 - val_MinusLogProbMetric: 33.8399 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 626/1000
2023-10-31 03:05:22.578 
Epoch 626/1000 
	 loss: 33.4360, MinusLogProbMetric: 33.4360, val_loss: 33.7676, val_MinusLogProbMetric: 33.7676

Epoch 626: val_loss improved from 33.83994 to 33.76759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 33.4360 - MinusLogProbMetric: 33.4360 - val_loss: 33.7676 - val_MinusLogProbMetric: 33.7676 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 627/1000
2023-10-31 03:06:25.002 
Epoch 627/1000 
	 loss: 33.4340, MinusLogProbMetric: 33.4340, val_loss: 33.9108, val_MinusLogProbMetric: 33.9108

Epoch 627: val_loss did not improve from 33.76759
196/196 - 61s - loss: 33.4340 - MinusLogProbMetric: 33.4340 - val_loss: 33.9108 - val_MinusLogProbMetric: 33.9108 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 628/1000
2023-10-31 03:07:26.428 
Epoch 628/1000 
	 loss: 33.4684, MinusLogProbMetric: 33.4684, val_loss: 33.9420, val_MinusLogProbMetric: 33.9420

Epoch 628: val_loss did not improve from 33.76759
196/196 - 61s - loss: 33.4684 - MinusLogProbMetric: 33.4684 - val_loss: 33.9420 - val_MinusLogProbMetric: 33.9420 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 629/1000
2023-10-31 03:08:29.675 
Epoch 629/1000 
	 loss: 33.4108, MinusLogProbMetric: 33.4108, val_loss: 33.8916, val_MinusLogProbMetric: 33.8916

Epoch 629: val_loss did not improve from 33.76759
196/196 - 63s - loss: 33.4108 - MinusLogProbMetric: 33.4108 - val_loss: 33.8916 - val_MinusLogProbMetric: 33.8916 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 630/1000
2023-10-31 03:09:31.763 
Epoch 630/1000 
	 loss: 33.3956, MinusLogProbMetric: 33.3956, val_loss: 33.8275, val_MinusLogProbMetric: 33.8275

Epoch 630: val_loss did not improve from 33.76759
196/196 - 62s - loss: 33.3956 - MinusLogProbMetric: 33.3956 - val_loss: 33.8275 - val_MinusLogProbMetric: 33.8275 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 631/1000
2023-10-31 03:10:34.194 
Epoch 631/1000 
	 loss: 33.4055, MinusLogProbMetric: 33.4055, val_loss: 34.0287, val_MinusLogProbMetric: 34.0287

Epoch 631: val_loss did not improve from 33.76759
196/196 - 62s - loss: 33.4055 - MinusLogProbMetric: 33.4055 - val_loss: 34.0287 - val_MinusLogProbMetric: 34.0287 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 632/1000
2023-10-31 03:11:36.212 
Epoch 632/1000 
	 loss: 33.3931, MinusLogProbMetric: 33.3931, val_loss: 33.7784, val_MinusLogProbMetric: 33.7784

Epoch 632: val_loss did not improve from 33.76759
196/196 - 62s - loss: 33.3931 - MinusLogProbMetric: 33.3931 - val_loss: 33.7784 - val_MinusLogProbMetric: 33.7784 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 633/1000
2023-10-31 03:12:38.406 
Epoch 633/1000 
	 loss: 33.3642, MinusLogProbMetric: 33.3642, val_loss: 33.8180, val_MinusLogProbMetric: 33.8180

Epoch 633: val_loss did not improve from 33.76759
196/196 - 62s - loss: 33.3642 - MinusLogProbMetric: 33.3642 - val_loss: 33.8180 - val_MinusLogProbMetric: 33.8180 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 634/1000
2023-10-31 03:13:36.192 
Epoch 634/1000 
	 loss: 33.4003, MinusLogProbMetric: 33.4003, val_loss: 33.7694, val_MinusLogProbMetric: 33.7694

Epoch 634: val_loss did not improve from 33.76759
196/196 - 58s - loss: 33.4003 - MinusLogProbMetric: 33.4003 - val_loss: 33.7694 - val_MinusLogProbMetric: 33.7694 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 635/1000
2023-10-31 03:14:34.820 
Epoch 635/1000 
	 loss: 33.3469, MinusLogProbMetric: 33.3469, val_loss: 33.8885, val_MinusLogProbMetric: 33.8885

Epoch 635: val_loss did not improve from 33.76759
196/196 - 59s - loss: 33.3469 - MinusLogProbMetric: 33.3469 - val_loss: 33.8885 - val_MinusLogProbMetric: 33.8885 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 636/1000
2023-10-31 03:15:30.917 
Epoch 636/1000 
	 loss: 33.4232, MinusLogProbMetric: 33.4232, val_loss: 33.8103, val_MinusLogProbMetric: 33.8103

Epoch 636: val_loss did not improve from 33.76759
196/196 - 56s - loss: 33.4232 - MinusLogProbMetric: 33.4232 - val_loss: 33.8103 - val_MinusLogProbMetric: 33.8103 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 637/1000
2023-10-31 03:16:26.864 
Epoch 637/1000 
	 loss: 33.4082, MinusLogProbMetric: 33.4082, val_loss: 33.8101, val_MinusLogProbMetric: 33.8101

Epoch 637: val_loss did not improve from 33.76759
196/196 - 56s - loss: 33.4082 - MinusLogProbMetric: 33.4082 - val_loss: 33.8101 - val_MinusLogProbMetric: 33.8101 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 638/1000
2023-10-31 03:17:22.346 
Epoch 638/1000 
	 loss: 33.3558, MinusLogProbMetric: 33.3558, val_loss: 33.7419, val_MinusLogProbMetric: 33.7419

Epoch 638: val_loss improved from 33.76759 to 33.74187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 33.3558 - MinusLogProbMetric: 33.3558 - val_loss: 33.7419 - val_MinusLogProbMetric: 33.7419 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 639/1000
2023-10-31 03:18:19.295 
Epoch 639/1000 
	 loss: 33.3381, MinusLogProbMetric: 33.3381, val_loss: 33.7773, val_MinusLogProbMetric: 33.7773

Epoch 639: val_loss did not improve from 33.74187
196/196 - 56s - loss: 33.3381 - MinusLogProbMetric: 33.3381 - val_loss: 33.7773 - val_MinusLogProbMetric: 33.7773 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 640/1000
2023-10-31 03:19:16.199 
Epoch 640/1000 
	 loss: 33.3576, MinusLogProbMetric: 33.3576, val_loss: 33.8376, val_MinusLogProbMetric: 33.8376

Epoch 640: val_loss did not improve from 33.74187
196/196 - 57s - loss: 33.3576 - MinusLogProbMetric: 33.3576 - val_loss: 33.8376 - val_MinusLogProbMetric: 33.8376 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 641/1000
2023-10-31 03:20:11.582 
Epoch 641/1000 
	 loss: 33.3607, MinusLogProbMetric: 33.3607, val_loss: 33.8265, val_MinusLogProbMetric: 33.8265

Epoch 641: val_loss did not improve from 33.74187
196/196 - 55s - loss: 33.3607 - MinusLogProbMetric: 33.3607 - val_loss: 33.8265 - val_MinusLogProbMetric: 33.8265 - lr: 4.1152e-06 - 55s/epoch - 283ms/step
Epoch 642/1000
2023-10-31 03:21:08.469 
Epoch 642/1000 
	 loss: 33.3674, MinusLogProbMetric: 33.3674, val_loss: 33.7085, val_MinusLogProbMetric: 33.7085

Epoch 642: val_loss improved from 33.74187 to 33.70847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 33.3674 - MinusLogProbMetric: 33.3674 - val_loss: 33.7085 - val_MinusLogProbMetric: 33.7085 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 643/1000
2023-10-31 03:22:06.488 
Epoch 643/1000 
	 loss: 33.3341, MinusLogProbMetric: 33.3341, val_loss: 33.7444, val_MinusLogProbMetric: 33.7444

Epoch 643: val_loss did not improve from 33.70847
196/196 - 57s - loss: 33.3341 - MinusLogProbMetric: 33.3341 - val_loss: 33.7444 - val_MinusLogProbMetric: 33.7444 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 644/1000
2023-10-31 03:23:01.498 
Epoch 644/1000 
	 loss: 33.3360, MinusLogProbMetric: 33.3360, val_loss: 33.9087, val_MinusLogProbMetric: 33.9087

Epoch 644: val_loss did not improve from 33.70847
196/196 - 55s - loss: 33.3360 - MinusLogProbMetric: 33.3360 - val_loss: 33.9087 - val_MinusLogProbMetric: 33.9087 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 645/1000
2023-10-31 03:23:59.756 
Epoch 645/1000 
	 loss: 33.3220, MinusLogProbMetric: 33.3220, val_loss: 33.7089, val_MinusLogProbMetric: 33.7089

Epoch 645: val_loss did not improve from 33.70847
196/196 - 58s - loss: 33.3220 - MinusLogProbMetric: 33.3220 - val_loss: 33.7089 - val_MinusLogProbMetric: 33.7089 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 646/1000
2023-10-31 03:25:00.041 
Epoch 646/1000 
	 loss: 33.3058, MinusLogProbMetric: 33.3058, val_loss: 33.8441, val_MinusLogProbMetric: 33.8441

Epoch 646: val_loss did not improve from 33.70847
196/196 - 60s - loss: 33.3058 - MinusLogProbMetric: 33.3058 - val_loss: 33.8441 - val_MinusLogProbMetric: 33.8441 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 647/1000
2023-10-31 03:25:59.944 
Epoch 647/1000 
	 loss: 33.3196, MinusLogProbMetric: 33.3196, val_loss: 33.7681, val_MinusLogProbMetric: 33.7681

Epoch 647: val_loss did not improve from 33.70847
196/196 - 60s - loss: 33.3196 - MinusLogProbMetric: 33.3196 - val_loss: 33.7681 - val_MinusLogProbMetric: 33.7681 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 648/1000
2023-10-31 03:26:59.547 
Epoch 648/1000 
	 loss: 33.2857, MinusLogProbMetric: 33.2857, val_loss: 33.6761, val_MinusLogProbMetric: 33.6761

Epoch 648: val_loss improved from 33.70847 to 33.67606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 33.2857 - MinusLogProbMetric: 33.2857 - val_loss: 33.6761 - val_MinusLogProbMetric: 33.6761 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 649/1000
2023-10-31 03:28:03.100 
Epoch 649/1000 
	 loss: 33.2998, MinusLogProbMetric: 33.2998, val_loss: 33.8715, val_MinusLogProbMetric: 33.8715

Epoch 649: val_loss did not improve from 33.67606
196/196 - 63s - loss: 33.2998 - MinusLogProbMetric: 33.2998 - val_loss: 33.8715 - val_MinusLogProbMetric: 33.8715 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 650/1000
2023-10-31 03:29:07.646 
Epoch 650/1000 
	 loss: 33.2771, MinusLogProbMetric: 33.2771, val_loss: 33.8071, val_MinusLogProbMetric: 33.8071

Epoch 650: val_loss did not improve from 33.67606
196/196 - 65s - loss: 33.2771 - MinusLogProbMetric: 33.2771 - val_loss: 33.8071 - val_MinusLogProbMetric: 33.8071 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 651/1000
2023-10-31 03:30:09.632 
Epoch 651/1000 
	 loss: 33.2986, MinusLogProbMetric: 33.2986, val_loss: 33.7099, val_MinusLogProbMetric: 33.7099

Epoch 651: val_loss did not improve from 33.67606
196/196 - 62s - loss: 33.2986 - MinusLogProbMetric: 33.2986 - val_loss: 33.7099 - val_MinusLogProbMetric: 33.7099 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 652/1000
2023-10-31 03:31:09.815 
Epoch 652/1000 
	 loss: 33.2813, MinusLogProbMetric: 33.2813, val_loss: 33.7896, val_MinusLogProbMetric: 33.7896

Epoch 652: val_loss did not improve from 33.67606
196/196 - 60s - loss: 33.2813 - MinusLogProbMetric: 33.2813 - val_loss: 33.7896 - val_MinusLogProbMetric: 33.7896 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 653/1000
2023-10-31 03:32:07.971 
Epoch 653/1000 
	 loss: 33.3049, MinusLogProbMetric: 33.3049, val_loss: 33.7361, val_MinusLogProbMetric: 33.7361

Epoch 653: val_loss did not improve from 33.67606
196/196 - 58s - loss: 33.3049 - MinusLogProbMetric: 33.3049 - val_loss: 33.7361 - val_MinusLogProbMetric: 33.7361 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 654/1000
2023-10-31 03:33:05.529 
Epoch 654/1000 
	 loss: 33.2859, MinusLogProbMetric: 33.2859, val_loss: 33.6790, val_MinusLogProbMetric: 33.6790

Epoch 654: val_loss did not improve from 33.67606
196/196 - 58s - loss: 33.2859 - MinusLogProbMetric: 33.2859 - val_loss: 33.6790 - val_MinusLogProbMetric: 33.6790 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 655/1000
2023-10-31 03:34:04.869 
Epoch 655/1000 
	 loss: 33.2708, MinusLogProbMetric: 33.2708, val_loss: 33.7566, val_MinusLogProbMetric: 33.7566

Epoch 655: val_loss did not improve from 33.67606
196/196 - 59s - loss: 33.2708 - MinusLogProbMetric: 33.2708 - val_loss: 33.7566 - val_MinusLogProbMetric: 33.7566 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 656/1000
2023-10-31 03:35:07.849 
Epoch 656/1000 
	 loss: 33.2630, MinusLogProbMetric: 33.2630, val_loss: 33.9520, val_MinusLogProbMetric: 33.9520

Epoch 656: val_loss did not improve from 33.67606
196/196 - 63s - loss: 33.2630 - MinusLogProbMetric: 33.2630 - val_loss: 33.9520 - val_MinusLogProbMetric: 33.9520 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 657/1000
2023-10-31 03:36:08.383 
Epoch 657/1000 
	 loss: 33.2353, MinusLogProbMetric: 33.2353, val_loss: 33.6425, val_MinusLogProbMetric: 33.6425

Epoch 657: val_loss improved from 33.67606 to 33.64249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.2353 - MinusLogProbMetric: 33.2353 - val_loss: 33.6425 - val_MinusLogProbMetric: 33.6425 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 658/1000
2023-10-31 03:37:09.531 
Epoch 658/1000 
	 loss: 33.2390, MinusLogProbMetric: 33.2390, val_loss: 33.8193, val_MinusLogProbMetric: 33.8193

Epoch 658: val_loss did not improve from 33.64249
196/196 - 59s - loss: 33.2390 - MinusLogProbMetric: 33.2390 - val_loss: 33.8193 - val_MinusLogProbMetric: 33.8193 - lr: 4.1152e-06 - 59s/epoch - 302ms/step
Epoch 659/1000
2023-10-31 03:38:08.095 
Epoch 659/1000 
	 loss: 33.2223, MinusLogProbMetric: 33.2223, val_loss: 33.7712, val_MinusLogProbMetric: 33.7712

Epoch 659: val_loss did not improve from 33.64249
196/196 - 59s - loss: 33.2223 - MinusLogProbMetric: 33.2223 - val_loss: 33.7712 - val_MinusLogProbMetric: 33.7712 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 660/1000
2023-10-31 03:39:07.890 
Epoch 660/1000 
	 loss: 33.2505, MinusLogProbMetric: 33.2505, val_loss: 33.7266, val_MinusLogProbMetric: 33.7266

Epoch 660: val_loss did not improve from 33.64249
196/196 - 60s - loss: 33.2505 - MinusLogProbMetric: 33.2505 - val_loss: 33.7266 - val_MinusLogProbMetric: 33.7266 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 661/1000
2023-10-31 03:40:06.296 
Epoch 661/1000 
	 loss: 33.2597, MinusLogProbMetric: 33.2597, val_loss: 33.6447, val_MinusLogProbMetric: 33.6447

Epoch 661: val_loss did not improve from 33.64249
196/196 - 58s - loss: 33.2597 - MinusLogProbMetric: 33.2597 - val_loss: 33.6447 - val_MinusLogProbMetric: 33.6447 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 662/1000
2023-10-31 03:41:04.595 
Epoch 662/1000 
	 loss: 33.2551, MinusLogProbMetric: 33.2551, val_loss: 33.8785, val_MinusLogProbMetric: 33.8785

Epoch 662: val_loss did not improve from 33.64249
196/196 - 58s - loss: 33.2551 - MinusLogProbMetric: 33.2551 - val_loss: 33.8785 - val_MinusLogProbMetric: 33.8785 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 663/1000
2023-10-31 03:42:01.818 
Epoch 663/1000 
	 loss: 33.9806, MinusLogProbMetric: 33.9806, val_loss: 82.1468, val_MinusLogProbMetric: 82.1468

Epoch 663: val_loss did not improve from 33.64249
196/196 - 57s - loss: 33.9806 - MinusLogProbMetric: 33.9806 - val_loss: 82.1468 - val_MinusLogProbMetric: 82.1468 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 664/1000
2023-10-31 03:43:05.029 
Epoch 664/1000 
	 loss: 40.0385, MinusLogProbMetric: 40.0385, val_loss: 36.5411, val_MinusLogProbMetric: 36.5411

Epoch 664: val_loss did not improve from 33.64249
196/196 - 63s - loss: 40.0385 - MinusLogProbMetric: 40.0385 - val_loss: 36.5411 - val_MinusLogProbMetric: 36.5411 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 665/1000
2023-10-31 03:44:07.245 
Epoch 665/1000 
	 loss: 35.7600, MinusLogProbMetric: 35.7600, val_loss: 35.8046, val_MinusLogProbMetric: 35.8046

Epoch 665: val_loss did not improve from 33.64249
196/196 - 62s - loss: 35.7600 - MinusLogProbMetric: 35.7600 - val_loss: 35.8046 - val_MinusLogProbMetric: 35.8046 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 666/1000
2023-10-31 03:45:10.774 
Epoch 666/1000 
	 loss: 34.6115, MinusLogProbMetric: 34.6115, val_loss: 34.7901, val_MinusLogProbMetric: 34.7901

Epoch 666: val_loss did not improve from 33.64249
196/196 - 64s - loss: 34.6115 - MinusLogProbMetric: 34.6115 - val_loss: 34.7901 - val_MinusLogProbMetric: 34.7901 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 667/1000
2023-10-31 03:46:11.670 
Epoch 667/1000 
	 loss: 34.2260, MinusLogProbMetric: 34.2260, val_loss: 34.8080, val_MinusLogProbMetric: 34.8080

Epoch 667: val_loss did not improve from 33.64249
196/196 - 61s - loss: 34.2260 - MinusLogProbMetric: 34.2260 - val_loss: 34.8080 - val_MinusLogProbMetric: 34.8080 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 668/1000
2023-10-31 03:47:08.748 
Epoch 668/1000 
	 loss: 34.1075, MinusLogProbMetric: 34.1075, val_loss: 34.3731, val_MinusLogProbMetric: 34.3731

Epoch 668: val_loss did not improve from 33.64249
196/196 - 57s - loss: 34.1075 - MinusLogProbMetric: 34.1075 - val_loss: 34.3731 - val_MinusLogProbMetric: 34.3731 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 669/1000
2023-10-31 03:48:03.074 
Epoch 669/1000 
	 loss: 33.9780, MinusLogProbMetric: 33.9780, val_loss: 34.4180, val_MinusLogProbMetric: 34.4180

Epoch 669: val_loss did not improve from 33.64249
196/196 - 54s - loss: 33.9780 - MinusLogProbMetric: 33.9780 - val_loss: 34.4180 - val_MinusLogProbMetric: 34.4180 - lr: 4.1152e-06 - 54s/epoch - 277ms/step
Epoch 670/1000
2023-10-31 03:48:58.717 
Epoch 670/1000 
	 loss: 33.9658, MinusLogProbMetric: 33.9658, val_loss: 34.3311, val_MinusLogProbMetric: 34.3311

Epoch 670: val_loss did not improve from 33.64249
196/196 - 56s - loss: 33.9658 - MinusLogProbMetric: 33.9658 - val_loss: 34.3311 - val_MinusLogProbMetric: 34.3311 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 671/1000
2023-10-31 03:49:55.341 
Epoch 671/1000 
	 loss: 33.8159, MinusLogProbMetric: 33.8159, val_loss: 34.0830, val_MinusLogProbMetric: 34.0830

Epoch 671: val_loss did not improve from 33.64249
196/196 - 57s - loss: 33.8159 - MinusLogProbMetric: 33.8159 - val_loss: 34.0830 - val_MinusLogProbMetric: 34.0830 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 672/1000
2023-10-31 03:50:55.516 
Epoch 672/1000 
	 loss: 33.4485, MinusLogProbMetric: 33.4485, val_loss: 33.7432, val_MinusLogProbMetric: 33.7432

Epoch 672: val_loss did not improve from 33.64249
196/196 - 60s - loss: 33.4485 - MinusLogProbMetric: 33.4485 - val_loss: 33.7432 - val_MinusLogProbMetric: 33.7432 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 673/1000
2023-10-31 03:51:57.946 
Epoch 673/1000 
	 loss: 33.3628, MinusLogProbMetric: 33.3628, val_loss: 33.8209, val_MinusLogProbMetric: 33.8209

Epoch 673: val_loss did not improve from 33.64249
196/196 - 62s - loss: 33.3628 - MinusLogProbMetric: 33.3628 - val_loss: 33.8209 - val_MinusLogProbMetric: 33.8209 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 674/1000
2023-10-31 03:53:01.972 
Epoch 674/1000 
	 loss: 33.2972, MinusLogProbMetric: 33.2972, val_loss: 33.7213, val_MinusLogProbMetric: 33.7213

Epoch 674: val_loss did not improve from 33.64249
196/196 - 64s - loss: 33.2972 - MinusLogProbMetric: 33.2972 - val_loss: 33.7213 - val_MinusLogProbMetric: 33.7213 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 675/1000
2023-10-31 03:54:04.984 
Epoch 675/1000 
	 loss: 33.2945, MinusLogProbMetric: 33.2945, val_loss: 33.9802, val_MinusLogProbMetric: 33.9802

Epoch 675: val_loss did not improve from 33.64249
196/196 - 63s - loss: 33.2945 - MinusLogProbMetric: 33.2945 - val_loss: 33.9802 - val_MinusLogProbMetric: 33.9802 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 676/1000
2023-10-31 03:55:03.616 
Epoch 676/1000 
	 loss: 33.2994, MinusLogProbMetric: 33.2994, val_loss: 33.7950, val_MinusLogProbMetric: 33.7950

Epoch 676: val_loss did not improve from 33.64249
196/196 - 59s - loss: 33.2994 - MinusLogProbMetric: 33.2994 - val_loss: 33.7950 - val_MinusLogProbMetric: 33.7950 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 677/1000
2023-10-31 03:56:06.258 
Epoch 677/1000 
	 loss: 33.2597, MinusLogProbMetric: 33.2597, val_loss: 33.8061, val_MinusLogProbMetric: 33.8061

Epoch 677: val_loss did not improve from 33.64249
196/196 - 63s - loss: 33.2597 - MinusLogProbMetric: 33.2597 - val_loss: 33.8061 - val_MinusLogProbMetric: 33.8061 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 678/1000
2023-10-31 03:57:07.038 
Epoch 678/1000 
	 loss: 33.2420, MinusLogProbMetric: 33.2420, val_loss: 33.8160, val_MinusLogProbMetric: 33.8160

Epoch 678: val_loss did not improve from 33.64249
196/196 - 61s - loss: 33.2420 - MinusLogProbMetric: 33.2420 - val_loss: 33.8160 - val_MinusLogProbMetric: 33.8160 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 679/1000
2023-10-31 03:58:06.708 
Epoch 679/1000 
	 loss: 33.2296, MinusLogProbMetric: 33.2296, val_loss: 33.8094, val_MinusLogProbMetric: 33.8094

Epoch 679: val_loss did not improve from 33.64249
196/196 - 60s - loss: 33.2296 - MinusLogProbMetric: 33.2296 - val_loss: 33.8094 - val_MinusLogProbMetric: 33.8094 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 680/1000
2023-10-31 03:59:07.786 
Epoch 680/1000 
	 loss: 33.2541, MinusLogProbMetric: 33.2541, val_loss: 33.6324, val_MinusLogProbMetric: 33.6324

Epoch 680: val_loss improved from 33.64249 to 33.63243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.2541 - MinusLogProbMetric: 33.2541 - val_loss: 33.6324 - val_MinusLogProbMetric: 33.6324 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 681/1000
2023-10-31 04:00:09.392 
Epoch 681/1000 
	 loss: 33.1962, MinusLogProbMetric: 33.1962, val_loss: 33.8149, val_MinusLogProbMetric: 33.8149

Epoch 681: val_loss did not improve from 33.63243
196/196 - 61s - loss: 33.1962 - MinusLogProbMetric: 33.1962 - val_loss: 33.8149 - val_MinusLogProbMetric: 33.8149 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 682/1000
2023-10-31 04:01:11.930 
Epoch 682/1000 
	 loss: 33.2207, MinusLogProbMetric: 33.2207, val_loss: 33.6724, val_MinusLogProbMetric: 33.6724

Epoch 682: val_loss did not improve from 33.63243
196/196 - 63s - loss: 33.2207 - MinusLogProbMetric: 33.2207 - val_loss: 33.6724 - val_MinusLogProbMetric: 33.6724 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 683/1000
2023-10-31 04:02:14.373 
Epoch 683/1000 
	 loss: 33.2202, MinusLogProbMetric: 33.2202, val_loss: 33.7531, val_MinusLogProbMetric: 33.7531

Epoch 683: val_loss did not improve from 33.63243
196/196 - 62s - loss: 33.2202 - MinusLogProbMetric: 33.2202 - val_loss: 33.7531 - val_MinusLogProbMetric: 33.7531 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 684/1000
2023-10-31 04:03:17.133 
Epoch 684/1000 
	 loss: 33.2028, MinusLogProbMetric: 33.2028, val_loss: 33.5852, val_MinusLogProbMetric: 33.5852

Epoch 684: val_loss improved from 33.63243 to 33.58525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 33.2028 - MinusLogProbMetric: 33.2028 - val_loss: 33.5852 - val_MinusLogProbMetric: 33.5852 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 685/1000
2023-10-31 04:04:16.498 
Epoch 685/1000 
	 loss: 33.1934, MinusLogProbMetric: 33.1934, val_loss: 33.6106, val_MinusLogProbMetric: 33.6106

Epoch 685: val_loss did not improve from 33.58525
196/196 - 58s - loss: 33.1934 - MinusLogProbMetric: 33.1934 - val_loss: 33.6106 - val_MinusLogProbMetric: 33.6106 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 686/1000
2023-10-31 04:05:12.634 
Epoch 686/1000 
	 loss: 33.1656, MinusLogProbMetric: 33.1656, val_loss: 33.5949, val_MinusLogProbMetric: 33.5949

Epoch 686: val_loss did not improve from 33.58525
196/196 - 56s - loss: 33.1656 - MinusLogProbMetric: 33.1656 - val_loss: 33.5949 - val_MinusLogProbMetric: 33.5949 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 687/1000
2023-10-31 04:06:09.627 
Epoch 687/1000 
	 loss: 33.1995, MinusLogProbMetric: 33.1995, val_loss: 33.6514, val_MinusLogProbMetric: 33.6514

Epoch 687: val_loss did not improve from 33.58525
196/196 - 57s - loss: 33.1995 - MinusLogProbMetric: 33.1995 - val_loss: 33.6514 - val_MinusLogProbMetric: 33.6514 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 688/1000
2023-10-31 04:07:05.483 
Epoch 688/1000 
	 loss: 33.1591, MinusLogProbMetric: 33.1591, val_loss: 33.6188, val_MinusLogProbMetric: 33.6188

Epoch 688: val_loss did not improve from 33.58525
196/196 - 56s - loss: 33.1591 - MinusLogProbMetric: 33.1591 - val_loss: 33.6188 - val_MinusLogProbMetric: 33.6188 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 689/1000
2023-10-31 04:08:04.743 
Epoch 689/1000 
	 loss: 33.1493, MinusLogProbMetric: 33.1493, val_loss: 33.5792, val_MinusLogProbMetric: 33.5792

Epoch 689: val_loss improved from 33.58525 to 33.57923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 33.1493 - MinusLogProbMetric: 33.1493 - val_loss: 33.5792 - val_MinusLogProbMetric: 33.5792 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 690/1000
2023-10-31 04:09:02.838 
Epoch 690/1000 
	 loss: 33.1630, MinusLogProbMetric: 33.1630, val_loss: 33.6186, val_MinusLogProbMetric: 33.6186

Epoch 690: val_loss did not improve from 33.57923
196/196 - 57s - loss: 33.1630 - MinusLogProbMetric: 33.1630 - val_loss: 33.6186 - val_MinusLogProbMetric: 33.6186 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 691/1000
2023-10-31 04:10:00.375 
Epoch 691/1000 
	 loss: 33.1508, MinusLogProbMetric: 33.1508, val_loss: 33.5954, val_MinusLogProbMetric: 33.5954

Epoch 691: val_loss did not improve from 33.57923
196/196 - 58s - loss: 33.1508 - MinusLogProbMetric: 33.1508 - val_loss: 33.5954 - val_MinusLogProbMetric: 33.5954 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 692/1000
2023-10-31 04:10:58.816 
Epoch 692/1000 
	 loss: 33.1181, MinusLogProbMetric: 33.1181, val_loss: 33.6084, val_MinusLogProbMetric: 33.6084

Epoch 692: val_loss did not improve from 33.57923
196/196 - 58s - loss: 33.1181 - MinusLogProbMetric: 33.1181 - val_loss: 33.6084 - val_MinusLogProbMetric: 33.6084 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 693/1000
2023-10-31 04:11:55.830 
Epoch 693/1000 
	 loss: 33.1145, MinusLogProbMetric: 33.1145, val_loss: 33.5640, val_MinusLogProbMetric: 33.5640

Epoch 693: val_loss improved from 33.57923 to 33.56398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 33.1145 - MinusLogProbMetric: 33.1145 - val_loss: 33.5640 - val_MinusLogProbMetric: 33.5640 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 694/1000
2023-10-31 04:12:55.014 
Epoch 694/1000 
	 loss: 33.1270, MinusLogProbMetric: 33.1270, val_loss: 33.6814, val_MinusLogProbMetric: 33.6814

Epoch 694: val_loss did not improve from 33.56398
196/196 - 57s - loss: 33.1270 - MinusLogProbMetric: 33.1270 - val_loss: 33.6814 - val_MinusLogProbMetric: 33.6814 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 695/1000
2023-10-31 04:13:52.247 
Epoch 695/1000 
	 loss: 33.1220, MinusLogProbMetric: 33.1220, val_loss: 33.5181, val_MinusLogProbMetric: 33.5181

Epoch 695: val_loss improved from 33.56398 to 33.51809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 33.1220 - MinusLogProbMetric: 33.1220 - val_loss: 33.5181 - val_MinusLogProbMetric: 33.5181 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 696/1000
2023-10-31 04:14:50.968 
Epoch 696/1000 
	 loss: 33.1805, MinusLogProbMetric: 33.1805, val_loss: 33.5191, val_MinusLogProbMetric: 33.5191

Epoch 696: val_loss did not improve from 33.51809
196/196 - 58s - loss: 33.1805 - MinusLogProbMetric: 33.1805 - val_loss: 33.5191 - val_MinusLogProbMetric: 33.5191 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 697/1000
2023-10-31 04:15:52.141 
Epoch 697/1000 
	 loss: 33.1059, MinusLogProbMetric: 33.1059, val_loss: 33.6318, val_MinusLogProbMetric: 33.6318

Epoch 697: val_loss did not improve from 33.51809
196/196 - 61s - loss: 33.1059 - MinusLogProbMetric: 33.1059 - val_loss: 33.6318 - val_MinusLogProbMetric: 33.6318 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 698/1000
2023-10-31 04:16:52.023 
Epoch 698/1000 
	 loss: 33.1027, MinusLogProbMetric: 33.1027, val_loss: 33.5549, val_MinusLogProbMetric: 33.5549

Epoch 698: val_loss did not improve from 33.51809
196/196 - 60s - loss: 33.1027 - MinusLogProbMetric: 33.1027 - val_loss: 33.5549 - val_MinusLogProbMetric: 33.5549 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 699/1000
2023-10-31 04:17:53.247 
Epoch 699/1000 
	 loss: 33.0795, MinusLogProbMetric: 33.0795, val_loss: 33.4973, val_MinusLogProbMetric: 33.4973

Epoch 699: val_loss improved from 33.51809 to 33.49725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 33.0795 - MinusLogProbMetric: 33.0795 - val_loss: 33.4973 - val_MinusLogProbMetric: 33.4973 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 700/1000
2023-10-31 04:18:54.444 
Epoch 700/1000 
	 loss: 33.1056, MinusLogProbMetric: 33.1056, val_loss: 33.7819, val_MinusLogProbMetric: 33.7819

Epoch 700: val_loss did not improve from 33.49725
196/196 - 60s - loss: 33.1056 - MinusLogProbMetric: 33.1056 - val_loss: 33.7819 - val_MinusLogProbMetric: 33.7819 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 701/1000
2023-10-31 04:19:55.063 
Epoch 701/1000 
	 loss: 33.1078, MinusLogProbMetric: 33.1078, val_loss: 33.6953, val_MinusLogProbMetric: 33.6953

Epoch 701: val_loss did not improve from 33.49725
196/196 - 61s - loss: 33.1078 - MinusLogProbMetric: 33.1078 - val_loss: 33.6953 - val_MinusLogProbMetric: 33.6953 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 702/1000
2023-10-31 04:20:57.625 
Epoch 702/1000 
	 loss: 33.0810, MinusLogProbMetric: 33.0810, val_loss: 33.6332, val_MinusLogProbMetric: 33.6332

Epoch 702: val_loss did not improve from 33.49725
196/196 - 63s - loss: 33.0810 - MinusLogProbMetric: 33.0810 - val_loss: 33.6332 - val_MinusLogProbMetric: 33.6332 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 703/1000
2023-10-31 04:21:56.053 
Epoch 703/1000 
	 loss: 33.0890, MinusLogProbMetric: 33.0890, val_loss: 33.8209, val_MinusLogProbMetric: 33.8209

Epoch 703: val_loss did not improve from 33.49725
196/196 - 58s - loss: 33.0890 - MinusLogProbMetric: 33.0890 - val_loss: 33.8209 - val_MinusLogProbMetric: 33.8209 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 704/1000
2023-10-31 04:22:57.879 
Epoch 704/1000 
	 loss: 33.0764, MinusLogProbMetric: 33.0764, val_loss: 33.4773, val_MinusLogProbMetric: 33.4773

Epoch 704: val_loss improved from 33.49725 to 33.47725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 33.0764 - MinusLogProbMetric: 33.0764 - val_loss: 33.4773 - val_MinusLogProbMetric: 33.4773 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 705/1000
2023-10-31 04:24:02.014 
Epoch 705/1000 
	 loss: 33.0902, MinusLogProbMetric: 33.0902, val_loss: 33.5508, val_MinusLogProbMetric: 33.5508

Epoch 705: val_loss did not improve from 33.47725
196/196 - 63s - loss: 33.0902 - MinusLogProbMetric: 33.0902 - val_loss: 33.5508 - val_MinusLogProbMetric: 33.5508 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 706/1000
2023-10-31 04:25:02.311 
Epoch 706/1000 
	 loss: 33.0532, MinusLogProbMetric: 33.0532, val_loss: 33.6159, val_MinusLogProbMetric: 33.6159

Epoch 706: val_loss did not improve from 33.47725
196/196 - 60s - loss: 33.0532 - MinusLogProbMetric: 33.0532 - val_loss: 33.6159 - val_MinusLogProbMetric: 33.6159 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 707/1000
2023-10-31 04:26:05.480 
Epoch 707/1000 
	 loss: 33.0821, MinusLogProbMetric: 33.0821, val_loss: 33.5035, val_MinusLogProbMetric: 33.5035

Epoch 707: val_loss did not improve from 33.47725
196/196 - 63s - loss: 33.0821 - MinusLogProbMetric: 33.0821 - val_loss: 33.5035 - val_MinusLogProbMetric: 33.5035 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 708/1000
2023-10-31 04:27:08.830 
Epoch 708/1000 
	 loss: 33.0768, MinusLogProbMetric: 33.0768, val_loss: 33.5292, val_MinusLogProbMetric: 33.5292

Epoch 708: val_loss did not improve from 33.47725
196/196 - 63s - loss: 33.0768 - MinusLogProbMetric: 33.0768 - val_loss: 33.5292 - val_MinusLogProbMetric: 33.5292 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 709/1000
2023-10-31 04:28:07.643 
Epoch 709/1000 
	 loss: 33.0477, MinusLogProbMetric: 33.0477, val_loss: 33.4992, val_MinusLogProbMetric: 33.4992

Epoch 709: val_loss did not improve from 33.47725
196/196 - 59s - loss: 33.0477 - MinusLogProbMetric: 33.0477 - val_loss: 33.4992 - val_MinusLogProbMetric: 33.4992 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 710/1000
2023-10-31 04:29:09.744 
Epoch 710/1000 
	 loss: 33.0365, MinusLogProbMetric: 33.0365, val_loss: 33.4099, val_MinusLogProbMetric: 33.4099

Epoch 710: val_loss improved from 33.47725 to 33.40992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 33.0365 - MinusLogProbMetric: 33.0365 - val_loss: 33.4099 - val_MinusLogProbMetric: 33.4099 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 711/1000
2023-10-31 04:30:13.521 
Epoch 711/1000 
	 loss: 33.0290, MinusLogProbMetric: 33.0290, val_loss: 33.4222, val_MinusLogProbMetric: 33.4222

Epoch 711: val_loss did not improve from 33.40992
196/196 - 63s - loss: 33.0290 - MinusLogProbMetric: 33.0290 - val_loss: 33.4222 - val_MinusLogProbMetric: 33.4222 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 712/1000
2023-10-31 04:31:18.168 
Epoch 712/1000 
	 loss: 33.0171, MinusLogProbMetric: 33.0171, val_loss: 33.4158, val_MinusLogProbMetric: 33.4158

Epoch 712: val_loss did not improve from 33.40992
196/196 - 65s - loss: 33.0171 - MinusLogProbMetric: 33.0171 - val_loss: 33.4158 - val_MinusLogProbMetric: 33.4158 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 713/1000
2023-10-31 04:32:17.893 
Epoch 713/1000 
	 loss: 33.0209, MinusLogProbMetric: 33.0209, val_loss: 33.5574, val_MinusLogProbMetric: 33.5574

Epoch 713: val_loss did not improve from 33.40992
196/196 - 60s - loss: 33.0209 - MinusLogProbMetric: 33.0209 - val_loss: 33.5574 - val_MinusLogProbMetric: 33.5574 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 714/1000
2023-10-31 04:33:18.684 
Epoch 714/1000 
	 loss: 33.0089, MinusLogProbMetric: 33.0089, val_loss: 33.6521, val_MinusLogProbMetric: 33.6521

Epoch 714: val_loss did not improve from 33.40992
196/196 - 61s - loss: 33.0089 - MinusLogProbMetric: 33.0089 - val_loss: 33.6521 - val_MinusLogProbMetric: 33.6521 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 715/1000
2023-10-31 04:34:20.149 
Epoch 715/1000 
	 loss: 33.0247, MinusLogProbMetric: 33.0247, val_loss: 33.6595, val_MinusLogProbMetric: 33.6595

Epoch 715: val_loss did not improve from 33.40992
196/196 - 61s - loss: 33.0247 - MinusLogProbMetric: 33.0247 - val_loss: 33.6595 - val_MinusLogProbMetric: 33.6595 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 716/1000
2023-10-31 04:35:20.113 
Epoch 716/1000 
	 loss: 32.9732, MinusLogProbMetric: 32.9732, val_loss: 33.4371, val_MinusLogProbMetric: 33.4371

Epoch 716: val_loss did not improve from 33.40992
196/196 - 60s - loss: 32.9732 - MinusLogProbMetric: 32.9732 - val_loss: 33.4371 - val_MinusLogProbMetric: 33.4371 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 717/1000
2023-10-31 04:36:18.776 
Epoch 717/1000 
	 loss: 32.9634, MinusLogProbMetric: 32.9634, val_loss: 33.4709, val_MinusLogProbMetric: 33.4709

Epoch 717: val_loss did not improve from 33.40992
196/196 - 59s - loss: 32.9634 - MinusLogProbMetric: 32.9634 - val_loss: 33.4709 - val_MinusLogProbMetric: 33.4709 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 718/1000
2023-10-31 04:37:15.949 
Epoch 718/1000 
	 loss: 32.9963, MinusLogProbMetric: 32.9963, val_loss: 33.5459, val_MinusLogProbMetric: 33.5459

Epoch 718: val_loss did not improve from 33.40992
196/196 - 57s - loss: 32.9963 - MinusLogProbMetric: 32.9963 - val_loss: 33.5459 - val_MinusLogProbMetric: 33.5459 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 719/1000
2023-10-31 04:38:11.451 
Epoch 719/1000 
	 loss: 33.0042, MinusLogProbMetric: 33.0042, val_loss: 33.3707, val_MinusLogProbMetric: 33.3707

Epoch 719: val_loss improved from 33.40992 to 33.37069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 33.0042 - MinusLogProbMetric: 33.0042 - val_loss: 33.3707 - val_MinusLogProbMetric: 33.3707 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 720/1000
2023-10-31 04:39:08.119 
Epoch 720/1000 
	 loss: 32.9993, MinusLogProbMetric: 32.9993, val_loss: 33.5676, val_MinusLogProbMetric: 33.5676

Epoch 720: val_loss did not improve from 33.37069
196/196 - 56s - loss: 32.9993 - MinusLogProbMetric: 32.9993 - val_loss: 33.5676 - val_MinusLogProbMetric: 33.5676 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 721/1000
2023-10-31 04:40:04.135 
Epoch 721/1000 
	 loss: 32.9797, MinusLogProbMetric: 32.9797, val_loss: 33.5137, val_MinusLogProbMetric: 33.5137

Epoch 721: val_loss did not improve from 33.37069
196/196 - 56s - loss: 32.9797 - MinusLogProbMetric: 32.9797 - val_loss: 33.5137 - val_MinusLogProbMetric: 33.5137 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 722/1000
2023-10-31 04:41:03.123 
Epoch 722/1000 
	 loss: 32.9645, MinusLogProbMetric: 32.9645, val_loss: 33.7276, val_MinusLogProbMetric: 33.7276

Epoch 722: val_loss did not improve from 33.37069
196/196 - 59s - loss: 32.9645 - MinusLogProbMetric: 32.9645 - val_loss: 33.7276 - val_MinusLogProbMetric: 33.7276 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 723/1000
2023-10-31 04:42:03.924 
Epoch 723/1000 
	 loss: 32.9882, MinusLogProbMetric: 32.9882, val_loss: 33.5685, val_MinusLogProbMetric: 33.5685

Epoch 723: val_loss did not improve from 33.37069
196/196 - 61s - loss: 32.9882 - MinusLogProbMetric: 32.9882 - val_loss: 33.5685 - val_MinusLogProbMetric: 33.5685 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 724/1000
2023-10-31 04:43:08.894 
Epoch 724/1000 
	 loss: 32.9811, MinusLogProbMetric: 32.9811, val_loss: 33.4769, val_MinusLogProbMetric: 33.4769

Epoch 724: val_loss did not improve from 33.37069
196/196 - 65s - loss: 32.9811 - MinusLogProbMetric: 32.9811 - val_loss: 33.4769 - val_MinusLogProbMetric: 33.4769 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 725/1000
2023-10-31 04:44:08.788 
Epoch 725/1000 
	 loss: 32.9777, MinusLogProbMetric: 32.9777, val_loss: 33.5199, val_MinusLogProbMetric: 33.5199

Epoch 725: val_loss did not improve from 33.37069
196/196 - 60s - loss: 32.9777 - MinusLogProbMetric: 32.9777 - val_loss: 33.5199 - val_MinusLogProbMetric: 33.5199 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 726/1000
2023-10-31 04:45:08.950 
Epoch 726/1000 
	 loss: 32.9479, MinusLogProbMetric: 32.9479, val_loss: 33.5420, val_MinusLogProbMetric: 33.5420

Epoch 726: val_loss did not improve from 33.37069
196/196 - 60s - loss: 32.9479 - MinusLogProbMetric: 32.9479 - val_loss: 33.5420 - val_MinusLogProbMetric: 33.5420 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 727/1000
2023-10-31 04:46:09.665 
Epoch 727/1000 
	 loss: 32.9447, MinusLogProbMetric: 32.9447, val_loss: 33.5265, val_MinusLogProbMetric: 33.5265

Epoch 727: val_loss did not improve from 33.37069
196/196 - 61s - loss: 32.9447 - MinusLogProbMetric: 32.9447 - val_loss: 33.5265 - val_MinusLogProbMetric: 33.5265 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 728/1000
2023-10-31 04:47:06.979 
Epoch 728/1000 
	 loss: 32.9505, MinusLogProbMetric: 32.9505, val_loss: 33.3970, val_MinusLogProbMetric: 33.3970

Epoch 728: val_loss did not improve from 33.37069
196/196 - 57s - loss: 32.9505 - MinusLogProbMetric: 32.9505 - val_loss: 33.3970 - val_MinusLogProbMetric: 33.3970 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 729/1000
2023-10-31 04:48:06.026 
Epoch 729/1000 
	 loss: 32.9269, MinusLogProbMetric: 32.9269, val_loss: 33.4228, val_MinusLogProbMetric: 33.4228

Epoch 729: val_loss did not improve from 33.37069
196/196 - 59s - loss: 32.9269 - MinusLogProbMetric: 32.9269 - val_loss: 33.4228 - val_MinusLogProbMetric: 33.4228 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 730/1000
2023-10-31 04:49:08.265 
Epoch 730/1000 
	 loss: 32.9435, MinusLogProbMetric: 32.9435, val_loss: 33.3256, val_MinusLogProbMetric: 33.3256

Epoch 730: val_loss improved from 33.37069 to 33.32563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 32.9435 - MinusLogProbMetric: 32.9435 - val_loss: 33.3256 - val_MinusLogProbMetric: 33.3256 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 731/1000
2023-10-31 04:50:05.487 
Epoch 731/1000 
	 loss: 32.9509, MinusLogProbMetric: 32.9509, val_loss: 33.3390, val_MinusLogProbMetric: 33.3390

Epoch 731: val_loss did not improve from 33.32563
196/196 - 56s - loss: 32.9509 - MinusLogProbMetric: 32.9509 - val_loss: 33.3390 - val_MinusLogProbMetric: 33.3390 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 732/1000
2023-10-31 04:51:03.282 
Epoch 732/1000 
	 loss: 32.9297, MinusLogProbMetric: 32.9297, val_loss: 33.4520, val_MinusLogProbMetric: 33.4520

Epoch 732: val_loss did not improve from 33.32563
196/196 - 58s - loss: 32.9297 - MinusLogProbMetric: 32.9297 - val_loss: 33.4520 - val_MinusLogProbMetric: 33.4520 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 733/1000
2023-10-31 04:52:07.122 
Epoch 733/1000 
	 loss: 32.9212, MinusLogProbMetric: 32.9212, val_loss: 33.3166, val_MinusLogProbMetric: 33.3166

Epoch 733: val_loss improved from 33.32563 to 33.31663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 32.9212 - MinusLogProbMetric: 32.9212 - val_loss: 33.3166 - val_MinusLogProbMetric: 33.3166 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 734/1000
2023-10-31 04:53:09.135 
Epoch 734/1000 
	 loss: 32.9224, MinusLogProbMetric: 32.9224, val_loss: 33.4685, val_MinusLogProbMetric: 33.4685

Epoch 734: val_loss did not improve from 33.31663
196/196 - 61s - loss: 32.9224 - MinusLogProbMetric: 32.9224 - val_loss: 33.4685 - val_MinusLogProbMetric: 33.4685 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 735/1000
2023-10-31 04:54:12.414 
Epoch 735/1000 
	 loss: 32.9406, MinusLogProbMetric: 32.9406, val_loss: 33.7030, val_MinusLogProbMetric: 33.7030

Epoch 735: val_loss did not improve from 33.31663
196/196 - 63s - loss: 32.9406 - MinusLogProbMetric: 32.9406 - val_loss: 33.7030 - val_MinusLogProbMetric: 33.7030 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 736/1000
2023-10-31 04:55:15.252 
Epoch 736/1000 
	 loss: 32.9461, MinusLogProbMetric: 32.9461, val_loss: 33.3651, val_MinusLogProbMetric: 33.3651

Epoch 736: val_loss did not improve from 33.31663
196/196 - 63s - loss: 32.9461 - MinusLogProbMetric: 32.9461 - val_loss: 33.3651 - val_MinusLogProbMetric: 33.3651 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 737/1000
2023-10-31 04:56:13.908 
Epoch 737/1000 
	 loss: 32.9285, MinusLogProbMetric: 32.9285, val_loss: 33.4030, val_MinusLogProbMetric: 33.4030

Epoch 737: val_loss did not improve from 33.31663
196/196 - 59s - loss: 32.9285 - MinusLogProbMetric: 32.9285 - val_loss: 33.4030 - val_MinusLogProbMetric: 33.4030 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 738/1000
2023-10-31 04:57:13.597 
Epoch 738/1000 
	 loss: 32.9027, MinusLogProbMetric: 32.9027, val_loss: 33.5712, val_MinusLogProbMetric: 33.5712

Epoch 738: val_loss did not improve from 33.31663
196/196 - 60s - loss: 32.9027 - MinusLogProbMetric: 32.9027 - val_loss: 33.5712 - val_MinusLogProbMetric: 33.5712 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 739/1000
2023-10-31 04:58:17.196 
Epoch 739/1000 
	 loss: 32.8871, MinusLogProbMetric: 32.8871, val_loss: 33.4303, val_MinusLogProbMetric: 33.4303

Epoch 739: val_loss did not improve from 33.31663
196/196 - 64s - loss: 32.8871 - MinusLogProbMetric: 32.8871 - val_loss: 33.4303 - val_MinusLogProbMetric: 33.4303 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 740/1000
2023-10-31 04:59:18.703 
Epoch 740/1000 
	 loss: 32.9024, MinusLogProbMetric: 32.9024, val_loss: 33.4683, val_MinusLogProbMetric: 33.4683

Epoch 740: val_loss did not improve from 33.31663
196/196 - 62s - loss: 32.9024 - MinusLogProbMetric: 32.9024 - val_loss: 33.4683 - val_MinusLogProbMetric: 33.4683 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 741/1000
2023-10-31 05:00:21.029 
Epoch 741/1000 
	 loss: 32.8713, MinusLogProbMetric: 32.8713, val_loss: 33.5037, val_MinusLogProbMetric: 33.5037

Epoch 741: val_loss did not improve from 33.31663
196/196 - 62s - loss: 32.8713 - MinusLogProbMetric: 32.8713 - val_loss: 33.5037 - val_MinusLogProbMetric: 33.5037 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 742/1000
2023-10-31 05:01:23.630 
Epoch 742/1000 
	 loss: 32.8793, MinusLogProbMetric: 32.8793, val_loss: 33.3007, val_MinusLogProbMetric: 33.3007

Epoch 742: val_loss improved from 33.31663 to 33.30066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 32.8793 - MinusLogProbMetric: 32.8793 - val_loss: 33.3007 - val_MinusLogProbMetric: 33.3007 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 743/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 129: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 05:02:08.477 
Epoch 743/1000 
	 loss: nan, MinusLogProbMetric: 32.8375, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 743: val_loss did not improve from 33.30066
196/196 - 44s - loss: nan - MinusLogProbMetric: 32.8375 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 44s/epoch - 224ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 382.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_384"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_385 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f465c2ff4f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f48e05294e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f48e05294e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4858500940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f47846c2920>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46e366fb20>, <keras.callbacks.ModelCheckpoint object at 0x7f46e366ea10>, <keras.callbacks.EarlyStopping object at 0x7f46e366e5c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46e366c7f0>, <keras.callbacks.TerminateOnNaN object at 0x7f46e366f130>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-31 05:02:16.445963
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-31 05:05:04.226 
Epoch 1/1000 
	 loss: 32.9676, MinusLogProbMetric: 32.9676, val_loss: 33.3950, val_MinusLogProbMetric: 33.3950

Epoch 1: val_loss improved from inf to 33.39503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 169s - loss: 32.9676 - MinusLogProbMetric: 32.9676 - val_loss: 33.3950 - val_MinusLogProbMetric: 33.3950 - lr: 1.3717e-06 - 169s/epoch - 860ms/step
Epoch 2/1000
2023-10-31 05:06:00.419 
Epoch 2/1000 
	 loss: 32.8848, MinusLogProbMetric: 32.8848, val_loss: 33.2788, val_MinusLogProbMetric: 33.2788

Epoch 2: val_loss improved from 33.39503 to 33.27880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.8848 - MinusLogProbMetric: 32.8848 - val_loss: 33.2788 - val_MinusLogProbMetric: 33.2788 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 3/1000
2023-10-31 05:07:03.169 
Epoch 3/1000 
	 loss: 32.8441, MinusLogProbMetric: 32.8441, val_loss: 33.5164, val_MinusLogProbMetric: 33.5164

Epoch 3: val_loss did not improve from 33.27880
196/196 - 61s - loss: 32.8441 - MinusLogProbMetric: 32.8441 - val_loss: 33.5164 - val_MinusLogProbMetric: 33.5164 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 4/1000
2023-10-31 05:08:01.864 
Epoch 4/1000 
	 loss: 32.8620, MinusLogProbMetric: 32.8620, val_loss: 33.4511, val_MinusLogProbMetric: 33.4511

Epoch 4: val_loss did not improve from 33.27880
196/196 - 59s - loss: 32.8620 - MinusLogProbMetric: 32.8620 - val_loss: 33.4511 - val_MinusLogProbMetric: 33.4511 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 5/1000
2023-10-31 05:08:59.145 
Epoch 5/1000 
	 loss: 32.8969, MinusLogProbMetric: 32.8969, val_loss: 33.3438, val_MinusLogProbMetric: 33.3438

Epoch 5: val_loss did not improve from 33.27880
196/196 - 57s - loss: 32.8969 - MinusLogProbMetric: 32.8969 - val_loss: 33.3438 - val_MinusLogProbMetric: 33.3438 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 6/1000
2023-10-31 05:09:54.822 
Epoch 6/1000 
	 loss: 32.8430, MinusLogProbMetric: 32.8430, val_loss: 33.3888, val_MinusLogProbMetric: 33.3888

Epoch 6: val_loss did not improve from 33.27880
196/196 - 56s - loss: 32.8430 - MinusLogProbMetric: 32.8430 - val_loss: 33.3888 - val_MinusLogProbMetric: 33.3888 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 7/1000
2023-10-31 05:10:51.754 
Epoch 7/1000 
	 loss: 32.8725, MinusLogProbMetric: 32.8725, val_loss: 33.2655, val_MinusLogProbMetric: 33.2655

Epoch 7: val_loss improved from 33.27880 to 33.26553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 32.8725 - MinusLogProbMetric: 32.8725 - val_loss: 33.2655 - val_MinusLogProbMetric: 33.2655 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 8/1000
2023-10-31 05:11:48.844 
Epoch 8/1000 
	 loss: 32.8622, MinusLogProbMetric: 32.8622, val_loss: 33.3441, val_MinusLogProbMetric: 33.3441

Epoch 8: val_loss did not improve from 33.26553
196/196 - 56s - loss: 32.8622 - MinusLogProbMetric: 32.8622 - val_loss: 33.3441 - val_MinusLogProbMetric: 33.3441 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 9/1000
2023-10-31 05:12:46.095 
Epoch 9/1000 
	 loss: 32.8227, MinusLogProbMetric: 32.8227, val_loss: 33.3965, val_MinusLogProbMetric: 33.3965

Epoch 9: val_loss did not improve from 33.26553
196/196 - 57s - loss: 32.8227 - MinusLogProbMetric: 32.8227 - val_loss: 33.3965 - val_MinusLogProbMetric: 33.3965 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 10/1000
2023-10-31 05:13:43.163 
Epoch 10/1000 
	 loss: 32.8266, MinusLogProbMetric: 32.8266, val_loss: 33.2415, val_MinusLogProbMetric: 33.2415

Epoch 10: val_loss improved from 33.26553 to 33.24150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 32.8266 - MinusLogProbMetric: 32.8266 - val_loss: 33.2415 - val_MinusLogProbMetric: 33.2415 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 11/1000
2023-10-31 05:14:41.227 
Epoch 11/1000 
	 loss: 32.8219, MinusLogProbMetric: 32.8219, val_loss: 33.3383, val_MinusLogProbMetric: 33.3383

Epoch 11: val_loss did not improve from 33.24150
196/196 - 57s - loss: 32.8219 - MinusLogProbMetric: 32.8219 - val_loss: 33.3383 - val_MinusLogProbMetric: 33.3383 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 12/1000
2023-10-31 05:15:37.536 
Epoch 12/1000 
	 loss: 32.7766, MinusLogProbMetric: 32.7766, val_loss: 33.3120, val_MinusLogProbMetric: 33.3120

Epoch 12: val_loss did not improve from 33.24150
196/196 - 56s - loss: 32.7766 - MinusLogProbMetric: 32.7766 - val_loss: 33.3120 - val_MinusLogProbMetric: 33.3120 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 13/1000
2023-10-31 05:16:37.725 
Epoch 13/1000 
	 loss: 32.8395, MinusLogProbMetric: 32.8395, val_loss: 33.1935, val_MinusLogProbMetric: 33.1935

Epoch 13: val_loss improved from 33.24150 to 33.19351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 32.8395 - MinusLogProbMetric: 32.8395 - val_loss: 33.1935 - val_MinusLogProbMetric: 33.1935 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 14/1000
2023-10-31 05:17:35.530 
Epoch 14/1000 
	 loss: 32.7937, MinusLogProbMetric: 32.7937, val_loss: 33.3511, val_MinusLogProbMetric: 33.3511

Epoch 14: val_loss did not improve from 33.19351
196/196 - 57s - loss: 32.7937 - MinusLogProbMetric: 32.7937 - val_loss: 33.3511 - val_MinusLogProbMetric: 33.3511 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 15/1000
2023-10-31 05:18:31.061 
Epoch 15/1000 
	 loss: 41.2113, MinusLogProbMetric: 41.2113, val_loss: 38.1461, val_MinusLogProbMetric: 38.1461

Epoch 15: val_loss did not improve from 33.19351
196/196 - 56s - loss: 41.2113 - MinusLogProbMetric: 41.2113 - val_loss: 38.1461 - val_MinusLogProbMetric: 38.1461 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 16/1000
2023-10-31 05:19:27.178 
Epoch 16/1000 
	 loss: 36.1230, MinusLogProbMetric: 36.1230, val_loss: 35.8407, val_MinusLogProbMetric: 35.8407

Epoch 16: val_loss did not improve from 33.19351
196/196 - 56s - loss: 36.1230 - MinusLogProbMetric: 36.1230 - val_loss: 35.8407 - val_MinusLogProbMetric: 35.8407 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 17/1000
2023-10-31 05:20:24.426 
Epoch 17/1000 
	 loss: 35.1703, MinusLogProbMetric: 35.1703, val_loss: 35.3721, val_MinusLogProbMetric: 35.3721

Epoch 17: val_loss did not improve from 33.19351
196/196 - 57s - loss: 35.1703 - MinusLogProbMetric: 35.1703 - val_loss: 35.3721 - val_MinusLogProbMetric: 35.3721 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 18/1000
2023-10-31 05:21:21.654 
Epoch 18/1000 
	 loss: 34.6111, MinusLogProbMetric: 34.6111, val_loss: 34.6506, val_MinusLogProbMetric: 34.6506

Epoch 18: val_loss did not improve from 33.19351
196/196 - 57s - loss: 34.6111 - MinusLogProbMetric: 34.6111 - val_loss: 34.6506 - val_MinusLogProbMetric: 34.6506 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 19/1000
2023-10-31 05:22:18.781 
Epoch 19/1000 
	 loss: 33.9757, MinusLogProbMetric: 33.9757, val_loss: 34.1720, val_MinusLogProbMetric: 34.1720

Epoch 19: val_loss did not improve from 33.19351
196/196 - 57s - loss: 33.9757 - MinusLogProbMetric: 33.9757 - val_loss: 34.1720 - val_MinusLogProbMetric: 34.1720 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 20/1000
2023-10-31 05:23:14.146 
Epoch 20/1000 
	 loss: 33.6170, MinusLogProbMetric: 33.6170, val_loss: 33.9495, val_MinusLogProbMetric: 33.9495

Epoch 20: val_loss did not improve from 33.19351
196/196 - 55s - loss: 33.6170 - MinusLogProbMetric: 33.6170 - val_loss: 33.9495 - val_MinusLogProbMetric: 33.9495 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 21/1000
2023-10-31 05:24:09.240 
Epoch 21/1000 
	 loss: 33.4889, MinusLogProbMetric: 33.4889, val_loss: 33.9810, val_MinusLogProbMetric: 33.9810

Epoch 21: val_loss did not improve from 33.19351
196/196 - 55s - loss: 33.4889 - MinusLogProbMetric: 33.4889 - val_loss: 33.9810 - val_MinusLogProbMetric: 33.9810 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 22/1000
2023-10-31 05:25:05.161 
Epoch 22/1000 
	 loss: 33.4495, MinusLogProbMetric: 33.4495, val_loss: 33.8662, val_MinusLogProbMetric: 33.8662

Epoch 22: val_loss did not improve from 33.19351
196/196 - 56s - loss: 33.4495 - MinusLogProbMetric: 33.4495 - val_loss: 33.8662 - val_MinusLogProbMetric: 33.8662 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 23/1000
2023-10-31 05:26:01.303 
Epoch 23/1000 
	 loss: 33.4151, MinusLogProbMetric: 33.4151, val_loss: 33.8448, val_MinusLogProbMetric: 33.8448

Epoch 23: val_loss did not improve from 33.19351
196/196 - 56s - loss: 33.4151 - MinusLogProbMetric: 33.4151 - val_loss: 33.8448 - val_MinusLogProbMetric: 33.8448 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 24/1000
2023-10-31 05:27:01.424 
Epoch 24/1000 
	 loss: 33.3463, MinusLogProbMetric: 33.3463, val_loss: 33.8202, val_MinusLogProbMetric: 33.8202

Epoch 24: val_loss did not improve from 33.19351
196/196 - 60s - loss: 33.3463 - MinusLogProbMetric: 33.3463 - val_loss: 33.8202 - val_MinusLogProbMetric: 33.8202 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 25/1000
2023-10-31 05:28:01.991 
Epoch 25/1000 
	 loss: 33.3444, MinusLogProbMetric: 33.3444, val_loss: 33.8277, val_MinusLogProbMetric: 33.8277

Epoch 25: val_loss did not improve from 33.19351
196/196 - 61s - loss: 33.3444 - MinusLogProbMetric: 33.3444 - val_loss: 33.8277 - val_MinusLogProbMetric: 33.8277 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 26/1000
2023-10-31 05:29:00.688 
Epoch 26/1000 
	 loss: 33.3100, MinusLogProbMetric: 33.3100, val_loss: 33.7366, val_MinusLogProbMetric: 33.7366

Epoch 26: val_loss did not improve from 33.19351
196/196 - 59s - loss: 33.3100 - MinusLogProbMetric: 33.3100 - val_loss: 33.7366 - val_MinusLogProbMetric: 33.7366 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 27/1000
2023-10-31 05:29:57.943 
Epoch 27/1000 
	 loss: 33.2889, MinusLogProbMetric: 33.2889, val_loss: 33.7105, val_MinusLogProbMetric: 33.7105

Epoch 27: val_loss did not improve from 33.19351
196/196 - 57s - loss: 33.2889 - MinusLogProbMetric: 33.2889 - val_loss: 33.7105 - val_MinusLogProbMetric: 33.7105 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 28/1000
2023-10-31 05:30:54.183 
Epoch 28/1000 
	 loss: 33.2883, MinusLogProbMetric: 33.2883, val_loss: 33.7207, val_MinusLogProbMetric: 33.7207

Epoch 28: val_loss did not improve from 33.19351
196/196 - 56s - loss: 33.2883 - MinusLogProbMetric: 33.2883 - val_loss: 33.7207 - val_MinusLogProbMetric: 33.7207 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 29/1000
2023-10-31 05:31:53.697 
Epoch 29/1000 
	 loss: 33.2552, MinusLogProbMetric: 33.2552, val_loss: 33.6590, val_MinusLogProbMetric: 33.6590

Epoch 29: val_loss did not improve from 33.19351
196/196 - 60s - loss: 33.2552 - MinusLogProbMetric: 33.2552 - val_loss: 33.6590 - val_MinusLogProbMetric: 33.6590 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 30/1000
2023-10-31 05:32:53.750 
Epoch 30/1000 
	 loss: 33.4408, MinusLogProbMetric: 33.4408, val_loss: 34.5818, val_MinusLogProbMetric: 34.5818

Epoch 30: val_loss did not improve from 33.19351
196/196 - 60s - loss: 33.4408 - MinusLogProbMetric: 33.4408 - val_loss: 34.5818 - val_MinusLogProbMetric: 34.5818 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 31/1000
2023-10-31 05:33:49.877 
Epoch 31/1000 
	 loss: 33.2621, MinusLogProbMetric: 33.2621, val_loss: 33.7273, val_MinusLogProbMetric: 33.7273

Epoch 31: val_loss did not improve from 33.19351
196/196 - 56s - loss: 33.2621 - MinusLogProbMetric: 33.2621 - val_loss: 33.7273 - val_MinusLogProbMetric: 33.7273 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 32/1000
2023-10-31 05:34:45.342 
Epoch 32/1000 
	 loss: 33.1908, MinusLogProbMetric: 33.1908, val_loss: 33.5966, val_MinusLogProbMetric: 33.5966

Epoch 32: val_loss did not improve from 33.19351
196/196 - 55s - loss: 33.1908 - MinusLogProbMetric: 33.1908 - val_loss: 33.5966 - val_MinusLogProbMetric: 33.5966 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 33/1000
2023-10-31 05:35:43.153 
Epoch 33/1000 
	 loss: 33.1719, MinusLogProbMetric: 33.1719, val_loss: 33.6899, val_MinusLogProbMetric: 33.6899

Epoch 33: val_loss did not improve from 33.19351
196/196 - 58s - loss: 33.1719 - MinusLogProbMetric: 33.1719 - val_loss: 33.6899 - val_MinusLogProbMetric: 33.6899 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 34/1000
2023-10-31 05:36:40.548 
Epoch 34/1000 
	 loss: 33.1940, MinusLogProbMetric: 33.1940, val_loss: 33.6227, val_MinusLogProbMetric: 33.6227

Epoch 34: val_loss did not improve from 33.19351
196/196 - 57s - loss: 33.1940 - MinusLogProbMetric: 33.1940 - val_loss: 33.6227 - val_MinusLogProbMetric: 33.6227 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 35/1000
2023-10-31 05:37:36.034 
Epoch 35/1000 
	 loss: 33.1502, MinusLogProbMetric: 33.1502, val_loss: 33.6088, val_MinusLogProbMetric: 33.6088

Epoch 35: val_loss did not improve from 33.19351
196/196 - 55s - loss: 33.1502 - MinusLogProbMetric: 33.1502 - val_loss: 33.6088 - val_MinusLogProbMetric: 33.6088 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 36/1000
2023-10-31 05:38:32.801 
Epoch 36/1000 
	 loss: 33.1253, MinusLogProbMetric: 33.1253, val_loss: 33.5626, val_MinusLogProbMetric: 33.5626

Epoch 36: val_loss did not improve from 33.19351
196/196 - 57s - loss: 33.1253 - MinusLogProbMetric: 33.1253 - val_loss: 33.5626 - val_MinusLogProbMetric: 33.5626 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 37/1000
2023-10-31 05:39:28.458 
Epoch 37/1000 
	 loss: 33.1110, MinusLogProbMetric: 33.1110, val_loss: 33.5073, val_MinusLogProbMetric: 33.5073

Epoch 37: val_loss did not improve from 33.19351
196/196 - 56s - loss: 33.1110 - MinusLogProbMetric: 33.1110 - val_loss: 33.5073 - val_MinusLogProbMetric: 33.5073 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 38/1000
2023-10-31 05:40:26.368 
Epoch 38/1000 
	 loss: 33.0880, MinusLogProbMetric: 33.0880, val_loss: 33.5725, val_MinusLogProbMetric: 33.5725

Epoch 38: val_loss did not improve from 33.19351
196/196 - 58s - loss: 33.0880 - MinusLogProbMetric: 33.0880 - val_loss: 33.5725 - val_MinusLogProbMetric: 33.5725 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 39/1000
2023-10-31 05:41:28.298 
Epoch 39/1000 
	 loss: 33.0651, MinusLogProbMetric: 33.0651, val_loss: 33.5577, val_MinusLogProbMetric: 33.5577

Epoch 39: val_loss did not improve from 33.19351
196/196 - 62s - loss: 33.0651 - MinusLogProbMetric: 33.0651 - val_loss: 33.5577 - val_MinusLogProbMetric: 33.5577 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 40/1000
2023-10-31 05:42:26.190 
Epoch 40/1000 
	 loss: 33.0629, MinusLogProbMetric: 33.0629, val_loss: 33.5241, val_MinusLogProbMetric: 33.5241

Epoch 40: val_loss did not improve from 33.19351
196/196 - 58s - loss: 33.0629 - MinusLogProbMetric: 33.0629 - val_loss: 33.5241 - val_MinusLogProbMetric: 33.5241 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 41/1000
2023-10-31 05:43:22.302 
Epoch 41/1000 
	 loss: 33.0297, MinusLogProbMetric: 33.0297, val_loss: 33.4620, val_MinusLogProbMetric: 33.4620

Epoch 41: val_loss did not improve from 33.19351
196/196 - 56s - loss: 33.0297 - MinusLogProbMetric: 33.0297 - val_loss: 33.4620 - val_MinusLogProbMetric: 33.4620 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 42/1000
2023-10-31 05:44:17.084 
Epoch 42/1000 
	 loss: 33.0289, MinusLogProbMetric: 33.0289, val_loss: 33.6046, val_MinusLogProbMetric: 33.6046

Epoch 42: val_loss did not improve from 33.19351
196/196 - 55s - loss: 33.0289 - MinusLogProbMetric: 33.0289 - val_loss: 33.6046 - val_MinusLogProbMetric: 33.6046 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 43/1000
2023-10-31 05:45:12.655 
Epoch 43/1000 
	 loss: 32.9780, MinusLogProbMetric: 32.9780, val_loss: 33.4024, val_MinusLogProbMetric: 33.4024

Epoch 43: val_loss did not improve from 33.19351
196/196 - 56s - loss: 32.9780 - MinusLogProbMetric: 32.9780 - val_loss: 33.4024 - val_MinusLogProbMetric: 33.4024 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 44/1000
2023-10-31 05:46:10.283 
Epoch 44/1000 
	 loss: 32.8124, MinusLogProbMetric: 32.8124, val_loss: 33.2262, val_MinusLogProbMetric: 33.2262

Epoch 44: val_loss did not improve from 33.19351
196/196 - 58s - loss: 32.8124 - MinusLogProbMetric: 32.8124 - val_loss: 33.2262 - val_MinusLogProbMetric: 33.2262 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 45/1000
2023-10-31 05:47:08.277 
Epoch 45/1000 
	 loss: 32.6997, MinusLogProbMetric: 32.6997, val_loss: 33.1721, val_MinusLogProbMetric: 33.1721

Epoch 45: val_loss improved from 33.19351 to 33.17211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 32.6997 - MinusLogProbMetric: 32.6997 - val_loss: 33.1721 - val_MinusLogProbMetric: 33.1721 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 46/1000
2023-10-31 05:48:05.619 
Epoch 46/1000 
	 loss: 32.6612, MinusLogProbMetric: 32.6612, val_loss: 33.1064, val_MinusLogProbMetric: 33.1064

Epoch 46: val_loss improved from 33.17211 to 33.10639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.6612 - MinusLogProbMetric: 32.6612 - val_loss: 33.1064 - val_MinusLogProbMetric: 33.1064 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 47/1000
2023-10-31 05:49:02.846 
Epoch 47/1000 
	 loss: 32.6419, MinusLogProbMetric: 32.6419, val_loss: 33.1040, val_MinusLogProbMetric: 33.1040

Epoch 47: val_loss improved from 33.10639 to 33.10397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.6419 - MinusLogProbMetric: 32.6419 - val_loss: 33.1040 - val_MinusLogProbMetric: 33.1040 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 48/1000
2023-10-31 05:49:59.214 
Epoch 48/1000 
	 loss: 32.6497, MinusLogProbMetric: 32.6497, val_loss: 33.1421, val_MinusLogProbMetric: 33.1421

Epoch 48: val_loss did not improve from 33.10397
196/196 - 56s - loss: 32.6497 - MinusLogProbMetric: 32.6497 - val_loss: 33.1421 - val_MinusLogProbMetric: 33.1421 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 49/1000
2023-10-31 05:50:59.401 
Epoch 49/1000 
	 loss: 32.6396, MinusLogProbMetric: 32.6396, val_loss: 33.7088, val_MinusLogProbMetric: 33.7088

Epoch 49: val_loss did not improve from 33.10397
196/196 - 60s - loss: 32.6396 - MinusLogProbMetric: 32.6396 - val_loss: 33.7088 - val_MinusLogProbMetric: 33.7088 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 50/1000
2023-10-31 05:52:00.699 
Epoch 50/1000 
	 loss: 33.2788, MinusLogProbMetric: 33.2788, val_loss: 33.8422, val_MinusLogProbMetric: 33.8422

Epoch 50: val_loss did not improve from 33.10397
196/196 - 61s - loss: 33.2788 - MinusLogProbMetric: 33.2788 - val_loss: 33.8422 - val_MinusLogProbMetric: 33.8422 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 51/1000
2023-10-31 05:53:04.624 
Epoch 51/1000 
	 loss: 33.0780, MinusLogProbMetric: 33.0780, val_loss: 33.5587, val_MinusLogProbMetric: 33.5587

Epoch 51: val_loss did not improve from 33.10397
196/196 - 64s - loss: 33.0780 - MinusLogProbMetric: 33.0780 - val_loss: 33.5587 - val_MinusLogProbMetric: 33.5587 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 52/1000
2023-10-31 05:54:06.806 
Epoch 52/1000 
	 loss: 32.7680, MinusLogProbMetric: 32.7680, val_loss: 33.0664, val_MinusLogProbMetric: 33.0664

Epoch 52: val_loss improved from 33.10397 to 33.06636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 32.7680 - MinusLogProbMetric: 32.7680 - val_loss: 33.0664 - val_MinusLogProbMetric: 33.0664 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 53/1000
2023-10-31 05:55:07.988 
Epoch 53/1000 
	 loss: 32.5990, MinusLogProbMetric: 32.5990, val_loss: 33.0382, val_MinusLogProbMetric: 33.0382

Epoch 53: val_loss improved from 33.06636 to 33.03819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 32.5990 - MinusLogProbMetric: 32.5990 - val_loss: 33.0382 - val_MinusLogProbMetric: 33.0382 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 54/1000
2023-10-31 05:56:07.305 
Epoch 54/1000 
	 loss: 32.5733, MinusLogProbMetric: 32.5733, val_loss: 33.1162, val_MinusLogProbMetric: 33.1162

Epoch 54: val_loss did not improve from 33.03819
196/196 - 58s - loss: 32.5733 - MinusLogProbMetric: 32.5733 - val_loss: 33.1162 - val_MinusLogProbMetric: 33.1162 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 55/1000
2023-10-31 05:57:04.004 
Epoch 55/1000 
	 loss: 32.5849, MinusLogProbMetric: 32.5849, val_loss: 33.0311, val_MinusLogProbMetric: 33.0311

Epoch 55: val_loss improved from 33.03819 to 33.03114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 32.5849 - MinusLogProbMetric: 32.5849 - val_loss: 33.0311 - val_MinusLogProbMetric: 33.0311 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 56/1000
2023-10-31 05:58:02.580 
Epoch 56/1000 
	 loss: 32.5766, MinusLogProbMetric: 32.5766, val_loss: 33.0479, val_MinusLogProbMetric: 33.0479

Epoch 56: val_loss did not improve from 33.03114
196/196 - 58s - loss: 32.5766 - MinusLogProbMetric: 32.5766 - val_loss: 33.0479 - val_MinusLogProbMetric: 33.0479 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 57/1000
2023-10-31 05:59:00.560 
Epoch 57/1000 
	 loss: 32.5656, MinusLogProbMetric: 32.5656, val_loss: 33.0822, val_MinusLogProbMetric: 33.0822

Epoch 57: val_loss did not improve from 33.03114
196/196 - 58s - loss: 32.5656 - MinusLogProbMetric: 32.5656 - val_loss: 33.0822 - val_MinusLogProbMetric: 33.0822 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 58/1000
2023-10-31 05:59:57.453 
Epoch 58/1000 
	 loss: 32.5702, MinusLogProbMetric: 32.5702, val_loss: 33.0457, val_MinusLogProbMetric: 33.0457

Epoch 58: val_loss did not improve from 33.03114
196/196 - 57s - loss: 32.5702 - MinusLogProbMetric: 32.5702 - val_loss: 33.0457 - val_MinusLogProbMetric: 33.0457 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 59/1000
2023-10-31 06:00:57.230 
Epoch 59/1000 
	 loss: 32.5334, MinusLogProbMetric: 32.5334, val_loss: 32.9891, val_MinusLogProbMetric: 32.9891

Epoch 59: val_loss improved from 33.03114 to 32.98907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 32.5334 - MinusLogProbMetric: 32.5334 - val_loss: 32.9891 - val_MinusLogProbMetric: 32.9891 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 60/1000
2023-10-31 06:01:55.317 
Epoch 60/1000 
	 loss: 32.5344, MinusLogProbMetric: 32.5344, val_loss: 33.0418, val_MinusLogProbMetric: 33.0418

Epoch 60: val_loss did not improve from 32.98907
196/196 - 56s - loss: 32.5344 - MinusLogProbMetric: 32.5344 - val_loss: 33.0418 - val_MinusLogProbMetric: 33.0418 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 61/1000
2023-10-31 06:02:55.618 
Epoch 61/1000 
	 loss: 32.5280, MinusLogProbMetric: 32.5280, val_loss: 33.1163, val_MinusLogProbMetric: 33.1163

Epoch 61: val_loss did not improve from 32.98907
196/196 - 60s - loss: 32.5280 - MinusLogProbMetric: 32.5280 - val_loss: 33.1163 - val_MinusLogProbMetric: 33.1163 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 62/1000
2023-10-31 06:03:52.189 
Epoch 62/1000 
	 loss: 32.5456, MinusLogProbMetric: 32.5456, val_loss: 33.0334, val_MinusLogProbMetric: 33.0334

Epoch 62: val_loss did not improve from 32.98907
196/196 - 57s - loss: 32.5456 - MinusLogProbMetric: 32.5456 - val_loss: 33.0334 - val_MinusLogProbMetric: 33.0334 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 63/1000
2023-10-31 06:04:46.981 
Epoch 63/1000 
	 loss: 32.5323, MinusLogProbMetric: 32.5323, val_loss: 33.0126, val_MinusLogProbMetric: 33.0126

Epoch 63: val_loss did not improve from 32.98907
196/196 - 55s - loss: 32.5323 - MinusLogProbMetric: 32.5323 - val_loss: 33.0126 - val_MinusLogProbMetric: 33.0126 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 64/1000
2023-10-31 06:05:45.387 
Epoch 64/1000 
	 loss: 32.5120, MinusLogProbMetric: 32.5120, val_loss: 32.9985, val_MinusLogProbMetric: 32.9985

Epoch 64: val_loss did not improve from 32.98907
196/196 - 58s - loss: 32.5120 - MinusLogProbMetric: 32.5120 - val_loss: 32.9985 - val_MinusLogProbMetric: 32.9985 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 65/1000
2023-10-31 06:06:48.342 
Epoch 65/1000 
	 loss: 32.5204, MinusLogProbMetric: 32.5204, val_loss: 33.0466, val_MinusLogProbMetric: 33.0466

Epoch 65: val_loss did not improve from 32.98907
196/196 - 63s - loss: 32.5204 - MinusLogProbMetric: 32.5204 - val_loss: 33.0466 - val_MinusLogProbMetric: 33.0466 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 66/1000
2023-10-31 06:07:47.619 
Epoch 66/1000 
	 loss: 32.5256, MinusLogProbMetric: 32.5256, val_loss: 33.0370, val_MinusLogProbMetric: 33.0370

Epoch 66: val_loss did not improve from 32.98907
196/196 - 59s - loss: 32.5256 - MinusLogProbMetric: 32.5256 - val_loss: 33.0370 - val_MinusLogProbMetric: 33.0370 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 67/1000
2023-10-31 06:08:44.942 
Epoch 67/1000 
	 loss: 32.5076, MinusLogProbMetric: 32.5076, val_loss: 32.9864, val_MinusLogProbMetric: 32.9864

Epoch 67: val_loss improved from 32.98907 to 32.98643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 32.5076 - MinusLogProbMetric: 32.5076 - val_loss: 32.9864 - val_MinusLogProbMetric: 32.9864 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 68/1000
2023-10-31 06:09:44.623 
Epoch 68/1000 
	 loss: 32.5120, MinusLogProbMetric: 32.5120, val_loss: 33.0500, val_MinusLogProbMetric: 33.0500

Epoch 68: val_loss did not improve from 32.98643
196/196 - 58s - loss: 32.5120 - MinusLogProbMetric: 32.5120 - val_loss: 33.0500 - val_MinusLogProbMetric: 33.0500 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 69/1000
2023-10-31 06:10:42.059 
Epoch 69/1000 
	 loss: 32.5249, MinusLogProbMetric: 32.5249, val_loss: 33.0028, val_MinusLogProbMetric: 33.0028

Epoch 69: val_loss did not improve from 32.98643
196/196 - 57s - loss: 32.5249 - MinusLogProbMetric: 32.5249 - val_loss: 33.0028 - val_MinusLogProbMetric: 33.0028 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 70/1000
2023-10-31 06:11:41.471 
Epoch 70/1000 
	 loss: 32.5202, MinusLogProbMetric: 32.5202, val_loss: 33.0518, val_MinusLogProbMetric: 33.0518

Epoch 70: val_loss did not improve from 32.98643
196/196 - 59s - loss: 32.5202 - MinusLogProbMetric: 32.5202 - val_loss: 33.0518 - val_MinusLogProbMetric: 33.0518 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 71/1000
2023-10-31 06:12:38.430 
Epoch 71/1000 
	 loss: 32.5054, MinusLogProbMetric: 32.5054, val_loss: 32.9957, val_MinusLogProbMetric: 32.9957

Epoch 71: val_loss did not improve from 32.98643
196/196 - 57s - loss: 32.5054 - MinusLogProbMetric: 32.5054 - val_loss: 32.9957 - val_MinusLogProbMetric: 32.9957 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 72/1000
2023-10-31 06:13:34.690 
Epoch 72/1000 
	 loss: 32.4972, MinusLogProbMetric: 32.4972, val_loss: 32.9397, val_MinusLogProbMetric: 32.9397

Epoch 72: val_loss improved from 32.98643 to 32.93969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.4972 - MinusLogProbMetric: 32.4972 - val_loss: 32.9397 - val_MinusLogProbMetric: 32.9397 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 73/1000
2023-10-31 06:14:31.936 
Epoch 73/1000 
	 loss: 32.4754, MinusLogProbMetric: 32.4754, val_loss: 32.9865, val_MinusLogProbMetric: 32.9865

Epoch 73: val_loss did not improve from 32.93969
196/196 - 56s - loss: 32.4754 - MinusLogProbMetric: 32.4754 - val_loss: 32.9865 - val_MinusLogProbMetric: 32.9865 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 74/1000
2023-10-31 06:15:26.678 
Epoch 74/1000 
	 loss: 32.4859, MinusLogProbMetric: 32.4859, val_loss: 32.9360, val_MinusLogProbMetric: 32.9360

Epoch 74: val_loss improved from 32.93969 to 32.93596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.4859 - MinusLogProbMetric: 32.4859 - val_loss: 32.9360 - val_MinusLogProbMetric: 32.9360 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 75/1000
2023-10-31 06:16:23.667 
Epoch 75/1000 
	 loss: 32.4767, MinusLogProbMetric: 32.4767, val_loss: 32.9583, val_MinusLogProbMetric: 32.9583

Epoch 75: val_loss did not improve from 32.93596
196/196 - 56s - loss: 32.4767 - MinusLogProbMetric: 32.4767 - val_loss: 32.9583 - val_MinusLogProbMetric: 32.9583 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 76/1000
2023-10-31 06:17:19.357 
Epoch 76/1000 
	 loss: 32.4921, MinusLogProbMetric: 32.4921, val_loss: 33.0247, val_MinusLogProbMetric: 33.0247

Epoch 76: val_loss did not improve from 32.93596
196/196 - 56s - loss: 32.4921 - MinusLogProbMetric: 32.4921 - val_loss: 33.0247 - val_MinusLogProbMetric: 33.0247 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 77/1000
2023-10-31 06:18:14.496 
Epoch 77/1000 
	 loss: 32.4642, MinusLogProbMetric: 32.4642, val_loss: 33.0015, val_MinusLogProbMetric: 33.0015

Epoch 77: val_loss did not improve from 32.93596
196/196 - 55s - loss: 32.4642 - MinusLogProbMetric: 32.4642 - val_loss: 33.0015 - val_MinusLogProbMetric: 33.0015 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 78/1000
2023-10-31 06:19:10.150 
Epoch 78/1000 
	 loss: 32.4856, MinusLogProbMetric: 32.4856, val_loss: 32.9999, val_MinusLogProbMetric: 32.9999

Epoch 78: val_loss did not improve from 32.93596
196/196 - 56s - loss: 32.4856 - MinusLogProbMetric: 32.4856 - val_loss: 32.9999 - val_MinusLogProbMetric: 32.9999 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 79/1000
2023-10-31 06:20:05.920 
Epoch 79/1000 
	 loss: 32.4631, MinusLogProbMetric: 32.4631, val_loss: 32.9819, val_MinusLogProbMetric: 32.9819

Epoch 79: val_loss did not improve from 32.93596
196/196 - 56s - loss: 32.4631 - MinusLogProbMetric: 32.4631 - val_loss: 32.9819 - val_MinusLogProbMetric: 32.9819 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 80/1000
2023-10-31 06:21:01.845 
Epoch 80/1000 
	 loss: 32.4904, MinusLogProbMetric: 32.4904, val_loss: 32.9298, val_MinusLogProbMetric: 32.9298

Epoch 80: val_loss improved from 32.93596 to 32.92981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.4904 - MinusLogProbMetric: 32.4904 - val_loss: 32.9298 - val_MinusLogProbMetric: 32.9298 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 81/1000
2023-10-31 06:22:01.117 
Epoch 81/1000 
	 loss: 32.4525, MinusLogProbMetric: 32.4525, val_loss: 32.9121, val_MinusLogProbMetric: 32.9121

Epoch 81: val_loss improved from 32.92981 to 32.91213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 32.4525 - MinusLogProbMetric: 32.4525 - val_loss: 32.9121 - val_MinusLogProbMetric: 32.9121 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 82/1000
2023-10-31 06:23:00.353 
Epoch 82/1000 
	 loss: 32.4581, MinusLogProbMetric: 32.4581, val_loss: 32.9474, val_MinusLogProbMetric: 32.9474

Epoch 82: val_loss did not improve from 32.91213
196/196 - 58s - loss: 32.4581 - MinusLogProbMetric: 32.4581 - val_loss: 32.9474 - val_MinusLogProbMetric: 32.9474 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 83/1000
2023-10-31 06:23:56.302 
Epoch 83/1000 
	 loss: 32.4580, MinusLogProbMetric: 32.4580, val_loss: 33.1539, val_MinusLogProbMetric: 33.1539

Epoch 83: val_loss did not improve from 32.91213
196/196 - 56s - loss: 32.4580 - MinusLogProbMetric: 32.4580 - val_loss: 33.1539 - val_MinusLogProbMetric: 33.1539 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 84/1000
2023-10-31 06:24:52.322 
Epoch 84/1000 
	 loss: 32.4489, MinusLogProbMetric: 32.4489, val_loss: 32.9478, val_MinusLogProbMetric: 32.9478

Epoch 84: val_loss did not improve from 32.91213
196/196 - 56s - loss: 32.4489 - MinusLogProbMetric: 32.4489 - val_loss: 32.9478 - val_MinusLogProbMetric: 32.9478 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 85/1000
2023-10-31 06:25:48.746 
Epoch 85/1000 
	 loss: 32.4355, MinusLogProbMetric: 32.4355, val_loss: 32.8824, val_MinusLogProbMetric: 32.8824

Epoch 85: val_loss improved from 32.91213 to 32.88241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.4355 - MinusLogProbMetric: 32.4355 - val_loss: 32.8824 - val_MinusLogProbMetric: 32.8824 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 86/1000
2023-10-31 06:26:48.129 
Epoch 86/1000 
	 loss: 32.4437, MinusLogProbMetric: 32.4437, val_loss: 32.9995, val_MinusLogProbMetric: 32.9995

Epoch 86: val_loss did not improve from 32.88241
196/196 - 58s - loss: 32.4437 - MinusLogProbMetric: 32.4437 - val_loss: 32.9995 - val_MinusLogProbMetric: 32.9995 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 87/1000
2023-10-31 06:27:48.781 
Epoch 87/1000 
	 loss: 32.4536, MinusLogProbMetric: 32.4536, val_loss: 32.9717, val_MinusLogProbMetric: 32.9717

Epoch 87: val_loss did not improve from 32.88241
196/196 - 61s - loss: 32.4536 - MinusLogProbMetric: 32.4536 - val_loss: 32.9717 - val_MinusLogProbMetric: 32.9717 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 88/1000
2023-10-31 06:28:46.730 
Epoch 88/1000 
	 loss: 32.4560, MinusLogProbMetric: 32.4560, val_loss: 33.0008, val_MinusLogProbMetric: 33.0008

Epoch 88: val_loss did not improve from 32.88241
196/196 - 58s - loss: 32.4560 - MinusLogProbMetric: 32.4560 - val_loss: 33.0008 - val_MinusLogProbMetric: 33.0008 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 89/1000
2023-10-31 06:29:43.844 
Epoch 89/1000 
	 loss: 32.4626, MinusLogProbMetric: 32.4626, val_loss: 32.9037, val_MinusLogProbMetric: 32.9037

Epoch 89: val_loss did not improve from 32.88241
196/196 - 57s - loss: 32.4626 - MinusLogProbMetric: 32.4626 - val_loss: 32.9037 - val_MinusLogProbMetric: 32.9037 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 90/1000
2023-10-31 06:30:40.354 
Epoch 90/1000 
	 loss: 32.4402, MinusLogProbMetric: 32.4402, val_loss: 33.0467, val_MinusLogProbMetric: 33.0467

Epoch 90: val_loss did not improve from 32.88241
196/196 - 57s - loss: 32.4402 - MinusLogProbMetric: 32.4402 - val_loss: 33.0467 - val_MinusLogProbMetric: 33.0467 - lr: 1.3717e-06 - 57s/epoch - 288ms/step
Epoch 91/1000
2023-10-31 06:31:34.800 
Epoch 91/1000 
	 loss: 32.4314, MinusLogProbMetric: 32.4314, val_loss: 32.9823, val_MinusLogProbMetric: 32.9823

Epoch 91: val_loss did not improve from 32.88241
196/196 - 54s - loss: 32.4314 - MinusLogProbMetric: 32.4314 - val_loss: 32.9823 - val_MinusLogProbMetric: 32.9823 - lr: 1.3717e-06 - 54s/epoch - 278ms/step
Epoch 92/1000
2023-10-31 06:32:30.289 
Epoch 92/1000 
	 loss: 32.4245, MinusLogProbMetric: 32.4245, val_loss: 32.9574, val_MinusLogProbMetric: 32.9574

Epoch 92: val_loss did not improve from 32.88241
196/196 - 55s - loss: 32.4245 - MinusLogProbMetric: 32.4245 - val_loss: 32.9574 - val_MinusLogProbMetric: 32.9574 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 93/1000
2023-10-31 06:33:25.530 
Epoch 93/1000 
	 loss: 32.4317, MinusLogProbMetric: 32.4317, val_loss: 32.9025, val_MinusLogProbMetric: 32.9025

Epoch 93: val_loss did not improve from 32.88241
196/196 - 55s - loss: 32.4317 - MinusLogProbMetric: 32.4317 - val_loss: 32.9025 - val_MinusLogProbMetric: 32.9025 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 94/1000
2023-10-31 06:34:20.891 
Epoch 94/1000 
	 loss: 32.4256, MinusLogProbMetric: 32.4256, val_loss: 32.8887, val_MinusLogProbMetric: 32.8887

Epoch 94: val_loss did not improve from 32.88241
196/196 - 55s - loss: 32.4256 - MinusLogProbMetric: 32.4256 - val_loss: 32.8887 - val_MinusLogProbMetric: 32.8887 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 95/1000
2023-10-31 06:35:16.720 
Epoch 95/1000 
	 loss: 32.4139, MinusLogProbMetric: 32.4139, val_loss: 32.8972, val_MinusLogProbMetric: 32.8972

Epoch 95: val_loss did not improve from 32.88241
196/196 - 56s - loss: 32.4139 - MinusLogProbMetric: 32.4139 - val_loss: 32.8972 - val_MinusLogProbMetric: 32.8972 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 96/1000
2023-10-31 06:36:13.592 
Epoch 96/1000 
	 loss: 32.4249, MinusLogProbMetric: 32.4249, val_loss: 32.9229, val_MinusLogProbMetric: 32.9229

Epoch 96: val_loss did not improve from 32.88241
196/196 - 57s - loss: 32.4249 - MinusLogProbMetric: 32.4249 - val_loss: 32.9229 - val_MinusLogProbMetric: 32.9229 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 97/1000
2023-10-31 06:37:10.082 
Epoch 97/1000 
	 loss: 32.4183, MinusLogProbMetric: 32.4183, val_loss: 32.8981, val_MinusLogProbMetric: 32.8981

Epoch 97: val_loss did not improve from 32.88241
196/196 - 56s - loss: 32.4183 - MinusLogProbMetric: 32.4183 - val_loss: 32.8981 - val_MinusLogProbMetric: 32.8981 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 98/1000
2023-10-31 06:38:04.492 
Epoch 98/1000 
	 loss: 32.4174, MinusLogProbMetric: 32.4174, val_loss: 32.9907, val_MinusLogProbMetric: 32.9907

Epoch 98: val_loss did not improve from 32.88241
196/196 - 54s - loss: 32.4174 - MinusLogProbMetric: 32.4174 - val_loss: 32.9907 - val_MinusLogProbMetric: 32.9907 - lr: 1.3717e-06 - 54s/epoch - 278ms/step
Epoch 99/1000
2023-10-31 06:38:59.817 
Epoch 99/1000 
	 loss: 32.4482, MinusLogProbMetric: 32.4482, val_loss: 32.9393, val_MinusLogProbMetric: 32.9393

Epoch 99: val_loss did not improve from 32.88241
196/196 - 55s - loss: 32.4482 - MinusLogProbMetric: 32.4482 - val_loss: 32.9393 - val_MinusLogProbMetric: 32.9393 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 100/1000
2023-10-31 06:39:55.934 
Epoch 100/1000 
	 loss: 32.4288, MinusLogProbMetric: 32.4288, val_loss: 32.8725, val_MinusLogProbMetric: 32.8725

Epoch 100: val_loss improved from 32.88241 to 32.87246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.4288 - MinusLogProbMetric: 32.4288 - val_loss: 32.8725 - val_MinusLogProbMetric: 32.8725 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 101/1000
2023-10-31 06:40:56.239 
Epoch 101/1000 
	 loss: 32.3875, MinusLogProbMetric: 32.3875, val_loss: 32.8985, val_MinusLogProbMetric: 32.8985

Epoch 101: val_loss did not improve from 32.87246
196/196 - 59s - loss: 32.3875 - MinusLogProbMetric: 32.3875 - val_loss: 32.8985 - val_MinusLogProbMetric: 32.8985 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 102/1000
2023-10-31 06:41:56.873 
Epoch 102/1000 
	 loss: 32.3921, MinusLogProbMetric: 32.3921, val_loss: 32.9002, val_MinusLogProbMetric: 32.9002

Epoch 102: val_loss did not improve from 32.87246
196/196 - 61s - loss: 32.3921 - MinusLogProbMetric: 32.3921 - val_loss: 32.9002 - val_MinusLogProbMetric: 32.9002 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 103/1000
2023-10-31 06:42:55.636 
Epoch 103/1000 
	 loss: 32.3969, MinusLogProbMetric: 32.3969, val_loss: 33.0303, val_MinusLogProbMetric: 33.0303

Epoch 103: val_loss did not improve from 32.87246
196/196 - 59s - loss: 32.3969 - MinusLogProbMetric: 32.3969 - val_loss: 33.0303 - val_MinusLogProbMetric: 33.0303 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 104/1000
2023-10-31 06:43:53.860 
Epoch 104/1000 
	 loss: 32.3899, MinusLogProbMetric: 32.3899, val_loss: 32.8557, val_MinusLogProbMetric: 32.8557

Epoch 104: val_loss improved from 32.87246 to 32.85566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 32.3899 - MinusLogProbMetric: 32.3899 - val_loss: 32.8557 - val_MinusLogProbMetric: 32.8557 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 105/1000
2023-10-31 06:44:56.393 
Epoch 105/1000 
	 loss: 32.3972, MinusLogProbMetric: 32.3972, val_loss: 32.9266, val_MinusLogProbMetric: 32.9266

Epoch 105: val_loss did not improve from 32.85566
196/196 - 62s - loss: 32.3972 - MinusLogProbMetric: 32.3972 - val_loss: 32.9266 - val_MinusLogProbMetric: 32.9266 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 106/1000
2023-10-31 06:45:52.356 
Epoch 106/1000 
	 loss: 32.3890, MinusLogProbMetric: 32.3890, val_loss: 32.9012, val_MinusLogProbMetric: 32.9012

Epoch 106: val_loss did not improve from 32.85566
196/196 - 56s - loss: 32.3890 - MinusLogProbMetric: 32.3890 - val_loss: 32.9012 - val_MinusLogProbMetric: 32.9012 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 107/1000
2023-10-31 06:46:51.785 
Epoch 107/1000 
	 loss: 32.3751, MinusLogProbMetric: 32.3751, val_loss: 32.8616, val_MinusLogProbMetric: 32.8616

Epoch 107: val_loss did not improve from 32.85566
196/196 - 59s - loss: 32.3751 - MinusLogProbMetric: 32.3751 - val_loss: 32.8616 - val_MinusLogProbMetric: 32.8616 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 108/1000
2023-10-31 06:47:49.143 
Epoch 108/1000 
	 loss: 32.3584, MinusLogProbMetric: 32.3584, val_loss: 32.8690, val_MinusLogProbMetric: 32.8690

Epoch 108: val_loss did not improve from 32.85566
196/196 - 57s - loss: 32.3584 - MinusLogProbMetric: 32.3584 - val_loss: 32.8690 - val_MinusLogProbMetric: 32.8690 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 109/1000
2023-10-31 06:48:44.279 
Epoch 109/1000 
	 loss: 32.3752, MinusLogProbMetric: 32.3752, val_loss: 32.8524, val_MinusLogProbMetric: 32.8524

Epoch 109: val_loss improved from 32.85566 to 32.85244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.3752 - MinusLogProbMetric: 32.3752 - val_loss: 32.8524 - val_MinusLogProbMetric: 32.8524 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 110/1000
2023-10-31 06:49:40.463 
Epoch 110/1000 
	 loss: 32.3553, MinusLogProbMetric: 32.3553, val_loss: 32.8711, val_MinusLogProbMetric: 32.8711

Epoch 110: val_loss did not improve from 32.85244
196/196 - 55s - loss: 32.3553 - MinusLogProbMetric: 32.3553 - val_loss: 32.8711 - val_MinusLogProbMetric: 32.8711 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 111/1000
2023-10-31 06:50:35.808 
Epoch 111/1000 
	 loss: 32.3904, MinusLogProbMetric: 32.3904, val_loss: 32.8470, val_MinusLogProbMetric: 32.8470

Epoch 111: val_loss improved from 32.85244 to 32.84704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.3904 - MinusLogProbMetric: 32.3904 - val_loss: 32.8470 - val_MinusLogProbMetric: 32.8470 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 112/1000
2023-10-31 06:51:32.624 
Epoch 112/1000 
	 loss: 32.3634, MinusLogProbMetric: 32.3634, val_loss: 32.9092, val_MinusLogProbMetric: 32.9092

Epoch 112: val_loss did not improve from 32.84704
196/196 - 56s - loss: 32.3634 - MinusLogProbMetric: 32.3634 - val_loss: 32.9092 - val_MinusLogProbMetric: 32.9092 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 113/1000
2023-10-31 06:52:28.635 
Epoch 113/1000 
	 loss: 32.3764, MinusLogProbMetric: 32.3764, val_loss: 32.8858, val_MinusLogProbMetric: 32.8858

Epoch 113: val_loss did not improve from 32.84704
196/196 - 56s - loss: 32.3764 - MinusLogProbMetric: 32.3764 - val_loss: 32.8858 - val_MinusLogProbMetric: 32.8858 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 114/1000
2023-10-31 06:53:28.080 
Epoch 114/1000 
	 loss: 32.3480, MinusLogProbMetric: 32.3480, val_loss: 32.8823, val_MinusLogProbMetric: 32.8823

Epoch 114: val_loss did not improve from 32.84704
196/196 - 59s - loss: 32.3480 - MinusLogProbMetric: 32.3480 - val_loss: 32.8823 - val_MinusLogProbMetric: 32.8823 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 115/1000
2023-10-31 06:54:28.263 
Epoch 115/1000 
	 loss: 32.3579, MinusLogProbMetric: 32.3579, val_loss: 32.8992, val_MinusLogProbMetric: 32.8992

Epoch 115: val_loss did not improve from 32.84704
196/196 - 60s - loss: 32.3579 - MinusLogProbMetric: 32.3579 - val_loss: 32.8992 - val_MinusLogProbMetric: 32.8992 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 116/1000
2023-10-31 06:55:25.302 
Epoch 116/1000 
	 loss: 32.3608, MinusLogProbMetric: 32.3608, val_loss: 32.8392, val_MinusLogProbMetric: 32.8392

Epoch 116: val_loss improved from 32.84704 to 32.83921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 32.3608 - MinusLogProbMetric: 32.3608 - val_loss: 32.8392 - val_MinusLogProbMetric: 32.8392 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 117/1000
2023-10-31 06:56:23.178 
Epoch 117/1000 
	 loss: 32.3515, MinusLogProbMetric: 32.3515, val_loss: 32.8622, val_MinusLogProbMetric: 32.8622

Epoch 117: val_loss did not improve from 32.83921
196/196 - 56s - loss: 32.3515 - MinusLogProbMetric: 32.3515 - val_loss: 32.8622 - val_MinusLogProbMetric: 32.8622 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 118/1000
2023-10-31 06:57:18.184 
Epoch 118/1000 
	 loss: 32.3502, MinusLogProbMetric: 32.3502, val_loss: 32.8790, val_MinusLogProbMetric: 32.8790

Epoch 118: val_loss did not improve from 32.83921
196/196 - 55s - loss: 32.3502 - MinusLogProbMetric: 32.3502 - val_loss: 32.8790 - val_MinusLogProbMetric: 32.8790 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 119/1000
2023-10-31 06:58:15.609 
Epoch 119/1000 
	 loss: 32.3533, MinusLogProbMetric: 32.3533, val_loss: 32.8631, val_MinusLogProbMetric: 32.8631

Epoch 119: val_loss did not improve from 32.83921
196/196 - 57s - loss: 32.3533 - MinusLogProbMetric: 32.3533 - val_loss: 32.8631 - val_MinusLogProbMetric: 32.8631 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 120/1000
2023-10-31 06:59:14.798 
Epoch 120/1000 
	 loss: 32.3476, MinusLogProbMetric: 32.3476, val_loss: 33.3072, val_MinusLogProbMetric: 33.3072

Epoch 120: val_loss did not improve from 32.83921
196/196 - 59s - loss: 32.3476 - MinusLogProbMetric: 32.3476 - val_loss: 33.3072 - val_MinusLogProbMetric: 33.3072 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 121/1000
2023-10-31 07:00:14.287 
Epoch 121/1000 
	 loss: 32.3812, MinusLogProbMetric: 32.3812, val_loss: 32.9470, val_MinusLogProbMetric: 32.9470

Epoch 121: val_loss did not improve from 32.83921
196/196 - 59s - loss: 32.3812 - MinusLogProbMetric: 32.3812 - val_loss: 32.9470 - val_MinusLogProbMetric: 32.9470 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 122/1000
2023-10-31 07:01:12.980 
Epoch 122/1000 
	 loss: 32.3609, MinusLogProbMetric: 32.3609, val_loss: 32.9944, val_MinusLogProbMetric: 32.9944

Epoch 122: val_loss did not improve from 32.83921
196/196 - 59s - loss: 32.3609 - MinusLogProbMetric: 32.3609 - val_loss: 32.9944 - val_MinusLogProbMetric: 32.9944 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 123/1000
2023-10-31 07:02:08.261 
Epoch 123/1000 
	 loss: 32.3477, MinusLogProbMetric: 32.3477, val_loss: 32.8773, val_MinusLogProbMetric: 32.8773

Epoch 123: val_loss did not improve from 32.83921
196/196 - 55s - loss: 32.3477 - MinusLogProbMetric: 32.3477 - val_loss: 32.8773 - val_MinusLogProbMetric: 32.8773 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 124/1000
2023-10-31 07:03:08.009 
Epoch 124/1000 
	 loss: 32.3331, MinusLogProbMetric: 32.3331, val_loss: 32.8092, val_MinusLogProbMetric: 32.8092

Epoch 124: val_loss improved from 32.83921 to 32.80920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 32.3331 - MinusLogProbMetric: 32.3331 - val_loss: 32.8092 - val_MinusLogProbMetric: 32.8092 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 125/1000
2023-10-31 07:04:09.875 
Epoch 125/1000 
	 loss: 32.3208, MinusLogProbMetric: 32.3208, val_loss: 32.8398, val_MinusLogProbMetric: 32.8398

Epoch 125: val_loss did not improve from 32.80920
196/196 - 59s - loss: 32.3208 - MinusLogProbMetric: 32.3208 - val_loss: 32.8398 - val_MinusLogProbMetric: 32.8398 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 126/1000
2023-10-31 07:05:06.242 
Epoch 126/1000 
	 loss: 32.3297, MinusLogProbMetric: 32.3297, val_loss: 32.8802, val_MinusLogProbMetric: 32.8802

Epoch 126: val_loss did not improve from 32.80920
196/196 - 56s - loss: 32.3297 - MinusLogProbMetric: 32.3297 - val_loss: 32.8802 - val_MinusLogProbMetric: 32.8802 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 127/1000
2023-10-31 07:06:02.856 
Epoch 127/1000 
	 loss: 32.3252, MinusLogProbMetric: 32.3252, val_loss: 32.9239, val_MinusLogProbMetric: 32.9239

Epoch 127: val_loss did not improve from 32.80920
196/196 - 57s - loss: 32.3252 - MinusLogProbMetric: 32.3252 - val_loss: 32.9239 - val_MinusLogProbMetric: 32.9239 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 128/1000
2023-10-31 07:06:58.916 
Epoch 128/1000 
	 loss: 32.3176, MinusLogProbMetric: 32.3176, val_loss: 32.8882, val_MinusLogProbMetric: 32.8882

Epoch 128: val_loss did not improve from 32.80920
196/196 - 56s - loss: 32.3176 - MinusLogProbMetric: 32.3176 - val_loss: 32.8882 - val_MinusLogProbMetric: 32.8882 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 129/1000
2023-10-31 07:07:55.611 
Epoch 129/1000 
	 loss: 32.3315, MinusLogProbMetric: 32.3315, val_loss: 32.8925, val_MinusLogProbMetric: 32.8925

Epoch 129: val_loss did not improve from 32.80920
196/196 - 57s - loss: 32.3315 - MinusLogProbMetric: 32.3315 - val_loss: 32.8925 - val_MinusLogProbMetric: 32.8925 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 130/1000
2023-10-31 07:08:53.182 
Epoch 130/1000 
	 loss: 32.3069, MinusLogProbMetric: 32.3069, val_loss: 32.9419, val_MinusLogProbMetric: 32.9419

Epoch 130: val_loss did not improve from 32.80920
196/196 - 58s - loss: 32.3069 - MinusLogProbMetric: 32.3069 - val_loss: 32.9419 - val_MinusLogProbMetric: 32.9419 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 131/1000
2023-10-31 07:09:53.640 
Epoch 131/1000 
	 loss: 32.3151, MinusLogProbMetric: 32.3151, val_loss: 32.8597, val_MinusLogProbMetric: 32.8597

Epoch 131: val_loss did not improve from 32.80920
196/196 - 60s - loss: 32.3151 - MinusLogProbMetric: 32.3151 - val_loss: 32.8597 - val_MinusLogProbMetric: 32.8597 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 132/1000
2023-10-31 07:10:53.584 
Epoch 132/1000 
	 loss: 32.3209, MinusLogProbMetric: 32.3209, val_loss: 32.9021, val_MinusLogProbMetric: 32.9021

Epoch 132: val_loss did not improve from 32.80920
196/196 - 60s - loss: 32.3209 - MinusLogProbMetric: 32.3209 - val_loss: 32.9021 - val_MinusLogProbMetric: 32.9021 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 133/1000
2023-10-31 07:11:49.655 
Epoch 133/1000 
	 loss: 32.2838, MinusLogProbMetric: 32.2838, val_loss: 32.9636, val_MinusLogProbMetric: 32.9636

Epoch 133: val_loss did not improve from 32.80920
196/196 - 56s - loss: 32.2838 - MinusLogProbMetric: 32.2838 - val_loss: 32.9636 - val_MinusLogProbMetric: 32.9636 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 134/1000
2023-10-31 07:12:45.079 
Epoch 134/1000 
	 loss: 32.3159, MinusLogProbMetric: 32.3159, val_loss: 32.8753, val_MinusLogProbMetric: 32.8753

Epoch 134: val_loss did not improve from 32.80920
196/196 - 55s - loss: 32.3159 - MinusLogProbMetric: 32.3159 - val_loss: 32.8753 - val_MinusLogProbMetric: 32.8753 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 135/1000
2023-10-31 07:13:41.215 
Epoch 135/1000 
	 loss: 32.3234, MinusLogProbMetric: 32.3234, val_loss: 32.9597, val_MinusLogProbMetric: 32.9597

Epoch 135: val_loss did not improve from 32.80920
196/196 - 56s - loss: 32.3234 - MinusLogProbMetric: 32.3234 - val_loss: 32.9597 - val_MinusLogProbMetric: 32.9597 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 136/1000
2023-10-31 07:14:36.715 
Epoch 136/1000 
	 loss: 32.3257, MinusLogProbMetric: 32.3257, val_loss: 32.9229, val_MinusLogProbMetric: 32.9229

Epoch 136: val_loss did not improve from 32.80920
196/196 - 55s - loss: 32.3257 - MinusLogProbMetric: 32.3257 - val_loss: 32.9229 - val_MinusLogProbMetric: 32.9229 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 137/1000
2023-10-31 07:15:31.759 
Epoch 137/1000 
	 loss: 32.2960, MinusLogProbMetric: 32.2960, val_loss: 32.8175, val_MinusLogProbMetric: 32.8175

Epoch 137: val_loss did not improve from 32.80920
196/196 - 55s - loss: 32.2960 - MinusLogProbMetric: 32.2960 - val_loss: 32.8175 - val_MinusLogProbMetric: 32.8175 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 138/1000
2023-10-31 07:16:27.009 
Epoch 138/1000 
	 loss: 32.2807, MinusLogProbMetric: 32.2807, val_loss: 32.9309, val_MinusLogProbMetric: 32.9309

Epoch 138: val_loss did not improve from 32.80920
196/196 - 55s - loss: 32.2807 - MinusLogProbMetric: 32.2807 - val_loss: 32.9309 - val_MinusLogProbMetric: 32.9309 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 139/1000
2023-10-31 07:17:23.094 
Epoch 139/1000 
	 loss: 32.2987, MinusLogProbMetric: 32.2987, val_loss: 32.8524, val_MinusLogProbMetric: 32.8524

Epoch 139: val_loss did not improve from 32.80920
196/196 - 56s - loss: 32.2987 - MinusLogProbMetric: 32.2987 - val_loss: 32.8524 - val_MinusLogProbMetric: 32.8524 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 140/1000
2023-10-31 07:18:17.971 
Epoch 140/1000 
	 loss: 32.3035, MinusLogProbMetric: 32.3035, val_loss: 32.7888, val_MinusLogProbMetric: 32.7888

Epoch 140: val_loss improved from 32.80920 to 32.78882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.3035 - MinusLogProbMetric: 32.3035 - val_loss: 32.7888 - val_MinusLogProbMetric: 32.7888 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 141/1000
2023-10-31 07:19:14.642 
Epoch 141/1000 
	 loss: 32.2753, MinusLogProbMetric: 32.2753, val_loss: 32.8905, val_MinusLogProbMetric: 32.8905

Epoch 141: val_loss did not improve from 32.78882
196/196 - 56s - loss: 32.2753 - MinusLogProbMetric: 32.2753 - val_loss: 32.8905 - val_MinusLogProbMetric: 32.8905 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 142/1000
2023-10-31 07:20:09.453 
Epoch 142/1000 
	 loss: 32.2831, MinusLogProbMetric: 32.2831, val_loss: 32.8335, val_MinusLogProbMetric: 32.8335

Epoch 142: val_loss did not improve from 32.78882
196/196 - 55s - loss: 32.2831 - MinusLogProbMetric: 32.2831 - val_loss: 32.8335 - val_MinusLogProbMetric: 32.8335 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 143/1000
2023-10-31 07:21:06.413 
Epoch 143/1000 
	 loss: 32.2711, MinusLogProbMetric: 32.2711, val_loss: 32.8473, val_MinusLogProbMetric: 32.8473

Epoch 143: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.2711 - MinusLogProbMetric: 32.2711 - val_loss: 32.8473 - val_MinusLogProbMetric: 32.8473 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 144/1000
2023-10-31 07:22:02.133 
Epoch 144/1000 
	 loss: 32.2759, MinusLogProbMetric: 32.2759, val_loss: 32.8493, val_MinusLogProbMetric: 32.8493

Epoch 144: val_loss did not improve from 32.78882
196/196 - 56s - loss: 32.2759 - MinusLogProbMetric: 32.2759 - val_loss: 32.8493 - val_MinusLogProbMetric: 32.8493 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 145/1000
2023-10-31 07:22:59.547 
Epoch 145/1000 
	 loss: 32.2773, MinusLogProbMetric: 32.2773, val_loss: 32.8781, val_MinusLogProbMetric: 32.8781

Epoch 145: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.2773 - MinusLogProbMetric: 32.2773 - val_loss: 32.8781 - val_MinusLogProbMetric: 32.8781 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 146/1000
2023-10-31 07:23:55.170 
Epoch 146/1000 
	 loss: 35.5413, MinusLogProbMetric: 35.5413, val_loss: 33.6609, val_MinusLogProbMetric: 33.6609

Epoch 146: val_loss did not improve from 32.78882
196/196 - 56s - loss: 35.5413 - MinusLogProbMetric: 35.5413 - val_loss: 33.6609 - val_MinusLogProbMetric: 33.6609 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 147/1000
2023-10-31 07:24:54.989 
Epoch 147/1000 
	 loss: 32.8717, MinusLogProbMetric: 32.8717, val_loss: 33.1728, val_MinusLogProbMetric: 33.1728

Epoch 147: val_loss did not improve from 32.78882
196/196 - 60s - loss: 32.8717 - MinusLogProbMetric: 32.8717 - val_loss: 33.1728 - val_MinusLogProbMetric: 33.1728 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 148/1000
2023-10-31 07:25:52.313 
Epoch 148/1000 
	 loss: 32.6239, MinusLogProbMetric: 32.6239, val_loss: 32.9993, val_MinusLogProbMetric: 32.9993

Epoch 148: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.6239 - MinusLogProbMetric: 32.6239 - val_loss: 32.9993 - val_MinusLogProbMetric: 32.9993 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 149/1000
2023-10-31 07:26:48.839 
Epoch 149/1000 
	 loss: 32.5056, MinusLogProbMetric: 32.5056, val_loss: 32.9803, val_MinusLogProbMetric: 32.9803

Epoch 149: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.5056 - MinusLogProbMetric: 32.5056 - val_loss: 32.9803 - val_MinusLogProbMetric: 32.9803 - lr: 1.3717e-06 - 57s/epoch - 288ms/step
Epoch 150/1000
2023-10-31 07:27:46.215 
Epoch 150/1000 
	 loss: 32.4404, MinusLogProbMetric: 32.4404, val_loss: 32.9466, val_MinusLogProbMetric: 32.9466

Epoch 150: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.4404 - MinusLogProbMetric: 32.4404 - val_loss: 32.9466 - val_MinusLogProbMetric: 32.9466 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 151/1000
2023-10-31 07:28:43.380 
Epoch 151/1000 
	 loss: 32.4254, MinusLogProbMetric: 32.4254, val_loss: 32.9602, val_MinusLogProbMetric: 32.9602

Epoch 151: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.4254 - MinusLogProbMetric: 32.4254 - val_loss: 32.9602 - val_MinusLogProbMetric: 32.9602 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 152/1000
2023-10-31 07:29:38.829 
Epoch 152/1000 
	 loss: 32.4118, MinusLogProbMetric: 32.4118, val_loss: 32.9156, val_MinusLogProbMetric: 32.9156

Epoch 152: val_loss did not improve from 32.78882
196/196 - 55s - loss: 32.4118 - MinusLogProbMetric: 32.4118 - val_loss: 32.9156 - val_MinusLogProbMetric: 32.9156 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 153/1000
2023-10-31 07:30:36.496 
Epoch 153/1000 
	 loss: 32.3726, MinusLogProbMetric: 32.3726, val_loss: 32.8601, val_MinusLogProbMetric: 32.8601

Epoch 153: val_loss did not improve from 32.78882
196/196 - 58s - loss: 32.3726 - MinusLogProbMetric: 32.3726 - val_loss: 32.8601 - val_MinusLogProbMetric: 32.8601 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 154/1000
2023-10-31 07:31:34.678 
Epoch 154/1000 
	 loss: 32.3681, MinusLogProbMetric: 32.3681, val_loss: 32.9111, val_MinusLogProbMetric: 32.9111

Epoch 154: val_loss did not improve from 32.78882
196/196 - 58s - loss: 32.3681 - MinusLogProbMetric: 32.3681 - val_loss: 32.9111 - val_MinusLogProbMetric: 32.9111 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 155/1000
2023-10-31 07:32:29.585 
Epoch 155/1000 
	 loss: 32.3580, MinusLogProbMetric: 32.3580, val_loss: 32.9046, val_MinusLogProbMetric: 32.9046

Epoch 155: val_loss did not improve from 32.78882
196/196 - 55s - loss: 32.3580 - MinusLogProbMetric: 32.3580 - val_loss: 32.9046 - val_MinusLogProbMetric: 32.9046 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 156/1000
2023-10-31 07:33:25.079 
Epoch 156/1000 
	 loss: 32.3444, MinusLogProbMetric: 32.3444, val_loss: 32.8149, val_MinusLogProbMetric: 32.8149

Epoch 156: val_loss did not improve from 32.78882
196/196 - 55s - loss: 32.3444 - MinusLogProbMetric: 32.3444 - val_loss: 32.8149 - val_MinusLogProbMetric: 32.8149 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 157/1000
2023-10-31 07:34:21.902 
Epoch 157/1000 
	 loss: 32.3197, MinusLogProbMetric: 32.3197, val_loss: 32.8203, val_MinusLogProbMetric: 32.8203

Epoch 157: val_loss did not improve from 32.78882
196/196 - 57s - loss: 32.3197 - MinusLogProbMetric: 32.3197 - val_loss: 32.8203 - val_MinusLogProbMetric: 32.8203 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 158/1000
2023-10-31 07:35:21.927 
Epoch 158/1000 
	 loss: 32.3336, MinusLogProbMetric: 32.3336, val_loss: 32.8953, val_MinusLogProbMetric: 32.8953

Epoch 158: val_loss did not improve from 32.78882
196/196 - 60s - loss: 32.3336 - MinusLogProbMetric: 32.3336 - val_loss: 32.8953 - val_MinusLogProbMetric: 32.8953 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 159/1000
2023-10-31 07:36:18.157 
Epoch 159/1000 
	 loss: 32.3109, MinusLogProbMetric: 32.3109, val_loss: 32.9705, val_MinusLogProbMetric: 32.9705

Epoch 159: val_loss did not improve from 32.78882
196/196 - 56s - loss: 32.3109 - MinusLogProbMetric: 32.3109 - val_loss: 32.9705 - val_MinusLogProbMetric: 32.9705 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 160/1000
2023-10-31 07:37:18.224 
Epoch 160/1000 
	 loss: 32.3102, MinusLogProbMetric: 32.3102, val_loss: 32.8494, val_MinusLogProbMetric: 32.8494

Epoch 160: val_loss did not improve from 32.78882
196/196 - 60s - loss: 32.3102 - MinusLogProbMetric: 32.3102 - val_loss: 32.8494 - val_MinusLogProbMetric: 32.8494 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 161/1000
2023-10-31 07:38:17.485 
Epoch 161/1000 
	 loss: 32.2990, MinusLogProbMetric: 32.2990, val_loss: 32.7905, val_MinusLogProbMetric: 32.7905

Epoch 161: val_loss did not improve from 32.78882
196/196 - 59s - loss: 32.2990 - MinusLogProbMetric: 32.2990 - val_loss: 32.7905 - val_MinusLogProbMetric: 32.7905 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 162/1000
2023-10-31 07:39:13.777 
Epoch 162/1000 
	 loss: 32.2947, MinusLogProbMetric: 32.2947, val_loss: 32.7842, val_MinusLogProbMetric: 32.7842

Epoch 162: val_loss improved from 32.78882 to 32.78418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 32.2947 - MinusLogProbMetric: 32.2947 - val_loss: 32.7842 - val_MinusLogProbMetric: 32.7842 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 163/1000
2023-10-31 07:40:09.432 
Epoch 163/1000 
	 loss: 32.2795, MinusLogProbMetric: 32.2795, val_loss: 32.7868, val_MinusLogProbMetric: 32.7868

Epoch 163: val_loss did not improve from 32.78418
196/196 - 55s - loss: 32.2795 - MinusLogProbMetric: 32.2795 - val_loss: 32.7868 - val_MinusLogProbMetric: 32.7868 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 164/1000
2023-10-31 07:41:06.847 
Epoch 164/1000 
	 loss: 32.2960, MinusLogProbMetric: 32.2960, val_loss: 32.7957, val_MinusLogProbMetric: 32.7957

Epoch 164: val_loss did not improve from 32.78418
196/196 - 57s - loss: 32.2960 - MinusLogProbMetric: 32.2960 - val_loss: 32.7957 - val_MinusLogProbMetric: 32.7957 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 165/1000
2023-10-31 07:42:06.990 
Epoch 165/1000 
	 loss: 32.2776, MinusLogProbMetric: 32.2776, val_loss: 32.8196, val_MinusLogProbMetric: 32.8196

Epoch 165: val_loss did not improve from 32.78418
196/196 - 60s - loss: 32.2776 - MinusLogProbMetric: 32.2776 - val_loss: 32.8196 - val_MinusLogProbMetric: 32.8196 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 166/1000
2023-10-31 07:43:08.376 
Epoch 166/1000 
	 loss: 32.2578, MinusLogProbMetric: 32.2578, val_loss: 32.7944, val_MinusLogProbMetric: 32.7944

Epoch 166: val_loss did not improve from 32.78418
196/196 - 61s - loss: 32.2578 - MinusLogProbMetric: 32.2578 - val_loss: 32.7944 - val_MinusLogProbMetric: 32.7944 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 167/1000
2023-10-31 07:44:09.048 
Epoch 167/1000 
	 loss: 32.2685, MinusLogProbMetric: 32.2685, val_loss: 32.8494, val_MinusLogProbMetric: 32.8494

Epoch 167: val_loss did not improve from 32.78418
196/196 - 61s - loss: 32.2685 - MinusLogProbMetric: 32.2685 - val_loss: 32.8494 - val_MinusLogProbMetric: 32.8494 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 168/1000
2023-10-31 07:45:06.229 
Epoch 168/1000 
	 loss: 32.2576, MinusLogProbMetric: 32.2576, val_loss: 32.8651, val_MinusLogProbMetric: 32.8651

Epoch 168: val_loss did not improve from 32.78418
196/196 - 57s - loss: 32.2576 - MinusLogProbMetric: 32.2576 - val_loss: 32.8651 - val_MinusLogProbMetric: 32.8651 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 169/1000
2023-10-31 07:46:05.261 
Epoch 169/1000 
	 loss: 32.2370, MinusLogProbMetric: 32.2370, val_loss: 32.7922, val_MinusLogProbMetric: 32.7922

Epoch 169: val_loss did not improve from 32.78418
196/196 - 59s - loss: 32.2370 - MinusLogProbMetric: 32.2370 - val_loss: 32.7922 - val_MinusLogProbMetric: 32.7922 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 170/1000
2023-10-31 07:47:05.279 
Epoch 170/1000 
	 loss: 32.2467, MinusLogProbMetric: 32.2467, val_loss: 32.7476, val_MinusLogProbMetric: 32.7476

Epoch 170: val_loss improved from 32.78418 to 32.74762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 32.2467 - MinusLogProbMetric: 32.2467 - val_loss: 32.7476 - val_MinusLogProbMetric: 32.7476 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 171/1000
2023-10-31 07:48:06.605 
Epoch 171/1000 
	 loss: 32.2398, MinusLogProbMetric: 32.2398, val_loss: 32.7608, val_MinusLogProbMetric: 32.7608

Epoch 171: val_loss did not improve from 32.74762
196/196 - 60s - loss: 32.2398 - MinusLogProbMetric: 32.2398 - val_loss: 32.7608 - val_MinusLogProbMetric: 32.7608 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 172/1000
2023-10-31 07:49:06.429 
Epoch 172/1000 
	 loss: 32.2241, MinusLogProbMetric: 32.2241, val_loss: 32.8132, val_MinusLogProbMetric: 32.8132

Epoch 172: val_loss did not improve from 32.74762
196/196 - 60s - loss: 32.2241 - MinusLogProbMetric: 32.2241 - val_loss: 32.8132 - val_MinusLogProbMetric: 32.8132 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 173/1000
2023-10-31 07:50:07.162 
Epoch 173/1000 
	 loss: 32.2450, MinusLogProbMetric: 32.2450, val_loss: 32.7429, val_MinusLogProbMetric: 32.7429

Epoch 173: val_loss improved from 32.74762 to 32.74290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 32.2450 - MinusLogProbMetric: 32.2450 - val_loss: 32.7429 - val_MinusLogProbMetric: 32.7429 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 174/1000
2023-10-31 07:51:08.313 
Epoch 174/1000 
	 loss: 32.2403, MinusLogProbMetric: 32.2403, val_loss: 32.8319, val_MinusLogProbMetric: 32.8319

Epoch 174: val_loss did not improve from 32.74290
196/196 - 60s - loss: 32.2403 - MinusLogProbMetric: 32.2403 - val_loss: 32.8319 - val_MinusLogProbMetric: 32.8319 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 175/1000
2023-10-31 07:52:05.869 
Epoch 175/1000 
	 loss: 32.2363, MinusLogProbMetric: 32.2363, val_loss: 32.7899, val_MinusLogProbMetric: 32.7899

Epoch 175: val_loss did not improve from 32.74290
196/196 - 58s - loss: 32.2363 - MinusLogProbMetric: 32.2363 - val_loss: 32.7899 - val_MinusLogProbMetric: 32.7899 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 176/1000
2023-10-31 07:53:03.030 
Epoch 176/1000 
	 loss: 32.2228, MinusLogProbMetric: 32.2228, val_loss: 32.7166, val_MinusLogProbMetric: 32.7166

Epoch 176: val_loss improved from 32.74290 to 32.71656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 32.2228 - MinusLogProbMetric: 32.2228 - val_loss: 32.7166 - val_MinusLogProbMetric: 32.7166 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 177/1000
2023-10-31 07:54:00.954 
Epoch 177/1000 
	 loss: 32.2101, MinusLogProbMetric: 32.2101, val_loss: 32.7353, val_MinusLogProbMetric: 32.7353

Epoch 177: val_loss did not improve from 32.71656
196/196 - 57s - loss: 32.2101 - MinusLogProbMetric: 32.2101 - val_loss: 32.7353 - val_MinusLogProbMetric: 32.7353 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 178/1000
2023-10-31 07:54:57.995 
Epoch 178/1000 
	 loss: 32.2291, MinusLogProbMetric: 32.2291, val_loss: 32.7283, val_MinusLogProbMetric: 32.7283

Epoch 178: val_loss did not improve from 32.71656
196/196 - 57s - loss: 32.2291 - MinusLogProbMetric: 32.2291 - val_loss: 32.7283 - val_MinusLogProbMetric: 32.7283 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 179/1000
2023-10-31 07:55:54.725 
Epoch 179/1000 
	 loss: 32.2168, MinusLogProbMetric: 32.2168, val_loss: 32.7351, val_MinusLogProbMetric: 32.7351

Epoch 179: val_loss did not improve from 32.71656
196/196 - 57s - loss: 32.2168 - MinusLogProbMetric: 32.2168 - val_loss: 32.7351 - val_MinusLogProbMetric: 32.7351 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 180/1000
2023-10-31 07:56:53.086 
Epoch 180/1000 
	 loss: 32.2224, MinusLogProbMetric: 32.2224, val_loss: 32.6928, val_MinusLogProbMetric: 32.6928

Epoch 180: val_loss improved from 32.71656 to 32.69285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 32.2224 - MinusLogProbMetric: 32.2224 - val_loss: 32.6928 - val_MinusLogProbMetric: 32.6928 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 181/1000
2023-10-31 07:57:40.905 
Epoch 181/1000 
	 loss: 32.2078, MinusLogProbMetric: 32.2078, val_loss: 32.8160, val_MinusLogProbMetric: 32.8160

Epoch 181: val_loss did not improve from 32.69285
196/196 - 46s - loss: 32.2078 - MinusLogProbMetric: 32.2078 - val_loss: 32.8160 - val_MinusLogProbMetric: 32.8160 - lr: 1.3717e-06 - 46s/epoch - 236ms/step
Epoch 182/1000
2023-10-31 07:58:30.431 
Epoch 182/1000 
	 loss: 32.1974, MinusLogProbMetric: 32.1974, val_loss: 32.7874, val_MinusLogProbMetric: 32.7874

Epoch 182: val_loss did not improve from 32.69285
196/196 - 50s - loss: 32.1974 - MinusLogProbMetric: 32.1974 - val_loss: 32.7874 - val_MinusLogProbMetric: 32.7874 - lr: 1.3717e-06 - 50s/epoch - 253ms/step
Epoch 183/1000
2023-10-31 07:59:21.729 
Epoch 183/1000 
	 loss: 32.2045, MinusLogProbMetric: 32.2045, val_loss: 32.8231, val_MinusLogProbMetric: 32.8231

Epoch 183: val_loss did not improve from 32.69285
196/196 - 51s - loss: 32.2045 - MinusLogProbMetric: 32.2045 - val_loss: 32.8231 - val_MinusLogProbMetric: 32.8231 - lr: 1.3717e-06 - 51s/epoch - 262ms/step
Epoch 184/1000
2023-10-31 08:00:16.920 
Epoch 184/1000 
	 loss: 32.3689, MinusLogProbMetric: 32.3689, val_loss: 32.7178, val_MinusLogProbMetric: 32.7178

Epoch 184: val_loss did not improve from 32.69285
196/196 - 55s - loss: 32.3689 - MinusLogProbMetric: 32.3689 - val_loss: 32.7178 - val_MinusLogProbMetric: 32.7178 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 185/1000
2023-10-31 08:01:11.572 
Epoch 185/1000 
	 loss: 32.2438, MinusLogProbMetric: 32.2438, val_loss: 32.7199, val_MinusLogProbMetric: 32.7199

Epoch 185: val_loss did not improve from 32.69285
196/196 - 55s - loss: 32.2438 - MinusLogProbMetric: 32.2438 - val_loss: 32.7199 - val_MinusLogProbMetric: 32.7199 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 186/1000
2023-10-31 08:02:07.706 
Epoch 186/1000 
	 loss: 32.2589, MinusLogProbMetric: 32.2589, val_loss: 32.7727, val_MinusLogProbMetric: 32.7727

Epoch 186: val_loss did not improve from 32.69285
196/196 - 56s - loss: 32.2589 - MinusLogProbMetric: 32.2589 - val_loss: 32.7727 - val_MinusLogProbMetric: 32.7727 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 187/1000
2023-10-31 08:03:02.716 
Epoch 187/1000 
	 loss: 32.2048, MinusLogProbMetric: 32.2048, val_loss: 32.6747, val_MinusLogProbMetric: 32.6747

Epoch 187: val_loss improved from 32.69285 to 32.67472, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.2048 - MinusLogProbMetric: 32.2048 - val_loss: 32.6747 - val_MinusLogProbMetric: 32.6747 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 188/1000
2023-10-31 08:03:56.812 
Epoch 188/1000 
	 loss: 32.2137, MinusLogProbMetric: 32.2137, val_loss: 32.7090, val_MinusLogProbMetric: 32.7090

Epoch 188: val_loss did not improve from 32.67472
196/196 - 53s - loss: 32.2137 - MinusLogProbMetric: 32.2137 - val_loss: 32.7090 - val_MinusLogProbMetric: 32.7090 - lr: 1.3717e-06 - 53s/epoch - 272ms/step
Epoch 189/1000
2023-10-31 08:04:52.831 
Epoch 189/1000 
	 loss: 32.1913, MinusLogProbMetric: 32.1913, val_loss: 32.7603, val_MinusLogProbMetric: 32.7603

Epoch 189: val_loss did not improve from 32.67472
196/196 - 56s - loss: 32.1913 - MinusLogProbMetric: 32.1913 - val_loss: 32.7603 - val_MinusLogProbMetric: 32.7603 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 190/1000
2023-10-31 08:05:45.356 
Epoch 190/1000 
	 loss: 32.2008, MinusLogProbMetric: 32.2008, val_loss: 32.6715, val_MinusLogProbMetric: 32.6715

Epoch 190: val_loss improved from 32.67472 to 32.67150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 32.2008 - MinusLogProbMetric: 32.2008 - val_loss: 32.6715 - val_MinusLogProbMetric: 32.6715 - lr: 1.3717e-06 - 54s/epoch - 273ms/step
Epoch 191/1000
2023-10-31 08:06:40.140 
Epoch 191/1000 
	 loss: 32.1787, MinusLogProbMetric: 32.1787, val_loss: 32.7162, val_MinusLogProbMetric: 32.7162

Epoch 191: val_loss did not improve from 32.67150
196/196 - 54s - loss: 32.1787 - MinusLogProbMetric: 32.1787 - val_loss: 32.7162 - val_MinusLogProbMetric: 32.7162 - lr: 1.3717e-06 - 54s/epoch - 274ms/step
Epoch 192/1000
2023-10-31 08:07:33.726 
Epoch 192/1000 
	 loss: 32.1846, MinusLogProbMetric: 32.1846, val_loss: 32.6967, val_MinusLogProbMetric: 32.6967

Epoch 192: val_loss did not improve from 32.67150
196/196 - 54s - loss: 32.1846 - MinusLogProbMetric: 32.1846 - val_loss: 32.6967 - val_MinusLogProbMetric: 32.6967 - lr: 1.3717e-06 - 54s/epoch - 273ms/step
Epoch 193/1000
2023-10-31 08:08:28.436 
Epoch 193/1000 
	 loss: 32.1914, MinusLogProbMetric: 32.1914, val_loss: 32.8202, val_MinusLogProbMetric: 32.8202

Epoch 193: val_loss did not improve from 32.67150
196/196 - 55s - loss: 32.1914 - MinusLogProbMetric: 32.1914 - val_loss: 32.8202 - val_MinusLogProbMetric: 32.8202 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 194/1000
2023-10-31 08:09:18.538 
Epoch 194/1000 
	 loss: 32.1938, MinusLogProbMetric: 32.1938, val_loss: 32.6534, val_MinusLogProbMetric: 32.6534

Epoch 194: val_loss improved from 32.67150 to 32.65336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 51s - loss: 32.1938 - MinusLogProbMetric: 32.1938 - val_loss: 32.6534 - val_MinusLogProbMetric: 32.6534 - lr: 1.3717e-06 - 51s/epoch - 260ms/step
Epoch 195/1000
2023-10-31 08:10:12.207 
Epoch 195/1000 
	 loss: 32.1614, MinusLogProbMetric: 32.1614, val_loss: 32.7198, val_MinusLogProbMetric: 32.7198

Epoch 195: val_loss did not improve from 32.65336
196/196 - 53s - loss: 32.1614 - MinusLogProbMetric: 32.1614 - val_loss: 32.7198 - val_MinusLogProbMetric: 32.7198 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 196/1000
2023-10-31 08:11:00.583 
Epoch 196/1000 
	 loss: 32.1652, MinusLogProbMetric: 32.1652, val_loss: 32.6728, val_MinusLogProbMetric: 32.6728

Epoch 196: val_loss did not improve from 32.65336
196/196 - 48s - loss: 32.1652 - MinusLogProbMetric: 32.1652 - val_loss: 32.6728 - val_MinusLogProbMetric: 32.6728 - lr: 1.3717e-06 - 48s/epoch - 247ms/step
Epoch 197/1000
2023-10-31 08:11:50.127 
Epoch 197/1000 
	 loss: 32.1687, MinusLogProbMetric: 32.1687, val_loss: 32.8090, val_MinusLogProbMetric: 32.8090

Epoch 197: val_loss did not improve from 32.65336
196/196 - 50s - loss: 32.1687 - MinusLogProbMetric: 32.1687 - val_loss: 32.8090 - val_MinusLogProbMetric: 32.8090 - lr: 1.3717e-06 - 50s/epoch - 253ms/step
Epoch 198/1000
2023-10-31 08:12:44.365 
Epoch 198/1000 
	 loss: 32.1806, MinusLogProbMetric: 32.1806, val_loss: 32.6873, val_MinusLogProbMetric: 32.6873

Epoch 198: val_loss did not improve from 32.65336
196/196 - 54s - loss: 32.1806 - MinusLogProbMetric: 32.1806 - val_loss: 32.6873 - val_MinusLogProbMetric: 32.6873 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 199/1000
2023-10-31 08:13:39.239 
Epoch 199/1000 
	 loss: 32.1545, MinusLogProbMetric: 32.1545, val_loss: 32.7546, val_MinusLogProbMetric: 32.7546

Epoch 199: val_loss did not improve from 32.65336
196/196 - 55s - loss: 32.1545 - MinusLogProbMetric: 32.1545 - val_loss: 32.7546 - val_MinusLogProbMetric: 32.7546 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 200/1000
2023-10-31 08:14:34.363 
Epoch 200/1000 
	 loss: 32.1678, MinusLogProbMetric: 32.1678, val_loss: 32.6639, val_MinusLogProbMetric: 32.6639

Epoch 200: val_loss did not improve from 32.65336
196/196 - 55s - loss: 32.1678 - MinusLogProbMetric: 32.1678 - val_loss: 32.6639 - val_MinusLogProbMetric: 32.6639 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 201/1000
2023-10-31 08:15:27.151 
Epoch 201/1000 
	 loss: 32.1563, MinusLogProbMetric: 32.1563, val_loss: 32.6918, val_MinusLogProbMetric: 32.6918

Epoch 201: val_loss did not improve from 32.65336
196/196 - 53s - loss: 32.1563 - MinusLogProbMetric: 32.1563 - val_loss: 32.6918 - val_MinusLogProbMetric: 32.6918 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 202/1000
2023-10-31 08:16:20.421 
Epoch 202/1000 
	 loss: 32.1576, MinusLogProbMetric: 32.1576, val_loss: 32.6704, val_MinusLogProbMetric: 32.6704

Epoch 202: val_loss did not improve from 32.65336
196/196 - 53s - loss: 32.1576 - MinusLogProbMetric: 32.1576 - val_loss: 32.6704 - val_MinusLogProbMetric: 32.6704 - lr: 1.3717e-06 - 53s/epoch - 272ms/step
Epoch 203/1000
2023-10-31 08:17:08.688 
Epoch 203/1000 
	 loss: 32.1417, MinusLogProbMetric: 32.1417, val_loss: 32.7302, val_MinusLogProbMetric: 32.7302

Epoch 203: val_loss did not improve from 32.65336
196/196 - 48s - loss: 32.1417 - MinusLogProbMetric: 32.1417 - val_loss: 32.7302 - val_MinusLogProbMetric: 32.7302 - lr: 1.3717e-06 - 48s/epoch - 246ms/step
Epoch 204/1000
2023-10-31 08:18:03.923 
Epoch 204/1000 
	 loss: 32.1425, MinusLogProbMetric: 32.1425, val_loss: 32.6330, val_MinusLogProbMetric: 32.6330

Epoch 204: val_loss improved from 32.65336 to 32.63301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.1425 - MinusLogProbMetric: 32.1425 - val_loss: 32.6330 - val_MinusLogProbMetric: 32.6330 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 205/1000
2023-10-31 08:18:53.083 
Epoch 205/1000 
	 loss: 32.1334, MinusLogProbMetric: 32.1334, val_loss: 32.7260, val_MinusLogProbMetric: 32.7260

Epoch 205: val_loss did not improve from 32.63301
196/196 - 48s - loss: 32.1334 - MinusLogProbMetric: 32.1334 - val_loss: 32.7260 - val_MinusLogProbMetric: 32.7260 - lr: 1.3717e-06 - 48s/epoch - 247ms/step
Epoch 206/1000
2023-10-31 08:19:44.092 
Epoch 206/1000 
	 loss: 32.1313, MinusLogProbMetric: 32.1313, val_loss: 32.6579, val_MinusLogProbMetric: 32.6579

Epoch 206: val_loss did not improve from 32.63301
196/196 - 51s - loss: 32.1313 - MinusLogProbMetric: 32.1313 - val_loss: 32.6579 - val_MinusLogProbMetric: 32.6579 - lr: 1.3717e-06 - 51s/epoch - 260ms/step
Epoch 207/1000
2023-10-31 08:20:35.790 
Epoch 207/1000 
	 loss: 32.1272, MinusLogProbMetric: 32.1272, val_loss: 32.7059, val_MinusLogProbMetric: 32.7059

Epoch 207: val_loss did not improve from 32.63301
196/196 - 52s - loss: 32.1272 - MinusLogProbMetric: 32.1272 - val_loss: 32.7059 - val_MinusLogProbMetric: 32.7059 - lr: 1.3717e-06 - 52s/epoch - 264ms/step
Epoch 208/1000
2023-10-31 08:21:27.475 
Epoch 208/1000 
	 loss: 32.1816, MinusLogProbMetric: 32.1816, val_loss: 32.7219, val_MinusLogProbMetric: 32.7219

Epoch 208: val_loss did not improve from 32.63301
196/196 - 52s - loss: 32.1816 - MinusLogProbMetric: 32.1816 - val_loss: 32.7219 - val_MinusLogProbMetric: 32.7219 - lr: 1.3717e-06 - 52s/epoch - 264ms/step
Epoch 209/1000
2023-10-31 08:22:23.424 
Epoch 209/1000 
	 loss: 32.1489, MinusLogProbMetric: 32.1489, val_loss: 32.6617, val_MinusLogProbMetric: 32.6617

Epoch 209: val_loss did not improve from 32.63301
196/196 - 56s - loss: 32.1489 - MinusLogProbMetric: 32.1489 - val_loss: 32.6617 - val_MinusLogProbMetric: 32.6617 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 210/1000
2023-10-31 08:23:16.282 
Epoch 210/1000 
	 loss: 32.1412, MinusLogProbMetric: 32.1412, val_loss: 32.6601, val_MinusLogProbMetric: 32.6601

Epoch 210: val_loss did not improve from 32.63301
196/196 - 53s - loss: 32.1412 - MinusLogProbMetric: 32.1412 - val_loss: 32.6601 - val_MinusLogProbMetric: 32.6601 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 211/1000
2023-10-31 08:24:07.824 
Epoch 211/1000 
	 loss: 32.1411, MinusLogProbMetric: 32.1411, val_loss: 32.6460, val_MinusLogProbMetric: 32.6460

Epoch 211: val_loss did not improve from 32.63301
196/196 - 52s - loss: 32.1411 - MinusLogProbMetric: 32.1411 - val_loss: 32.6460 - val_MinusLogProbMetric: 32.6460 - lr: 1.3717e-06 - 52s/epoch - 263ms/step
Epoch 212/1000
2023-10-31 08:25:04.024 
Epoch 212/1000 
	 loss: 32.1587, MinusLogProbMetric: 32.1587, val_loss: 32.7512, val_MinusLogProbMetric: 32.7512

Epoch 212: val_loss did not improve from 32.63301
196/196 - 56s - loss: 32.1587 - MinusLogProbMetric: 32.1587 - val_loss: 32.7512 - val_MinusLogProbMetric: 32.7512 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 213/1000
2023-10-31 08:25:59.429 
Epoch 213/1000 
	 loss: 32.1129, MinusLogProbMetric: 32.1129, val_loss: 32.6325, val_MinusLogProbMetric: 32.6325

Epoch 213: val_loss improved from 32.63301 to 32.63248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 32.1129 - MinusLogProbMetric: 32.1129 - val_loss: 32.6325 - val_MinusLogProbMetric: 32.6325 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 214/1000
2023-10-31 08:26:55.613 
Epoch 214/1000 
	 loss: 32.1208, MinusLogProbMetric: 32.1208, val_loss: 32.6679, val_MinusLogProbMetric: 32.6679

Epoch 214: val_loss did not improve from 32.63248
196/196 - 55s - loss: 32.1208 - MinusLogProbMetric: 32.1208 - val_loss: 32.6679 - val_MinusLogProbMetric: 32.6679 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 215/1000
2023-10-31 08:27:50.000 
Epoch 215/1000 
	 loss: 32.1170, MinusLogProbMetric: 32.1170, val_loss: 32.7580, val_MinusLogProbMetric: 32.7580

Epoch 215: val_loss did not improve from 32.63248
196/196 - 54s - loss: 32.1170 - MinusLogProbMetric: 32.1170 - val_loss: 32.7580 - val_MinusLogProbMetric: 32.7580 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 216/1000
2023-10-31 08:28:38.994 
Epoch 216/1000 
	 loss: 32.1091, MinusLogProbMetric: 32.1091, val_loss: 32.6250, val_MinusLogProbMetric: 32.6250

Epoch 216: val_loss improved from 32.63248 to 32.62503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 32.1091 - MinusLogProbMetric: 32.1091 - val_loss: 32.6250 - val_MinusLogProbMetric: 32.6250 - lr: 1.3717e-06 - 50s/epoch - 254ms/step
Epoch 217/1000
2023-10-31 08:29:30.085 
Epoch 217/1000 
	 loss: 32.1292, MinusLogProbMetric: 32.1292, val_loss: 32.6261, val_MinusLogProbMetric: 32.6261

Epoch 217: val_loss did not improve from 32.62503
196/196 - 50s - loss: 32.1292 - MinusLogProbMetric: 32.1292 - val_loss: 32.6261 - val_MinusLogProbMetric: 32.6261 - lr: 1.3717e-06 - 50s/epoch - 257ms/step
Epoch 218/1000
2023-10-31 08:30:22.994 
Epoch 218/1000 
	 loss: 32.1535, MinusLogProbMetric: 32.1535, val_loss: 32.6564, val_MinusLogProbMetric: 32.6564

Epoch 218: val_loss did not improve from 32.62503
196/196 - 53s - loss: 32.1535 - MinusLogProbMetric: 32.1535 - val_loss: 32.6564 - val_MinusLogProbMetric: 32.6564 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 219/1000
2023-10-31 08:31:14.945 
Epoch 219/1000 
	 loss: 32.1112, MinusLogProbMetric: 32.1112, val_loss: 32.6518, val_MinusLogProbMetric: 32.6518

Epoch 219: val_loss did not improve from 32.62503
196/196 - 52s - loss: 32.1112 - MinusLogProbMetric: 32.1112 - val_loss: 32.6518 - val_MinusLogProbMetric: 32.6518 - lr: 1.3717e-06 - 52s/epoch - 265ms/step
Epoch 220/1000
2023-10-31 08:32:06.696 
Epoch 220/1000 
	 loss: 32.1177, MinusLogProbMetric: 32.1177, val_loss: 32.5751, val_MinusLogProbMetric: 32.5751

Epoch 220: val_loss improved from 32.62503 to 32.57506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 32.1177 - MinusLogProbMetric: 32.1177 - val_loss: 32.5751 - val_MinusLogProbMetric: 32.5751 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 221/1000
2023-10-31 08:32:59.308 
Epoch 221/1000 
	 loss: 32.1122, MinusLogProbMetric: 32.1122, val_loss: 32.6230, val_MinusLogProbMetric: 32.6230

Epoch 221: val_loss did not improve from 32.57506
196/196 - 52s - loss: 32.1122 - MinusLogProbMetric: 32.1122 - val_loss: 32.6230 - val_MinusLogProbMetric: 32.6230 - lr: 1.3717e-06 - 52s/epoch - 264ms/step
Epoch 222/1000
2023-10-31 08:33:46.194 
Epoch 222/1000 
	 loss: 32.0971, MinusLogProbMetric: 32.0971, val_loss: 32.6482, val_MinusLogProbMetric: 32.6482

Epoch 222: val_loss did not improve from 32.57506
196/196 - 47s - loss: 32.0971 - MinusLogProbMetric: 32.0971 - val_loss: 32.6482 - val_MinusLogProbMetric: 32.6482 - lr: 1.3717e-06 - 47s/epoch - 239ms/step
Epoch 223/1000
2023-10-31 08:34:35.360 
Epoch 223/1000 
	 loss: 32.1051, MinusLogProbMetric: 32.1051, val_loss: 32.5843, val_MinusLogProbMetric: 32.5843

Epoch 223: val_loss did not improve from 32.57506
196/196 - 49s - loss: 32.1051 - MinusLogProbMetric: 32.1051 - val_loss: 32.5843 - val_MinusLogProbMetric: 32.5843 - lr: 1.3717e-06 - 49s/epoch - 251ms/step
Epoch 224/1000
2023-10-31 08:35:24.476 
Epoch 224/1000 
	 loss: 32.0694, MinusLogProbMetric: 32.0694, val_loss: 32.6324, val_MinusLogProbMetric: 32.6324

Epoch 224: val_loss did not improve from 32.57506
196/196 - 49s - loss: 32.0694 - MinusLogProbMetric: 32.0694 - val_loss: 32.6324 - val_MinusLogProbMetric: 32.6324 - lr: 1.3717e-06 - 49s/epoch - 251ms/step
Epoch 225/1000
2023-10-31 08:36:13.272 
Epoch 225/1000 
	 loss: 32.0894, MinusLogProbMetric: 32.0894, val_loss: 32.6254, val_MinusLogProbMetric: 32.6254

Epoch 225: val_loss did not improve from 32.57506
196/196 - 49s - loss: 32.0894 - MinusLogProbMetric: 32.0894 - val_loss: 32.6254 - val_MinusLogProbMetric: 32.6254 - lr: 1.3717e-06 - 49s/epoch - 249ms/step
Epoch 226/1000
2023-10-31 08:37:07.305 
Epoch 226/1000 
	 loss: 32.1013, MinusLogProbMetric: 32.1013, val_loss: 32.6273, val_MinusLogProbMetric: 32.6273

Epoch 226: val_loss did not improve from 32.57506
196/196 - 54s - loss: 32.1013 - MinusLogProbMetric: 32.1013 - val_loss: 32.6273 - val_MinusLogProbMetric: 32.6273 - lr: 1.3717e-06 - 54s/epoch - 276ms/step
Epoch 227/1000
2023-10-31 08:38:02.757 
Epoch 227/1000 
	 loss: 32.0909, MinusLogProbMetric: 32.0909, val_loss: 32.6600, val_MinusLogProbMetric: 32.6600

Epoch 227: val_loss did not improve from 32.57506
196/196 - 55s - loss: 32.0909 - MinusLogProbMetric: 32.0909 - val_loss: 32.6600 - val_MinusLogProbMetric: 32.6600 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 228/1000
2023-10-31 08:38:51.927 
Epoch 228/1000 
	 loss: 32.0943, MinusLogProbMetric: 32.0943, val_loss: 32.5615, val_MinusLogProbMetric: 32.5615

Epoch 228: val_loss improved from 32.57506 to 32.56152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 50s - loss: 32.0943 - MinusLogProbMetric: 32.0943 - val_loss: 32.5615 - val_MinusLogProbMetric: 32.5615 - lr: 1.3717e-06 - 50s/epoch - 255ms/step
Epoch 229/1000
2023-10-31 08:39:42.887 
Epoch 229/1000 
	 loss: 32.0780, MinusLogProbMetric: 32.0780, val_loss: 32.5994, val_MinusLogProbMetric: 32.5994

Epoch 229: val_loss did not improve from 32.56152
196/196 - 50s - loss: 32.0780 - MinusLogProbMetric: 32.0780 - val_loss: 32.5994 - val_MinusLogProbMetric: 32.5994 - lr: 1.3717e-06 - 50s/epoch - 256ms/step
Epoch 230/1000
2023-10-31 08:40:39.507 
Epoch 230/1000 
	 loss: 32.0708, MinusLogProbMetric: 32.0708, val_loss: 32.6983, val_MinusLogProbMetric: 32.6983

Epoch 230: val_loss did not improve from 32.56152
196/196 - 57s - loss: 32.0708 - MinusLogProbMetric: 32.0708 - val_loss: 32.6983 - val_MinusLogProbMetric: 32.6983 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 231/1000
2023-10-31 08:41:33.899 
Epoch 231/1000 
	 loss: 32.0798, MinusLogProbMetric: 32.0798, val_loss: 32.6006, val_MinusLogProbMetric: 32.6006

Epoch 231: val_loss did not improve from 32.56152
196/196 - 54s - loss: 32.0798 - MinusLogProbMetric: 32.0798 - val_loss: 32.6006 - val_MinusLogProbMetric: 32.6006 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 232/1000
2023-10-31 08:42:28.163 
Epoch 232/1000 
	 loss: 32.0849, MinusLogProbMetric: 32.0849, val_loss: 32.5676, val_MinusLogProbMetric: 32.5676

Epoch 232: val_loss did not improve from 32.56152
196/196 - 54s - loss: 32.0849 - MinusLogProbMetric: 32.0849 - val_loss: 32.5676 - val_MinusLogProbMetric: 32.5676 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 233/1000
2023-10-31 08:43:25.197 
Epoch 233/1000 
	 loss: 32.0545, MinusLogProbMetric: 32.0545, val_loss: 32.6364, val_MinusLogProbMetric: 32.6364

Epoch 233: val_loss did not improve from 32.56152
196/196 - 57s - loss: 32.0545 - MinusLogProbMetric: 32.0545 - val_loss: 32.6364 - val_MinusLogProbMetric: 32.6364 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 234/1000
2023-10-31 08:44:14.553 
Epoch 234/1000 
	 loss: 32.0830, MinusLogProbMetric: 32.0830, val_loss: 32.7154, val_MinusLogProbMetric: 32.7154

Epoch 234: val_loss did not improve from 32.56152
196/196 - 49s - loss: 32.0830 - MinusLogProbMetric: 32.0830 - val_loss: 32.7154 - val_MinusLogProbMetric: 32.7154 - lr: 1.3717e-06 - 49s/epoch - 252ms/step
Epoch 235/1000
2023-10-31 08:45:02.209 
Epoch 235/1000 
	 loss: 32.1007, MinusLogProbMetric: 32.1007, val_loss: 32.6804, val_MinusLogProbMetric: 32.6804

Epoch 235: val_loss did not improve from 32.56152
196/196 - 48s - loss: 32.1007 - MinusLogProbMetric: 32.1007 - val_loss: 32.6804 - val_MinusLogProbMetric: 32.6804 - lr: 1.3717e-06 - 48s/epoch - 243ms/step
Epoch 236/1000
2023-10-31 08:45:56.334 
Epoch 236/1000 
	 loss: 32.0716, MinusLogProbMetric: 32.0716, val_loss: 32.5575, val_MinusLogProbMetric: 32.5575

Epoch 236: val_loss improved from 32.56152 to 32.55754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 32.0716 - MinusLogProbMetric: 32.0716 - val_loss: 32.5575 - val_MinusLogProbMetric: 32.5575 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 237/1000
2023-10-31 08:46:52.741 
Epoch 237/1000 
	 loss: 32.0667, MinusLogProbMetric: 32.0667, val_loss: 32.5803, val_MinusLogProbMetric: 32.5803

Epoch 237: val_loss did not improve from 32.55754
196/196 - 55s - loss: 32.0667 - MinusLogProbMetric: 32.0667 - val_loss: 32.5803 - val_MinusLogProbMetric: 32.5803 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 238/1000
2023-10-31 08:47:43.602 
Epoch 238/1000 
	 loss: 32.0559, MinusLogProbMetric: 32.0559, val_loss: 32.6913, val_MinusLogProbMetric: 32.6913

Epoch 238: val_loss did not improve from 32.55754
196/196 - 51s - loss: 32.0559 - MinusLogProbMetric: 32.0559 - val_loss: 32.6913 - val_MinusLogProbMetric: 32.6913 - lr: 1.3717e-06 - 51s/epoch - 259ms/step
Epoch 239/1000
2023-10-31 08:48:39.594 
Epoch 239/1000 
	 loss: 32.0694, MinusLogProbMetric: 32.0694, val_loss: 32.5669, val_MinusLogProbMetric: 32.5669

Epoch 239: val_loss did not improve from 32.55754
196/196 - 56s - loss: 32.0694 - MinusLogProbMetric: 32.0694 - val_loss: 32.5669 - val_MinusLogProbMetric: 32.5669 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 240/1000
2023-10-31 08:49:37.124 
Epoch 240/1000 
	 loss: 32.0533, MinusLogProbMetric: 32.0533, val_loss: 32.6606, val_MinusLogProbMetric: 32.6606

Epoch 240: val_loss did not improve from 32.55754
196/196 - 58s - loss: 32.0533 - MinusLogProbMetric: 32.0533 - val_loss: 32.6606 - val_MinusLogProbMetric: 32.6606 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 241/1000
2023-10-31 08:50:30.460 
Epoch 241/1000 
	 loss: 32.0418, MinusLogProbMetric: 32.0418, val_loss: 32.5663, val_MinusLogProbMetric: 32.5663

Epoch 241: val_loss did not improve from 32.55754
196/196 - 53s - loss: 32.0418 - MinusLogProbMetric: 32.0418 - val_loss: 32.5663 - val_MinusLogProbMetric: 32.5663 - lr: 1.3717e-06 - 53s/epoch - 272ms/step
Epoch 242/1000
2023-10-31 08:51:26.323 
Epoch 242/1000 
	 loss: 32.0536, MinusLogProbMetric: 32.0536, val_loss: 32.6560, val_MinusLogProbMetric: 32.6560

Epoch 242: val_loss did not improve from 32.55754
196/196 - 56s - loss: 32.0536 - MinusLogProbMetric: 32.0536 - val_loss: 32.6560 - val_MinusLogProbMetric: 32.6560 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 243/1000
2023-10-31 08:52:18.374 
Epoch 243/1000 
	 loss: 32.0424, MinusLogProbMetric: 32.0424, val_loss: 32.6554, val_MinusLogProbMetric: 32.6554

Epoch 243: val_loss did not improve from 32.55754
196/196 - 52s - loss: 32.0424 - MinusLogProbMetric: 32.0424 - val_loss: 32.6554 - val_MinusLogProbMetric: 32.6554 - lr: 1.3717e-06 - 52s/epoch - 266ms/step
Epoch 244/1000
2023-10-31 08:53:06.821 
Epoch 244/1000 
	 loss: 32.0410, MinusLogProbMetric: 32.0410, val_loss: 32.5933, val_MinusLogProbMetric: 32.5933

Epoch 244: val_loss did not improve from 32.55754
196/196 - 48s - loss: 32.0410 - MinusLogProbMetric: 32.0410 - val_loss: 32.5933 - val_MinusLogProbMetric: 32.5933 - lr: 1.3717e-06 - 48s/epoch - 247ms/step
Epoch 245/1000
2023-10-31 08:53:58.423 
Epoch 245/1000 
	 loss: 32.0494, MinusLogProbMetric: 32.0494, val_loss: 32.6589, val_MinusLogProbMetric: 32.6589

Epoch 245: val_loss did not improve from 32.55754
196/196 - 52s - loss: 32.0494 - MinusLogProbMetric: 32.0494 - val_loss: 32.6589 - val_MinusLogProbMetric: 32.6589 - lr: 1.3717e-06 - 52s/epoch - 263ms/step
Epoch 246/1000
2023-10-31 08:54:50.856 
Epoch 246/1000 
	 loss: 32.0500, MinusLogProbMetric: 32.0500, val_loss: 32.5163, val_MinusLogProbMetric: 32.5163

Epoch 246: val_loss improved from 32.55754 to 32.51625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 32.0500 - MinusLogProbMetric: 32.0500 - val_loss: 32.5163 - val_MinusLogProbMetric: 32.5163 - lr: 1.3717e-06 - 53s/epoch - 272ms/step
Epoch 247/1000
2023-10-31 08:55:39.582 
Epoch 247/1000 
	 loss: 32.0498, MinusLogProbMetric: 32.0498, val_loss: 32.6539, val_MinusLogProbMetric: 32.6539

Epoch 247: val_loss did not improve from 32.51625
196/196 - 48s - loss: 32.0498 - MinusLogProbMetric: 32.0498 - val_loss: 32.6539 - val_MinusLogProbMetric: 32.6539 - lr: 1.3717e-06 - 48s/epoch - 244ms/step
Epoch 248/1000
2023-10-31 08:56:32.289 
Epoch 248/1000 
	 loss: 32.0298, MinusLogProbMetric: 32.0298, val_loss: 32.5590, val_MinusLogProbMetric: 32.5590

Epoch 248: val_loss did not improve from 32.51625
196/196 - 53s - loss: 32.0298 - MinusLogProbMetric: 32.0298 - val_loss: 32.5590 - val_MinusLogProbMetric: 32.5590 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 249/1000
2023-10-31 08:57:31.134 
Epoch 249/1000 
	 loss: 32.0427, MinusLogProbMetric: 32.0427, val_loss: 32.5100, val_MinusLogProbMetric: 32.5100

Epoch 249: val_loss improved from 32.51625 to 32.51001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 32.0427 - MinusLogProbMetric: 32.0427 - val_loss: 32.5100 - val_MinusLogProbMetric: 32.5100 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 250/1000
2023-10-31 08:58:22.136 
Epoch 250/1000 
	 loss: 32.0453, MinusLogProbMetric: 32.0453, val_loss: 32.5835, val_MinusLogProbMetric: 32.5835

Epoch 250: val_loss did not improve from 32.51001
196/196 - 50s - loss: 32.0453 - MinusLogProbMetric: 32.0453 - val_loss: 32.5835 - val_MinusLogProbMetric: 32.5835 - lr: 1.3717e-06 - 50s/epoch - 256ms/step
Epoch 251/1000
2023-10-31 08:59:13.534 
Epoch 251/1000 
	 loss: 32.0245, MinusLogProbMetric: 32.0245, val_loss: 32.5142, val_MinusLogProbMetric: 32.5142

Epoch 251: val_loss did not improve from 32.51001
196/196 - 51s - loss: 32.0245 - MinusLogProbMetric: 32.0245 - val_loss: 32.5142 - val_MinusLogProbMetric: 32.5142 - lr: 1.3717e-06 - 51s/epoch - 262ms/step
Epoch 252/1000
2023-10-31 09:00:05.727 
Epoch 252/1000 
	 loss: 32.0230, MinusLogProbMetric: 32.0230, val_loss: 32.6013, val_MinusLogProbMetric: 32.6013

Epoch 252: val_loss did not improve from 32.51001
196/196 - 52s - loss: 32.0230 - MinusLogProbMetric: 32.0230 - val_loss: 32.6013 - val_MinusLogProbMetric: 32.6013 - lr: 1.3717e-06 - 52s/epoch - 266ms/step
Epoch 253/1000
2023-10-31 09:00:59.446 
Epoch 253/1000 
	 loss: 32.0384, MinusLogProbMetric: 32.0384, val_loss: 32.7608, val_MinusLogProbMetric: 32.7608

Epoch 253: val_loss did not improve from 32.51001
196/196 - 54s - loss: 32.0384 - MinusLogProbMetric: 32.0384 - val_loss: 32.7608 - val_MinusLogProbMetric: 32.7608 - lr: 1.3717e-06 - 54s/epoch - 274ms/step
Epoch 254/1000
2023-10-31 09:01:55.483 
Epoch 254/1000 
	 loss: 32.0547, MinusLogProbMetric: 32.0547, val_loss: 32.7632, val_MinusLogProbMetric: 32.7632

Epoch 254: val_loss did not improve from 32.51001
196/196 - 56s - loss: 32.0547 - MinusLogProbMetric: 32.0547 - val_loss: 32.7632 - val_MinusLogProbMetric: 32.7632 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 255/1000
2023-10-31 09:02:52.484 
Epoch 255/1000 
	 loss: 32.0089, MinusLogProbMetric: 32.0089, val_loss: 32.6025, val_MinusLogProbMetric: 32.6025

Epoch 255: val_loss did not improve from 32.51001
196/196 - 57s - loss: 32.0089 - MinusLogProbMetric: 32.0089 - val_loss: 32.6025 - val_MinusLogProbMetric: 32.6025 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 256/1000
2023-10-31 09:03:50.233 
Epoch 256/1000 
	 loss: 32.0754, MinusLogProbMetric: 32.0754, val_loss: 33.4555, val_MinusLogProbMetric: 33.4555

Epoch 256: val_loss did not improve from 32.51001
196/196 - 58s - loss: 32.0754 - MinusLogProbMetric: 32.0754 - val_loss: 33.4555 - val_MinusLogProbMetric: 33.4555 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 257/1000
2023-10-31 09:04:47.220 
Epoch 257/1000 
	 loss: 32.0921, MinusLogProbMetric: 32.0921, val_loss: 32.5973, val_MinusLogProbMetric: 32.5973

Epoch 257: val_loss did not improve from 32.51001
196/196 - 57s - loss: 32.0921 - MinusLogProbMetric: 32.0921 - val_loss: 32.5973 - val_MinusLogProbMetric: 32.5973 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 258/1000
2023-10-31 09:05:41.950 
Epoch 258/1000 
	 loss: 32.0258, MinusLogProbMetric: 32.0258, val_loss: 32.5578, val_MinusLogProbMetric: 32.5578

Epoch 258: val_loss did not improve from 32.51001
196/196 - 55s - loss: 32.0258 - MinusLogProbMetric: 32.0258 - val_loss: 32.5578 - val_MinusLogProbMetric: 32.5578 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 259/1000
2023-10-31 09:06:37.211 
Epoch 259/1000 
	 loss: 32.0204, MinusLogProbMetric: 32.0204, val_loss: 32.6840, val_MinusLogProbMetric: 32.6840

Epoch 259: val_loss did not improve from 32.51001
196/196 - 55s - loss: 32.0204 - MinusLogProbMetric: 32.0204 - val_loss: 32.6840 - val_MinusLogProbMetric: 32.6840 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 260/1000
2023-10-31 09:07:32.995 
Epoch 260/1000 
	 loss: 32.0341, MinusLogProbMetric: 32.0341, val_loss: 32.6790, val_MinusLogProbMetric: 32.6790

Epoch 260: val_loss did not improve from 32.51001
196/196 - 56s - loss: 32.0341 - MinusLogProbMetric: 32.0341 - val_loss: 32.6790 - val_MinusLogProbMetric: 32.6790 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 261/1000
2023-10-31 09:08:31.848 
Epoch 261/1000 
	 loss: 32.0084, MinusLogProbMetric: 32.0084, val_loss: 32.4894, val_MinusLogProbMetric: 32.4894

Epoch 261: val_loss improved from 32.51001 to 32.48937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 32.0084 - MinusLogProbMetric: 32.0084 - val_loss: 32.4894 - val_MinusLogProbMetric: 32.4894 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 262/1000
2023-10-31 09:09:31.423 
Epoch 262/1000 
	 loss: 32.0113, MinusLogProbMetric: 32.0113, val_loss: 32.5443, val_MinusLogProbMetric: 32.5443

Epoch 262: val_loss did not improve from 32.48937
196/196 - 59s - loss: 32.0113 - MinusLogProbMetric: 32.0113 - val_loss: 32.5443 - val_MinusLogProbMetric: 32.5443 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 263/1000
2023-10-31 09:10:29.992 
Epoch 263/1000 
	 loss: 32.0170, MinusLogProbMetric: 32.0170, val_loss: 32.5994, val_MinusLogProbMetric: 32.5994

Epoch 263: val_loss did not improve from 32.48937
196/196 - 59s - loss: 32.0170 - MinusLogProbMetric: 32.0170 - val_loss: 32.5994 - val_MinusLogProbMetric: 32.5994 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 264/1000
2023-10-31 09:11:24.438 
Epoch 264/1000 
	 loss: 32.0083, MinusLogProbMetric: 32.0083, val_loss: 32.4785, val_MinusLogProbMetric: 32.4785

Epoch 264: val_loss improved from 32.48937 to 32.47852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 32.0083 - MinusLogProbMetric: 32.0083 - val_loss: 32.4785 - val_MinusLogProbMetric: 32.4785 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 265/1000
2023-10-31 09:12:21.390 
Epoch 265/1000 
	 loss: 32.0084, MinusLogProbMetric: 32.0084, val_loss: 32.5882, val_MinusLogProbMetric: 32.5882

Epoch 265: val_loss did not improve from 32.47852
196/196 - 56s - loss: 32.0084 - MinusLogProbMetric: 32.0084 - val_loss: 32.5882 - val_MinusLogProbMetric: 32.5882 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 266/1000
2023-10-31 09:13:19.564 
Epoch 266/1000 
	 loss: 31.9989, MinusLogProbMetric: 31.9989, val_loss: 32.5146, val_MinusLogProbMetric: 32.5146

Epoch 266: val_loss did not improve from 32.47852
196/196 - 58s - loss: 31.9989 - MinusLogProbMetric: 31.9989 - val_loss: 32.5146 - val_MinusLogProbMetric: 32.5146 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 267/1000
2023-10-31 09:14:15.728 
Epoch 267/1000 
	 loss: 32.0001, MinusLogProbMetric: 32.0001, val_loss: 32.6370, val_MinusLogProbMetric: 32.6370

Epoch 267: val_loss did not improve from 32.47852
196/196 - 56s - loss: 32.0001 - MinusLogProbMetric: 32.0001 - val_loss: 32.6370 - val_MinusLogProbMetric: 32.6370 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 268/1000
2023-10-31 09:15:15.244 
Epoch 268/1000 
	 loss: 31.9949, MinusLogProbMetric: 31.9949, val_loss: 32.5483, val_MinusLogProbMetric: 32.5483

Epoch 268: val_loss did not improve from 32.47852
196/196 - 60s - loss: 31.9949 - MinusLogProbMetric: 31.9949 - val_loss: 32.5483 - val_MinusLogProbMetric: 32.5483 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 269/1000
2023-10-31 09:16:14.380 
Epoch 269/1000 
	 loss: 32.0186, MinusLogProbMetric: 32.0186, val_loss: 32.5626, val_MinusLogProbMetric: 32.5626

Epoch 269: val_loss did not improve from 32.47852
196/196 - 59s - loss: 32.0186 - MinusLogProbMetric: 32.0186 - val_loss: 32.5626 - val_MinusLogProbMetric: 32.5626 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 270/1000
2023-10-31 09:17:10.948 
Epoch 270/1000 
	 loss: 32.0032, MinusLogProbMetric: 32.0032, val_loss: 32.6460, val_MinusLogProbMetric: 32.6460

Epoch 270: val_loss did not improve from 32.47852
196/196 - 57s - loss: 32.0032 - MinusLogProbMetric: 32.0032 - val_loss: 32.6460 - val_MinusLogProbMetric: 32.6460 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 271/1000
2023-10-31 09:18:09.421 
Epoch 271/1000 
	 loss: 31.9921, MinusLogProbMetric: 31.9921, val_loss: 32.5117, val_MinusLogProbMetric: 32.5117

Epoch 271: val_loss did not improve from 32.47852
196/196 - 58s - loss: 31.9921 - MinusLogProbMetric: 31.9921 - val_loss: 32.5117 - val_MinusLogProbMetric: 32.5117 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 272/1000
2023-10-31 09:19:08.554 
Epoch 272/1000 
	 loss: 31.9745, MinusLogProbMetric: 31.9745, val_loss: 32.4886, val_MinusLogProbMetric: 32.4886

Epoch 272: val_loss did not improve from 32.47852
196/196 - 59s - loss: 31.9745 - MinusLogProbMetric: 31.9745 - val_loss: 32.4886 - val_MinusLogProbMetric: 32.4886 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 273/1000
2023-10-31 09:20:08.437 
Epoch 273/1000 
	 loss: 31.9839, MinusLogProbMetric: 31.9839, val_loss: 32.5719, val_MinusLogProbMetric: 32.5719

Epoch 273: val_loss did not improve from 32.47852
196/196 - 60s - loss: 31.9839 - MinusLogProbMetric: 31.9839 - val_loss: 32.5719 - val_MinusLogProbMetric: 32.5719 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 274/1000
2023-10-31 09:21:08.743 
Epoch 274/1000 
	 loss: 31.9960, MinusLogProbMetric: 31.9960, val_loss: 32.5551, val_MinusLogProbMetric: 32.5551

Epoch 274: val_loss did not improve from 32.47852
196/196 - 60s - loss: 31.9960 - MinusLogProbMetric: 31.9960 - val_loss: 32.5551 - val_MinusLogProbMetric: 32.5551 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 275/1000
2023-10-31 09:22:09.460 
Epoch 275/1000 
	 loss: 31.9672, MinusLogProbMetric: 31.9672, val_loss: 32.6460, val_MinusLogProbMetric: 32.6460

Epoch 275: val_loss did not improve from 32.47852
196/196 - 61s - loss: 31.9672 - MinusLogProbMetric: 31.9672 - val_loss: 32.6460 - val_MinusLogProbMetric: 32.6460 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 276/1000
2023-10-31 09:23:08.723 
Epoch 276/1000 
	 loss: 31.9755, MinusLogProbMetric: 31.9755, val_loss: 32.5949, val_MinusLogProbMetric: 32.5949

Epoch 276: val_loss did not improve from 32.47852
196/196 - 59s - loss: 31.9755 - MinusLogProbMetric: 31.9755 - val_loss: 32.5949 - val_MinusLogProbMetric: 32.5949 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 277/1000
2023-10-31 09:24:10.068 
Epoch 277/1000 
	 loss: 31.9743, MinusLogProbMetric: 31.9743, val_loss: 32.5691, val_MinusLogProbMetric: 32.5691

Epoch 277: val_loss did not improve from 32.47852
196/196 - 61s - loss: 31.9743 - MinusLogProbMetric: 31.9743 - val_loss: 32.5691 - val_MinusLogProbMetric: 32.5691 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 278/1000
2023-10-31 09:25:11.705 
Epoch 278/1000 
	 loss: 31.9824, MinusLogProbMetric: 31.9824, val_loss: 32.5848, val_MinusLogProbMetric: 32.5848

Epoch 278: val_loss did not improve from 32.47852
196/196 - 62s - loss: 31.9824 - MinusLogProbMetric: 31.9824 - val_loss: 32.5848 - val_MinusLogProbMetric: 32.5848 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 279/1000
2023-10-31 09:26:12.817 
Epoch 279/1000 
	 loss: 31.9834, MinusLogProbMetric: 31.9834, val_loss: 32.4606, val_MinusLogProbMetric: 32.4606

Epoch 279: val_loss improved from 32.47852 to 32.46056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 31.9834 - MinusLogProbMetric: 31.9834 - val_loss: 32.4606 - val_MinusLogProbMetric: 32.4606 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 280/1000
2023-10-31 09:27:16.467 
Epoch 280/1000 
	 loss: 31.9771, MinusLogProbMetric: 31.9771, val_loss: 32.5071, val_MinusLogProbMetric: 32.5071

Epoch 280: val_loss did not improve from 32.46056
196/196 - 63s - loss: 31.9771 - MinusLogProbMetric: 31.9771 - val_loss: 32.5071 - val_MinusLogProbMetric: 32.5071 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 281/1000
2023-10-31 09:28:14.747 
Epoch 281/1000 
	 loss: 31.9601, MinusLogProbMetric: 31.9601, val_loss: 32.5461, val_MinusLogProbMetric: 32.5461

Epoch 281: val_loss did not improve from 32.46056
196/196 - 58s - loss: 31.9601 - MinusLogProbMetric: 31.9601 - val_loss: 32.5461 - val_MinusLogProbMetric: 32.5461 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 282/1000
2023-10-31 09:29:16.266 
Epoch 282/1000 
	 loss: 31.9871, MinusLogProbMetric: 31.9871, val_loss: 32.4900, val_MinusLogProbMetric: 32.4900

Epoch 282: val_loss did not improve from 32.46056
196/196 - 62s - loss: 31.9871 - MinusLogProbMetric: 31.9871 - val_loss: 32.4900 - val_MinusLogProbMetric: 32.4900 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 283/1000
2023-10-31 09:30:18.532 
Epoch 283/1000 
	 loss: 31.9583, MinusLogProbMetric: 31.9583, val_loss: 32.5743, val_MinusLogProbMetric: 32.5743

Epoch 283: val_loss did not improve from 32.46056
196/196 - 62s - loss: 31.9583 - MinusLogProbMetric: 31.9583 - val_loss: 32.5743 - val_MinusLogProbMetric: 32.5743 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 284/1000
2023-10-31 09:31:20.334 
Epoch 284/1000 
	 loss: 31.9578, MinusLogProbMetric: 31.9578, val_loss: 32.5592, val_MinusLogProbMetric: 32.5592

Epoch 284: val_loss did not improve from 32.46056
196/196 - 62s - loss: 31.9578 - MinusLogProbMetric: 31.9578 - val_loss: 32.5592 - val_MinusLogProbMetric: 32.5592 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 285/1000
2023-10-31 09:32:22.952 
Epoch 285/1000 
	 loss: 31.9612, MinusLogProbMetric: 31.9612, val_loss: 32.4900, val_MinusLogProbMetric: 32.4900

Epoch 285: val_loss did not improve from 32.46056
196/196 - 63s - loss: 31.9612 - MinusLogProbMetric: 31.9612 - val_loss: 32.4900 - val_MinusLogProbMetric: 32.4900 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 286/1000
2023-10-31 09:33:22.463 
Epoch 286/1000 
	 loss: 31.9623, MinusLogProbMetric: 31.9623, val_loss: 32.4581, val_MinusLogProbMetric: 32.4581

Epoch 286: val_loss improved from 32.46056 to 32.45813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 31.9623 - MinusLogProbMetric: 31.9623 - val_loss: 32.4581 - val_MinusLogProbMetric: 32.4581 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 287/1000
2023-10-31 09:34:20.845 
Epoch 287/1000 
	 loss: 31.9430, MinusLogProbMetric: 31.9430, val_loss: 32.4369, val_MinusLogProbMetric: 32.4369

Epoch 287: val_loss improved from 32.45813 to 32.43689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 31.9430 - MinusLogProbMetric: 31.9430 - val_loss: 32.4369 - val_MinusLogProbMetric: 32.4369 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 288/1000
2023-10-31 09:35:19.190 
Epoch 288/1000 
	 loss: 31.9658, MinusLogProbMetric: 31.9658, val_loss: 32.4578, val_MinusLogProbMetric: 32.4578

Epoch 288: val_loss did not improve from 32.43689
196/196 - 57s - loss: 31.9658 - MinusLogProbMetric: 31.9658 - val_loss: 32.4578 - val_MinusLogProbMetric: 32.4578 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 289/1000
2023-10-31 09:36:17.073 
Epoch 289/1000 
	 loss: 31.9326, MinusLogProbMetric: 31.9326, val_loss: 32.4613, val_MinusLogProbMetric: 32.4613

Epoch 289: val_loss did not improve from 32.43689
196/196 - 58s - loss: 31.9326 - MinusLogProbMetric: 31.9326 - val_loss: 32.4613 - val_MinusLogProbMetric: 32.4613 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 290/1000
2023-10-31 09:37:15.963 
Epoch 290/1000 
	 loss: 31.9345, MinusLogProbMetric: 31.9345, val_loss: 32.5111, val_MinusLogProbMetric: 32.5111

Epoch 290: val_loss did not improve from 32.43689
196/196 - 59s - loss: 31.9345 - MinusLogProbMetric: 31.9345 - val_loss: 32.5111 - val_MinusLogProbMetric: 32.5111 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 291/1000
2023-10-31 09:38:11.980 
Epoch 291/1000 
	 loss: 31.9593, MinusLogProbMetric: 31.9593, val_loss: 32.4692, val_MinusLogProbMetric: 32.4692

Epoch 291: val_loss did not improve from 32.43689
196/196 - 56s - loss: 31.9593 - MinusLogProbMetric: 31.9593 - val_loss: 32.4692 - val_MinusLogProbMetric: 32.4692 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 292/1000
2023-10-31 09:39:08.550 
Epoch 292/1000 
	 loss: 31.9534, MinusLogProbMetric: 31.9534, val_loss: 32.4593, val_MinusLogProbMetric: 32.4593

Epoch 292: val_loss did not improve from 32.43689
196/196 - 57s - loss: 31.9534 - MinusLogProbMetric: 31.9534 - val_loss: 32.4593 - val_MinusLogProbMetric: 32.4593 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 293/1000
2023-10-31 09:40:10.868 
Epoch 293/1000 
	 loss: 31.9338, MinusLogProbMetric: 31.9338, val_loss: 32.5289, val_MinusLogProbMetric: 32.5289

Epoch 293: val_loss did not improve from 32.43689
196/196 - 62s - loss: 31.9338 - MinusLogProbMetric: 31.9338 - val_loss: 32.5289 - val_MinusLogProbMetric: 32.5289 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 294/1000
2023-10-31 09:41:10.473 
Epoch 294/1000 
	 loss: 31.9242, MinusLogProbMetric: 31.9242, val_loss: 32.5793, val_MinusLogProbMetric: 32.5793

Epoch 294: val_loss did not improve from 32.43689
196/196 - 60s - loss: 31.9242 - MinusLogProbMetric: 31.9242 - val_loss: 32.5793 - val_MinusLogProbMetric: 32.5793 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 295/1000
2023-10-31 09:42:08.034 
Epoch 295/1000 
	 loss: 31.9327, MinusLogProbMetric: 31.9327, val_loss: 32.4850, val_MinusLogProbMetric: 32.4850

Epoch 295: val_loss did not improve from 32.43689
196/196 - 58s - loss: 31.9327 - MinusLogProbMetric: 31.9327 - val_loss: 32.4850 - val_MinusLogProbMetric: 32.4850 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 296/1000
2023-10-31 09:43:05.621 
Epoch 296/1000 
	 loss: 31.9282, MinusLogProbMetric: 31.9282, val_loss: 32.4842, val_MinusLogProbMetric: 32.4842

Epoch 296: val_loss did not improve from 32.43689
196/196 - 58s - loss: 31.9282 - MinusLogProbMetric: 31.9282 - val_loss: 32.4842 - val_MinusLogProbMetric: 32.4842 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 297/1000
2023-10-31 09:44:04.613 
Epoch 297/1000 
	 loss: 31.9303, MinusLogProbMetric: 31.9303, val_loss: 32.4944, val_MinusLogProbMetric: 32.4944

Epoch 297: val_loss did not improve from 32.43689
196/196 - 59s - loss: 31.9303 - MinusLogProbMetric: 31.9303 - val_loss: 32.4944 - val_MinusLogProbMetric: 32.4944 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 298/1000
2023-10-31 09:45:02.706 
Epoch 298/1000 
	 loss: 31.9015, MinusLogProbMetric: 31.9015, val_loss: 32.5718, val_MinusLogProbMetric: 32.5718

Epoch 298: val_loss did not improve from 32.43689
196/196 - 58s - loss: 31.9015 - MinusLogProbMetric: 31.9015 - val_loss: 32.5718 - val_MinusLogProbMetric: 32.5718 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 299/1000
2023-10-31 09:45:59.036 
Epoch 299/1000 
	 loss: 31.9633, MinusLogProbMetric: 31.9633, val_loss: 32.4908, val_MinusLogProbMetric: 32.4908

Epoch 299: val_loss did not improve from 32.43689
196/196 - 56s - loss: 31.9633 - MinusLogProbMetric: 31.9633 - val_loss: 32.4908 - val_MinusLogProbMetric: 32.4908 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 300/1000
2023-10-31 09:46:54.718 
Epoch 300/1000 
	 loss: 31.9228, MinusLogProbMetric: 31.9228, val_loss: 32.4686, val_MinusLogProbMetric: 32.4686

Epoch 300: val_loss did not improve from 32.43689
196/196 - 56s - loss: 31.9228 - MinusLogProbMetric: 31.9228 - val_loss: 32.4686 - val_MinusLogProbMetric: 32.4686 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 301/1000
2023-10-31 09:47:51.486 
Epoch 301/1000 
	 loss: 31.9258, MinusLogProbMetric: 31.9258, val_loss: 32.4865, val_MinusLogProbMetric: 32.4865

Epoch 301: val_loss did not improve from 32.43689
196/196 - 57s - loss: 31.9258 - MinusLogProbMetric: 31.9258 - val_loss: 32.4865 - val_MinusLogProbMetric: 32.4865 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 302/1000
2023-10-31 09:48:48.509 
Epoch 302/1000 
	 loss: 31.9604, MinusLogProbMetric: 31.9604, val_loss: 32.4749, val_MinusLogProbMetric: 32.4749

Epoch 302: val_loss did not improve from 32.43689
196/196 - 57s - loss: 31.9604 - MinusLogProbMetric: 31.9604 - val_loss: 32.4749 - val_MinusLogProbMetric: 32.4749 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 303/1000
2023-10-31 09:49:45.053 
Epoch 303/1000 
	 loss: 31.9232, MinusLogProbMetric: 31.9232, val_loss: 32.5040, val_MinusLogProbMetric: 32.5040

Epoch 303: val_loss did not improve from 32.43689
196/196 - 57s - loss: 31.9232 - MinusLogProbMetric: 31.9232 - val_loss: 32.5040 - val_MinusLogProbMetric: 32.5040 - lr: 1.3717e-06 - 57s/epoch - 288ms/step
Epoch 304/1000
2023-10-31 09:50:43.535 
Epoch 304/1000 
	 loss: 31.9102, MinusLogProbMetric: 31.9102, val_loss: 32.4814, val_MinusLogProbMetric: 32.4814

Epoch 304: val_loss did not improve from 32.43689
196/196 - 58s - loss: 31.9102 - MinusLogProbMetric: 31.9102 - val_loss: 32.4814 - val_MinusLogProbMetric: 32.4814 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 305/1000
2023-10-31 09:51:39.325 
Epoch 305/1000 
	 loss: 31.9183, MinusLogProbMetric: 31.9183, val_loss: 32.5044, val_MinusLogProbMetric: 32.5044

Epoch 305: val_loss did not improve from 32.43689
196/196 - 56s - loss: 31.9183 - MinusLogProbMetric: 31.9183 - val_loss: 32.5044 - val_MinusLogProbMetric: 32.5044 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 306/1000
2023-10-31 09:52:32.563 
Epoch 306/1000 
	 loss: 31.9132, MinusLogProbMetric: 31.9132, val_loss: 32.4509, val_MinusLogProbMetric: 32.4509

Epoch 306: val_loss did not improve from 32.43689
196/196 - 53s - loss: 31.9132 - MinusLogProbMetric: 31.9132 - val_loss: 32.4509 - val_MinusLogProbMetric: 32.4509 - lr: 1.3717e-06 - 53s/epoch - 272ms/step
Epoch 307/1000
2023-10-31 09:53:30.528 
Epoch 307/1000 
	 loss: 31.9071, MinusLogProbMetric: 31.9071, val_loss: 32.4455, val_MinusLogProbMetric: 32.4455

Epoch 307: val_loss did not improve from 32.43689
196/196 - 58s - loss: 31.9071 - MinusLogProbMetric: 31.9071 - val_loss: 32.4455 - val_MinusLogProbMetric: 32.4455 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 308/1000
2023-10-31 09:54:24.774 
Epoch 308/1000 
	 loss: 31.9106, MinusLogProbMetric: 31.9106, val_loss: 32.5007, val_MinusLogProbMetric: 32.5007

Epoch 308: val_loss did not improve from 32.43689
196/196 - 54s - loss: 31.9106 - MinusLogProbMetric: 31.9106 - val_loss: 32.5007 - val_MinusLogProbMetric: 32.5007 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 309/1000
2023-10-31 09:55:19.853 
Epoch 309/1000 
	 loss: 31.8971, MinusLogProbMetric: 31.8971, val_loss: 32.5268, val_MinusLogProbMetric: 32.5268

Epoch 309: val_loss did not improve from 32.43689
196/196 - 55s - loss: 31.8971 - MinusLogProbMetric: 31.8971 - val_loss: 32.5268 - val_MinusLogProbMetric: 32.5268 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 310/1000
2023-10-31 09:56:18.396 
Epoch 310/1000 
	 loss: 31.9093, MinusLogProbMetric: 31.9093, val_loss: 32.4254, val_MinusLogProbMetric: 32.4254

Epoch 310: val_loss improved from 32.43689 to 32.42540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 31.9093 - MinusLogProbMetric: 31.9093 - val_loss: 32.4254 - val_MinusLogProbMetric: 32.4254 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 311/1000
2023-10-31 09:57:17.150 
Epoch 311/1000 
	 loss: 31.8928, MinusLogProbMetric: 31.8928, val_loss: 32.5289, val_MinusLogProbMetric: 32.5289

Epoch 311: val_loss did not improve from 32.42540
196/196 - 58s - loss: 31.8928 - MinusLogProbMetric: 31.8928 - val_loss: 32.5289 - val_MinusLogProbMetric: 32.5289 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 312/1000
2023-10-31 09:58:17.284 
Epoch 312/1000 
	 loss: 31.8883, MinusLogProbMetric: 31.8883, val_loss: 32.4530, val_MinusLogProbMetric: 32.4530

Epoch 312: val_loss did not improve from 32.42540
196/196 - 60s - loss: 31.8883 - MinusLogProbMetric: 31.8883 - val_loss: 32.4530 - val_MinusLogProbMetric: 32.4530 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 313/1000
2023-10-31 09:59:18.996 
Epoch 313/1000 
	 loss: 31.8902, MinusLogProbMetric: 31.8902, val_loss: 32.6390, val_MinusLogProbMetric: 32.6390

Epoch 313: val_loss did not improve from 32.42540
196/196 - 62s - loss: 31.8902 - MinusLogProbMetric: 31.8902 - val_loss: 32.6390 - val_MinusLogProbMetric: 32.6390 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 314/1000
2023-10-31 10:00:23.537 
Epoch 314/1000 
	 loss: 31.8974, MinusLogProbMetric: 31.8974, val_loss: 32.5288, val_MinusLogProbMetric: 32.5288

Epoch 314: val_loss did not improve from 32.42540
196/196 - 65s - loss: 31.8974 - MinusLogProbMetric: 31.8974 - val_loss: 32.5288 - val_MinusLogProbMetric: 32.5288 - lr: 1.3717e-06 - 65s/epoch - 329ms/step
Epoch 315/1000
2023-10-31 10:01:29.405 
Epoch 315/1000 
	 loss: 31.9183, MinusLogProbMetric: 31.9183, val_loss: 32.4259, val_MinusLogProbMetric: 32.4259

Epoch 315: val_loss did not improve from 32.42540
196/196 - 66s - loss: 31.9183 - MinusLogProbMetric: 31.9183 - val_loss: 32.4259 - val_MinusLogProbMetric: 32.4259 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 316/1000
2023-10-31 10:02:32.948 
Epoch 316/1000 
	 loss: 31.8734, MinusLogProbMetric: 31.8734, val_loss: 32.4911, val_MinusLogProbMetric: 32.4911

Epoch 316: val_loss did not improve from 32.42540
196/196 - 64s - loss: 31.8734 - MinusLogProbMetric: 31.8734 - val_loss: 32.4911 - val_MinusLogProbMetric: 32.4911 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 317/1000
2023-10-31 10:03:36.195 
Epoch 317/1000 
	 loss: 31.8769, MinusLogProbMetric: 31.8769, val_loss: 32.4897, val_MinusLogProbMetric: 32.4897

Epoch 317: val_loss did not improve from 32.42540
196/196 - 63s - loss: 31.8769 - MinusLogProbMetric: 31.8769 - val_loss: 32.4897 - val_MinusLogProbMetric: 32.4897 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 318/1000
2023-10-31 10:04:37.354 
Epoch 318/1000 
	 loss: 31.8898, MinusLogProbMetric: 31.8898, val_loss: 32.5170, val_MinusLogProbMetric: 32.5170

Epoch 318: val_loss did not improve from 32.42540
196/196 - 61s - loss: 31.8898 - MinusLogProbMetric: 31.8898 - val_loss: 32.5170 - val_MinusLogProbMetric: 32.5170 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 319/1000
2023-10-31 10:05:38.700 
Epoch 319/1000 
	 loss: 31.8785, MinusLogProbMetric: 31.8785, val_loss: 32.5494, val_MinusLogProbMetric: 32.5494

Epoch 319: val_loss did not improve from 32.42540
196/196 - 61s - loss: 31.8785 - MinusLogProbMetric: 31.8785 - val_loss: 32.5494 - val_MinusLogProbMetric: 32.5494 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 320/1000
2023-10-31 10:06:40.333 
Epoch 320/1000 
	 loss: 31.8811, MinusLogProbMetric: 31.8811, val_loss: 32.5676, val_MinusLogProbMetric: 32.5676

Epoch 320: val_loss did not improve from 32.42540
196/196 - 62s - loss: 31.8811 - MinusLogProbMetric: 31.8811 - val_loss: 32.5676 - val_MinusLogProbMetric: 32.5676 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 321/1000
2023-10-31 10:07:43.248 
Epoch 321/1000 
	 loss: 31.8861, MinusLogProbMetric: 31.8861, val_loss: 32.5497, val_MinusLogProbMetric: 32.5497

Epoch 321: val_loss did not improve from 32.42540
196/196 - 63s - loss: 31.8861 - MinusLogProbMetric: 31.8861 - val_loss: 32.5497 - val_MinusLogProbMetric: 32.5497 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 322/1000
2023-10-31 10:08:44.579 
Epoch 322/1000 
	 loss: 31.8721, MinusLogProbMetric: 31.8721, val_loss: 32.4194, val_MinusLogProbMetric: 32.4194

Epoch 322: val_loss improved from 32.42540 to 32.41936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 31.8721 - MinusLogProbMetric: 31.8721 - val_loss: 32.4194 - val_MinusLogProbMetric: 32.4194 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 323/1000
2023-10-31 10:09:49.964 
Epoch 323/1000 
	 loss: 31.8898, MinusLogProbMetric: 31.8898, val_loss: 32.5457, val_MinusLogProbMetric: 32.5457

Epoch 323: val_loss did not improve from 32.41936
196/196 - 64s - loss: 31.8898 - MinusLogProbMetric: 31.8898 - val_loss: 32.5457 - val_MinusLogProbMetric: 32.5457 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 324/1000
2023-10-31 10:10:51.145 
Epoch 324/1000 
	 loss: 31.8752, MinusLogProbMetric: 31.8752, val_loss: 32.3962, val_MinusLogProbMetric: 32.3962

Epoch 324: val_loss improved from 32.41936 to 32.39619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 31.8752 - MinusLogProbMetric: 31.8752 - val_loss: 32.3962 - val_MinusLogProbMetric: 32.3962 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 325/1000
2023-10-31 10:11:54.028 
Epoch 325/1000 
	 loss: 31.8672, MinusLogProbMetric: 31.8672, val_loss: 32.4658, val_MinusLogProbMetric: 32.4658

Epoch 325: val_loss did not improve from 32.39619
196/196 - 62s - loss: 31.8672 - MinusLogProbMetric: 31.8672 - val_loss: 32.4658 - val_MinusLogProbMetric: 32.4658 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 326/1000
2023-10-31 10:12:57.158 
Epoch 326/1000 
	 loss: 31.8676, MinusLogProbMetric: 31.8676, val_loss: 32.3946, val_MinusLogProbMetric: 32.3946

Epoch 326: val_loss improved from 32.39619 to 32.39458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.8676 - MinusLogProbMetric: 31.8676 - val_loss: 32.3946 - val_MinusLogProbMetric: 32.3946 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 327/1000
2023-10-31 10:14:00.317 
Epoch 327/1000 
	 loss: 31.8582, MinusLogProbMetric: 31.8582, val_loss: 32.4735, val_MinusLogProbMetric: 32.4735

Epoch 327: val_loss did not improve from 32.39458
196/196 - 62s - loss: 31.8582 - MinusLogProbMetric: 31.8582 - val_loss: 32.4735 - val_MinusLogProbMetric: 32.4735 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 328/1000
2023-10-31 10:15:02.723 
Epoch 328/1000 
	 loss: 31.8650, MinusLogProbMetric: 31.8650, val_loss: 32.5432, val_MinusLogProbMetric: 32.5432

Epoch 328: val_loss did not improve from 32.39458
196/196 - 62s - loss: 31.8650 - MinusLogProbMetric: 31.8650 - val_loss: 32.5432 - val_MinusLogProbMetric: 32.5432 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 329/1000
2023-10-31 10:16:05.068 
Epoch 329/1000 
	 loss: 31.8626, MinusLogProbMetric: 31.8626, val_loss: 32.4020, val_MinusLogProbMetric: 32.4020

Epoch 329: val_loss did not improve from 32.39458
196/196 - 62s - loss: 31.8626 - MinusLogProbMetric: 31.8626 - val_loss: 32.4020 - val_MinusLogProbMetric: 32.4020 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 330/1000
2023-10-31 10:17:07.571 
Epoch 330/1000 
	 loss: 31.8628, MinusLogProbMetric: 31.8628, val_loss: 32.6399, val_MinusLogProbMetric: 32.6399

Epoch 330: val_loss did not improve from 32.39458
196/196 - 62s - loss: 31.8628 - MinusLogProbMetric: 31.8628 - val_loss: 32.6399 - val_MinusLogProbMetric: 32.6399 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 331/1000
2023-10-31 10:18:09.952 
Epoch 331/1000 
	 loss: 31.8709, MinusLogProbMetric: 31.8709, val_loss: 32.3992, val_MinusLogProbMetric: 32.3992

Epoch 331: val_loss did not improve from 32.39458
196/196 - 62s - loss: 31.8709 - MinusLogProbMetric: 31.8709 - val_loss: 32.3992 - val_MinusLogProbMetric: 32.3992 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 332/1000
2023-10-31 10:19:09.739 
Epoch 332/1000 
	 loss: 31.8765, MinusLogProbMetric: 31.8765, val_loss: 32.3882, val_MinusLogProbMetric: 32.3882

Epoch 332: val_loss improved from 32.39458 to 32.38821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 31.8765 - MinusLogProbMetric: 31.8765 - val_loss: 32.3882 - val_MinusLogProbMetric: 32.3882 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 333/1000
2023-10-31 10:20:12.113 
Epoch 333/1000 
	 loss: 31.8447, MinusLogProbMetric: 31.8447, val_loss: 32.5033, val_MinusLogProbMetric: 32.5033

Epoch 333: val_loss did not improve from 32.38821
196/196 - 61s - loss: 31.8447 - MinusLogProbMetric: 31.8447 - val_loss: 32.5033 - val_MinusLogProbMetric: 32.5033 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 334/1000
2023-10-31 10:21:13.320 
Epoch 334/1000 
	 loss: 31.8799, MinusLogProbMetric: 31.8799, val_loss: 32.3852, val_MinusLogProbMetric: 32.3852

Epoch 334: val_loss improved from 32.38821 to 32.38519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 31.8799 - MinusLogProbMetric: 31.8799 - val_loss: 32.3852 - val_MinusLogProbMetric: 32.3852 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 335/1000
2023-10-31 10:22:14.783 
Epoch 335/1000 
	 loss: 31.8582, MinusLogProbMetric: 31.8582, val_loss: 32.4379, val_MinusLogProbMetric: 32.4379

Epoch 335: val_loss did not improve from 32.38519
196/196 - 60s - loss: 31.8582 - MinusLogProbMetric: 31.8582 - val_loss: 32.4379 - val_MinusLogProbMetric: 32.4379 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 336/1000
2023-10-31 10:23:17.857 
Epoch 336/1000 
	 loss: 31.8582, MinusLogProbMetric: 31.8582, val_loss: 32.4712, val_MinusLogProbMetric: 32.4712

Epoch 336: val_loss did not improve from 32.38519
196/196 - 63s - loss: 31.8582 - MinusLogProbMetric: 31.8582 - val_loss: 32.4712 - val_MinusLogProbMetric: 32.4712 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 337/1000
2023-10-31 10:24:19.449 
Epoch 337/1000 
	 loss: 31.8407, MinusLogProbMetric: 31.8407, val_loss: 32.4530, val_MinusLogProbMetric: 32.4530

Epoch 337: val_loss did not improve from 32.38519
196/196 - 62s - loss: 31.8407 - MinusLogProbMetric: 31.8407 - val_loss: 32.4530 - val_MinusLogProbMetric: 32.4530 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 338/1000
2023-10-31 10:25:21.880 
Epoch 338/1000 
	 loss: 31.8307, MinusLogProbMetric: 31.8307, val_loss: 32.4727, val_MinusLogProbMetric: 32.4727

Epoch 338: val_loss did not improve from 32.38519
196/196 - 62s - loss: 31.8307 - MinusLogProbMetric: 31.8307 - val_loss: 32.4727 - val_MinusLogProbMetric: 32.4727 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 339/1000
2023-10-31 10:26:24.928 
Epoch 339/1000 
	 loss: 31.8409, MinusLogProbMetric: 31.8409, val_loss: 32.3637, val_MinusLogProbMetric: 32.3637

Epoch 339: val_loss improved from 32.38519 to 32.36367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.8409 - MinusLogProbMetric: 31.8409 - val_loss: 32.3637 - val_MinusLogProbMetric: 32.3637 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 340/1000
2023-10-31 10:27:28.109 
Epoch 340/1000 
	 loss: 31.8552, MinusLogProbMetric: 31.8552, val_loss: 32.4319, val_MinusLogProbMetric: 32.4319

Epoch 340: val_loss did not improve from 32.36367
196/196 - 62s - loss: 31.8552 - MinusLogProbMetric: 31.8552 - val_loss: 32.4319 - val_MinusLogProbMetric: 32.4319 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 341/1000
2023-10-31 10:28:30.841 
Epoch 341/1000 
	 loss: 31.8368, MinusLogProbMetric: 31.8368, val_loss: 32.4068, val_MinusLogProbMetric: 32.4068

Epoch 341: val_loss did not improve from 32.36367
196/196 - 63s - loss: 31.8368 - MinusLogProbMetric: 31.8368 - val_loss: 32.4068 - val_MinusLogProbMetric: 32.4068 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 342/1000
2023-10-31 10:29:38.282 
Epoch 342/1000 
	 loss: 31.8402, MinusLogProbMetric: 31.8402, val_loss: 32.4748, val_MinusLogProbMetric: 32.4748

Epoch 342: val_loss did not improve from 32.36367
196/196 - 67s - loss: 31.8402 - MinusLogProbMetric: 31.8402 - val_loss: 32.4748 - val_MinusLogProbMetric: 32.4748 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 343/1000
2023-10-31 10:30:41.339 
Epoch 343/1000 
	 loss: 31.8408, MinusLogProbMetric: 31.8408, val_loss: 32.4174, val_MinusLogProbMetric: 32.4174

Epoch 343: val_loss did not improve from 32.36367
196/196 - 63s - loss: 31.8408 - MinusLogProbMetric: 31.8408 - val_loss: 32.4174 - val_MinusLogProbMetric: 32.4174 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 344/1000
2023-10-31 10:31:45.989 
Epoch 344/1000 
	 loss: 31.8314, MinusLogProbMetric: 31.8314, val_loss: 32.4330, val_MinusLogProbMetric: 32.4330

Epoch 344: val_loss did not improve from 32.36367
196/196 - 65s - loss: 31.8314 - MinusLogProbMetric: 31.8314 - val_loss: 32.4330 - val_MinusLogProbMetric: 32.4330 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 345/1000
2023-10-31 10:32:49.080 
Epoch 345/1000 
	 loss: 31.8324, MinusLogProbMetric: 31.8324, val_loss: 32.3916, val_MinusLogProbMetric: 32.3916

Epoch 345: val_loss did not improve from 32.36367
196/196 - 63s - loss: 31.8324 - MinusLogProbMetric: 31.8324 - val_loss: 32.3916 - val_MinusLogProbMetric: 32.3916 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 346/1000
2023-10-31 10:33:51.523 
Epoch 346/1000 
	 loss: 31.8354, MinusLogProbMetric: 31.8354, val_loss: 32.4628, val_MinusLogProbMetric: 32.4628

Epoch 346: val_loss did not improve from 32.36367
196/196 - 62s - loss: 31.8354 - MinusLogProbMetric: 31.8354 - val_loss: 32.4628 - val_MinusLogProbMetric: 32.4628 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 347/1000
2023-10-31 10:34:53.992 
Epoch 347/1000 
	 loss: 31.8483, MinusLogProbMetric: 31.8483, val_loss: 32.4281, val_MinusLogProbMetric: 32.4281

Epoch 347: val_loss did not improve from 32.36367
196/196 - 62s - loss: 31.8483 - MinusLogProbMetric: 31.8483 - val_loss: 32.4281 - val_MinusLogProbMetric: 32.4281 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 348/1000
2023-10-31 10:35:56.622 
Epoch 348/1000 
	 loss: 31.8320, MinusLogProbMetric: 31.8320, val_loss: 32.4793, val_MinusLogProbMetric: 32.4793

Epoch 348: val_loss did not improve from 32.36367
196/196 - 63s - loss: 31.8320 - MinusLogProbMetric: 31.8320 - val_loss: 32.4793 - val_MinusLogProbMetric: 32.4793 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 349/1000
2023-10-31 10:36:58.936 
Epoch 349/1000 
	 loss: 31.8178, MinusLogProbMetric: 31.8178, val_loss: 32.3382, val_MinusLogProbMetric: 32.3382

Epoch 349: val_loss improved from 32.36367 to 32.33822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.8178 - MinusLogProbMetric: 31.8178 - val_loss: 32.3382 - val_MinusLogProbMetric: 32.3382 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 350/1000
2023-10-31 10:38:03.601 
Epoch 350/1000 
	 loss: 31.8205, MinusLogProbMetric: 31.8205, val_loss: 32.4673, val_MinusLogProbMetric: 32.4673

Epoch 350: val_loss did not improve from 32.33822
196/196 - 63s - loss: 31.8205 - MinusLogProbMetric: 31.8205 - val_loss: 32.4673 - val_MinusLogProbMetric: 32.4673 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 351/1000
2023-10-31 10:39:07.906 
Epoch 351/1000 
	 loss: 31.8155, MinusLogProbMetric: 31.8155, val_loss: 32.3546, val_MinusLogProbMetric: 32.3546

Epoch 351: val_loss did not improve from 32.33822
196/196 - 64s - loss: 31.8155 - MinusLogProbMetric: 31.8155 - val_loss: 32.3546 - val_MinusLogProbMetric: 32.3546 - lr: 1.3717e-06 - 64s/epoch - 328ms/step
Epoch 352/1000
2023-10-31 10:40:09.802 
Epoch 352/1000 
	 loss: 31.8314, MinusLogProbMetric: 31.8314, val_loss: 32.4424, val_MinusLogProbMetric: 32.4424

Epoch 352: val_loss did not improve from 32.33822
196/196 - 62s - loss: 31.8314 - MinusLogProbMetric: 31.8314 - val_loss: 32.4424 - val_MinusLogProbMetric: 32.4424 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 353/1000
2023-10-31 10:41:11.160 
Epoch 353/1000 
	 loss: 31.8319, MinusLogProbMetric: 31.8319, val_loss: 32.3987, val_MinusLogProbMetric: 32.3987

Epoch 353: val_loss did not improve from 32.33822
196/196 - 61s - loss: 31.8319 - MinusLogProbMetric: 31.8319 - val_loss: 32.3987 - val_MinusLogProbMetric: 32.3987 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 354/1000
2023-10-31 10:42:12.797 
Epoch 354/1000 
	 loss: 31.8313, MinusLogProbMetric: 31.8313, val_loss: 32.3756, val_MinusLogProbMetric: 32.3756

Epoch 354: val_loss did not improve from 32.33822
196/196 - 62s - loss: 31.8313 - MinusLogProbMetric: 31.8313 - val_loss: 32.3756 - val_MinusLogProbMetric: 32.3756 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 355/1000
2023-10-31 10:43:12.789 
Epoch 355/1000 
	 loss: 31.8371, MinusLogProbMetric: 31.8371, val_loss: 32.3696, val_MinusLogProbMetric: 32.3696

Epoch 355: val_loss did not improve from 32.33822
196/196 - 60s - loss: 31.8371 - MinusLogProbMetric: 31.8371 - val_loss: 32.3696 - val_MinusLogProbMetric: 32.3696 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 356/1000
2023-10-31 10:44:13.847 
Epoch 356/1000 
	 loss: 31.8098, MinusLogProbMetric: 31.8098, val_loss: 32.3622, val_MinusLogProbMetric: 32.3622

Epoch 356: val_loss did not improve from 32.33822
196/196 - 61s - loss: 31.8098 - MinusLogProbMetric: 31.8098 - val_loss: 32.3622 - val_MinusLogProbMetric: 32.3622 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 357/1000
2023-10-31 10:45:15.055 
Epoch 357/1000 
	 loss: 31.7942, MinusLogProbMetric: 31.7942, val_loss: 32.3900, val_MinusLogProbMetric: 32.3900

Epoch 357: val_loss did not improve from 32.33822
196/196 - 61s - loss: 31.7942 - MinusLogProbMetric: 31.7942 - val_loss: 32.3900 - val_MinusLogProbMetric: 32.3900 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 358/1000
2023-10-31 10:46:14.540 
Epoch 358/1000 
	 loss: 31.8070, MinusLogProbMetric: 31.8070, val_loss: 32.4446, val_MinusLogProbMetric: 32.4446

Epoch 358: val_loss did not improve from 32.33822
196/196 - 59s - loss: 31.8070 - MinusLogProbMetric: 31.8070 - val_loss: 32.4446 - val_MinusLogProbMetric: 32.4446 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 359/1000
2023-10-31 10:47:11.631 
Epoch 359/1000 
	 loss: 31.8145, MinusLogProbMetric: 31.8145, val_loss: 32.4148, val_MinusLogProbMetric: 32.4148

Epoch 359: val_loss did not improve from 32.33822
196/196 - 57s - loss: 31.8145 - MinusLogProbMetric: 31.8145 - val_loss: 32.4148 - val_MinusLogProbMetric: 32.4148 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 360/1000
2023-10-31 10:48:05.989 
Epoch 360/1000 
	 loss: 31.8350, MinusLogProbMetric: 31.8350, val_loss: 32.5528, val_MinusLogProbMetric: 32.5528

Epoch 360: val_loss did not improve from 32.33822
196/196 - 54s - loss: 31.8350 - MinusLogProbMetric: 31.8350 - val_loss: 32.5528 - val_MinusLogProbMetric: 32.5528 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 361/1000
2023-10-31 10:49:01.618 
Epoch 361/1000 
	 loss: 31.8282, MinusLogProbMetric: 31.8282, val_loss: 32.4079, val_MinusLogProbMetric: 32.4079

Epoch 361: val_loss did not improve from 32.33822
196/196 - 56s - loss: 31.8282 - MinusLogProbMetric: 31.8282 - val_loss: 32.4079 - val_MinusLogProbMetric: 32.4079 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 362/1000
2023-10-31 10:50:00.875 
Epoch 362/1000 
	 loss: 31.7986, MinusLogProbMetric: 31.7986, val_loss: 32.3141, val_MinusLogProbMetric: 32.3141

Epoch 362: val_loss improved from 32.33822 to 32.31411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.7986 - MinusLogProbMetric: 31.7986 - val_loss: 32.3141 - val_MinusLogProbMetric: 32.3141 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 363/1000
2023-10-31 10:51:00.249 
Epoch 363/1000 
	 loss: 31.7785, MinusLogProbMetric: 31.7785, val_loss: 32.3921, val_MinusLogProbMetric: 32.3921

Epoch 363: val_loss did not improve from 32.31411
196/196 - 58s - loss: 31.7785 - MinusLogProbMetric: 31.7785 - val_loss: 32.3921 - val_MinusLogProbMetric: 32.3921 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 364/1000
2023-10-31 10:51:57.915 
Epoch 364/1000 
	 loss: 31.7843, MinusLogProbMetric: 31.7843, val_loss: 32.4700, val_MinusLogProbMetric: 32.4700

Epoch 364: val_loss did not improve from 32.31411
196/196 - 58s - loss: 31.7843 - MinusLogProbMetric: 31.7843 - val_loss: 32.4700 - val_MinusLogProbMetric: 32.4700 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 365/1000
2023-10-31 10:53:00.686 
Epoch 365/1000 
	 loss: 31.7934, MinusLogProbMetric: 31.7934, val_loss: 32.3525, val_MinusLogProbMetric: 32.3525

Epoch 365: val_loss did not improve from 32.31411
196/196 - 63s - loss: 31.7934 - MinusLogProbMetric: 31.7934 - val_loss: 32.3525 - val_MinusLogProbMetric: 32.3525 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 366/1000
2023-10-31 10:53:59.858 
Epoch 366/1000 
	 loss: 31.7775, MinusLogProbMetric: 31.7775, val_loss: 32.3526, val_MinusLogProbMetric: 32.3526

Epoch 366: val_loss did not improve from 32.31411
196/196 - 59s - loss: 31.7775 - MinusLogProbMetric: 31.7775 - val_loss: 32.3526 - val_MinusLogProbMetric: 32.3526 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 367/1000
2023-10-31 10:55:02.133 
Epoch 367/1000 
	 loss: 31.7820, MinusLogProbMetric: 31.7820, val_loss: 32.3875, val_MinusLogProbMetric: 32.3875

Epoch 367: val_loss did not improve from 32.31411
196/196 - 62s - loss: 31.7820 - MinusLogProbMetric: 31.7820 - val_loss: 32.3875 - val_MinusLogProbMetric: 32.3875 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 368/1000
2023-10-31 10:56:04.747 
Epoch 368/1000 
	 loss: 31.7886, MinusLogProbMetric: 31.7886, val_loss: 32.3755, val_MinusLogProbMetric: 32.3755

Epoch 368: val_loss did not improve from 32.31411
196/196 - 63s - loss: 31.7886 - MinusLogProbMetric: 31.7886 - val_loss: 32.3755 - val_MinusLogProbMetric: 32.3755 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 369/1000
2023-10-31 10:57:07.853 
Epoch 369/1000 
	 loss: 31.7839, MinusLogProbMetric: 31.7839, val_loss: 32.3700, val_MinusLogProbMetric: 32.3700

Epoch 369: val_loss did not improve from 32.31411
196/196 - 63s - loss: 31.7839 - MinusLogProbMetric: 31.7839 - val_loss: 32.3700 - val_MinusLogProbMetric: 32.3700 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 370/1000
2023-10-31 10:58:10.191 
Epoch 370/1000 
	 loss: 31.7707, MinusLogProbMetric: 31.7707, val_loss: 32.4067, val_MinusLogProbMetric: 32.4067

Epoch 370: val_loss did not improve from 32.31411
196/196 - 62s - loss: 31.7707 - MinusLogProbMetric: 31.7707 - val_loss: 32.4067 - val_MinusLogProbMetric: 32.4067 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 371/1000
2023-10-31 10:59:12.189 
Epoch 371/1000 
	 loss: 31.7949, MinusLogProbMetric: 31.7949, val_loss: 32.3945, val_MinusLogProbMetric: 32.3945

Epoch 371: val_loss did not improve from 32.31411
196/196 - 62s - loss: 31.7949 - MinusLogProbMetric: 31.7949 - val_loss: 32.3945 - val_MinusLogProbMetric: 32.3945 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 372/1000
2023-10-31 11:00:16.696 
Epoch 372/1000 
	 loss: 31.7780, MinusLogProbMetric: 31.7780, val_loss: 32.4694, val_MinusLogProbMetric: 32.4694

Epoch 372: val_loss did not improve from 32.31411
196/196 - 65s - loss: 31.7780 - MinusLogProbMetric: 31.7780 - val_loss: 32.4694 - val_MinusLogProbMetric: 32.4694 - lr: 1.3717e-06 - 65s/epoch - 329ms/step
Epoch 373/1000
2023-10-31 11:01:19.093 
Epoch 373/1000 
	 loss: 31.7684, MinusLogProbMetric: 31.7684, val_loss: 32.2824, val_MinusLogProbMetric: 32.2824

Epoch 373: val_loss improved from 32.31411 to 32.28240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.7684 - MinusLogProbMetric: 31.7684 - val_loss: 32.2824 - val_MinusLogProbMetric: 32.2824 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 374/1000
2023-10-31 11:02:07.836 
Epoch 374/1000 
	 loss: 31.7534, MinusLogProbMetric: 31.7534, val_loss: 32.3340, val_MinusLogProbMetric: 32.3340

Epoch 374: val_loss did not improve from 32.28240
196/196 - 48s - loss: 31.7534 - MinusLogProbMetric: 31.7534 - val_loss: 32.3340 - val_MinusLogProbMetric: 32.3340 - lr: 1.3717e-06 - 48s/epoch - 243ms/step
Epoch 375/1000
2023-10-31 11:02:56.406 
Epoch 375/1000 
	 loss: 31.7544, MinusLogProbMetric: 31.7544, val_loss: 32.3586, val_MinusLogProbMetric: 32.3586

Epoch 375: val_loss did not improve from 32.28240
196/196 - 49s - loss: 31.7544 - MinusLogProbMetric: 31.7544 - val_loss: 32.3586 - val_MinusLogProbMetric: 32.3586 - lr: 1.3717e-06 - 49s/epoch - 248ms/step
Epoch 376/1000
2023-10-31 11:03:52.948 
Epoch 376/1000 
	 loss: 31.7711, MinusLogProbMetric: 31.7711, val_loss: 32.3909, val_MinusLogProbMetric: 32.3909

Epoch 376: val_loss did not improve from 32.28240
196/196 - 57s - loss: 31.7711 - MinusLogProbMetric: 31.7711 - val_loss: 32.3909 - val_MinusLogProbMetric: 32.3909 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 377/1000
2023-10-31 11:04:49.759 
Epoch 377/1000 
	 loss: 31.7613, MinusLogProbMetric: 31.7613, val_loss: 32.4056, val_MinusLogProbMetric: 32.4056

Epoch 377: val_loss did not improve from 32.28240
196/196 - 57s - loss: 31.7613 - MinusLogProbMetric: 31.7613 - val_loss: 32.4056 - val_MinusLogProbMetric: 32.4056 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 378/1000
2023-10-31 11:05:47.521 
Epoch 378/1000 
	 loss: 31.7478, MinusLogProbMetric: 31.7478, val_loss: 32.3312, val_MinusLogProbMetric: 32.3312

Epoch 378: val_loss did not improve from 32.28240
196/196 - 58s - loss: 31.7478 - MinusLogProbMetric: 31.7478 - val_loss: 32.3312 - val_MinusLogProbMetric: 32.3312 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 379/1000
2023-10-31 11:06:49.083 
Epoch 379/1000 
	 loss: 31.7823, MinusLogProbMetric: 31.7823, val_loss: 32.5952, val_MinusLogProbMetric: 32.5952

Epoch 379: val_loss did not improve from 32.28240
196/196 - 62s - loss: 31.7823 - MinusLogProbMetric: 31.7823 - val_loss: 32.5952 - val_MinusLogProbMetric: 32.5952 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 380/1000
2023-10-31 11:07:49.910 
Epoch 380/1000 
	 loss: 31.7543, MinusLogProbMetric: 31.7543, val_loss: 32.4241, val_MinusLogProbMetric: 32.4241

Epoch 380: val_loss did not improve from 32.28240
196/196 - 61s - loss: 31.7543 - MinusLogProbMetric: 31.7543 - val_loss: 32.4241 - val_MinusLogProbMetric: 32.4241 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 381/1000
2023-10-31 11:08:50.024 
Epoch 381/1000 
	 loss: 31.7639, MinusLogProbMetric: 31.7639, val_loss: 32.3651, val_MinusLogProbMetric: 32.3651

Epoch 381: val_loss did not improve from 32.28240
196/196 - 60s - loss: 31.7639 - MinusLogProbMetric: 31.7639 - val_loss: 32.3651 - val_MinusLogProbMetric: 32.3651 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 382/1000
2023-10-31 11:09:47.855 
Epoch 382/1000 
	 loss: 31.7667, MinusLogProbMetric: 31.7667, val_loss: 32.3278, val_MinusLogProbMetric: 32.3278

Epoch 382: val_loss did not improve from 32.28240
196/196 - 58s - loss: 31.7667 - MinusLogProbMetric: 31.7667 - val_loss: 32.3278 - val_MinusLogProbMetric: 32.3278 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 383/1000
2023-10-31 11:10:48.733 
Epoch 383/1000 
	 loss: 31.7597, MinusLogProbMetric: 31.7597, val_loss: 32.3542, val_MinusLogProbMetric: 32.3542

Epoch 383: val_loss did not improve from 32.28240
196/196 - 61s - loss: 31.7597 - MinusLogProbMetric: 31.7597 - val_loss: 32.3542 - val_MinusLogProbMetric: 32.3542 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 384/1000
2023-10-31 11:11:50.378 
Epoch 384/1000 
	 loss: 31.7446, MinusLogProbMetric: 31.7446, val_loss: 32.3154, val_MinusLogProbMetric: 32.3154

Epoch 384: val_loss did not improve from 32.28240
196/196 - 62s - loss: 31.7446 - MinusLogProbMetric: 31.7446 - val_loss: 32.3154 - val_MinusLogProbMetric: 32.3154 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 385/1000
2023-10-31 11:12:49.488 
Epoch 385/1000 
	 loss: 31.7558, MinusLogProbMetric: 31.7558, val_loss: 32.4082, val_MinusLogProbMetric: 32.4082

Epoch 385: val_loss did not improve from 32.28240
196/196 - 59s - loss: 31.7558 - MinusLogProbMetric: 31.7558 - val_loss: 32.4082 - val_MinusLogProbMetric: 32.4082 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 386/1000
2023-10-31 11:13:47.904 
Epoch 386/1000 
	 loss: 31.7469, MinusLogProbMetric: 31.7469, val_loss: 32.3629, val_MinusLogProbMetric: 32.3629

Epoch 386: val_loss did not improve from 32.28240
196/196 - 58s - loss: 31.7469 - MinusLogProbMetric: 31.7469 - val_loss: 32.3629 - val_MinusLogProbMetric: 32.3629 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 387/1000
2023-10-31 11:14:50.364 
Epoch 387/1000 
	 loss: 31.7592, MinusLogProbMetric: 31.7592, val_loss: 32.4181, val_MinusLogProbMetric: 32.4181

Epoch 387: val_loss did not improve from 32.28240
196/196 - 62s - loss: 31.7592 - MinusLogProbMetric: 31.7592 - val_loss: 32.4181 - val_MinusLogProbMetric: 32.4181 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 388/1000
2023-10-31 11:15:50.724 
Epoch 388/1000 
	 loss: 31.7523, MinusLogProbMetric: 31.7523, val_loss: 32.3697, val_MinusLogProbMetric: 32.3697

Epoch 388: val_loss did not improve from 32.28240
196/196 - 60s - loss: 31.7523 - MinusLogProbMetric: 31.7523 - val_loss: 32.3697 - val_MinusLogProbMetric: 32.3697 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 389/1000
2023-10-31 11:16:49.528 
Epoch 389/1000 
	 loss: 31.7421, MinusLogProbMetric: 31.7421, val_loss: 32.3454, val_MinusLogProbMetric: 32.3454

Epoch 389: val_loss did not improve from 32.28240
196/196 - 59s - loss: 31.7421 - MinusLogProbMetric: 31.7421 - val_loss: 32.3454 - val_MinusLogProbMetric: 32.3454 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 390/1000
2023-10-31 11:17:48.484 
Epoch 390/1000 
	 loss: 31.7403, MinusLogProbMetric: 31.7403, val_loss: 32.4445, val_MinusLogProbMetric: 32.4445

Epoch 390: val_loss did not improve from 32.28240
196/196 - 59s - loss: 31.7403 - MinusLogProbMetric: 31.7403 - val_loss: 32.4445 - val_MinusLogProbMetric: 32.4445 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 391/1000
2023-10-31 11:18:47.515 
Epoch 391/1000 
	 loss: 31.7400, MinusLogProbMetric: 31.7400, val_loss: 32.3844, val_MinusLogProbMetric: 32.3844

Epoch 391: val_loss did not improve from 32.28240
196/196 - 59s - loss: 31.7400 - MinusLogProbMetric: 31.7400 - val_loss: 32.3844 - val_MinusLogProbMetric: 32.3844 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 392/1000
2023-10-31 11:19:49.275 
Epoch 392/1000 
	 loss: 31.7304, MinusLogProbMetric: 31.7304, val_loss: 32.3570, val_MinusLogProbMetric: 32.3570

Epoch 392: val_loss did not improve from 32.28240
196/196 - 62s - loss: 31.7304 - MinusLogProbMetric: 31.7304 - val_loss: 32.3570 - val_MinusLogProbMetric: 32.3570 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 393/1000
2023-10-31 11:20:39.597 
Epoch 393/1000 
	 loss: 31.7276, MinusLogProbMetric: 31.7276, val_loss: 32.3176, val_MinusLogProbMetric: 32.3176

Epoch 393: val_loss did not improve from 32.28240
196/196 - 50s - loss: 31.7276 - MinusLogProbMetric: 31.7276 - val_loss: 32.3176 - val_MinusLogProbMetric: 32.3176 - lr: 1.3717e-06 - 50s/epoch - 257ms/step
Epoch 394/1000
2023-10-31 11:21:26.556 
Epoch 394/1000 
	 loss: 31.7317, MinusLogProbMetric: 31.7317, val_loss: 32.2986, val_MinusLogProbMetric: 32.2986

Epoch 394: val_loss did not improve from 32.28240
196/196 - 47s - loss: 31.7317 - MinusLogProbMetric: 31.7317 - val_loss: 32.2986 - val_MinusLogProbMetric: 32.2986 - lr: 1.3717e-06 - 47s/epoch - 240ms/step
Epoch 395/1000
2023-10-31 11:22:16.780 
Epoch 395/1000 
	 loss: 31.7251, MinusLogProbMetric: 31.7251, val_loss: 32.3377, val_MinusLogProbMetric: 32.3377

Epoch 395: val_loss did not improve from 32.28240
196/196 - 50s - loss: 31.7251 - MinusLogProbMetric: 31.7251 - val_loss: 32.3377 - val_MinusLogProbMetric: 32.3377 - lr: 1.3717e-06 - 50s/epoch - 256ms/step
Epoch 396/1000
2023-10-31 11:23:13.555 
Epoch 396/1000 
	 loss: 31.7361, MinusLogProbMetric: 31.7361, val_loss: 32.2646, val_MinusLogProbMetric: 32.2646

Epoch 396: val_loss improved from 32.28240 to 32.26464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 31.7361 - MinusLogProbMetric: 31.7361 - val_loss: 32.2646 - val_MinusLogProbMetric: 32.2646 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 397/1000
2023-10-31 11:24:12.842 
Epoch 397/1000 
	 loss: 31.7141, MinusLogProbMetric: 31.7141, val_loss: 32.3093, val_MinusLogProbMetric: 32.3093

Epoch 397: val_loss did not improve from 32.26464
196/196 - 58s - loss: 31.7141 - MinusLogProbMetric: 31.7141 - val_loss: 32.3093 - val_MinusLogProbMetric: 32.3093 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 398/1000
2023-10-31 11:25:09.556 
Epoch 398/1000 
	 loss: 31.7212, MinusLogProbMetric: 31.7212, val_loss: 32.3333, val_MinusLogProbMetric: 32.3333

Epoch 398: val_loss did not improve from 32.26464
196/196 - 57s - loss: 31.7212 - MinusLogProbMetric: 31.7212 - val_loss: 32.3333 - val_MinusLogProbMetric: 32.3333 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 399/1000
2023-10-31 11:26:08.053 
Epoch 399/1000 
	 loss: 31.7199, MinusLogProbMetric: 31.7199, val_loss: 32.2876, val_MinusLogProbMetric: 32.2876

Epoch 399: val_loss did not improve from 32.26464
196/196 - 58s - loss: 31.7199 - MinusLogProbMetric: 31.7199 - val_loss: 32.2876 - val_MinusLogProbMetric: 32.2876 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 400/1000
2023-10-31 11:27:05.207 
Epoch 400/1000 
	 loss: 31.7168, MinusLogProbMetric: 31.7168, val_loss: 32.4215, val_MinusLogProbMetric: 32.4215

Epoch 400: val_loss did not improve from 32.26464
196/196 - 57s - loss: 31.7168 - MinusLogProbMetric: 31.7168 - val_loss: 32.4215 - val_MinusLogProbMetric: 32.4215 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 401/1000
2023-10-31 11:28:02.601 
Epoch 401/1000 
	 loss: 31.7223, MinusLogProbMetric: 31.7223, val_loss: 32.3045, val_MinusLogProbMetric: 32.3045

Epoch 401: val_loss did not improve from 32.26464
196/196 - 57s - loss: 31.7223 - MinusLogProbMetric: 31.7223 - val_loss: 32.3045 - val_MinusLogProbMetric: 32.3045 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 402/1000
2023-10-31 11:29:05.059 
Epoch 402/1000 
	 loss: 31.7110, MinusLogProbMetric: 31.7110, val_loss: 32.3327, val_MinusLogProbMetric: 32.3327

Epoch 402: val_loss did not improve from 32.26464
196/196 - 62s - loss: 31.7110 - MinusLogProbMetric: 31.7110 - val_loss: 32.3327 - val_MinusLogProbMetric: 32.3327 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 403/1000
2023-10-31 11:30:03.035 
Epoch 403/1000 
	 loss: 31.7109, MinusLogProbMetric: 31.7109, val_loss: 32.4106, val_MinusLogProbMetric: 32.4106

Epoch 403: val_loss did not improve from 32.26464
196/196 - 58s - loss: 31.7109 - MinusLogProbMetric: 31.7109 - val_loss: 32.4106 - val_MinusLogProbMetric: 32.4106 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 404/1000
2023-10-31 11:31:02.770 
Epoch 404/1000 
	 loss: 31.7306, MinusLogProbMetric: 31.7306, val_loss: 32.2767, val_MinusLogProbMetric: 32.2767

Epoch 404: val_loss did not improve from 32.26464
196/196 - 60s - loss: 31.7306 - MinusLogProbMetric: 31.7306 - val_loss: 32.2767 - val_MinusLogProbMetric: 32.2767 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 405/1000
2023-10-31 11:32:01.603 
Epoch 405/1000 
	 loss: 31.7184, MinusLogProbMetric: 31.7184, val_loss: 32.2791, val_MinusLogProbMetric: 32.2791

Epoch 405: val_loss did not improve from 32.26464
196/196 - 59s - loss: 31.7184 - MinusLogProbMetric: 31.7184 - val_loss: 32.2791 - val_MinusLogProbMetric: 32.2791 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 406/1000
2023-10-31 11:33:00.756 
Epoch 406/1000 
	 loss: 31.7045, MinusLogProbMetric: 31.7045, val_loss: 32.2731, val_MinusLogProbMetric: 32.2731

Epoch 406: val_loss did not improve from 32.26464
196/196 - 59s - loss: 31.7045 - MinusLogProbMetric: 31.7045 - val_loss: 32.2731 - val_MinusLogProbMetric: 32.2731 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 407/1000
2023-10-31 11:33:59.846 
Epoch 407/1000 
	 loss: 31.6890, MinusLogProbMetric: 31.6890, val_loss: 32.4077, val_MinusLogProbMetric: 32.4077

Epoch 407: val_loss did not improve from 32.26464
196/196 - 59s - loss: 31.6890 - MinusLogProbMetric: 31.6890 - val_loss: 32.4077 - val_MinusLogProbMetric: 32.4077 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 408/1000
2023-10-31 11:34:58.876 
Epoch 408/1000 
	 loss: 31.7112, MinusLogProbMetric: 31.7112, val_loss: 32.2675, val_MinusLogProbMetric: 32.2675

Epoch 408: val_loss did not improve from 32.26464
196/196 - 59s - loss: 31.7112 - MinusLogProbMetric: 31.7112 - val_loss: 32.2675 - val_MinusLogProbMetric: 32.2675 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 409/1000
2023-10-31 11:35:58.166 
Epoch 409/1000 
	 loss: 31.6927, MinusLogProbMetric: 31.6927, val_loss: 32.2695, val_MinusLogProbMetric: 32.2695

Epoch 409: val_loss did not improve from 32.26464
196/196 - 59s - loss: 31.6927 - MinusLogProbMetric: 31.6927 - val_loss: 32.2695 - val_MinusLogProbMetric: 32.2695 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 410/1000
2023-10-31 11:36:57.502 
Epoch 410/1000 
	 loss: 31.7049, MinusLogProbMetric: 31.7049, val_loss: 32.2604, val_MinusLogProbMetric: 32.2604

Epoch 410: val_loss improved from 32.26464 to 32.26038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 31.7049 - MinusLogProbMetric: 31.7049 - val_loss: 32.2604 - val_MinusLogProbMetric: 32.2604 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 411/1000
2023-10-31 11:37:55.334 
Epoch 411/1000 
	 loss: 31.6979, MinusLogProbMetric: 31.6979, val_loss: 32.2974, val_MinusLogProbMetric: 32.2974

Epoch 411: val_loss did not improve from 32.26038
196/196 - 56s - loss: 31.6979 - MinusLogProbMetric: 31.6979 - val_loss: 32.2974 - val_MinusLogProbMetric: 32.2974 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 412/1000
2023-10-31 11:38:50.899 
Epoch 412/1000 
	 loss: 31.7135, MinusLogProbMetric: 31.7135, val_loss: 32.2810, val_MinusLogProbMetric: 32.2810

Epoch 412: val_loss did not improve from 32.26038
196/196 - 56s - loss: 31.7135 - MinusLogProbMetric: 31.7135 - val_loss: 32.2810 - val_MinusLogProbMetric: 32.2810 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 413/1000
2023-10-31 11:39:48.265 
Epoch 413/1000 
	 loss: 31.7231, MinusLogProbMetric: 31.7231, val_loss: 32.3035, val_MinusLogProbMetric: 32.3035

Epoch 413: val_loss did not improve from 32.26038
196/196 - 57s - loss: 31.7231 - MinusLogProbMetric: 31.7231 - val_loss: 32.3035 - val_MinusLogProbMetric: 32.3035 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 414/1000
2023-10-31 11:40:45.294 
Epoch 414/1000 
	 loss: 31.9100, MinusLogProbMetric: 31.9100, val_loss: 32.2417, val_MinusLogProbMetric: 32.2417

Epoch 414: val_loss improved from 32.26038 to 32.24168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 31.9100 - MinusLogProbMetric: 31.9100 - val_loss: 32.2417 - val_MinusLogProbMetric: 32.2417 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 415/1000
2023-10-31 11:41:44.234 
Epoch 415/1000 
	 loss: 31.7071, MinusLogProbMetric: 31.7071, val_loss: 32.4632, val_MinusLogProbMetric: 32.4632

Epoch 415: val_loss did not improve from 32.24168
196/196 - 58s - loss: 31.7071 - MinusLogProbMetric: 31.7071 - val_loss: 32.4632 - val_MinusLogProbMetric: 32.4632 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 416/1000
2023-10-31 11:42:47.358 
Epoch 416/1000 
	 loss: 31.7269, MinusLogProbMetric: 31.7269, val_loss: 32.4786, val_MinusLogProbMetric: 32.4786

Epoch 416: val_loss did not improve from 32.24168
196/196 - 63s - loss: 31.7269 - MinusLogProbMetric: 31.7269 - val_loss: 32.4786 - val_MinusLogProbMetric: 32.4786 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 417/1000
2023-10-31 11:43:49.792 
Epoch 417/1000 
	 loss: 31.6921, MinusLogProbMetric: 31.6921, val_loss: 32.2231, val_MinusLogProbMetric: 32.2231

Epoch 417: val_loss improved from 32.24168 to 32.22308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.6921 - MinusLogProbMetric: 31.6921 - val_loss: 32.2231 - val_MinusLogProbMetric: 32.2231 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 418/1000
2023-10-31 11:44:51.896 
Epoch 418/1000 
	 loss: 31.6650, MinusLogProbMetric: 31.6650, val_loss: 32.2925, val_MinusLogProbMetric: 32.2925

Epoch 418: val_loss did not improve from 32.22308
196/196 - 60s - loss: 31.6650 - MinusLogProbMetric: 31.6650 - val_loss: 32.2925 - val_MinusLogProbMetric: 32.2925 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 419/1000
2023-10-31 11:45:52.421 
Epoch 419/1000 
	 loss: 31.6735, MinusLogProbMetric: 31.6735, val_loss: 32.3155, val_MinusLogProbMetric: 32.3155

Epoch 419: val_loss did not improve from 32.22308
196/196 - 61s - loss: 31.6735 - MinusLogProbMetric: 31.6735 - val_loss: 32.3155 - val_MinusLogProbMetric: 32.3155 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 420/1000
2023-10-31 11:46:55.889 
Epoch 420/1000 
	 loss: 31.6789, MinusLogProbMetric: 31.6789, val_loss: 32.2576, val_MinusLogProbMetric: 32.2576

Epoch 420: val_loss did not improve from 32.22308
196/196 - 63s - loss: 31.6789 - MinusLogProbMetric: 31.6789 - val_loss: 32.2576 - val_MinusLogProbMetric: 32.2576 - lr: 1.3717e-06 - 63s/epoch - 324ms/step
Epoch 421/1000
2023-10-31 11:47:59.589 
Epoch 421/1000 
	 loss: 31.6703, MinusLogProbMetric: 31.6703, val_loss: 32.2810, val_MinusLogProbMetric: 32.2810

Epoch 421: val_loss did not improve from 32.22308
196/196 - 64s - loss: 31.6703 - MinusLogProbMetric: 31.6703 - val_loss: 32.2810 - val_MinusLogProbMetric: 32.2810 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 422/1000
2023-10-31 11:49:01.388 
Epoch 422/1000 
	 loss: 31.6694, MinusLogProbMetric: 31.6694, val_loss: 32.3275, val_MinusLogProbMetric: 32.3275

Epoch 422: val_loss did not improve from 32.22308
196/196 - 62s - loss: 31.6694 - MinusLogProbMetric: 31.6694 - val_loss: 32.3275 - val_MinusLogProbMetric: 32.3275 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 423/1000
2023-10-31 11:50:04.020 
Epoch 423/1000 
	 loss: 31.7031, MinusLogProbMetric: 31.7031, val_loss: 32.3245, val_MinusLogProbMetric: 32.3245

Epoch 423: val_loss did not improve from 32.22308
196/196 - 63s - loss: 31.7031 - MinusLogProbMetric: 31.7031 - val_loss: 32.3245 - val_MinusLogProbMetric: 32.3245 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 424/1000
2023-10-31 11:51:05.992 
Epoch 424/1000 
	 loss: 31.6619, MinusLogProbMetric: 31.6619, val_loss: 32.2781, val_MinusLogProbMetric: 32.2781

Epoch 424: val_loss did not improve from 32.22308
196/196 - 62s - loss: 31.6619 - MinusLogProbMetric: 31.6619 - val_loss: 32.2781 - val_MinusLogProbMetric: 32.2781 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 425/1000
2023-10-31 11:52:09.252 
Epoch 425/1000 
	 loss: 31.6761, MinusLogProbMetric: 31.6761, val_loss: 32.2660, val_MinusLogProbMetric: 32.2660

Epoch 425: val_loss did not improve from 32.22308
196/196 - 63s - loss: 31.6761 - MinusLogProbMetric: 31.6761 - val_loss: 32.2660 - val_MinusLogProbMetric: 32.2660 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 426/1000
2023-10-31 11:53:13.269 
Epoch 426/1000 
	 loss: 31.6921, MinusLogProbMetric: 31.6921, val_loss: 32.2612, val_MinusLogProbMetric: 32.2612

Epoch 426: val_loss did not improve from 32.22308
196/196 - 64s - loss: 31.6921 - MinusLogProbMetric: 31.6921 - val_loss: 32.2612 - val_MinusLogProbMetric: 32.2612 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 427/1000
2023-10-31 11:54:13.751 
Epoch 427/1000 
	 loss: 31.6882, MinusLogProbMetric: 31.6882, val_loss: 32.2099, val_MinusLogProbMetric: 32.2099

Epoch 427: val_loss improved from 32.22308 to 32.20992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 31.6882 - MinusLogProbMetric: 31.6882 - val_loss: 32.2099 - val_MinusLogProbMetric: 32.2099 - lr: 1.3717e-06 - 61s/epoch - 314ms/step
Epoch 428/1000
2023-10-31 11:55:15.962 
Epoch 428/1000 
	 loss: 31.6885, MinusLogProbMetric: 31.6885, val_loss: 32.2248, val_MinusLogProbMetric: 32.2248

Epoch 428: val_loss did not improve from 32.20992
196/196 - 61s - loss: 31.6885 - MinusLogProbMetric: 31.6885 - val_loss: 32.2248 - val_MinusLogProbMetric: 32.2248 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 429/1000
2023-10-31 11:56:19.811 
Epoch 429/1000 
	 loss: 31.7192, MinusLogProbMetric: 31.7192, val_loss: 32.3258, val_MinusLogProbMetric: 32.3258

Epoch 429: val_loss did not improve from 32.20992
196/196 - 64s - loss: 31.7192 - MinusLogProbMetric: 31.7192 - val_loss: 32.3258 - val_MinusLogProbMetric: 32.3258 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 430/1000
2023-10-31 11:57:20.552 
Epoch 430/1000 
	 loss: 31.6804, MinusLogProbMetric: 31.6804, val_loss: 32.2214, val_MinusLogProbMetric: 32.2214

Epoch 430: val_loss did not improve from 32.20992
196/196 - 61s - loss: 31.6804 - MinusLogProbMetric: 31.6804 - val_loss: 32.2214 - val_MinusLogProbMetric: 32.2214 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 431/1000
2023-10-31 11:58:22.409 
Epoch 431/1000 
	 loss: 31.6580, MinusLogProbMetric: 31.6580, val_loss: 32.2428, val_MinusLogProbMetric: 32.2428

Epoch 431: val_loss did not improve from 32.20992
196/196 - 62s - loss: 31.6580 - MinusLogProbMetric: 31.6580 - val_loss: 32.2428 - val_MinusLogProbMetric: 32.2428 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 432/1000
2023-10-31 11:59:24.039 
Epoch 432/1000 
	 loss: 31.6669, MinusLogProbMetric: 31.6669, val_loss: 32.2693, val_MinusLogProbMetric: 32.2693

Epoch 432: val_loss did not improve from 32.20992
196/196 - 62s - loss: 31.6669 - MinusLogProbMetric: 31.6669 - val_loss: 32.2693 - val_MinusLogProbMetric: 32.2693 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 433/1000
2023-10-31 12:00:27.523 
Epoch 433/1000 
	 loss: 31.6687, MinusLogProbMetric: 31.6687, val_loss: 32.2193, val_MinusLogProbMetric: 32.2193

Epoch 433: val_loss did not improve from 32.20992
196/196 - 63s - loss: 31.6687 - MinusLogProbMetric: 31.6687 - val_loss: 32.2193 - val_MinusLogProbMetric: 32.2193 - lr: 1.3717e-06 - 63s/epoch - 324ms/step
Epoch 434/1000
2023-10-31 12:01:28.708 
Epoch 434/1000 
	 loss: 31.6491, MinusLogProbMetric: 31.6491, val_loss: 32.2938, val_MinusLogProbMetric: 32.2938

Epoch 434: val_loss did not improve from 32.20992
196/196 - 61s - loss: 31.6491 - MinusLogProbMetric: 31.6491 - val_loss: 32.2938 - val_MinusLogProbMetric: 32.2938 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 435/1000
2023-10-31 12:02:33.148 
Epoch 435/1000 
	 loss: 31.6620, MinusLogProbMetric: 31.6620, val_loss: 32.2230, val_MinusLogProbMetric: 32.2230

Epoch 435: val_loss did not improve from 32.20992
196/196 - 64s - loss: 31.6620 - MinusLogProbMetric: 31.6620 - val_loss: 32.2230 - val_MinusLogProbMetric: 32.2230 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 436/1000
2023-10-31 12:03:34.011 
Epoch 436/1000 
	 loss: 31.6428, MinusLogProbMetric: 31.6428, val_loss: 32.1918, val_MinusLogProbMetric: 32.1918

Epoch 436: val_loss improved from 32.20992 to 32.19176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 31.6428 - MinusLogProbMetric: 31.6428 - val_loss: 32.1918 - val_MinusLogProbMetric: 32.1918 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 437/1000
2023-10-31 12:04:32.772 
Epoch 437/1000 
	 loss: 31.6408, MinusLogProbMetric: 31.6408, val_loss: 32.2739, val_MinusLogProbMetric: 32.2739

Epoch 437: val_loss did not improve from 32.19176
196/196 - 58s - loss: 31.6408 - MinusLogProbMetric: 31.6408 - val_loss: 32.2739 - val_MinusLogProbMetric: 32.2739 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 438/1000
2023-10-31 12:05:31.505 
Epoch 438/1000 
	 loss: 31.6427, MinusLogProbMetric: 31.6427, val_loss: 32.2574, val_MinusLogProbMetric: 32.2574

Epoch 438: val_loss did not improve from 32.19176
196/196 - 59s - loss: 31.6427 - MinusLogProbMetric: 31.6427 - val_loss: 32.2574 - val_MinusLogProbMetric: 32.2574 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 439/1000
2023-10-31 12:06:29.627 
Epoch 439/1000 
	 loss: 31.6583, MinusLogProbMetric: 31.6583, val_loss: 32.2532, val_MinusLogProbMetric: 32.2532

Epoch 439: val_loss did not improve from 32.19176
196/196 - 58s - loss: 31.6583 - MinusLogProbMetric: 31.6583 - val_loss: 32.2532 - val_MinusLogProbMetric: 32.2532 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 440/1000
2023-10-31 12:07:26.668 
Epoch 440/1000 
	 loss: 31.6555, MinusLogProbMetric: 31.6555, val_loss: 32.1993, val_MinusLogProbMetric: 32.1993

Epoch 440: val_loss did not improve from 32.19176
196/196 - 57s - loss: 31.6555 - MinusLogProbMetric: 31.6555 - val_loss: 32.1993 - val_MinusLogProbMetric: 32.1993 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 441/1000
2023-10-31 12:08:25.214 
Epoch 441/1000 
	 loss: 31.6480, MinusLogProbMetric: 31.6480, val_loss: 32.2911, val_MinusLogProbMetric: 32.2911

Epoch 441: val_loss did not improve from 32.19176
196/196 - 59s - loss: 31.6480 - MinusLogProbMetric: 31.6480 - val_loss: 32.2911 - val_MinusLogProbMetric: 32.2911 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 442/1000
2023-10-31 12:09:23.828 
Epoch 442/1000 
	 loss: 31.6443, MinusLogProbMetric: 31.6443, val_loss: 32.1819, val_MinusLogProbMetric: 32.1819

Epoch 442: val_loss improved from 32.19176 to 32.18191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.6443 - MinusLogProbMetric: 31.6443 - val_loss: 32.1819 - val_MinusLogProbMetric: 32.1819 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 443/1000
2023-10-31 12:10:25.972 
Epoch 443/1000 
	 loss: 31.6447, MinusLogProbMetric: 31.6447, val_loss: 32.3449, val_MinusLogProbMetric: 32.3449

Epoch 443: val_loss did not improve from 32.18191
196/196 - 61s - loss: 31.6447 - MinusLogProbMetric: 31.6447 - val_loss: 32.3449 - val_MinusLogProbMetric: 32.3449 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 444/1000
2023-10-31 12:11:23.478 
Epoch 444/1000 
	 loss: 31.6978, MinusLogProbMetric: 31.6978, val_loss: 32.4881, val_MinusLogProbMetric: 32.4881

Epoch 444: val_loss did not improve from 32.18191
196/196 - 57s - loss: 31.6978 - MinusLogProbMetric: 31.6978 - val_loss: 32.4881 - val_MinusLogProbMetric: 32.4881 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 445/1000
2023-10-31 12:12:20.340 
Epoch 445/1000 
	 loss: 31.6378, MinusLogProbMetric: 31.6378, val_loss: 32.2282, val_MinusLogProbMetric: 32.2282

Epoch 445: val_loss did not improve from 32.18191
196/196 - 57s - loss: 31.6378 - MinusLogProbMetric: 31.6378 - val_loss: 32.2282 - val_MinusLogProbMetric: 32.2282 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 446/1000
2023-10-31 12:13:17.999 
Epoch 446/1000 
	 loss: 31.6186, MinusLogProbMetric: 31.6186, val_loss: 32.2926, val_MinusLogProbMetric: 32.2926

Epoch 446: val_loss did not improve from 32.18191
196/196 - 58s - loss: 31.6186 - MinusLogProbMetric: 31.6186 - val_loss: 32.2926 - val_MinusLogProbMetric: 32.2926 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 447/1000
2023-10-31 12:14:15.320 
Epoch 447/1000 
	 loss: 31.6393, MinusLogProbMetric: 31.6393, val_loss: 32.2042, val_MinusLogProbMetric: 32.2042

Epoch 447: val_loss did not improve from 32.18191
196/196 - 57s - loss: 31.6393 - MinusLogProbMetric: 31.6393 - val_loss: 32.2042 - val_MinusLogProbMetric: 32.2042 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 448/1000
2023-10-31 12:15:11.419 
Epoch 448/1000 
	 loss: 31.6252, MinusLogProbMetric: 31.6252, val_loss: 32.1789, val_MinusLogProbMetric: 32.1789

Epoch 448: val_loss improved from 32.18191 to 32.17891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 31.6252 - MinusLogProbMetric: 31.6252 - val_loss: 32.1789 - val_MinusLogProbMetric: 32.1789 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 449/1000
2023-10-31 12:16:08.646 
Epoch 449/1000 
	 loss: 31.6173, MinusLogProbMetric: 31.6173, val_loss: 32.1996, val_MinusLogProbMetric: 32.1996

Epoch 449: val_loss did not improve from 32.17891
196/196 - 56s - loss: 31.6173 - MinusLogProbMetric: 31.6173 - val_loss: 32.1996 - val_MinusLogProbMetric: 32.1996 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 450/1000
2023-10-31 12:17:03.407 
Epoch 450/1000 
	 loss: 31.6304, MinusLogProbMetric: 31.6304, val_loss: 32.5382, val_MinusLogProbMetric: 32.5382

Epoch 450: val_loss did not improve from 32.17891
196/196 - 55s - loss: 31.6304 - MinusLogProbMetric: 31.6304 - val_loss: 32.5382 - val_MinusLogProbMetric: 32.5382 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 451/1000
2023-10-31 12:17:59.639 
Epoch 451/1000 
	 loss: 31.6408, MinusLogProbMetric: 31.6408, val_loss: 32.2050, val_MinusLogProbMetric: 32.2050

Epoch 451: val_loss did not improve from 32.17891
196/196 - 56s - loss: 31.6408 - MinusLogProbMetric: 31.6408 - val_loss: 32.2050 - val_MinusLogProbMetric: 32.2050 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 452/1000
2023-10-31 12:18:57.403 
Epoch 452/1000 
	 loss: 31.6259, MinusLogProbMetric: 31.6259, val_loss: 32.2568, val_MinusLogProbMetric: 32.2568

Epoch 452: val_loss did not improve from 32.17891
196/196 - 58s - loss: 31.6259 - MinusLogProbMetric: 31.6259 - val_loss: 32.2568 - val_MinusLogProbMetric: 32.2568 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 453/1000
2023-10-31 12:19:56.787 
Epoch 453/1000 
	 loss: 31.6396, MinusLogProbMetric: 31.6396, val_loss: 32.2639, val_MinusLogProbMetric: 32.2639

Epoch 453: val_loss did not improve from 32.17891
196/196 - 59s - loss: 31.6396 - MinusLogProbMetric: 31.6396 - val_loss: 32.2639 - val_MinusLogProbMetric: 32.2639 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 454/1000
2023-10-31 12:20:55.366 
Epoch 454/1000 
	 loss: 31.6394, MinusLogProbMetric: 31.6394, val_loss: 32.2087, val_MinusLogProbMetric: 32.2087

Epoch 454: val_loss did not improve from 32.17891
196/196 - 59s - loss: 31.6394 - MinusLogProbMetric: 31.6394 - val_loss: 32.2087 - val_MinusLogProbMetric: 32.2087 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 455/1000
2023-10-31 12:21:52.876 
Epoch 455/1000 
	 loss: 31.6315, MinusLogProbMetric: 31.6315, val_loss: 32.2042, val_MinusLogProbMetric: 32.2042

Epoch 455: val_loss did not improve from 32.17891
196/196 - 58s - loss: 31.6315 - MinusLogProbMetric: 31.6315 - val_loss: 32.2042 - val_MinusLogProbMetric: 32.2042 - lr: 1.3717e-06 - 58s/epoch - 293ms/step
Epoch 456/1000
2023-10-31 12:22:49.611 
Epoch 456/1000 
	 loss: 31.6076, MinusLogProbMetric: 31.6076, val_loss: 32.1838, val_MinusLogProbMetric: 32.1838

Epoch 456: val_loss did not improve from 32.17891
196/196 - 57s - loss: 31.6076 - MinusLogProbMetric: 31.6076 - val_loss: 32.1838 - val_MinusLogProbMetric: 32.1838 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 457/1000
2023-10-31 12:23:47.513 
Epoch 457/1000 
	 loss: 31.6127, MinusLogProbMetric: 31.6127, val_loss: 32.1948, val_MinusLogProbMetric: 32.1948

Epoch 457: val_loss did not improve from 32.17891
196/196 - 58s - loss: 31.6127 - MinusLogProbMetric: 31.6127 - val_loss: 32.1948 - val_MinusLogProbMetric: 32.1948 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 458/1000
2023-10-31 12:24:46.568 
Epoch 458/1000 
	 loss: 31.6568, MinusLogProbMetric: 31.6568, val_loss: 32.2299, val_MinusLogProbMetric: 32.2299

Epoch 458: val_loss did not improve from 32.17891
196/196 - 59s - loss: 31.6568 - MinusLogProbMetric: 31.6568 - val_loss: 32.2299 - val_MinusLogProbMetric: 32.2299 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 459/1000
2023-10-31 12:25:43.848 
Epoch 459/1000 
	 loss: 31.6364, MinusLogProbMetric: 31.6364, val_loss: 32.1866, val_MinusLogProbMetric: 32.1866

Epoch 459: val_loss did not improve from 32.17891
196/196 - 57s - loss: 31.6364 - MinusLogProbMetric: 31.6364 - val_loss: 32.1866 - val_MinusLogProbMetric: 32.1866 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 460/1000
2023-10-31 12:26:49.194 
Epoch 460/1000 
	 loss: 31.6372, MinusLogProbMetric: 31.6372, val_loss: 32.1887, val_MinusLogProbMetric: 32.1887

Epoch 460: val_loss did not improve from 32.17891
196/196 - 65s - loss: 31.6372 - MinusLogProbMetric: 31.6372 - val_loss: 32.1887 - val_MinusLogProbMetric: 32.1887 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 461/1000
2023-10-31 12:27:54.335 
Epoch 461/1000 
	 loss: 31.6330, MinusLogProbMetric: 31.6330, val_loss: 32.2247, val_MinusLogProbMetric: 32.2247

Epoch 461: val_loss did not improve from 32.17891
196/196 - 65s - loss: 31.6330 - MinusLogProbMetric: 31.6330 - val_loss: 32.2247 - val_MinusLogProbMetric: 32.2247 - lr: 1.3717e-06 - 65s/epoch - 332ms/step
Epoch 462/1000
2023-10-31 12:28:57.862 
Epoch 462/1000 
	 loss: 31.6212, MinusLogProbMetric: 31.6212, val_loss: 32.2208, val_MinusLogProbMetric: 32.2208

Epoch 462: val_loss did not improve from 32.17891
196/196 - 64s - loss: 31.6212 - MinusLogProbMetric: 31.6212 - val_loss: 32.2208 - val_MinusLogProbMetric: 32.2208 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 463/1000
2023-10-31 12:29:59.990 
Epoch 463/1000 
	 loss: 31.6233, MinusLogProbMetric: 31.6233, val_loss: 32.2676, val_MinusLogProbMetric: 32.2676

Epoch 463: val_loss did not improve from 32.17891
196/196 - 62s - loss: 31.6233 - MinusLogProbMetric: 31.6233 - val_loss: 32.2676 - val_MinusLogProbMetric: 32.2676 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 464/1000
2023-10-31 12:31:02.438 
Epoch 464/1000 
	 loss: 31.6177, MinusLogProbMetric: 31.6177, val_loss: 32.1682, val_MinusLogProbMetric: 32.1682

Epoch 464: val_loss improved from 32.17891 to 32.16821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.6177 - MinusLogProbMetric: 31.6177 - val_loss: 32.1682 - val_MinusLogProbMetric: 32.1682 - lr: 1.3717e-06 - 63s/epoch - 324ms/step
Epoch 465/1000
2023-10-31 12:32:06.485 
Epoch 465/1000 
	 loss: 31.5882, MinusLogProbMetric: 31.5882, val_loss: 32.2212, val_MinusLogProbMetric: 32.2212

Epoch 465: val_loss did not improve from 32.16821
196/196 - 63s - loss: 31.5882 - MinusLogProbMetric: 31.5882 - val_loss: 32.2212 - val_MinusLogProbMetric: 32.2212 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 466/1000
2023-10-31 12:33:05.458 
Epoch 466/1000 
	 loss: 31.5978, MinusLogProbMetric: 31.5978, val_loss: 32.1488, val_MinusLogProbMetric: 32.1488

Epoch 466: val_loss improved from 32.16821 to 32.14883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.5978 - MinusLogProbMetric: 31.5978 - val_loss: 32.1488 - val_MinusLogProbMetric: 32.1488 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 467/1000
2023-10-31 12:34:04.161 
Epoch 467/1000 
	 loss: 31.5987, MinusLogProbMetric: 31.5987, val_loss: 32.1746, val_MinusLogProbMetric: 32.1746

Epoch 467: val_loss did not improve from 32.14883
196/196 - 58s - loss: 31.5987 - MinusLogProbMetric: 31.5987 - val_loss: 32.1746 - val_MinusLogProbMetric: 32.1746 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 468/1000
2023-10-31 12:35:09.906 
Epoch 468/1000 
	 loss: 31.5896, MinusLogProbMetric: 31.5896, val_loss: 32.1635, val_MinusLogProbMetric: 32.1635

Epoch 468: val_loss did not improve from 32.14883
196/196 - 66s - loss: 31.5896 - MinusLogProbMetric: 31.5896 - val_loss: 32.1635 - val_MinusLogProbMetric: 32.1635 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 469/1000
2023-10-31 12:36:17.596 
Epoch 469/1000 
	 loss: 31.5844, MinusLogProbMetric: 31.5844, val_loss: 32.1905, val_MinusLogProbMetric: 32.1905

Epoch 469: val_loss did not improve from 32.14883
196/196 - 68s - loss: 31.5844 - MinusLogProbMetric: 31.5844 - val_loss: 32.1905 - val_MinusLogProbMetric: 32.1905 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 470/1000
2023-10-31 12:37:23.288 
Epoch 470/1000 
	 loss: 31.5986, MinusLogProbMetric: 31.5986, val_loss: 32.2104, val_MinusLogProbMetric: 32.2104

Epoch 470: val_loss did not improve from 32.14883
196/196 - 66s - loss: 31.5986 - MinusLogProbMetric: 31.5986 - val_loss: 32.2104 - val_MinusLogProbMetric: 32.2104 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 471/1000
2023-10-31 12:38:31.324 
Epoch 471/1000 
	 loss: 31.5833, MinusLogProbMetric: 31.5833, val_loss: 32.2004, val_MinusLogProbMetric: 32.2004

Epoch 471: val_loss did not improve from 32.14883
196/196 - 68s - loss: 31.5833 - MinusLogProbMetric: 31.5833 - val_loss: 32.2004 - val_MinusLogProbMetric: 32.2004 - lr: 1.3717e-06 - 68s/epoch - 347ms/step
Epoch 472/1000
2023-10-31 12:39:38.856 
Epoch 472/1000 
	 loss: 31.5851, MinusLogProbMetric: 31.5851, val_loss: 32.2010, val_MinusLogProbMetric: 32.2010

Epoch 472: val_loss did not improve from 32.14883
196/196 - 68s - loss: 31.5851 - MinusLogProbMetric: 31.5851 - val_loss: 32.2010 - val_MinusLogProbMetric: 32.2010 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 473/1000
2023-10-31 12:40:42.886 
Epoch 473/1000 
	 loss: 31.5920, MinusLogProbMetric: 31.5920, val_loss: 32.1845, val_MinusLogProbMetric: 32.1845

Epoch 473: val_loss did not improve from 32.14883
196/196 - 64s - loss: 31.5920 - MinusLogProbMetric: 31.5920 - val_loss: 32.1845 - val_MinusLogProbMetric: 32.1845 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 474/1000
2023-10-31 12:41:45.503 
Epoch 474/1000 
	 loss: 31.5924, MinusLogProbMetric: 31.5924, val_loss: 32.2341, val_MinusLogProbMetric: 32.2341

Epoch 474: val_loss did not improve from 32.14883
196/196 - 63s - loss: 31.5924 - MinusLogProbMetric: 31.5924 - val_loss: 32.2341 - val_MinusLogProbMetric: 32.2341 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 475/1000
2023-10-31 12:42:48.422 
Epoch 475/1000 
	 loss: 31.5857, MinusLogProbMetric: 31.5857, val_loss: 32.2295, val_MinusLogProbMetric: 32.2295

Epoch 475: val_loss did not improve from 32.14883
196/196 - 63s - loss: 31.5857 - MinusLogProbMetric: 31.5857 - val_loss: 32.2295 - val_MinusLogProbMetric: 32.2295 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 476/1000
2023-10-31 12:43:54.219 
Epoch 476/1000 
	 loss: 31.5904, MinusLogProbMetric: 31.5904, val_loss: 32.1911, val_MinusLogProbMetric: 32.1911

Epoch 476: val_loss did not improve from 32.14883
196/196 - 66s - loss: 31.5904 - MinusLogProbMetric: 31.5904 - val_loss: 32.1911 - val_MinusLogProbMetric: 32.1911 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 477/1000
2023-10-31 12:44:56.312 
Epoch 477/1000 
	 loss: 31.5689, MinusLogProbMetric: 31.5689, val_loss: 32.1203, val_MinusLogProbMetric: 32.1203

Epoch 477: val_loss improved from 32.14883 to 32.12027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.5689 - MinusLogProbMetric: 31.5689 - val_loss: 32.1203 - val_MinusLogProbMetric: 32.1203 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 478/1000
2023-10-31 12:45:57.481 
Epoch 478/1000 
	 loss: 31.5719, MinusLogProbMetric: 31.5719, val_loss: 32.1995, val_MinusLogProbMetric: 32.1995

Epoch 478: val_loss did not improve from 32.12027
196/196 - 60s - loss: 31.5719 - MinusLogProbMetric: 31.5719 - val_loss: 32.1995 - val_MinusLogProbMetric: 32.1995 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 479/1000
2023-10-31 12:46:59.652 
Epoch 479/1000 
	 loss: 31.5816, MinusLogProbMetric: 31.5816, val_loss: 32.1775, val_MinusLogProbMetric: 32.1775

Epoch 479: val_loss did not improve from 32.12027
196/196 - 62s - loss: 31.5816 - MinusLogProbMetric: 31.5816 - val_loss: 32.1775 - val_MinusLogProbMetric: 32.1775 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 480/1000
2023-10-31 12:47:59.624 
Epoch 480/1000 
	 loss: 31.6109, MinusLogProbMetric: 31.6109, val_loss: 32.3951, val_MinusLogProbMetric: 32.3951

Epoch 480: val_loss did not improve from 32.12027
196/196 - 60s - loss: 31.6109 - MinusLogProbMetric: 31.6109 - val_loss: 32.3951 - val_MinusLogProbMetric: 32.3951 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 481/1000
2023-10-31 12:48:58.194 
Epoch 481/1000 
	 loss: 31.9874, MinusLogProbMetric: 31.9874, val_loss: 32.2107, val_MinusLogProbMetric: 32.2107

Epoch 481: val_loss did not improve from 32.12027
196/196 - 59s - loss: 31.9874 - MinusLogProbMetric: 31.9874 - val_loss: 32.2107 - val_MinusLogProbMetric: 32.2107 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 482/1000
2023-10-31 12:49:57.054 
Epoch 482/1000 
	 loss: 31.6126, MinusLogProbMetric: 31.6126, val_loss: 32.1365, val_MinusLogProbMetric: 32.1365

Epoch 482: val_loss did not improve from 32.12027
196/196 - 59s - loss: 31.6126 - MinusLogProbMetric: 31.6126 - val_loss: 32.1365 - val_MinusLogProbMetric: 32.1365 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 483/1000
2023-10-31 12:50:57.940 
Epoch 483/1000 
	 loss: 31.5942, MinusLogProbMetric: 31.5942, val_loss: 32.1522, val_MinusLogProbMetric: 32.1522

Epoch 483: val_loss did not improve from 32.12027
196/196 - 61s - loss: 31.5942 - MinusLogProbMetric: 31.5942 - val_loss: 32.1522 - val_MinusLogProbMetric: 32.1522 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 484/1000
2023-10-31 12:51:59.072 
Epoch 484/1000 
	 loss: 31.5716, MinusLogProbMetric: 31.5716, val_loss: 32.1812, val_MinusLogProbMetric: 32.1812

Epoch 484: val_loss did not improve from 32.12027
196/196 - 61s - loss: 31.5716 - MinusLogProbMetric: 31.5716 - val_loss: 32.1812 - val_MinusLogProbMetric: 32.1812 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 485/1000
2023-10-31 12:53:01.013 
Epoch 485/1000 
	 loss: 31.5571, MinusLogProbMetric: 31.5571, val_loss: 32.1685, val_MinusLogProbMetric: 32.1685

Epoch 485: val_loss did not improve from 32.12027
196/196 - 62s - loss: 31.5571 - MinusLogProbMetric: 31.5571 - val_loss: 32.1685 - val_MinusLogProbMetric: 32.1685 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 486/1000
2023-10-31 12:54:04.022 
Epoch 486/1000 
	 loss: 31.5738, MinusLogProbMetric: 31.5738, val_loss: 32.1113, val_MinusLogProbMetric: 32.1113

Epoch 486: val_loss improved from 32.12027 to 32.11129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.5738 - MinusLogProbMetric: 31.5738 - val_loss: 32.1113 - val_MinusLogProbMetric: 32.1113 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 487/1000
2023-10-31 12:55:04.011 
Epoch 487/1000 
	 loss: 31.5774, MinusLogProbMetric: 31.5774, val_loss: 32.1182, val_MinusLogProbMetric: 32.1182

Epoch 487: val_loss did not improve from 32.11129
196/196 - 59s - loss: 31.5774 - MinusLogProbMetric: 31.5774 - val_loss: 32.1182 - val_MinusLogProbMetric: 32.1182 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 488/1000
2023-10-31 12:56:08.762 
Epoch 488/1000 
	 loss: 31.5488, MinusLogProbMetric: 31.5488, val_loss: 32.1243, val_MinusLogProbMetric: 32.1243

Epoch 488: val_loss did not improve from 32.11129
196/196 - 65s - loss: 31.5488 - MinusLogProbMetric: 31.5488 - val_loss: 32.1243 - val_MinusLogProbMetric: 32.1243 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 489/1000
2023-10-31 12:57:11.805 
Epoch 489/1000 
	 loss: 31.5595, MinusLogProbMetric: 31.5595, val_loss: 32.2266, val_MinusLogProbMetric: 32.2266

Epoch 489: val_loss did not improve from 32.11129
196/196 - 63s - loss: 31.5595 - MinusLogProbMetric: 31.5595 - val_loss: 32.2266 - val_MinusLogProbMetric: 32.2266 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 490/1000
2023-10-31 12:58:18.670 
Epoch 490/1000 
	 loss: 31.5522, MinusLogProbMetric: 31.5522, val_loss: 32.1333, val_MinusLogProbMetric: 32.1333

Epoch 490: val_loss did not improve from 32.11129
196/196 - 67s - loss: 31.5522 - MinusLogProbMetric: 31.5522 - val_loss: 32.1333 - val_MinusLogProbMetric: 32.1333 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 491/1000
2023-10-31 12:59:25.642 
Epoch 491/1000 
	 loss: 31.5556, MinusLogProbMetric: 31.5556, val_loss: 32.2865, val_MinusLogProbMetric: 32.2865

Epoch 491: val_loss did not improve from 32.11129
196/196 - 67s - loss: 31.5556 - MinusLogProbMetric: 31.5556 - val_loss: 32.2865 - val_MinusLogProbMetric: 32.2865 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 492/1000
2023-10-31 13:00:30.926 
Epoch 492/1000 
	 loss: 31.5738, MinusLogProbMetric: 31.5738, val_loss: 32.2466, val_MinusLogProbMetric: 32.2466

Epoch 492: val_loss did not improve from 32.11129
196/196 - 65s - loss: 31.5738 - MinusLogProbMetric: 31.5738 - val_loss: 32.2466 - val_MinusLogProbMetric: 32.2466 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 493/1000
2023-10-31 13:01:34.685 
Epoch 493/1000 
	 loss: 31.5511, MinusLogProbMetric: 31.5511, val_loss: 32.1008, val_MinusLogProbMetric: 32.1008

Epoch 493: val_loss improved from 32.11129 to 32.10081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 31.5511 - MinusLogProbMetric: 31.5511 - val_loss: 32.1008 - val_MinusLogProbMetric: 32.1008 - lr: 1.3717e-06 - 65s/epoch - 331ms/step
Epoch 494/1000
2023-10-31 13:02:36.720 
Epoch 494/1000 
	 loss: 31.5416, MinusLogProbMetric: 31.5416, val_loss: 32.1893, val_MinusLogProbMetric: 32.1893

Epoch 494: val_loss did not improve from 32.10081
196/196 - 61s - loss: 31.5416 - MinusLogProbMetric: 31.5416 - val_loss: 32.1893 - val_MinusLogProbMetric: 32.1893 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 495/1000
2023-10-31 13:03:39.784 
Epoch 495/1000 
	 loss: 31.5674, MinusLogProbMetric: 31.5674, val_loss: 32.1682, val_MinusLogProbMetric: 32.1682

Epoch 495: val_loss did not improve from 32.10081
196/196 - 63s - loss: 31.5674 - MinusLogProbMetric: 31.5674 - val_loss: 32.1682 - val_MinusLogProbMetric: 32.1682 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 496/1000
2023-10-31 13:04:42.646 
Epoch 496/1000 
	 loss: 31.5451, MinusLogProbMetric: 31.5451, val_loss: 32.1504, val_MinusLogProbMetric: 32.1504

Epoch 496: val_loss did not improve from 32.10081
196/196 - 63s - loss: 31.5451 - MinusLogProbMetric: 31.5451 - val_loss: 32.1504 - val_MinusLogProbMetric: 32.1504 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 497/1000
2023-10-31 13:05:44.778 
Epoch 497/1000 
	 loss: 31.5533, MinusLogProbMetric: 31.5533, val_loss: 32.1630, val_MinusLogProbMetric: 32.1630

Epoch 497: val_loss did not improve from 32.10081
196/196 - 62s - loss: 31.5533 - MinusLogProbMetric: 31.5533 - val_loss: 32.1630 - val_MinusLogProbMetric: 32.1630 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 498/1000
2023-10-31 13:06:46.822 
Epoch 498/1000 
	 loss: 31.5524, MinusLogProbMetric: 31.5524, val_loss: 32.1140, val_MinusLogProbMetric: 32.1140

Epoch 498: val_loss did not improve from 32.10081
196/196 - 62s - loss: 31.5524 - MinusLogProbMetric: 31.5524 - val_loss: 32.1140 - val_MinusLogProbMetric: 32.1140 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 499/1000
2023-10-31 13:07:49.133 
Epoch 499/1000 
	 loss: 31.5614, MinusLogProbMetric: 31.5614, val_loss: 32.2894, val_MinusLogProbMetric: 32.2894

Epoch 499: val_loss did not improve from 32.10081
196/196 - 62s - loss: 31.5614 - MinusLogProbMetric: 31.5614 - val_loss: 32.2894 - val_MinusLogProbMetric: 32.2894 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 500/1000
2023-10-31 13:08:51.575 
Epoch 500/1000 
	 loss: 31.5922, MinusLogProbMetric: 31.5922, val_loss: 32.1813, val_MinusLogProbMetric: 32.1813

Epoch 500: val_loss did not improve from 32.10081
196/196 - 62s - loss: 31.5922 - MinusLogProbMetric: 31.5922 - val_loss: 32.1813 - val_MinusLogProbMetric: 32.1813 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 501/1000
2023-10-31 13:09:53.643 
Epoch 501/1000 
	 loss: 31.5580, MinusLogProbMetric: 31.5580, val_loss: 32.1910, val_MinusLogProbMetric: 32.1910

Epoch 501: val_loss did not improve from 32.10081
196/196 - 62s - loss: 31.5580 - MinusLogProbMetric: 31.5580 - val_loss: 32.1910 - val_MinusLogProbMetric: 32.1910 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 502/1000
2023-10-31 13:10:57.631 
Epoch 502/1000 
	 loss: 31.5351, MinusLogProbMetric: 31.5351, val_loss: 32.1823, val_MinusLogProbMetric: 32.1823

Epoch 502: val_loss did not improve from 32.10081
196/196 - 64s - loss: 31.5351 - MinusLogProbMetric: 31.5351 - val_loss: 32.1823 - val_MinusLogProbMetric: 32.1823 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 503/1000
2023-10-31 13:12:04.060 
Epoch 503/1000 
	 loss: 31.6148, MinusLogProbMetric: 31.6148, val_loss: 32.3130, val_MinusLogProbMetric: 32.3130

Epoch 503: val_loss did not improve from 32.10081
196/196 - 66s - loss: 31.6148 - MinusLogProbMetric: 31.6148 - val_loss: 32.3130 - val_MinusLogProbMetric: 32.3130 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 504/1000
2023-10-31 13:13:07.188 
Epoch 504/1000 
	 loss: 31.5445, MinusLogProbMetric: 31.5445, val_loss: 32.1478, val_MinusLogProbMetric: 32.1478

Epoch 504: val_loss did not improve from 32.10081
196/196 - 63s - loss: 31.5445 - MinusLogProbMetric: 31.5445 - val_loss: 32.1478 - val_MinusLogProbMetric: 32.1478 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 505/1000
2023-10-31 13:14:06.243 
Epoch 505/1000 
	 loss: 31.5559, MinusLogProbMetric: 31.5559, val_loss: 32.0893, val_MinusLogProbMetric: 32.0893

Epoch 505: val_loss improved from 32.10081 to 32.08926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.5559 - MinusLogProbMetric: 31.5559 - val_loss: 32.0893 - val_MinusLogProbMetric: 32.0893 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 506/1000
2023-10-31 13:15:14.428 
Epoch 506/1000 
	 loss: 31.5455, MinusLogProbMetric: 31.5455, val_loss: 32.1452, val_MinusLogProbMetric: 32.1452

Epoch 506: val_loss did not improve from 32.08926
196/196 - 67s - loss: 31.5455 - MinusLogProbMetric: 31.5455 - val_loss: 32.1452 - val_MinusLogProbMetric: 32.1452 - lr: 1.3717e-06 - 67s/epoch - 343ms/step
Epoch 507/1000
2023-10-31 13:16:20.829 
Epoch 507/1000 
	 loss: 31.5333, MinusLogProbMetric: 31.5333, val_loss: 32.1397, val_MinusLogProbMetric: 32.1397

Epoch 507: val_loss did not improve from 32.08926
196/196 - 66s - loss: 31.5333 - MinusLogProbMetric: 31.5333 - val_loss: 32.1397 - val_MinusLogProbMetric: 32.1397 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 508/1000
2023-10-31 13:17:27.899 
Epoch 508/1000 
	 loss: 31.5255, MinusLogProbMetric: 31.5255, val_loss: 32.1156, val_MinusLogProbMetric: 32.1156

Epoch 508: val_loss did not improve from 32.08926
196/196 - 67s - loss: 31.5255 - MinusLogProbMetric: 31.5255 - val_loss: 32.1156 - val_MinusLogProbMetric: 32.1156 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 509/1000
2023-10-31 13:18:30.356 
Epoch 509/1000 
	 loss: 31.5492, MinusLogProbMetric: 31.5492, val_loss: 32.1255, val_MinusLogProbMetric: 32.1255

Epoch 509: val_loss did not improve from 32.08926
196/196 - 62s - loss: 31.5492 - MinusLogProbMetric: 31.5492 - val_loss: 32.1255 - val_MinusLogProbMetric: 32.1255 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 510/1000
2023-10-31 13:19:31.914 
Epoch 510/1000 
	 loss: 31.5204, MinusLogProbMetric: 31.5204, val_loss: 32.0808, val_MinusLogProbMetric: 32.0808

Epoch 510: val_loss improved from 32.08926 to 32.08080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.5204 - MinusLogProbMetric: 31.5204 - val_loss: 32.0808 - val_MinusLogProbMetric: 32.0808 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 511/1000
2023-10-31 13:20:32.565 
Epoch 511/1000 
	 loss: 31.5547, MinusLogProbMetric: 31.5547, val_loss: 32.0868, val_MinusLogProbMetric: 32.0868

Epoch 511: val_loss did not improve from 32.08080
196/196 - 60s - loss: 31.5547 - MinusLogProbMetric: 31.5547 - val_loss: 32.0868 - val_MinusLogProbMetric: 32.0868 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 512/1000
2023-10-31 13:21:35.482 
Epoch 512/1000 
	 loss: 31.5343, MinusLogProbMetric: 31.5343, val_loss: 32.2153, val_MinusLogProbMetric: 32.2153

Epoch 512: val_loss did not improve from 32.08080
196/196 - 63s - loss: 31.5343 - MinusLogProbMetric: 31.5343 - val_loss: 32.2153 - val_MinusLogProbMetric: 32.2153 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 513/1000
2023-10-31 13:22:41.324 
Epoch 513/1000 
	 loss: 31.5590, MinusLogProbMetric: 31.5590, val_loss: 32.1446, val_MinusLogProbMetric: 32.1446

Epoch 513: val_loss did not improve from 32.08080
196/196 - 66s - loss: 31.5590 - MinusLogProbMetric: 31.5590 - val_loss: 32.1446 - val_MinusLogProbMetric: 32.1446 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 514/1000
2023-10-31 13:23:48.408 
Epoch 514/1000 
	 loss: 31.4994, MinusLogProbMetric: 31.4994, val_loss: 32.0836, val_MinusLogProbMetric: 32.0836

Epoch 514: val_loss did not improve from 32.08080
196/196 - 67s - loss: 31.4994 - MinusLogProbMetric: 31.4994 - val_loss: 32.0836 - val_MinusLogProbMetric: 32.0836 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 515/1000
2023-10-31 13:24:55.674 
Epoch 515/1000 
	 loss: 31.5281, MinusLogProbMetric: 31.5281, val_loss: 32.1037, val_MinusLogProbMetric: 32.1037

Epoch 515: val_loss did not improve from 32.08080
196/196 - 67s - loss: 31.5281 - MinusLogProbMetric: 31.5281 - val_loss: 32.1037 - val_MinusLogProbMetric: 32.1037 - lr: 1.3717e-06 - 67s/epoch - 343ms/step
Epoch 516/1000
2023-10-31 13:26:02.695 
Epoch 516/1000 
	 loss: 31.5340, MinusLogProbMetric: 31.5340, val_loss: 32.1293, val_MinusLogProbMetric: 32.1293

Epoch 516: val_loss did not improve from 32.08080
196/196 - 67s - loss: 31.5340 - MinusLogProbMetric: 31.5340 - val_loss: 32.1293 - val_MinusLogProbMetric: 32.1293 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 517/1000
2023-10-31 13:27:09.392 
Epoch 517/1000 
	 loss: 31.5136, MinusLogProbMetric: 31.5136, val_loss: 32.0917, val_MinusLogProbMetric: 32.0917

Epoch 517: val_loss did not improve from 32.08080
196/196 - 67s - loss: 31.5136 - MinusLogProbMetric: 31.5136 - val_loss: 32.0917 - val_MinusLogProbMetric: 32.0917 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 518/1000
2023-10-31 13:28:12.811 
Epoch 518/1000 
	 loss: 31.5205, MinusLogProbMetric: 31.5205, val_loss: 32.1258, val_MinusLogProbMetric: 32.1258

Epoch 518: val_loss did not improve from 32.08080
196/196 - 63s - loss: 31.5205 - MinusLogProbMetric: 31.5205 - val_loss: 32.1258 - val_MinusLogProbMetric: 32.1258 - lr: 1.3717e-06 - 63s/epoch - 324ms/step
Epoch 519/1000
2023-10-31 13:29:14.507 
Epoch 519/1000 
	 loss: 31.5122, MinusLogProbMetric: 31.5122, val_loss: 32.0619, val_MinusLogProbMetric: 32.0619

Epoch 519: val_loss improved from 32.08080 to 32.06192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.5122 - MinusLogProbMetric: 31.5122 - val_loss: 32.0619 - val_MinusLogProbMetric: 32.0619 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 520/1000
2023-10-31 13:30:16.192 
Epoch 520/1000 
	 loss: 31.5181, MinusLogProbMetric: 31.5181, val_loss: 32.1103, val_MinusLogProbMetric: 32.1103

Epoch 520: val_loss did not improve from 32.06192
196/196 - 61s - loss: 31.5181 - MinusLogProbMetric: 31.5181 - val_loss: 32.1103 - val_MinusLogProbMetric: 32.1103 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 521/1000
2023-10-31 13:31:16.287 
Epoch 521/1000 
	 loss: 31.5152, MinusLogProbMetric: 31.5152, val_loss: 32.1520, val_MinusLogProbMetric: 32.1520

Epoch 521: val_loss did not improve from 32.06192
196/196 - 60s - loss: 31.5152 - MinusLogProbMetric: 31.5152 - val_loss: 32.1520 - val_MinusLogProbMetric: 32.1520 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 522/1000
2023-10-31 13:32:16.818 
Epoch 522/1000 
	 loss: 31.5228, MinusLogProbMetric: 31.5228, val_loss: 32.1360, val_MinusLogProbMetric: 32.1360

Epoch 522: val_loss did not improve from 32.06192
196/196 - 61s - loss: 31.5228 - MinusLogProbMetric: 31.5228 - val_loss: 32.1360 - val_MinusLogProbMetric: 32.1360 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 523/1000
2023-10-31 13:33:16.671 
Epoch 523/1000 
	 loss: 31.5191, MinusLogProbMetric: 31.5191, val_loss: 32.1482, val_MinusLogProbMetric: 32.1482

Epoch 523: val_loss did not improve from 32.06192
196/196 - 60s - loss: 31.5191 - MinusLogProbMetric: 31.5191 - val_loss: 32.1482 - val_MinusLogProbMetric: 32.1482 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 524/1000
2023-10-31 13:34:14.431 
Epoch 524/1000 
	 loss: 31.4970, MinusLogProbMetric: 31.4970, val_loss: 32.1558, val_MinusLogProbMetric: 32.1558

Epoch 524: val_loss did not improve from 32.06192
196/196 - 58s - loss: 31.4970 - MinusLogProbMetric: 31.4970 - val_loss: 32.1558 - val_MinusLogProbMetric: 32.1558 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 525/1000
2023-10-31 13:35:12.836 
Epoch 525/1000 
	 loss: 31.5047, MinusLogProbMetric: 31.5047, val_loss: 32.1697, val_MinusLogProbMetric: 32.1697

Epoch 525: val_loss did not improve from 32.06192
196/196 - 58s - loss: 31.5047 - MinusLogProbMetric: 31.5047 - val_loss: 32.1697 - val_MinusLogProbMetric: 32.1697 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 526/1000
2023-10-31 13:36:12.572 
Epoch 526/1000 
	 loss: 31.4996, MinusLogProbMetric: 31.4996, val_loss: 32.1544, val_MinusLogProbMetric: 32.1544

Epoch 526: val_loss did not improve from 32.06192
196/196 - 60s - loss: 31.4996 - MinusLogProbMetric: 31.4996 - val_loss: 32.1544 - val_MinusLogProbMetric: 32.1544 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 527/1000
2023-10-31 13:37:11.103 
Epoch 527/1000 
	 loss: 31.5249, MinusLogProbMetric: 31.5249, val_loss: 32.1272, val_MinusLogProbMetric: 32.1272

Epoch 527: val_loss did not improve from 32.06192
196/196 - 59s - loss: 31.5249 - MinusLogProbMetric: 31.5249 - val_loss: 32.1272 - val_MinusLogProbMetric: 32.1272 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 528/1000
2023-10-31 13:38:11.093 
Epoch 528/1000 
	 loss: 31.5192, MinusLogProbMetric: 31.5192, val_loss: 32.0856, val_MinusLogProbMetric: 32.0856

Epoch 528: val_loss did not improve from 32.06192
196/196 - 60s - loss: 31.5192 - MinusLogProbMetric: 31.5192 - val_loss: 32.0856 - val_MinusLogProbMetric: 32.0856 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 529/1000
2023-10-31 13:39:12.745 
Epoch 529/1000 
	 loss: 31.4874, MinusLogProbMetric: 31.4874, val_loss: 32.0809, val_MinusLogProbMetric: 32.0809

Epoch 529: val_loss did not improve from 32.06192
196/196 - 62s - loss: 31.4874 - MinusLogProbMetric: 31.4874 - val_loss: 32.0809 - val_MinusLogProbMetric: 32.0809 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 530/1000
2023-10-31 13:40:13.457 
Epoch 530/1000 
	 loss: 31.5136, MinusLogProbMetric: 31.5136, val_loss: 32.0737, val_MinusLogProbMetric: 32.0737

Epoch 530: val_loss did not improve from 32.06192
196/196 - 61s - loss: 31.5136 - MinusLogProbMetric: 31.5136 - val_loss: 32.0737 - val_MinusLogProbMetric: 32.0737 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 531/1000
2023-10-31 13:41:12.120 
Epoch 531/1000 
	 loss: 31.5123, MinusLogProbMetric: 31.5123, val_loss: 32.0458, val_MinusLogProbMetric: 32.0458

Epoch 531: val_loss improved from 32.06192 to 32.04581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.5123 - MinusLogProbMetric: 31.5123 - val_loss: 32.0458 - val_MinusLogProbMetric: 32.0458 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 532/1000
2023-10-31 13:42:12.464 
Epoch 532/1000 
	 loss: 31.4922, MinusLogProbMetric: 31.4922, val_loss: 32.1296, val_MinusLogProbMetric: 32.1296

Epoch 532: val_loss did not improve from 32.04581
196/196 - 59s - loss: 31.4922 - MinusLogProbMetric: 31.4922 - val_loss: 32.1296 - val_MinusLogProbMetric: 32.1296 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 533/1000
2023-10-31 13:43:15.082 
Epoch 533/1000 
	 loss: 31.4963, MinusLogProbMetric: 31.4963, val_loss: 32.0491, val_MinusLogProbMetric: 32.0491

Epoch 533: val_loss did not improve from 32.04581
196/196 - 63s - loss: 31.4963 - MinusLogProbMetric: 31.4963 - val_loss: 32.0491 - val_MinusLogProbMetric: 32.0491 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 534/1000
2023-10-31 13:44:16.513 
Epoch 534/1000 
	 loss: 31.5012, MinusLogProbMetric: 31.5012, val_loss: 32.1145, val_MinusLogProbMetric: 32.1145

Epoch 534: val_loss did not improve from 32.04581
196/196 - 61s - loss: 31.5012 - MinusLogProbMetric: 31.5012 - val_loss: 32.1145 - val_MinusLogProbMetric: 32.1145 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 535/1000
2023-10-31 13:45:19.546 
Epoch 535/1000 
	 loss: 31.4841, MinusLogProbMetric: 31.4841, val_loss: 32.0996, val_MinusLogProbMetric: 32.0996

Epoch 535: val_loss did not improve from 32.04581
196/196 - 63s - loss: 31.4841 - MinusLogProbMetric: 31.4841 - val_loss: 32.0996 - val_MinusLogProbMetric: 32.0996 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 536/1000
2023-10-31 13:46:20.871 
Epoch 536/1000 
	 loss: 31.4943, MinusLogProbMetric: 31.4943, val_loss: 32.0472, val_MinusLogProbMetric: 32.0472

Epoch 536: val_loss did not improve from 32.04581
196/196 - 61s - loss: 31.4943 - MinusLogProbMetric: 31.4943 - val_loss: 32.0472 - val_MinusLogProbMetric: 32.0472 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 537/1000
2023-10-31 13:47:18.125 
Epoch 537/1000 
	 loss: 31.5316, MinusLogProbMetric: 31.5316, val_loss: 32.0567, val_MinusLogProbMetric: 32.0567

Epoch 537: val_loss did not improve from 32.04581
196/196 - 57s - loss: 31.5316 - MinusLogProbMetric: 31.5316 - val_loss: 32.0567 - val_MinusLogProbMetric: 32.0567 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 538/1000
2023-10-31 13:48:14.883 
Epoch 538/1000 
	 loss: 31.4615, MinusLogProbMetric: 31.4615, val_loss: 32.0476, val_MinusLogProbMetric: 32.0476

Epoch 538: val_loss did not improve from 32.04581
196/196 - 57s - loss: 31.4615 - MinusLogProbMetric: 31.4615 - val_loss: 32.0476 - val_MinusLogProbMetric: 32.0476 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 539/1000
2023-10-31 13:49:10.991 
Epoch 539/1000 
	 loss: 31.4840, MinusLogProbMetric: 31.4840, val_loss: 32.1153, val_MinusLogProbMetric: 32.1153

Epoch 539: val_loss did not improve from 32.04581
196/196 - 56s - loss: 31.4840 - MinusLogProbMetric: 31.4840 - val_loss: 32.1153 - val_MinusLogProbMetric: 32.1153 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 540/1000
2023-10-31 13:50:08.356 
Epoch 540/1000 
	 loss: 31.4817, MinusLogProbMetric: 31.4817, val_loss: 32.0665, val_MinusLogProbMetric: 32.0665

Epoch 540: val_loss did not improve from 32.04581
196/196 - 57s - loss: 31.4817 - MinusLogProbMetric: 31.4817 - val_loss: 32.0665 - val_MinusLogProbMetric: 32.0665 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 541/1000
2023-10-31 13:51:05.979 
Epoch 541/1000 
	 loss: 31.4788, MinusLogProbMetric: 31.4788, val_loss: 32.0652, val_MinusLogProbMetric: 32.0652

Epoch 541: val_loss did not improve from 32.04581
196/196 - 58s - loss: 31.4788 - MinusLogProbMetric: 31.4788 - val_loss: 32.0652 - val_MinusLogProbMetric: 32.0652 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 542/1000
2023-10-31 13:52:07.089 
Epoch 542/1000 
	 loss: 31.4686, MinusLogProbMetric: 31.4686, val_loss: 32.0544, val_MinusLogProbMetric: 32.0544

Epoch 542: val_loss did not improve from 32.04581
196/196 - 61s - loss: 31.4686 - MinusLogProbMetric: 31.4686 - val_loss: 32.0544 - val_MinusLogProbMetric: 32.0544 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 543/1000
2023-10-31 13:53:07.416 
Epoch 543/1000 
	 loss: 31.4790, MinusLogProbMetric: 31.4790, val_loss: 32.1542, val_MinusLogProbMetric: 32.1542

Epoch 543: val_loss did not improve from 32.04581
196/196 - 60s - loss: 31.4790 - MinusLogProbMetric: 31.4790 - val_loss: 32.1542 - val_MinusLogProbMetric: 32.1542 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 544/1000
2023-10-31 13:54:07.661 
Epoch 544/1000 
	 loss: 31.4832, MinusLogProbMetric: 31.4832, val_loss: 32.0560, val_MinusLogProbMetric: 32.0560

Epoch 544: val_loss did not improve from 32.04581
196/196 - 60s - loss: 31.4832 - MinusLogProbMetric: 31.4832 - val_loss: 32.0560 - val_MinusLogProbMetric: 32.0560 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 545/1000
2023-10-31 13:55:06.927 
Epoch 545/1000 
	 loss: 31.4802, MinusLogProbMetric: 31.4802, val_loss: 32.0275, val_MinusLogProbMetric: 32.0275

Epoch 545: val_loss improved from 32.04581 to 32.02747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.4802 - MinusLogProbMetric: 31.4802 - val_loss: 32.0275 - val_MinusLogProbMetric: 32.0275 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 546/1000
2023-10-31 13:56:05.772 
Epoch 546/1000 
	 loss: 31.4776, MinusLogProbMetric: 31.4776, val_loss: 32.0763, val_MinusLogProbMetric: 32.0763

Epoch 546: val_loss did not improve from 32.02747
196/196 - 58s - loss: 31.4776 - MinusLogProbMetric: 31.4776 - val_loss: 32.0763 - val_MinusLogProbMetric: 32.0763 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 547/1000
2023-10-31 13:57:02.478 
Epoch 547/1000 
	 loss: 31.4827, MinusLogProbMetric: 31.4827, val_loss: 32.1876, val_MinusLogProbMetric: 32.1876

Epoch 547: val_loss did not improve from 32.02747
196/196 - 57s - loss: 31.4827 - MinusLogProbMetric: 31.4827 - val_loss: 32.1876 - val_MinusLogProbMetric: 32.1876 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 548/1000
2023-10-31 13:57:59.308 
Epoch 548/1000 
	 loss: 31.4766, MinusLogProbMetric: 31.4766, val_loss: 32.0470, val_MinusLogProbMetric: 32.0470

Epoch 548: val_loss did not improve from 32.02747
196/196 - 57s - loss: 31.4766 - MinusLogProbMetric: 31.4766 - val_loss: 32.0470 - val_MinusLogProbMetric: 32.0470 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 549/1000
2023-10-31 13:58:59.510 
Epoch 549/1000 
	 loss: 31.5053, MinusLogProbMetric: 31.5053, val_loss: 32.1954, val_MinusLogProbMetric: 32.1954

Epoch 549: val_loss did not improve from 32.02747
196/196 - 60s - loss: 31.5053 - MinusLogProbMetric: 31.5053 - val_loss: 32.1954 - val_MinusLogProbMetric: 32.1954 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 550/1000
2023-10-31 13:59:59.033 
Epoch 550/1000 
	 loss: 31.4731, MinusLogProbMetric: 31.4731, val_loss: 32.0448, val_MinusLogProbMetric: 32.0448

Epoch 550: val_loss did not improve from 32.02747
196/196 - 60s - loss: 31.4731 - MinusLogProbMetric: 31.4731 - val_loss: 32.0448 - val_MinusLogProbMetric: 32.0448 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 551/1000
2023-10-31 14:00:59.362 
Epoch 551/1000 
	 loss: 31.4683, MinusLogProbMetric: 31.4683, val_loss: 32.2592, val_MinusLogProbMetric: 32.2592

Epoch 551: val_loss did not improve from 32.02747
196/196 - 60s - loss: 31.4683 - MinusLogProbMetric: 31.4683 - val_loss: 32.2592 - val_MinusLogProbMetric: 32.2592 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 552/1000
2023-10-31 14:02:00.193 
Epoch 552/1000 
	 loss: 31.4662, MinusLogProbMetric: 31.4662, val_loss: 32.1496, val_MinusLogProbMetric: 32.1496

Epoch 552: val_loss did not improve from 32.02747
196/196 - 61s - loss: 31.4662 - MinusLogProbMetric: 31.4662 - val_loss: 32.1496 - val_MinusLogProbMetric: 32.1496 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 553/1000
2023-10-31 14:03:03.383 
Epoch 553/1000 
	 loss: 31.4459, MinusLogProbMetric: 31.4459, val_loss: 32.0541, val_MinusLogProbMetric: 32.0541

Epoch 553: val_loss did not improve from 32.02747
196/196 - 63s - loss: 31.4459 - MinusLogProbMetric: 31.4459 - val_loss: 32.0541 - val_MinusLogProbMetric: 32.0541 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 554/1000
2023-10-31 14:04:06.227 
Epoch 554/1000 
	 loss: 31.4532, MinusLogProbMetric: 31.4532, val_loss: 32.0620, val_MinusLogProbMetric: 32.0620

Epoch 554: val_loss did not improve from 32.02747
196/196 - 63s - loss: 31.4532 - MinusLogProbMetric: 31.4532 - val_loss: 32.0620 - val_MinusLogProbMetric: 32.0620 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 555/1000
2023-10-31 14:05:08.511 
Epoch 555/1000 
	 loss: 31.4490, MinusLogProbMetric: 31.4490, val_loss: 32.0564, val_MinusLogProbMetric: 32.0564

Epoch 555: val_loss did not improve from 32.02747
196/196 - 62s - loss: 31.4490 - MinusLogProbMetric: 31.4490 - val_loss: 32.0564 - val_MinusLogProbMetric: 32.0564 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 556/1000
2023-10-31 14:06:13.464 
Epoch 556/1000 
	 loss: 31.4783, MinusLogProbMetric: 31.4783, val_loss: 32.1465, val_MinusLogProbMetric: 32.1465

Epoch 556: val_loss did not improve from 32.02747
196/196 - 65s - loss: 31.4783 - MinusLogProbMetric: 31.4783 - val_loss: 32.1465 - val_MinusLogProbMetric: 32.1465 - lr: 1.3717e-06 - 65s/epoch - 331ms/step
Epoch 557/1000
2023-10-31 14:07:18.022 
Epoch 557/1000 
	 loss: 31.4404, MinusLogProbMetric: 31.4404, val_loss: 32.0701, val_MinusLogProbMetric: 32.0701

Epoch 557: val_loss did not improve from 32.02747
196/196 - 65s - loss: 31.4404 - MinusLogProbMetric: 31.4404 - val_loss: 32.0701 - val_MinusLogProbMetric: 32.0701 - lr: 1.3717e-06 - 65s/epoch - 329ms/step
Epoch 558/1000
2023-10-31 14:08:20.819 
Epoch 558/1000 
	 loss: 33.3184, MinusLogProbMetric: 33.3184, val_loss: 33.5276, val_MinusLogProbMetric: 33.5276

Epoch 558: val_loss did not improve from 32.02747
196/196 - 63s - loss: 33.3184 - MinusLogProbMetric: 33.3184 - val_loss: 33.5276 - val_MinusLogProbMetric: 33.5276 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 559/1000
2023-10-31 14:09:19.867 
Epoch 559/1000 
	 loss: 32.4254, MinusLogProbMetric: 32.4254, val_loss: 32.7260, val_MinusLogProbMetric: 32.7260

Epoch 559: val_loss did not improve from 32.02747
196/196 - 59s - loss: 32.4254 - MinusLogProbMetric: 32.4254 - val_loss: 32.7260 - val_MinusLogProbMetric: 32.7260 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 560/1000
2023-10-31 14:10:17.551 
Epoch 560/1000 
	 loss: 32.1502, MinusLogProbMetric: 32.1502, val_loss: 32.6353, val_MinusLogProbMetric: 32.6353

Epoch 560: val_loss did not improve from 32.02747
196/196 - 58s - loss: 32.1502 - MinusLogProbMetric: 32.1502 - val_loss: 32.6353 - val_MinusLogProbMetric: 32.6353 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 561/1000
2023-10-31 14:11:14.028 
Epoch 561/1000 
	 loss: 32.1006, MinusLogProbMetric: 32.1006, val_loss: 32.6962, val_MinusLogProbMetric: 32.6962

Epoch 561: val_loss did not improve from 32.02747
196/196 - 56s - loss: 32.1006 - MinusLogProbMetric: 32.1006 - val_loss: 32.6962 - val_MinusLogProbMetric: 32.6962 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 562/1000
2023-10-31 14:12:13.581 
Epoch 562/1000 
	 loss: 32.0357, MinusLogProbMetric: 32.0357, val_loss: 32.5435, val_MinusLogProbMetric: 32.5435

Epoch 562: val_loss did not improve from 32.02747
196/196 - 60s - loss: 32.0357 - MinusLogProbMetric: 32.0357 - val_loss: 32.5435 - val_MinusLogProbMetric: 32.5435 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 563/1000
2023-10-31 14:13:15.711 
Epoch 563/1000 
	 loss: 32.0463, MinusLogProbMetric: 32.0463, val_loss: 32.6753, val_MinusLogProbMetric: 32.6753

Epoch 563: val_loss did not improve from 32.02747
196/196 - 62s - loss: 32.0463 - MinusLogProbMetric: 32.0463 - val_loss: 32.6753 - val_MinusLogProbMetric: 32.6753 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 564/1000
2023-10-31 14:14:16.582 
Epoch 564/1000 
	 loss: 31.9834, MinusLogProbMetric: 31.9834, val_loss: 32.5272, val_MinusLogProbMetric: 32.5272

Epoch 564: val_loss did not improve from 32.02747
196/196 - 61s - loss: 31.9834 - MinusLogProbMetric: 31.9834 - val_loss: 32.5272 - val_MinusLogProbMetric: 32.5272 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 565/1000
2023-10-31 14:15:15.021 
Epoch 565/1000 
	 loss: 31.9647, MinusLogProbMetric: 31.9647, val_loss: 32.5286, val_MinusLogProbMetric: 32.5286

Epoch 565: val_loss did not improve from 32.02747
196/196 - 58s - loss: 31.9647 - MinusLogProbMetric: 31.9647 - val_loss: 32.5286 - val_MinusLogProbMetric: 32.5286 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 566/1000
2023-10-31 14:16:17.640 
Epoch 566/1000 
	 loss: 31.9615, MinusLogProbMetric: 31.9615, val_loss: 32.5001, val_MinusLogProbMetric: 32.5001

Epoch 566: val_loss did not improve from 32.02747
196/196 - 63s - loss: 31.9615 - MinusLogProbMetric: 31.9615 - val_loss: 32.5001 - val_MinusLogProbMetric: 32.5001 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 567/1000
2023-10-31 14:17:23.574 
Epoch 567/1000 
	 loss: 31.9360, MinusLogProbMetric: 31.9360, val_loss: 32.7151, val_MinusLogProbMetric: 32.7151

Epoch 567: val_loss did not improve from 32.02747
196/196 - 66s - loss: 31.9360 - MinusLogProbMetric: 31.9360 - val_loss: 32.7151 - val_MinusLogProbMetric: 32.7151 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 568/1000
2023-10-31 14:18:26.602 
Epoch 568/1000 
	 loss: 31.9819, MinusLogProbMetric: 31.9819, val_loss: 32.5404, val_MinusLogProbMetric: 32.5404

Epoch 568: val_loss did not improve from 32.02747
196/196 - 63s - loss: 31.9819 - MinusLogProbMetric: 31.9819 - val_loss: 32.5404 - val_MinusLogProbMetric: 32.5404 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 569/1000
2023-10-31 14:19:28.814 
Epoch 569/1000 
	 loss: 31.9256, MinusLogProbMetric: 31.9256, val_loss: 32.4702, val_MinusLogProbMetric: 32.4702

Epoch 569: val_loss did not improve from 32.02747
196/196 - 62s - loss: 31.9256 - MinusLogProbMetric: 31.9256 - val_loss: 32.4702 - val_MinusLogProbMetric: 32.4702 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 570/1000
2023-10-31 14:20:27.772 
Epoch 570/1000 
	 loss: 31.9054, MinusLogProbMetric: 31.9054, val_loss: 32.4759, val_MinusLogProbMetric: 32.4759

Epoch 570: val_loss did not improve from 32.02747
196/196 - 59s - loss: 31.9054 - MinusLogProbMetric: 31.9054 - val_loss: 32.4759 - val_MinusLogProbMetric: 32.4759 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 571/1000
2023-10-31 14:21:27.371 
Epoch 571/1000 
	 loss: 31.8394, MinusLogProbMetric: 31.8394, val_loss: 32.3955, val_MinusLogProbMetric: 32.3955

Epoch 571: val_loss did not improve from 32.02747
196/196 - 60s - loss: 31.8394 - MinusLogProbMetric: 31.8394 - val_loss: 32.3955 - val_MinusLogProbMetric: 32.3955 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 572/1000
2023-10-31 14:22:28.127 
Epoch 572/1000 
	 loss: 31.7428, MinusLogProbMetric: 31.7428, val_loss: 32.1724, val_MinusLogProbMetric: 32.1724

Epoch 572: val_loss did not improve from 32.02747
196/196 - 61s - loss: 31.7428 - MinusLogProbMetric: 31.7428 - val_loss: 32.1724 - val_MinusLogProbMetric: 32.1724 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 573/1000
2023-10-31 14:23:34.579 
Epoch 573/1000 
	 loss: 31.5449, MinusLogProbMetric: 31.5449, val_loss: 32.0959, val_MinusLogProbMetric: 32.0959

Epoch 573: val_loss did not improve from 32.02747
196/196 - 66s - loss: 31.5449 - MinusLogProbMetric: 31.5449 - val_loss: 32.0959 - val_MinusLogProbMetric: 32.0959 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 574/1000
2023-10-31 14:24:39.973 
Epoch 574/1000 
	 loss: 31.5043, MinusLogProbMetric: 31.5043, val_loss: 32.1374, val_MinusLogProbMetric: 32.1374

Epoch 574: val_loss did not improve from 32.02747
196/196 - 65s - loss: 31.5043 - MinusLogProbMetric: 31.5043 - val_loss: 32.1374 - val_MinusLogProbMetric: 32.1374 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 575/1000
2023-10-31 14:25:42.850 
Epoch 575/1000 
	 loss: 31.4823, MinusLogProbMetric: 31.4823, val_loss: 32.0427, val_MinusLogProbMetric: 32.0427

Epoch 575: val_loss did not improve from 32.02747
196/196 - 63s - loss: 31.4823 - MinusLogProbMetric: 31.4823 - val_loss: 32.0427 - val_MinusLogProbMetric: 32.0427 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 576/1000
2023-10-31 14:26:45.888 
Epoch 576/1000 
	 loss: 31.4851, MinusLogProbMetric: 31.4851, val_loss: 32.0681, val_MinusLogProbMetric: 32.0681

Epoch 576: val_loss did not improve from 32.02747
196/196 - 63s - loss: 31.4851 - MinusLogProbMetric: 31.4851 - val_loss: 32.0681 - val_MinusLogProbMetric: 32.0681 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 577/1000
2023-10-31 14:27:48.192 
Epoch 577/1000 
	 loss: 31.4524, MinusLogProbMetric: 31.4524, val_loss: 32.0161, val_MinusLogProbMetric: 32.0161

Epoch 577: val_loss improved from 32.02747 to 32.01612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.4524 - MinusLogProbMetric: 31.4524 - val_loss: 32.0161 - val_MinusLogProbMetric: 32.0161 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 578/1000
2023-10-31 14:28:51.382 
Epoch 578/1000 
	 loss: 31.4601, MinusLogProbMetric: 31.4601, val_loss: 32.0058, val_MinusLogProbMetric: 32.0058

Epoch 578: val_loss improved from 32.01612 to 32.00577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.4601 - MinusLogProbMetric: 31.4601 - val_loss: 32.0058 - val_MinusLogProbMetric: 32.0058 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 579/1000
2023-10-31 14:29:54.205 
Epoch 579/1000 
	 loss: 31.4405, MinusLogProbMetric: 31.4405, val_loss: 32.0011, val_MinusLogProbMetric: 32.0011

Epoch 579: val_loss improved from 32.00577 to 32.00110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.4405 - MinusLogProbMetric: 31.4405 - val_loss: 32.0011 - val_MinusLogProbMetric: 32.0011 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 580/1000
2023-10-31 14:30:57.732 
Epoch 580/1000 
	 loss: 31.4487, MinusLogProbMetric: 31.4487, val_loss: 32.1182, val_MinusLogProbMetric: 32.1182

Epoch 580: val_loss did not improve from 32.00110
196/196 - 63s - loss: 31.4487 - MinusLogProbMetric: 31.4487 - val_loss: 32.1182 - val_MinusLogProbMetric: 32.1182 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 581/1000
2023-10-31 14:32:02.520 
Epoch 581/1000 
	 loss: 31.4393, MinusLogProbMetric: 31.4393, val_loss: 32.0877, val_MinusLogProbMetric: 32.0877

Epoch 581: val_loss did not improve from 32.00110
196/196 - 65s - loss: 31.4393 - MinusLogProbMetric: 31.4393 - val_loss: 32.0877 - val_MinusLogProbMetric: 32.0877 - lr: 1.3717e-06 - 65s/epoch - 331ms/step
Epoch 582/1000
2023-10-31 14:33:05.487 
Epoch 582/1000 
	 loss: 31.4569, MinusLogProbMetric: 31.4569, val_loss: 32.1257, val_MinusLogProbMetric: 32.1257

Epoch 582: val_loss did not improve from 32.00110
196/196 - 63s - loss: 31.4569 - MinusLogProbMetric: 31.4569 - val_loss: 32.1257 - val_MinusLogProbMetric: 32.1257 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 583/1000
2023-10-31 14:34:07.630 
Epoch 583/1000 
	 loss: 31.4306, MinusLogProbMetric: 31.4306, val_loss: 32.1339, val_MinusLogProbMetric: 32.1339

Epoch 583: val_loss did not improve from 32.00110
196/196 - 62s - loss: 31.4306 - MinusLogProbMetric: 31.4306 - val_loss: 32.1339 - val_MinusLogProbMetric: 32.1339 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 584/1000
2023-10-31 14:35:08.076 
Epoch 584/1000 
	 loss: 31.4705, MinusLogProbMetric: 31.4705, val_loss: 31.9976, val_MinusLogProbMetric: 31.9976

Epoch 584: val_loss improved from 32.00110 to 31.99764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 31.4705 - MinusLogProbMetric: 31.4705 - val_loss: 31.9976 - val_MinusLogProbMetric: 31.9976 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 585/1000
2023-10-31 14:36:07.749 
Epoch 585/1000 
	 loss: 31.4343, MinusLogProbMetric: 31.4343, val_loss: 31.9868, val_MinusLogProbMetric: 31.9868

Epoch 585: val_loss improved from 31.99764 to 31.98676, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.4343 - MinusLogProbMetric: 31.4343 - val_loss: 31.9868 - val_MinusLogProbMetric: 31.9868 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 586/1000
2023-10-31 14:37:13.028 
Epoch 586/1000 
	 loss: 31.4173, MinusLogProbMetric: 31.4173, val_loss: 32.0281, val_MinusLogProbMetric: 32.0281

Epoch 586: val_loss did not improve from 31.98676
196/196 - 64s - loss: 31.4173 - MinusLogProbMetric: 31.4173 - val_loss: 32.0281 - val_MinusLogProbMetric: 32.0281 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 587/1000
2023-10-31 14:38:15.523 
Epoch 587/1000 
	 loss: 31.4114, MinusLogProbMetric: 31.4114, val_loss: 32.1142, val_MinusLogProbMetric: 32.1142

Epoch 587: val_loss did not improve from 31.98676
196/196 - 62s - loss: 31.4114 - MinusLogProbMetric: 31.4114 - val_loss: 32.1142 - val_MinusLogProbMetric: 32.1142 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 588/1000
2023-10-31 14:39:17.984 
Epoch 588/1000 
	 loss: 31.4248, MinusLogProbMetric: 31.4248, val_loss: 32.0949, val_MinusLogProbMetric: 32.0949

Epoch 588: val_loss did not improve from 31.98676
196/196 - 62s - loss: 31.4248 - MinusLogProbMetric: 31.4248 - val_loss: 32.0949 - val_MinusLogProbMetric: 32.0949 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 589/1000
2023-10-31 14:40:19.173 
Epoch 589/1000 
	 loss: 31.4324, MinusLogProbMetric: 31.4324, val_loss: 32.0394, val_MinusLogProbMetric: 32.0394

Epoch 589: val_loss did not improve from 31.98676
196/196 - 61s - loss: 31.4324 - MinusLogProbMetric: 31.4324 - val_loss: 32.0394 - val_MinusLogProbMetric: 32.0394 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 590/1000
2023-10-31 14:41:24.021 
Epoch 590/1000 
	 loss: 31.4174, MinusLogProbMetric: 31.4174, val_loss: 31.9918, val_MinusLogProbMetric: 31.9918

Epoch 590: val_loss did not improve from 31.98676
196/196 - 65s - loss: 31.4174 - MinusLogProbMetric: 31.4174 - val_loss: 31.9918 - val_MinusLogProbMetric: 31.9918 - lr: 1.3717e-06 - 65s/epoch - 331ms/step
Epoch 591/1000
2023-10-31 14:42:31.345 
Epoch 591/1000 
	 loss: 31.4057, MinusLogProbMetric: 31.4057, val_loss: 31.9866, val_MinusLogProbMetric: 31.9866

Epoch 591: val_loss improved from 31.98676 to 31.98660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 68s - loss: 31.4057 - MinusLogProbMetric: 31.4057 - val_loss: 31.9866 - val_MinusLogProbMetric: 31.9866 - lr: 1.3717e-06 - 68s/epoch - 349ms/step
Epoch 592/1000
2023-10-31 14:43:35.046 
Epoch 592/1000 
	 loss: 31.4127, MinusLogProbMetric: 31.4127, val_loss: 32.0271, val_MinusLogProbMetric: 32.0271

Epoch 592: val_loss did not improve from 31.98660
196/196 - 63s - loss: 31.4127 - MinusLogProbMetric: 31.4127 - val_loss: 32.0271 - val_MinusLogProbMetric: 32.0271 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 593/1000
2023-10-31 14:44:36.619 
Epoch 593/1000 
	 loss: 31.3963, MinusLogProbMetric: 31.3963, val_loss: 31.9372, val_MinusLogProbMetric: 31.9372

Epoch 593: val_loss improved from 31.98660 to 31.93716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.3963 - MinusLogProbMetric: 31.3963 - val_loss: 31.9372 - val_MinusLogProbMetric: 31.9372 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 594/1000
2023-10-31 14:45:40.780 
Epoch 594/1000 
	 loss: 31.4086, MinusLogProbMetric: 31.4086, val_loss: 32.0174, val_MinusLogProbMetric: 32.0174

Epoch 594: val_loss did not improve from 31.93716
196/196 - 63s - loss: 31.4086 - MinusLogProbMetric: 31.4086 - val_loss: 32.0174 - val_MinusLogProbMetric: 32.0174 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 595/1000
2023-10-31 14:46:43.893 
Epoch 595/1000 
	 loss: 31.4075, MinusLogProbMetric: 31.4075, val_loss: 31.9882, val_MinusLogProbMetric: 31.9882

Epoch 595: val_loss did not improve from 31.93716
196/196 - 63s - loss: 31.4075 - MinusLogProbMetric: 31.4075 - val_loss: 31.9882 - val_MinusLogProbMetric: 31.9882 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 596/1000
2023-10-31 14:47:46.337 
Epoch 596/1000 
	 loss: 31.4004, MinusLogProbMetric: 31.4004, val_loss: 32.0156, val_MinusLogProbMetric: 32.0156

Epoch 596: val_loss did not improve from 31.93716
196/196 - 62s - loss: 31.4004 - MinusLogProbMetric: 31.4004 - val_loss: 32.0156 - val_MinusLogProbMetric: 32.0156 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 597/1000
2023-10-31 14:48:48.204 
Epoch 597/1000 
	 loss: 31.4062, MinusLogProbMetric: 31.4062, val_loss: 31.9764, val_MinusLogProbMetric: 31.9764

Epoch 597: val_loss did not improve from 31.93716
196/196 - 62s - loss: 31.4062 - MinusLogProbMetric: 31.4062 - val_loss: 31.9764 - val_MinusLogProbMetric: 31.9764 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 598/1000
2023-10-31 14:49:49.572 
Epoch 598/1000 
	 loss: 31.4111, MinusLogProbMetric: 31.4111, val_loss: 31.9738, val_MinusLogProbMetric: 31.9738

Epoch 598: val_loss did not improve from 31.93716
196/196 - 61s - loss: 31.4111 - MinusLogProbMetric: 31.4111 - val_loss: 31.9738 - val_MinusLogProbMetric: 31.9738 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 599/1000
2023-10-31 14:50:53.430 
Epoch 599/1000 
	 loss: 31.4153, MinusLogProbMetric: 31.4153, val_loss: 31.9524, val_MinusLogProbMetric: 31.9524

Epoch 599: val_loss did not improve from 31.93716
196/196 - 64s - loss: 31.4153 - MinusLogProbMetric: 31.4153 - val_loss: 31.9524 - val_MinusLogProbMetric: 31.9524 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 600/1000
2023-10-31 14:51:55.202 
Epoch 600/1000 
	 loss: 31.3773, MinusLogProbMetric: 31.3773, val_loss: 32.0243, val_MinusLogProbMetric: 32.0243

Epoch 600: val_loss did not improve from 31.93716
196/196 - 62s - loss: 31.3773 - MinusLogProbMetric: 31.3773 - val_loss: 32.0243 - val_MinusLogProbMetric: 32.0243 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 601/1000
2023-10-31 14:53:02.728 
Epoch 601/1000 
	 loss: 31.3987, MinusLogProbMetric: 31.3987, val_loss: 32.0300, val_MinusLogProbMetric: 32.0300

Epoch 601: val_loss did not improve from 31.93716
196/196 - 68s - loss: 31.3987 - MinusLogProbMetric: 31.3987 - val_loss: 32.0300 - val_MinusLogProbMetric: 32.0300 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 602/1000
2023-10-31 14:54:05.925 
Epoch 602/1000 
	 loss: 31.3901, MinusLogProbMetric: 31.3901, val_loss: 32.0292, val_MinusLogProbMetric: 32.0292

Epoch 602: val_loss did not improve from 31.93716
196/196 - 63s - loss: 31.3901 - MinusLogProbMetric: 31.3901 - val_loss: 32.0292 - val_MinusLogProbMetric: 32.0292 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 603/1000
2023-10-31 14:55:06.537 
Epoch 603/1000 
	 loss: 31.3857, MinusLogProbMetric: 31.3857, val_loss: 32.0308, val_MinusLogProbMetric: 32.0308

Epoch 603: val_loss did not improve from 31.93716
196/196 - 61s - loss: 31.3857 - MinusLogProbMetric: 31.3857 - val_loss: 32.0308 - val_MinusLogProbMetric: 32.0308 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 604/1000
2023-10-31 14:56:07.471 
Epoch 604/1000 
	 loss: 31.4384, MinusLogProbMetric: 31.4384, val_loss: 32.1262, val_MinusLogProbMetric: 32.1262

Epoch 604: val_loss did not improve from 31.93716
196/196 - 61s - loss: 31.4384 - MinusLogProbMetric: 31.4384 - val_loss: 32.1262 - val_MinusLogProbMetric: 32.1262 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 605/1000
2023-10-31 14:57:09.976 
Epoch 605/1000 
	 loss: 31.4195, MinusLogProbMetric: 31.4195, val_loss: 32.0349, val_MinusLogProbMetric: 32.0349

Epoch 605: val_loss did not improve from 31.93716
196/196 - 63s - loss: 31.4195 - MinusLogProbMetric: 31.4195 - val_loss: 32.0349 - val_MinusLogProbMetric: 32.0349 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 606/1000
2023-10-31 14:58:12.356 
Epoch 606/1000 
	 loss: 31.3837, MinusLogProbMetric: 31.3837, val_loss: 31.9701, val_MinusLogProbMetric: 31.9701

Epoch 606: val_loss did not improve from 31.93716
196/196 - 62s - loss: 31.3837 - MinusLogProbMetric: 31.3837 - val_loss: 31.9701 - val_MinusLogProbMetric: 31.9701 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 607/1000
2023-10-31 14:59:12.178 
Epoch 607/1000 
	 loss: 31.3879, MinusLogProbMetric: 31.3879, val_loss: 31.9852, val_MinusLogProbMetric: 31.9852

Epoch 607: val_loss did not improve from 31.93716
196/196 - 60s - loss: 31.3879 - MinusLogProbMetric: 31.3879 - val_loss: 31.9852 - val_MinusLogProbMetric: 31.9852 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 608/1000
2023-10-31 15:00:12.740 
Epoch 608/1000 
	 loss: 31.3887, MinusLogProbMetric: 31.3887, val_loss: 31.9475, val_MinusLogProbMetric: 31.9475

Epoch 608: val_loss did not improve from 31.93716
196/196 - 61s - loss: 31.3887 - MinusLogProbMetric: 31.3887 - val_loss: 31.9475 - val_MinusLogProbMetric: 31.9475 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 609/1000
2023-10-31 15:01:13.312 
Epoch 609/1000 
	 loss: 31.3665, MinusLogProbMetric: 31.3665, val_loss: 32.0054, val_MinusLogProbMetric: 32.0054

Epoch 609: val_loss did not improve from 31.93716
196/196 - 61s - loss: 31.3665 - MinusLogProbMetric: 31.3665 - val_loss: 32.0054 - val_MinusLogProbMetric: 32.0054 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 610/1000
2023-10-31 15:02:15.081 
Epoch 610/1000 
	 loss: 31.3914, MinusLogProbMetric: 31.3914, val_loss: 32.0026, val_MinusLogProbMetric: 32.0026

Epoch 610: val_loss did not improve from 31.93716
196/196 - 62s - loss: 31.3914 - MinusLogProbMetric: 31.3914 - val_loss: 32.0026 - val_MinusLogProbMetric: 32.0026 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 611/1000
2023-10-31 15:03:22.559 
Epoch 611/1000 
	 loss: 31.3698, MinusLogProbMetric: 31.3698, val_loss: 31.9594, val_MinusLogProbMetric: 31.9594

Epoch 611: val_loss did not improve from 31.93716
196/196 - 67s - loss: 31.3698 - MinusLogProbMetric: 31.3698 - val_loss: 31.9594 - val_MinusLogProbMetric: 31.9594 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 612/1000
2023-10-31 15:04:29.982 
Epoch 612/1000 
	 loss: 31.3839, MinusLogProbMetric: 31.3839, val_loss: 32.2237, val_MinusLogProbMetric: 32.2237

Epoch 612: val_loss did not improve from 31.93716
196/196 - 67s - loss: 31.3839 - MinusLogProbMetric: 31.3839 - val_loss: 32.2237 - val_MinusLogProbMetric: 32.2237 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 613/1000
2023-10-31 15:05:35.507 
Epoch 613/1000 
	 loss: 31.4054, MinusLogProbMetric: 31.4054, val_loss: 31.9054, val_MinusLogProbMetric: 31.9054

Epoch 613: val_loss improved from 31.93716 to 31.90538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 31.4054 - MinusLogProbMetric: 31.4054 - val_loss: 31.9054 - val_MinusLogProbMetric: 31.9054 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 614/1000
2023-10-31 15:06:38.858 
Epoch 614/1000 
	 loss: 31.3769, MinusLogProbMetric: 31.3769, val_loss: 31.9665, val_MinusLogProbMetric: 31.9665

Epoch 614: val_loss did not improve from 31.90538
196/196 - 62s - loss: 31.3769 - MinusLogProbMetric: 31.3769 - val_loss: 31.9665 - val_MinusLogProbMetric: 31.9665 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 615/1000
2023-10-31 15:07:37.427 
Epoch 615/1000 
	 loss: 31.3696, MinusLogProbMetric: 31.3696, val_loss: 31.9807, val_MinusLogProbMetric: 31.9807

Epoch 615: val_loss did not improve from 31.90538
196/196 - 59s - loss: 31.3696 - MinusLogProbMetric: 31.3696 - val_loss: 31.9807 - val_MinusLogProbMetric: 31.9807 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 616/1000
2023-10-31 15:08:36.581 
Epoch 616/1000 
	 loss: 31.3719, MinusLogProbMetric: 31.3719, val_loss: 31.9344, val_MinusLogProbMetric: 31.9344

Epoch 616: val_loss did not improve from 31.90538
196/196 - 59s - loss: 31.3719 - MinusLogProbMetric: 31.3719 - val_loss: 31.9344 - val_MinusLogProbMetric: 31.9344 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 617/1000
2023-10-31 15:09:36.741 
Epoch 617/1000 
	 loss: 31.3736, MinusLogProbMetric: 31.3736, val_loss: 31.9646, val_MinusLogProbMetric: 31.9646

Epoch 617: val_loss did not improve from 31.90538
196/196 - 60s - loss: 31.3736 - MinusLogProbMetric: 31.3736 - val_loss: 31.9646 - val_MinusLogProbMetric: 31.9646 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 618/1000
2023-10-31 15:10:38.756 
Epoch 618/1000 
	 loss: 31.3689, MinusLogProbMetric: 31.3689, val_loss: 32.0352, val_MinusLogProbMetric: 32.0352

Epoch 618: val_loss did not improve from 31.90538
196/196 - 62s - loss: 31.3689 - MinusLogProbMetric: 31.3689 - val_loss: 32.0352 - val_MinusLogProbMetric: 32.0352 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 619/1000
2023-10-31 15:11:37.251 
Epoch 619/1000 
	 loss: 31.3577, MinusLogProbMetric: 31.3577, val_loss: 31.9672, val_MinusLogProbMetric: 31.9672

Epoch 619: val_loss did not improve from 31.90538
196/196 - 58s - loss: 31.3577 - MinusLogProbMetric: 31.3577 - val_loss: 31.9672 - val_MinusLogProbMetric: 31.9672 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 620/1000
2023-10-31 15:12:34.068 
Epoch 620/1000 
	 loss: 31.3676, MinusLogProbMetric: 31.3676, val_loss: 32.0187, val_MinusLogProbMetric: 32.0187

Epoch 620: val_loss did not improve from 31.90538
196/196 - 57s - loss: 31.3676 - MinusLogProbMetric: 31.3676 - val_loss: 32.0187 - val_MinusLogProbMetric: 32.0187 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 621/1000
2023-10-31 15:13:30.693 
Epoch 621/1000 
	 loss: 31.3754, MinusLogProbMetric: 31.3754, val_loss: 32.0121, val_MinusLogProbMetric: 32.0121

Epoch 621: val_loss did not improve from 31.90538
196/196 - 57s - loss: 31.3754 - MinusLogProbMetric: 31.3754 - val_loss: 32.0121 - val_MinusLogProbMetric: 32.0121 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 622/1000
2023-10-31 15:14:27.313 
Epoch 622/1000 
	 loss: 31.3767, MinusLogProbMetric: 31.3767, val_loss: 32.0091, val_MinusLogProbMetric: 32.0091

Epoch 622: val_loss did not improve from 31.90538
196/196 - 57s - loss: 31.3767 - MinusLogProbMetric: 31.3767 - val_loss: 32.0091 - val_MinusLogProbMetric: 32.0091 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 623/1000
2023-10-31 15:15:21.803 
Epoch 623/1000 
	 loss: 31.3650, MinusLogProbMetric: 31.3650, val_loss: 31.9691, val_MinusLogProbMetric: 31.9691

Epoch 623: val_loss did not improve from 31.90538
196/196 - 54s - loss: 31.3650 - MinusLogProbMetric: 31.3650 - val_loss: 31.9691 - val_MinusLogProbMetric: 31.9691 - lr: 1.3717e-06 - 54s/epoch - 278ms/step
Epoch 624/1000
2023-10-31 15:16:16.066 
Epoch 624/1000 
	 loss: 31.3604, MinusLogProbMetric: 31.3604, val_loss: 32.0314, val_MinusLogProbMetric: 32.0314

Epoch 624: val_loss did not improve from 31.90538
196/196 - 54s - loss: 31.3604 - MinusLogProbMetric: 31.3604 - val_loss: 32.0314 - val_MinusLogProbMetric: 32.0314 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 625/1000
2023-10-31 15:17:11.490 
Epoch 625/1000 
	 loss: 31.3672, MinusLogProbMetric: 31.3672, val_loss: 32.0484, val_MinusLogProbMetric: 32.0484

Epoch 625: val_loss did not improve from 31.90538
196/196 - 55s - loss: 31.3672 - MinusLogProbMetric: 31.3672 - val_loss: 32.0484 - val_MinusLogProbMetric: 32.0484 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 626/1000
2023-10-31 15:18:04.479 
Epoch 626/1000 
	 loss: 31.3570, MinusLogProbMetric: 31.3570, val_loss: 32.0013, val_MinusLogProbMetric: 32.0013

Epoch 626: val_loss did not improve from 31.90538
196/196 - 53s - loss: 31.3570 - MinusLogProbMetric: 31.3570 - val_loss: 32.0013 - val_MinusLogProbMetric: 32.0013 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 627/1000
2023-10-31 15:19:00.051 
Epoch 627/1000 
	 loss: 31.3481, MinusLogProbMetric: 31.3481, val_loss: 31.9921, val_MinusLogProbMetric: 31.9921

Epoch 627: val_loss did not improve from 31.90538
196/196 - 56s - loss: 31.3481 - MinusLogProbMetric: 31.3481 - val_loss: 31.9921 - val_MinusLogProbMetric: 31.9921 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 628/1000
2023-10-31 15:19:53.683 
Epoch 628/1000 
	 loss: 31.3709, MinusLogProbMetric: 31.3709, val_loss: 31.9917, val_MinusLogProbMetric: 31.9917

Epoch 628: val_loss did not improve from 31.90538
196/196 - 54s - loss: 31.3709 - MinusLogProbMetric: 31.3709 - val_loss: 31.9917 - val_MinusLogProbMetric: 31.9917 - lr: 1.3717e-06 - 54s/epoch - 274ms/step
Epoch 629/1000
2023-10-31 15:20:48.839 
Epoch 629/1000 
	 loss: 31.3491, MinusLogProbMetric: 31.3491, val_loss: 31.9306, val_MinusLogProbMetric: 31.9306

Epoch 629: val_loss did not improve from 31.90538
196/196 - 55s - loss: 31.3491 - MinusLogProbMetric: 31.3491 - val_loss: 31.9306 - val_MinusLogProbMetric: 31.9306 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 630/1000
2023-10-31 15:21:43.441 
Epoch 630/1000 
	 loss: 31.3400, MinusLogProbMetric: 31.3400, val_loss: 31.9270, val_MinusLogProbMetric: 31.9270

Epoch 630: val_loss did not improve from 31.90538
196/196 - 55s - loss: 31.3400 - MinusLogProbMetric: 31.3400 - val_loss: 31.9270 - val_MinusLogProbMetric: 31.9270 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 631/1000
2023-10-31 15:22:37.652 
Epoch 631/1000 
	 loss: 31.3399, MinusLogProbMetric: 31.3399, val_loss: 32.0309, val_MinusLogProbMetric: 32.0309

Epoch 631: val_loss did not improve from 31.90538
196/196 - 54s - loss: 31.3399 - MinusLogProbMetric: 31.3399 - val_loss: 32.0309 - val_MinusLogProbMetric: 32.0309 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 632/1000
2023-10-31 15:23:29.079 
Epoch 632/1000 
	 loss: 31.3624, MinusLogProbMetric: 31.3624, val_loss: 32.0993, val_MinusLogProbMetric: 32.0993

Epoch 632: val_loss did not improve from 31.90538
196/196 - 51s - loss: 31.3624 - MinusLogProbMetric: 31.3624 - val_loss: 32.0993 - val_MinusLogProbMetric: 32.0993 - lr: 1.3717e-06 - 51s/epoch - 262ms/step
Epoch 633/1000
2023-10-31 15:24:29.604 
Epoch 633/1000 
	 loss: 31.3366, MinusLogProbMetric: 31.3366, val_loss: 32.1515, val_MinusLogProbMetric: 32.1515

Epoch 633: val_loss did not improve from 31.90538
196/196 - 61s - loss: 31.3366 - MinusLogProbMetric: 31.3366 - val_loss: 32.1515 - val_MinusLogProbMetric: 32.1515 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 634/1000
2023-10-31 15:25:21.905 
Epoch 634/1000 
	 loss: 31.3975, MinusLogProbMetric: 31.3975, val_loss: 31.9438, val_MinusLogProbMetric: 31.9438

Epoch 634: val_loss did not improve from 31.90538
196/196 - 52s - loss: 31.3975 - MinusLogProbMetric: 31.3975 - val_loss: 31.9438 - val_MinusLogProbMetric: 31.9438 - lr: 1.3717e-06 - 52s/epoch - 267ms/step
Epoch 635/1000
2023-10-31 15:26:13.523 
Epoch 635/1000 
	 loss: 31.3279, MinusLogProbMetric: 31.3279, val_loss: 31.9354, val_MinusLogProbMetric: 31.9354

Epoch 635: val_loss did not improve from 31.90538
196/196 - 52s - loss: 31.3279 - MinusLogProbMetric: 31.3279 - val_loss: 31.9354 - val_MinusLogProbMetric: 31.9354 - lr: 1.3717e-06 - 52s/epoch - 263ms/step
Epoch 636/1000
2023-10-31 15:27:04.922 
Epoch 636/1000 
	 loss: 31.3505, MinusLogProbMetric: 31.3505, val_loss: 31.9113, val_MinusLogProbMetric: 31.9113

Epoch 636: val_loss did not improve from 31.90538
196/196 - 51s - loss: 31.3505 - MinusLogProbMetric: 31.3505 - val_loss: 31.9113 - val_MinusLogProbMetric: 31.9113 - lr: 1.3717e-06 - 51s/epoch - 262ms/step
Epoch 637/1000
2023-10-31 15:27:54.681 
Epoch 637/1000 
	 loss: 31.3669, MinusLogProbMetric: 31.3669, val_loss: 32.0010, val_MinusLogProbMetric: 32.0010

Epoch 637: val_loss did not improve from 31.90538
196/196 - 50s - loss: 31.3669 - MinusLogProbMetric: 31.3669 - val_loss: 32.0010 - val_MinusLogProbMetric: 32.0010 - lr: 1.3717e-06 - 50s/epoch - 254ms/step
Epoch 638/1000
2023-10-31 15:28:54.970 
Epoch 638/1000 
	 loss: 31.3481, MinusLogProbMetric: 31.3481, val_loss: 31.9900, val_MinusLogProbMetric: 31.9900

Epoch 638: val_loss did not improve from 31.90538
196/196 - 60s - loss: 31.3481 - MinusLogProbMetric: 31.3481 - val_loss: 31.9900 - val_MinusLogProbMetric: 31.9900 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 639/1000
2023-10-31 15:29:45.756 
Epoch 639/1000 
	 loss: 31.3279, MinusLogProbMetric: 31.3279, val_loss: 32.0693, val_MinusLogProbMetric: 32.0693

Epoch 639: val_loss did not improve from 31.90538
196/196 - 51s - loss: 31.3279 - MinusLogProbMetric: 31.3279 - val_loss: 32.0693 - val_MinusLogProbMetric: 32.0693 - lr: 1.3717e-06 - 51s/epoch - 259ms/step
Epoch 640/1000
2023-10-31 15:30:39.449 
Epoch 640/1000 
	 loss: 31.3431, MinusLogProbMetric: 31.3431, val_loss: 31.9216, val_MinusLogProbMetric: 31.9216

Epoch 640: val_loss did not improve from 31.90538
196/196 - 54s - loss: 31.3431 - MinusLogProbMetric: 31.3431 - val_loss: 31.9216 - val_MinusLogProbMetric: 31.9216 - lr: 1.3717e-06 - 54s/epoch - 274ms/step
Epoch 641/1000
2023-10-31 15:31:34.035 
Epoch 641/1000 
	 loss: 31.3473, MinusLogProbMetric: 31.3473, val_loss: 32.0612, val_MinusLogProbMetric: 32.0612

Epoch 641: val_loss did not improve from 31.90538
196/196 - 55s - loss: 31.3473 - MinusLogProbMetric: 31.3473 - val_loss: 32.0612 - val_MinusLogProbMetric: 32.0612 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 642/1000
2023-10-31 15:32:22.599 
Epoch 642/1000 
	 loss: 31.3438, MinusLogProbMetric: 31.3438, val_loss: 31.9262, val_MinusLogProbMetric: 31.9262

Epoch 642: val_loss did not improve from 31.90538
196/196 - 49s - loss: 31.3438 - MinusLogProbMetric: 31.3438 - val_loss: 31.9262 - val_MinusLogProbMetric: 31.9262 - lr: 1.3717e-06 - 49s/epoch - 248ms/step
Epoch 643/1000
2023-10-31 15:33:15.253 
Epoch 643/1000 
	 loss: 31.3486, MinusLogProbMetric: 31.3486, val_loss: 31.9187, val_MinusLogProbMetric: 31.9187

Epoch 643: val_loss did not improve from 31.90538
196/196 - 53s - loss: 31.3486 - MinusLogProbMetric: 31.3486 - val_loss: 31.9187 - val_MinusLogProbMetric: 31.9187 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 644/1000
2023-10-31 15:34:13.162 
Epoch 644/1000 
	 loss: 31.3356, MinusLogProbMetric: 31.3356, val_loss: 31.9576, val_MinusLogProbMetric: 31.9576

Epoch 644: val_loss did not improve from 31.90538
196/196 - 58s - loss: 31.3356 - MinusLogProbMetric: 31.3356 - val_loss: 31.9576 - val_MinusLogProbMetric: 31.9576 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 645/1000
2023-10-31 15:35:11.266 
Epoch 645/1000 
	 loss: 31.3448, MinusLogProbMetric: 31.3448, val_loss: 32.0373, val_MinusLogProbMetric: 32.0373

Epoch 645: val_loss did not improve from 31.90538
196/196 - 58s - loss: 31.3448 - MinusLogProbMetric: 31.3448 - val_loss: 32.0373 - val_MinusLogProbMetric: 32.0373 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 646/1000
2023-10-31 15:36:07.181 
Epoch 646/1000 
	 loss: 31.3294, MinusLogProbMetric: 31.3294, val_loss: 31.8700, val_MinusLogProbMetric: 31.8700

Epoch 646: val_loss improved from 31.90538 to 31.86997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 31.3294 - MinusLogProbMetric: 31.3294 - val_loss: 31.8700 - val_MinusLogProbMetric: 31.8700 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 647/1000
2023-10-31 15:37:00.782 
Epoch 647/1000 
	 loss: 31.3318, MinusLogProbMetric: 31.3318, val_loss: 31.9354, val_MinusLogProbMetric: 31.9354

Epoch 647: val_loss did not improve from 31.86997
196/196 - 53s - loss: 31.3318 - MinusLogProbMetric: 31.3318 - val_loss: 31.9354 - val_MinusLogProbMetric: 31.9354 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 648/1000
2023-10-31 15:37:58.779 
Epoch 648/1000 
	 loss: 31.3129, MinusLogProbMetric: 31.3129, val_loss: 32.0773, val_MinusLogProbMetric: 32.0773

Epoch 648: val_loss did not improve from 31.86997
196/196 - 58s - loss: 31.3129 - MinusLogProbMetric: 31.3129 - val_loss: 32.0773 - val_MinusLogProbMetric: 32.0773 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 649/1000
2023-10-31 15:38:55.632 
Epoch 649/1000 
	 loss: 31.3326, MinusLogProbMetric: 31.3326, val_loss: 31.9810, val_MinusLogProbMetric: 31.9810

Epoch 649: val_loss did not improve from 31.86997
196/196 - 57s - loss: 31.3326 - MinusLogProbMetric: 31.3326 - val_loss: 31.9810 - val_MinusLogProbMetric: 31.9810 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 650/1000
2023-10-31 15:39:55.070 
Epoch 650/1000 
	 loss: 31.3218, MinusLogProbMetric: 31.3218, val_loss: 32.0639, val_MinusLogProbMetric: 32.0639

Epoch 650: val_loss did not improve from 31.86997
196/196 - 59s - loss: 31.3218 - MinusLogProbMetric: 31.3218 - val_loss: 32.0639 - val_MinusLogProbMetric: 32.0639 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 651/1000
2023-10-31 15:40:46.631 
Epoch 651/1000 
	 loss: 31.3402, MinusLogProbMetric: 31.3402, val_loss: 31.9174, val_MinusLogProbMetric: 31.9174

Epoch 651: val_loss did not improve from 31.86997
196/196 - 52s - loss: 31.3402 - MinusLogProbMetric: 31.3402 - val_loss: 31.9174 - val_MinusLogProbMetric: 31.9174 - lr: 1.3717e-06 - 52s/epoch - 263ms/step
Epoch 652/1000
2023-10-31 15:41:39.139 
Epoch 652/1000 
	 loss: 31.3219, MinusLogProbMetric: 31.3219, val_loss: 31.9081, val_MinusLogProbMetric: 31.9081

Epoch 652: val_loss did not improve from 31.86997
196/196 - 53s - loss: 31.3219 - MinusLogProbMetric: 31.3219 - val_loss: 31.9081 - val_MinusLogProbMetric: 31.9081 - lr: 1.3717e-06 - 53s/epoch - 268ms/step
Epoch 653/1000
2023-10-31 15:42:32.207 
Epoch 653/1000 
	 loss: 31.3168, MinusLogProbMetric: 31.3168, val_loss: 31.8925, val_MinusLogProbMetric: 31.8925

Epoch 653: val_loss did not improve from 31.86997
196/196 - 53s - loss: 31.3168 - MinusLogProbMetric: 31.3168 - val_loss: 31.8925 - val_MinusLogProbMetric: 31.8925 - lr: 1.3717e-06 - 53s/epoch - 271ms/step
Epoch 654/1000
2023-10-31 15:43:29.199 
Epoch 654/1000 
	 loss: 31.3155, MinusLogProbMetric: 31.3155, val_loss: 31.9209, val_MinusLogProbMetric: 31.9209

Epoch 654: val_loss did not improve from 31.86997
196/196 - 57s - loss: 31.3155 - MinusLogProbMetric: 31.3155 - val_loss: 31.9209 - val_MinusLogProbMetric: 31.9209 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 655/1000
2023-10-31 15:44:26.633 
Epoch 655/1000 
	 loss: 31.3207, MinusLogProbMetric: 31.3207, val_loss: 32.0576, val_MinusLogProbMetric: 32.0576

Epoch 655: val_loss did not improve from 31.86997
196/196 - 57s - loss: 31.3207 - MinusLogProbMetric: 31.3207 - val_loss: 32.0576 - val_MinusLogProbMetric: 32.0576 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 656/1000
2023-10-31 15:45:18.610 
Epoch 656/1000 
	 loss: 31.3066, MinusLogProbMetric: 31.3066, val_loss: 31.9241, val_MinusLogProbMetric: 31.9241

Epoch 656: val_loss did not improve from 31.86997
196/196 - 52s - loss: 31.3066 - MinusLogProbMetric: 31.3066 - val_loss: 31.9241 - val_MinusLogProbMetric: 31.9241 - lr: 1.3717e-06 - 52s/epoch - 265ms/step
Epoch 657/1000
2023-10-31 15:46:08.238 
Epoch 657/1000 
	 loss: 31.3137, MinusLogProbMetric: 31.3137, val_loss: 31.9386, val_MinusLogProbMetric: 31.9386

Epoch 657: val_loss did not improve from 31.86997
196/196 - 50s - loss: 31.3137 - MinusLogProbMetric: 31.3137 - val_loss: 31.9386 - val_MinusLogProbMetric: 31.9386 - lr: 1.3717e-06 - 50s/epoch - 253ms/step
Epoch 658/1000
2023-10-31 15:47:00.587 
Epoch 658/1000 
	 loss: 31.3071, MinusLogProbMetric: 31.3071, val_loss: 31.9106, val_MinusLogProbMetric: 31.9106

Epoch 658: val_loss did not improve from 31.86997
196/196 - 52s - loss: 31.3071 - MinusLogProbMetric: 31.3071 - val_loss: 31.9106 - val_MinusLogProbMetric: 31.9106 - lr: 1.3717e-06 - 52s/epoch - 267ms/step
Epoch 659/1000
2023-10-31 15:47:51.608 
Epoch 659/1000 
	 loss: 31.3610, MinusLogProbMetric: 31.3610, val_loss: 31.9065, val_MinusLogProbMetric: 31.9065

Epoch 659: val_loss did not improve from 31.86997
196/196 - 51s - loss: 31.3610 - MinusLogProbMetric: 31.3610 - val_loss: 31.9065 - val_MinusLogProbMetric: 31.9065 - lr: 1.3717e-06 - 51s/epoch - 260ms/step
Epoch 660/1000
2023-10-31 15:48:49.595 
Epoch 660/1000 
	 loss: 31.3175, MinusLogProbMetric: 31.3175, val_loss: 31.8612, val_MinusLogProbMetric: 31.8612

Epoch 660: val_loss improved from 31.86997 to 31.86123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 31.3175 - MinusLogProbMetric: 31.3175 - val_loss: 31.8612 - val_MinusLogProbMetric: 31.8612 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 661/1000
2023-10-31 15:49:48.764 
Epoch 661/1000 
	 loss: 31.2944, MinusLogProbMetric: 31.2944, val_loss: 31.9494, val_MinusLogProbMetric: 31.9494

Epoch 661: val_loss did not improve from 31.86123
196/196 - 58s - loss: 31.2944 - MinusLogProbMetric: 31.2944 - val_loss: 31.9494 - val_MinusLogProbMetric: 31.9494 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 662/1000
2023-10-31 15:50:47.995 
Epoch 662/1000 
	 loss: 31.2994, MinusLogProbMetric: 31.2994, val_loss: 31.9889, val_MinusLogProbMetric: 31.9889

Epoch 662: val_loss did not improve from 31.86123
196/196 - 59s - loss: 31.2994 - MinusLogProbMetric: 31.2994 - val_loss: 31.9889 - val_MinusLogProbMetric: 31.9889 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 663/1000
2023-10-31 15:51:50.585 
Epoch 663/1000 
	 loss: 31.2954, MinusLogProbMetric: 31.2954, val_loss: 31.8638, val_MinusLogProbMetric: 31.8638

Epoch 663: val_loss did not improve from 31.86123
196/196 - 63s - loss: 31.2954 - MinusLogProbMetric: 31.2954 - val_loss: 31.8638 - val_MinusLogProbMetric: 31.8638 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 664/1000
2023-10-31 15:52:48.142 
Epoch 664/1000 
	 loss: 31.2875, MinusLogProbMetric: 31.2875, val_loss: 31.9145, val_MinusLogProbMetric: 31.9145

Epoch 664: val_loss did not improve from 31.86123
196/196 - 58s - loss: 31.2875 - MinusLogProbMetric: 31.2875 - val_loss: 31.9145 - val_MinusLogProbMetric: 31.9145 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 665/1000
2023-10-31 15:53:46.272 
Epoch 665/1000 
	 loss: 31.3094, MinusLogProbMetric: 31.3094, val_loss: 31.9192, val_MinusLogProbMetric: 31.9192

Epoch 665: val_loss did not improve from 31.86123
196/196 - 58s - loss: 31.3094 - MinusLogProbMetric: 31.3094 - val_loss: 31.9192 - val_MinusLogProbMetric: 31.9192 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 666/1000
2023-10-31 15:54:46.510 
Epoch 666/1000 
	 loss: 31.3086, MinusLogProbMetric: 31.3086, val_loss: 31.8934, val_MinusLogProbMetric: 31.8934

Epoch 666: val_loss did not improve from 31.86123
196/196 - 60s - loss: 31.3086 - MinusLogProbMetric: 31.3086 - val_loss: 31.8934 - val_MinusLogProbMetric: 31.8934 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 667/1000
2023-10-31 15:55:48.638 
Epoch 667/1000 
	 loss: 31.3038, MinusLogProbMetric: 31.3038, val_loss: 32.0633, val_MinusLogProbMetric: 32.0633

Epoch 667: val_loss did not improve from 31.86123
196/196 - 62s - loss: 31.3038 - MinusLogProbMetric: 31.3038 - val_loss: 32.0633 - val_MinusLogProbMetric: 32.0633 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 668/1000
2023-10-31 15:56:52.214 
Epoch 668/1000 
	 loss: 31.3087, MinusLogProbMetric: 31.3087, val_loss: 31.8859, val_MinusLogProbMetric: 31.8859

Epoch 668: val_loss did not improve from 31.86123
196/196 - 64s - loss: 31.3087 - MinusLogProbMetric: 31.3087 - val_loss: 31.8859 - val_MinusLogProbMetric: 31.8859 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 669/1000
2023-10-31 15:57:54.645 
Epoch 669/1000 
	 loss: 31.2970, MinusLogProbMetric: 31.2970, val_loss: 31.9622, val_MinusLogProbMetric: 31.9622

Epoch 669: val_loss did not improve from 31.86123
196/196 - 62s - loss: 31.2970 - MinusLogProbMetric: 31.2970 - val_loss: 31.9622 - val_MinusLogProbMetric: 31.9622 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 670/1000
2023-10-31 15:59:00.817 
Epoch 670/1000 
	 loss: 31.3194, MinusLogProbMetric: 31.3194, val_loss: 31.9147, val_MinusLogProbMetric: 31.9147

Epoch 670: val_loss did not improve from 31.86123
196/196 - 66s - loss: 31.3194 - MinusLogProbMetric: 31.3194 - val_loss: 31.9147 - val_MinusLogProbMetric: 31.9147 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 671/1000
2023-10-31 16:00:05.364 
Epoch 671/1000 
	 loss: 31.3034, MinusLogProbMetric: 31.3034, val_loss: 31.9193, val_MinusLogProbMetric: 31.9193

Epoch 671: val_loss did not improve from 31.86123
196/196 - 65s - loss: 31.3034 - MinusLogProbMetric: 31.3034 - val_loss: 31.9193 - val_MinusLogProbMetric: 31.9193 - lr: 1.3717e-06 - 65s/epoch - 329ms/step
Epoch 672/1000
2023-10-31 16:01:08.760 
Epoch 672/1000 
	 loss: 31.3030, MinusLogProbMetric: 31.3030, val_loss: 31.9787, val_MinusLogProbMetric: 31.9787

Epoch 672: val_loss did not improve from 31.86123
196/196 - 63s - loss: 31.3030 - MinusLogProbMetric: 31.3030 - val_loss: 31.9787 - val_MinusLogProbMetric: 31.9787 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 673/1000
2023-10-31 16:02:01.680 
Epoch 673/1000 
	 loss: 31.3323, MinusLogProbMetric: 31.3323, val_loss: 31.9439, val_MinusLogProbMetric: 31.9439

Epoch 673: val_loss did not improve from 31.86123
196/196 - 53s - loss: 31.3323 - MinusLogProbMetric: 31.3323 - val_loss: 31.9439 - val_MinusLogProbMetric: 31.9439 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 674/1000
2023-10-31 16:02:56.090 
Epoch 674/1000 
	 loss: 31.2948, MinusLogProbMetric: 31.2948, val_loss: 32.0310, val_MinusLogProbMetric: 32.0310

Epoch 674: val_loss did not improve from 31.86123
196/196 - 54s - loss: 31.2948 - MinusLogProbMetric: 31.2948 - val_loss: 32.0310 - val_MinusLogProbMetric: 32.0310 - lr: 1.3717e-06 - 54s/epoch - 278ms/step
Epoch 675/1000
2023-10-31 16:03:47.678 
Epoch 675/1000 
	 loss: 31.2956, MinusLogProbMetric: 31.2956, val_loss: 31.9482, val_MinusLogProbMetric: 31.9482

Epoch 675: val_loss did not improve from 31.86123
196/196 - 52s - loss: 31.2956 - MinusLogProbMetric: 31.2956 - val_loss: 31.9482 - val_MinusLogProbMetric: 31.9482 - lr: 1.3717e-06 - 52s/epoch - 263ms/step
Epoch 676/1000
2023-10-31 16:04:42.490 
Epoch 676/1000 
	 loss: 31.3119, MinusLogProbMetric: 31.3119, val_loss: 31.9251, val_MinusLogProbMetric: 31.9251

Epoch 676: val_loss did not improve from 31.86123
196/196 - 55s - loss: 31.3119 - MinusLogProbMetric: 31.3119 - val_loss: 31.9251 - val_MinusLogProbMetric: 31.9251 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 677/1000
2023-10-31 16:05:34.363 
Epoch 677/1000 
	 loss: 31.3160, MinusLogProbMetric: 31.3160, val_loss: 31.8762, val_MinusLogProbMetric: 31.8762

Epoch 677: val_loss did not improve from 31.86123
196/196 - 52s - loss: 31.3160 - MinusLogProbMetric: 31.3160 - val_loss: 31.8762 - val_MinusLogProbMetric: 31.8762 - lr: 1.3717e-06 - 52s/epoch - 265ms/step
Epoch 678/1000
2023-10-31 16:06:30.778 
Epoch 678/1000 
	 loss: 31.2819, MinusLogProbMetric: 31.2819, val_loss: 31.9012, val_MinusLogProbMetric: 31.9012

Epoch 678: val_loss did not improve from 31.86123
196/196 - 56s - loss: 31.2819 - MinusLogProbMetric: 31.2819 - val_loss: 31.9012 - val_MinusLogProbMetric: 31.9012 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 679/1000
2023-10-31 16:07:27.035 
Epoch 679/1000 
	 loss: 31.2620, MinusLogProbMetric: 31.2620, val_loss: 31.8900, val_MinusLogProbMetric: 31.8900

Epoch 679: val_loss did not improve from 31.86123
196/196 - 56s - loss: 31.2620 - MinusLogProbMetric: 31.2620 - val_loss: 31.8900 - val_MinusLogProbMetric: 31.8900 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 680/1000
2023-10-31 16:08:25.104 
Epoch 680/1000 
	 loss: 31.2890, MinusLogProbMetric: 31.2890, val_loss: 31.8716, val_MinusLogProbMetric: 31.8716

Epoch 680: val_loss did not improve from 31.86123
196/196 - 58s - loss: 31.2890 - MinusLogProbMetric: 31.2890 - val_loss: 31.8716 - val_MinusLogProbMetric: 31.8716 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 681/1000
2023-10-31 16:09:19.362 
Epoch 681/1000 
	 loss: 31.2776, MinusLogProbMetric: 31.2776, val_loss: 31.9681, val_MinusLogProbMetric: 31.9681

Epoch 681: val_loss did not improve from 31.86123
196/196 - 54s - loss: 31.2776 - MinusLogProbMetric: 31.2776 - val_loss: 31.9681 - val_MinusLogProbMetric: 31.9681 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 682/1000
2023-10-31 16:10:10.801 
Epoch 682/1000 
	 loss: 31.2914, MinusLogProbMetric: 31.2914, val_loss: 31.9189, val_MinusLogProbMetric: 31.9189

Epoch 682: val_loss did not improve from 31.86123
196/196 - 51s - loss: 31.2914 - MinusLogProbMetric: 31.2914 - val_loss: 31.9189 - val_MinusLogProbMetric: 31.9189 - lr: 1.3717e-06 - 51s/epoch - 262ms/step
Epoch 683/1000
2023-10-31 16:11:02.473 
Epoch 683/1000 
	 loss: 31.2929, MinusLogProbMetric: 31.2929, val_loss: 31.8487, val_MinusLogProbMetric: 31.8487

Epoch 683: val_loss improved from 31.86123 to 31.84868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 31.2929 - MinusLogProbMetric: 31.2929 - val_loss: 31.8487 - val_MinusLogProbMetric: 31.8487 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 684/1000
2023-10-31 16:11:59.539 
Epoch 684/1000 
	 loss: 31.2597, MinusLogProbMetric: 31.2597, val_loss: 31.8597, val_MinusLogProbMetric: 31.8597

Epoch 684: val_loss did not improve from 31.84868
196/196 - 56s - loss: 31.2597 - MinusLogProbMetric: 31.2597 - val_loss: 31.8597 - val_MinusLogProbMetric: 31.8597 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 685/1000
2023-10-31 16:12:56.770 
Epoch 685/1000 
	 loss: 31.2927, MinusLogProbMetric: 31.2927, val_loss: 31.8393, val_MinusLogProbMetric: 31.8393

Epoch 685: val_loss improved from 31.84868 to 31.83932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 31.2927 - MinusLogProbMetric: 31.2927 - val_loss: 31.8393 - val_MinusLogProbMetric: 31.8393 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 686/1000
2023-10-31 16:13:51.920 
Epoch 686/1000 
	 loss: 31.2651, MinusLogProbMetric: 31.2651, val_loss: 31.8791, val_MinusLogProbMetric: 31.8791

Epoch 686: val_loss did not improve from 31.83932
196/196 - 54s - loss: 31.2651 - MinusLogProbMetric: 31.2651 - val_loss: 31.8791 - val_MinusLogProbMetric: 31.8791 - lr: 1.3717e-06 - 54s/epoch - 276ms/step
Epoch 687/1000
2023-10-31 16:14:43.338 
Epoch 687/1000 
	 loss: 31.2738, MinusLogProbMetric: 31.2738, val_loss: 31.9005, val_MinusLogProbMetric: 31.9005

Epoch 687: val_loss did not improve from 31.83932
196/196 - 51s - loss: 31.2738 - MinusLogProbMetric: 31.2738 - val_loss: 31.9005 - val_MinusLogProbMetric: 31.9005 - lr: 1.3717e-06 - 51s/epoch - 262ms/step
Epoch 688/1000
2023-10-31 16:15:35.733 
Epoch 688/1000 
	 loss: 31.2890, MinusLogProbMetric: 31.2890, val_loss: 31.8974, val_MinusLogProbMetric: 31.8974

Epoch 688: val_loss did not improve from 31.83932
196/196 - 52s - loss: 31.2890 - MinusLogProbMetric: 31.2890 - val_loss: 31.8974 - val_MinusLogProbMetric: 31.8974 - lr: 1.3717e-06 - 52s/epoch - 267ms/step
Epoch 689/1000
2023-10-31 16:16:25.440 
Epoch 689/1000 
	 loss: 31.2648, MinusLogProbMetric: 31.2648, val_loss: 31.8888, val_MinusLogProbMetric: 31.8888

Epoch 689: val_loss did not improve from 31.83932
196/196 - 50s - loss: 31.2648 - MinusLogProbMetric: 31.2648 - val_loss: 31.8888 - val_MinusLogProbMetric: 31.8888 - lr: 1.3717e-06 - 50s/epoch - 254ms/step
Epoch 690/1000
2023-10-31 16:17:13.726 
Epoch 690/1000 
	 loss: 31.2538, MinusLogProbMetric: 31.2538, val_loss: 31.9707, val_MinusLogProbMetric: 31.9707

Epoch 690: val_loss did not improve from 31.83932
196/196 - 48s - loss: 31.2538 - MinusLogProbMetric: 31.2538 - val_loss: 31.9707 - val_MinusLogProbMetric: 31.9707 - lr: 1.3717e-06 - 48s/epoch - 246ms/step
Epoch 691/1000
2023-10-31 16:18:08.178 
Epoch 691/1000 
	 loss: 31.2840, MinusLogProbMetric: 31.2840, val_loss: 31.8830, val_MinusLogProbMetric: 31.8830

Epoch 691: val_loss did not improve from 31.83932
196/196 - 54s - loss: 31.2840 - MinusLogProbMetric: 31.2840 - val_loss: 31.8830 - val_MinusLogProbMetric: 31.8830 - lr: 1.3717e-06 - 54s/epoch - 278ms/step
Epoch 692/1000
2023-10-31 16:19:02.048 
Epoch 692/1000 
	 loss: 31.2636, MinusLogProbMetric: 31.2636, val_loss: 31.8325, val_MinusLogProbMetric: 31.8325

Epoch 692: val_loss improved from 31.83932 to 31.83250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 31.2636 - MinusLogProbMetric: 31.2636 - val_loss: 31.8325 - val_MinusLogProbMetric: 31.8325 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 693/1000
2023-10-31 16:19:58.980 
Epoch 693/1000 
	 loss: 31.2424, MinusLogProbMetric: 31.2424, val_loss: 31.9141, val_MinusLogProbMetric: 31.9141

Epoch 693: val_loss did not improve from 31.83250
196/196 - 56s - loss: 31.2424 - MinusLogProbMetric: 31.2424 - val_loss: 31.9141 - val_MinusLogProbMetric: 31.9141 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 694/1000
2023-10-31 16:20:53.739 
Epoch 694/1000 
	 loss: 31.2567, MinusLogProbMetric: 31.2567, val_loss: 31.9171, val_MinusLogProbMetric: 31.9171

Epoch 694: val_loss did not improve from 31.83250
196/196 - 55s - loss: 31.2567 - MinusLogProbMetric: 31.2567 - val_loss: 31.9171 - val_MinusLogProbMetric: 31.9171 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 695/1000
2023-10-31 16:21:42.662 
Epoch 695/1000 
	 loss: 31.2709, MinusLogProbMetric: 31.2709, val_loss: 31.8564, val_MinusLogProbMetric: 31.8564

Epoch 695: val_loss did not improve from 31.83250
196/196 - 49s - loss: 31.2709 - MinusLogProbMetric: 31.2709 - val_loss: 31.8564 - val_MinusLogProbMetric: 31.8564 - lr: 1.3717e-06 - 49s/epoch - 250ms/step
Epoch 696/1000
2023-10-31 16:22:37.657 
Epoch 696/1000 
	 loss: 31.2663, MinusLogProbMetric: 31.2663, val_loss: 31.8930, val_MinusLogProbMetric: 31.8930

Epoch 696: val_loss did not improve from 31.83250
196/196 - 55s - loss: 31.2663 - MinusLogProbMetric: 31.2663 - val_loss: 31.8930 - val_MinusLogProbMetric: 31.8930 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 697/1000
2023-10-31 16:23:31.365 
Epoch 697/1000 
	 loss: 31.2541, MinusLogProbMetric: 31.2541, val_loss: 31.9641, val_MinusLogProbMetric: 31.9641

Epoch 697: val_loss did not improve from 31.83250
196/196 - 54s - loss: 31.2541 - MinusLogProbMetric: 31.2541 - val_loss: 31.9641 - val_MinusLogProbMetric: 31.9641 - lr: 1.3717e-06 - 54s/epoch - 274ms/step
Epoch 698/1000
2023-10-31 16:24:20.784 
Epoch 698/1000 
	 loss: 31.2482, MinusLogProbMetric: 31.2482, val_loss: 31.8746, val_MinusLogProbMetric: 31.8746

Epoch 698: val_loss did not improve from 31.83250
196/196 - 49s - loss: 31.2482 - MinusLogProbMetric: 31.2482 - val_loss: 31.8746 - val_MinusLogProbMetric: 31.8746 - lr: 1.3717e-06 - 49s/epoch - 252ms/step
Epoch 699/1000
2023-10-31 16:25:15.874 
Epoch 699/1000 
	 loss: 31.2498, MinusLogProbMetric: 31.2498, val_loss: 31.9125, val_MinusLogProbMetric: 31.9125

Epoch 699: val_loss did not improve from 31.83250
196/196 - 55s - loss: 31.2498 - MinusLogProbMetric: 31.2498 - val_loss: 31.9125 - val_MinusLogProbMetric: 31.9125 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 700/1000
2023-10-31 16:26:13.338 
Epoch 700/1000 
	 loss: 31.2598, MinusLogProbMetric: 31.2598, val_loss: 31.9056, val_MinusLogProbMetric: 31.9056

Epoch 700: val_loss did not improve from 31.83250
196/196 - 57s - loss: 31.2598 - MinusLogProbMetric: 31.2598 - val_loss: 31.9056 - val_MinusLogProbMetric: 31.9056 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 701/1000
2023-10-31 16:27:09.776 
Epoch 701/1000 
	 loss: 31.2681, MinusLogProbMetric: 31.2681, val_loss: 31.8869, val_MinusLogProbMetric: 31.8869

Epoch 701: val_loss did not improve from 31.83250
196/196 - 56s - loss: 31.2681 - MinusLogProbMetric: 31.2681 - val_loss: 31.8869 - val_MinusLogProbMetric: 31.8869 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 702/1000
2023-10-31 16:28:07.638 
Epoch 702/1000 
	 loss: 31.2482, MinusLogProbMetric: 31.2482, val_loss: 31.8384, val_MinusLogProbMetric: 31.8384

Epoch 702: val_loss did not improve from 31.83250
196/196 - 58s - loss: 31.2482 - MinusLogProbMetric: 31.2482 - val_loss: 31.8384 - val_MinusLogProbMetric: 31.8384 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 703/1000
2023-10-31 16:29:02.535 
Epoch 703/1000 
	 loss: 31.2473, MinusLogProbMetric: 31.2473, val_loss: 31.9111, val_MinusLogProbMetric: 31.9111

Epoch 703: val_loss did not improve from 31.83250
196/196 - 55s - loss: 31.2473 - MinusLogProbMetric: 31.2473 - val_loss: 31.9111 - val_MinusLogProbMetric: 31.9111 - lr: 1.3717e-06 - 55s/epoch - 280ms/step
Epoch 704/1000
2023-10-31 16:29:57.597 
Epoch 704/1000 
	 loss: 31.2598, MinusLogProbMetric: 31.2598, val_loss: 31.8945, val_MinusLogProbMetric: 31.8945

Epoch 704: val_loss did not improve from 31.83250
196/196 - 55s - loss: 31.2598 - MinusLogProbMetric: 31.2598 - val_loss: 31.8945 - val_MinusLogProbMetric: 31.8945 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 705/1000
2023-10-31 16:30:54.387 
Epoch 705/1000 
	 loss: 31.2631, MinusLogProbMetric: 31.2631, val_loss: 31.8780, val_MinusLogProbMetric: 31.8780

Epoch 705: val_loss did not improve from 31.83250
196/196 - 57s - loss: 31.2631 - MinusLogProbMetric: 31.2631 - val_loss: 31.8780 - val_MinusLogProbMetric: 31.8780 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 706/1000
2023-10-31 16:31:48.685 
Epoch 706/1000 
	 loss: 31.2257, MinusLogProbMetric: 31.2257, val_loss: 31.8631, val_MinusLogProbMetric: 31.8631

Epoch 706: val_loss did not improve from 31.83250
196/196 - 54s - loss: 31.2257 - MinusLogProbMetric: 31.2257 - val_loss: 31.8631 - val_MinusLogProbMetric: 31.8631 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 707/1000
2023-10-31 16:32:41.066 
Epoch 707/1000 
	 loss: 31.2603, MinusLogProbMetric: 31.2603, val_loss: 31.9514, val_MinusLogProbMetric: 31.9514

Epoch 707: val_loss did not improve from 31.83250
196/196 - 52s - loss: 31.2603 - MinusLogProbMetric: 31.2603 - val_loss: 31.9514 - val_MinusLogProbMetric: 31.9514 - lr: 1.3717e-06 - 52s/epoch - 267ms/step
Epoch 708/1000
2023-10-31 16:33:37.004 
Epoch 708/1000 
	 loss: 31.2483, MinusLogProbMetric: 31.2483, val_loss: 31.8588, val_MinusLogProbMetric: 31.8588

Epoch 708: val_loss did not improve from 31.83250
196/196 - 56s - loss: 31.2483 - MinusLogProbMetric: 31.2483 - val_loss: 31.8588 - val_MinusLogProbMetric: 31.8588 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 709/1000
2023-10-31 16:34:34.251 
Epoch 709/1000 
	 loss: 31.2453, MinusLogProbMetric: 31.2453, val_loss: 31.8887, val_MinusLogProbMetric: 31.8887

Epoch 709: val_loss did not improve from 31.83250
196/196 - 57s - loss: 31.2453 - MinusLogProbMetric: 31.2453 - val_loss: 31.8887 - val_MinusLogProbMetric: 31.8887 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 710/1000
2023-10-31 16:35:31.547 
Epoch 710/1000 
	 loss: 31.2529, MinusLogProbMetric: 31.2529, val_loss: 31.8055, val_MinusLogProbMetric: 31.8055

Epoch 710: val_loss improved from 31.83250 to 31.80553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 31.2529 - MinusLogProbMetric: 31.2529 - val_loss: 31.8055 - val_MinusLogProbMetric: 31.8055 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 711/1000
2023-10-31 16:36:28.689 
Epoch 711/1000 
	 loss: 31.2341, MinusLogProbMetric: 31.2341, val_loss: 31.9157, val_MinusLogProbMetric: 31.9157

Epoch 711: val_loss did not improve from 31.80553
196/196 - 56s - loss: 31.2341 - MinusLogProbMetric: 31.2341 - val_loss: 31.9157 - val_MinusLogProbMetric: 31.9157 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 712/1000
2023-10-31 16:37:25.254 
Epoch 712/1000 
	 loss: 31.2333, MinusLogProbMetric: 31.2333, val_loss: 31.8097, val_MinusLogProbMetric: 31.8097

Epoch 712: val_loss did not improve from 31.80553
196/196 - 57s - loss: 31.2333 - MinusLogProbMetric: 31.2333 - val_loss: 31.8097 - val_MinusLogProbMetric: 31.8097 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 713/1000
2023-10-31 16:38:17.291 
Epoch 713/1000 
	 loss: 31.2477, MinusLogProbMetric: 31.2477, val_loss: 31.7998, val_MinusLogProbMetric: 31.7998

Epoch 713: val_loss improved from 31.80553 to 31.79977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 31.2477 - MinusLogProbMetric: 31.2477 - val_loss: 31.7998 - val_MinusLogProbMetric: 31.7998 - lr: 1.3717e-06 - 53s/epoch - 271ms/step
Epoch 714/1000
2023-10-31 16:39:11.135 
Epoch 714/1000 
	 loss: 31.2206, MinusLogProbMetric: 31.2206, val_loss: 31.8963, val_MinusLogProbMetric: 31.8963

Epoch 714: val_loss did not improve from 31.79977
196/196 - 53s - loss: 31.2206 - MinusLogProbMetric: 31.2206 - val_loss: 31.8963 - val_MinusLogProbMetric: 31.8963 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 715/1000
2023-10-31 16:40:07.156 
Epoch 715/1000 
	 loss: 31.2401, MinusLogProbMetric: 31.2401, val_loss: 31.9324, val_MinusLogProbMetric: 31.9324

Epoch 715: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.2401 - MinusLogProbMetric: 31.2401 - val_loss: 31.9324 - val_MinusLogProbMetric: 31.9324 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 716/1000
2023-10-31 16:41:05.358 
Epoch 716/1000 
	 loss: 31.2237, MinusLogProbMetric: 31.2237, val_loss: 31.8916, val_MinusLogProbMetric: 31.8916

Epoch 716: val_loss did not improve from 31.79977
196/196 - 58s - loss: 31.2237 - MinusLogProbMetric: 31.2237 - val_loss: 31.8916 - val_MinusLogProbMetric: 31.8916 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 717/1000
2023-10-31 16:42:02.298 
Epoch 717/1000 
	 loss: 31.2348, MinusLogProbMetric: 31.2348, val_loss: 31.9638, val_MinusLogProbMetric: 31.9638

Epoch 717: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.2348 - MinusLogProbMetric: 31.2348 - val_loss: 31.9638 - val_MinusLogProbMetric: 31.9638 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 718/1000
2023-10-31 16:42:59.861 
Epoch 718/1000 
	 loss: 39.1188, MinusLogProbMetric: 39.1188, val_loss: 41.8216, val_MinusLogProbMetric: 41.8216

Epoch 718: val_loss did not improve from 31.79977
196/196 - 58s - loss: 39.1188 - MinusLogProbMetric: 39.1188 - val_loss: 41.8216 - val_MinusLogProbMetric: 41.8216 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 719/1000
2023-10-31 16:43:54.013 
Epoch 719/1000 
	 loss: 35.7078, MinusLogProbMetric: 35.7078, val_loss: 34.8993, val_MinusLogProbMetric: 34.8993

Epoch 719: val_loss did not improve from 31.79977
196/196 - 54s - loss: 35.7078 - MinusLogProbMetric: 35.7078 - val_loss: 34.8993 - val_MinusLogProbMetric: 34.8993 - lr: 1.3717e-06 - 54s/epoch - 276ms/step
Epoch 720/1000
2023-10-31 16:44:43.612 
Epoch 720/1000 
	 loss: 33.9942, MinusLogProbMetric: 33.9942, val_loss: 34.3622, val_MinusLogProbMetric: 34.3622

Epoch 720: val_loss did not improve from 31.79977
196/196 - 50s - loss: 33.9942 - MinusLogProbMetric: 33.9942 - val_loss: 34.3622 - val_MinusLogProbMetric: 34.3622 - lr: 1.3717e-06 - 50s/epoch - 253ms/step
Epoch 721/1000
2023-10-31 16:45:31.741 
Epoch 721/1000 
	 loss: 33.5588, MinusLogProbMetric: 33.5588, val_loss: 33.9752, val_MinusLogProbMetric: 33.9752

Epoch 721: val_loss did not improve from 31.79977
196/196 - 48s - loss: 33.5588 - MinusLogProbMetric: 33.5588 - val_loss: 33.9752 - val_MinusLogProbMetric: 33.9752 - lr: 1.3717e-06 - 48s/epoch - 246ms/step
Epoch 722/1000
2023-10-31 16:46:21.632 
Epoch 722/1000 
	 loss: 33.3132, MinusLogProbMetric: 33.3132, val_loss: 33.6931, val_MinusLogProbMetric: 33.6931

Epoch 722: val_loss did not improve from 31.79977
196/196 - 50s - loss: 33.3132 - MinusLogProbMetric: 33.3132 - val_loss: 33.6931 - val_MinusLogProbMetric: 33.6931 - lr: 1.3717e-06 - 50s/epoch - 255ms/step
Epoch 723/1000
2023-10-31 16:47:20.015 
Epoch 723/1000 
	 loss: 32.9198, MinusLogProbMetric: 32.9198, val_loss: 33.1385, val_MinusLogProbMetric: 33.1385

Epoch 723: val_loss did not improve from 31.79977
196/196 - 58s - loss: 32.9198 - MinusLogProbMetric: 32.9198 - val_loss: 33.1385 - val_MinusLogProbMetric: 33.1385 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 724/1000
2023-10-31 16:48:22.877 
Epoch 724/1000 
	 loss: 32.5351, MinusLogProbMetric: 32.5351, val_loss: 32.9799, val_MinusLogProbMetric: 32.9799

Epoch 724: val_loss did not improve from 31.79977
196/196 - 63s - loss: 32.5351 - MinusLogProbMetric: 32.5351 - val_loss: 32.9799 - val_MinusLogProbMetric: 32.9799 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 725/1000
2023-10-31 16:49:23.283 
Epoch 725/1000 
	 loss: 32.2818, MinusLogProbMetric: 32.2818, val_loss: 32.7640, val_MinusLogProbMetric: 32.7640

Epoch 725: val_loss did not improve from 31.79977
196/196 - 60s - loss: 32.2818 - MinusLogProbMetric: 32.2818 - val_loss: 32.7640 - val_MinusLogProbMetric: 32.7640 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 726/1000
2023-10-31 16:50:15.186 
Epoch 726/1000 
	 loss: 32.0416, MinusLogProbMetric: 32.0416, val_loss: 32.6226, val_MinusLogProbMetric: 32.6226

Epoch 726: val_loss did not improve from 31.79977
196/196 - 52s - loss: 32.0416 - MinusLogProbMetric: 32.0416 - val_loss: 32.6226 - val_MinusLogProbMetric: 32.6226 - lr: 1.3717e-06 - 52s/epoch - 265ms/step
Epoch 727/1000
2023-10-31 16:51:11.136 
Epoch 727/1000 
	 loss: 31.9491, MinusLogProbMetric: 31.9491, val_loss: 32.5270, val_MinusLogProbMetric: 32.5270

Epoch 727: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.9491 - MinusLogProbMetric: 31.9491 - val_loss: 32.5270 - val_MinusLogProbMetric: 32.5270 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 728/1000
2023-10-31 16:52:07.307 
Epoch 728/1000 
	 loss: 31.9248, MinusLogProbMetric: 31.9248, val_loss: 32.4809, val_MinusLogProbMetric: 32.4809

Epoch 728: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.9248 - MinusLogProbMetric: 31.9248 - val_loss: 32.4809 - val_MinusLogProbMetric: 32.4809 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 729/1000
2023-10-31 16:53:03.424 
Epoch 729/1000 
	 loss: 31.8801, MinusLogProbMetric: 31.8801, val_loss: 32.5003, val_MinusLogProbMetric: 32.5003

Epoch 729: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.8801 - MinusLogProbMetric: 31.8801 - val_loss: 32.5003 - val_MinusLogProbMetric: 32.5003 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 730/1000
2023-10-31 16:54:01.267 
Epoch 730/1000 
	 loss: 31.8557, MinusLogProbMetric: 31.8557, val_loss: 32.5168, val_MinusLogProbMetric: 32.5168

Epoch 730: val_loss did not improve from 31.79977
196/196 - 58s - loss: 31.8557 - MinusLogProbMetric: 31.8557 - val_loss: 32.5168 - val_MinusLogProbMetric: 32.5168 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 731/1000
2023-10-31 16:55:01.784 
Epoch 731/1000 
	 loss: 31.8446, MinusLogProbMetric: 31.8446, val_loss: 32.4052, val_MinusLogProbMetric: 32.4052

Epoch 731: val_loss did not improve from 31.79977
196/196 - 61s - loss: 31.8446 - MinusLogProbMetric: 31.8446 - val_loss: 32.4052 - val_MinusLogProbMetric: 32.4052 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 732/1000
2023-10-31 16:56:00.684 
Epoch 732/1000 
	 loss: 31.8202, MinusLogProbMetric: 31.8202, val_loss: 32.3659, val_MinusLogProbMetric: 32.3659

Epoch 732: val_loss did not improve from 31.79977
196/196 - 59s - loss: 31.8202 - MinusLogProbMetric: 31.8202 - val_loss: 32.3659 - val_MinusLogProbMetric: 32.3659 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 733/1000
2023-10-31 16:56:58.947 
Epoch 733/1000 
	 loss: 31.8208, MinusLogProbMetric: 31.8208, val_loss: 32.3888, val_MinusLogProbMetric: 32.3888

Epoch 733: val_loss did not improve from 31.79977
196/196 - 58s - loss: 31.8208 - MinusLogProbMetric: 31.8208 - val_loss: 32.3888 - val_MinusLogProbMetric: 32.3888 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 734/1000
2023-10-31 16:57:55.195 
Epoch 734/1000 
	 loss: 31.8116, MinusLogProbMetric: 31.8116, val_loss: 32.3596, val_MinusLogProbMetric: 32.3596

Epoch 734: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.8116 - MinusLogProbMetric: 31.8116 - val_loss: 32.3596 - val_MinusLogProbMetric: 32.3596 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 735/1000
2023-10-31 16:58:50.695 
Epoch 735/1000 
	 loss: 31.7808, MinusLogProbMetric: 31.7808, val_loss: 32.4095, val_MinusLogProbMetric: 32.4095

Epoch 735: val_loss did not improve from 31.79977
196/196 - 55s - loss: 31.7808 - MinusLogProbMetric: 31.7808 - val_loss: 32.4095 - val_MinusLogProbMetric: 32.4095 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 736/1000
2023-10-31 16:59:47.822 
Epoch 736/1000 
	 loss: 31.7985, MinusLogProbMetric: 31.7985, val_loss: 32.3880, val_MinusLogProbMetric: 32.3880

Epoch 736: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.7985 - MinusLogProbMetric: 31.7985 - val_loss: 32.3880 - val_MinusLogProbMetric: 32.3880 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 737/1000
2023-10-31 17:00:45.315 
Epoch 737/1000 
	 loss: 31.7704, MinusLogProbMetric: 31.7704, val_loss: 32.4370, val_MinusLogProbMetric: 32.4370

Epoch 737: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.7704 - MinusLogProbMetric: 31.7704 - val_loss: 32.4370 - val_MinusLogProbMetric: 32.4370 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 738/1000
2023-10-31 17:01:42.045 
Epoch 738/1000 
	 loss: 31.7638, MinusLogProbMetric: 31.7638, val_loss: 32.2704, val_MinusLogProbMetric: 32.2704

Epoch 738: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.7638 - MinusLogProbMetric: 31.7638 - val_loss: 32.2704 - val_MinusLogProbMetric: 32.2704 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 739/1000
2023-10-31 17:02:39.355 
Epoch 739/1000 
	 loss: 31.7500, MinusLogProbMetric: 31.7500, val_loss: 32.3570, val_MinusLogProbMetric: 32.3570

Epoch 739: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.7500 - MinusLogProbMetric: 31.7500 - val_loss: 32.3570 - val_MinusLogProbMetric: 32.3570 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 740/1000
2023-10-31 17:03:36.619 
Epoch 740/1000 
	 loss: 31.7263, MinusLogProbMetric: 31.7263, val_loss: 32.3818, val_MinusLogProbMetric: 32.3818

Epoch 740: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.7263 - MinusLogProbMetric: 31.7263 - val_loss: 32.3818 - val_MinusLogProbMetric: 32.3818 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 741/1000
2023-10-31 17:04:32.416 
Epoch 741/1000 
	 loss: 31.7089, MinusLogProbMetric: 31.7089, val_loss: 32.3286, val_MinusLogProbMetric: 32.3286

Epoch 741: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.7089 - MinusLogProbMetric: 31.7089 - val_loss: 32.3286 - val_MinusLogProbMetric: 32.3286 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 742/1000
2023-10-31 17:05:32.397 
Epoch 742/1000 
	 loss: 31.6938, MinusLogProbMetric: 31.6938, val_loss: 32.3896, val_MinusLogProbMetric: 32.3896

Epoch 742: val_loss did not improve from 31.79977
196/196 - 60s - loss: 31.6938 - MinusLogProbMetric: 31.6938 - val_loss: 32.3896 - val_MinusLogProbMetric: 32.3896 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 743/1000
2023-10-31 17:06:31.987 
Epoch 743/1000 
	 loss: 31.7062, MinusLogProbMetric: 31.7062, val_loss: 32.3268, val_MinusLogProbMetric: 32.3268

Epoch 743: val_loss did not improve from 31.79977
196/196 - 60s - loss: 31.7062 - MinusLogProbMetric: 31.7062 - val_loss: 32.3268 - val_MinusLogProbMetric: 32.3268 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 744/1000
2023-10-31 17:07:31.316 
Epoch 744/1000 
	 loss: 31.6782, MinusLogProbMetric: 31.6782, val_loss: 32.2848, val_MinusLogProbMetric: 32.2848

Epoch 744: val_loss did not improve from 31.79977
196/196 - 59s - loss: 31.6782 - MinusLogProbMetric: 31.6782 - val_loss: 32.2848 - val_MinusLogProbMetric: 32.2848 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 745/1000
2023-10-31 17:08:27.683 
Epoch 745/1000 
	 loss: 31.6798, MinusLogProbMetric: 31.6798, val_loss: 32.2915, val_MinusLogProbMetric: 32.2915

Epoch 745: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.6798 - MinusLogProbMetric: 31.6798 - val_loss: 32.2915 - val_MinusLogProbMetric: 32.2915 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 746/1000
2023-10-31 17:09:24.797 
Epoch 746/1000 
	 loss: 31.6543, MinusLogProbMetric: 31.6543, val_loss: 32.2615, val_MinusLogProbMetric: 32.2615

Epoch 746: val_loss did not improve from 31.79977
196/196 - 57s - loss: 31.6543 - MinusLogProbMetric: 31.6543 - val_loss: 32.2615 - val_MinusLogProbMetric: 32.2615 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 747/1000
2023-10-31 17:10:25.929 
Epoch 747/1000 
	 loss: 31.6470, MinusLogProbMetric: 31.6470, val_loss: 32.3330, val_MinusLogProbMetric: 32.3330

Epoch 747: val_loss did not improve from 31.79977
196/196 - 61s - loss: 31.6470 - MinusLogProbMetric: 31.6470 - val_loss: 32.3330 - val_MinusLogProbMetric: 32.3330 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 748/1000
2023-10-31 17:11:29.412 
Epoch 748/1000 
	 loss: 31.6319, MinusLogProbMetric: 31.6319, val_loss: 32.1724, val_MinusLogProbMetric: 32.1724

Epoch 748: val_loss did not improve from 31.79977
196/196 - 63s - loss: 31.6319 - MinusLogProbMetric: 31.6319 - val_loss: 32.1724 - val_MinusLogProbMetric: 32.1724 - lr: 1.3717e-06 - 63s/epoch - 324ms/step
Epoch 749/1000
2023-10-31 17:12:31.358 
Epoch 749/1000 
	 loss: 31.6241, MinusLogProbMetric: 31.6241, val_loss: 32.2094, val_MinusLogProbMetric: 32.2094

Epoch 749: val_loss did not improve from 31.79977
196/196 - 62s - loss: 31.6241 - MinusLogProbMetric: 31.6241 - val_loss: 32.2094 - val_MinusLogProbMetric: 32.2094 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 750/1000
2023-10-31 17:13:34.453 
Epoch 750/1000 
	 loss: 31.6042, MinusLogProbMetric: 31.6042, val_loss: 32.1590, val_MinusLogProbMetric: 32.1590

Epoch 750: val_loss did not improve from 31.79977
196/196 - 63s - loss: 31.6042 - MinusLogProbMetric: 31.6042 - val_loss: 32.1590 - val_MinusLogProbMetric: 32.1590 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 751/1000
2023-10-31 17:14:37.265 
Epoch 751/1000 
	 loss: 31.5749, MinusLogProbMetric: 31.5749, val_loss: 32.3704, val_MinusLogProbMetric: 32.3704

Epoch 751: val_loss did not improve from 31.79977
196/196 - 63s - loss: 31.5749 - MinusLogProbMetric: 31.5749 - val_loss: 32.3704 - val_MinusLogProbMetric: 32.3704 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 752/1000
2023-10-31 17:15:35.987 
Epoch 752/1000 
	 loss: 31.5562, MinusLogProbMetric: 31.5562, val_loss: 32.1319, val_MinusLogProbMetric: 32.1319

Epoch 752: val_loss did not improve from 31.79977
196/196 - 59s - loss: 31.5562 - MinusLogProbMetric: 31.5562 - val_loss: 32.1319 - val_MinusLogProbMetric: 32.1319 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 753/1000
2023-10-31 17:16:34.096 
Epoch 753/1000 
	 loss: 31.5790, MinusLogProbMetric: 31.5790, val_loss: 32.1831, val_MinusLogProbMetric: 32.1831

Epoch 753: val_loss did not improve from 31.79977
196/196 - 58s - loss: 31.5790 - MinusLogProbMetric: 31.5790 - val_loss: 32.1831 - val_MinusLogProbMetric: 32.1831 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 754/1000
2023-10-31 17:17:37.443 
Epoch 754/1000 
	 loss: 31.5057, MinusLogProbMetric: 31.5057, val_loss: 32.0095, val_MinusLogProbMetric: 32.0095

Epoch 754: val_loss did not improve from 31.79977
196/196 - 63s - loss: 31.5057 - MinusLogProbMetric: 31.5057 - val_loss: 32.0095 - val_MinusLogProbMetric: 32.0095 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 755/1000
2023-10-31 17:18:39.510 
Epoch 755/1000 
	 loss: 31.4503, MinusLogProbMetric: 31.4503, val_loss: 31.9635, val_MinusLogProbMetric: 31.9635

Epoch 755: val_loss did not improve from 31.79977
196/196 - 62s - loss: 31.4503 - MinusLogProbMetric: 31.4503 - val_loss: 31.9635 - val_MinusLogProbMetric: 31.9635 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 756/1000
2023-10-31 17:19:40.500 
Epoch 756/1000 
	 loss: 31.3233, MinusLogProbMetric: 31.3233, val_loss: 31.9947, val_MinusLogProbMetric: 31.9947

Epoch 756: val_loss did not improve from 31.79977
196/196 - 61s - loss: 31.3233 - MinusLogProbMetric: 31.3233 - val_loss: 31.9947 - val_MinusLogProbMetric: 31.9947 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 757/1000
2023-10-31 17:20:42.491 
Epoch 757/1000 
	 loss: 31.2924, MinusLogProbMetric: 31.2924, val_loss: 31.8388, val_MinusLogProbMetric: 31.8388

Epoch 757: val_loss did not improve from 31.79977
196/196 - 62s - loss: 31.2924 - MinusLogProbMetric: 31.2924 - val_loss: 31.8388 - val_MinusLogProbMetric: 31.8388 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 758/1000
2023-10-31 17:21:43.724 
Epoch 758/1000 
	 loss: 31.2710, MinusLogProbMetric: 31.2710, val_loss: 31.8383, val_MinusLogProbMetric: 31.8383

Epoch 758: val_loss did not improve from 31.79977
196/196 - 61s - loss: 31.2710 - MinusLogProbMetric: 31.2710 - val_loss: 31.8383 - val_MinusLogProbMetric: 31.8383 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 759/1000
2023-10-31 17:22:38.095 
Epoch 759/1000 
	 loss: 31.2460, MinusLogProbMetric: 31.2460, val_loss: 31.8718, val_MinusLogProbMetric: 31.8718

Epoch 759: val_loss did not improve from 31.79977
196/196 - 54s - loss: 31.2460 - MinusLogProbMetric: 31.2460 - val_loss: 31.8718 - val_MinusLogProbMetric: 31.8718 - lr: 1.3717e-06 - 54s/epoch - 277ms/step
Epoch 760/1000
2023-10-31 17:23:24.798 
Epoch 760/1000 
	 loss: 31.2505, MinusLogProbMetric: 31.2505, val_loss: 31.8730, val_MinusLogProbMetric: 31.8730

Epoch 760: val_loss did not improve from 31.79977
196/196 - 47s - loss: 31.2505 - MinusLogProbMetric: 31.2505 - val_loss: 31.8730 - val_MinusLogProbMetric: 31.8730 - lr: 1.3717e-06 - 47s/epoch - 238ms/step
Epoch 761/1000
2023-10-31 17:24:17.631 
Epoch 761/1000 
	 loss: 31.2346, MinusLogProbMetric: 31.2346, val_loss: 31.8700, val_MinusLogProbMetric: 31.8700

Epoch 761: val_loss did not improve from 31.79977
196/196 - 53s - loss: 31.2346 - MinusLogProbMetric: 31.2346 - val_loss: 31.8700 - val_MinusLogProbMetric: 31.8700 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 762/1000
2023-10-31 17:25:13.178 
Epoch 762/1000 
	 loss: 31.2136, MinusLogProbMetric: 31.2136, val_loss: 31.8143, val_MinusLogProbMetric: 31.8143

Epoch 762: val_loss did not improve from 31.79977
196/196 - 56s - loss: 31.2136 - MinusLogProbMetric: 31.2136 - val_loss: 31.8143 - val_MinusLogProbMetric: 31.8143 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 763/1000
2023-10-31 17:26:11.124 
Epoch 763/1000 
	 loss: 31.1977, MinusLogProbMetric: 31.1977, val_loss: 31.7902, val_MinusLogProbMetric: 31.7902

Epoch 763: val_loss improved from 31.79977 to 31.79020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 31.1977 - MinusLogProbMetric: 31.1977 - val_loss: 31.7902 - val_MinusLogProbMetric: 31.7902 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 764/1000
2023-10-31 17:27:02.537 
Epoch 764/1000 
	 loss: 31.2257, MinusLogProbMetric: 31.2257, val_loss: 31.8758, val_MinusLogProbMetric: 31.8758

Epoch 764: val_loss did not improve from 31.79020
196/196 - 50s - loss: 31.2257 - MinusLogProbMetric: 31.2257 - val_loss: 31.8758 - val_MinusLogProbMetric: 31.8758 - lr: 1.3717e-06 - 50s/epoch - 257ms/step
Epoch 765/1000
2023-10-31 17:27:58.140 
Epoch 765/1000 
	 loss: 31.2030, MinusLogProbMetric: 31.2030, val_loss: 31.8768, val_MinusLogProbMetric: 31.8768

Epoch 765: val_loss did not improve from 31.79020
196/196 - 56s - loss: 31.2030 - MinusLogProbMetric: 31.2030 - val_loss: 31.8768 - val_MinusLogProbMetric: 31.8768 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 766/1000
2023-10-31 17:28:54.542 
Epoch 766/1000 
	 loss: 31.1947, MinusLogProbMetric: 31.1947, val_loss: 31.8182, val_MinusLogProbMetric: 31.8182

Epoch 766: val_loss did not improve from 31.79020
196/196 - 56s - loss: 31.1947 - MinusLogProbMetric: 31.1947 - val_loss: 31.8182 - val_MinusLogProbMetric: 31.8182 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 767/1000
2023-10-31 17:29:50.489 
Epoch 767/1000 
	 loss: 31.1836, MinusLogProbMetric: 31.1836, val_loss: 31.8394, val_MinusLogProbMetric: 31.8394

Epoch 767: val_loss did not improve from 31.79020
196/196 - 56s - loss: 31.1836 - MinusLogProbMetric: 31.1836 - val_loss: 31.8394 - val_MinusLogProbMetric: 31.8394 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 768/1000
2023-10-31 17:30:39.190 
Epoch 768/1000 
	 loss: 31.1930, MinusLogProbMetric: 31.1930, val_loss: 31.8246, val_MinusLogProbMetric: 31.8246

Epoch 768: val_loss did not improve from 31.79020
196/196 - 49s - loss: 31.1930 - MinusLogProbMetric: 31.1930 - val_loss: 31.8246 - val_MinusLogProbMetric: 31.8246 - lr: 1.3717e-06 - 49s/epoch - 248ms/step
Epoch 769/1000
2023-10-31 17:31:35.041 
Epoch 769/1000 
	 loss: 31.1981, MinusLogProbMetric: 31.1981, val_loss: 31.8058, val_MinusLogProbMetric: 31.8058

Epoch 769: val_loss did not improve from 31.79020
196/196 - 56s - loss: 31.1981 - MinusLogProbMetric: 31.1981 - val_loss: 31.8058 - val_MinusLogProbMetric: 31.8058 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 770/1000
2023-10-31 17:32:31.824 
Epoch 770/1000 
	 loss: 31.2188, MinusLogProbMetric: 31.2188, val_loss: 31.8193, val_MinusLogProbMetric: 31.8193

Epoch 770: val_loss did not improve from 31.79020
196/196 - 57s - loss: 31.2188 - MinusLogProbMetric: 31.2188 - val_loss: 31.8193 - val_MinusLogProbMetric: 31.8193 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 771/1000
2023-10-31 17:33:27.620 
Epoch 771/1000 
	 loss: 31.1782, MinusLogProbMetric: 31.1782, val_loss: 31.7973, val_MinusLogProbMetric: 31.7973

Epoch 771: val_loss did not improve from 31.79020
196/196 - 56s - loss: 31.1782 - MinusLogProbMetric: 31.1782 - val_loss: 31.7973 - val_MinusLogProbMetric: 31.7973 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 772/1000
2023-10-31 17:34:18.375 
Epoch 772/1000 
	 loss: 31.1726, MinusLogProbMetric: 31.1726, val_loss: 31.8725, val_MinusLogProbMetric: 31.8725

Epoch 772: val_loss did not improve from 31.79020
196/196 - 51s - loss: 31.1726 - MinusLogProbMetric: 31.1726 - val_loss: 31.8725 - val_MinusLogProbMetric: 31.8725 - lr: 1.3717e-06 - 51s/epoch - 259ms/step
Epoch 773/1000
2023-10-31 17:35:11.535 
Epoch 773/1000 
	 loss: 31.1878, MinusLogProbMetric: 31.1878, val_loss: 31.8379, val_MinusLogProbMetric: 31.8379

Epoch 773: val_loss did not improve from 31.79020
196/196 - 53s - loss: 31.1878 - MinusLogProbMetric: 31.1878 - val_loss: 31.8379 - val_MinusLogProbMetric: 31.8379 - lr: 1.3717e-06 - 53s/epoch - 271ms/step
Epoch 774/1000
2023-10-31 17:36:04.250 
Epoch 774/1000 
	 loss: 31.1804, MinusLogProbMetric: 31.1804, val_loss: 31.9536, val_MinusLogProbMetric: 31.9536

Epoch 774: val_loss did not improve from 31.79020
196/196 - 53s - loss: 31.1804 - MinusLogProbMetric: 31.1804 - val_loss: 31.9536 - val_MinusLogProbMetric: 31.9536 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 775/1000
2023-10-31 17:36:52.727 
Epoch 775/1000 
	 loss: 31.1986, MinusLogProbMetric: 31.1986, val_loss: 31.7657, val_MinusLogProbMetric: 31.7657

Epoch 775: val_loss improved from 31.79020 to 31.76574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 49s - loss: 31.1986 - MinusLogProbMetric: 31.1986 - val_loss: 31.7657 - val_MinusLogProbMetric: 31.7657 - lr: 1.3717e-06 - 49s/epoch - 251ms/step
Epoch 776/1000
2023-10-31 17:37:45.650 
Epoch 776/1000 
	 loss: 31.1691, MinusLogProbMetric: 31.1691, val_loss: 31.7372, val_MinusLogProbMetric: 31.7372

Epoch 776: val_loss improved from 31.76574 to 31.73716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 53s - loss: 31.1691 - MinusLogProbMetric: 31.1691 - val_loss: 31.7372 - val_MinusLogProbMetric: 31.7372 - lr: 1.3717e-06 - 53s/epoch - 272ms/step
Epoch 777/1000
2023-10-31 17:38:43.491 
Epoch 777/1000 
	 loss: 31.1841, MinusLogProbMetric: 31.1841, val_loss: 32.0078, val_MinusLogProbMetric: 32.0078

Epoch 777: val_loss did not improve from 31.73716
196/196 - 57s - loss: 31.1841 - MinusLogProbMetric: 31.1841 - val_loss: 32.0078 - val_MinusLogProbMetric: 32.0078 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 778/1000
2023-10-31 17:39:40.438 
Epoch 778/1000 
	 loss: 31.1766, MinusLogProbMetric: 31.1766, val_loss: 31.8126, val_MinusLogProbMetric: 31.8126

Epoch 778: val_loss did not improve from 31.73716
196/196 - 57s - loss: 31.1766 - MinusLogProbMetric: 31.1766 - val_loss: 31.8126 - val_MinusLogProbMetric: 31.8126 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 779/1000
2023-10-31 17:40:36.734 
Epoch 779/1000 
	 loss: 31.1696, MinusLogProbMetric: 31.1696, val_loss: 31.8155, val_MinusLogProbMetric: 31.8155

Epoch 779: val_loss did not improve from 31.73716
196/196 - 56s - loss: 31.1696 - MinusLogProbMetric: 31.1696 - val_loss: 31.8155 - val_MinusLogProbMetric: 31.8155 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 780/1000
2023-10-31 17:41:40.730 
Epoch 780/1000 
	 loss: 31.1712, MinusLogProbMetric: 31.1712, val_loss: 31.7643, val_MinusLogProbMetric: 31.7643

Epoch 780: val_loss did not improve from 31.73716
196/196 - 64s - loss: 31.1712 - MinusLogProbMetric: 31.1712 - val_loss: 31.7643 - val_MinusLogProbMetric: 31.7643 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 781/1000
2023-10-31 17:42:48.128 
Epoch 781/1000 
	 loss: 31.1615, MinusLogProbMetric: 31.1615, val_loss: 31.7564, val_MinusLogProbMetric: 31.7564

Epoch 781: val_loss did not improve from 31.73716
196/196 - 67s - loss: 31.1615 - MinusLogProbMetric: 31.1615 - val_loss: 31.7564 - val_MinusLogProbMetric: 31.7564 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 782/1000
2023-10-31 17:43:52.537 
Epoch 782/1000 
	 loss: 31.1574, MinusLogProbMetric: 31.1574, val_loss: 31.7261, val_MinusLogProbMetric: 31.7261

Epoch 782: val_loss improved from 31.73716 to 31.72610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 31.1574 - MinusLogProbMetric: 31.1574 - val_loss: 31.7261 - val_MinusLogProbMetric: 31.7261 - lr: 1.3717e-06 - 66s/epoch - 334ms/step
Epoch 783/1000
2023-10-31 17:44:50.738 
Epoch 783/1000 
	 loss: 31.1737, MinusLogProbMetric: 31.1737, val_loss: 31.8723, val_MinusLogProbMetric: 31.8723

Epoch 783: val_loss did not improve from 31.72610
196/196 - 57s - loss: 31.1737 - MinusLogProbMetric: 31.1737 - val_loss: 31.8723 - val_MinusLogProbMetric: 31.8723 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 784/1000
2023-10-31 17:45:45.215 
Epoch 784/1000 
	 loss: 31.1534, MinusLogProbMetric: 31.1534, val_loss: 31.7629, val_MinusLogProbMetric: 31.7629

Epoch 784: val_loss did not improve from 31.72610
196/196 - 54s - loss: 31.1534 - MinusLogProbMetric: 31.1534 - val_loss: 31.7629 - val_MinusLogProbMetric: 31.7629 - lr: 1.3717e-06 - 54s/epoch - 278ms/step
Epoch 785/1000
2023-10-31 17:46:38.178 
Epoch 785/1000 
	 loss: 31.1535, MinusLogProbMetric: 31.1535, val_loss: 31.7400, val_MinusLogProbMetric: 31.7400

Epoch 785: val_loss did not improve from 31.72610
196/196 - 53s - loss: 31.1535 - MinusLogProbMetric: 31.1535 - val_loss: 31.7400 - val_MinusLogProbMetric: 31.7400 - lr: 1.3717e-06 - 53s/epoch - 270ms/step
Epoch 786/1000
2023-10-31 17:47:30.435 
Epoch 786/1000 
	 loss: 31.1483, MinusLogProbMetric: 31.1483, val_loss: 31.8427, val_MinusLogProbMetric: 31.8427

Epoch 786: val_loss did not improve from 31.72610
196/196 - 52s - loss: 31.1483 - MinusLogProbMetric: 31.1483 - val_loss: 31.8427 - val_MinusLogProbMetric: 31.8427 - lr: 1.3717e-06 - 52s/epoch - 267ms/step
Epoch 787/1000
2023-10-31 17:48:23.026 
Epoch 787/1000 
	 loss: 31.1651, MinusLogProbMetric: 31.1651, val_loss: 31.7820, val_MinusLogProbMetric: 31.7820

Epoch 787: val_loss did not improve from 31.72610
196/196 - 53s - loss: 31.1651 - MinusLogProbMetric: 31.1651 - val_loss: 31.7820 - val_MinusLogProbMetric: 31.7820 - lr: 1.3717e-06 - 53s/epoch - 268ms/step
Epoch 788/1000
2023-10-31 17:49:18.366 
Epoch 788/1000 
	 loss: 31.1939, MinusLogProbMetric: 31.1939, val_loss: 31.7292, val_MinusLogProbMetric: 31.7292

Epoch 788: val_loss did not improve from 31.72610
196/196 - 55s - loss: 31.1939 - MinusLogProbMetric: 31.1939 - val_loss: 31.7292 - val_MinusLogProbMetric: 31.7292 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 789/1000
2023-10-31 17:50:13.718 
Epoch 789/1000 
	 loss: 31.1723, MinusLogProbMetric: 31.1723, val_loss: 31.8888, val_MinusLogProbMetric: 31.8888

Epoch 789: val_loss did not improve from 31.72610
196/196 - 55s - loss: 31.1723 - MinusLogProbMetric: 31.1723 - val_loss: 31.8888 - val_MinusLogProbMetric: 31.8888 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 790/1000
2023-10-31 17:51:09.436 
Epoch 790/1000 
	 loss: 31.1813, MinusLogProbMetric: 31.1813, val_loss: 31.8052, val_MinusLogProbMetric: 31.8052

Epoch 790: val_loss did not improve from 31.72610
196/196 - 56s - loss: 31.1813 - MinusLogProbMetric: 31.1813 - val_loss: 31.8052 - val_MinusLogProbMetric: 31.8052 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 791/1000
2023-10-31 17:52:05.670 
Epoch 791/1000 
	 loss: 31.1674, MinusLogProbMetric: 31.1674, val_loss: 31.8851, val_MinusLogProbMetric: 31.8851

Epoch 791: val_loss did not improve from 31.72610
196/196 - 56s - loss: 31.1674 - MinusLogProbMetric: 31.1674 - val_loss: 31.8851 - val_MinusLogProbMetric: 31.8851 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 792/1000
2023-10-31 17:53:02.068 
Epoch 792/1000 
	 loss: 31.1571, MinusLogProbMetric: 31.1571, val_loss: 31.7857, val_MinusLogProbMetric: 31.7857

Epoch 792: val_loss did not improve from 31.72610
196/196 - 56s - loss: 31.1571 - MinusLogProbMetric: 31.1571 - val_loss: 31.7857 - val_MinusLogProbMetric: 31.7857 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 793/1000
2023-10-31 17:53:58.841 
Epoch 793/1000 
	 loss: 31.1529, MinusLogProbMetric: 31.1529, val_loss: 31.7939, val_MinusLogProbMetric: 31.7939

Epoch 793: val_loss did not improve from 31.72610
196/196 - 57s - loss: 31.1529 - MinusLogProbMetric: 31.1529 - val_loss: 31.7939 - val_MinusLogProbMetric: 31.7939 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 794/1000
2023-10-31 17:54:55.787 
Epoch 794/1000 
	 loss: 31.1686, MinusLogProbMetric: 31.1686, val_loss: 31.8413, val_MinusLogProbMetric: 31.8413

Epoch 794: val_loss did not improve from 31.72610
196/196 - 57s - loss: 31.1686 - MinusLogProbMetric: 31.1686 - val_loss: 31.8413 - val_MinusLogProbMetric: 31.8413 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 795/1000
2023-10-31 17:55:55.157 
Epoch 795/1000 
	 loss: 31.1561, MinusLogProbMetric: 31.1561, val_loss: 31.8628, val_MinusLogProbMetric: 31.8628

Epoch 795: val_loss did not improve from 31.72610
196/196 - 59s - loss: 31.1561 - MinusLogProbMetric: 31.1561 - val_loss: 31.8628 - val_MinusLogProbMetric: 31.8628 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 796/1000
2023-10-31 17:56:56.842 
Epoch 796/1000 
	 loss: 31.1549, MinusLogProbMetric: 31.1549, val_loss: 31.7924, val_MinusLogProbMetric: 31.7924

Epoch 796: val_loss did not improve from 31.72610
196/196 - 62s - loss: 31.1549 - MinusLogProbMetric: 31.1549 - val_loss: 31.7924 - val_MinusLogProbMetric: 31.7924 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 797/1000
2023-10-31 17:57:57.767 
Epoch 797/1000 
	 loss: 31.1537, MinusLogProbMetric: 31.1537, val_loss: 31.7461, val_MinusLogProbMetric: 31.7461

Epoch 797: val_loss did not improve from 31.72610
196/196 - 61s - loss: 31.1537 - MinusLogProbMetric: 31.1537 - val_loss: 31.7461 - val_MinusLogProbMetric: 31.7461 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 798/1000
2023-10-31 17:58:56.549 
Epoch 798/1000 
	 loss: 31.1378, MinusLogProbMetric: 31.1378, val_loss: 31.8077, val_MinusLogProbMetric: 31.8077

Epoch 798: val_loss did not improve from 31.72610
196/196 - 59s - loss: 31.1378 - MinusLogProbMetric: 31.1378 - val_loss: 31.8077 - val_MinusLogProbMetric: 31.8077 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 799/1000
2023-10-31 17:59:55.311 
Epoch 799/1000 
	 loss: 31.1302, MinusLogProbMetric: 31.1302, val_loss: 31.8162, val_MinusLogProbMetric: 31.8162

Epoch 799: val_loss did not improve from 31.72610
196/196 - 59s - loss: 31.1302 - MinusLogProbMetric: 31.1302 - val_loss: 31.8162 - val_MinusLogProbMetric: 31.8162 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 800/1000
2023-10-31 18:00:56.348 
Epoch 800/1000 
	 loss: 31.1452, MinusLogProbMetric: 31.1452, val_loss: 31.8112, val_MinusLogProbMetric: 31.8112

Epoch 800: val_loss did not improve from 31.72610
196/196 - 61s - loss: 31.1452 - MinusLogProbMetric: 31.1452 - val_loss: 31.8112 - val_MinusLogProbMetric: 31.8112 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 801/1000
2023-10-31 18:01:58.686 
Epoch 801/1000 
	 loss: 31.1332, MinusLogProbMetric: 31.1332, val_loss: 31.7452, val_MinusLogProbMetric: 31.7452

Epoch 801: val_loss did not improve from 31.72610
196/196 - 62s - loss: 31.1332 - MinusLogProbMetric: 31.1332 - val_loss: 31.7452 - val_MinusLogProbMetric: 31.7452 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 802/1000
2023-10-31 18:02:58.044 
Epoch 802/1000 
	 loss: 31.1445, MinusLogProbMetric: 31.1445, val_loss: 31.7394, val_MinusLogProbMetric: 31.7394

Epoch 802: val_loss did not improve from 31.72610
196/196 - 59s - loss: 31.1445 - MinusLogProbMetric: 31.1445 - val_loss: 31.7394 - val_MinusLogProbMetric: 31.7394 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 803/1000
2023-10-31 18:03:52.629 
Epoch 803/1000 
	 loss: 31.1536, MinusLogProbMetric: 31.1536, val_loss: 31.7462, val_MinusLogProbMetric: 31.7462

Epoch 803: val_loss did not improve from 31.72610
196/196 - 55s - loss: 31.1536 - MinusLogProbMetric: 31.1536 - val_loss: 31.7462 - val_MinusLogProbMetric: 31.7462 - lr: 1.3717e-06 - 55s/epoch - 278ms/step
Epoch 804/1000
2023-10-31 18:04:47.297 
Epoch 804/1000 
	 loss: 31.1448, MinusLogProbMetric: 31.1448, val_loss: 31.9218, val_MinusLogProbMetric: 31.9218

Epoch 804: val_loss did not improve from 31.72610
196/196 - 55s - loss: 31.1448 - MinusLogProbMetric: 31.1448 - val_loss: 31.9218 - val_MinusLogProbMetric: 31.9218 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 805/1000
2023-10-31 18:05:43.552 
Epoch 805/1000 
	 loss: 31.1492, MinusLogProbMetric: 31.1492, val_loss: 31.7584, val_MinusLogProbMetric: 31.7584

Epoch 805: val_loss did not improve from 31.72610
196/196 - 56s - loss: 31.1492 - MinusLogProbMetric: 31.1492 - val_loss: 31.7584 - val_MinusLogProbMetric: 31.7584 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 806/1000
2023-10-31 18:06:40.286 
Epoch 806/1000 
	 loss: 31.1233, MinusLogProbMetric: 31.1233, val_loss: 31.9672, val_MinusLogProbMetric: 31.9672

Epoch 806: val_loss did not improve from 31.72610
196/196 - 57s - loss: 31.1233 - MinusLogProbMetric: 31.1233 - val_loss: 31.9672 - val_MinusLogProbMetric: 31.9672 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 807/1000
2023-10-31 18:07:38.393 
Epoch 807/1000 
	 loss: 31.1391, MinusLogProbMetric: 31.1391, val_loss: 31.8054, val_MinusLogProbMetric: 31.8054

Epoch 807: val_loss did not improve from 31.72610
196/196 - 58s - loss: 31.1391 - MinusLogProbMetric: 31.1391 - val_loss: 31.8054 - val_MinusLogProbMetric: 31.8054 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 808/1000
2023-10-31 18:08:35.166 
Epoch 808/1000 
	 loss: 31.1328, MinusLogProbMetric: 31.1328, val_loss: 31.7437, val_MinusLogProbMetric: 31.7437

Epoch 808: val_loss did not improve from 31.72610
196/196 - 57s - loss: 31.1328 - MinusLogProbMetric: 31.1328 - val_loss: 31.7437 - val_MinusLogProbMetric: 31.7437 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 809/1000
2023-10-31 18:09:29.890 
Epoch 809/1000 
	 loss: 31.1301, MinusLogProbMetric: 31.1301, val_loss: 31.7413, val_MinusLogProbMetric: 31.7413

Epoch 809: val_loss did not improve from 31.72610
196/196 - 55s - loss: 31.1301 - MinusLogProbMetric: 31.1301 - val_loss: 31.7413 - val_MinusLogProbMetric: 31.7413 - lr: 1.3717e-06 - 55s/epoch - 279ms/step
Epoch 810/1000
2023-10-31 18:10:18.664 
Epoch 810/1000 
	 loss: 31.1195, MinusLogProbMetric: 31.1195, val_loss: 31.7456, val_MinusLogProbMetric: 31.7456

Epoch 810: val_loss did not improve from 31.72610
196/196 - 49s - loss: 31.1195 - MinusLogProbMetric: 31.1195 - val_loss: 31.7456 - val_MinusLogProbMetric: 31.7456 - lr: 1.3717e-06 - 49s/epoch - 249ms/step
Epoch 811/1000
2023-10-31 18:11:10.957 
Epoch 811/1000 
	 loss: 31.1215, MinusLogProbMetric: 31.1215, val_loss: 31.8269, val_MinusLogProbMetric: 31.8269

Epoch 811: val_loss did not improve from 31.72610
196/196 - 52s - loss: 31.1215 - MinusLogProbMetric: 31.1215 - val_loss: 31.8269 - val_MinusLogProbMetric: 31.8269 - lr: 1.3717e-06 - 52s/epoch - 267ms/step
Epoch 812/1000
2023-10-31 18:12:04.874 
Epoch 812/1000 
	 loss: 31.1237, MinusLogProbMetric: 31.1237, val_loss: 31.7401, val_MinusLogProbMetric: 31.7401

Epoch 812: val_loss did not improve from 31.72610
196/196 - 54s - loss: 31.1237 - MinusLogProbMetric: 31.1237 - val_loss: 31.7401 - val_MinusLogProbMetric: 31.7401 - lr: 1.3717e-06 - 54s/epoch - 275ms/step
Epoch 813/1000
2023-10-31 18:13:00.435 
Epoch 813/1000 
	 loss: 31.1134, MinusLogProbMetric: 31.1134, val_loss: 31.8259, val_MinusLogProbMetric: 31.8259

Epoch 813: val_loss did not improve from 31.72610
196/196 - 56s - loss: 31.1134 - MinusLogProbMetric: 31.1134 - val_loss: 31.8259 - val_MinusLogProbMetric: 31.8259 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 814/1000
2023-10-31 18:13:56.397 
Epoch 814/1000 
	 loss: 31.1358, MinusLogProbMetric: 31.1358, val_loss: 31.7100, val_MinusLogProbMetric: 31.7100

Epoch 814: val_loss improved from 31.72610 to 31.71003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 31.1358 - MinusLogProbMetric: 31.1358 - val_loss: 31.7100 - val_MinusLogProbMetric: 31.7100 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 815/1000
2023-10-31 18:14:56.521 
Epoch 815/1000 
	 loss: 31.1278, MinusLogProbMetric: 31.1278, val_loss: 31.7043, val_MinusLogProbMetric: 31.7043

Epoch 815: val_loss improved from 31.71003 to 31.70433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 31.1278 - MinusLogProbMetric: 31.1278 - val_loss: 31.7043 - val_MinusLogProbMetric: 31.7043 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 816/1000
2023-10-31 18:16:06.865 
Epoch 816/1000 
	 loss: 31.1150, MinusLogProbMetric: 31.1150, val_loss: 31.7978, val_MinusLogProbMetric: 31.7978

Epoch 816: val_loss did not improve from 31.70433
196/196 - 68s - loss: 31.1150 - MinusLogProbMetric: 31.1150 - val_loss: 31.7978 - val_MinusLogProbMetric: 31.7978 - lr: 1.3717e-06 - 68s/epoch - 347ms/step
Epoch 817/1000
2023-10-31 18:17:12.139 
Epoch 817/1000 
	 loss: 31.1117, MinusLogProbMetric: 31.1117, val_loss: 31.8044, val_MinusLogProbMetric: 31.8044

Epoch 817: val_loss did not improve from 31.70433
196/196 - 65s - loss: 31.1117 - MinusLogProbMetric: 31.1117 - val_loss: 31.8044 - val_MinusLogProbMetric: 31.8044 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 818/1000
2023-10-31 18:18:17.968 
Epoch 818/1000 
	 loss: 31.1493, MinusLogProbMetric: 31.1493, val_loss: 31.9479, val_MinusLogProbMetric: 31.9479

Epoch 818: val_loss did not improve from 31.70433
196/196 - 66s - loss: 31.1493 - MinusLogProbMetric: 31.1493 - val_loss: 31.9479 - val_MinusLogProbMetric: 31.9479 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 819/1000
2023-10-31 18:19:18.521 
Epoch 819/1000 
	 loss: 31.1311, MinusLogProbMetric: 31.1311, val_loss: 31.8404, val_MinusLogProbMetric: 31.8404

Epoch 819: val_loss did not improve from 31.70433
196/196 - 61s - loss: 31.1311 - MinusLogProbMetric: 31.1311 - val_loss: 31.8404 - val_MinusLogProbMetric: 31.8404 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 820/1000
2023-10-31 18:20:15.488 
Epoch 820/1000 
	 loss: 31.1174, MinusLogProbMetric: 31.1174, val_loss: 31.8057, val_MinusLogProbMetric: 31.8057

Epoch 820: val_loss did not improve from 31.70433
196/196 - 57s - loss: 31.1174 - MinusLogProbMetric: 31.1174 - val_loss: 31.8057 - val_MinusLogProbMetric: 31.8057 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 821/1000
2023-10-31 18:21:10.850 
Epoch 821/1000 
	 loss: 31.1144, MinusLogProbMetric: 31.1144, val_loss: 31.7407, val_MinusLogProbMetric: 31.7407

Epoch 821: val_loss did not improve from 31.70433
196/196 - 55s - loss: 31.1144 - MinusLogProbMetric: 31.1144 - val_loss: 31.7407 - val_MinusLogProbMetric: 31.7407 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 822/1000
2023-10-31 18:22:03.607 
Epoch 822/1000 
	 loss: 31.1296, MinusLogProbMetric: 31.1296, val_loss: 31.8453, val_MinusLogProbMetric: 31.8453

Epoch 822: val_loss did not improve from 31.70433
196/196 - 53s - loss: 31.1296 - MinusLogProbMetric: 31.1296 - val_loss: 31.8453 - val_MinusLogProbMetric: 31.8453 - lr: 1.3717e-06 - 53s/epoch - 269ms/step
Epoch 823/1000
2023-10-31 18:22:56.407 
Epoch 823/1000 
	 loss: 31.1090, MinusLogProbMetric: 31.1090, val_loss: 31.6985, val_MinusLogProbMetric: 31.6985

Epoch 823: val_loss improved from 31.70433 to 31.69854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 31.1090 - MinusLogProbMetric: 31.1090 - val_loss: 31.6985 - val_MinusLogProbMetric: 31.6985 - lr: 1.3717e-06 - 54s/epoch - 274ms/step
Epoch 824/1000
2023-10-31 18:23:52.737 
Epoch 824/1000 
	 loss: 31.1359, MinusLogProbMetric: 31.1359, val_loss: 31.9863, val_MinusLogProbMetric: 31.9863

Epoch 824: val_loss did not improve from 31.69854
196/196 - 56s - loss: 31.1359 - MinusLogProbMetric: 31.1359 - val_loss: 31.9863 - val_MinusLogProbMetric: 31.9863 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 825/1000
2023-10-31 18:24:49.331 
Epoch 825/1000 
	 loss: 31.1145, MinusLogProbMetric: 31.1145, val_loss: 31.6828, val_MinusLogProbMetric: 31.6828

Epoch 825: val_loss improved from 31.69854 to 31.68280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 31.1145 - MinusLogProbMetric: 31.1145 - val_loss: 31.6828 - val_MinusLogProbMetric: 31.6828 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 826/1000
2023-10-31 18:25:45.416 
Epoch 826/1000 
	 loss: 31.1259, MinusLogProbMetric: 31.1259, val_loss: 31.8358, val_MinusLogProbMetric: 31.8358

Epoch 826: val_loss did not improve from 31.68280
196/196 - 55s - loss: 31.1259 - MinusLogProbMetric: 31.1259 - val_loss: 31.8358 - val_MinusLogProbMetric: 31.8358 - lr: 1.3717e-06 - 55s/epoch - 283ms/step
Epoch 827/1000
2023-10-31 18:26:43.028 
Epoch 827/1000 
	 loss: 31.1015, MinusLogProbMetric: 31.1015, val_loss: 31.9879, val_MinusLogProbMetric: 31.9879

Epoch 827: val_loss did not improve from 31.68280
196/196 - 58s - loss: 31.1015 - MinusLogProbMetric: 31.1015 - val_loss: 31.9879 - val_MinusLogProbMetric: 31.9879 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 828/1000
2023-10-31 18:27:40.079 
Epoch 828/1000 
	 loss: 31.1213, MinusLogProbMetric: 31.1213, val_loss: 31.8277, val_MinusLogProbMetric: 31.8277

Epoch 828: val_loss did not improve from 31.68280
196/196 - 57s - loss: 31.1213 - MinusLogProbMetric: 31.1213 - val_loss: 31.8277 - val_MinusLogProbMetric: 31.8277 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 829/1000
2023-10-31 18:28:37.275 
Epoch 829/1000 
	 loss: 31.0951, MinusLogProbMetric: 31.0951, val_loss: 31.8135, val_MinusLogProbMetric: 31.8135

Epoch 829: val_loss did not improve from 31.68280
196/196 - 57s - loss: 31.0951 - MinusLogProbMetric: 31.0951 - val_loss: 31.8135 - val_MinusLogProbMetric: 31.8135 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 830/1000
2023-10-31 18:29:39.087 
Epoch 830/1000 
	 loss: 31.1101, MinusLogProbMetric: 31.1101, val_loss: 31.7016, val_MinusLogProbMetric: 31.7016

Epoch 830: val_loss did not improve from 31.68280
196/196 - 62s - loss: 31.1101 - MinusLogProbMetric: 31.1101 - val_loss: 31.7016 - val_MinusLogProbMetric: 31.7016 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 831/1000
2023-10-31 18:30:44.733 
Epoch 831/1000 
	 loss: 31.1154, MinusLogProbMetric: 31.1154, val_loss: 31.7094, val_MinusLogProbMetric: 31.7094

Epoch 831: val_loss did not improve from 31.68280
196/196 - 66s - loss: 31.1154 - MinusLogProbMetric: 31.1154 - val_loss: 31.7094 - val_MinusLogProbMetric: 31.7094 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 832/1000
2023-10-31 18:31:47.969 
Epoch 832/1000 
	 loss: 31.1073, MinusLogProbMetric: 31.1073, val_loss: 31.7454, val_MinusLogProbMetric: 31.7454

Epoch 832: val_loss did not improve from 31.68280
196/196 - 63s - loss: 31.1073 - MinusLogProbMetric: 31.1073 - val_loss: 31.7454 - val_MinusLogProbMetric: 31.7454 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 833/1000
2023-10-31 18:32:49.952 
Epoch 833/1000 
	 loss: 31.1099, MinusLogProbMetric: 31.1099, val_loss: 31.7256, val_MinusLogProbMetric: 31.7256

Epoch 833: val_loss did not improve from 31.68280
196/196 - 62s - loss: 31.1099 - MinusLogProbMetric: 31.1099 - val_loss: 31.7256 - val_MinusLogProbMetric: 31.7256 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 834/1000
2023-10-31 18:33:56.491 
Epoch 834/1000 
	 loss: 31.0783, MinusLogProbMetric: 31.0783, val_loss: 31.7037, val_MinusLogProbMetric: 31.7037

Epoch 834: val_loss did not improve from 31.68280
196/196 - 67s - loss: 31.0783 - MinusLogProbMetric: 31.0783 - val_loss: 31.7037 - val_MinusLogProbMetric: 31.7037 - lr: 1.3717e-06 - 67s/epoch - 339ms/step
Epoch 835/1000
2023-10-31 18:34:59.058 
Epoch 835/1000 
	 loss: 31.0923, MinusLogProbMetric: 31.0923, val_loss: 31.7843, val_MinusLogProbMetric: 31.7843

Epoch 835: val_loss did not improve from 31.68280
196/196 - 63s - loss: 31.0923 - MinusLogProbMetric: 31.0923 - val_loss: 31.7843 - val_MinusLogProbMetric: 31.7843 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 836/1000
2023-10-31 18:36:04.488 
Epoch 836/1000 
	 loss: 31.1046, MinusLogProbMetric: 31.1046, val_loss: 31.7123, val_MinusLogProbMetric: 31.7123

Epoch 836: val_loss did not improve from 31.68280
196/196 - 65s - loss: 31.1046 - MinusLogProbMetric: 31.1046 - val_loss: 31.7123 - val_MinusLogProbMetric: 31.7123 - lr: 1.3717e-06 - 65s/epoch - 334ms/step
Epoch 837/1000
2023-10-31 18:37:07.687 
Epoch 837/1000 
	 loss: 31.1031, MinusLogProbMetric: 31.1031, val_loss: 31.7262, val_MinusLogProbMetric: 31.7262

Epoch 837: val_loss did not improve from 31.68280
196/196 - 63s - loss: 31.1031 - MinusLogProbMetric: 31.1031 - val_loss: 31.7262 - val_MinusLogProbMetric: 31.7262 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 838/1000
2023-10-31 18:38:11.783 
Epoch 838/1000 
	 loss: 31.0860, MinusLogProbMetric: 31.0860, val_loss: 31.6406, val_MinusLogProbMetric: 31.6406

Epoch 838: val_loss improved from 31.68280 to 31.64060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 31.0860 - MinusLogProbMetric: 31.0860 - val_loss: 31.6406 - val_MinusLogProbMetric: 31.6406 - lr: 1.3717e-06 - 65s/epoch - 332ms/step
Epoch 839/1000
2023-10-31 18:39:15.352 
Epoch 839/1000 
	 loss: 31.1018, MinusLogProbMetric: 31.1018, val_loss: 31.7789, val_MinusLogProbMetric: 31.7789

Epoch 839: val_loss did not improve from 31.64060
196/196 - 63s - loss: 31.1018 - MinusLogProbMetric: 31.1018 - val_loss: 31.7789 - val_MinusLogProbMetric: 31.7789 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 840/1000
2023-10-31 18:40:16.965 
Epoch 840/1000 
	 loss: 31.0896, MinusLogProbMetric: 31.0896, val_loss: 31.8417, val_MinusLogProbMetric: 31.8417

Epoch 840: val_loss did not improve from 31.64060
196/196 - 62s - loss: 31.0896 - MinusLogProbMetric: 31.0896 - val_loss: 31.8417 - val_MinusLogProbMetric: 31.8417 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 841/1000
2023-10-31 18:41:19.776 
Epoch 841/1000 
	 loss: 31.1027, MinusLogProbMetric: 31.1027, val_loss: 31.6911, val_MinusLogProbMetric: 31.6911

Epoch 841: val_loss did not improve from 31.64060
196/196 - 63s - loss: 31.1027 - MinusLogProbMetric: 31.1027 - val_loss: 31.6911 - val_MinusLogProbMetric: 31.6911 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 842/1000
2023-10-31 18:42:26.690 
Epoch 842/1000 
	 loss: 31.1270, MinusLogProbMetric: 31.1270, val_loss: 31.6588, val_MinusLogProbMetric: 31.6588

Epoch 842: val_loss did not improve from 31.64060
196/196 - 67s - loss: 31.1270 - MinusLogProbMetric: 31.1270 - val_loss: 31.6588 - val_MinusLogProbMetric: 31.6588 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 843/1000
2023-10-31 18:43:29.006 
Epoch 843/1000 
	 loss: 31.0930, MinusLogProbMetric: 31.0930, val_loss: 31.7215, val_MinusLogProbMetric: 31.7215

Epoch 843: val_loss did not improve from 31.64060
196/196 - 62s - loss: 31.0930 - MinusLogProbMetric: 31.0930 - val_loss: 31.7215 - val_MinusLogProbMetric: 31.7215 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 844/1000
2023-10-31 18:44:33.675 
Epoch 844/1000 
	 loss: 31.0960, MinusLogProbMetric: 31.0960, val_loss: 31.6610, val_MinusLogProbMetric: 31.6610

Epoch 844: val_loss did not improve from 31.64060
196/196 - 65s - loss: 31.0960 - MinusLogProbMetric: 31.0960 - val_loss: 31.6610 - val_MinusLogProbMetric: 31.6610 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 845/1000
2023-10-31 18:45:34.502 
Epoch 845/1000 
	 loss: 31.0861, MinusLogProbMetric: 31.0861, val_loss: 31.6682, val_MinusLogProbMetric: 31.6682

Epoch 845: val_loss did not improve from 31.64060
196/196 - 61s - loss: 31.0861 - MinusLogProbMetric: 31.0861 - val_loss: 31.6682 - val_MinusLogProbMetric: 31.6682 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 846/1000
2023-10-31 18:46:35.134 
Epoch 846/1000 
	 loss: 31.0995, MinusLogProbMetric: 31.0995, val_loss: 31.7463, val_MinusLogProbMetric: 31.7463

Epoch 846: val_loss did not improve from 31.64060
196/196 - 61s - loss: 31.0995 - MinusLogProbMetric: 31.0995 - val_loss: 31.7463 - val_MinusLogProbMetric: 31.7463 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 847/1000
2023-10-31 18:47:37.328 
Epoch 847/1000 
	 loss: 31.0749, MinusLogProbMetric: 31.0749, val_loss: 31.6765, val_MinusLogProbMetric: 31.6765

Epoch 847: val_loss did not improve from 31.64060
196/196 - 62s - loss: 31.0749 - MinusLogProbMetric: 31.0749 - val_loss: 31.6765 - val_MinusLogProbMetric: 31.6765 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 848/1000
2023-10-31 18:48:38.235 
Epoch 848/1000 
	 loss: 31.0774, MinusLogProbMetric: 31.0774, val_loss: 31.6764, val_MinusLogProbMetric: 31.6764

Epoch 848: val_loss did not improve from 31.64060
196/196 - 61s - loss: 31.0774 - MinusLogProbMetric: 31.0774 - val_loss: 31.6764 - val_MinusLogProbMetric: 31.6764 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 849/1000
2023-10-31 18:49:38.154 
Epoch 849/1000 
	 loss: 31.0625, MinusLogProbMetric: 31.0625, val_loss: 31.7840, val_MinusLogProbMetric: 31.7840

Epoch 849: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0625 - MinusLogProbMetric: 31.0625 - val_loss: 31.7840 - val_MinusLogProbMetric: 31.7840 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 850/1000
2023-10-31 18:50:39.289 
Epoch 850/1000 
	 loss: 31.0905, MinusLogProbMetric: 31.0905, val_loss: 31.7421, val_MinusLogProbMetric: 31.7421

Epoch 850: val_loss did not improve from 31.64060
196/196 - 61s - loss: 31.0905 - MinusLogProbMetric: 31.0905 - val_loss: 31.7421 - val_MinusLogProbMetric: 31.7421 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 851/1000
2023-10-31 18:51:39.613 
Epoch 851/1000 
	 loss: 31.0834, MinusLogProbMetric: 31.0834, val_loss: 31.7098, val_MinusLogProbMetric: 31.7098

Epoch 851: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0834 - MinusLogProbMetric: 31.0834 - val_loss: 31.7098 - val_MinusLogProbMetric: 31.7098 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 852/1000
2023-10-31 18:52:39.276 
Epoch 852/1000 
	 loss: 31.0690, MinusLogProbMetric: 31.0690, val_loss: 31.7502, val_MinusLogProbMetric: 31.7502

Epoch 852: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0690 - MinusLogProbMetric: 31.0690 - val_loss: 31.7502 - val_MinusLogProbMetric: 31.7502 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 853/1000
2023-10-31 18:53:40.041 
Epoch 853/1000 
	 loss: 31.1044, MinusLogProbMetric: 31.1044, val_loss: 31.7056, val_MinusLogProbMetric: 31.7056

Epoch 853: val_loss did not improve from 31.64060
196/196 - 61s - loss: 31.1044 - MinusLogProbMetric: 31.1044 - val_loss: 31.7056 - val_MinusLogProbMetric: 31.7056 - lr: 1.3717e-06 - 61s/epoch - 310ms/step
Epoch 854/1000
2023-10-31 18:54:42.593 
Epoch 854/1000 
	 loss: 31.0845, MinusLogProbMetric: 31.0845, val_loss: 31.6683, val_MinusLogProbMetric: 31.6683

Epoch 854: val_loss did not improve from 31.64060
196/196 - 63s - loss: 31.0845 - MinusLogProbMetric: 31.0845 - val_loss: 31.6683 - val_MinusLogProbMetric: 31.6683 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 855/1000
2023-10-31 18:55:45.290 
Epoch 855/1000 
	 loss: 31.0803, MinusLogProbMetric: 31.0803, val_loss: 31.8057, val_MinusLogProbMetric: 31.8057

Epoch 855: val_loss did not improve from 31.64060
196/196 - 63s - loss: 31.0803 - MinusLogProbMetric: 31.0803 - val_loss: 31.8057 - val_MinusLogProbMetric: 31.8057 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 856/1000
2023-10-31 18:56:45.131 
Epoch 856/1000 
	 loss: 31.0639, MinusLogProbMetric: 31.0639, val_loss: 31.7248, val_MinusLogProbMetric: 31.7248

Epoch 856: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0639 - MinusLogProbMetric: 31.0639 - val_loss: 31.7248 - val_MinusLogProbMetric: 31.7248 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 857/1000
2023-10-31 18:57:44.087 
Epoch 857/1000 
	 loss: 31.0833, MinusLogProbMetric: 31.0833, val_loss: 31.6732, val_MinusLogProbMetric: 31.6732

Epoch 857: val_loss did not improve from 31.64060
196/196 - 59s - loss: 31.0833 - MinusLogProbMetric: 31.0833 - val_loss: 31.6732 - val_MinusLogProbMetric: 31.6732 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 858/1000
2023-10-31 18:58:46.104 
Epoch 858/1000 
	 loss: 31.0563, MinusLogProbMetric: 31.0563, val_loss: 31.6876, val_MinusLogProbMetric: 31.6876

Epoch 858: val_loss did not improve from 31.64060
196/196 - 62s - loss: 31.0563 - MinusLogProbMetric: 31.0563 - val_loss: 31.6876 - val_MinusLogProbMetric: 31.6876 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 859/1000
2023-10-31 18:59:45.837 
Epoch 859/1000 
	 loss: 31.0774, MinusLogProbMetric: 31.0774, val_loss: 31.6873, val_MinusLogProbMetric: 31.6873

Epoch 859: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0774 - MinusLogProbMetric: 31.0774 - val_loss: 31.6873 - val_MinusLogProbMetric: 31.6873 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 860/1000
2023-10-31 19:00:45.628 
Epoch 860/1000 
	 loss: 31.0694, MinusLogProbMetric: 31.0694, val_loss: 31.6538, val_MinusLogProbMetric: 31.6538

Epoch 860: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0694 - MinusLogProbMetric: 31.0694 - val_loss: 31.6538 - val_MinusLogProbMetric: 31.6538 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 861/1000
2023-10-31 19:01:44.352 
Epoch 861/1000 
	 loss: 31.0686, MinusLogProbMetric: 31.0686, val_loss: 31.7820, val_MinusLogProbMetric: 31.7820

Epoch 861: val_loss did not improve from 31.64060
196/196 - 59s - loss: 31.0686 - MinusLogProbMetric: 31.0686 - val_loss: 31.7820 - val_MinusLogProbMetric: 31.7820 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 862/1000
2023-10-31 19:02:43.496 
Epoch 862/1000 
	 loss: 31.0920, MinusLogProbMetric: 31.0920, val_loss: 31.6789, val_MinusLogProbMetric: 31.6789

Epoch 862: val_loss did not improve from 31.64060
196/196 - 59s - loss: 31.0920 - MinusLogProbMetric: 31.0920 - val_loss: 31.6789 - val_MinusLogProbMetric: 31.6789 - lr: 1.3717e-06 - 59s/epoch - 302ms/step
Epoch 863/1000
2023-10-31 19:03:43.161 
Epoch 863/1000 
	 loss: 31.0745, MinusLogProbMetric: 31.0745, val_loss: 31.9314, val_MinusLogProbMetric: 31.9314

Epoch 863: val_loss did not improve from 31.64060
196/196 - 60s - loss: 31.0745 - MinusLogProbMetric: 31.0745 - val_loss: 31.9314 - val_MinusLogProbMetric: 31.9314 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 864/1000
2023-10-31 19:04:45.891 
Epoch 864/1000 
	 loss: 31.0958, MinusLogProbMetric: 31.0958, val_loss: 31.7582, val_MinusLogProbMetric: 31.7582

Epoch 864: val_loss did not improve from 31.64060
196/196 - 63s - loss: 31.0958 - MinusLogProbMetric: 31.0958 - val_loss: 31.7582 - val_MinusLogProbMetric: 31.7582 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 865/1000
2023-10-31 19:05:50.036 
Epoch 865/1000 
	 loss: 31.0730, MinusLogProbMetric: 31.0730, val_loss: 31.7088, val_MinusLogProbMetric: 31.7088

Epoch 865: val_loss did not improve from 31.64060
196/196 - 64s - loss: 31.0730 - MinusLogProbMetric: 31.0730 - val_loss: 31.7088 - val_MinusLogProbMetric: 31.7088 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 866/1000
2023-10-31 19:06:53.898 
Epoch 866/1000 
	 loss: 31.0807, MinusLogProbMetric: 31.0807, val_loss: 31.7100, val_MinusLogProbMetric: 31.7100

Epoch 866: val_loss did not improve from 31.64060
196/196 - 64s - loss: 31.0807 - MinusLogProbMetric: 31.0807 - val_loss: 31.7100 - val_MinusLogProbMetric: 31.7100 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 867/1000
2023-10-31 19:07:59.973 
Epoch 867/1000 
	 loss: 31.0563, MinusLogProbMetric: 31.0563, val_loss: 31.8119, val_MinusLogProbMetric: 31.8119

Epoch 867: val_loss did not improve from 31.64060
196/196 - 66s - loss: 31.0563 - MinusLogProbMetric: 31.0563 - val_loss: 31.8119 - val_MinusLogProbMetric: 31.8119 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 868/1000
2023-10-31 19:09:01.220 
Epoch 868/1000 
	 loss: 31.0512, MinusLogProbMetric: 31.0512, val_loss: 31.6612, val_MinusLogProbMetric: 31.6612

Epoch 868: val_loss did not improve from 31.64060
196/196 - 61s - loss: 31.0512 - MinusLogProbMetric: 31.0512 - val_loss: 31.6612 - val_MinusLogProbMetric: 31.6612 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 869/1000
2023-10-31 19:10:03.040 
Epoch 869/1000 
	 loss: 31.0580, MinusLogProbMetric: 31.0580, val_loss: 31.6329, val_MinusLogProbMetric: 31.6329

Epoch 869: val_loss improved from 31.64060 to 31.63291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 31.0580 - MinusLogProbMetric: 31.0580 - val_loss: 31.6329 - val_MinusLogProbMetric: 31.6329 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 870/1000
2023-10-31 19:11:07.421 
Epoch 870/1000 
	 loss: 31.0788, MinusLogProbMetric: 31.0788, val_loss: 31.7293, val_MinusLogProbMetric: 31.7293

Epoch 870: val_loss did not improve from 31.63291
196/196 - 63s - loss: 31.0788 - MinusLogProbMetric: 31.0788 - val_loss: 31.7293 - val_MinusLogProbMetric: 31.7293 - lr: 1.3717e-06 - 63s/epoch - 323ms/step
Epoch 871/1000
2023-10-31 19:12:11.899 
Epoch 871/1000 
	 loss: 31.0540, MinusLogProbMetric: 31.0540, val_loss: 31.8032, val_MinusLogProbMetric: 31.8032

Epoch 871: val_loss did not improve from 31.63291
196/196 - 64s - loss: 31.0540 - MinusLogProbMetric: 31.0540 - val_loss: 31.8032 - val_MinusLogProbMetric: 31.8032 - lr: 1.3717e-06 - 64s/epoch - 329ms/step
Epoch 872/1000
2023-10-31 19:13:13.662 
Epoch 872/1000 
	 loss: 31.0682, MinusLogProbMetric: 31.0682, val_loss: 31.6780, val_MinusLogProbMetric: 31.6780

Epoch 872: val_loss did not improve from 31.63291
196/196 - 62s - loss: 31.0682 - MinusLogProbMetric: 31.0682 - val_loss: 31.6780 - val_MinusLogProbMetric: 31.6780 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 873/1000
2023-10-31 19:14:15.475 
Epoch 873/1000 
	 loss: 31.0578, MinusLogProbMetric: 31.0578, val_loss: 31.6384, val_MinusLogProbMetric: 31.6384

Epoch 873: val_loss did not improve from 31.63291
196/196 - 62s - loss: 31.0578 - MinusLogProbMetric: 31.0578 - val_loss: 31.6384 - val_MinusLogProbMetric: 31.6384 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 874/1000
2023-10-31 19:15:19.722 
Epoch 874/1000 
	 loss: 31.0702, MinusLogProbMetric: 31.0702, val_loss: 31.8589, val_MinusLogProbMetric: 31.8589

Epoch 874: val_loss did not improve from 31.63291
196/196 - 64s - loss: 31.0702 - MinusLogProbMetric: 31.0702 - val_loss: 31.8589 - val_MinusLogProbMetric: 31.8589 - lr: 1.3717e-06 - 64s/epoch - 328ms/step
Epoch 875/1000
2023-10-31 19:16:22.934 
Epoch 875/1000 
	 loss: 31.0482, MinusLogProbMetric: 31.0482, val_loss: 31.7008, val_MinusLogProbMetric: 31.7008

Epoch 875: val_loss did not improve from 31.63291
196/196 - 63s - loss: 31.0482 - MinusLogProbMetric: 31.0482 - val_loss: 31.7008 - val_MinusLogProbMetric: 31.7008 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 876/1000
2023-10-31 19:17:30.519 
Epoch 876/1000 
	 loss: 31.0629, MinusLogProbMetric: 31.0629, val_loss: 31.7157, val_MinusLogProbMetric: 31.7157

Epoch 876: val_loss did not improve from 31.63291
196/196 - 68s - loss: 31.0629 - MinusLogProbMetric: 31.0629 - val_loss: 31.7157 - val_MinusLogProbMetric: 31.7157 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 877/1000
2023-10-31 19:18:34.794 
Epoch 877/1000 
	 loss: 31.0424, MinusLogProbMetric: 31.0424, val_loss: 31.7278, val_MinusLogProbMetric: 31.7278

Epoch 877: val_loss did not improve from 31.63291
196/196 - 64s - loss: 31.0424 - MinusLogProbMetric: 31.0424 - val_loss: 31.7278 - val_MinusLogProbMetric: 31.7278 - lr: 1.3717e-06 - 64s/epoch - 328ms/step
Epoch 878/1000
2023-10-31 19:19:36.692 
Epoch 878/1000 
	 loss: 31.0727, MinusLogProbMetric: 31.0727, val_loss: 31.8887, val_MinusLogProbMetric: 31.8887

Epoch 878: val_loss did not improve from 31.63291
196/196 - 62s - loss: 31.0727 - MinusLogProbMetric: 31.0727 - val_loss: 31.8887 - val_MinusLogProbMetric: 31.8887 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 879/1000
2023-10-31 19:20:38.072 
Epoch 879/1000 
	 loss: 31.0422, MinusLogProbMetric: 31.0422, val_loss: 31.7209, val_MinusLogProbMetric: 31.7209

Epoch 879: val_loss did not improve from 31.63291
196/196 - 61s - loss: 31.0422 - MinusLogProbMetric: 31.0422 - val_loss: 31.7209 - val_MinusLogProbMetric: 31.7209 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 880/1000
2023-10-31 19:21:40.096 
Epoch 880/1000 
	 loss: 31.0488, MinusLogProbMetric: 31.0488, val_loss: 31.6967, val_MinusLogProbMetric: 31.6967

Epoch 880: val_loss did not improve from 31.63291
196/196 - 62s - loss: 31.0488 - MinusLogProbMetric: 31.0488 - val_loss: 31.6967 - val_MinusLogProbMetric: 31.6967 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 881/1000
2023-10-31 19:22:42.941 
Epoch 881/1000 
	 loss: 31.0726, MinusLogProbMetric: 31.0726, val_loss: 31.7862, val_MinusLogProbMetric: 31.7862

Epoch 881: val_loss did not improve from 31.63291
196/196 - 63s - loss: 31.0726 - MinusLogProbMetric: 31.0726 - val_loss: 31.7862 - val_MinusLogProbMetric: 31.7862 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 882/1000
2023-10-31 19:23:45.458 
Epoch 882/1000 
	 loss: 31.0853, MinusLogProbMetric: 31.0853, val_loss: 31.8090, val_MinusLogProbMetric: 31.8090

Epoch 882: val_loss did not improve from 31.63291
196/196 - 63s - loss: 31.0853 - MinusLogProbMetric: 31.0853 - val_loss: 31.8090 - val_MinusLogProbMetric: 31.8090 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 883/1000
2023-10-31 19:24:50.537 
Epoch 883/1000 
	 loss: 31.0518, MinusLogProbMetric: 31.0518, val_loss: 31.7313, val_MinusLogProbMetric: 31.7313

Epoch 883: val_loss did not improve from 31.63291
196/196 - 65s - loss: 31.0518 - MinusLogProbMetric: 31.0518 - val_loss: 31.7313 - val_MinusLogProbMetric: 31.7313 - lr: 1.3717e-06 - 65s/epoch - 332ms/step
Epoch 884/1000
2023-10-31 19:25:57.011 
Epoch 884/1000 
	 loss: 31.0471, MinusLogProbMetric: 31.0471, val_loss: 31.6304, val_MinusLogProbMetric: 31.6304

Epoch 884: val_loss improved from 31.63291 to 31.63036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 69s - loss: 31.0471 - MinusLogProbMetric: 31.0471 - val_loss: 31.6304 - val_MinusLogProbMetric: 31.6304 - lr: 1.3717e-06 - 69s/epoch - 352ms/step
Epoch 885/1000
2023-10-31 19:27:06.512 
Epoch 885/1000 
	 loss: 31.0369, MinusLogProbMetric: 31.0369, val_loss: 31.8730, val_MinusLogProbMetric: 31.8730

Epoch 885: val_loss did not improve from 31.63036
196/196 - 67s - loss: 31.0369 - MinusLogProbMetric: 31.0369 - val_loss: 31.8730 - val_MinusLogProbMetric: 31.8730 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 886/1000
2023-10-31 19:28:12.755 
Epoch 886/1000 
	 loss: 31.0606, MinusLogProbMetric: 31.0606, val_loss: 31.7903, val_MinusLogProbMetric: 31.7903

Epoch 886: val_loss did not improve from 31.63036
196/196 - 66s - loss: 31.0606 - MinusLogProbMetric: 31.0606 - val_loss: 31.7903 - val_MinusLogProbMetric: 31.7903 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 887/1000
2023-10-31 19:29:18.742 
Epoch 887/1000 
	 loss: 31.0388, MinusLogProbMetric: 31.0388, val_loss: 31.6805, val_MinusLogProbMetric: 31.6805

Epoch 887: val_loss did not improve from 31.63036
196/196 - 66s - loss: 31.0388 - MinusLogProbMetric: 31.0388 - val_loss: 31.6805 - val_MinusLogProbMetric: 31.6805 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 888/1000
2023-10-31 19:30:20.793 
Epoch 888/1000 
	 loss: 31.0272, MinusLogProbMetric: 31.0272, val_loss: 31.6517, val_MinusLogProbMetric: 31.6517

Epoch 888: val_loss did not improve from 31.63036
196/196 - 62s - loss: 31.0272 - MinusLogProbMetric: 31.0272 - val_loss: 31.6517 - val_MinusLogProbMetric: 31.6517 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 889/1000
2023-10-31 19:31:22.396 
Epoch 889/1000 
	 loss: 31.0407, MinusLogProbMetric: 31.0407, val_loss: 31.8015, val_MinusLogProbMetric: 31.8015

Epoch 889: val_loss did not improve from 31.63036
196/196 - 62s - loss: 31.0407 - MinusLogProbMetric: 31.0407 - val_loss: 31.8015 - val_MinusLogProbMetric: 31.8015 - lr: 1.3717e-06 - 62s/epoch - 314ms/step
Epoch 890/1000
2023-10-31 19:32:25.027 
Epoch 890/1000 
	 loss: 31.0326, MinusLogProbMetric: 31.0326, val_loss: 31.6245, val_MinusLogProbMetric: 31.6245

Epoch 890: val_loss improved from 31.63036 to 31.62455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 31.0326 - MinusLogProbMetric: 31.0326 - val_loss: 31.6245 - val_MinusLogProbMetric: 31.6245 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 891/1000
2023-10-31 19:33:34.116 
Epoch 891/1000 
	 loss: 31.0447, MinusLogProbMetric: 31.0447, val_loss: 31.7624, val_MinusLogProbMetric: 31.7624

Epoch 891: val_loss did not improve from 31.62455
196/196 - 68s - loss: 31.0447 - MinusLogProbMetric: 31.0447 - val_loss: 31.7624 - val_MinusLogProbMetric: 31.7624 - lr: 1.3717e-06 - 68s/epoch - 347ms/step
Epoch 892/1000
2023-10-31 19:34:38.209 
Epoch 892/1000 
	 loss: 31.0445, MinusLogProbMetric: 31.0445, val_loss: 31.6695, val_MinusLogProbMetric: 31.6695

Epoch 892: val_loss did not improve from 31.62455
196/196 - 64s - loss: 31.0445 - MinusLogProbMetric: 31.0445 - val_loss: 31.6695 - val_MinusLogProbMetric: 31.6695 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 893/1000
2023-10-31 19:35:37.021 
Epoch 893/1000 
	 loss: 31.0573, MinusLogProbMetric: 31.0573, val_loss: 31.7072, val_MinusLogProbMetric: 31.7072

Epoch 893: val_loss did not improve from 31.62455
196/196 - 59s - loss: 31.0573 - MinusLogProbMetric: 31.0573 - val_loss: 31.7072 - val_MinusLogProbMetric: 31.7072 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 894/1000
2023-10-31 19:36:37.244 
Epoch 894/1000 
	 loss: 31.0054, MinusLogProbMetric: 31.0054, val_loss: 31.6714, val_MinusLogProbMetric: 31.6714

Epoch 894: val_loss did not improve from 31.62455
196/196 - 60s - loss: 31.0054 - MinusLogProbMetric: 31.0054 - val_loss: 31.6714 - val_MinusLogProbMetric: 31.6714 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 895/1000
2023-10-31 19:37:35.098 
Epoch 895/1000 
	 loss: 31.0388, MinusLogProbMetric: 31.0388, val_loss: 31.6658, val_MinusLogProbMetric: 31.6658

Epoch 895: val_loss did not improve from 31.62455
196/196 - 58s - loss: 31.0388 - MinusLogProbMetric: 31.0388 - val_loss: 31.6658 - val_MinusLogProbMetric: 31.6658 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 896/1000
2023-10-31 19:38:32.583 
Epoch 896/1000 
	 loss: 31.0271, MinusLogProbMetric: 31.0271, val_loss: 31.6747, val_MinusLogProbMetric: 31.6747

Epoch 896: val_loss did not improve from 31.62455
196/196 - 57s - loss: 31.0271 - MinusLogProbMetric: 31.0271 - val_loss: 31.6747 - val_MinusLogProbMetric: 31.6747 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 897/1000
2023-10-31 19:39:28.757 
Epoch 897/1000 
	 loss: 31.0377, MinusLogProbMetric: 31.0377, val_loss: 31.7185, val_MinusLogProbMetric: 31.7185

Epoch 897: val_loss did not improve from 31.62455
196/196 - 56s - loss: 31.0377 - MinusLogProbMetric: 31.0377 - val_loss: 31.7185 - val_MinusLogProbMetric: 31.7185 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 898/1000
2023-10-31 19:40:27.815 
Epoch 898/1000 
	 loss: 31.0177, MinusLogProbMetric: 31.0177, val_loss: 31.6329, val_MinusLogProbMetric: 31.6329

Epoch 898: val_loss did not improve from 31.62455
196/196 - 59s - loss: 31.0177 - MinusLogProbMetric: 31.0177 - val_loss: 31.6329 - val_MinusLogProbMetric: 31.6329 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 899/1000
2023-10-31 19:41:25.339 
Epoch 899/1000 
	 loss: 31.0350, MinusLogProbMetric: 31.0350, val_loss: 31.6710, val_MinusLogProbMetric: 31.6710

Epoch 899: val_loss did not improve from 31.62455
196/196 - 58s - loss: 31.0350 - MinusLogProbMetric: 31.0350 - val_loss: 31.6710 - val_MinusLogProbMetric: 31.6710 - lr: 1.3717e-06 - 58s/epoch - 293ms/step
Epoch 900/1000
2023-10-31 19:42:22.133 
Epoch 900/1000 
	 loss: 31.0093, MinusLogProbMetric: 31.0093, val_loss: 31.6446, val_MinusLogProbMetric: 31.6446

Epoch 900: val_loss did not improve from 31.62455
196/196 - 57s - loss: 31.0093 - MinusLogProbMetric: 31.0093 - val_loss: 31.6446 - val_MinusLogProbMetric: 31.6446 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 901/1000
2023-10-31 19:43:24.010 
Epoch 901/1000 
	 loss: 31.0272, MinusLogProbMetric: 31.0272, val_loss: 31.6736, val_MinusLogProbMetric: 31.6736

Epoch 901: val_loss did not improve from 31.62455
196/196 - 62s - loss: 31.0272 - MinusLogProbMetric: 31.0272 - val_loss: 31.6736 - val_MinusLogProbMetric: 31.6736 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 902/1000
2023-10-31 19:44:27.538 
Epoch 902/1000 
	 loss: 31.0272, MinusLogProbMetric: 31.0272, val_loss: 31.6216, val_MinusLogProbMetric: 31.6216

Epoch 902: val_loss improved from 31.62455 to 31.62157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 31.0272 - MinusLogProbMetric: 31.0272 - val_loss: 31.6216 - val_MinusLogProbMetric: 31.6216 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 903/1000
2023-10-31 19:45:33.197 
Epoch 903/1000 
	 loss: 31.0406, MinusLogProbMetric: 31.0406, val_loss: 31.7005, val_MinusLogProbMetric: 31.7005

Epoch 903: val_loss did not improve from 31.62157
196/196 - 64s - loss: 31.0406 - MinusLogProbMetric: 31.0406 - val_loss: 31.7005 - val_MinusLogProbMetric: 31.7005 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 904/1000
2023-10-31 19:46:39.087 
Epoch 904/1000 
	 loss: 31.0171, MinusLogProbMetric: 31.0171, val_loss: 31.6350, val_MinusLogProbMetric: 31.6350

Epoch 904: val_loss did not improve from 31.62157
196/196 - 66s - loss: 31.0171 - MinusLogProbMetric: 31.0171 - val_loss: 31.6350 - val_MinusLogProbMetric: 31.6350 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 905/1000
2023-10-31 19:47:38.788 
Epoch 905/1000 
	 loss: 31.0210, MinusLogProbMetric: 31.0210, val_loss: 31.6396, val_MinusLogProbMetric: 31.6396

Epoch 905: val_loss did not improve from 31.62157
196/196 - 60s - loss: 31.0210 - MinusLogProbMetric: 31.0210 - val_loss: 31.6396 - val_MinusLogProbMetric: 31.6396 - lr: 1.3717e-06 - 60s/epoch - 305ms/step
Epoch 906/1000
2023-10-31 19:48:40.809 
Epoch 906/1000 
	 loss: 31.0357, MinusLogProbMetric: 31.0357, val_loss: 31.6465, val_MinusLogProbMetric: 31.6465

Epoch 906: val_loss did not improve from 31.62157
196/196 - 62s - loss: 31.0357 - MinusLogProbMetric: 31.0357 - val_loss: 31.6465 - val_MinusLogProbMetric: 31.6465 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 907/1000
2023-10-31 19:49:40.286 
Epoch 907/1000 
	 loss: 31.0070, MinusLogProbMetric: 31.0070, val_loss: 31.7702, val_MinusLogProbMetric: 31.7702

Epoch 907: val_loss did not improve from 31.62157
196/196 - 59s - loss: 31.0070 - MinusLogProbMetric: 31.0070 - val_loss: 31.7702 - val_MinusLogProbMetric: 31.7702 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 908/1000
2023-10-31 19:50:38.856 
Epoch 908/1000 
	 loss: 31.0461, MinusLogProbMetric: 31.0461, val_loss: 31.7732, val_MinusLogProbMetric: 31.7732

Epoch 908: val_loss did not improve from 31.62157
196/196 - 59s - loss: 31.0461 - MinusLogProbMetric: 31.0461 - val_loss: 31.7732 - val_MinusLogProbMetric: 31.7732 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 909/1000
2023-10-31 19:51:37.367 
Epoch 909/1000 
	 loss: 31.0234, MinusLogProbMetric: 31.0234, val_loss: 31.6902, val_MinusLogProbMetric: 31.6902

Epoch 909: val_loss did not improve from 31.62157
196/196 - 59s - loss: 31.0234 - MinusLogProbMetric: 31.0234 - val_loss: 31.6902 - val_MinusLogProbMetric: 31.6902 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 910/1000
2023-10-31 19:52:43.524 
Epoch 910/1000 
	 loss: 31.0112, MinusLogProbMetric: 31.0112, val_loss: 31.6948, val_MinusLogProbMetric: 31.6948

Epoch 910: val_loss did not improve from 31.62157
196/196 - 66s - loss: 31.0112 - MinusLogProbMetric: 31.0112 - val_loss: 31.6948 - val_MinusLogProbMetric: 31.6948 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 911/1000
2023-10-31 19:53:49.862 
Epoch 911/1000 
	 loss: 31.0234, MinusLogProbMetric: 31.0234, val_loss: 31.7139, val_MinusLogProbMetric: 31.7139

Epoch 911: val_loss did not improve from 31.62157
196/196 - 66s - loss: 31.0234 - MinusLogProbMetric: 31.0234 - val_loss: 31.7139 - val_MinusLogProbMetric: 31.7139 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 912/1000
2023-10-31 19:54:51.649 
Epoch 912/1000 
	 loss: 31.0225, MinusLogProbMetric: 31.0225, val_loss: 31.6639, val_MinusLogProbMetric: 31.6639

Epoch 912: val_loss did not improve from 31.62157
196/196 - 62s - loss: 31.0225 - MinusLogProbMetric: 31.0225 - val_loss: 31.6639 - val_MinusLogProbMetric: 31.6639 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 913/1000
2023-10-31 19:55:54.134 
Epoch 913/1000 
	 loss: 31.0204, MinusLogProbMetric: 31.0204, val_loss: 31.6348, val_MinusLogProbMetric: 31.6348

Epoch 913: val_loss did not improve from 31.62157
196/196 - 62s - loss: 31.0204 - MinusLogProbMetric: 31.0204 - val_loss: 31.6348 - val_MinusLogProbMetric: 31.6348 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 914/1000
2023-10-31 19:57:00.534 
Epoch 914/1000 
	 loss: 31.0121, MinusLogProbMetric: 31.0121, val_loss: 31.6846, val_MinusLogProbMetric: 31.6846

Epoch 914: val_loss did not improve from 31.62157
196/196 - 66s - loss: 31.0121 - MinusLogProbMetric: 31.0121 - val_loss: 31.6846 - val_MinusLogProbMetric: 31.6846 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 915/1000
2023-10-31 19:58:08.303 
Epoch 915/1000 
	 loss: 31.0426, MinusLogProbMetric: 31.0426, val_loss: 31.6104, val_MinusLogProbMetric: 31.6104

Epoch 915: val_loss improved from 31.62157 to 31.61045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 70s - loss: 31.0426 - MinusLogProbMetric: 31.0426 - val_loss: 31.6104 - val_MinusLogProbMetric: 31.6104 - lr: 1.3717e-06 - 70s/epoch - 357ms/step
Epoch 916/1000
2023-10-31 19:59:17.448 
Epoch 916/1000 
	 loss: 31.0254, MinusLogProbMetric: 31.0254, val_loss: 31.6957, val_MinusLogProbMetric: 31.6957

Epoch 916: val_loss did not improve from 31.61045
196/196 - 67s - loss: 31.0254 - MinusLogProbMetric: 31.0254 - val_loss: 31.6957 - val_MinusLogProbMetric: 31.6957 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 917/1000
2023-10-31 20:00:19.480 
Epoch 917/1000 
	 loss: 31.0060, MinusLogProbMetric: 31.0060, val_loss: 31.6262, val_MinusLogProbMetric: 31.6262

Epoch 917: val_loss did not improve from 31.61045
196/196 - 62s - loss: 31.0060 - MinusLogProbMetric: 31.0060 - val_loss: 31.6262 - val_MinusLogProbMetric: 31.6262 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 918/1000
2023-10-31 20:01:22.196 
Epoch 918/1000 
	 loss: 31.0051, MinusLogProbMetric: 31.0051, val_loss: 31.6484, val_MinusLogProbMetric: 31.6484

Epoch 918: val_loss did not improve from 31.61045
196/196 - 63s - loss: 31.0051 - MinusLogProbMetric: 31.0051 - val_loss: 31.6484 - val_MinusLogProbMetric: 31.6484 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 919/1000
2023-10-31 20:02:20.902 
Epoch 919/1000 
	 loss: 31.0354, MinusLogProbMetric: 31.0354, val_loss: 31.5821, val_MinusLogProbMetric: 31.5821

Epoch 919: val_loss improved from 31.61045 to 31.58212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 31.0354 - MinusLogProbMetric: 31.0354 - val_loss: 31.5821 - val_MinusLogProbMetric: 31.5821 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 920/1000
2023-10-31 20:03:23.316 
Epoch 920/1000 
	 loss: 30.9864, MinusLogProbMetric: 30.9864, val_loss: 31.6401, val_MinusLogProbMetric: 31.6401

Epoch 920: val_loss did not improve from 31.58212
196/196 - 61s - loss: 30.9864 - MinusLogProbMetric: 30.9864 - val_loss: 31.6401 - val_MinusLogProbMetric: 31.6401 - lr: 1.3717e-06 - 61s/epoch - 314ms/step
Epoch 921/1000
2023-10-31 20:04:21.175 
Epoch 921/1000 
	 loss: 31.0553, MinusLogProbMetric: 31.0553, val_loss: 31.6362, val_MinusLogProbMetric: 31.6362

Epoch 921: val_loss did not improve from 31.58212
196/196 - 58s - loss: 31.0553 - MinusLogProbMetric: 31.0553 - val_loss: 31.6362 - val_MinusLogProbMetric: 31.6362 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 922/1000
2023-10-31 20:05:25.050 
Epoch 922/1000 
	 loss: 30.9983, MinusLogProbMetric: 30.9983, val_loss: 31.7678, val_MinusLogProbMetric: 31.7678

Epoch 922: val_loss did not improve from 31.58212
196/196 - 64s - loss: 30.9983 - MinusLogProbMetric: 30.9983 - val_loss: 31.7678 - val_MinusLogProbMetric: 31.7678 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 923/1000
2023-10-31 20:06:27.822 
Epoch 923/1000 
	 loss: 30.9901, MinusLogProbMetric: 30.9901, val_loss: 31.6891, val_MinusLogProbMetric: 31.6891

Epoch 923: val_loss did not improve from 31.58212
196/196 - 63s - loss: 30.9901 - MinusLogProbMetric: 30.9901 - val_loss: 31.6891 - val_MinusLogProbMetric: 31.6891 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 924/1000
2023-10-31 20:07:30.104 
Epoch 924/1000 
	 loss: 30.9902, MinusLogProbMetric: 30.9902, val_loss: 31.6845, val_MinusLogProbMetric: 31.6845

Epoch 924: val_loss did not improve from 31.58212
196/196 - 62s - loss: 30.9902 - MinusLogProbMetric: 30.9902 - val_loss: 31.6845 - val_MinusLogProbMetric: 31.6845 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 925/1000
2023-10-31 20:08:34.136 
Epoch 925/1000 
	 loss: 30.9878, MinusLogProbMetric: 30.9878, val_loss: 31.6277, val_MinusLogProbMetric: 31.6277

Epoch 925: val_loss did not improve from 31.58212
196/196 - 64s - loss: 30.9878 - MinusLogProbMetric: 30.9878 - val_loss: 31.6277 - val_MinusLogProbMetric: 31.6277 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 926/1000
2023-10-31 20:09:38.435 
Epoch 926/1000 
	 loss: 31.0243, MinusLogProbMetric: 31.0243, val_loss: 31.6458, val_MinusLogProbMetric: 31.6458

Epoch 926: val_loss did not improve from 31.58212
196/196 - 64s - loss: 31.0243 - MinusLogProbMetric: 31.0243 - val_loss: 31.6458 - val_MinusLogProbMetric: 31.6458 - lr: 1.3717e-06 - 64s/epoch - 328ms/step
Epoch 927/1000
2023-10-31 20:10:41.120 
Epoch 927/1000 
	 loss: 30.9782, MinusLogProbMetric: 30.9782, val_loss: 31.7863, val_MinusLogProbMetric: 31.7863

Epoch 927: val_loss did not improve from 31.58212
196/196 - 63s - loss: 30.9782 - MinusLogProbMetric: 30.9782 - val_loss: 31.7863 - val_MinusLogProbMetric: 31.7863 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 928/1000
2023-10-31 20:11:43.595 
Epoch 928/1000 
	 loss: 31.0193, MinusLogProbMetric: 31.0193, val_loss: 31.6500, val_MinusLogProbMetric: 31.6500

Epoch 928: val_loss did not improve from 31.58212
196/196 - 62s - loss: 31.0193 - MinusLogProbMetric: 31.0193 - val_loss: 31.6500 - val_MinusLogProbMetric: 31.6500 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 929/1000
2023-10-31 20:12:48.376 
Epoch 929/1000 
	 loss: 30.9884, MinusLogProbMetric: 30.9884, val_loss: 31.6397, val_MinusLogProbMetric: 31.6397

Epoch 929: val_loss did not improve from 31.58212
196/196 - 65s - loss: 30.9884 - MinusLogProbMetric: 30.9884 - val_loss: 31.6397 - val_MinusLogProbMetric: 31.6397 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 930/1000
2023-10-31 20:13:53.563 
Epoch 930/1000 
	 loss: 31.0147, MinusLogProbMetric: 31.0147, val_loss: 31.6887, val_MinusLogProbMetric: 31.6887

Epoch 930: val_loss did not improve from 31.58212
196/196 - 65s - loss: 31.0147 - MinusLogProbMetric: 31.0147 - val_loss: 31.6887 - val_MinusLogProbMetric: 31.6887 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 931/1000
2023-10-31 20:15:01.648 
Epoch 931/1000 
	 loss: 30.9989, MinusLogProbMetric: 30.9989, val_loss: 31.6782, val_MinusLogProbMetric: 31.6782

Epoch 931: val_loss did not improve from 31.58212
196/196 - 68s - loss: 30.9989 - MinusLogProbMetric: 30.9989 - val_loss: 31.6782 - val_MinusLogProbMetric: 31.6782 - lr: 1.3717e-06 - 68s/epoch - 347ms/step
Epoch 932/1000
2023-10-31 20:16:05.898 
Epoch 932/1000 
	 loss: 31.0087, MinusLogProbMetric: 31.0087, val_loss: 31.6588, val_MinusLogProbMetric: 31.6588

Epoch 932: val_loss did not improve from 31.58212
196/196 - 64s - loss: 31.0087 - MinusLogProbMetric: 31.0087 - val_loss: 31.6588 - val_MinusLogProbMetric: 31.6588 - lr: 1.3717e-06 - 64s/epoch - 328ms/step
Epoch 933/1000
2023-10-31 20:17:07.081 
Epoch 933/1000 
	 loss: 30.9916, MinusLogProbMetric: 30.9916, val_loss: 31.6302, val_MinusLogProbMetric: 31.6302

Epoch 933: val_loss did not improve from 31.58212
196/196 - 61s - loss: 30.9916 - MinusLogProbMetric: 30.9916 - val_loss: 31.6302 - val_MinusLogProbMetric: 31.6302 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 934/1000
2023-10-31 20:18:07.094 
Epoch 934/1000 
	 loss: 30.9961, MinusLogProbMetric: 30.9961, val_loss: 31.7086, val_MinusLogProbMetric: 31.7086

Epoch 934: val_loss did not improve from 31.58212
196/196 - 60s - loss: 30.9961 - MinusLogProbMetric: 30.9961 - val_loss: 31.7086 - val_MinusLogProbMetric: 31.7086 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 935/1000
2023-10-31 20:19:05.919 
Epoch 935/1000 
	 loss: 30.9848, MinusLogProbMetric: 30.9848, val_loss: 31.5746, val_MinusLogProbMetric: 31.5746

Epoch 935: val_loss improved from 31.58212 to 31.57458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 30.9848 - MinusLogProbMetric: 30.9848 - val_loss: 31.5746 - val_MinusLogProbMetric: 31.5746 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 936/1000
2023-10-31 20:20:08.466 
Epoch 936/1000 
	 loss: 31.0001, MinusLogProbMetric: 31.0001, val_loss: 31.6555, val_MinusLogProbMetric: 31.6555

Epoch 936: val_loss did not improve from 31.57458
196/196 - 61s - loss: 31.0001 - MinusLogProbMetric: 31.0001 - val_loss: 31.6555 - val_MinusLogProbMetric: 31.6555 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 937/1000
2023-10-31 20:21:08.365 
Epoch 937/1000 
	 loss: 30.9868, MinusLogProbMetric: 30.9868, val_loss: 31.6681, val_MinusLogProbMetric: 31.6681

Epoch 937: val_loss did not improve from 31.57458
196/196 - 60s - loss: 30.9868 - MinusLogProbMetric: 30.9868 - val_loss: 31.6681 - val_MinusLogProbMetric: 31.6681 - lr: 1.3717e-06 - 60s/epoch - 306ms/step
Epoch 938/1000
2023-10-31 20:22:08.006 
Epoch 938/1000 
	 loss: 30.9868, MinusLogProbMetric: 30.9868, val_loss: 31.6399, val_MinusLogProbMetric: 31.6399

Epoch 938: val_loss did not improve from 31.57458
196/196 - 60s - loss: 30.9868 - MinusLogProbMetric: 30.9868 - val_loss: 31.6399 - val_MinusLogProbMetric: 31.6399 - lr: 1.3717e-06 - 60s/epoch - 304ms/step
Epoch 939/1000
2023-10-31 20:23:12.027 
Epoch 939/1000 
	 loss: 31.0156, MinusLogProbMetric: 31.0156, val_loss: 31.6970, val_MinusLogProbMetric: 31.6970

Epoch 939: val_loss did not improve from 31.57458
196/196 - 64s - loss: 31.0156 - MinusLogProbMetric: 31.0156 - val_loss: 31.6970 - val_MinusLogProbMetric: 31.6970 - lr: 1.3717e-06 - 64s/epoch - 327ms/step
Epoch 940/1000
2023-10-31 20:24:15.641 
Epoch 940/1000 
	 loss: 30.9735, MinusLogProbMetric: 30.9735, val_loss: 31.6953, val_MinusLogProbMetric: 31.6953

Epoch 940: val_loss did not improve from 31.57458
196/196 - 64s - loss: 30.9735 - MinusLogProbMetric: 30.9735 - val_loss: 31.6953 - val_MinusLogProbMetric: 31.6953 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 941/1000
2023-10-31 20:25:15.958 
Epoch 941/1000 
	 loss: 31.0141, MinusLogProbMetric: 31.0141, val_loss: 31.6037, val_MinusLogProbMetric: 31.6037

Epoch 941: val_loss did not improve from 31.57458
196/196 - 60s - loss: 31.0141 - MinusLogProbMetric: 31.0141 - val_loss: 31.6037 - val_MinusLogProbMetric: 31.6037 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 942/1000
2023-10-31 20:26:17.891 
Epoch 942/1000 
	 loss: 31.0225, MinusLogProbMetric: 31.0225, val_loss: 31.6188, val_MinusLogProbMetric: 31.6188

Epoch 942: val_loss did not improve from 31.57458
196/196 - 62s - loss: 31.0225 - MinusLogProbMetric: 31.0225 - val_loss: 31.6188 - val_MinusLogProbMetric: 31.6188 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 943/1000
2023-10-31 20:27:20.508 
Epoch 943/1000 
	 loss: 30.9750, MinusLogProbMetric: 30.9750, val_loss: 31.5944, val_MinusLogProbMetric: 31.5944

Epoch 943: val_loss did not improve from 31.57458
196/196 - 63s - loss: 30.9750 - MinusLogProbMetric: 30.9750 - val_loss: 31.5944 - val_MinusLogProbMetric: 31.5944 - lr: 1.3717e-06 - 63s/epoch - 319ms/step
Epoch 944/1000
2023-10-31 20:28:23.942 
Epoch 944/1000 
	 loss: 30.9740, MinusLogProbMetric: 30.9740, val_loss: 31.6424, val_MinusLogProbMetric: 31.6424

Epoch 944: val_loss did not improve from 31.57458
196/196 - 63s - loss: 30.9740 - MinusLogProbMetric: 30.9740 - val_loss: 31.6424 - val_MinusLogProbMetric: 31.6424 - lr: 1.3717e-06 - 63s/epoch - 324ms/step
Epoch 945/1000
2023-10-31 20:29:30.131 
Epoch 945/1000 
	 loss: 31.0001, MinusLogProbMetric: 31.0001, val_loss: 31.6851, val_MinusLogProbMetric: 31.6851

Epoch 945: val_loss did not improve from 31.57458
196/196 - 66s - loss: 31.0001 - MinusLogProbMetric: 31.0001 - val_loss: 31.6851 - val_MinusLogProbMetric: 31.6851 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 946/1000
2023-10-31 20:30:32.348 
Epoch 946/1000 
	 loss: 30.9979, MinusLogProbMetric: 30.9979, val_loss: 31.6398, val_MinusLogProbMetric: 31.6398

Epoch 946: val_loss did not improve from 31.57458
196/196 - 62s - loss: 30.9979 - MinusLogProbMetric: 30.9979 - val_loss: 31.6398 - val_MinusLogProbMetric: 31.6398 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 947/1000
2023-10-31 20:31:31.273 
Epoch 947/1000 
	 loss: 31.0412, MinusLogProbMetric: 31.0412, val_loss: 31.6400, val_MinusLogProbMetric: 31.6400

Epoch 947: val_loss did not improve from 31.57458
196/196 - 59s - loss: 31.0412 - MinusLogProbMetric: 31.0412 - val_loss: 31.6400 - val_MinusLogProbMetric: 31.6400 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 948/1000
2023-10-31 20:32:31.368 
Epoch 948/1000 
	 loss: 30.9760, MinusLogProbMetric: 30.9760, val_loss: 31.7058, val_MinusLogProbMetric: 31.7058

Epoch 948: val_loss did not improve from 31.57458
196/196 - 60s - loss: 30.9760 - MinusLogProbMetric: 30.9760 - val_loss: 31.7058 - val_MinusLogProbMetric: 31.7058 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 949/1000
2023-10-31 20:33:32.391 
Epoch 949/1000 
	 loss: 30.9931, MinusLogProbMetric: 30.9931, val_loss: 31.6750, val_MinusLogProbMetric: 31.6750

Epoch 949: val_loss did not improve from 31.57458
196/196 - 61s - loss: 30.9931 - MinusLogProbMetric: 30.9931 - val_loss: 31.6750 - val_MinusLogProbMetric: 31.6750 - lr: 1.3717e-06 - 61s/epoch - 311ms/step
Epoch 950/1000
2023-10-31 20:34:32.646 
Epoch 950/1000 
	 loss: 30.9873, MinusLogProbMetric: 30.9873, val_loss: 31.6190, val_MinusLogProbMetric: 31.6190

Epoch 950: val_loss did not improve from 31.57458
196/196 - 60s - loss: 30.9873 - MinusLogProbMetric: 30.9873 - val_loss: 31.6190 - val_MinusLogProbMetric: 31.6190 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 951/1000
2023-10-31 20:35:32.911 
Epoch 951/1000 
	 loss: 30.9565, MinusLogProbMetric: 30.9565, val_loss: 31.5797, val_MinusLogProbMetric: 31.5797

Epoch 951: val_loss did not improve from 31.57458
196/196 - 60s - loss: 30.9565 - MinusLogProbMetric: 30.9565 - val_loss: 31.5797 - val_MinusLogProbMetric: 31.5797 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 952/1000
2023-10-31 20:36:34.615 
Epoch 952/1000 
	 loss: 30.9632, MinusLogProbMetric: 30.9632, val_loss: 31.6448, val_MinusLogProbMetric: 31.6448

Epoch 952: val_loss did not improve from 31.57458
196/196 - 62s - loss: 30.9632 - MinusLogProbMetric: 30.9632 - val_loss: 31.6448 - val_MinusLogProbMetric: 31.6448 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 953/1000
2023-10-31 20:37:33.926 
Epoch 953/1000 
	 loss: 30.9946, MinusLogProbMetric: 30.9946, val_loss: 31.6870, val_MinusLogProbMetric: 31.6870

Epoch 953: val_loss did not improve from 31.57458
196/196 - 59s - loss: 30.9946 - MinusLogProbMetric: 30.9946 - val_loss: 31.6870 - val_MinusLogProbMetric: 31.6870 - lr: 1.3717e-06 - 59s/epoch - 303ms/step
Epoch 954/1000
2023-10-31 20:38:41.359 
Epoch 954/1000 
	 loss: 30.9647, MinusLogProbMetric: 30.9647, val_loss: 31.7056, val_MinusLogProbMetric: 31.7056

Epoch 954: val_loss did not improve from 31.57458
196/196 - 67s - loss: 30.9647 - MinusLogProbMetric: 30.9647 - val_loss: 31.7056 - val_MinusLogProbMetric: 31.7056 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 955/1000
2023-10-31 20:39:48.755 
Epoch 955/1000 
	 loss: 31.0039, MinusLogProbMetric: 31.0039, val_loss: 31.6247, val_MinusLogProbMetric: 31.6247

Epoch 955: val_loss did not improve from 31.57458
196/196 - 67s - loss: 31.0039 - MinusLogProbMetric: 31.0039 - val_loss: 31.6247 - val_MinusLogProbMetric: 31.6247 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 956/1000
2023-10-31 20:40:55.296 
Epoch 956/1000 
	 loss: 30.9899, MinusLogProbMetric: 30.9899, val_loss: 31.6034, val_MinusLogProbMetric: 31.6034

Epoch 956: val_loss did not improve from 31.57458
196/196 - 67s - loss: 30.9899 - MinusLogProbMetric: 30.9899 - val_loss: 31.6034 - val_MinusLogProbMetric: 31.6034 - lr: 1.3717e-06 - 67s/epoch - 339ms/step
Epoch 957/1000
2023-10-31 20:41:57.960 
Epoch 957/1000 
	 loss: 40.2790, MinusLogProbMetric: 40.2790, val_loss: 34.5626, val_MinusLogProbMetric: 34.5626

Epoch 957: val_loss did not improve from 31.57458
196/196 - 63s - loss: 40.2790 - MinusLogProbMetric: 40.2790 - val_loss: 34.5626 - val_MinusLogProbMetric: 34.5626 - lr: 1.3717e-06 - 63s/epoch - 320ms/step
Epoch 958/1000
2023-10-31 20:43:01.883 
Epoch 958/1000 
	 loss: 33.4340, MinusLogProbMetric: 33.4340, val_loss: 33.5755, val_MinusLogProbMetric: 33.5755

Epoch 958: val_loss did not improve from 31.57458
196/196 - 64s - loss: 33.4340 - MinusLogProbMetric: 33.4340 - val_loss: 33.5755 - val_MinusLogProbMetric: 33.5755 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 959/1000
2023-10-31 20:44:03.556 
Epoch 959/1000 
	 loss: 32.5728, MinusLogProbMetric: 32.5728, val_loss: 33.0224, val_MinusLogProbMetric: 33.0224

Epoch 959: val_loss did not improve from 31.57458
196/196 - 62s - loss: 32.5728 - MinusLogProbMetric: 32.5728 - val_loss: 33.0224 - val_MinusLogProbMetric: 33.0224 - lr: 1.3717e-06 - 62s/epoch - 315ms/step
Epoch 960/1000
2023-10-31 20:45:04.190 
Epoch 960/1000 
	 loss: 32.4073, MinusLogProbMetric: 32.4073, val_loss: 33.0229, val_MinusLogProbMetric: 33.0229

Epoch 960: val_loss did not improve from 31.57458
196/196 - 61s - loss: 32.4073 - MinusLogProbMetric: 32.4073 - val_loss: 33.0229 - val_MinusLogProbMetric: 33.0229 - lr: 1.3717e-06 - 61s/epoch - 309ms/step
Epoch 961/1000
2023-10-31 20:46:01.725 
Epoch 961/1000 
	 loss: 32.2768, MinusLogProbMetric: 32.2768, val_loss: 32.9216, val_MinusLogProbMetric: 32.9216

Epoch 961: val_loss did not improve from 31.57458
196/196 - 58s - loss: 32.2768 - MinusLogProbMetric: 32.2768 - val_loss: 32.9216 - val_MinusLogProbMetric: 32.9216 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 962/1000
2023-10-31 20:46:58.346 
Epoch 962/1000 
	 loss: 32.2000, MinusLogProbMetric: 32.2000, val_loss: 32.7473, val_MinusLogProbMetric: 32.7473

Epoch 962: val_loss did not improve from 31.57458
196/196 - 57s - loss: 32.2000 - MinusLogProbMetric: 32.2000 - val_loss: 32.7473 - val_MinusLogProbMetric: 32.7473 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 963/1000
2023-10-31 20:47:56.395 
Epoch 963/1000 
	 loss: 32.0996, MinusLogProbMetric: 32.0996, val_loss: 32.6089, val_MinusLogProbMetric: 32.6089

Epoch 963: val_loss did not improve from 31.57458
196/196 - 58s - loss: 32.0996 - MinusLogProbMetric: 32.0996 - val_loss: 32.6089 - val_MinusLogProbMetric: 32.6089 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 964/1000
2023-10-31 20:49:00.333 
Epoch 964/1000 
	 loss: 32.0024, MinusLogProbMetric: 32.0024, val_loss: 32.4259, val_MinusLogProbMetric: 32.4259

Epoch 964: val_loss did not improve from 31.57458
196/196 - 64s - loss: 32.0024 - MinusLogProbMetric: 32.0024 - val_loss: 32.4259 - val_MinusLogProbMetric: 32.4259 - lr: 1.3717e-06 - 64s/epoch - 326ms/step
Epoch 965/1000
2023-10-31 20:50:01.615 
Epoch 965/1000 
	 loss: 31.7272, MinusLogProbMetric: 31.7272, val_loss: 32.4071, val_MinusLogProbMetric: 32.4071

Epoch 965: val_loss did not improve from 31.57458
196/196 - 61s - loss: 31.7272 - MinusLogProbMetric: 31.7272 - val_loss: 32.4071 - val_MinusLogProbMetric: 32.4071 - lr: 1.3717e-06 - 61s/epoch - 313ms/step
Epoch 966/1000
2023-10-31 20:51:03.480 
Epoch 966/1000 
	 loss: 31.5915, MinusLogProbMetric: 31.5915, val_loss: 32.1668, val_MinusLogProbMetric: 32.1668

Epoch 966: val_loss did not improve from 31.57458
196/196 - 62s - loss: 31.5915 - MinusLogProbMetric: 31.5915 - val_loss: 32.1668 - val_MinusLogProbMetric: 32.1668 - lr: 1.3717e-06 - 62s/epoch - 316ms/step
Epoch 967/1000
2023-10-31 20:52:07.043 
Epoch 967/1000 
	 loss: 31.5559, MinusLogProbMetric: 31.5559, val_loss: 32.1161, val_MinusLogProbMetric: 32.1161

Epoch 967: val_loss did not improve from 31.57458
196/196 - 64s - loss: 31.5559 - MinusLogProbMetric: 31.5559 - val_loss: 32.1161 - val_MinusLogProbMetric: 32.1161 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 968/1000
2023-10-31 20:53:13.923 
Epoch 968/1000 
	 loss: 31.5222, MinusLogProbMetric: 31.5222, val_loss: 32.0654, val_MinusLogProbMetric: 32.0654

Epoch 968: val_loss did not improve from 31.57458
196/196 - 67s - loss: 31.5222 - MinusLogProbMetric: 31.5222 - val_loss: 32.0654 - val_MinusLogProbMetric: 32.0654 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 969/1000
2023-10-31 20:54:21.554 
Epoch 969/1000 
	 loss: 31.5103, MinusLogProbMetric: 31.5103, val_loss: 32.1513, val_MinusLogProbMetric: 32.1513

Epoch 969: val_loss did not improve from 31.57458
196/196 - 68s - loss: 31.5103 - MinusLogProbMetric: 31.5103 - val_loss: 32.1513 - val_MinusLogProbMetric: 32.1513 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 970/1000
2023-10-31 20:55:26.799 
Epoch 970/1000 
	 loss: 31.4986, MinusLogProbMetric: 31.4986, val_loss: 32.0917, val_MinusLogProbMetric: 32.0917

Epoch 970: val_loss did not improve from 31.57458
196/196 - 65s - loss: 31.4986 - MinusLogProbMetric: 31.4986 - val_loss: 32.0917 - val_MinusLogProbMetric: 32.0917 - lr: 1.3717e-06 - 65s/epoch - 333ms/step
Epoch 971/1000
2023-10-31 20:56:28.860 
Epoch 971/1000 
	 loss: 31.4695, MinusLogProbMetric: 31.4695, val_loss: 32.1787, val_MinusLogProbMetric: 32.1787

Epoch 971: val_loss did not improve from 31.57458
196/196 - 62s - loss: 31.4695 - MinusLogProbMetric: 31.4695 - val_loss: 32.1787 - val_MinusLogProbMetric: 32.1787 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 972/1000
2023-10-31 20:57:30.968 
Epoch 972/1000 
	 loss: 31.4772, MinusLogProbMetric: 31.4772, val_loss: 32.0868, val_MinusLogProbMetric: 32.0868

Epoch 972: val_loss did not improve from 31.57458
196/196 - 62s - loss: 31.4772 - MinusLogProbMetric: 31.4772 - val_loss: 32.0868 - val_MinusLogProbMetric: 32.0868 - lr: 1.3717e-06 - 62s/epoch - 317ms/step
Epoch 973/1000
2023-10-31 20:58:35.696 
Epoch 973/1000 
	 loss: 31.4341, MinusLogProbMetric: 31.4341, val_loss: 32.1802, val_MinusLogProbMetric: 32.1802

Epoch 973: val_loss did not improve from 31.57458
196/196 - 65s - loss: 31.4341 - MinusLogProbMetric: 31.4341 - val_loss: 32.1802 - val_MinusLogProbMetric: 32.1802 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 974/1000
2023-10-31 20:59:40.379 
Epoch 974/1000 
	 loss: 31.4533, MinusLogProbMetric: 31.4533, val_loss: 32.0057, val_MinusLogProbMetric: 32.0057

Epoch 974: val_loss did not improve from 31.57458
196/196 - 65s - loss: 31.4533 - MinusLogProbMetric: 31.4533 - val_loss: 32.0057 - val_MinusLogProbMetric: 32.0057 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 975/1000
2023-10-31 21:00:40.844 
Epoch 975/1000 
	 loss: 31.4151, MinusLogProbMetric: 31.4151, val_loss: 31.9706, val_MinusLogProbMetric: 31.9706

Epoch 975: val_loss did not improve from 31.57458
196/196 - 60s - loss: 31.4151 - MinusLogProbMetric: 31.4151 - val_loss: 31.9706 - val_MinusLogProbMetric: 31.9706 - lr: 1.3717e-06 - 60s/epoch - 308ms/step
Epoch 976/1000
2023-10-31 21:01:39.879 
Epoch 976/1000 
	 loss: 31.4110, MinusLogProbMetric: 31.4110, val_loss: 31.9998, val_MinusLogProbMetric: 31.9998

Epoch 976: val_loss did not improve from 31.57458
196/196 - 59s - loss: 31.4110 - MinusLogProbMetric: 31.4110 - val_loss: 31.9998 - val_MinusLogProbMetric: 31.9998 - lr: 1.3717e-06 - 59s/epoch - 301ms/step
Epoch 977/1000
2023-10-31 21:02:38.717 
Epoch 977/1000 
	 loss: 31.4004, MinusLogProbMetric: 31.4004, val_loss: 31.9891, val_MinusLogProbMetric: 31.9891

Epoch 977: val_loss did not improve from 31.57458
196/196 - 59s - loss: 31.4004 - MinusLogProbMetric: 31.4004 - val_loss: 31.9891 - val_MinusLogProbMetric: 31.9891 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 978/1000
2023-10-31 21:03:45.479 
Epoch 978/1000 
	 loss: 31.3659, MinusLogProbMetric: 31.3659, val_loss: 32.0248, val_MinusLogProbMetric: 32.0248

Epoch 978: val_loss did not improve from 31.57458
196/196 - 67s - loss: 31.3659 - MinusLogProbMetric: 31.3659 - val_loss: 32.0248 - val_MinusLogProbMetric: 32.0248 - lr: 1.3717e-06 - 67s/epoch - 341ms/step
Epoch 979/1000
2023-10-31 21:04:53.385 
Epoch 979/1000 
	 loss: 31.3514, MinusLogProbMetric: 31.3514, val_loss: 32.0276, val_MinusLogProbMetric: 32.0276

Epoch 979: val_loss did not improve from 31.57458
196/196 - 68s - loss: 31.3514 - MinusLogProbMetric: 31.3514 - val_loss: 32.0276 - val_MinusLogProbMetric: 32.0276 - lr: 1.3717e-06 - 68s/epoch - 346ms/step
Epoch 980/1000
2023-10-31 21:06:00.950 
Epoch 980/1000 
	 loss: 31.3351, MinusLogProbMetric: 31.3351, val_loss: 31.9435, val_MinusLogProbMetric: 31.9435

Epoch 980: val_loss did not improve from 31.57458
196/196 - 68s - loss: 31.3351 - MinusLogProbMetric: 31.3351 - val_loss: 31.9435 - val_MinusLogProbMetric: 31.9435 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 981/1000
2023-10-31 21:07:07.121 
Epoch 981/1000 
	 loss: 31.3115, MinusLogProbMetric: 31.3115, val_loss: 31.9010, val_MinusLogProbMetric: 31.9010

Epoch 981: val_loss did not improve from 31.57458
196/196 - 66s - loss: 31.3115 - MinusLogProbMetric: 31.3115 - val_loss: 31.9010 - val_MinusLogProbMetric: 31.9010 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 982/1000
2023-10-31 21:08:10.236 
Epoch 982/1000 
	 loss: 31.2791, MinusLogProbMetric: 31.2791, val_loss: 31.9783, val_MinusLogProbMetric: 31.9783

Epoch 982: val_loss did not improve from 31.57458
196/196 - 63s - loss: 31.2791 - MinusLogProbMetric: 31.2791 - val_loss: 31.9783 - val_MinusLogProbMetric: 31.9783 - lr: 1.3717e-06 - 63s/epoch - 322ms/step
Epoch 983/1000
2023-10-31 21:09:12.609 
Epoch 983/1000 
	 loss: 31.1961, MinusLogProbMetric: 31.1961, val_loss: 31.7120, val_MinusLogProbMetric: 31.7120

Epoch 983: val_loss did not improve from 31.57458
196/196 - 62s - loss: 31.1961 - MinusLogProbMetric: 31.1961 - val_loss: 31.7120 - val_MinusLogProbMetric: 31.7120 - lr: 1.3717e-06 - 62s/epoch - 318ms/step
Epoch 984/1000
2023-10-31 21:10:17.357 
Epoch 984/1000 
	 loss: 31.0919, MinusLogProbMetric: 31.0919, val_loss: 31.7174, val_MinusLogProbMetric: 31.7174

Epoch 984: val_loss did not improve from 31.57458
196/196 - 65s - loss: 31.0919 - MinusLogProbMetric: 31.0919 - val_loss: 31.7174 - val_MinusLogProbMetric: 31.7174 - lr: 1.3717e-06 - 65s/epoch - 330ms/step
Epoch 985/1000
2023-10-31 21:11:21.676 
Epoch 985/1000 
	 loss: 31.0716, MinusLogProbMetric: 31.0716, val_loss: 31.6796, val_MinusLogProbMetric: 31.6796

Epoch 985: val_loss did not improve from 31.57458
196/196 - 64s - loss: 31.0716 - MinusLogProbMetric: 31.0716 - val_loss: 31.6796 - val_MinusLogProbMetric: 31.6796 - lr: 1.3717e-06 - 64s/epoch - 328ms/step
Epoch 986/1000
2023-10-31 21:12:24.815 
Epoch 986/1000 
	 loss: 30.9909, MinusLogProbMetric: 30.9909, val_loss: 31.6413, val_MinusLogProbMetric: 31.6413

Epoch 986: val_loss did not improve from 31.57458
196/196 - 63s - loss: 30.9909 - MinusLogProbMetric: 30.9909 - val_loss: 31.6413 - val_MinusLogProbMetric: 31.6413 - lr: 1.0000e-06 - 63s/epoch - 322ms/step
Epoch 987/1000
2023-10-31 21:13:26.036 
Epoch 987/1000 
	 loss: 30.9908, MinusLogProbMetric: 30.9908, val_loss: 31.6606, val_MinusLogProbMetric: 31.6606

Epoch 987: val_loss did not improve from 31.57458
196/196 - 61s - loss: 30.9908 - MinusLogProbMetric: 30.9908 - val_loss: 31.6606 - val_MinusLogProbMetric: 31.6606 - lr: 1.0000e-06 - 61s/epoch - 312ms/step
Epoch 988/1000
2023-10-31 21:14:29.520 
Epoch 988/1000 
	 loss: 30.9726, MinusLogProbMetric: 30.9726, val_loss: 31.7176, val_MinusLogProbMetric: 31.7176

Epoch 988: val_loss did not improve from 31.57458
196/196 - 63s - loss: 30.9726 - MinusLogProbMetric: 30.9726 - val_loss: 31.7176 - val_MinusLogProbMetric: 31.7176 - lr: 1.0000e-06 - 63s/epoch - 324ms/step
Epoch 989/1000
2023-10-31 21:15:31.580 
Epoch 989/1000 
	 loss: 30.9701, MinusLogProbMetric: 30.9701, val_loss: 31.6057, val_MinusLogProbMetric: 31.6057

Epoch 989: val_loss did not improve from 31.57458
196/196 - 62s - loss: 30.9701 - MinusLogProbMetric: 30.9701 - val_loss: 31.6057 - val_MinusLogProbMetric: 31.6057 - lr: 1.0000e-06 - 62s/epoch - 317ms/step
Epoch 990/1000
2023-10-31 21:16:35.272 
Epoch 990/1000 
	 loss: 30.9496, MinusLogProbMetric: 30.9496, val_loss: 31.5732, val_MinusLogProbMetric: 31.5732

Epoch 990: val_loss improved from 31.57458 to 31.57323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 30.9496 - MinusLogProbMetric: 30.9496 - val_loss: 31.5732 - val_MinusLogProbMetric: 31.5732 - lr: 1.0000e-06 - 65s/epoch - 331ms/step
Epoch 991/1000
2023-10-31 21:17:41.354 
Epoch 991/1000 
	 loss: 30.9432, MinusLogProbMetric: 30.9432, val_loss: 31.7217, val_MinusLogProbMetric: 31.7217

Epoch 991: val_loss did not improve from 31.57323
196/196 - 65s - loss: 30.9432 - MinusLogProbMetric: 30.9432 - val_loss: 31.7217 - val_MinusLogProbMetric: 31.7217 - lr: 1.0000e-06 - 65s/epoch - 331ms/step
Epoch 992/1000
2023-10-31 21:18:48.570 
Epoch 992/1000 
	 loss: 30.9585, MinusLogProbMetric: 30.9585, val_loss: 31.6229, val_MinusLogProbMetric: 31.6229

Epoch 992: val_loss did not improve from 31.57323
196/196 - 67s - loss: 30.9585 - MinusLogProbMetric: 30.9585 - val_loss: 31.6229 - val_MinusLogProbMetric: 31.6229 - lr: 1.0000e-06 - 67s/epoch - 343ms/step
Epoch 993/1000
2023-10-31 21:19:56.624 
Epoch 993/1000 
	 loss: 30.9341, MinusLogProbMetric: 30.9341, val_loss: 31.6047, val_MinusLogProbMetric: 31.6047

Epoch 993: val_loss did not improve from 31.57323
196/196 - 68s - loss: 30.9341 - MinusLogProbMetric: 30.9341 - val_loss: 31.6047 - val_MinusLogProbMetric: 31.6047 - lr: 1.0000e-06 - 68s/epoch - 347ms/step
Epoch 994/1000
2023-10-31 21:21:04.083 
Epoch 994/1000 
	 loss: 30.9322, MinusLogProbMetric: 30.9322, val_loss: 31.7057, val_MinusLogProbMetric: 31.7057

Epoch 994: val_loss did not improve from 31.57323
196/196 - 67s - loss: 30.9322 - MinusLogProbMetric: 30.9322 - val_loss: 31.7057 - val_MinusLogProbMetric: 31.7057 - lr: 1.0000e-06 - 67s/epoch - 344ms/step
Epoch 995/1000
2023-10-31 21:22:10.368 
Epoch 995/1000 
	 loss: 30.9286, MinusLogProbMetric: 30.9286, val_loss: 31.5898, val_MinusLogProbMetric: 31.5898

Epoch 995: val_loss did not improve from 31.57323
196/196 - 66s - loss: 30.9286 - MinusLogProbMetric: 30.9286 - val_loss: 31.5898 - val_MinusLogProbMetric: 31.5898 - lr: 1.0000e-06 - 66s/epoch - 338ms/step
Epoch 996/1000
2023-10-31 21:23:10.805 
Epoch 996/1000 
	 loss: 30.9123, MinusLogProbMetric: 30.9123, val_loss: 31.5947, val_MinusLogProbMetric: 31.5947

Epoch 996: val_loss did not improve from 31.57323
196/196 - 60s - loss: 30.9123 - MinusLogProbMetric: 30.9123 - val_loss: 31.5947 - val_MinusLogProbMetric: 31.5947 - lr: 1.0000e-06 - 60s/epoch - 308ms/step
Epoch 997/1000
2023-10-31 21:24:09.213 
Epoch 997/1000 
	 loss: 30.9203, MinusLogProbMetric: 30.9203, val_loss: 31.5854, val_MinusLogProbMetric: 31.5854

Epoch 997: val_loss did not improve from 31.57323
196/196 - 58s - loss: 30.9203 - MinusLogProbMetric: 30.9203 - val_loss: 31.5854 - val_MinusLogProbMetric: 31.5854 - lr: 1.0000e-06 - 58s/epoch - 298ms/step
Epoch 998/1000
2023-10-31 21:25:09.052 
Epoch 998/1000 
	 loss: 30.9166, MinusLogProbMetric: 30.9166, val_loss: 31.6033, val_MinusLogProbMetric: 31.6033

Epoch 998: val_loss did not improve from 31.57323
196/196 - 60s - loss: 30.9166 - MinusLogProbMetric: 30.9166 - val_loss: 31.6033 - val_MinusLogProbMetric: 31.6033 - lr: 1.0000e-06 - 60s/epoch - 305ms/step
Epoch 999/1000
2023-10-31 21:26:08.574 
Epoch 999/1000 
	 loss: 30.9212, MinusLogProbMetric: 30.9212, val_loss: 31.6542, val_MinusLogProbMetric: 31.6542

Epoch 999: val_loss did not improve from 31.57323
196/196 - 60s - loss: 30.9212 - MinusLogProbMetric: 30.9212 - val_loss: 31.6542 - val_MinusLogProbMetric: 31.6542 - lr: 1.0000e-06 - 60s/epoch - 304ms/step
Epoch 1000/1000
2023-10-31 21:27:08.116 
Epoch 1000/1000 
	 loss: 30.9070, MinusLogProbMetric: 30.9070, val_loss: 31.5654, val_MinusLogProbMetric: 31.5654

Epoch 1000: val_loss improved from 31.57323 to 31.56544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 30.9070 - MinusLogProbMetric: 30.9070 - val_loss: 31.5654 - val_MinusLogProbMetric: 31.5654 - lr: 1.0000e-06 - 61s/epoch - 309ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 869.
Model trained in 59092.86 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.86 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 2.42 s.
===========
Run 382/720 done in 109283.23 s.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_414/ already exists.
Skipping it.
===========
Run 414/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

===========
Generating train data for run 417.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_390"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_391 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f4654a6dc60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4e2b6f6920>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4e2b6f6920>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4680a429b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f489c5d4310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f489c5d47c0>, <keras.callbacks.ModelCheckpoint object at 0x7f489c5d64a0>, <keras.callbacks.EarlyStopping object at 0x7f489c5d7d00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f489c5d4490>, <keras.callbacks.TerminateOnNaN object at 0x7f489c5d67d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:27:16.779570
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:28:13.601 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 57s/epoch - 289ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 417.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_396"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_397 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f46c4c000d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46e28b7b80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46e28b7b80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f469cb8ead0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f462680db10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46268c04f0>, <keras.callbacks.ModelCheckpoint object at 0x7f46268c05b0>, <keras.callbacks.EarlyStopping object at 0x7f46268c0820>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46268c0850>, <keras.callbacks.TerminateOnNaN object at 0x7f46268c0490>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:28:18.338962
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:29:10.230 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 52s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 52s/epoch - 264ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 417.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_402"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_403 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f4de01ef640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45c0118430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45c0118430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4ddfb0f790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4ddfbabc70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4ddfa00220>, <keras.callbacks.ModelCheckpoint object at 0x7f4ddfa002e0>, <keras.callbacks.EarlyStopping object at 0x7f4ddfa00550>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4ddfa00580>, <keras.callbacks.TerminateOnNaN object at 0x7f4ddfa001c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:29:12.942919
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:30:10.771 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 58s/epoch - 295ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 417.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_408"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_409 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f4785db84f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f459d872bf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f459d872bf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4990227ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f49982a91e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f49982a9750>, <keras.callbacks.ModelCheckpoint object at 0x7f49982a9810>, <keras.callbacks.EarlyStopping object at 0x7f49982a9a80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f49982a9ab0>, <keras.callbacks.TerminateOnNaN object at 0x7f49982a96f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:30:15.352800
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:31:05.039 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 50s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 50s/epoch - 253ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 417.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_414"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_415 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f482019f3d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46193c9a50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46193c9a50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4619525030>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4619793bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4619700160>, <keras.callbacks.ModelCheckpoint object at 0x7f4619700220>, <keras.callbacks.EarlyStopping object at 0x7f4619700490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46197004c0>, <keras.callbacks.TerminateOnNaN object at 0x7f4619700100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:31:09.169058
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:31:58.608 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 49s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 49s/epoch - 252ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 417.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_420"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_421 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f4ddf11b820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4ddf2286d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4ddf2286d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f453c771720>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4ddf176830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4ddf176da0>, <keras.callbacks.ModelCheckpoint object at 0x7f4ddf176e60>, <keras.callbacks.EarlyStopping object at 0x7f4ddf1770d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4ddf177100>, <keras.callbacks.TerminateOnNaN object at 0x7f4ddf176d40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:32:01.276605
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:32:58.673 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 57s/epoch - 293ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 417.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_426"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_427 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f455546a3b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4591436920>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4591436920>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f475ab69c90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46826a7910>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46826a7e80>, <keras.callbacks.ModelCheckpoint object at 0x7f46826a7f40>, <keras.callbacks.EarlyStopping object at 0x7f46826a7e50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46826a7e20>, <keras.callbacks.TerminateOnNaN object at 0x7f46826dc1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:33:02.953312
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:33:54.187 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 51s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 51s/epoch - 261ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 417.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_432"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_433 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f45d542bd60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4dddef7b50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4dddef7b50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45c06fb9a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45d5433550>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45d5433ac0>, <keras.callbacks.ModelCheckpoint object at 0x7f45d5433b80>, <keras.callbacks.EarlyStopping object at 0x7f45d5433df0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45d5433e20>, <keras.callbacks.TerminateOnNaN object at 0x7f45d5433a60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:33:56.835832
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:34:46.911 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 50s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 50s/epoch - 255ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 417.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_438"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_439 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f4dd59ef4c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45a94552a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45a94552a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45a94c7b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4dd5a53100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4dd5a53670>, <keras.callbacks.ModelCheckpoint object at 0x7f4dd5a53730>, <keras.callbacks.EarlyStopping object at 0x7f4dd5a539a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4dd5a539d0>, <keras.callbacks.TerminateOnNaN object at 0x7f4dd5a53610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:34:49.616818
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:35:46.974 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 57s/epoch - 293ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 417.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_444"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_445 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f45a90c3fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46196ac4c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46196ac4c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f469d9809a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4682302a10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4682302f80>, <keras.callbacks.ModelCheckpoint object at 0x7f4682303040>, <keras.callbacks.EarlyStopping object at 0x7f46823032b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46823032e0>, <keras.callbacks.TerminateOnNaN object at 0x7f4682302f20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:35:51.546825
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:36:42.789 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 51s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 51s/epoch - 261ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 417.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_450"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_451 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f4de8df5930>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45a8711720>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45a8711720>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4dde5ede70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45d503f9d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45d503ff40>, <keras.callbacks.ModelCheckpoint object at 0x7f45d503fee0>, <keras.callbacks.EarlyStopping object at 0x7f45d503ff10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45d50580d0>, <keras.callbacks.TerminateOnNaN object at 0x7f45d50582b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-31 21:36:47.042791
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:37:36.892 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10439.1074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 50s - loss: nan - MinusLogProbMetric: 10439.1074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 50s/epoch - 254ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 417/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_418/ already exists.
Skipping it.
===========
Run 418/720 already exists. Skipping it.
===========

===========
Generating train data for run 419.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_456"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_457 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f4dad069900>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45d5e13430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45d5e13430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f454d4486a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4dacb8ae60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4dacb8b3d0>, <keras.callbacks.ModelCheckpoint object at 0x7f4dacb8b490>, <keras.callbacks.EarlyStopping object at 0x7f4dacb8b700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4dacb8b730>, <keras.callbacks.TerminateOnNaN object at 0x7f4dacb8b370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:37:40.205793
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:38:51.195 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 71s/epoch - 362ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 419.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_462"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_463 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f478d055ae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4555e72380>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4555e72380>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46c79c75b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4624f4e860>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4624f4dd80>, <keras.callbacks.ModelCheckpoint object at 0x7f4624f4dc00>, <keras.callbacks.EarlyStopping object at 0x7f4624f4d720>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4624f4d6c0>, <keras.callbacks.TerminateOnNaN object at 0x7f4624f4de40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:38:56.445107
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:40:00.173 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 64s/epoch - 325ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 419.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_468"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_469 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f459d66e290>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f469d7f5e10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f469d7f5e10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4751f86d10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4591320f10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4591321480>, <keras.callbacks.ModelCheckpoint object at 0x7f4591321540>, <keras.callbacks.EarlyStopping object at 0x7f45913217b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45913217e0>, <keras.callbacks.TerminateOnNaN object at 0x7f4591321420>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:40:05.380702
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:41:20.834 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 75s/epoch - 385ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 419.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_474"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_475 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f45d4af3f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f469cb63310>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f469cb63310>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45cc954e50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4e34182560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4e34181960>, <keras.callbacks.ModelCheckpoint object at 0x7f4e34182170>, <keras.callbacks.EarlyStopping object at 0x7f4e341817b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4e34183040>, <keras.callbacks.TerminateOnNaN object at 0x7f4e34182020>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:41:24.393294
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:42:28.267 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 64s/epoch - 326ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 419.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_480"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_481 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f45cd15b0a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45c06b29b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45c06b29b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45d4c3fd60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45d55b2b30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45d55b30a0>, <keras.callbacks.ModelCheckpoint object at 0x7f45d55b3160>, <keras.callbacks.EarlyStopping object at 0x7f45d55b33d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45d55b3400>, <keras.callbacks.TerminateOnNaN object at 0x7f45d55b3040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:42:31.641342
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:43:34.306 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 63s/epoch - 320ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 419.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_486"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_487 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f4dac84f490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4dac117550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4dac117550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43d7b99ff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4dabe50a60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4dabe50fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f4dabe51090>, <keras.callbacks.EarlyStopping object at 0x7f4dabe51300>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4dabe51330>, <keras.callbacks.TerminateOnNaN object at 0x7f4dabe50f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:43:37.615767
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:44:47.489 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 70s/epoch - 356ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 419.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_492"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_493 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f4a244f6770>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f469cf32da0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f469cf32da0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46268c2350>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46e19e6440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46e19e4c10>, <keras.callbacks.ModelCheckpoint object at 0x7f46e19e7010>, <keras.callbacks.EarlyStopping object at 0x7f46e19e41c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46e19e6020>, <keras.callbacks.TerminateOnNaN object at 0x7f46e19e6cb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:44:50.877582
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:45:52.904 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 62s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 62s/epoch - 316ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 419.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_498"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_499 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f46c74899f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f459028b190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f459028b190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f44f4b50370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4de8ab3100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4de8ab3670>, <keras.callbacks.ModelCheckpoint object at 0x7f4de8ab3730>, <keras.callbacks.EarlyStopping object at 0x7f4de8ab39a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4de8ab39d0>, <keras.callbacks.TerminateOnNaN object at 0x7f4de8ab3610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:45:58.076288
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:47:00.777 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 63s/epoch - 320ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 419.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_504"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_505 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f44a507f8b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4944bc31f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4944bc31f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4944bc12a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f49d841ef20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f49d841d5d0>, <keras.callbacks.ModelCheckpoint object at 0x7f49d841f640>, <keras.callbacks.EarlyStopping object at 0x7f49d841c0d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f49d841d870>, <keras.callbacks.TerminateOnNaN object at 0x7f49d841d3f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:47:11.374509
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:48:17.845 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 66s/epoch - 339ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 419.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_510"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_511 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f453cf2ba30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46fd179ff0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46fd179ff0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46822c12a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f44f4bb7fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f44f4bf4550>, <keras.callbacks.ModelCheckpoint object at 0x7f44f4bf4610>, <keras.callbacks.EarlyStopping object at 0x7f44f4bf4880>, <keras.callbacks.ReduceLROnPlateau object at 0x7f44f4bf48b0>, <keras.callbacks.TerminateOnNaN object at 0x7f44f4bf44f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:48:23.148158
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:49:25.750 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 63s/epoch - 319ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 419.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_419/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_419
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_516"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_517 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f4d7a657490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4d7a164580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4d7a164580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43d46aaa40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4d7a015f60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_419/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4d7a0164d0>, <keras.callbacks.ModelCheckpoint object at 0x7f4d7a016590>, <keras.callbacks.EarlyStopping object at 0x7f4d7a016800>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4d7a016830>, <keras.callbacks.TerminateOnNaN object at 0x7f4d7a016470>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_419/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 419/720 with hyperparameters:
timestamp = 2023-10-31 21:49:30.916794
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:50:45.563 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10970.7861, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 10970.7861 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 75s/epoch - 381ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 419/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_420/ already exists.
Skipping it.
===========
Run 420/720 already exists. Skipping it.
===========

===========
Generating train data for run 421.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_527"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_528 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7f45d5761b10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45a902b250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45a902b250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43ac470f70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4de01ed5a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46e10078b0>, <keras.callbacks.ModelCheckpoint object at 0x7f46e1006c20>, <keras.callbacks.EarlyStopping object at 0x7f46e1007370>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46e1005300>, <keras.callbacks.TerminateOnNaN object at 0x7f46e1007ac0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 21:50:52.448702
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:52:36.447 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 104s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 104s/epoch - 530ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 421.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_538"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_539 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7f4dab379ed0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f49b831d210>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f49b831d210>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46811d13c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f44a5bb0be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f44a5bb1150>, <keras.callbacks.ModelCheckpoint object at 0x7f44a5bb1210>, <keras.callbacks.EarlyStopping object at 0x7f44a5bb1480>, <keras.callbacks.ReduceLROnPlateau object at 0x7f44a5bb14b0>, <keras.callbacks.TerminateOnNaN object at 0x7f44a5bb10f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 21:52:42.510195
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:54:35.162 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 113s/epoch - 575ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 421.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_549"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_550 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7f4de02f3160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4682b29c00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4682b29c00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4683ccf2e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45d5005ea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45d5004550>, <keras.callbacks.ModelCheckpoint object at 0x7f45d5006020>, <keras.callbacks.EarlyStopping object at 0x7f45d5004b20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45d5004910>, <keras.callbacks.TerminateOnNaN object at 0x7f45d5005f00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 21:54:43.588019
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:56:29.695 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 106s/epoch - 540ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 421.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7f4656a4be80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46563a6530>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46563a6530>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45a94be2c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f44a492b6d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f44a492bc40>, <keras.callbacks.ModelCheckpoint object at 0x7f44a492bd00>, <keras.callbacks.EarlyStopping object at 0x7f44a492bf70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f44a492bfa0>, <keras.callbacks.TerminateOnNaN object at 0x7f44a492bbe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 21:56:37.741486
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 21:58:30.863 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 113s/epoch - 577ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 421.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_571"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_572 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_66 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_66/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_66'")
self.model: <keras.engine.functional.Functional object at 0x7f4656927dc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f475b15bc10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f475b15bc10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f48fc1ced40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f44a5e47df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f44a5e46e00>, <keras.callbacks.ModelCheckpoint object at 0x7f44a5e46350>, <keras.callbacks.EarlyStopping object at 0x7f44a5e463b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f44a5e45ab0>, <keras.callbacks.TerminateOnNaN object at 0x7f44a5e46d40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 21:58:37.721522
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:00:23.310 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 106s/epoch - 539ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 421.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_582"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_583 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_67 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_67/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_67'")
self.model: <keras.engine.functional.Functional object at 0x7f4daafdf8b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f469d3497e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f469d3497e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43bc289720>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f43bc259ae0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f43bc25a050>, <keras.callbacks.ModelCheckpoint object at 0x7f43bc25a110>, <keras.callbacks.EarlyStopping object at 0x7f43bc25a380>, <keras.callbacks.ReduceLROnPlateau object at 0x7f43bc25a3b0>, <keras.callbacks.TerminateOnNaN object at 0x7f43bc259ff0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 22:00:30.622392
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:02:26.304 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 116s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 116s/epoch - 590ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 421.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_593"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_594 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_68 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_68/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_68'")
self.model: <keras.engine.functional.Functional object at 0x7f4682dbe590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f461cca0340>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f461cca0340>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f44fcb094b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4682d2ccd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4682d2d240>, <keras.callbacks.ModelCheckpoint object at 0x7f4682d2d300>, <keras.callbacks.EarlyStopping object at 0x7f4682d2d570>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4682d2d5a0>, <keras.callbacks.TerminateOnNaN object at 0x7f4682d2d1e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 22:02:33.397359
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:04:17.816 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 104s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 104s/epoch - 533ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 421.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_604"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_605 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_69 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_69/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_69'")
self.model: <keras.engine.functional.Functional object at 0x7f45917c7ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f475afd6470>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f475afd6470>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46c5486440>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f44c532d2d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f44c532c580>, <keras.callbacks.ModelCheckpoint object at 0x7f44c532c040>, <keras.callbacks.EarlyStopping object at 0x7f44c532c640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f44c532f6d0>, <keras.callbacks.TerminateOnNaN object at 0x7f44c53480a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 22:04:24.809751
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:06:22.227 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 117s/epoch - 599ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 421.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_615"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_616 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_70 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_70/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_70'")
self.model: <keras.engine.functional.Functional object at 0x7f46e2a96a40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45911abaf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45911abaf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4654105900>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f43d4bd02e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f43d4bd0610>, <keras.callbacks.ModelCheckpoint object at 0x7f43d4bd11e0>, <keras.callbacks.EarlyStopping object at 0x7f43d4bd2110>, <keras.callbacks.ReduceLROnPlateau object at 0x7f43d4bd0dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f43d4bd0e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 22:06:29.538125
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:08:13.050 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 103s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 103s/epoch - 528ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 421.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_626"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_627 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_71 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_71/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_71'")
self.model: <keras.engine.functional.Functional object at 0x7f4daad2fee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f43d61de6e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f43d61de6e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4dab8b2c20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4daad7f6d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4daad7fc40>, <keras.callbacks.ModelCheckpoint object at 0x7f4daad7fd00>, <keras.callbacks.EarlyStopping object at 0x7f4daad7ff70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4daad7ffa0>, <keras.callbacks.TerminateOnNaN object at 0x7f4daad7fbe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 22:08:20.019997
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:10:14.860 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 115s/epoch - 586ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 421.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_637"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_638 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_72 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_72/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_72'")
self.model: <keras.engine.functional.Functional object at 0x7f44c4c46980>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f459ca39390>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f459ca39390>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f44a4580b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f465c2a5390>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f465c2a5900>, <keras.callbacks.ModelCheckpoint object at 0x7f465c2a59c0>, <keras.callbacks.EarlyStopping object at 0x7f465c2a5c30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f465c2a5c60>, <keras.callbacks.TerminateOnNaN object at 0x7f465c2a58a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-31 22:10:22.299526
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:12:07.012 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 104s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 104s/epoch - 533ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 421/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 422.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_648"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_649 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_73 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_73/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_73'")
self.model: <keras.engine.functional.Functional object at 0x7f45b1731cf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f465c2f4f70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f465c2f4f70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45cc3138b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f459d734fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f459d735510>, <keras.callbacks.ModelCheckpoint object at 0x7f459d7355d0>, <keras.callbacks.EarlyStopping object at 0x7f459d735840>, <keras.callbacks.ReduceLROnPlateau object at 0x7f459d735870>, <keras.callbacks.TerminateOnNaN object at 0x7f459d7354b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-31 22:12:13.703271
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:14:22.516 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11090.8467, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 11090.8467 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 129s/epoch - 657ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 422.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_659"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_660 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_74 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_74/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_74'")
self.model: <keras.engine.functional.Functional object at 0x7f44a5818a60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f47b1735b70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f47b1735b70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46e337feb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f465c413d30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f465c413c10>, <keras.callbacks.ModelCheckpoint object at 0x7f465c4135b0>, <keras.callbacks.EarlyStopping object at 0x7f465c4131c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f465c410100>, <keras.callbacks.TerminateOnNaN object at 0x7f465c413940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-31 22:14:30.076600
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 29: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:16:25.345 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9609.1523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 9609.1523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 115s/epoch - 588ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 422.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_670"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_671 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_75 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_75/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_75'")
self.model: <keras.engine.functional.Functional object at 0x7f439f6537c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f465c413d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f465c413d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43bdf11c90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f43adb95360>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f43adb958d0>, <keras.callbacks.ModelCheckpoint object at 0x7f43adb95990>, <keras.callbacks.EarlyStopping object at 0x7f43adb95c00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f43adb95c30>, <keras.callbacks.TerminateOnNaN object at 0x7f43adb95870>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-31 22:16:32.205853
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 14: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:18:42.286 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11056.8428, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 11056.8428 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 130s/epoch - 664ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 422.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_681"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_682 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_76 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_76/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_76'")
self.model: <keras.engine.functional.Functional object at 0x7f453c889240>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f43d7b80730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f43d7b80730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43ac5801f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46e22d2b90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46e22d2950>, <keras.callbacks.ModelCheckpoint object at 0x7f46e22d1720>, <keras.callbacks.EarlyStopping object at 0x7f46e22d3cd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46e22d0df0>, <keras.callbacks.TerminateOnNaN object at 0x7f46e22d3010>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-31 22:18:50.784552
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 11: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 22:20:44.256 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11332.3291, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 11332.3291 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 113s/epoch - 578ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 422.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_692"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_693 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_77 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_77/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_77'")
self.model: <keras.engine.functional.Functional object at 0x7f45a8ffbe50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f44c574a8f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f44c574a8f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45cd8bdf60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f439f021de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f439f022350>, <keras.callbacks.ModelCheckpoint object at 0x7f439f022410>, <keras.callbacks.EarlyStopping object at 0x7f439f022680>, <keras.callbacks.ReduceLROnPlateau object at 0x7f439f0226b0>, <keras.callbacks.TerminateOnNaN object at 0x7f439f0222f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-31 22:20:51.241839
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
