2023-10-30 13:57:40.392964: Importing os...
2023-10-30 13:57:40.393036: Importing sys...
2023-10-30 13:57:40.393052: Importing and initializing argparse...
Visible devices: [0]
2023-10-30 13:57:40.410287: Importing timer from timeit...
2023-10-30 13:57:40.410952: Setting env variables for tf import (only device [0] will be available)...
2023-10-30 13:57:40.411001: Importing numpy...
2023-10-30 13:57:40.591841: Importing pandas...
2023-10-30 13:57:40.776750: Importing shutil...
2023-10-30 13:57:40.776774: Importing subprocess...
2023-10-30 13:57:40.776781: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-30 13:57:43.103685: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-30 13:57:43.510555: Importing textwrap...
2023-10-30 13:57:43.510592: Importing timeit...
2023-10-30 13:57:43.510602: Importing traceback...
2023-10-30 13:57:43.510608: Importing typing...
2023-10-30 13:57:43.510619: Setting tf configs...
2023-10-30 13:57:43.950538: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-30 13:57:45.391346: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

===========
Generating train data for run 269.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  826720    
 r)                                                              
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f80e0572c20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f808063e170>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f808063e170>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f80e05576a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8080543dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8080470370>, <keras.callbacks.ModelCheckpoint object at 0x7f80804704c0>, <keras.callbacks.EarlyStopping object at 0x7f80804706d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8080470700>, <keras.callbacks.TerminateOnNaN object at 0x7f8080470430>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 13:57:53.787033
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 13:59:49.435 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 116s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 116s/epoch - 589ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 269.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f8486ff8e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f848674ab60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f848674ab60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f80803eee00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f848659e530>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f848659eaa0>, <keras.callbacks.ModelCheckpoint object at 0x7f848659eb60>, <keras.callbacks.EarlyStopping object at 0x7f848659edd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f848659ee00>, <keras.callbacks.TerminateOnNaN object at 0x7f848659ea40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 13:59:55.593152
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:01:49.012 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 113s/epoch - 578ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f7f24209210>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f80402233a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f80402233a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f24342680>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8486150a90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8486151000>, <keras.callbacks.ModelCheckpoint object at 0x7f84861510c0>, <keras.callbacks.EarlyStopping object at 0x7f8486151330>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8486151360>, <keras.callbacks.TerminateOnNaN object at 0x7f8486150fa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:01:55.978453
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:03:47.578 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 111s/epoch - 568ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f7f1c142380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7de4913e50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7de4913e50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f80801d5540>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8454d9df00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8454d9e470>, <keras.callbacks.ModelCheckpoint object at 0x7f8454d9e530>, <keras.callbacks.EarlyStopping object at 0x7f8454d9e7a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8454d9e7d0>, <keras.callbacks.TerminateOnNaN object at 0x7f8454d9e410>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:03:53.981910
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:05:45.106 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 111s/epoch - 567ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f7de4e7ec80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8485fc7ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8485fc7ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7eb46762c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8486021d50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f84860222c0>, <keras.callbacks.ModelCheckpoint object at 0x7f8486022380>, <keras.callbacks.EarlyStopping object at 0x7f84860225f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8486022620>, <keras.callbacks.TerminateOnNaN object at 0x7f8486022260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:05:51.223049
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f80402d7ac0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:07:49.363 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 118s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 118s/epoch - 602ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f7f1c2cf730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f145e2ec0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f145e2ec0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7dd06c7ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7fe011c250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7fe011c7c0>, <keras.callbacks.ModelCheckpoint object at 0x7f7fe011c880>, <keras.callbacks.EarlyStopping object at 0x7f7fe011caf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7fe011cb20>, <keras.callbacks.TerminateOnNaN object at 0x7f7fe011c760>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:07:55.631287
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f7f24786cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:09:51.579 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 116s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 116s/epoch - 591ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 269.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f8452cd0520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7dbc8d5a80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7dbc8d5a80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7db5fe8d90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7db7869360>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7db78698d0>, <keras.callbacks.ModelCheckpoint object at 0x7f7db7869990>, <keras.callbacks.EarlyStopping object at 0x7f7db7869c00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7db7869c30>, <keras.callbacks.TerminateOnNaN object at 0x7f7db7869870>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:09:58.889162
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:11:55.787 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 117s/epoch - 595ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f7fa07f4fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7e846a7d30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7e846a7d30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7e9477ea40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7e947725f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7e94772b60>, <keras.callbacks.ModelCheckpoint object at 0x7f7e94772c20>, <keras.callbacks.EarlyStopping object at 0x7f7e94772e90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7e94772ec0>, <keras.callbacks.TerminateOnNaN object at 0x7f7e94772b00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:12:02.749606
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:13:57.103 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 114s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 114s/epoch - 583ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 269.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f7dbc2eb9a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7db476f730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7db476f730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7de4806620>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d8fcbe650>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d8fcbebc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d8fcbec80>, <keras.callbacks.EarlyStopping object at 0x7f7d8fcbeef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d8fcbef20>, <keras.callbacks.TerminateOnNaN object at 0x7f7d8fcbeb60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:14:04.217437
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:16:03.797 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 119s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 119s/epoch - 609ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f84862efdc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7e1412beb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7e1412beb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f147b3f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7dd051f5b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7dd051fb20>, <keras.callbacks.ModelCheckpoint object at 0x7f7dd051fbe0>, <keras.callbacks.EarlyStopping object at 0x7f7dd051fe50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7dd051fe80>, <keras.callbacks.TerminateOnNaN object at 0x7f7dd051fac0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:16:10.807962
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:18:07.128 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 116s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 116s/epoch - 593ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f7db57f7610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f044b6bf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f044b6bf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7db462d6f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d8ef11930>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d8ef11ea0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d8ef11f60>, <keras.callbacks.EarlyStopping object at 0x7f7d8ef121d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d8ef12200>, <keras.callbacks.TerminateOnNaN object at 0x7f7d8ef11e40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-30 14:18:13.248997
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:20:08.471 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 115s/epoch - 587ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 269/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

===========
Generating train data for run 349.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_131"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_132 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f7eb42f36d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f04522f80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f04522f80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7fa015a530>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7fe05e8a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7fe05e8fa0>, <keras.callbacks.ModelCheckpoint object at 0x7f7fe05e9060>, <keras.callbacks.EarlyStopping object at 0x7f7fe05e92d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7fe05e9300>, <keras.callbacks.TerminateOnNaN object at 0x7f7fe05e8f40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:20:15.626825
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:22:03.623 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 108s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 108s/epoch - 551ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 349.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_142"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_143 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f7d22fcd270>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d23949fc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d23949fc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f84545d5480>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f84541d3ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f84541d3fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f845420c130>, <keras.callbacks.EarlyStopping object at 0x7f845420c3a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f845420c3d0>, <keras.callbacks.TerminateOnNaN object at 0x7f845420c040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:22:11.965570
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:24:07.200 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 115s/epoch - 587ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 349.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_153"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_154 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f80104d7e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d23ed1090>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d23ed1090>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d21b95ff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d235b0e50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d235b13c0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d235b1480>, <keras.callbacks.EarlyStopping object at 0x7f7d235b16f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d235b1720>, <keras.callbacks.TerminateOnNaN object at 0x7f7d235b1360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:24:13.711743
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:26:10.259 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 116s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 116s/epoch - 594ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 349.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_164"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_165 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f7d23423460>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7cfa6a7c10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7cfa6a7c10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d599c7fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f84536d0e50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f84536d13c0>, <keras.callbacks.ModelCheckpoint object at 0x7f84536d1480>, <keras.callbacks.EarlyStopping object at 0x7f84536d16f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f84536d1720>, <keras.callbacks.TerminateOnNaN object at 0x7f84536d1360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:26:16.667839
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:28:13.763 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 117s/epoch - 597ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 349.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_175"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_176 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f7dbcb67160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d21733e80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d21733e80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d20fdbfa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d59c2f2b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d59c2f820>, <keras.callbacks.ModelCheckpoint object at 0x7f7d59c2f8e0>, <keras.callbacks.EarlyStopping object at 0x7f7d59c2fb50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d59c2fb80>, <keras.callbacks.TerminateOnNaN object at 0x7f7d59c2f7c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:28:20.276640
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:30:13.745 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 113s/epoch - 579ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 349.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f7d20c87f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d71948910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d71948910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d1b3d87c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d1ae9ded0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d1ae9e440>, <keras.callbacks.ModelCheckpoint object at 0x7f7d1ae9e500>, <keras.callbacks.EarlyStopping object at 0x7f7d1ae9e770>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d1ae9e7a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d1ae9e3e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:30:20.188715
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:32:11.571 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 111s/epoch - 568ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 349.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f7d5ab5fee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d207d0c10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d207d0c10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d220d6050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d202c8220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d202c8790>, <keras.callbacks.ModelCheckpoint object at 0x7f7d202c8850>, <keras.callbacks.EarlyStopping object at 0x7f7d202c8ac0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d202c8af0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d202c8730>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:32:18.191412
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:34:13.522 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 115s/epoch - 588ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 349.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f7d02a7a260>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d58698280>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d58698280>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d029154e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cf9d39270>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cf9d397e0>, <keras.callbacks.ModelCheckpoint object at 0x7f7cf9d398a0>, <keras.callbacks.EarlyStopping object at 0x7f7cf9d39b10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cf9d39b40>, <keras.callbacks.TerminateOnNaN object at 0x7f7cf9d39780>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:34:20.041637
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:36:15.065 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 115s/epoch - 587ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 349.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f7d206a7df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d1acc9930>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d1acc9930>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d1ac52dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d206335e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d20633b50>, <keras.callbacks.ModelCheckpoint object at 0x7f7d20633c10>, <keras.callbacks.EarlyStopping object at 0x7f7d20633e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d20633eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d20633af0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:36:21.164622
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:38:17.771 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 117s/epoch - 594ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 349.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f7d23e37d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d59a4d4e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d59a4d4e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f845420d060>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d014474f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d01447a60>, <keras.callbacks.ModelCheckpoint object at 0x7f7d01447b20>, <keras.callbacks.EarlyStopping object at 0x7f7d01447d90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d01447dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d01447a00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:38:24.630903
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:40:23.985 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 119s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 119s/epoch - 608ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 349.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f7d59613490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d59284d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d59284d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d23999b10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d58872e00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d58871930>, <keras.callbacks.ModelCheckpoint object at 0x7f7d58870940>, <keras.callbacks.EarlyStopping object at 0x7f7d58872590>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d58873d90>, <keras.callbacks.TerminateOnNaN object at 0x7f7d58872b90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-30 14:40:30.795588
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:42:20.373 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 109s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 109s/epoch - 559ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 349/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

===========
Generating train data for run 373.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_252"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_253 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f7d5ae6f040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d58c32fe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d58c32fe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d59b57e80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d586522c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f84540386d0>, <keras.callbacks.ModelCheckpoint object at 0x7f8454039360>, <keras.callbacks.EarlyStopping object at 0x7f8454038a30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f845403b3a0>, <keras.callbacks.TerminateOnNaN object at 0x7f8454038610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:42:33.777996
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:44:35.299 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 121s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 121s/epoch - 619ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 373.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_263"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_264 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f8453568fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f80802e90f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f80802e90f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d03b80fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cfbfea4d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cfbfeaa40>, <keras.callbacks.ModelCheckpoint object at 0x7f7cfbfeab00>, <keras.callbacks.EarlyStopping object at 0x7f7cfbfead70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cfbfeada0>, <keras.callbacks.TerminateOnNaN object at 0x7f7cfbfea9e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:44:42.614844
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:46:47.002 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 124s/epoch - 634ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 373.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_274"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_275 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f7d02e79ed0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7de4b5d5a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7de4b5d5a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d713631c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d71399900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d71399e70>, <keras.callbacks.ModelCheckpoint object at 0x7f7d71399f30>, <keras.callbacks.EarlyStopping object at 0x7f7d7139a1a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d7139a1d0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d71399e10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:46:53.268902
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:49:00.036 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 127s/epoch - 646ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 3.703703703703703e-05.
===========
Generating train data for run 373.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_285"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_286 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f7d70836590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d01e55b70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d01e55b70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f2c577040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d1b63d780>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d1b63dcf0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d1b63ddb0>, <keras.callbacks.EarlyStopping object at 0x7f7d1b63e020>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d1b63e050>, <keras.callbacks.TerminateOnNaN object at 0x7f7d1b63dc90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:49:07.963447
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:51:12.480 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 124s/epoch - 634ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.2345679012345677e-05.
===========
Generating train data for run 373.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_296"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_297 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f7dbc727e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7db53b0a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7db53b0a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7dbc471ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d0267ab30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d0267b0a0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d0267b160>, <keras.callbacks.EarlyStopping object at 0x7f7d0267b3d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d0267b400>, <keras.callbacks.TerminateOnNaN object at 0x7f7d0267b040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:51:20.018183
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:53:26.614 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 126s/epoch - 645ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.115226337448558e-06.
===========
Generating train data for run 373.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f7d038b1f90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7dd0747fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7dd0747fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7db5d45420>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d005dfa00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d005dff70>, <keras.callbacks.ModelCheckpoint object at 0x7f7d005dff10>, <keras.callbacks.EarlyStopping object at 0x7f7d005dff40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d00544100>, <keras.callbacks.TerminateOnNaN object at 0x7f7d005442e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:53:32.956659
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:55:36.656 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 124s/epoch - 630ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.3717421124828526e-06.
===========
Generating train data for run 373.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_318"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_319 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f7fa01608b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f6018dcf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f6018dcf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f481ceec0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7db78f9510>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7db78f99c0>, <keras.callbacks.ModelCheckpoint object at 0x7f7db78fa9e0>, <keras.callbacks.EarlyStopping object at 0x7f7db78fadd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7db78fada0>, <keras.callbacks.TerminateOnNaN object at 0x7f7db78f9ff0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:55:43.279212
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:57:49.975 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 127s/epoch - 646ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.572473708276175e-07.
===========
Generating train data for run 373.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_329"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_330 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f7fc0268640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8486350b80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8486350b80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d5bd814b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7dbc9a9690>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7dbc9aabc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7dbc9abd90>, <keras.callbacks.EarlyStopping object at 0x7f7dbc9ab520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7dbc9a8eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f7dbc9a9510>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 14:57:57.912055
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:00:09.537 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 131s/epoch - 670ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.524157902758725e-07.
===========
Generating train data for run 373.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_340"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_341 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f7d8efdfd30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f4837aa70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f4837aa70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7dd07a79a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d22fcd2a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d22fce6b0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d22fcc250>, <keras.callbacks.EarlyStopping object at 0x7f7d22fcf4c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d22fcd330>, <keras.callbacks.TerminateOnNaN object at 0x7f7d22fcd1e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 15:00:17.020036
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:02:24.387 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 127s/epoch - 649ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 5.0805263425290834e-08.
===========
Generating train data for run 373.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_351"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_352 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f844a6867a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f844a6362c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f844a6362c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f80205ce650>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8060604640>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8060604820>, <keras.callbacks.ModelCheckpoint object at 0x7f7d0267bc70>, <keras.callbacks.EarlyStopping object at 0x7f7d0267abc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d0267b940>, <keras.callbacks.TerminateOnNaN object at 0x7f7d0267bd00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 15:02:36.200188
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:04:47.704 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 131s/epoch - 670ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.6935087808430278e-08.
===========
Generating train data for run 373.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_373/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_373
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_362"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_363 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f8441be7f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7e6c434b50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7e6c434b50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c804182e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f84415d4a90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_373/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f84415d5000>, <keras.callbacks.ModelCheckpoint object at 0x7f84415d50c0>, <keras.callbacks.EarlyStopping object at 0x7f84415d5330>, <keras.callbacks.ReduceLROnPlateau object at 0x7f84415d5360>, <keras.callbacks.TerminateOnNaN object at 0x7f84415d4fa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_373/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 373/720 with hyperparameters:
timestamp = 2023-10-30 15:04:54.871699
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:07:09.137 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7435.9155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7435.9155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 134s/epoch - 685ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 5.645029269476759e-09.
===========
Run 373/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

===========
Generating train data for run 383.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f7d8e2979a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d5bfbbfa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d5bfbbfa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d5887aef0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d033f4520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d033f4a90>, <keras.callbacks.ModelCheckpoint object at 0x7f7d033f4b50>, <keras.callbacks.EarlyStopping object at 0x7f7d033f4dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d033f4df0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d033f4a30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:07:19.598679
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:09:58.801 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 159s/epoch - 812ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 383.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_384"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_385 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f7d21e544f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d2297ef50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d2297ef50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d5947ec80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d21e7f490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d21e7fa00>, <keras.callbacks.ModelCheckpoint object at 0x7f7d21e7fac0>, <keras.callbacks.EarlyStopping object at 0x7f7d21e7fd30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d21e7fd60>, <keras.callbacks.TerminateOnNaN object at 0x7f7d21e7f9a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:10:08.490068
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:13:23.582 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 195s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 195s/epoch - 993ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 383.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_395"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_396 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f7c630ab8e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7c43d49000>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7c43d49000>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8452f4dff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cb9511510>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cb9511a80>, <keras.callbacks.ModelCheckpoint object at 0x7f7cb9511b40>, <keras.callbacks.EarlyStopping object at 0x7f7cb9511db0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cb9511de0>, <keras.callbacks.TerminateOnNaN object at 0x7f7cb9511a20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:13:33.777202
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:16:27.653 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 174s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 174s/epoch - 887ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 383.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_406"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_407 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f7d5983c850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d22e0fc40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d22e0fc40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7cf9527a60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d018d1c30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d018d0dc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d018d0a00>, <keras.callbacks.EarlyStopping object at 0x7f7d018d0790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d59ebaf20>, <keras.callbacks.TerminateOnNaN object at 0x7f7d59eb8f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:16:41.871042
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:19:25.940 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 164s/epoch - 836ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 383.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_417"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_418 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f8408198730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d59eb9480>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d59eb9480>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8407b129e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f84079a9750>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f84079a9cc0>, <keras.callbacks.ModelCheckpoint object at 0x7f84079a9d80>, <keras.callbacks.EarlyStopping object at 0x7f84079a9ff0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f84079aa020>, <keras.callbacks.TerminateOnNaN object at 0x7f84079a9c60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:19:36.578686
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:22:28.006 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 171s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 171s/epoch - 873ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 383.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_428"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_429 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f7d23bcefb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d716d8400>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d716d8400>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f84412d3e80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d5ad25ff0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d5ad26560>, <keras.callbacks.ModelCheckpoint object at 0x7f7d5ad26620>, <keras.callbacks.EarlyStopping object at 0x7f7d5ad26890>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d5ad268c0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d5ad26500>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:22:37.275048
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:25:19.582 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 162s/epoch - 826ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 383.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_439"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_440 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f7cd8373ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d5a31ad10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d5a31ad10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d198165c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d21a001c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d21a00730>, <keras.callbacks.ModelCheckpoint object at 0x7f7d21a007f0>, <keras.callbacks.EarlyStopping object at 0x7f7d21a00a60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d21a00a90>, <keras.callbacks.TerminateOnNaN object at 0x7f7d21a006d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:25:28.538736
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:28:15.724 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 167s/epoch - 852ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 383.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_450"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_451 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f7d035bb880>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7cf948e860>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7cf948e860>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d5a8a22f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d035ae890>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d035aee00>, <keras.callbacks.ModelCheckpoint object at 0x7f7d035aeec0>, <keras.callbacks.EarlyStopping object at 0x7f7d035af130>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d035af160>, <keras.callbacks.TerminateOnNaN object at 0x7f7d035aeda0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:28:25.462517
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:31:01.876 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 156s/epoch - 797ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 383.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_461"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_462 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f83f61cada0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d8e302830>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d8e302830>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7bc4446e00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f83f62014e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f83f6201a50>, <keras.callbacks.ModelCheckpoint object at 0x7f83f6201b10>, <keras.callbacks.EarlyStopping object at 0x7f83f6201d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f83f6201db0>, <keras.callbacks.TerminateOnNaN object at 0x7f83f62019f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:31:14.034466
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:34:01.418 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 167s/epoch - 852ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 383.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_472"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_473 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f7d8c0ceda0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7cfbaa5d80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7cfbaa5d80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c3c1e6950>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c3c168c70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c3c1691e0>, <keras.callbacks.ModelCheckpoint object at 0x7f7c3c1692a0>, <keras.callbacks.EarlyStopping object at 0x7f7c3c169510>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c3c169540>, <keras.callbacks.TerminateOnNaN object at 0x7f7c3c169180>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:34:09.515103
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:36:56.383 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 167s/epoch - 850ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 383.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_483"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_484 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f7c2c6ab280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7dbdf978b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7dbdf978b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d1a29c880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c426cd780>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c426ce2f0>, <keras.callbacks.ModelCheckpoint object at 0x7f7c426ce5c0>, <keras.callbacks.EarlyStopping object at 0x7f7c426ce950>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c426cee00>, <keras.callbacks.TerminateOnNaN object at 0x7f7c426ce200>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-30 15:37:05.439731
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:39:53.332 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 168s/epoch - 856ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 383/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

===========
Generating train data for run 387.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_489"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_490 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f7c5010c970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d00b4c430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d00b4c430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7cdb2fe4a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d00b6f670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d00b6f7c0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d00b6dc00>, <keras.callbacks.EarlyStopping object at 0x7f7d00b6e590>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d00b6dc90>, <keras.callbacks.TerminateOnNaN object at 0x7f7d00b6c070>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:39:58.699419
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:41:10.310 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6288.2095, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 6288.2095 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 71s/epoch - 364ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 387.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_495"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_496 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f7be1027e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7bcc012950>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7bcc012950>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f84411b1f30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c3f9d0850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c3f9d0dc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7c3f9d0e80>, <keras.callbacks.EarlyStopping object at 0x7f7c3f9d10f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c3f9d1120>, <keras.callbacks.TerminateOnNaN object at 0x7f7c3f9d0d60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:41:15.167458
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:42:29.083 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6291.5903, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 6291.5903 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 74s/epoch - 376ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 387.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_501"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_502 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f83ed7c7cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f83d5173010>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f83d5173010>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b35387190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f83d4eedea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f83d4eee410>, <keras.callbacks.ModelCheckpoint object at 0x7f83d4eee4d0>, <keras.callbacks.EarlyStopping object at 0x7f83d4eee740>, <keras.callbacks.ReduceLROnPlateau object at 0x7f83d4eee770>, <keras.callbacks.TerminateOnNaN object at 0x7f83d4eee3b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:42:33.741012
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 8: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:44:28.892 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6241.0474, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 6241.0474 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 115s/epoch - 587ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 3.703703703703703e-05.
===========
Generating train data for run 387.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_507"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_508 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f7d22f68ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f800c6c20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f800c6c20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c350e63e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d03c130a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d03c13df0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d03c10c40>, <keras.callbacks.EarlyStopping object at 0x7f7d03c13850>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d03c13130>, <keras.callbacks.TerminateOnNaN object at 0x7f7d03c113c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:44:34.880073
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:45:49.907 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6375.6240, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 6375.6240 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 75s/epoch - 381ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.2345679012345677e-05.
===========
Generating train data for run 387.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_513"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_514 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f7be824b190>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b6d481d50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b6d481d50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d58f144f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cd8ea61d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cd8ea6740>, <keras.callbacks.ModelCheckpoint object at 0x7f7cd8ea6800>, <keras.callbacks.EarlyStopping object at 0x7f7cd8ea6a70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cd8ea6aa0>, <keras.callbacks.TerminateOnNaN object at 0x7f7cd8ea66e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:45:54.474307
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 13: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:47:06.286 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6402.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 6402.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 72s/epoch - 366ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.115226337448558e-06.
===========
Generating train data for run 387.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_519"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_520 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f7cd8e91e40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b854d0160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b854d0160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b6cbac0d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7bc4eb8730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7bc4eb8070>, <keras.callbacks.ModelCheckpoint object at 0x7f7bc4ebaad0>, <keras.callbacks.EarlyStopping object at 0x7f7bc4eb8160>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7bc4eb8370>, <keras.callbacks.TerminateOnNaN object at 0x7f7bc4eb0280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:47:11.473042
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:48:35.988 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6435.3828, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 84s - loss: nan - MinusLogProbMetric: 6435.3828 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 84s/epoch - 430ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.3717421124828526e-06.
===========
Generating train data for run 387.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_525"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_526 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f7d71159480>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f81f23eb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f81f23eb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7cfb62a830>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f806076c310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f806076e4a0>, <keras.callbacks.ModelCheckpoint object at 0x7f806076e350>, <keras.callbacks.EarlyStopping object at 0x7f806076df90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f806076e890>, <keras.callbacks.TerminateOnNaN object at 0x7f806076e800>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:48:40.068413
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:49:51.957 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6449.4604, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 6449.4604 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 72s/epoch - 366ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.572473708276175e-07.
===========
Generating train data for run 387.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_531"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_532 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f7b6c497a90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2cf4f790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2cf4f790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d58d03340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c254fc670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c254fcbe0>, <keras.callbacks.ModelCheckpoint object at 0x7f7c254fcca0>, <keras.callbacks.EarlyStopping object at 0x7f7c254fcf10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c254fcf40>, <keras.callbacks.TerminateOnNaN object at 0x7f7c254fcb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:49:56.808346
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:51:09.505 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6450.6484, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 73s - loss: nan - MinusLogProbMetric: 6450.6484 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 73s/epoch - 370ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.524157902758725e-07.
===========
Generating train data for run 387.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_537"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_538 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f7b2d04ffa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b2cee9f60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b2cee9f60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c81d0fd00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7b6d09dbd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7b6d09e140>, <keras.callbacks.ModelCheckpoint object at 0x7f7b6d09e200>, <keras.callbacks.EarlyStopping object at 0x7f7b6d09e470>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7b6d09e4a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7b6d09e0e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:51:15.737857
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 33: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:52:41.701 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6451.2051, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 86s - loss: nan - MinusLogProbMetric: 6451.2051 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 86s/epoch - 438ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 5.0805263425290834e-08.
===========
Generating train data for run 387.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_543"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_544 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f7c5025d9c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2c7340d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2c7340d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c2477de40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c5024a350>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c5024bd00>, <keras.callbacks.ModelCheckpoint object at 0x7f7c50248910>, <keras.callbacks.EarlyStopping object at 0x7f7c5024b3a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c5024a890>, <keras.callbacks.TerminateOnNaN object at 0x7f7c50249900>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:52:45.291274
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 33: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:54:06.243 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6451.9287, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 81s - loss: nan - MinusLogProbMetric: 6451.9287 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 81s/epoch - 413ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.6935087808430278e-08.
===========
Generating train data for run 387.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_549"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_550 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f7bd8c872e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f83d4ac7d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f83d4ac7d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b85cf77f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c2c80ee30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c2c80f3a0>, <keras.callbacks.ModelCheckpoint object at 0x7f7c2c80f460>, <keras.callbacks.EarlyStopping object at 0x7f7c2c80f6d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c2c80f700>, <keras.callbacks.TerminateOnNaN object at 0x7f7c2c80f340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-30 15:54:11.225997
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 33: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:55:25.370 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6452.4282, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 6452.4282 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 74s/epoch - 377ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 5.645029269476759e-09.
===========
Run 387/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

===========
Generating train data for run 391.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f83d4d8a410>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f1c507100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f1c507100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7cd8f0a290>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7f1c57c430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7f1c57db70>, <keras.callbacks.ModelCheckpoint object at 0x7f7f1c57f730>, <keras.callbacks.EarlyStopping object at 0x7f7f1c57e7d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7f1c57f2e0>, <keras.callbacks.TerminateOnNaN object at 0x7f7f1c57d7e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 15:55:44.965505
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:58:26.885 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7817.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7817.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 162s/epoch - 825ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 391.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_571"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_572 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f7c241c1540>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b25e42800>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b25e42800>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c351501f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c35185c60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c351861d0>, <keras.callbacks.ModelCheckpoint object at 0x7f7c35186290>, <keras.callbacks.EarlyStopping object at 0x7f7c35186500>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c35186530>, <keras.callbacks.TerminateOnNaN object at 0x7f7c35186170>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 15:58:38.344686
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:01:27.068 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7817.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 7817.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 169s/epoch - 860ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 391.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_582"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_583 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f7de4f313c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f80801cb8e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f80801cb8e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d018d0460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7bfd5d0ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7bfd5d3a60>, <keras.callbacks.ModelCheckpoint object at 0x7f7bfd5d1f00>, <keras.callbacks.EarlyStopping object at 0x7f7bfd5d1ea0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7bfd5d39d0>, <keras.callbacks.TerminateOnNaN object at 0x7f7bfd5d2500>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 16:01:36.916316
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:04:39.206 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7817.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 182s - loss: nan - MinusLogProbMetric: 7817.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 182s/epoch - 929ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 3.703703703703703e-05.
===========
Generating train data for run 391.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_593"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_594 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f83ed8ad7e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d1b68acb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d1b68acb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c3ccff3d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7fa073ab30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7fa073ace0>, <keras.callbacks.ModelCheckpoint object at 0x7f7fa073b6d0>, <keras.callbacks.EarlyStopping object at 0x7f7fa073aa40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7fa073a050>, <keras.callbacks.TerminateOnNaN object at 0x7f7fc0653b50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 16:04:50.342829
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:07:31.330 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7817.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 7817.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 161s/epoch - 820ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.2345679012345677e-05.
===========
Generating train data for run 391.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_604"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_605 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f83cb87a8f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f83cb987b80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f83cb987b80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f83cbd3f760>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f83cb8b1030>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f83cb8b15a0>, <keras.callbacks.ModelCheckpoint object at 0x7f83cb8b1660>, <keras.callbacks.EarlyStopping object at 0x7f83cb8b18d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f83cb8b1900>, <keras.callbacks.TerminateOnNaN object at 0x7f83cb8b1540>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 16:07:40.429002
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:10:41.098 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7817.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 180s - loss: nan - MinusLogProbMetric: 7817.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 180s/epoch - 921ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.115226337448558e-06.
===========
Generating train data for run 391.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_615"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_616 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f7c24a26680>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7be0b1a950>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7be0b1a950>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b258a3640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c419120e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c41912650>, <keras.callbacks.ModelCheckpoint object at 0x7f7c41912710>, <keras.callbacks.EarlyStopping object at 0x7f7c41912980>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c419129b0>, <keras.callbacks.TerminateOnNaN object at 0x7f7c419125f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 16:10:51.445721
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 16:13:54.884 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7817.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 183s - loss: nan - MinusLogProbMetric: 7817.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 183s/epoch - 935ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.3717421124828526e-06.
===========
Generating train data for run 391.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_391/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_391
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_626"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_627 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f7d581901f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7bfd6685b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7bfd6685b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d18e99870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cb96e9240>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_391/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c242db280>, <keras.callbacks.ModelCheckpoint object at 0x7f7c242dad40>, <keras.callbacks.EarlyStopping object at 0x7f7c242dbf10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c242daec0>, <keras.callbacks.TerminateOnNaN object at 0x7f7c242dabf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_391/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 391/720 with hyperparameters:
timestamp = 2023-10-30 16:14:06.314117
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
JIT session error: JIT session error: Cannot allocate memoryCannot allocate memory

JIT session error: Cannot allocate memory
LLVM ERROR: Unable to allocate section memory!
