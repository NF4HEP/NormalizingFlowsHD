2023-10-09 17:14:06.292268: Importing os...
2023-10-09 17:14:06.292340: Importing sys...
2023-10-09 17:14:06.292358: Importing and initializing argparse...
Visible devices: [3]
2023-10-09 17:14:06.309169: Importing timer from timeit...
2023-10-09 17:14:06.309754: Setting env variables for tf import (only device [3] will be available)...
2023-10-09 17:14:06.309800: Importing numpy...
2023-10-09 17:14:06.480639: Importing pandas...
2023-10-09 17:14:06.664107: Importing shutil...
2023-10-09 17:14:06.664138: Importing subprocess...
2023-10-09 17:14:06.664145: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-09 17:14:08.907788: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-09 17:14:09.247495: Importing textwrap...
2023-10-09 17:14:09.247525: Importing timeit...
2023-10-09 17:14:09.247535: Importing traceback...
2023-10-09 17:14:09.247541: Importing typing...
2023-10-09 17:14:09.247551: Setting tf configs...
2023-10-09 17:14:09.383077: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-09 17:14:10.945075: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

===========
Generating train data for run 247.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_247/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_247/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_247/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_247
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1074400   
 r)                                                              
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f3fa8525120>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fa8677c40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fa8677c40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3f8049ee30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3f58719210>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3f58719780>, <keras.callbacks.ModelCheckpoint object at 0x7f3f587198d0>, <keras.callbacks.EarlyStopping object at 0x7f3f58719ae0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3f58719b10>, <keras.callbacks.TerminateOnNaN object at 0x7f3f58719840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_247/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 247/720 with hyperparameters:
timestamp = 2023-10-09 17:14:21.303461
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 42: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-09 17:17:09.695 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2343.2288, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 2343.2288 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 168s/epoch - 858ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 247.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_247/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_247/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_247/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_247
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f43723ddcf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3bfc7c2e90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3bfc7c2e90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4369f52c80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4369de8520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4369de8a90>, <keras.callbacks.ModelCheckpoint object at 0x7f4369de8b50>, <keras.callbacks.EarlyStopping object at 0x7f4369de8dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4369de8df0>, <keras.callbacks.TerminateOnNaN object at 0x7f4369de8a30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_247/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 247/720 with hyperparameters:
timestamp = 2023-10-09 17:17:19.402019
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
2023-10-09 17:20:47.590 
Epoch 1/1000 
	 loss: 1091.7368, MinusLogProbMetric: 1091.7368, val_loss: 345.6022, val_MinusLogProbMetric: 345.6022

Epoch 1: val_loss improved from inf to 345.60217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 209s - loss: 1091.7368 - MinusLogProbMetric: 1091.7368 - val_loss: 345.6022 - val_MinusLogProbMetric: 345.6022 - lr: 3.3333e-04 - 209s/epoch - 1s/step
Epoch 2/1000
2023-10-09 17:21:48.536 
Epoch 2/1000 
	 loss: 331.6586, MinusLogProbMetric: 331.6586, val_loss: 557.9550, val_MinusLogProbMetric: 557.9550

Epoch 2: val_loss did not improve from 345.60217
196/196 - 60s - loss: 331.6586 - MinusLogProbMetric: 331.6586 - val_loss: 557.9550 - val_MinusLogProbMetric: 557.9550 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 3/1000
2023-10-09 17:22:47.516 
Epoch 3/1000 
	 loss: 324.0813, MinusLogProbMetric: 324.0813, val_loss: 362.9130, val_MinusLogProbMetric: 362.9130

Epoch 3: val_loss did not improve from 345.60217
196/196 - 59s - loss: 324.0813 - MinusLogProbMetric: 324.0813 - val_loss: 362.9130 - val_MinusLogProbMetric: 362.9130 - lr: 3.3333e-04 - 59s/epoch - 301ms/step
Epoch 4/1000
2023-10-09 17:23:47.435 
Epoch 4/1000 
	 loss: 197.2797, MinusLogProbMetric: 197.2797, val_loss: 173.4179, val_MinusLogProbMetric: 173.4179

Epoch 4: val_loss improved from 345.60217 to 173.41792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 197.2797 - MinusLogProbMetric: 197.2797 - val_loss: 173.4179 - val_MinusLogProbMetric: 173.4179 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 5/1000
2023-10-09 17:24:52.081 
Epoch 5/1000 
	 loss: 133.8222, MinusLogProbMetric: 133.8222, val_loss: 119.8284, val_MinusLogProbMetric: 119.8284

Epoch 5: val_loss improved from 173.41792 to 119.82836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 65s - loss: 133.8222 - MinusLogProbMetric: 133.8222 - val_loss: 119.8284 - val_MinusLogProbMetric: 119.8284 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 6/1000
2023-10-09 17:25:54.654 
Epoch 6/1000 
	 loss: 106.2695, MinusLogProbMetric: 106.2695, val_loss: 95.1072, val_MinusLogProbMetric: 95.1072

Epoch 6: val_loss improved from 119.82836 to 95.10725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 106.2695 - MinusLogProbMetric: 106.2695 - val_loss: 95.1072 - val_MinusLogProbMetric: 95.1072 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 7/1000
2023-10-09 17:26:54.902 
Epoch 7/1000 
	 loss: 90.9911, MinusLogProbMetric: 90.9911, val_loss: 85.4080, val_MinusLogProbMetric: 85.4080

Epoch 7: val_loss improved from 95.10725 to 85.40799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 90.9911 - MinusLogProbMetric: 90.9911 - val_loss: 85.4080 - val_MinusLogProbMetric: 85.4080 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 8/1000
2023-10-09 17:27:57.373 
Epoch 8/1000 
	 loss: 79.1831, MinusLogProbMetric: 79.1831, val_loss: 74.9188, val_MinusLogProbMetric: 74.9188

Epoch 8: val_loss improved from 85.40799 to 74.91879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 79.1831 - MinusLogProbMetric: 79.1831 - val_loss: 74.9188 - val_MinusLogProbMetric: 74.9188 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 9/1000
2023-10-09 17:28:57.998 
Epoch 9/1000 
	 loss: 73.2284, MinusLogProbMetric: 73.2284, val_loss: 67.5045, val_MinusLogProbMetric: 67.5045

Epoch 9: val_loss improved from 74.91879 to 67.50446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 73.2284 - MinusLogProbMetric: 73.2284 - val_loss: 67.5045 - val_MinusLogProbMetric: 67.5045 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 10/1000
2023-10-09 17:30:01.772 
Epoch 10/1000 
	 loss: 62.6826, MinusLogProbMetric: 62.6826, val_loss: 59.6858, val_MinusLogProbMetric: 59.6858

Epoch 10: val_loss improved from 67.50446 to 59.68577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 62.6826 - MinusLogProbMetric: 62.6826 - val_loss: 59.6858 - val_MinusLogProbMetric: 59.6858 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 11/1000
2023-10-09 17:31:03.836 
Epoch 11/1000 
	 loss: 59.1211, MinusLogProbMetric: 59.1211, val_loss: 57.1479, val_MinusLogProbMetric: 57.1479

Epoch 11: val_loss improved from 59.68577 to 57.14793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 59.1211 - MinusLogProbMetric: 59.1211 - val_loss: 57.1479 - val_MinusLogProbMetric: 57.1479 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 12/1000
2023-10-09 17:32:05.369 
Epoch 12/1000 
	 loss: 61.9395, MinusLogProbMetric: 61.9395, val_loss: 53.7157, val_MinusLogProbMetric: 53.7157

Epoch 12: val_loss improved from 57.14793 to 53.71566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 61.9395 - MinusLogProbMetric: 61.9395 - val_loss: 53.7157 - val_MinusLogProbMetric: 53.7157 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 13/1000
2023-10-09 17:33:06.711 
Epoch 13/1000 
	 loss: 51.0639, MinusLogProbMetric: 51.0639, val_loss: 49.1461, val_MinusLogProbMetric: 49.1461

Epoch 13: val_loss improved from 53.71566 to 49.14612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 51.0639 - MinusLogProbMetric: 51.0639 - val_loss: 49.1461 - val_MinusLogProbMetric: 49.1461 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 14/1000
2023-10-09 17:34:09.980 
Epoch 14/1000 
	 loss: 47.3028, MinusLogProbMetric: 47.3028, val_loss: 45.9268, val_MinusLogProbMetric: 45.9268

Epoch 14: val_loss improved from 49.14612 to 45.92683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 47.3028 - MinusLogProbMetric: 47.3028 - val_loss: 45.9268 - val_MinusLogProbMetric: 45.9268 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 15/1000
2023-10-09 17:35:12.476 
Epoch 15/1000 
	 loss: 43.5911, MinusLogProbMetric: 43.5911, val_loss: 42.2501, val_MinusLogProbMetric: 42.2501

Epoch 15: val_loss improved from 45.92683 to 42.25007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 43.5911 - MinusLogProbMetric: 43.5911 - val_loss: 42.2501 - val_MinusLogProbMetric: 42.2501 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 16/1000
2023-10-09 17:36:13.857 
Epoch 16/1000 
	 loss: 43.9179, MinusLogProbMetric: 43.9179, val_loss: 40.8000, val_MinusLogProbMetric: 40.8000

Epoch 16: val_loss improved from 42.25007 to 40.79999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 43.9179 - MinusLogProbMetric: 43.9179 - val_loss: 40.8000 - val_MinusLogProbMetric: 40.8000 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 17/1000
2023-10-09 17:37:15.148 
Epoch 17/1000 
	 loss: 39.2303, MinusLogProbMetric: 39.2303, val_loss: 38.5315, val_MinusLogProbMetric: 38.5315

Epoch 17: val_loss improved from 40.79999 to 38.53154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 39.2303 - MinusLogProbMetric: 39.2303 - val_loss: 38.5315 - val_MinusLogProbMetric: 38.5315 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 18/1000
2023-10-09 17:38:16.312 
Epoch 18/1000 
	 loss: 37.2634, MinusLogProbMetric: 37.2634, val_loss: 37.3292, val_MinusLogProbMetric: 37.3292

Epoch 18: val_loss improved from 38.53154 to 37.32919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 37.2634 - MinusLogProbMetric: 37.2634 - val_loss: 37.3292 - val_MinusLogProbMetric: 37.3292 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 19/1000
2023-10-09 17:39:17.076 
Epoch 19/1000 
	 loss: 36.7739, MinusLogProbMetric: 36.7739, val_loss: 35.4612, val_MinusLogProbMetric: 35.4612

Epoch 19: val_loss improved from 37.32919 to 35.46115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 36.7739 - MinusLogProbMetric: 36.7739 - val_loss: 35.4612 - val_MinusLogProbMetric: 35.4612 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 20/1000
2023-10-09 17:40:17.706 
Epoch 20/1000 
	 loss: 34.9997, MinusLogProbMetric: 34.9997, val_loss: 34.1387, val_MinusLogProbMetric: 34.1387

Epoch 20: val_loss improved from 35.46115 to 34.13867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 34.9997 - MinusLogProbMetric: 34.9997 - val_loss: 34.1387 - val_MinusLogProbMetric: 34.1387 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 21/1000
2023-10-09 17:41:18.402 
Epoch 21/1000 
	 loss: 33.4535, MinusLogProbMetric: 33.4535, val_loss: 33.7413, val_MinusLogProbMetric: 33.7413

Epoch 21: val_loss improved from 34.13867 to 33.74129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 33.4535 - MinusLogProbMetric: 33.4535 - val_loss: 33.7413 - val_MinusLogProbMetric: 33.7413 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 22/1000
2023-10-09 17:42:19.180 
Epoch 22/1000 
	 loss: 32.4192, MinusLogProbMetric: 32.4192, val_loss: 32.1530, val_MinusLogProbMetric: 32.1530

Epoch 22: val_loss improved from 33.74129 to 32.15302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 32.4192 - MinusLogProbMetric: 32.4192 - val_loss: 32.1530 - val_MinusLogProbMetric: 32.1530 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 23/1000
2023-10-09 17:43:20.870 
Epoch 23/1000 
	 loss: 53.9169, MinusLogProbMetric: 53.9169, val_loss: 56.3376, val_MinusLogProbMetric: 56.3376

Epoch 23: val_loss did not improve from 32.15302
196/196 - 61s - loss: 53.9169 - MinusLogProbMetric: 53.9169 - val_loss: 56.3376 - val_MinusLogProbMetric: 56.3376 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 24/1000
2023-10-09 17:44:21.055 
Epoch 24/1000 
	 loss: 46.5036, MinusLogProbMetric: 46.5036, val_loss: 41.9503, val_MinusLogProbMetric: 41.9503

Epoch 24: val_loss did not improve from 32.15302
196/196 - 60s - loss: 46.5036 - MinusLogProbMetric: 46.5036 - val_loss: 41.9503 - val_MinusLogProbMetric: 41.9503 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 25/1000
2023-10-09 17:45:21.599 
Epoch 25/1000 
	 loss: 39.0009, MinusLogProbMetric: 39.0009, val_loss: 35.5864, val_MinusLogProbMetric: 35.5864

Epoch 25: val_loss did not improve from 32.15302
196/196 - 61s - loss: 39.0009 - MinusLogProbMetric: 39.0009 - val_loss: 35.5864 - val_MinusLogProbMetric: 35.5864 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 26/1000
2023-10-09 17:46:22.491 
Epoch 26/1000 
	 loss: 34.8770, MinusLogProbMetric: 34.8770, val_loss: 33.5320, val_MinusLogProbMetric: 33.5320

Epoch 26: val_loss did not improve from 32.15302
196/196 - 61s - loss: 34.8770 - MinusLogProbMetric: 34.8770 - val_loss: 33.5320 - val_MinusLogProbMetric: 33.5320 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 27/1000
2023-10-09 17:47:21.932 
Epoch 27/1000 
	 loss: 38.7641, MinusLogProbMetric: 38.7641, val_loss: 39.8826, val_MinusLogProbMetric: 39.8826

Epoch 27: val_loss did not improve from 32.15302
196/196 - 59s - loss: 38.7641 - MinusLogProbMetric: 38.7641 - val_loss: 39.8826 - val_MinusLogProbMetric: 39.8826 - lr: 3.3333e-04 - 59s/epoch - 303ms/step
Epoch 28/1000
2023-10-09 17:48:22.103 
Epoch 28/1000 
	 loss: 34.8960, MinusLogProbMetric: 34.8960, val_loss: 32.5691, val_MinusLogProbMetric: 32.5691

Epoch 28: val_loss did not improve from 32.15302
196/196 - 60s - loss: 34.8960 - MinusLogProbMetric: 34.8960 - val_loss: 32.5691 - val_MinusLogProbMetric: 32.5691 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 29/1000
2023-10-09 17:49:23.311 
Epoch 29/1000 
	 loss: 31.8366, MinusLogProbMetric: 31.8366, val_loss: 31.1309, val_MinusLogProbMetric: 31.1309

Epoch 29: val_loss improved from 32.15302 to 31.13092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 31.8366 - MinusLogProbMetric: 31.8366 - val_loss: 31.1309 - val_MinusLogProbMetric: 31.1309 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 30/1000
2023-10-09 17:50:25.603 
Epoch 30/1000 
	 loss: 30.2973, MinusLogProbMetric: 30.2973, val_loss: 30.1380, val_MinusLogProbMetric: 30.1380

Epoch 30: val_loss improved from 31.13092 to 30.13804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 30.2973 - MinusLogProbMetric: 30.2973 - val_loss: 30.1380 - val_MinusLogProbMetric: 30.1380 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 31/1000
2023-10-09 17:51:26.006 
Epoch 31/1000 
	 loss: 29.5486, MinusLogProbMetric: 29.5486, val_loss: 29.0461, val_MinusLogProbMetric: 29.0461

Epoch 31: val_loss improved from 30.13804 to 29.04606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 29.5486 - MinusLogProbMetric: 29.5486 - val_loss: 29.0461 - val_MinusLogProbMetric: 29.0461 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 32/1000
2023-10-09 17:52:26.624 
Epoch 32/1000 
	 loss: 28.8603, MinusLogProbMetric: 28.8603, val_loss: 27.9592, val_MinusLogProbMetric: 27.9592

Epoch 32: val_loss improved from 29.04606 to 27.95924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 28.8603 - MinusLogProbMetric: 28.8603 - val_loss: 27.9592 - val_MinusLogProbMetric: 27.9592 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 33/1000
2023-10-09 17:53:30.711 
Epoch 33/1000 
	 loss: 27.7247, MinusLogProbMetric: 27.7247, val_loss: 27.4583, val_MinusLogProbMetric: 27.4583

Epoch 33: val_loss improved from 27.95924 to 27.45829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 27.7247 - MinusLogProbMetric: 27.7247 - val_loss: 27.4583 - val_MinusLogProbMetric: 27.4583 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 34/1000
2023-10-09 17:54:31.836 
Epoch 34/1000 
	 loss: 27.3393, MinusLogProbMetric: 27.3393, val_loss: 27.3100, val_MinusLogProbMetric: 27.3100

Epoch 34: val_loss improved from 27.45829 to 27.31000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 27.3393 - MinusLogProbMetric: 27.3393 - val_loss: 27.3100 - val_MinusLogProbMetric: 27.3100 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 35/1000
2023-10-09 17:55:33.286 
Epoch 35/1000 
	 loss: 26.7800, MinusLogProbMetric: 26.7800, val_loss: 26.6101, val_MinusLogProbMetric: 26.6101

Epoch 35: val_loss improved from 27.31000 to 26.61007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 26.7800 - MinusLogProbMetric: 26.7800 - val_loss: 26.6101 - val_MinusLogProbMetric: 26.6101 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 36/1000
2023-10-09 17:56:34.459 
Epoch 36/1000 
	 loss: 26.3397, MinusLogProbMetric: 26.3397, val_loss: 26.2986, val_MinusLogProbMetric: 26.2986

Epoch 36: val_loss improved from 26.61007 to 26.29865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 26.3397 - MinusLogProbMetric: 26.3397 - val_loss: 26.2986 - val_MinusLogProbMetric: 26.2986 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 37/1000
2023-10-09 17:57:37.666 
Epoch 37/1000 
	 loss: 25.9793, MinusLogProbMetric: 25.9793, val_loss: 26.1271, val_MinusLogProbMetric: 26.1271

Epoch 37: val_loss improved from 26.29865 to 26.12713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 25.9793 - MinusLogProbMetric: 25.9793 - val_loss: 26.1271 - val_MinusLogProbMetric: 26.1271 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 38/1000
2023-10-09 17:58:41.834 
Epoch 38/1000 
	 loss: 25.6421, MinusLogProbMetric: 25.6421, val_loss: 25.5376, val_MinusLogProbMetric: 25.5376

Epoch 38: val_loss improved from 26.12713 to 25.53761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 25.6421 - MinusLogProbMetric: 25.6421 - val_loss: 25.5376 - val_MinusLogProbMetric: 25.5376 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 39/1000
2023-10-09 17:59:45.094 
Epoch 39/1000 
	 loss: 25.2799, MinusLogProbMetric: 25.2799, val_loss: 25.7369, val_MinusLogProbMetric: 25.7369

Epoch 39: val_loss did not improve from 25.53761
196/196 - 62s - loss: 25.2799 - MinusLogProbMetric: 25.2799 - val_loss: 25.7369 - val_MinusLogProbMetric: 25.7369 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 40/1000
2023-10-09 18:00:46.672 
Epoch 40/1000 
	 loss: 25.1323, MinusLogProbMetric: 25.1323, val_loss: 24.9174, val_MinusLogProbMetric: 24.9174

Epoch 40: val_loss improved from 25.53761 to 24.91741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 25.1323 - MinusLogProbMetric: 25.1323 - val_loss: 24.9174 - val_MinusLogProbMetric: 24.9174 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 41/1000
2023-10-09 18:01:50.032 
Epoch 41/1000 
	 loss: 25.0079, MinusLogProbMetric: 25.0079, val_loss: 63.2545, val_MinusLogProbMetric: 63.2545

Epoch 41: val_loss did not improve from 24.91741
196/196 - 62s - loss: 25.0079 - MinusLogProbMetric: 25.0079 - val_loss: 63.2545 - val_MinusLogProbMetric: 63.2545 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 42/1000
2023-10-09 18:02:52.284 
Epoch 42/1000 
	 loss: 28.8682, MinusLogProbMetric: 28.8682, val_loss: 25.1579, val_MinusLogProbMetric: 25.1579

Epoch 42: val_loss did not improve from 24.91741
196/196 - 62s - loss: 28.8682 - MinusLogProbMetric: 28.8682 - val_loss: 25.1579 - val_MinusLogProbMetric: 25.1579 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 43/1000
2023-10-09 18:03:54.082 
Epoch 43/1000 
	 loss: 24.7286, MinusLogProbMetric: 24.7286, val_loss: 24.5783, val_MinusLogProbMetric: 24.5783

Epoch 43: val_loss improved from 24.91741 to 24.57829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 24.7286 - MinusLogProbMetric: 24.7286 - val_loss: 24.5783 - val_MinusLogProbMetric: 24.5783 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 44/1000
2023-10-09 18:04:55.410 
Epoch 44/1000 
	 loss: 24.4720, MinusLogProbMetric: 24.4720, val_loss: 24.5996, val_MinusLogProbMetric: 24.5996

Epoch 44: val_loss did not improve from 24.57829
196/196 - 60s - loss: 24.4720 - MinusLogProbMetric: 24.4720 - val_loss: 24.5996 - val_MinusLogProbMetric: 24.5996 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 45/1000
2023-10-09 18:05:55.947 
Epoch 45/1000 
	 loss: 24.0838, MinusLogProbMetric: 24.0838, val_loss: 23.9347, val_MinusLogProbMetric: 23.9347

Epoch 45: val_loss improved from 24.57829 to 23.93475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 24.0838 - MinusLogProbMetric: 24.0838 - val_loss: 23.9347 - val_MinusLogProbMetric: 23.9347 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 46/1000
2023-10-09 18:06:56.078 
Epoch 46/1000 
	 loss: 23.7456, MinusLogProbMetric: 23.7456, val_loss: 23.7694, val_MinusLogProbMetric: 23.7694

Epoch 46: val_loss improved from 23.93475 to 23.76944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 23.7456 - MinusLogProbMetric: 23.7456 - val_loss: 23.7694 - val_MinusLogProbMetric: 23.7694 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 47/1000
2023-10-09 18:07:56.449 
Epoch 47/1000 
	 loss: 23.5539, MinusLogProbMetric: 23.5539, val_loss: 23.7156, val_MinusLogProbMetric: 23.7156

Epoch 47: val_loss improved from 23.76944 to 23.71557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 23.5539 - MinusLogProbMetric: 23.5539 - val_loss: 23.7156 - val_MinusLogProbMetric: 23.7156 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 48/1000
2023-10-09 18:08:57.352 
Epoch 48/1000 
	 loss: 23.3638, MinusLogProbMetric: 23.3638, val_loss: 23.4340, val_MinusLogProbMetric: 23.4340

Epoch 48: val_loss improved from 23.71557 to 23.43403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 23.3638 - MinusLogProbMetric: 23.3638 - val_loss: 23.4340 - val_MinusLogProbMetric: 23.4340 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 49/1000
2023-10-09 18:09:58.585 
Epoch 49/1000 
	 loss: 23.2185, MinusLogProbMetric: 23.2185, val_loss: 23.5805, val_MinusLogProbMetric: 23.5805

Epoch 49: val_loss did not improve from 23.43403
196/196 - 60s - loss: 23.2185 - MinusLogProbMetric: 23.2185 - val_loss: 23.5805 - val_MinusLogProbMetric: 23.5805 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 50/1000
2023-10-09 18:10:59.417 
Epoch 50/1000 
	 loss: 22.9956, MinusLogProbMetric: 22.9956, val_loss: 23.0459, val_MinusLogProbMetric: 23.0459

Epoch 50: val_loss improved from 23.43403 to 23.04589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 22.9956 - MinusLogProbMetric: 22.9956 - val_loss: 23.0459 - val_MinusLogProbMetric: 23.0459 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 51/1000
2023-10-09 18:12:02.168 
Epoch 51/1000 
	 loss: 29.1091, MinusLogProbMetric: 29.1091, val_loss: 26.3565, val_MinusLogProbMetric: 26.3565

Epoch 51: val_loss did not improve from 23.04589
196/196 - 62s - loss: 29.1091 - MinusLogProbMetric: 29.1091 - val_loss: 26.3565 - val_MinusLogProbMetric: 26.3565 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 52/1000
2023-10-09 18:13:02.275 
Epoch 52/1000 
	 loss: 24.8408, MinusLogProbMetric: 24.8408, val_loss: 24.1146, val_MinusLogProbMetric: 24.1146

Epoch 52: val_loss did not improve from 23.04589
196/196 - 60s - loss: 24.8408 - MinusLogProbMetric: 24.8408 - val_loss: 24.1146 - val_MinusLogProbMetric: 24.1146 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 53/1000
2023-10-09 18:14:02.437 
Epoch 53/1000 
	 loss: 23.2742, MinusLogProbMetric: 23.2742, val_loss: 22.8367, val_MinusLogProbMetric: 22.8367

Epoch 53: val_loss improved from 23.04589 to 22.83673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 23.2742 - MinusLogProbMetric: 23.2742 - val_loss: 22.8367 - val_MinusLogProbMetric: 22.8367 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 54/1000
2023-10-09 18:15:04.067 
Epoch 54/1000 
	 loss: 22.7508, MinusLogProbMetric: 22.7508, val_loss: 22.8896, val_MinusLogProbMetric: 22.8896

Epoch 54: val_loss did not improve from 22.83673
196/196 - 61s - loss: 22.7508 - MinusLogProbMetric: 22.7508 - val_loss: 22.8896 - val_MinusLogProbMetric: 22.8896 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 55/1000
2023-10-09 18:16:04.350 
Epoch 55/1000 
	 loss: 22.5422, MinusLogProbMetric: 22.5422, val_loss: 22.5469, val_MinusLogProbMetric: 22.5469

Epoch 55: val_loss improved from 22.83673 to 22.54691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 22.5422 - MinusLogProbMetric: 22.5422 - val_loss: 22.5469 - val_MinusLogProbMetric: 22.5469 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 56/1000
2023-10-09 18:17:07.070 
Epoch 56/1000 
	 loss: 22.3861, MinusLogProbMetric: 22.3861, val_loss: 22.4661, val_MinusLogProbMetric: 22.4661

Epoch 56: val_loss improved from 22.54691 to 22.46609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 22.3861 - MinusLogProbMetric: 22.3861 - val_loss: 22.4661 - val_MinusLogProbMetric: 22.4661 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 57/1000
2023-10-09 18:18:10.733 
Epoch 57/1000 
	 loss: 22.2344, MinusLogProbMetric: 22.2344, val_loss: 22.3631, val_MinusLogProbMetric: 22.3631

Epoch 57: val_loss improved from 22.46609 to 22.36308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 22.2344 - MinusLogProbMetric: 22.2344 - val_loss: 22.3631 - val_MinusLogProbMetric: 22.3631 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 58/1000
2023-10-09 18:19:12.011 
Epoch 58/1000 
	 loss: 22.0502, MinusLogProbMetric: 22.0502, val_loss: 22.0388, val_MinusLogProbMetric: 22.0388

Epoch 58: val_loss improved from 22.36308 to 22.03885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 22.0502 - MinusLogProbMetric: 22.0502 - val_loss: 22.0388 - val_MinusLogProbMetric: 22.0388 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 59/1000
2023-10-09 18:20:15.034 
Epoch 59/1000 
	 loss: 21.9291, MinusLogProbMetric: 21.9291, val_loss: 21.9344, val_MinusLogProbMetric: 21.9344

Epoch 59: val_loss improved from 22.03885 to 21.93436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 21.9291 - MinusLogProbMetric: 21.9291 - val_loss: 21.9344 - val_MinusLogProbMetric: 21.9344 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 60/1000
2023-10-09 18:21:17.407 
Epoch 60/1000 
	 loss: 21.8679, MinusLogProbMetric: 21.8679, val_loss: 21.8650, val_MinusLogProbMetric: 21.8650

Epoch 60: val_loss improved from 21.93436 to 21.86502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 21.8679 - MinusLogProbMetric: 21.8679 - val_loss: 21.8650 - val_MinusLogProbMetric: 21.8650 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 61/1000
2023-10-09 18:22:18.992 
Epoch 61/1000 
	 loss: 21.7418, MinusLogProbMetric: 21.7418, val_loss: 21.6762, val_MinusLogProbMetric: 21.6762

Epoch 61: val_loss improved from 21.86502 to 21.67615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 21.7418 - MinusLogProbMetric: 21.7418 - val_loss: 21.6762 - val_MinusLogProbMetric: 21.6762 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 62/1000
2023-10-09 18:23:22.691 
Epoch 62/1000 
	 loss: 21.6744, MinusLogProbMetric: 21.6744, val_loss: 21.7617, val_MinusLogProbMetric: 21.7617

Epoch 62: val_loss did not improve from 21.67615
196/196 - 63s - loss: 21.6744 - MinusLogProbMetric: 21.6744 - val_loss: 21.7617 - val_MinusLogProbMetric: 21.7617 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 63/1000
2023-10-09 18:24:23.657 
Epoch 63/1000 
	 loss: 21.6332, MinusLogProbMetric: 21.6332, val_loss: 21.5167, val_MinusLogProbMetric: 21.5167

Epoch 63: val_loss improved from 21.67615 to 21.51675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 21.6332 - MinusLogProbMetric: 21.6332 - val_loss: 21.5167 - val_MinusLogProbMetric: 21.5167 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 64/1000
2023-10-09 18:25:25.677 
Epoch 64/1000 
	 loss: 21.4214, MinusLogProbMetric: 21.4214, val_loss: 21.5587, val_MinusLogProbMetric: 21.5587

Epoch 64: val_loss did not improve from 21.51675
196/196 - 61s - loss: 21.4214 - MinusLogProbMetric: 21.4214 - val_loss: 21.5587 - val_MinusLogProbMetric: 21.5587 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 65/1000
2023-10-09 18:26:31.416 
Epoch 65/1000 
	 loss: 21.3439, MinusLogProbMetric: 21.3439, val_loss: 21.4825, val_MinusLogProbMetric: 21.4825

Epoch 65: val_loss improved from 21.51675 to 21.48253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 67s - loss: 21.3439 - MinusLogProbMetric: 21.3439 - val_loss: 21.4825 - val_MinusLogProbMetric: 21.4825 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 66/1000
2023-10-09 18:27:33.128 
Epoch 66/1000 
	 loss: 21.4097, MinusLogProbMetric: 21.4097, val_loss: 21.8412, val_MinusLogProbMetric: 21.8412

Epoch 66: val_loss did not improve from 21.48253
196/196 - 61s - loss: 21.4097 - MinusLogProbMetric: 21.4097 - val_loss: 21.8412 - val_MinusLogProbMetric: 21.8412 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 67/1000
2023-10-09 18:28:34.091 
Epoch 67/1000 
	 loss: 21.2422, MinusLogProbMetric: 21.2422, val_loss: 21.3011, val_MinusLogProbMetric: 21.3011

Epoch 67: val_loss improved from 21.48253 to 21.30111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 21.2422 - MinusLogProbMetric: 21.2422 - val_loss: 21.3011 - val_MinusLogProbMetric: 21.3011 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 68/1000
2023-10-09 18:29:35.967 
Epoch 68/1000 
	 loss: 21.1901, MinusLogProbMetric: 21.1901, val_loss: 21.3514, val_MinusLogProbMetric: 21.3514

Epoch 68: val_loss did not improve from 21.30111
196/196 - 61s - loss: 21.1901 - MinusLogProbMetric: 21.1901 - val_loss: 21.3514 - val_MinusLogProbMetric: 21.3514 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 69/1000
2023-10-09 18:30:36.936 
Epoch 69/1000 
	 loss: 21.0841, MinusLogProbMetric: 21.0841, val_loss: 21.1233, val_MinusLogProbMetric: 21.1233

Epoch 69: val_loss improved from 21.30111 to 21.12332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 21.0841 - MinusLogProbMetric: 21.0841 - val_loss: 21.1233 - val_MinusLogProbMetric: 21.1233 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 70/1000
2023-10-09 18:31:38.768 
Epoch 70/1000 
	 loss: 21.0119, MinusLogProbMetric: 21.0119, val_loss: 21.0154, val_MinusLogProbMetric: 21.0154

Epoch 70: val_loss improved from 21.12332 to 21.01544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 21.0119 - MinusLogProbMetric: 21.0119 - val_loss: 21.0154 - val_MinusLogProbMetric: 21.0154 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 71/1000
2023-10-09 18:32:43.363 
Epoch 71/1000 
	 loss: 21.0190, MinusLogProbMetric: 21.0190, val_loss: 21.0495, val_MinusLogProbMetric: 21.0495

Epoch 71: val_loss did not improve from 21.01544
196/196 - 64s - loss: 21.0190 - MinusLogProbMetric: 21.0190 - val_loss: 21.0495 - val_MinusLogProbMetric: 21.0495 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 72/1000
2023-10-09 18:33:46.938 
Epoch 72/1000 
	 loss: 21.1905, MinusLogProbMetric: 21.1905, val_loss: 21.3355, val_MinusLogProbMetric: 21.3355

Epoch 72: val_loss did not improve from 21.01544
196/196 - 64s - loss: 21.1905 - MinusLogProbMetric: 21.1905 - val_loss: 21.3355 - val_MinusLogProbMetric: 21.3355 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 73/1000
2023-10-09 18:34:49.111 
Epoch 73/1000 
	 loss: 20.8657, MinusLogProbMetric: 20.8657, val_loss: 21.4227, val_MinusLogProbMetric: 21.4227

Epoch 73: val_loss did not improve from 21.01544
196/196 - 62s - loss: 20.8657 - MinusLogProbMetric: 20.8657 - val_loss: 21.4227 - val_MinusLogProbMetric: 21.4227 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 74/1000
2023-10-09 18:35:52.026 
Epoch 74/1000 
	 loss: 20.8100, MinusLogProbMetric: 20.8100, val_loss: 21.1921, val_MinusLogProbMetric: 21.1921

Epoch 74: val_loss did not improve from 21.01544
196/196 - 63s - loss: 20.8100 - MinusLogProbMetric: 20.8100 - val_loss: 21.1921 - val_MinusLogProbMetric: 21.1921 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 75/1000
2023-10-09 18:36:56.769 
Epoch 75/1000 
	 loss: 20.7202, MinusLogProbMetric: 20.7202, val_loss: 20.9049, val_MinusLogProbMetric: 20.9049

Epoch 75: val_loss improved from 21.01544 to 20.90494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 66s - loss: 20.7202 - MinusLogProbMetric: 20.7202 - val_loss: 20.9049 - val_MinusLogProbMetric: 20.9049 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 76/1000
2023-10-09 18:37:58.501 
Epoch 76/1000 
	 loss: 20.7112, MinusLogProbMetric: 20.7112, val_loss: 20.7157, val_MinusLogProbMetric: 20.7157

Epoch 76: val_loss improved from 20.90494 to 20.71570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 20.7112 - MinusLogProbMetric: 20.7112 - val_loss: 20.7157 - val_MinusLogProbMetric: 20.7157 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 77/1000
2023-10-09 18:39:03.110 
Epoch 77/1000 
	 loss: 20.5836, MinusLogProbMetric: 20.5836, val_loss: 20.9906, val_MinusLogProbMetric: 20.9906

Epoch 77: val_loss did not improve from 20.71570
196/196 - 64s - loss: 20.5836 - MinusLogProbMetric: 20.5836 - val_loss: 20.9906 - val_MinusLogProbMetric: 20.9906 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 78/1000
2023-10-09 18:40:06.391 
Epoch 78/1000 
	 loss: 20.5852, MinusLogProbMetric: 20.5852, val_loss: 20.6660, val_MinusLogProbMetric: 20.6660

Epoch 78: val_loss improved from 20.71570 to 20.66598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 20.5852 - MinusLogProbMetric: 20.5852 - val_loss: 20.6660 - val_MinusLogProbMetric: 20.6660 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 79/1000
2023-10-09 18:41:08.849 
Epoch 79/1000 
	 loss: 20.5355, MinusLogProbMetric: 20.5355, val_loss: 20.6423, val_MinusLogProbMetric: 20.6423

Epoch 79: val_loss improved from 20.66598 to 20.64230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 20.5355 - MinusLogProbMetric: 20.5355 - val_loss: 20.6423 - val_MinusLogProbMetric: 20.6423 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 80/1000
2023-10-09 18:42:11.806 
Epoch 80/1000 
	 loss: 20.4158, MinusLogProbMetric: 20.4158, val_loss: 20.6979, val_MinusLogProbMetric: 20.6979

Epoch 80: val_loss did not improve from 20.64230
196/196 - 62s - loss: 20.4158 - MinusLogProbMetric: 20.4158 - val_loss: 20.6979 - val_MinusLogProbMetric: 20.6979 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 81/1000
2023-10-09 18:43:12.161 
Epoch 81/1000 
	 loss: 20.4711, MinusLogProbMetric: 20.4711, val_loss: 20.3256, val_MinusLogProbMetric: 20.3256

Epoch 81: val_loss improved from 20.64230 to 20.32564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 20.4711 - MinusLogProbMetric: 20.4711 - val_loss: 20.3256 - val_MinusLogProbMetric: 20.3256 - lr: 3.3333e-04 - 61s/epoch - 314ms/step
Epoch 82/1000
2023-10-09 18:44:18.014 
Epoch 82/1000 
	 loss: 20.3937, MinusLogProbMetric: 20.3937, val_loss: 20.9000, val_MinusLogProbMetric: 20.9000

Epoch 82: val_loss did not improve from 20.32564
196/196 - 65s - loss: 20.3937 - MinusLogProbMetric: 20.3937 - val_loss: 20.9000 - val_MinusLogProbMetric: 20.9000 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 83/1000
2023-10-09 18:45:22.190 
Epoch 83/1000 
	 loss: 20.3164, MinusLogProbMetric: 20.3164, val_loss: 20.6641, val_MinusLogProbMetric: 20.6641

Epoch 83: val_loss did not improve from 20.32564
196/196 - 64s - loss: 20.3164 - MinusLogProbMetric: 20.3164 - val_loss: 20.6641 - val_MinusLogProbMetric: 20.6641 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 84/1000
2023-10-09 18:46:25.088 
Epoch 84/1000 
	 loss: 20.2734, MinusLogProbMetric: 20.2734, val_loss: 20.4877, val_MinusLogProbMetric: 20.4877

Epoch 84: val_loss did not improve from 20.32564
196/196 - 63s - loss: 20.2734 - MinusLogProbMetric: 20.2734 - val_loss: 20.4877 - val_MinusLogProbMetric: 20.4877 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 85/1000
2023-10-09 18:47:28.889 
Epoch 85/1000 
	 loss: 20.3109, MinusLogProbMetric: 20.3109, val_loss: 20.6061, val_MinusLogProbMetric: 20.6061

Epoch 85: val_loss did not improve from 20.32564
196/196 - 64s - loss: 20.3109 - MinusLogProbMetric: 20.3109 - val_loss: 20.6061 - val_MinusLogProbMetric: 20.6061 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 86/1000
2023-10-09 18:48:30.232 
Epoch 86/1000 
	 loss: 20.2210, MinusLogProbMetric: 20.2210, val_loss: 20.4926, val_MinusLogProbMetric: 20.4926

Epoch 86: val_loss did not improve from 20.32564
196/196 - 61s - loss: 20.2210 - MinusLogProbMetric: 20.2210 - val_loss: 20.4926 - val_MinusLogProbMetric: 20.4926 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 87/1000
2023-10-09 18:49:32.254 
Epoch 87/1000 
	 loss: 20.1576, MinusLogProbMetric: 20.1576, val_loss: 20.4039, val_MinusLogProbMetric: 20.4039

Epoch 87: val_loss did not improve from 20.32564
196/196 - 62s - loss: 20.1576 - MinusLogProbMetric: 20.1576 - val_loss: 20.4039 - val_MinusLogProbMetric: 20.4039 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 88/1000
2023-10-09 18:50:36.262 
Epoch 88/1000 
	 loss: 20.0964, MinusLogProbMetric: 20.0964, val_loss: 20.1165, val_MinusLogProbMetric: 20.1165

Epoch 88: val_loss improved from 20.32564 to 20.11655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 65s - loss: 20.0964 - MinusLogProbMetric: 20.0964 - val_loss: 20.1165 - val_MinusLogProbMetric: 20.1165 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 89/1000
2023-10-09 18:51:41.862 
Epoch 89/1000 
	 loss: 20.0727, MinusLogProbMetric: 20.0727, val_loss: 20.1998, val_MinusLogProbMetric: 20.1998

Epoch 89: val_loss did not improve from 20.11655
196/196 - 64s - loss: 20.0727 - MinusLogProbMetric: 20.0727 - val_loss: 20.1998 - val_MinusLogProbMetric: 20.1998 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 90/1000
2023-10-09 18:52:42.283 
Epoch 90/1000 
	 loss: 20.0715, MinusLogProbMetric: 20.0715, val_loss: 20.4500, val_MinusLogProbMetric: 20.4500

Epoch 90: val_loss did not improve from 20.11655
196/196 - 60s - loss: 20.0715 - MinusLogProbMetric: 20.0715 - val_loss: 20.4500 - val_MinusLogProbMetric: 20.4500 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 91/1000
2023-10-09 18:53:43.825 
Epoch 91/1000 
	 loss: 20.0490, MinusLogProbMetric: 20.0490, val_loss: 20.1805, val_MinusLogProbMetric: 20.1805

Epoch 91: val_loss did not improve from 20.11655
196/196 - 62s - loss: 20.0490 - MinusLogProbMetric: 20.0490 - val_loss: 20.1805 - val_MinusLogProbMetric: 20.1805 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 92/1000
2023-10-09 18:54:50.499 
Epoch 92/1000 
	 loss: 19.9793, MinusLogProbMetric: 19.9793, val_loss: 19.9160, val_MinusLogProbMetric: 19.9160

Epoch 92: val_loss improved from 20.11655 to 19.91599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 68s - loss: 19.9793 - MinusLogProbMetric: 19.9793 - val_loss: 19.9160 - val_MinusLogProbMetric: 19.9160 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 93/1000
2023-10-09 18:55:55.408 
Epoch 93/1000 
	 loss: 19.9251, MinusLogProbMetric: 19.9251, val_loss: 20.1089, val_MinusLogProbMetric: 20.1089

Epoch 93: val_loss did not improve from 19.91599
196/196 - 64s - loss: 19.9251 - MinusLogProbMetric: 19.9251 - val_loss: 20.1089 - val_MinusLogProbMetric: 20.1089 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 94/1000
2023-10-09 18:56:56.947 
Epoch 94/1000 
	 loss: 19.9204, MinusLogProbMetric: 19.9204, val_loss: 20.0438, val_MinusLogProbMetric: 20.0438

Epoch 94: val_loss did not improve from 19.91599
196/196 - 62s - loss: 19.9204 - MinusLogProbMetric: 19.9204 - val_loss: 20.0438 - val_MinusLogProbMetric: 20.0438 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 95/1000
2023-10-09 18:58:01.710 
Epoch 95/1000 
	 loss: 19.8547, MinusLogProbMetric: 19.8547, val_loss: 20.0512, val_MinusLogProbMetric: 20.0512

Epoch 95: val_loss did not improve from 19.91599
196/196 - 65s - loss: 19.8547 - MinusLogProbMetric: 19.8547 - val_loss: 20.0512 - val_MinusLogProbMetric: 20.0512 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 96/1000
2023-10-09 18:59:05.359 
Epoch 96/1000 
	 loss: 19.7998, MinusLogProbMetric: 19.7998, val_loss: 19.8569, val_MinusLogProbMetric: 19.8569

Epoch 96: val_loss improved from 19.91599 to 19.85689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 65s - loss: 19.7998 - MinusLogProbMetric: 19.7998 - val_loss: 19.8569 - val_MinusLogProbMetric: 19.8569 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 97/1000
2023-10-09 19:00:11.458 
Epoch 97/1000 
	 loss: 19.7653, MinusLogProbMetric: 19.7653, val_loss: 20.0977, val_MinusLogProbMetric: 20.0977

Epoch 97: val_loss did not improve from 19.85689
196/196 - 65s - loss: 19.7653 - MinusLogProbMetric: 19.7653 - val_loss: 20.0977 - val_MinusLogProbMetric: 20.0977 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 98/1000
2023-10-09 19:01:12.972 
Epoch 98/1000 
	 loss: 19.7222, MinusLogProbMetric: 19.7222, val_loss: 19.7236, val_MinusLogProbMetric: 19.7236

Epoch 98: val_loss improved from 19.85689 to 19.72365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 19.7222 - MinusLogProbMetric: 19.7222 - val_loss: 19.7236 - val_MinusLogProbMetric: 19.7236 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 99/1000
2023-10-09 19:02:15.484 
Epoch 99/1000 
	 loss: 19.7347, MinusLogProbMetric: 19.7347, val_loss: 19.6105, val_MinusLogProbMetric: 19.6105

Epoch 99: val_loss improved from 19.72365 to 19.61047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 19.7347 - MinusLogProbMetric: 19.7347 - val_loss: 19.6105 - val_MinusLogProbMetric: 19.6105 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 100/1000
2023-10-09 19:03:16.879 
Epoch 100/1000 
	 loss: 19.6998, MinusLogProbMetric: 19.6998, val_loss: 19.9648, val_MinusLogProbMetric: 19.9648

Epoch 100: val_loss did not improve from 19.61047
196/196 - 60s - loss: 19.6998 - MinusLogProbMetric: 19.6998 - val_loss: 19.9648 - val_MinusLogProbMetric: 19.9648 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 101/1000
2023-10-09 19:04:20.741 
Epoch 101/1000 
	 loss: 19.6621, MinusLogProbMetric: 19.6621, val_loss: 20.1166, val_MinusLogProbMetric: 20.1166

Epoch 101: val_loss did not improve from 19.61047
196/196 - 64s - loss: 19.6621 - MinusLogProbMetric: 19.6621 - val_loss: 20.1166 - val_MinusLogProbMetric: 20.1166 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 102/1000
2023-10-09 19:05:22.696 
Epoch 102/1000 
	 loss: 19.6398, MinusLogProbMetric: 19.6398, val_loss: 19.9079, val_MinusLogProbMetric: 19.9079

Epoch 102: val_loss did not improve from 19.61047
196/196 - 62s - loss: 19.6398 - MinusLogProbMetric: 19.6398 - val_loss: 19.9079 - val_MinusLogProbMetric: 19.9079 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 103/1000
2023-10-09 19:06:26.706 
Epoch 103/1000 
	 loss: 19.5799, MinusLogProbMetric: 19.5799, val_loss: 19.9006, val_MinusLogProbMetric: 19.9006

Epoch 103: val_loss did not improve from 19.61047
196/196 - 64s - loss: 19.5799 - MinusLogProbMetric: 19.5799 - val_loss: 19.9006 - val_MinusLogProbMetric: 19.9006 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 104/1000
2023-10-09 19:07:32.138 
Epoch 104/1000 
	 loss: 19.5712, MinusLogProbMetric: 19.5712, val_loss: 19.9392, val_MinusLogProbMetric: 19.9392

Epoch 104: val_loss did not improve from 19.61047
196/196 - 65s - loss: 19.5712 - MinusLogProbMetric: 19.5712 - val_loss: 19.9392 - val_MinusLogProbMetric: 19.9392 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 105/1000
2023-10-09 19:08:33.634 
Epoch 105/1000 
	 loss: 19.5765, MinusLogProbMetric: 19.5765, val_loss: 19.8353, val_MinusLogProbMetric: 19.8353

Epoch 105: val_loss did not improve from 19.61047
196/196 - 61s - loss: 19.5765 - MinusLogProbMetric: 19.5765 - val_loss: 19.8353 - val_MinusLogProbMetric: 19.8353 - lr: 3.3333e-04 - 61s/epoch - 314ms/step
Epoch 106/1000
2023-10-09 19:09:34.641 
Epoch 106/1000 
	 loss: 19.5445, MinusLogProbMetric: 19.5445, val_loss: 20.1656, val_MinusLogProbMetric: 20.1656

Epoch 106: val_loss did not improve from 19.61047
196/196 - 61s - loss: 19.5445 - MinusLogProbMetric: 19.5445 - val_loss: 20.1656 - val_MinusLogProbMetric: 20.1656 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 107/1000
2023-10-09 19:10:36.171 
Epoch 107/1000 
	 loss: 19.5048, MinusLogProbMetric: 19.5048, val_loss: 19.6902, val_MinusLogProbMetric: 19.6902

Epoch 107: val_loss did not improve from 19.61047
196/196 - 62s - loss: 19.5048 - MinusLogProbMetric: 19.5048 - val_loss: 19.6902 - val_MinusLogProbMetric: 19.6902 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 108/1000
2023-10-09 19:11:37.028 
Epoch 108/1000 
	 loss: 19.4434, MinusLogProbMetric: 19.4434, val_loss: 19.9798, val_MinusLogProbMetric: 19.9798

Epoch 108: val_loss did not improve from 19.61047
196/196 - 61s - loss: 19.4434 - MinusLogProbMetric: 19.4434 - val_loss: 19.9798 - val_MinusLogProbMetric: 19.9798 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 109/1000
2023-10-09 19:12:40.456 
Epoch 109/1000 
	 loss: 19.4922, MinusLogProbMetric: 19.4922, val_loss: 19.9237, val_MinusLogProbMetric: 19.9237

Epoch 109: val_loss did not improve from 19.61047
196/196 - 63s - loss: 19.4922 - MinusLogProbMetric: 19.4922 - val_loss: 19.9237 - val_MinusLogProbMetric: 19.9237 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 110/1000
2023-10-09 19:13:45.345 
Epoch 110/1000 
	 loss: 19.4178, MinusLogProbMetric: 19.4178, val_loss: 19.7975, val_MinusLogProbMetric: 19.7975

Epoch 110: val_loss did not improve from 19.61047
196/196 - 65s - loss: 19.4178 - MinusLogProbMetric: 19.4178 - val_loss: 19.7975 - val_MinusLogProbMetric: 19.7975 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 111/1000
2023-10-09 19:14:46.946 
Epoch 111/1000 
	 loss: 19.4369, MinusLogProbMetric: 19.4369, val_loss: 19.5356, val_MinusLogProbMetric: 19.5356

Epoch 111: val_loss improved from 19.61047 to 19.53555, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 19.4369 - MinusLogProbMetric: 19.4369 - val_loss: 19.5356 - val_MinusLogProbMetric: 19.5356 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 112/1000
2023-10-09 19:15:49.660 
Epoch 112/1000 
	 loss: 19.3811, MinusLogProbMetric: 19.3811, val_loss: 19.7007, val_MinusLogProbMetric: 19.7007

Epoch 112: val_loss did not improve from 19.53555
196/196 - 62s - loss: 19.3811 - MinusLogProbMetric: 19.3811 - val_loss: 19.7007 - val_MinusLogProbMetric: 19.7007 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 113/1000
2023-10-09 19:16:51.142 
Epoch 113/1000 
	 loss: 19.3094, MinusLogProbMetric: 19.3094, val_loss: 19.4806, val_MinusLogProbMetric: 19.4806

Epoch 113: val_loss improved from 19.53555 to 19.48058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 19.3094 - MinusLogProbMetric: 19.3094 - val_loss: 19.4806 - val_MinusLogProbMetric: 19.4806 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 114/1000
2023-10-09 19:17:55.262 
Epoch 114/1000 
	 loss: 19.3082, MinusLogProbMetric: 19.3082, val_loss: 19.5424, val_MinusLogProbMetric: 19.5424

Epoch 114: val_loss did not improve from 19.48058
196/196 - 63s - loss: 19.3082 - MinusLogProbMetric: 19.3082 - val_loss: 19.5424 - val_MinusLogProbMetric: 19.5424 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 115/1000
2023-10-09 19:18:57.246 
Epoch 115/1000 
	 loss: 19.3058, MinusLogProbMetric: 19.3058, val_loss: 19.5240, val_MinusLogProbMetric: 19.5240

Epoch 115: val_loss did not improve from 19.48058
196/196 - 62s - loss: 19.3058 - MinusLogProbMetric: 19.3058 - val_loss: 19.5240 - val_MinusLogProbMetric: 19.5240 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 116/1000
2023-10-09 19:20:04.437 
Epoch 116/1000 
	 loss: 19.2415, MinusLogProbMetric: 19.2415, val_loss: 19.3224, val_MinusLogProbMetric: 19.3224

Epoch 116: val_loss improved from 19.48058 to 19.32237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 68s - loss: 19.2415 - MinusLogProbMetric: 19.2415 - val_loss: 19.3224 - val_MinusLogProbMetric: 19.3224 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 117/1000
2023-10-09 19:21:06.602 
Epoch 117/1000 
	 loss: 19.2343, MinusLogProbMetric: 19.2343, val_loss: 19.5672, val_MinusLogProbMetric: 19.5672

Epoch 117: val_loss did not improve from 19.32237
196/196 - 61s - loss: 19.2343 - MinusLogProbMetric: 19.2343 - val_loss: 19.5672 - val_MinusLogProbMetric: 19.5672 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 118/1000
2023-10-09 19:22:08.379 
Epoch 118/1000 
	 loss: 19.3021, MinusLogProbMetric: 19.3021, val_loss: 19.5178, val_MinusLogProbMetric: 19.5178

Epoch 118: val_loss did not improve from 19.32237
196/196 - 62s - loss: 19.3021 - MinusLogProbMetric: 19.3021 - val_loss: 19.5178 - val_MinusLogProbMetric: 19.5178 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 119/1000
2023-10-09 19:23:09.749 
Epoch 119/1000 
	 loss: 19.2263, MinusLogProbMetric: 19.2263, val_loss: 19.8663, val_MinusLogProbMetric: 19.8663

Epoch 119: val_loss did not improve from 19.32237
196/196 - 61s - loss: 19.2263 - MinusLogProbMetric: 19.2263 - val_loss: 19.8663 - val_MinusLogProbMetric: 19.8663 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 120/1000
2023-10-09 19:24:13.133 
Epoch 120/1000 
	 loss: 19.1728, MinusLogProbMetric: 19.1728, val_loss: 19.2897, val_MinusLogProbMetric: 19.2897

Epoch 120: val_loss improved from 19.32237 to 19.28966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 19.1728 - MinusLogProbMetric: 19.1728 - val_loss: 19.2897 - val_MinusLogProbMetric: 19.2897 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 121/1000
2023-10-09 19:25:15.714 
Epoch 121/1000 
	 loss: 19.2103, MinusLogProbMetric: 19.2103, val_loss: 19.5159, val_MinusLogProbMetric: 19.5159

Epoch 121: val_loss did not improve from 19.28966
196/196 - 62s - loss: 19.2103 - MinusLogProbMetric: 19.2103 - val_loss: 19.5159 - val_MinusLogProbMetric: 19.5159 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 122/1000
2023-10-09 19:26:18.641 
Epoch 122/1000 
	 loss: 19.2159, MinusLogProbMetric: 19.2159, val_loss: 19.1577, val_MinusLogProbMetric: 19.1577

Epoch 122: val_loss improved from 19.28966 to 19.15775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 19.2159 - MinusLogProbMetric: 19.2159 - val_loss: 19.1577 - val_MinusLogProbMetric: 19.1577 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 123/1000
2023-10-09 19:27:22.029 
Epoch 123/1000 
	 loss: 19.1135, MinusLogProbMetric: 19.1135, val_loss: 19.3483, val_MinusLogProbMetric: 19.3483

Epoch 123: val_loss did not improve from 19.15775
196/196 - 62s - loss: 19.1135 - MinusLogProbMetric: 19.1135 - val_loss: 19.3483 - val_MinusLogProbMetric: 19.3483 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 124/1000
2023-10-09 19:28:23.070 
Epoch 124/1000 
	 loss: 19.1524, MinusLogProbMetric: 19.1524, val_loss: 19.6083, val_MinusLogProbMetric: 19.6083

Epoch 124: val_loss did not improve from 19.15775
196/196 - 61s - loss: 19.1524 - MinusLogProbMetric: 19.1524 - val_loss: 19.6083 - val_MinusLogProbMetric: 19.6083 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 125/1000
2023-10-09 19:29:23.821 
Epoch 125/1000 
	 loss: 19.1225, MinusLogProbMetric: 19.1225, val_loss: 19.6085, val_MinusLogProbMetric: 19.6085

Epoch 125: val_loss did not improve from 19.15775
196/196 - 61s - loss: 19.1225 - MinusLogProbMetric: 19.1225 - val_loss: 19.6085 - val_MinusLogProbMetric: 19.6085 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 126/1000
2023-10-09 19:30:26.839 
Epoch 126/1000 
	 loss: 19.1208, MinusLogProbMetric: 19.1208, val_loss: 19.2888, val_MinusLogProbMetric: 19.2888

Epoch 126: val_loss did not improve from 19.15775
196/196 - 63s - loss: 19.1208 - MinusLogProbMetric: 19.1208 - val_loss: 19.2888 - val_MinusLogProbMetric: 19.2888 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 127/1000
2023-10-09 19:31:30.159 
Epoch 127/1000 
	 loss: 19.0534, MinusLogProbMetric: 19.0534, val_loss: 19.3603, val_MinusLogProbMetric: 19.3603

Epoch 127: val_loss did not improve from 19.15775
196/196 - 63s - loss: 19.0534 - MinusLogProbMetric: 19.0534 - val_loss: 19.3603 - val_MinusLogProbMetric: 19.3603 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 128/1000
2023-10-09 19:32:34.175 
Epoch 128/1000 
	 loss: 19.0360, MinusLogProbMetric: 19.0360, val_loss: 19.2141, val_MinusLogProbMetric: 19.2141

Epoch 128: val_loss did not improve from 19.15775
196/196 - 64s - loss: 19.0360 - MinusLogProbMetric: 19.0360 - val_loss: 19.2141 - val_MinusLogProbMetric: 19.2141 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 129/1000
2023-10-09 19:33:36.751 
Epoch 129/1000 
	 loss: 19.0766, MinusLogProbMetric: 19.0766, val_loss: 19.0436, val_MinusLogProbMetric: 19.0436

Epoch 129: val_loss improved from 19.15775 to 19.04363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 19.0766 - MinusLogProbMetric: 19.0766 - val_loss: 19.0436 - val_MinusLogProbMetric: 19.0436 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 130/1000
2023-10-09 19:34:40.798 
Epoch 130/1000 
	 loss: 19.0383, MinusLogProbMetric: 19.0383, val_loss: 19.1875, val_MinusLogProbMetric: 19.1875

Epoch 130: val_loss did not improve from 19.04363
196/196 - 63s - loss: 19.0383 - MinusLogProbMetric: 19.0383 - val_loss: 19.1875 - val_MinusLogProbMetric: 19.1875 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 131/1000
2023-10-09 19:35:41.882 
Epoch 131/1000 
	 loss: 19.0086, MinusLogProbMetric: 19.0086, val_loss: 19.3566, val_MinusLogProbMetric: 19.3566

Epoch 131: val_loss did not improve from 19.04363
196/196 - 61s - loss: 19.0086 - MinusLogProbMetric: 19.0086 - val_loss: 19.3566 - val_MinusLogProbMetric: 19.3566 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 132/1000
2023-10-09 19:36:43.140 
Epoch 132/1000 
	 loss: 18.9663, MinusLogProbMetric: 18.9663, val_loss: 18.8854, val_MinusLogProbMetric: 18.8854

Epoch 132: val_loss improved from 19.04363 to 18.88539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 18.9663 - MinusLogProbMetric: 18.9663 - val_loss: 18.8854 - val_MinusLogProbMetric: 18.8854 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 133/1000
2023-10-09 19:37:44.930 
Epoch 133/1000 
	 loss: 18.9683, MinusLogProbMetric: 18.9683, val_loss: 19.2332, val_MinusLogProbMetric: 19.2332

Epoch 133: val_loss did not improve from 18.88539
196/196 - 61s - loss: 18.9683 - MinusLogProbMetric: 18.9683 - val_loss: 19.2332 - val_MinusLogProbMetric: 19.2332 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 134/1000
2023-10-09 19:38:46.922 
Epoch 134/1000 
	 loss: 18.9498, MinusLogProbMetric: 18.9498, val_loss: 19.3492, val_MinusLogProbMetric: 19.3492

Epoch 134: val_loss did not improve from 18.88539
196/196 - 62s - loss: 18.9498 - MinusLogProbMetric: 18.9498 - val_loss: 19.3492 - val_MinusLogProbMetric: 19.3492 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 135/1000
2023-10-09 19:39:51.248 
Epoch 135/1000 
	 loss: 19.1375, MinusLogProbMetric: 19.1375, val_loss: 19.3720, val_MinusLogProbMetric: 19.3720

Epoch 135: val_loss did not improve from 18.88539
196/196 - 64s - loss: 19.1375 - MinusLogProbMetric: 19.1375 - val_loss: 19.3720 - val_MinusLogProbMetric: 19.3720 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 136/1000
2023-10-09 19:40:53.398 
Epoch 136/1000 
	 loss: 18.9798, MinusLogProbMetric: 18.9798, val_loss: 19.2291, val_MinusLogProbMetric: 19.2291

Epoch 136: val_loss did not improve from 18.88539
196/196 - 62s - loss: 18.9798 - MinusLogProbMetric: 18.9798 - val_loss: 19.2291 - val_MinusLogProbMetric: 19.2291 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 137/1000
2023-10-09 19:41:53.897 
Epoch 137/1000 
	 loss: 18.9318, MinusLogProbMetric: 18.9318, val_loss: 19.0077, val_MinusLogProbMetric: 19.0077

Epoch 137: val_loss did not improve from 18.88539
196/196 - 60s - loss: 18.9318 - MinusLogProbMetric: 18.9318 - val_loss: 19.0077 - val_MinusLogProbMetric: 19.0077 - lr: 3.3333e-04 - 60s/epoch - 309ms/step
Epoch 138/1000
2023-10-09 19:42:54.683 
Epoch 138/1000 
	 loss: 18.9232, MinusLogProbMetric: 18.9232, val_loss: 19.2021, val_MinusLogProbMetric: 19.2021

Epoch 138: val_loss did not improve from 18.88539
196/196 - 61s - loss: 18.9232 - MinusLogProbMetric: 18.9232 - val_loss: 19.2021 - val_MinusLogProbMetric: 19.2021 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 139/1000
2023-10-09 19:43:56.150 
Epoch 139/1000 
	 loss: 18.9147, MinusLogProbMetric: 18.9147, val_loss: 19.1611, val_MinusLogProbMetric: 19.1611

Epoch 139: val_loss did not improve from 18.88539
196/196 - 61s - loss: 18.9147 - MinusLogProbMetric: 18.9147 - val_loss: 19.1611 - val_MinusLogProbMetric: 19.1611 - lr: 3.3333e-04 - 61s/epoch - 314ms/step
Epoch 140/1000
2023-10-09 19:44:57.990 
Epoch 140/1000 
	 loss: 18.9033, MinusLogProbMetric: 18.9033, val_loss: 19.0659, val_MinusLogProbMetric: 19.0659

Epoch 140: val_loss did not improve from 18.88539
196/196 - 62s - loss: 18.9033 - MinusLogProbMetric: 18.9033 - val_loss: 19.0659 - val_MinusLogProbMetric: 19.0659 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 141/1000
2023-10-09 19:46:01.750 
Epoch 141/1000 
	 loss: 18.8801, MinusLogProbMetric: 18.8801, val_loss: 19.0929, val_MinusLogProbMetric: 19.0929

Epoch 141: val_loss did not improve from 18.88539
196/196 - 64s - loss: 18.8801 - MinusLogProbMetric: 18.8801 - val_loss: 19.0929 - val_MinusLogProbMetric: 19.0929 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 142/1000
2023-10-09 19:47:08.208 
Epoch 142/1000 
	 loss: 18.8406, MinusLogProbMetric: 18.8406, val_loss: 19.2875, val_MinusLogProbMetric: 19.2875

Epoch 142: val_loss did not improve from 18.88539
196/196 - 66s - loss: 18.8406 - MinusLogProbMetric: 18.8406 - val_loss: 19.2875 - val_MinusLogProbMetric: 19.2875 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 143/1000
2023-10-09 19:48:08.780 
Epoch 143/1000 
	 loss: 18.8612, MinusLogProbMetric: 18.8612, val_loss: 19.0905, val_MinusLogProbMetric: 19.0905

Epoch 143: val_loss did not improve from 18.88539
196/196 - 61s - loss: 18.8612 - MinusLogProbMetric: 18.8612 - val_loss: 19.0905 - val_MinusLogProbMetric: 19.0905 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 144/1000
2023-10-09 19:49:11.893 
Epoch 144/1000 
	 loss: 18.8953, MinusLogProbMetric: 18.8953, val_loss: 19.2308, val_MinusLogProbMetric: 19.2308

Epoch 144: val_loss did not improve from 18.88539
196/196 - 63s - loss: 18.8953 - MinusLogProbMetric: 18.8953 - val_loss: 19.2308 - val_MinusLogProbMetric: 19.2308 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 145/1000
2023-10-09 19:50:15.950 
Epoch 145/1000 
	 loss: 18.8062, MinusLogProbMetric: 18.8062, val_loss: 19.0978, val_MinusLogProbMetric: 19.0978

Epoch 145: val_loss did not improve from 18.88539
196/196 - 64s - loss: 18.8062 - MinusLogProbMetric: 18.8062 - val_loss: 19.0978 - val_MinusLogProbMetric: 19.0978 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 146/1000
2023-10-09 19:51:18.483 
Epoch 146/1000 
	 loss: 18.7991, MinusLogProbMetric: 18.7991, val_loss: 19.0254, val_MinusLogProbMetric: 19.0254

Epoch 146: val_loss did not improve from 18.88539
196/196 - 63s - loss: 18.7991 - MinusLogProbMetric: 18.7991 - val_loss: 19.0254 - val_MinusLogProbMetric: 19.0254 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 147/1000
2023-10-09 19:52:21.607 
Epoch 147/1000 
	 loss: 18.7834, MinusLogProbMetric: 18.7834, val_loss: 20.2320, val_MinusLogProbMetric: 20.2320

Epoch 147: val_loss did not improve from 18.88539
196/196 - 63s - loss: 18.7834 - MinusLogProbMetric: 18.7834 - val_loss: 20.2320 - val_MinusLogProbMetric: 20.2320 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 148/1000
2023-10-09 19:53:23.399 
Epoch 148/1000 
	 loss: 19.2391, MinusLogProbMetric: 19.2391, val_loss: 19.0984, val_MinusLogProbMetric: 19.0984

Epoch 148: val_loss did not improve from 18.88539
196/196 - 62s - loss: 19.2391 - MinusLogProbMetric: 19.2391 - val_loss: 19.0984 - val_MinusLogProbMetric: 19.0984 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 149/1000
2023-10-09 19:54:25.076 
Epoch 149/1000 
	 loss: 18.7811, MinusLogProbMetric: 18.7811, val_loss: 18.8117, val_MinusLogProbMetric: 18.8117

Epoch 149: val_loss improved from 18.88539 to 18.81169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 18.7811 - MinusLogProbMetric: 18.7811 - val_loss: 18.8117 - val_MinusLogProbMetric: 18.8117 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 150/1000
2023-10-09 19:55:28.594 
Epoch 150/1000 
	 loss: 18.7301, MinusLogProbMetric: 18.7301, val_loss: 18.8523, val_MinusLogProbMetric: 18.8523

Epoch 150: val_loss did not improve from 18.81169
196/196 - 63s - loss: 18.7301 - MinusLogProbMetric: 18.7301 - val_loss: 18.8523 - val_MinusLogProbMetric: 18.8523 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 151/1000
2023-10-09 19:56:30.979 
Epoch 151/1000 
	 loss: 18.7385, MinusLogProbMetric: 18.7385, val_loss: 18.8996, val_MinusLogProbMetric: 18.8996

Epoch 151: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.7385 - MinusLogProbMetric: 18.7385 - val_loss: 18.8996 - val_MinusLogProbMetric: 18.8996 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 152/1000
2023-10-09 19:57:31.853 
Epoch 152/1000 
	 loss: 18.7065, MinusLogProbMetric: 18.7065, val_loss: 18.9290, val_MinusLogProbMetric: 18.9290

Epoch 152: val_loss did not improve from 18.81169
196/196 - 61s - loss: 18.7065 - MinusLogProbMetric: 18.7065 - val_loss: 18.9290 - val_MinusLogProbMetric: 18.9290 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 153/1000
2023-10-09 19:58:33.973 
Epoch 153/1000 
	 loss: 18.7312, MinusLogProbMetric: 18.7312, val_loss: 18.8352, val_MinusLogProbMetric: 18.8352

Epoch 153: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.7312 - MinusLogProbMetric: 18.7312 - val_loss: 18.8352 - val_MinusLogProbMetric: 18.8352 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 154/1000
2023-10-09 19:59:36.372 
Epoch 154/1000 
	 loss: 18.7652, MinusLogProbMetric: 18.7652, val_loss: 19.0104, val_MinusLogProbMetric: 19.0104

Epoch 154: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.7652 - MinusLogProbMetric: 18.7652 - val_loss: 19.0104 - val_MinusLogProbMetric: 19.0104 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 155/1000
2023-10-09 20:00:37.902 
Epoch 155/1000 
	 loss: 18.7097, MinusLogProbMetric: 18.7097, val_loss: 19.1142, val_MinusLogProbMetric: 19.1142

Epoch 155: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.7097 - MinusLogProbMetric: 18.7097 - val_loss: 19.1142 - val_MinusLogProbMetric: 19.1142 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 156/1000
2023-10-09 20:01:39.215 
Epoch 156/1000 
	 loss: 18.6942, MinusLogProbMetric: 18.6942, val_loss: 18.9299, val_MinusLogProbMetric: 18.9299

Epoch 156: val_loss did not improve from 18.81169
196/196 - 61s - loss: 18.6942 - MinusLogProbMetric: 18.6942 - val_loss: 18.9299 - val_MinusLogProbMetric: 18.9299 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 157/1000
2023-10-09 20:02:42.374 
Epoch 157/1000 
	 loss: 18.7146, MinusLogProbMetric: 18.7146, val_loss: 19.0947, val_MinusLogProbMetric: 19.0947

Epoch 157: val_loss did not improve from 18.81169
196/196 - 63s - loss: 18.7146 - MinusLogProbMetric: 18.7146 - val_loss: 19.0947 - val_MinusLogProbMetric: 19.0947 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 158/1000
2023-10-09 20:03:43.251 
Epoch 158/1000 
	 loss: 18.6403, MinusLogProbMetric: 18.6403, val_loss: 19.0236, val_MinusLogProbMetric: 19.0236

Epoch 158: val_loss did not improve from 18.81169
196/196 - 61s - loss: 18.6403 - MinusLogProbMetric: 18.6403 - val_loss: 19.0236 - val_MinusLogProbMetric: 19.0236 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 159/1000
2023-10-09 20:04:44.065 
Epoch 159/1000 
	 loss: 18.6603, MinusLogProbMetric: 18.6603, val_loss: 19.0192, val_MinusLogProbMetric: 19.0192

Epoch 159: val_loss did not improve from 18.81169
196/196 - 61s - loss: 18.6603 - MinusLogProbMetric: 18.6603 - val_loss: 19.0192 - val_MinusLogProbMetric: 19.0192 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 160/1000
2023-10-09 20:05:47.186 
Epoch 160/1000 
	 loss: 18.6352, MinusLogProbMetric: 18.6352, val_loss: 18.9537, val_MinusLogProbMetric: 18.9537

Epoch 160: val_loss did not improve from 18.81169
196/196 - 63s - loss: 18.6352 - MinusLogProbMetric: 18.6352 - val_loss: 18.9537 - val_MinusLogProbMetric: 18.9537 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 161/1000
2023-10-09 20:06:49.471 
Epoch 161/1000 
	 loss: 18.6710, MinusLogProbMetric: 18.6710, val_loss: 19.0096, val_MinusLogProbMetric: 19.0096

Epoch 161: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.6710 - MinusLogProbMetric: 18.6710 - val_loss: 19.0096 - val_MinusLogProbMetric: 19.0096 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 162/1000
2023-10-09 20:07:51.138 
Epoch 162/1000 
	 loss: 18.6457, MinusLogProbMetric: 18.6457, val_loss: 18.9705, val_MinusLogProbMetric: 18.9705

Epoch 162: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.6457 - MinusLogProbMetric: 18.6457 - val_loss: 18.9705 - val_MinusLogProbMetric: 18.9705 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 163/1000
2023-10-09 20:08:53.011 
Epoch 163/1000 
	 loss: 18.6013, MinusLogProbMetric: 18.6013, val_loss: 19.0119, val_MinusLogProbMetric: 19.0119

Epoch 163: val_loss did not improve from 18.81169
196/196 - 62s - loss: 18.6013 - MinusLogProbMetric: 18.6013 - val_loss: 19.0119 - val_MinusLogProbMetric: 19.0119 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 164/1000
2023-10-09 20:09:54.520 
Epoch 164/1000 
	 loss: 18.6170, MinusLogProbMetric: 18.6170, val_loss: 18.7558, val_MinusLogProbMetric: 18.7558

Epoch 164: val_loss improved from 18.81169 to 18.75576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 18.6170 - MinusLogProbMetric: 18.6170 - val_loss: 18.7558 - val_MinusLogProbMetric: 18.7558 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 165/1000
2023-10-09 20:10:56.290 
Epoch 165/1000 
	 loss: 18.6401, MinusLogProbMetric: 18.6401, val_loss: 19.2894, val_MinusLogProbMetric: 19.2894

Epoch 165: val_loss did not improve from 18.75576
196/196 - 61s - loss: 18.6401 - MinusLogProbMetric: 18.6401 - val_loss: 19.2894 - val_MinusLogProbMetric: 19.2894 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 166/1000
2023-10-09 20:11:58.965 
Epoch 166/1000 
	 loss: 18.6025, MinusLogProbMetric: 18.6025, val_loss: 18.6639, val_MinusLogProbMetric: 18.6639

Epoch 166: val_loss improved from 18.75576 to 18.66386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 18.6025 - MinusLogProbMetric: 18.6025 - val_loss: 18.6639 - val_MinusLogProbMetric: 18.6639 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 167/1000
2023-10-09 20:13:00.700 
Epoch 167/1000 
	 loss: 18.6315, MinusLogProbMetric: 18.6315, val_loss: 19.6130, val_MinusLogProbMetric: 19.6130

Epoch 167: val_loss did not improve from 18.66386
196/196 - 61s - loss: 18.6315 - MinusLogProbMetric: 18.6315 - val_loss: 19.6130 - val_MinusLogProbMetric: 19.6130 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 168/1000
2023-10-09 20:14:03.793 
Epoch 168/1000 
	 loss: 18.6068, MinusLogProbMetric: 18.6068, val_loss: 18.5996, val_MinusLogProbMetric: 18.5996

Epoch 168: val_loss improved from 18.66386 to 18.59965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 18.6068 - MinusLogProbMetric: 18.6068 - val_loss: 18.5996 - val_MinusLogProbMetric: 18.5996 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 169/1000
2023-10-09 20:15:07.531 
Epoch 169/1000 
	 loss: 18.5819, MinusLogProbMetric: 18.5819, val_loss: 18.7186, val_MinusLogProbMetric: 18.7186

Epoch 169: val_loss did not improve from 18.59965
196/196 - 63s - loss: 18.5819 - MinusLogProbMetric: 18.5819 - val_loss: 18.7186 - val_MinusLogProbMetric: 18.7186 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 170/1000
2023-10-09 20:16:09.173 
Epoch 170/1000 
	 loss: 18.5051, MinusLogProbMetric: 18.5051, val_loss: 18.8336, val_MinusLogProbMetric: 18.8336

Epoch 170: val_loss did not improve from 18.59965
196/196 - 62s - loss: 18.5051 - MinusLogProbMetric: 18.5051 - val_loss: 18.8336 - val_MinusLogProbMetric: 18.8336 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 171/1000
2023-10-09 20:17:11.403 
Epoch 171/1000 
	 loss: 18.5229, MinusLogProbMetric: 18.5229, val_loss: 18.8062, val_MinusLogProbMetric: 18.8062

Epoch 171: val_loss did not improve from 18.59965
196/196 - 62s - loss: 18.5229 - MinusLogProbMetric: 18.5229 - val_loss: 18.8062 - val_MinusLogProbMetric: 18.8062 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 172/1000
2023-10-09 20:18:15.595 
Epoch 172/1000 
	 loss: 18.6196, MinusLogProbMetric: 18.6196, val_loss: 20.6124, val_MinusLogProbMetric: 20.6124

Epoch 172: val_loss did not improve from 18.59965
196/196 - 64s - loss: 18.6196 - MinusLogProbMetric: 18.6196 - val_loss: 20.6124 - val_MinusLogProbMetric: 20.6124 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 173/1000
2023-10-09 20:19:20.862 
Epoch 173/1000 
	 loss: 18.5942, MinusLogProbMetric: 18.5942, val_loss: 18.6070, val_MinusLogProbMetric: 18.6070

Epoch 173: val_loss did not improve from 18.59965
196/196 - 65s - loss: 18.5942 - MinusLogProbMetric: 18.5942 - val_loss: 18.6070 - val_MinusLogProbMetric: 18.6070 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 174/1000
2023-10-09 20:20:22.825 
Epoch 174/1000 
	 loss: 18.4993, MinusLogProbMetric: 18.4993, val_loss: 19.1766, val_MinusLogProbMetric: 19.1766

Epoch 174: val_loss did not improve from 18.59965
196/196 - 62s - loss: 18.4993 - MinusLogProbMetric: 18.4993 - val_loss: 19.1766 - val_MinusLogProbMetric: 19.1766 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 175/1000
2023-10-09 20:21:25.537 
Epoch 175/1000 
	 loss: 18.4779, MinusLogProbMetric: 18.4779, val_loss: 18.6205, val_MinusLogProbMetric: 18.6205

Epoch 175: val_loss did not improve from 18.59965
196/196 - 63s - loss: 18.4779 - MinusLogProbMetric: 18.4779 - val_loss: 18.6205 - val_MinusLogProbMetric: 18.6205 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 176/1000
2023-10-09 20:22:27.988 
Epoch 176/1000 
	 loss: 18.4771, MinusLogProbMetric: 18.4771, val_loss: 18.7253, val_MinusLogProbMetric: 18.7253

Epoch 176: val_loss did not improve from 18.59965
196/196 - 62s - loss: 18.4771 - MinusLogProbMetric: 18.4771 - val_loss: 18.7253 - val_MinusLogProbMetric: 18.7253 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 177/1000
2023-10-09 20:23:29.712 
Epoch 177/1000 
	 loss: 18.4462, MinusLogProbMetric: 18.4462, val_loss: 18.4351, val_MinusLogProbMetric: 18.4351

Epoch 177: val_loss improved from 18.59965 to 18.43505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 18.4462 - MinusLogProbMetric: 18.4462 - val_loss: 18.4351 - val_MinusLogProbMetric: 18.4351 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 178/1000
2023-10-09 20:24:33.243 
Epoch 178/1000 
	 loss: 18.4189, MinusLogProbMetric: 18.4189, val_loss: 18.5832, val_MinusLogProbMetric: 18.5832

Epoch 178: val_loss did not improve from 18.43505
196/196 - 63s - loss: 18.4189 - MinusLogProbMetric: 18.4189 - val_loss: 18.5832 - val_MinusLogProbMetric: 18.5832 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 179/1000
2023-10-09 20:25:33.573 
Epoch 179/1000 
	 loss: 18.4608, MinusLogProbMetric: 18.4608, val_loss: 18.6599, val_MinusLogProbMetric: 18.6599

Epoch 179: val_loss did not improve from 18.43505
196/196 - 60s - loss: 18.4608 - MinusLogProbMetric: 18.4608 - val_loss: 18.6599 - val_MinusLogProbMetric: 18.6599 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 180/1000
2023-10-09 20:26:36.867 
Epoch 180/1000 
	 loss: 18.4608, MinusLogProbMetric: 18.4608, val_loss: 18.7562, val_MinusLogProbMetric: 18.7562

Epoch 180: val_loss did not improve from 18.43505
196/196 - 63s - loss: 18.4608 - MinusLogProbMetric: 18.4608 - val_loss: 18.7562 - val_MinusLogProbMetric: 18.7562 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 181/1000
2023-10-09 20:27:38.140 
Epoch 181/1000 
	 loss: 18.4399, MinusLogProbMetric: 18.4399, val_loss: 18.5593, val_MinusLogProbMetric: 18.5593

Epoch 181: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.4399 - MinusLogProbMetric: 18.4399 - val_loss: 18.5593 - val_MinusLogProbMetric: 18.5593 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 182/1000
2023-10-09 20:28:39.743 
Epoch 182/1000 
	 loss: 18.3797, MinusLogProbMetric: 18.3797, val_loss: 18.6767, val_MinusLogProbMetric: 18.6767

Epoch 182: val_loss did not improve from 18.43505
196/196 - 62s - loss: 18.3797 - MinusLogProbMetric: 18.3797 - val_loss: 18.6767 - val_MinusLogProbMetric: 18.6767 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 183/1000
2023-10-09 20:29:43.076 
Epoch 183/1000 
	 loss: 18.4099, MinusLogProbMetric: 18.4099, val_loss: 18.7521, val_MinusLogProbMetric: 18.7521

Epoch 183: val_loss did not improve from 18.43505
196/196 - 63s - loss: 18.4099 - MinusLogProbMetric: 18.4099 - val_loss: 18.7521 - val_MinusLogProbMetric: 18.7521 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 184/1000
2023-10-09 20:30:44.556 
Epoch 184/1000 
	 loss: 18.4331, MinusLogProbMetric: 18.4331, val_loss: 18.6059, val_MinusLogProbMetric: 18.6059

Epoch 184: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.4331 - MinusLogProbMetric: 18.4331 - val_loss: 18.6059 - val_MinusLogProbMetric: 18.6059 - lr: 3.3333e-04 - 61s/epoch - 314ms/step
Epoch 185/1000
2023-10-09 20:31:45.651 
Epoch 185/1000 
	 loss: 18.3732, MinusLogProbMetric: 18.3732, val_loss: 18.4905, val_MinusLogProbMetric: 18.4905

Epoch 185: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.3732 - MinusLogProbMetric: 18.3732 - val_loss: 18.4905 - val_MinusLogProbMetric: 18.4905 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 186/1000
2023-10-09 20:32:46.707 
Epoch 186/1000 
	 loss: 18.4115, MinusLogProbMetric: 18.4115, val_loss: 18.5652, val_MinusLogProbMetric: 18.5652

Epoch 186: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.4115 - MinusLogProbMetric: 18.4115 - val_loss: 18.5652 - val_MinusLogProbMetric: 18.5652 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 187/1000
2023-10-09 20:33:48.903 
Epoch 187/1000 
	 loss: 18.3538, MinusLogProbMetric: 18.3538, val_loss: 18.4392, val_MinusLogProbMetric: 18.4392

Epoch 187: val_loss did not improve from 18.43505
196/196 - 62s - loss: 18.3538 - MinusLogProbMetric: 18.3538 - val_loss: 18.4392 - val_MinusLogProbMetric: 18.4392 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 188/1000
2023-10-09 20:34:49.614 
Epoch 188/1000 
	 loss: 18.4757, MinusLogProbMetric: 18.4757, val_loss: 18.8001, val_MinusLogProbMetric: 18.8001

Epoch 188: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.4757 - MinusLogProbMetric: 18.4757 - val_loss: 18.8001 - val_MinusLogProbMetric: 18.8001 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 189/1000
2023-10-09 20:35:50.965 
Epoch 189/1000 
	 loss: 18.3814, MinusLogProbMetric: 18.3814, val_loss: 18.6385, val_MinusLogProbMetric: 18.6385

Epoch 189: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.3814 - MinusLogProbMetric: 18.3814 - val_loss: 18.6385 - val_MinusLogProbMetric: 18.6385 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 190/1000
2023-10-09 20:36:54.501 
Epoch 190/1000 
	 loss: 18.3532, MinusLogProbMetric: 18.3532, val_loss: 18.6628, val_MinusLogProbMetric: 18.6628

Epoch 190: val_loss did not improve from 18.43505
196/196 - 64s - loss: 18.3532 - MinusLogProbMetric: 18.3532 - val_loss: 18.6628 - val_MinusLogProbMetric: 18.6628 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 191/1000
2023-10-09 20:37:54.680 
Epoch 191/1000 
	 loss: 18.3406, MinusLogProbMetric: 18.3406, val_loss: 18.4847, val_MinusLogProbMetric: 18.4847

Epoch 191: val_loss did not improve from 18.43505
196/196 - 60s - loss: 18.3406 - MinusLogProbMetric: 18.3406 - val_loss: 18.4847 - val_MinusLogProbMetric: 18.4847 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 192/1000
2023-10-09 20:38:58.209 
Epoch 192/1000 
	 loss: 18.3920, MinusLogProbMetric: 18.3920, val_loss: 18.8256, val_MinusLogProbMetric: 18.8256

Epoch 192: val_loss did not improve from 18.43505
196/196 - 64s - loss: 18.3920 - MinusLogProbMetric: 18.3920 - val_loss: 18.8256 - val_MinusLogProbMetric: 18.8256 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 193/1000
2023-10-09 20:39:59.930 
Epoch 193/1000 
	 loss: 18.4131, MinusLogProbMetric: 18.4131, val_loss: 18.6673, val_MinusLogProbMetric: 18.6673

Epoch 193: val_loss did not improve from 18.43505
196/196 - 62s - loss: 18.4131 - MinusLogProbMetric: 18.4131 - val_loss: 18.6673 - val_MinusLogProbMetric: 18.6673 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 194/1000
2023-10-09 20:41:04.995 
Epoch 194/1000 
	 loss: 18.3655, MinusLogProbMetric: 18.3655, val_loss: 18.6844, val_MinusLogProbMetric: 18.6844

Epoch 194: val_loss did not improve from 18.43505
196/196 - 65s - loss: 18.3655 - MinusLogProbMetric: 18.3655 - val_loss: 18.6844 - val_MinusLogProbMetric: 18.6844 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 195/1000
2023-10-09 20:42:05.146 
Epoch 195/1000 
	 loss: 18.3210, MinusLogProbMetric: 18.3210, val_loss: 18.6892, val_MinusLogProbMetric: 18.6892

Epoch 195: val_loss did not improve from 18.43505
196/196 - 60s - loss: 18.3210 - MinusLogProbMetric: 18.3210 - val_loss: 18.6892 - val_MinusLogProbMetric: 18.6892 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 196/1000
2023-10-09 20:43:10.800 
Epoch 196/1000 
	 loss: 18.2816, MinusLogProbMetric: 18.2816, val_loss: 18.5480, val_MinusLogProbMetric: 18.5480

Epoch 196: val_loss did not improve from 18.43505
196/196 - 66s - loss: 18.2816 - MinusLogProbMetric: 18.2816 - val_loss: 18.5480 - val_MinusLogProbMetric: 18.5480 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 197/1000
2023-10-09 20:44:11.902 
Epoch 197/1000 
	 loss: 18.4182, MinusLogProbMetric: 18.4182, val_loss: 18.5135, val_MinusLogProbMetric: 18.5135

Epoch 197: val_loss did not improve from 18.43505
196/196 - 61s - loss: 18.4182 - MinusLogProbMetric: 18.4182 - val_loss: 18.5135 - val_MinusLogProbMetric: 18.5135 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 198/1000
2023-10-09 20:45:13.800 
Epoch 198/1000 
	 loss: 18.3237, MinusLogProbMetric: 18.3237, val_loss: 18.2639, val_MinusLogProbMetric: 18.2639

Epoch 198: val_loss improved from 18.43505 to 18.26390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 18.3237 - MinusLogProbMetric: 18.3237 - val_loss: 18.2639 - val_MinusLogProbMetric: 18.2639 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 199/1000
2023-10-09 20:46:18.018 
Epoch 199/1000 
	 loss: 18.3632, MinusLogProbMetric: 18.3632, val_loss: 18.5304, val_MinusLogProbMetric: 18.5304

Epoch 199: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.3632 - MinusLogProbMetric: 18.3632 - val_loss: 18.5304 - val_MinusLogProbMetric: 18.5304 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 200/1000
2023-10-09 20:47:23.117 
Epoch 200/1000 
	 loss: 18.3521, MinusLogProbMetric: 18.3521, val_loss: 18.5981, val_MinusLogProbMetric: 18.5981

Epoch 200: val_loss did not improve from 18.26390
196/196 - 65s - loss: 18.3521 - MinusLogProbMetric: 18.3521 - val_loss: 18.5981 - val_MinusLogProbMetric: 18.5981 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 201/1000
2023-10-09 20:48:25.969 
Epoch 201/1000 
	 loss: 18.2338, MinusLogProbMetric: 18.2338, val_loss: 18.5238, val_MinusLogProbMetric: 18.5238

Epoch 201: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.2338 - MinusLogProbMetric: 18.2338 - val_loss: 18.5238 - val_MinusLogProbMetric: 18.5238 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 202/1000
2023-10-09 20:49:28.211 
Epoch 202/1000 
	 loss: 19.5438, MinusLogProbMetric: 19.5438, val_loss: 19.7626, val_MinusLogProbMetric: 19.7626

Epoch 202: val_loss did not improve from 18.26390
196/196 - 62s - loss: 19.5438 - MinusLogProbMetric: 19.5438 - val_loss: 19.7626 - val_MinusLogProbMetric: 19.7626 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 203/1000
2023-10-09 20:50:33.182 
Epoch 203/1000 
	 loss: 18.7758, MinusLogProbMetric: 18.7758, val_loss: 18.9641, val_MinusLogProbMetric: 18.9641

Epoch 203: val_loss did not improve from 18.26390
196/196 - 65s - loss: 18.7758 - MinusLogProbMetric: 18.7758 - val_loss: 18.9641 - val_MinusLogProbMetric: 18.9641 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 204/1000
2023-10-09 20:51:35.250 
Epoch 204/1000 
	 loss: 18.4334, MinusLogProbMetric: 18.4334, val_loss: 18.6673, val_MinusLogProbMetric: 18.6673

Epoch 204: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.4334 - MinusLogProbMetric: 18.4334 - val_loss: 18.6673 - val_MinusLogProbMetric: 18.6673 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 205/1000
2023-10-09 20:52:37.496 
Epoch 205/1000 
	 loss: 18.7078, MinusLogProbMetric: 18.7078, val_loss: 19.2550, val_MinusLogProbMetric: 19.2550

Epoch 205: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.7078 - MinusLogProbMetric: 18.7078 - val_loss: 19.2550 - val_MinusLogProbMetric: 19.2550 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 206/1000
2023-10-09 20:53:40.263 
Epoch 206/1000 
	 loss: 20.7741, MinusLogProbMetric: 20.7741, val_loss: 18.9553, val_MinusLogProbMetric: 18.9553

Epoch 206: val_loss did not improve from 18.26390
196/196 - 63s - loss: 20.7741 - MinusLogProbMetric: 20.7741 - val_loss: 18.9553 - val_MinusLogProbMetric: 18.9553 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 207/1000
2023-10-09 20:54:41.792 
Epoch 207/1000 
	 loss: 18.5150, MinusLogProbMetric: 18.5150, val_loss: 18.6092, val_MinusLogProbMetric: 18.6092

Epoch 207: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.5150 - MinusLogProbMetric: 18.5150 - val_loss: 18.6092 - val_MinusLogProbMetric: 18.6092 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 208/1000
2023-10-09 20:55:43.414 
Epoch 208/1000 
	 loss: 18.3661, MinusLogProbMetric: 18.3661, val_loss: 18.6023, val_MinusLogProbMetric: 18.6023

Epoch 208: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.3661 - MinusLogProbMetric: 18.3661 - val_loss: 18.6023 - val_MinusLogProbMetric: 18.6023 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 209/1000
2023-10-09 20:56:44.046 
Epoch 209/1000 
	 loss: 23.7201, MinusLogProbMetric: 23.7201, val_loss: 38.1723, val_MinusLogProbMetric: 38.1723

Epoch 209: val_loss did not improve from 18.26390
196/196 - 61s - loss: 23.7201 - MinusLogProbMetric: 23.7201 - val_loss: 38.1723 - val_MinusLogProbMetric: 38.1723 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 210/1000
2023-10-09 20:57:46.421 
Epoch 210/1000 
	 loss: 25.6726, MinusLogProbMetric: 25.6726, val_loss: 22.2210, val_MinusLogProbMetric: 22.2210

Epoch 210: val_loss did not improve from 18.26390
196/196 - 62s - loss: 25.6726 - MinusLogProbMetric: 25.6726 - val_loss: 22.2210 - val_MinusLogProbMetric: 22.2210 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 211/1000
2023-10-09 20:58:49.362 
Epoch 211/1000 
	 loss: 20.9688, MinusLogProbMetric: 20.9688, val_loss: 20.4062, val_MinusLogProbMetric: 20.4062

Epoch 211: val_loss did not improve from 18.26390
196/196 - 63s - loss: 20.9688 - MinusLogProbMetric: 20.9688 - val_loss: 20.4062 - val_MinusLogProbMetric: 20.4062 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 212/1000
2023-10-09 20:59:49.983 
Epoch 212/1000 
	 loss: 19.3942, MinusLogProbMetric: 19.3942, val_loss: 19.1988, val_MinusLogProbMetric: 19.1988

Epoch 212: val_loss did not improve from 18.26390
196/196 - 61s - loss: 19.3942 - MinusLogProbMetric: 19.3942 - val_loss: 19.1988 - val_MinusLogProbMetric: 19.1988 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 213/1000
2023-10-09 21:00:51.731 
Epoch 213/1000 
	 loss: 18.7676, MinusLogProbMetric: 18.7676, val_loss: 18.7763, val_MinusLogProbMetric: 18.7763

Epoch 213: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.7676 - MinusLogProbMetric: 18.7676 - val_loss: 18.7763 - val_MinusLogProbMetric: 18.7763 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 214/1000
2023-10-09 21:01:55.518 
Epoch 214/1000 
	 loss: 18.5427, MinusLogProbMetric: 18.5427, val_loss: 18.9209, val_MinusLogProbMetric: 18.9209

Epoch 214: val_loss did not improve from 18.26390
196/196 - 64s - loss: 18.5427 - MinusLogProbMetric: 18.5427 - val_loss: 18.9209 - val_MinusLogProbMetric: 18.9209 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 215/1000
2023-10-09 21:02:58.167 
Epoch 215/1000 
	 loss: 18.4858, MinusLogProbMetric: 18.4858, val_loss: 18.6100, val_MinusLogProbMetric: 18.6100

Epoch 215: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.4858 - MinusLogProbMetric: 18.4858 - val_loss: 18.6100 - val_MinusLogProbMetric: 18.6100 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 216/1000
2023-10-09 21:04:01.949 
Epoch 216/1000 
	 loss: 18.3852, MinusLogProbMetric: 18.3852, val_loss: 18.6559, val_MinusLogProbMetric: 18.6559

Epoch 216: val_loss did not improve from 18.26390
196/196 - 64s - loss: 18.3852 - MinusLogProbMetric: 18.3852 - val_loss: 18.6559 - val_MinusLogProbMetric: 18.6559 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 217/1000
2023-10-09 21:05:05.048 
Epoch 217/1000 
	 loss: 18.3780, MinusLogProbMetric: 18.3780, val_loss: 18.8320, val_MinusLogProbMetric: 18.8320

Epoch 217: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.3780 - MinusLogProbMetric: 18.3780 - val_loss: 18.8320 - val_MinusLogProbMetric: 18.8320 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 218/1000
2023-10-09 21:06:07.123 
Epoch 218/1000 
	 loss: 18.3035, MinusLogProbMetric: 18.3035, val_loss: 18.6832, val_MinusLogProbMetric: 18.6832

Epoch 218: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.3035 - MinusLogProbMetric: 18.3035 - val_loss: 18.6832 - val_MinusLogProbMetric: 18.6832 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 219/1000
2023-10-09 21:07:07.936 
Epoch 219/1000 
	 loss: 18.3187, MinusLogProbMetric: 18.3187, val_loss: 18.5335, val_MinusLogProbMetric: 18.5335

Epoch 219: val_loss did not improve from 18.26390
196/196 - 61s - loss: 18.3187 - MinusLogProbMetric: 18.3187 - val_loss: 18.5335 - val_MinusLogProbMetric: 18.5335 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 220/1000
2023-10-09 21:08:09.548 
Epoch 220/1000 
	 loss: 18.2841, MinusLogProbMetric: 18.2841, val_loss: 18.4004, val_MinusLogProbMetric: 18.4004

Epoch 220: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.2841 - MinusLogProbMetric: 18.2841 - val_loss: 18.4004 - val_MinusLogProbMetric: 18.4004 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 221/1000
2023-10-09 21:09:10.887 
Epoch 221/1000 
	 loss: 18.3157, MinusLogProbMetric: 18.3157, val_loss: 18.5466, val_MinusLogProbMetric: 18.5466

Epoch 221: val_loss did not improve from 18.26390
196/196 - 61s - loss: 18.3157 - MinusLogProbMetric: 18.3157 - val_loss: 18.5466 - val_MinusLogProbMetric: 18.5466 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 222/1000
2023-10-09 21:10:11.746 
Epoch 222/1000 
	 loss: 18.2527, MinusLogProbMetric: 18.2527, val_loss: 18.3439, val_MinusLogProbMetric: 18.3439

Epoch 222: val_loss did not improve from 18.26390
196/196 - 61s - loss: 18.2527 - MinusLogProbMetric: 18.2527 - val_loss: 18.3439 - val_MinusLogProbMetric: 18.3439 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 223/1000
2023-10-09 21:11:13.582 
Epoch 223/1000 
	 loss: 18.2337, MinusLogProbMetric: 18.2337, val_loss: 18.4534, val_MinusLogProbMetric: 18.4534

Epoch 223: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.2337 - MinusLogProbMetric: 18.2337 - val_loss: 18.4534 - val_MinusLogProbMetric: 18.4534 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 224/1000
2023-10-09 21:12:17.625 
Epoch 224/1000 
	 loss: 18.2126, MinusLogProbMetric: 18.2126, val_loss: 18.4032, val_MinusLogProbMetric: 18.4032

Epoch 224: val_loss did not improve from 18.26390
196/196 - 64s - loss: 18.2126 - MinusLogProbMetric: 18.2126 - val_loss: 18.4032 - val_MinusLogProbMetric: 18.4032 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 225/1000
2023-10-09 21:13:19.351 
Epoch 225/1000 
	 loss: 18.2472, MinusLogProbMetric: 18.2472, val_loss: 18.4979, val_MinusLogProbMetric: 18.4979

Epoch 225: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.2472 - MinusLogProbMetric: 18.2472 - val_loss: 18.4979 - val_MinusLogProbMetric: 18.4979 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 226/1000
2023-10-09 21:14:21.850 
Epoch 226/1000 
	 loss: 18.2159, MinusLogProbMetric: 18.2159, val_loss: 18.4873, val_MinusLogProbMetric: 18.4873

Epoch 226: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.2159 - MinusLogProbMetric: 18.2159 - val_loss: 18.4873 - val_MinusLogProbMetric: 18.4873 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 227/1000
2023-10-09 21:15:25.051 
Epoch 227/1000 
	 loss: 18.2178, MinusLogProbMetric: 18.2178, val_loss: 18.4825, val_MinusLogProbMetric: 18.4825

Epoch 227: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.2178 - MinusLogProbMetric: 18.2178 - val_loss: 18.4825 - val_MinusLogProbMetric: 18.4825 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 228/1000
2023-10-09 21:16:28.248 
Epoch 228/1000 
	 loss: 18.1778, MinusLogProbMetric: 18.1778, val_loss: 18.5605, val_MinusLogProbMetric: 18.5605

Epoch 228: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.1778 - MinusLogProbMetric: 18.1778 - val_loss: 18.5605 - val_MinusLogProbMetric: 18.5605 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 229/1000
2023-10-09 21:17:31.786 
Epoch 229/1000 
	 loss: 18.1867, MinusLogProbMetric: 18.1867, val_loss: 18.4209, val_MinusLogProbMetric: 18.4209

Epoch 229: val_loss did not improve from 18.26390
196/196 - 64s - loss: 18.1867 - MinusLogProbMetric: 18.1867 - val_loss: 18.4209 - val_MinusLogProbMetric: 18.4209 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 230/1000
2023-10-09 21:18:33.957 
Epoch 230/1000 
	 loss: 18.4103, MinusLogProbMetric: 18.4103, val_loss: 18.5065, val_MinusLogProbMetric: 18.5065

Epoch 230: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.4103 - MinusLogProbMetric: 18.4103 - val_loss: 18.5065 - val_MinusLogProbMetric: 18.5065 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 231/1000
2023-10-09 21:19:35.434 
Epoch 231/1000 
	 loss: 18.8659, MinusLogProbMetric: 18.8659, val_loss: 19.3214, val_MinusLogProbMetric: 19.3214

Epoch 231: val_loss did not improve from 18.26390
196/196 - 61s - loss: 18.8659 - MinusLogProbMetric: 18.8659 - val_loss: 19.3214 - val_MinusLogProbMetric: 19.3214 - lr: 3.3333e-04 - 61s/epoch - 314ms/step
Epoch 232/1000
2023-10-09 21:20:38.789 
Epoch 232/1000 
	 loss: 18.2233, MinusLogProbMetric: 18.2233, val_loss: 18.3412, val_MinusLogProbMetric: 18.3412

Epoch 232: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.2233 - MinusLogProbMetric: 18.2233 - val_loss: 18.3412 - val_MinusLogProbMetric: 18.3412 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 233/1000
2023-10-09 21:21:41.312 
Epoch 233/1000 
	 loss: 18.1293, MinusLogProbMetric: 18.1293, val_loss: 18.5634, val_MinusLogProbMetric: 18.5634

Epoch 233: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.1293 - MinusLogProbMetric: 18.1293 - val_loss: 18.5634 - val_MinusLogProbMetric: 18.5634 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 234/1000
2023-10-09 21:22:43.927 
Epoch 234/1000 
	 loss: 18.1346, MinusLogProbMetric: 18.1346, val_loss: 18.3705, val_MinusLogProbMetric: 18.3705

Epoch 234: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.1346 - MinusLogProbMetric: 18.1346 - val_loss: 18.3705 - val_MinusLogProbMetric: 18.3705 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 235/1000
2023-10-09 21:23:45.060 
Epoch 235/1000 
	 loss: 18.1306, MinusLogProbMetric: 18.1306, val_loss: 18.3931, val_MinusLogProbMetric: 18.3931

Epoch 235: val_loss did not improve from 18.26390
196/196 - 61s - loss: 18.1306 - MinusLogProbMetric: 18.1306 - val_loss: 18.3931 - val_MinusLogProbMetric: 18.3931 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 236/1000
2023-10-09 21:24:44.819 
Epoch 236/1000 
	 loss: 18.1388, MinusLogProbMetric: 18.1388, val_loss: 18.5176, val_MinusLogProbMetric: 18.5176

Epoch 236: val_loss did not improve from 18.26390
196/196 - 60s - loss: 18.1388 - MinusLogProbMetric: 18.1388 - val_loss: 18.5176 - val_MinusLogProbMetric: 18.5176 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 237/1000
2023-10-09 21:25:46.844 
Epoch 237/1000 
	 loss: 18.1609, MinusLogProbMetric: 18.1609, val_loss: 18.3736, val_MinusLogProbMetric: 18.3736

Epoch 237: val_loss did not improve from 18.26390
196/196 - 62s - loss: 18.1609 - MinusLogProbMetric: 18.1609 - val_loss: 18.3736 - val_MinusLogProbMetric: 18.3736 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 238/1000
2023-10-09 21:26:49.995 
Epoch 238/1000 
	 loss: 18.1311, MinusLogProbMetric: 18.1311, val_loss: 18.3915, val_MinusLogProbMetric: 18.3915

Epoch 238: val_loss did not improve from 18.26390
196/196 - 63s - loss: 18.1311 - MinusLogProbMetric: 18.1311 - val_loss: 18.3915 - val_MinusLogProbMetric: 18.3915 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 239/1000
2023-10-09 21:27:52.391 
Epoch 239/1000 
	 loss: 18.0890, MinusLogProbMetric: 18.0890, val_loss: 18.1708, val_MinusLogProbMetric: 18.1708

Epoch 239: val_loss improved from 18.26390 to 18.17079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 18.0890 - MinusLogProbMetric: 18.0890 - val_loss: 18.1708 - val_MinusLogProbMetric: 18.1708 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 240/1000
2023-10-09 21:28:56.326 
Epoch 240/1000 
	 loss: 18.0571, MinusLogProbMetric: 18.0571, val_loss: 18.3601, val_MinusLogProbMetric: 18.3601

Epoch 240: val_loss did not improve from 18.17079
196/196 - 63s - loss: 18.0571 - MinusLogProbMetric: 18.0571 - val_loss: 18.3601 - val_MinusLogProbMetric: 18.3601 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 241/1000
2023-10-09 21:29:57.081 
Epoch 241/1000 
	 loss: 18.0807, MinusLogProbMetric: 18.0807, val_loss: 18.5298, val_MinusLogProbMetric: 18.5298

Epoch 241: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0807 - MinusLogProbMetric: 18.0807 - val_loss: 18.5298 - val_MinusLogProbMetric: 18.5298 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 242/1000
2023-10-09 21:30:58.493 
Epoch 242/1000 
	 loss: 18.1098, MinusLogProbMetric: 18.1098, val_loss: 18.4294, val_MinusLogProbMetric: 18.4294

Epoch 242: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.1098 - MinusLogProbMetric: 18.1098 - val_loss: 18.4294 - val_MinusLogProbMetric: 18.4294 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 243/1000
2023-10-09 21:31:59.471 
Epoch 243/1000 
	 loss: 18.0592, MinusLogProbMetric: 18.0592, val_loss: 18.4886, val_MinusLogProbMetric: 18.4886

Epoch 243: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0592 - MinusLogProbMetric: 18.0592 - val_loss: 18.4886 - val_MinusLogProbMetric: 18.4886 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 244/1000
2023-10-09 21:32:59.332 
Epoch 244/1000 
	 loss: 18.0589, MinusLogProbMetric: 18.0589, val_loss: 18.2431, val_MinusLogProbMetric: 18.2431

Epoch 244: val_loss did not improve from 18.17079
196/196 - 60s - loss: 18.0589 - MinusLogProbMetric: 18.0589 - val_loss: 18.2431 - val_MinusLogProbMetric: 18.2431 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 245/1000
2023-10-09 21:34:01.909 
Epoch 245/1000 
	 loss: 18.0810, MinusLogProbMetric: 18.0810, val_loss: 18.2512, val_MinusLogProbMetric: 18.2512

Epoch 245: val_loss did not improve from 18.17079
196/196 - 63s - loss: 18.0810 - MinusLogProbMetric: 18.0810 - val_loss: 18.2512 - val_MinusLogProbMetric: 18.2512 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 246/1000
2023-10-09 21:35:03.241 
Epoch 246/1000 
	 loss: 18.0816, MinusLogProbMetric: 18.0816, val_loss: 18.2142, val_MinusLogProbMetric: 18.2142

Epoch 246: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0816 - MinusLogProbMetric: 18.0816 - val_loss: 18.2142 - val_MinusLogProbMetric: 18.2142 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 247/1000
2023-10-09 21:36:07.431 
Epoch 247/1000 
	 loss: 18.0435, MinusLogProbMetric: 18.0435, val_loss: 18.2415, val_MinusLogProbMetric: 18.2415

Epoch 247: val_loss did not improve from 18.17079
196/196 - 64s - loss: 18.0435 - MinusLogProbMetric: 18.0435 - val_loss: 18.2415 - val_MinusLogProbMetric: 18.2415 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 248/1000
2023-10-09 21:37:07.966 
Epoch 248/1000 
	 loss: 18.0633, MinusLogProbMetric: 18.0633, val_loss: 18.2379, val_MinusLogProbMetric: 18.2379

Epoch 248: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0633 - MinusLogProbMetric: 18.0633 - val_loss: 18.2379 - val_MinusLogProbMetric: 18.2379 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 249/1000
2023-10-09 21:38:08.385 
Epoch 249/1000 
	 loss: 18.0536, MinusLogProbMetric: 18.0536, val_loss: 18.5054, val_MinusLogProbMetric: 18.5054

Epoch 249: val_loss did not improve from 18.17079
196/196 - 60s - loss: 18.0536 - MinusLogProbMetric: 18.0536 - val_loss: 18.5054 - val_MinusLogProbMetric: 18.5054 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 250/1000
2023-10-09 21:39:08.956 
Epoch 250/1000 
	 loss: 18.0416, MinusLogProbMetric: 18.0416, val_loss: 18.7451, val_MinusLogProbMetric: 18.7451

Epoch 250: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0416 - MinusLogProbMetric: 18.0416 - val_loss: 18.7451 - val_MinusLogProbMetric: 18.7451 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 251/1000
2023-10-09 21:40:11.219 
Epoch 251/1000 
	 loss: 19.7108, MinusLogProbMetric: 19.7108, val_loss: 19.4827, val_MinusLogProbMetric: 19.4827

Epoch 251: val_loss did not improve from 18.17079
196/196 - 62s - loss: 19.7108 - MinusLogProbMetric: 19.7108 - val_loss: 19.4827 - val_MinusLogProbMetric: 19.4827 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 252/1000
2023-10-09 21:41:14.047 
Epoch 252/1000 
	 loss: 32.1292, MinusLogProbMetric: 32.1292, val_loss: 25.0037, val_MinusLogProbMetric: 25.0037

Epoch 252: val_loss did not improve from 18.17079
196/196 - 63s - loss: 32.1292 - MinusLogProbMetric: 32.1292 - val_loss: 25.0037 - val_MinusLogProbMetric: 25.0037 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 253/1000
2023-10-09 21:42:15.543 
Epoch 253/1000 
	 loss: 23.4229, MinusLogProbMetric: 23.4229, val_loss: 22.8306, val_MinusLogProbMetric: 22.8306

Epoch 253: val_loss did not improve from 18.17079
196/196 - 61s - loss: 23.4229 - MinusLogProbMetric: 23.4229 - val_loss: 22.8306 - val_MinusLogProbMetric: 22.8306 - lr: 3.3333e-04 - 61s/epoch - 314ms/step
Epoch 254/1000
2023-10-09 21:43:16.165 
Epoch 254/1000 
	 loss: 22.0639, MinusLogProbMetric: 22.0639, val_loss: 21.7013, val_MinusLogProbMetric: 21.7013

Epoch 254: val_loss did not improve from 18.17079
196/196 - 61s - loss: 22.0639 - MinusLogProbMetric: 22.0639 - val_loss: 21.7013 - val_MinusLogProbMetric: 21.7013 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 255/1000
2023-10-09 21:44:16.503 
Epoch 255/1000 
	 loss: 21.3638, MinusLogProbMetric: 21.3638, val_loss: 21.1065, val_MinusLogProbMetric: 21.1065

Epoch 255: val_loss did not improve from 18.17079
196/196 - 60s - loss: 21.3638 - MinusLogProbMetric: 21.3638 - val_loss: 21.1065 - val_MinusLogProbMetric: 21.1065 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 256/1000
2023-10-09 21:45:19.155 
Epoch 256/1000 
	 loss: 20.8809, MinusLogProbMetric: 20.8809, val_loss: 20.6570, val_MinusLogProbMetric: 20.6570

Epoch 256: val_loss did not improve from 18.17079
196/196 - 63s - loss: 20.8809 - MinusLogProbMetric: 20.8809 - val_loss: 20.6570 - val_MinusLogProbMetric: 20.6570 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 257/1000
2023-10-09 21:46:20.718 
Epoch 257/1000 
	 loss: 20.1525, MinusLogProbMetric: 20.1525, val_loss: 19.8467, val_MinusLogProbMetric: 19.8467

Epoch 257: val_loss did not improve from 18.17079
196/196 - 62s - loss: 20.1525 - MinusLogProbMetric: 20.1525 - val_loss: 19.8467 - val_MinusLogProbMetric: 19.8467 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 258/1000
2023-10-09 21:47:22.558 
Epoch 258/1000 
	 loss: 19.4548, MinusLogProbMetric: 19.4548, val_loss: 19.1806, val_MinusLogProbMetric: 19.1806

Epoch 258: val_loss did not improve from 18.17079
196/196 - 62s - loss: 19.4548 - MinusLogProbMetric: 19.4548 - val_loss: 19.1806 - val_MinusLogProbMetric: 19.1806 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 259/1000
2023-10-09 21:48:22.119 
Epoch 259/1000 
	 loss: 18.9883, MinusLogProbMetric: 18.9883, val_loss: 19.0205, val_MinusLogProbMetric: 19.0205

Epoch 259: val_loss did not improve from 18.17079
196/196 - 60s - loss: 18.9883 - MinusLogProbMetric: 18.9883 - val_loss: 19.0205 - val_MinusLogProbMetric: 19.0205 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 260/1000
2023-10-09 21:49:25.126 
Epoch 260/1000 
	 loss: 18.8760, MinusLogProbMetric: 18.8760, val_loss: 19.0054, val_MinusLogProbMetric: 19.0054

Epoch 260: val_loss did not improve from 18.17079
196/196 - 63s - loss: 18.8760 - MinusLogProbMetric: 18.8760 - val_loss: 19.0054 - val_MinusLogProbMetric: 19.0054 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 261/1000
2023-10-09 21:50:26.961 
Epoch 261/1000 
	 loss: 18.7652, MinusLogProbMetric: 18.7652, val_loss: 18.8301, val_MinusLogProbMetric: 18.8301

Epoch 261: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.7652 - MinusLogProbMetric: 18.7652 - val_loss: 18.8301 - val_MinusLogProbMetric: 18.8301 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 262/1000
2023-10-09 21:51:30.185 
Epoch 262/1000 
	 loss: 18.6721, MinusLogProbMetric: 18.6721, val_loss: 18.8029, val_MinusLogProbMetric: 18.8029

Epoch 262: val_loss did not improve from 18.17079
196/196 - 63s - loss: 18.6721 - MinusLogProbMetric: 18.6721 - val_loss: 18.8029 - val_MinusLogProbMetric: 18.8029 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 263/1000
2023-10-09 21:52:32.636 
Epoch 263/1000 
	 loss: 18.6511, MinusLogProbMetric: 18.6511, val_loss: 18.5858, val_MinusLogProbMetric: 18.5858

Epoch 263: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.6511 - MinusLogProbMetric: 18.6511 - val_loss: 18.5858 - val_MinusLogProbMetric: 18.5858 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 264/1000
2023-10-09 21:53:35.175 
Epoch 264/1000 
	 loss: 18.5300, MinusLogProbMetric: 18.5300, val_loss: 18.7196, val_MinusLogProbMetric: 18.7196

Epoch 264: val_loss did not improve from 18.17079
196/196 - 63s - loss: 18.5300 - MinusLogProbMetric: 18.5300 - val_loss: 18.7196 - val_MinusLogProbMetric: 18.7196 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 265/1000
2023-10-09 21:54:35.899 
Epoch 265/1000 
	 loss: 18.4684, MinusLogProbMetric: 18.4684, val_loss: 18.5914, val_MinusLogProbMetric: 18.5914

Epoch 265: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.4684 - MinusLogProbMetric: 18.4684 - val_loss: 18.5914 - val_MinusLogProbMetric: 18.5914 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 266/1000
2023-10-09 21:55:37.459 
Epoch 266/1000 
	 loss: 18.4113, MinusLogProbMetric: 18.4113, val_loss: 18.4870, val_MinusLogProbMetric: 18.4870

Epoch 266: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.4113 - MinusLogProbMetric: 18.4113 - val_loss: 18.4870 - val_MinusLogProbMetric: 18.4870 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 267/1000
2023-10-09 21:56:37.810 
Epoch 267/1000 
	 loss: 18.3774, MinusLogProbMetric: 18.3774, val_loss: 18.7328, val_MinusLogProbMetric: 18.7328

Epoch 267: val_loss did not improve from 18.17079
196/196 - 60s - loss: 18.3774 - MinusLogProbMetric: 18.3774 - val_loss: 18.7328 - val_MinusLogProbMetric: 18.7328 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 268/1000
2023-10-09 21:57:38.974 
Epoch 268/1000 
	 loss: 18.3609, MinusLogProbMetric: 18.3609, val_loss: 18.6769, val_MinusLogProbMetric: 18.6769

Epoch 268: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.3609 - MinusLogProbMetric: 18.3609 - val_loss: 18.6769 - val_MinusLogProbMetric: 18.6769 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 269/1000
2023-10-09 21:58:45.797 
Epoch 269/1000 
	 loss: 18.3204, MinusLogProbMetric: 18.3204, val_loss: 18.6064, val_MinusLogProbMetric: 18.6064

Epoch 269: val_loss did not improve from 18.17079
196/196 - 67s - loss: 18.3204 - MinusLogProbMetric: 18.3204 - val_loss: 18.6064 - val_MinusLogProbMetric: 18.6064 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 270/1000
2023-10-09 21:59:47.143 
Epoch 270/1000 
	 loss: 18.3043, MinusLogProbMetric: 18.3043, val_loss: 18.5826, val_MinusLogProbMetric: 18.5826

Epoch 270: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.3043 - MinusLogProbMetric: 18.3043 - val_loss: 18.5826 - val_MinusLogProbMetric: 18.5826 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 271/1000
2023-10-09 22:00:48.878 
Epoch 271/1000 
	 loss: 18.2874, MinusLogProbMetric: 18.2874, val_loss: 18.4706, val_MinusLogProbMetric: 18.4706

Epoch 271: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.2874 - MinusLogProbMetric: 18.2874 - val_loss: 18.4706 - val_MinusLogProbMetric: 18.4706 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 272/1000
2023-10-09 22:01:51.242 
Epoch 272/1000 
	 loss: 18.3304, MinusLogProbMetric: 18.3304, val_loss: 18.7144, val_MinusLogProbMetric: 18.7144

Epoch 272: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.3304 - MinusLogProbMetric: 18.3304 - val_loss: 18.7144 - val_MinusLogProbMetric: 18.7144 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 273/1000
2023-10-09 22:02:52.875 
Epoch 273/1000 
	 loss: 18.2406, MinusLogProbMetric: 18.2406, val_loss: 18.3589, val_MinusLogProbMetric: 18.3589

Epoch 273: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.2406 - MinusLogProbMetric: 18.2406 - val_loss: 18.3589 - val_MinusLogProbMetric: 18.3589 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 274/1000
2023-10-09 22:03:55.136 
Epoch 274/1000 
	 loss: 18.1880, MinusLogProbMetric: 18.1880, val_loss: 18.5358, val_MinusLogProbMetric: 18.5358

Epoch 274: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.1880 - MinusLogProbMetric: 18.1880 - val_loss: 18.5358 - val_MinusLogProbMetric: 18.5358 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 275/1000
2023-10-09 22:04:56.016 
Epoch 275/1000 
	 loss: 18.3080, MinusLogProbMetric: 18.3080, val_loss: 18.4908, val_MinusLogProbMetric: 18.4908

Epoch 275: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.3080 - MinusLogProbMetric: 18.3080 - val_loss: 18.4908 - val_MinusLogProbMetric: 18.4908 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 276/1000
2023-10-09 22:05:57.905 
Epoch 276/1000 
	 loss: 18.2035, MinusLogProbMetric: 18.2035, val_loss: 18.4280, val_MinusLogProbMetric: 18.4280

Epoch 276: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.2035 - MinusLogProbMetric: 18.2035 - val_loss: 18.4280 - val_MinusLogProbMetric: 18.4280 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 277/1000
2023-10-09 22:06:59.008 
Epoch 277/1000 
	 loss: 18.1885, MinusLogProbMetric: 18.1885, val_loss: 18.3503, val_MinusLogProbMetric: 18.3503

Epoch 277: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.1885 - MinusLogProbMetric: 18.1885 - val_loss: 18.3503 - val_MinusLogProbMetric: 18.3503 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 278/1000
2023-10-09 22:08:01.160 
Epoch 278/1000 
	 loss: 18.1683, MinusLogProbMetric: 18.1683, val_loss: 18.3115, val_MinusLogProbMetric: 18.3115

Epoch 278: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.1683 - MinusLogProbMetric: 18.1683 - val_loss: 18.3115 - val_MinusLogProbMetric: 18.3115 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 279/1000
2023-10-09 22:09:02.728 
Epoch 279/1000 
	 loss: 18.1299, MinusLogProbMetric: 18.1299, val_loss: 18.3508, val_MinusLogProbMetric: 18.3508

Epoch 279: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.1299 - MinusLogProbMetric: 18.1299 - val_loss: 18.3508 - val_MinusLogProbMetric: 18.3508 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 280/1000
2023-10-09 22:10:03.902 
Epoch 280/1000 
	 loss: 18.1081, MinusLogProbMetric: 18.1081, val_loss: 18.2133, val_MinusLogProbMetric: 18.2133

Epoch 280: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.1081 - MinusLogProbMetric: 18.1081 - val_loss: 18.2133 - val_MinusLogProbMetric: 18.2133 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 281/1000
2023-10-09 22:11:05.223 
Epoch 281/1000 
	 loss: 18.1294, MinusLogProbMetric: 18.1294, val_loss: 18.3602, val_MinusLogProbMetric: 18.3602

Epoch 281: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.1294 - MinusLogProbMetric: 18.1294 - val_loss: 18.3602 - val_MinusLogProbMetric: 18.3602 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 282/1000
2023-10-09 22:12:09.065 
Epoch 282/1000 
	 loss: 18.5122, MinusLogProbMetric: 18.5122, val_loss: 18.2652, val_MinusLogProbMetric: 18.2652

Epoch 282: val_loss did not improve from 18.17079
196/196 - 64s - loss: 18.5122 - MinusLogProbMetric: 18.5122 - val_loss: 18.2652 - val_MinusLogProbMetric: 18.2652 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 283/1000
2023-10-09 22:13:11.015 
Epoch 283/1000 
	 loss: 18.1507, MinusLogProbMetric: 18.1507, val_loss: 18.4710, val_MinusLogProbMetric: 18.4710

Epoch 283: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.1507 - MinusLogProbMetric: 18.1507 - val_loss: 18.4710 - val_MinusLogProbMetric: 18.4710 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 284/1000
2023-10-09 22:14:14.656 
Epoch 284/1000 
	 loss: 18.1293, MinusLogProbMetric: 18.1293, val_loss: 18.2510, val_MinusLogProbMetric: 18.2510

Epoch 284: val_loss did not improve from 18.17079
196/196 - 64s - loss: 18.1293 - MinusLogProbMetric: 18.1293 - val_loss: 18.2510 - val_MinusLogProbMetric: 18.2510 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 285/1000
2023-10-09 22:15:15.825 
Epoch 285/1000 
	 loss: 18.0896, MinusLogProbMetric: 18.0896, val_loss: 18.3731, val_MinusLogProbMetric: 18.3731

Epoch 285: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0896 - MinusLogProbMetric: 18.0896 - val_loss: 18.3731 - val_MinusLogProbMetric: 18.3731 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 286/1000
2023-10-09 22:16:18.119 
Epoch 286/1000 
	 loss: 18.1029, MinusLogProbMetric: 18.1029, val_loss: 18.3720, val_MinusLogProbMetric: 18.3720

Epoch 286: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.1029 - MinusLogProbMetric: 18.1029 - val_loss: 18.3720 - val_MinusLogProbMetric: 18.3720 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 287/1000
2023-10-09 22:17:20.022 
Epoch 287/1000 
	 loss: 18.0968, MinusLogProbMetric: 18.0968, val_loss: 18.2620, val_MinusLogProbMetric: 18.2620

Epoch 287: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.0968 - MinusLogProbMetric: 18.0968 - val_loss: 18.2620 - val_MinusLogProbMetric: 18.2620 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 288/1000
2023-10-09 22:18:20.689 
Epoch 288/1000 
	 loss: 18.0876, MinusLogProbMetric: 18.0876, val_loss: 18.4417, val_MinusLogProbMetric: 18.4417

Epoch 288: val_loss did not improve from 18.17079
196/196 - 61s - loss: 18.0876 - MinusLogProbMetric: 18.0876 - val_loss: 18.4417 - val_MinusLogProbMetric: 18.4417 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 289/1000
2023-10-09 22:19:22.242 
Epoch 289/1000 
	 loss: 18.0660, MinusLogProbMetric: 18.0660, val_loss: 18.1951, val_MinusLogProbMetric: 18.1951

Epoch 289: val_loss did not improve from 18.17079
196/196 - 62s - loss: 18.0660 - MinusLogProbMetric: 18.0660 - val_loss: 18.1951 - val_MinusLogProbMetric: 18.1951 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 290/1000
2023-10-09 22:20:25.911 
Epoch 290/1000 
	 loss: 17.8767, MinusLogProbMetric: 17.8767, val_loss: 18.0741, val_MinusLogProbMetric: 18.0741

Epoch 290: val_loss improved from 18.17079 to 18.07408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 65s - loss: 17.8767 - MinusLogProbMetric: 17.8767 - val_loss: 18.0741 - val_MinusLogProbMetric: 18.0741 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 291/1000
2023-10-09 22:21:31.084 
Epoch 291/1000 
	 loss: 17.8646, MinusLogProbMetric: 17.8646, val_loss: 18.0846, val_MinusLogProbMetric: 18.0846

Epoch 291: val_loss did not improve from 18.07408
196/196 - 64s - loss: 17.8646 - MinusLogProbMetric: 17.8646 - val_loss: 18.0846 - val_MinusLogProbMetric: 18.0846 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 292/1000
2023-10-09 22:22:32.583 
Epoch 292/1000 
	 loss: 17.8513, MinusLogProbMetric: 17.8513, val_loss: 18.1226, val_MinusLogProbMetric: 18.1226

Epoch 292: val_loss did not improve from 18.07408
196/196 - 61s - loss: 17.8513 - MinusLogProbMetric: 17.8513 - val_loss: 18.1226 - val_MinusLogProbMetric: 18.1226 - lr: 1.6667e-04 - 61s/epoch - 314ms/step
Epoch 293/1000
2023-10-09 22:23:33.693 
Epoch 293/1000 
	 loss: 17.8456, MinusLogProbMetric: 17.8456, val_loss: 17.9590, val_MinusLogProbMetric: 17.9590

Epoch 293: val_loss improved from 18.07408 to 17.95902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 17.8456 - MinusLogProbMetric: 17.8456 - val_loss: 17.9590 - val_MinusLogProbMetric: 17.9590 - lr: 1.6667e-04 - 62s/epoch - 317ms/step
Epoch 294/1000
2023-10-09 22:24:35.916 
Epoch 294/1000 
	 loss: 17.8590, MinusLogProbMetric: 17.8590, val_loss: 18.0124, val_MinusLogProbMetric: 18.0124

Epoch 294: val_loss did not improve from 17.95902
196/196 - 61s - loss: 17.8590 - MinusLogProbMetric: 17.8590 - val_loss: 18.0124 - val_MinusLogProbMetric: 18.0124 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 295/1000
2023-10-09 22:25:41.432 
Epoch 295/1000 
	 loss: 17.8941, MinusLogProbMetric: 17.8941, val_loss: 18.0378, val_MinusLogProbMetric: 18.0378

Epoch 295: val_loss did not improve from 17.95902
196/196 - 66s - loss: 17.8941 - MinusLogProbMetric: 17.8941 - val_loss: 18.0378 - val_MinusLogProbMetric: 18.0378 - lr: 1.6667e-04 - 66s/epoch - 334ms/step
Epoch 296/1000
2023-10-09 22:26:43.039 
Epoch 296/1000 
	 loss: 17.8320, MinusLogProbMetric: 17.8320, val_loss: 18.0486, val_MinusLogProbMetric: 18.0486

Epoch 296: val_loss did not improve from 17.95902
196/196 - 62s - loss: 17.8320 - MinusLogProbMetric: 17.8320 - val_loss: 18.0486 - val_MinusLogProbMetric: 18.0486 - lr: 1.6667e-04 - 62s/epoch - 314ms/step
Epoch 297/1000
2023-10-09 22:27:44.104 
Epoch 297/1000 
	 loss: 18.1688, MinusLogProbMetric: 18.1688, val_loss: 19.0514, val_MinusLogProbMetric: 19.0514

Epoch 297: val_loss did not improve from 17.95902
196/196 - 61s - loss: 18.1688 - MinusLogProbMetric: 18.1688 - val_loss: 19.0514 - val_MinusLogProbMetric: 19.0514 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 298/1000
2023-10-09 22:28:46.905 
Epoch 298/1000 
	 loss: 17.9795, MinusLogProbMetric: 17.9795, val_loss: 18.0205, val_MinusLogProbMetric: 18.0205

Epoch 298: val_loss did not improve from 17.95902
196/196 - 63s - loss: 17.9795 - MinusLogProbMetric: 17.9795 - val_loss: 18.0205 - val_MinusLogProbMetric: 18.0205 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 299/1000
2023-10-09 22:29:47.093 
Epoch 299/1000 
	 loss: 17.8231, MinusLogProbMetric: 17.8231, val_loss: 18.0108, val_MinusLogProbMetric: 18.0108

Epoch 299: val_loss did not improve from 17.95902
196/196 - 60s - loss: 17.8231 - MinusLogProbMetric: 17.8231 - val_loss: 18.0108 - val_MinusLogProbMetric: 18.0108 - lr: 1.6667e-04 - 60s/epoch - 307ms/step
Epoch 300/1000
2023-10-09 22:30:49.281 
Epoch 300/1000 
	 loss: 17.8334, MinusLogProbMetric: 17.8334, val_loss: 17.9568, val_MinusLogProbMetric: 17.9568

Epoch 300: val_loss improved from 17.95902 to 17.95678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 17.8334 - MinusLogProbMetric: 17.8334 - val_loss: 17.9568 - val_MinusLogProbMetric: 17.9568 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 301/1000
2023-10-09 22:31:50.403 
Epoch 301/1000 
	 loss: 17.8293, MinusLogProbMetric: 17.8293, val_loss: 17.9034, val_MinusLogProbMetric: 17.9034

Epoch 301: val_loss improved from 17.95678 to 17.90336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 17.8293 - MinusLogProbMetric: 17.8293 - val_loss: 17.9034 - val_MinusLogProbMetric: 17.9034 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 302/1000
2023-10-09 22:32:54.714 
Epoch 302/1000 
	 loss: 17.8202, MinusLogProbMetric: 17.8202, val_loss: 18.0169, val_MinusLogProbMetric: 18.0169

Epoch 302: val_loss did not improve from 17.90336
196/196 - 63s - loss: 17.8202 - MinusLogProbMetric: 17.8202 - val_loss: 18.0169 - val_MinusLogProbMetric: 18.0169 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 303/1000
2023-10-09 22:33:55.525 
Epoch 303/1000 
	 loss: 17.8188, MinusLogProbMetric: 17.8188, val_loss: 17.9494, val_MinusLogProbMetric: 17.9494

Epoch 303: val_loss did not improve from 17.90336
196/196 - 61s - loss: 17.8188 - MinusLogProbMetric: 17.8188 - val_loss: 17.9494 - val_MinusLogProbMetric: 17.9494 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 304/1000
2023-10-09 22:34:58.987 
Epoch 304/1000 
	 loss: 17.8046, MinusLogProbMetric: 17.8046, val_loss: 17.9962, val_MinusLogProbMetric: 17.9962

Epoch 304: val_loss did not improve from 17.90336
196/196 - 63s - loss: 17.8046 - MinusLogProbMetric: 17.8046 - val_loss: 17.9962 - val_MinusLogProbMetric: 17.9962 - lr: 1.6667e-04 - 63s/epoch - 324ms/step
Epoch 305/1000
2023-10-09 22:36:00.366 
Epoch 305/1000 
	 loss: 17.8388, MinusLogProbMetric: 17.8388, val_loss: 17.9669, val_MinusLogProbMetric: 17.9669

Epoch 305: val_loss did not improve from 17.90336
196/196 - 61s - loss: 17.8388 - MinusLogProbMetric: 17.8388 - val_loss: 17.9669 - val_MinusLogProbMetric: 17.9669 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 306/1000
2023-10-09 22:37:01.203 
Epoch 306/1000 
	 loss: 17.7943, MinusLogProbMetric: 17.7943, val_loss: 17.9906, val_MinusLogProbMetric: 17.9906

Epoch 306: val_loss did not improve from 17.90336
196/196 - 61s - loss: 17.7943 - MinusLogProbMetric: 17.7943 - val_loss: 17.9906 - val_MinusLogProbMetric: 17.9906 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 307/1000
2023-10-09 22:38:02.030 
Epoch 307/1000 
	 loss: 17.7931, MinusLogProbMetric: 17.7931, val_loss: 17.9629, val_MinusLogProbMetric: 17.9629

Epoch 307: val_loss did not improve from 17.90336
196/196 - 61s - loss: 17.7931 - MinusLogProbMetric: 17.7931 - val_loss: 17.9629 - val_MinusLogProbMetric: 17.9629 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 308/1000
2023-10-09 22:39:03.699 
Epoch 308/1000 
	 loss: 17.7877, MinusLogProbMetric: 17.7877, val_loss: 17.8910, val_MinusLogProbMetric: 17.8910

Epoch 308: val_loss improved from 17.90336 to 17.89104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 17.7877 - MinusLogProbMetric: 17.7877 - val_loss: 17.8910 - val_MinusLogProbMetric: 17.8910 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 309/1000
2023-10-09 22:40:06.180 
Epoch 309/1000 
	 loss: 17.8127, MinusLogProbMetric: 17.8127, val_loss: 17.9241, val_MinusLogProbMetric: 17.9241

Epoch 309: val_loss did not improve from 17.89104
196/196 - 61s - loss: 17.8127 - MinusLogProbMetric: 17.8127 - val_loss: 17.9241 - val_MinusLogProbMetric: 17.9241 - lr: 1.6667e-04 - 61s/epoch - 314ms/step
Epoch 310/1000
2023-10-09 22:41:11.098 
Epoch 310/1000 
	 loss: 17.7892, MinusLogProbMetric: 17.7892, val_loss: 17.8674, val_MinusLogProbMetric: 17.8674

Epoch 310: val_loss improved from 17.89104 to 17.86737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 66s - loss: 17.7892 - MinusLogProbMetric: 17.7892 - val_loss: 17.8674 - val_MinusLogProbMetric: 17.8674 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 311/1000
2023-10-09 22:42:15.721 
Epoch 311/1000 
	 loss: 17.7999, MinusLogProbMetric: 17.7999, val_loss: 17.9071, val_MinusLogProbMetric: 17.9071

Epoch 311: val_loss did not improve from 17.86737
196/196 - 64s - loss: 17.7999 - MinusLogProbMetric: 17.7999 - val_loss: 17.9071 - val_MinusLogProbMetric: 17.9071 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 312/1000
2023-10-09 22:43:17.045 
Epoch 312/1000 
	 loss: 17.7845, MinusLogProbMetric: 17.7845, val_loss: 17.9243, val_MinusLogProbMetric: 17.9243

Epoch 312: val_loss did not improve from 17.86737
196/196 - 61s - loss: 17.7845 - MinusLogProbMetric: 17.7845 - val_loss: 17.9243 - val_MinusLogProbMetric: 17.9243 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 313/1000
2023-10-09 22:44:23.348 
Epoch 313/1000 
	 loss: 17.7832, MinusLogProbMetric: 17.7832, val_loss: 17.8935, val_MinusLogProbMetric: 17.8935

Epoch 313: val_loss did not improve from 17.86737
196/196 - 66s - loss: 17.7832 - MinusLogProbMetric: 17.7832 - val_loss: 17.8935 - val_MinusLogProbMetric: 17.8935 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 314/1000
2023-10-09 22:45:27.038 
Epoch 314/1000 
	 loss: 17.7631, MinusLogProbMetric: 17.7631, val_loss: 17.9106, val_MinusLogProbMetric: 17.9106

Epoch 314: val_loss did not improve from 17.86737
196/196 - 64s - loss: 17.7631 - MinusLogProbMetric: 17.7631 - val_loss: 17.9106 - val_MinusLogProbMetric: 17.9106 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 315/1000
2023-10-09 22:46:29.920 
Epoch 315/1000 
	 loss: 17.7613, MinusLogProbMetric: 17.7613, val_loss: 17.9203, val_MinusLogProbMetric: 17.9203

Epoch 315: val_loss did not improve from 17.86737
196/196 - 63s - loss: 17.7613 - MinusLogProbMetric: 17.7613 - val_loss: 17.9203 - val_MinusLogProbMetric: 17.9203 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 316/1000
2023-10-09 22:47:31.248 
Epoch 316/1000 
	 loss: 17.7893, MinusLogProbMetric: 17.7893, val_loss: 17.9254, val_MinusLogProbMetric: 17.9254

Epoch 316: val_loss did not improve from 17.86737
196/196 - 61s - loss: 17.7893 - MinusLogProbMetric: 17.7893 - val_loss: 17.9254 - val_MinusLogProbMetric: 17.9254 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 317/1000
2023-10-09 22:48:34.420 
Epoch 317/1000 
	 loss: 17.7618, MinusLogProbMetric: 17.7618, val_loss: 17.8826, val_MinusLogProbMetric: 17.8826

Epoch 317: val_loss did not improve from 17.86737
196/196 - 63s - loss: 17.7618 - MinusLogProbMetric: 17.7618 - val_loss: 17.8826 - val_MinusLogProbMetric: 17.8826 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 318/1000
2023-10-09 22:49:37.461 
Epoch 318/1000 
	 loss: 17.7522, MinusLogProbMetric: 17.7522, val_loss: 17.9561, val_MinusLogProbMetric: 17.9561

Epoch 318: val_loss did not improve from 17.86737
196/196 - 63s - loss: 17.7522 - MinusLogProbMetric: 17.7522 - val_loss: 17.9561 - val_MinusLogProbMetric: 17.9561 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 319/1000
2023-10-09 22:50:38.897 
Epoch 319/1000 
	 loss: 17.7616, MinusLogProbMetric: 17.7616, val_loss: 17.8951, val_MinusLogProbMetric: 17.8951

Epoch 319: val_loss did not improve from 17.86737
196/196 - 61s - loss: 17.7616 - MinusLogProbMetric: 17.7616 - val_loss: 17.8951 - val_MinusLogProbMetric: 17.8951 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 320/1000
2023-10-09 22:51:41.498 
Epoch 320/1000 
	 loss: 17.7478, MinusLogProbMetric: 17.7478, val_loss: 17.9472, val_MinusLogProbMetric: 17.9472

Epoch 320: val_loss did not improve from 17.86737
196/196 - 63s - loss: 17.7478 - MinusLogProbMetric: 17.7478 - val_loss: 17.9472 - val_MinusLogProbMetric: 17.9472 - lr: 1.6667e-04 - 63s/epoch - 319ms/step
Epoch 321/1000
2023-10-09 22:52:44.284 
Epoch 321/1000 
	 loss: 17.7741, MinusLogProbMetric: 17.7741, val_loss: 17.8770, val_MinusLogProbMetric: 17.8770

Epoch 321: val_loss did not improve from 17.86737
196/196 - 63s - loss: 17.7741 - MinusLogProbMetric: 17.7741 - val_loss: 17.8770 - val_MinusLogProbMetric: 17.8770 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 322/1000
2023-10-09 22:53:45.965 
Epoch 322/1000 
	 loss: 17.7474, MinusLogProbMetric: 17.7474, val_loss: 17.9859, val_MinusLogProbMetric: 17.9859

Epoch 322: val_loss did not improve from 17.86737
196/196 - 62s - loss: 17.7474 - MinusLogProbMetric: 17.7474 - val_loss: 17.9859 - val_MinusLogProbMetric: 17.9859 - lr: 1.6667e-04 - 62s/epoch - 315ms/step
Epoch 323/1000
2023-10-09 22:54:46.710 
Epoch 323/1000 
	 loss: 17.7488, MinusLogProbMetric: 17.7488, val_loss: 17.9554, val_MinusLogProbMetric: 17.9554

Epoch 323: val_loss did not improve from 17.86737
196/196 - 61s - loss: 17.7488 - MinusLogProbMetric: 17.7488 - val_loss: 17.9554 - val_MinusLogProbMetric: 17.9554 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 324/1000
2023-10-09 22:55:47.497 
Epoch 324/1000 
	 loss: 17.7334, MinusLogProbMetric: 17.7334, val_loss: 17.8744, val_MinusLogProbMetric: 17.8744

Epoch 324: val_loss did not improve from 17.86737
196/196 - 61s - loss: 17.7334 - MinusLogProbMetric: 17.7334 - val_loss: 17.8744 - val_MinusLogProbMetric: 17.8744 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 325/1000
2023-10-09 22:56:48.918 
Epoch 325/1000 
	 loss: 17.8288, MinusLogProbMetric: 17.8288, val_loss: 19.3368, val_MinusLogProbMetric: 19.3368

Epoch 325: val_loss did not improve from 17.86737
196/196 - 61s - loss: 17.8288 - MinusLogProbMetric: 17.8288 - val_loss: 19.3368 - val_MinusLogProbMetric: 19.3368 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 326/1000
2023-10-09 22:57:54.167 
Epoch 326/1000 
	 loss: 17.8201, MinusLogProbMetric: 17.8201, val_loss: 17.9231, val_MinusLogProbMetric: 17.9231

Epoch 326: val_loss did not improve from 17.86737
196/196 - 65s - loss: 17.8201 - MinusLogProbMetric: 17.8201 - val_loss: 17.9231 - val_MinusLogProbMetric: 17.9231 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 327/1000
2023-10-09 22:58:56.333 
Epoch 327/1000 
	 loss: 17.8560, MinusLogProbMetric: 17.8560, val_loss: 17.9165, val_MinusLogProbMetric: 17.9165

Epoch 327: val_loss did not improve from 17.86737
196/196 - 62s - loss: 17.8560 - MinusLogProbMetric: 17.8560 - val_loss: 17.9165 - val_MinusLogProbMetric: 17.9165 - lr: 1.6667e-04 - 62s/epoch - 317ms/step
Epoch 328/1000
2023-10-09 23:00:00.271 
Epoch 328/1000 
	 loss: 17.7282, MinusLogProbMetric: 17.7282, val_loss: 17.8825, val_MinusLogProbMetric: 17.8825

Epoch 328: val_loss did not improve from 17.86737
196/196 - 64s - loss: 17.7282 - MinusLogProbMetric: 17.7282 - val_loss: 17.8825 - val_MinusLogProbMetric: 17.8825 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 329/1000
2023-10-09 23:01:03.162 
Epoch 329/1000 
	 loss: 17.7222, MinusLogProbMetric: 17.7222, val_loss: 17.8644, val_MinusLogProbMetric: 17.8644

Epoch 329: val_loss improved from 17.86737 to 17.86444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 64s - loss: 17.7222 - MinusLogProbMetric: 17.7222 - val_loss: 17.8644 - val_MinusLogProbMetric: 17.8644 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 330/1000
2023-10-09 23:02:06.368 
Epoch 330/1000 
	 loss: 17.7258, MinusLogProbMetric: 17.7258, val_loss: 17.8483, val_MinusLogProbMetric: 17.8483

Epoch 330: val_loss improved from 17.86444 to 17.84827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 63s - loss: 17.7258 - MinusLogProbMetric: 17.7258 - val_loss: 17.8483 - val_MinusLogProbMetric: 17.8483 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 331/1000
2023-10-09 23:03:10.799 
Epoch 331/1000 
	 loss: 17.7076, MinusLogProbMetric: 17.7076, val_loss: 17.8549, val_MinusLogProbMetric: 17.8549

Epoch 331: val_loss did not improve from 17.84827
196/196 - 63s - loss: 17.7076 - MinusLogProbMetric: 17.7076 - val_loss: 17.8549 - val_MinusLogProbMetric: 17.8549 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 332/1000
2023-10-09 23:04:14.345 
Epoch 332/1000 
	 loss: 17.7124, MinusLogProbMetric: 17.7124, val_loss: 17.9014, val_MinusLogProbMetric: 17.9014

Epoch 332: val_loss did not improve from 17.84827
196/196 - 64s - loss: 17.7124 - MinusLogProbMetric: 17.7124 - val_loss: 17.9014 - val_MinusLogProbMetric: 17.9014 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 333/1000
2023-10-09 23:05:18.420 
Epoch 333/1000 
	 loss: 17.8701, MinusLogProbMetric: 17.8701, val_loss: 17.8501, val_MinusLogProbMetric: 17.8501

Epoch 333: val_loss did not improve from 17.84827
196/196 - 64s - loss: 17.8701 - MinusLogProbMetric: 17.8701 - val_loss: 17.8501 - val_MinusLogProbMetric: 17.8501 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 334/1000
2023-10-09 23:06:22.578 
Epoch 334/1000 
	 loss: 17.7260, MinusLogProbMetric: 17.7260, val_loss: 17.8933, val_MinusLogProbMetric: 17.8933

Epoch 334: val_loss did not improve from 17.84827
196/196 - 64s - loss: 17.7260 - MinusLogProbMetric: 17.7260 - val_loss: 17.8933 - val_MinusLogProbMetric: 17.8933 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 335/1000
2023-10-09 23:07:24.760 
Epoch 335/1000 
	 loss: 17.7073, MinusLogProbMetric: 17.7073, val_loss: 17.8752, val_MinusLogProbMetric: 17.8752

Epoch 335: val_loss did not improve from 17.84827
196/196 - 62s - loss: 17.7073 - MinusLogProbMetric: 17.7073 - val_loss: 17.8752 - val_MinusLogProbMetric: 17.8752 - lr: 1.6667e-04 - 62s/epoch - 317ms/step
Epoch 336/1000
2023-10-09 23:08:25.636 
Epoch 336/1000 
	 loss: 17.7132, MinusLogProbMetric: 17.7132, val_loss: 17.9003, val_MinusLogProbMetric: 17.9003

Epoch 336: val_loss did not improve from 17.84827
196/196 - 61s - loss: 17.7132 - MinusLogProbMetric: 17.7132 - val_loss: 17.9003 - val_MinusLogProbMetric: 17.9003 - lr: 1.6667e-04 - 61s/epoch - 311ms/step
Epoch 337/1000
2023-10-09 23:09:26.307 
Epoch 337/1000 
	 loss: 17.7128, MinusLogProbMetric: 17.7128, val_loss: 17.9190, val_MinusLogProbMetric: 17.9190

Epoch 337: val_loss did not improve from 17.84827
196/196 - 61s - loss: 17.7128 - MinusLogProbMetric: 17.7128 - val_loss: 17.9190 - val_MinusLogProbMetric: 17.9190 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 338/1000
2023-10-09 23:10:26.776 
Epoch 338/1000 
	 loss: 17.6873, MinusLogProbMetric: 17.6873, val_loss: 17.8981, val_MinusLogProbMetric: 17.8981

Epoch 338: val_loss did not improve from 17.84827
196/196 - 60s - loss: 17.6873 - MinusLogProbMetric: 17.6873 - val_loss: 17.8981 - val_MinusLogProbMetric: 17.8981 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 339/1000
2023-10-09 23:11:27.163 
Epoch 339/1000 
	 loss: 17.7641, MinusLogProbMetric: 17.7641, val_loss: 17.7942, val_MinusLogProbMetric: 17.7942

Epoch 339: val_loss improved from 17.84827 to 17.79418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 61s - loss: 17.7641 - MinusLogProbMetric: 17.7641 - val_loss: 17.7942 - val_MinusLogProbMetric: 17.7942 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 340/1000
2023-10-09 23:12:32.154 
Epoch 340/1000 
	 loss: 17.7173, MinusLogProbMetric: 17.7173, val_loss: 17.9181, val_MinusLogProbMetric: 17.9181

Epoch 340: val_loss did not improve from 17.79418
196/196 - 64s - loss: 17.7173 - MinusLogProbMetric: 17.7173 - val_loss: 17.9181 - val_MinusLogProbMetric: 17.9181 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 341/1000
2023-10-09 23:13:34.937 
Epoch 341/1000 
	 loss: 17.7050, MinusLogProbMetric: 17.7050, val_loss: 17.8197, val_MinusLogProbMetric: 17.8197

Epoch 341: val_loss did not improve from 17.79418
196/196 - 63s - loss: 17.7050 - MinusLogProbMetric: 17.7050 - val_loss: 17.8197 - val_MinusLogProbMetric: 17.8197 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 342/1000
2023-10-09 23:14:35.375 
Epoch 342/1000 
	 loss: 17.7231, MinusLogProbMetric: 17.7231, val_loss: 17.8369, val_MinusLogProbMetric: 17.8369

Epoch 342: val_loss did not improve from 17.79418
196/196 - 60s - loss: 17.7231 - MinusLogProbMetric: 17.7231 - val_loss: 17.8369 - val_MinusLogProbMetric: 17.8369 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 343/1000
2023-10-09 23:15:35.646 
Epoch 343/1000 
	 loss: 17.6973, MinusLogProbMetric: 17.6973, val_loss: 17.8375, val_MinusLogProbMetric: 17.8375

Epoch 343: val_loss did not improve from 17.79418
196/196 - 60s - loss: 17.6973 - MinusLogProbMetric: 17.6973 - val_loss: 17.8375 - val_MinusLogProbMetric: 17.8375 - lr: 1.6667e-04 - 60s/epoch - 307ms/step
Epoch 344/1000
2023-10-09 23:16:39.729 
Epoch 344/1000 
	 loss: 17.6954, MinusLogProbMetric: 17.6954, val_loss: 17.8466, val_MinusLogProbMetric: 17.8466

Epoch 344: val_loss did not improve from 17.79418
196/196 - 64s - loss: 17.6954 - MinusLogProbMetric: 17.6954 - val_loss: 17.8466 - val_MinusLogProbMetric: 17.8466 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 345/1000
2023-10-09 23:17:43.891 
Epoch 345/1000 
	 loss: 17.6798, MinusLogProbMetric: 17.6798, val_loss: 18.0114, val_MinusLogProbMetric: 18.0114

Epoch 345: val_loss did not improve from 17.79418
196/196 - 64s - loss: 17.6798 - MinusLogProbMetric: 17.6798 - val_loss: 18.0114 - val_MinusLogProbMetric: 18.0114 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 346/1000
2023-10-09 23:18:45.101 
Epoch 346/1000 
	 loss: 17.7076, MinusLogProbMetric: 17.7076, val_loss: 17.8820, val_MinusLogProbMetric: 17.8820

Epoch 346: val_loss did not improve from 17.79418
196/196 - 61s - loss: 17.7076 - MinusLogProbMetric: 17.7076 - val_loss: 17.8820 - val_MinusLogProbMetric: 17.8820 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 347/1000
2023-10-09 23:19:47.034 
Epoch 347/1000 
	 loss: 17.9279, MinusLogProbMetric: 17.9279, val_loss: 17.9066, val_MinusLogProbMetric: 17.9066

Epoch 347: val_loss did not improve from 17.79418
196/196 - 62s - loss: 17.9279 - MinusLogProbMetric: 17.9279 - val_loss: 17.9066 - val_MinusLogProbMetric: 17.9066 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 348/1000
2023-10-09 23:20:48.092 
Epoch 348/1000 
	 loss: 17.6822, MinusLogProbMetric: 17.6822, val_loss: 17.8873, val_MinusLogProbMetric: 17.8873

Epoch 348: val_loss did not improve from 17.79418
196/196 - 61s - loss: 17.6822 - MinusLogProbMetric: 17.6822 - val_loss: 17.8873 - val_MinusLogProbMetric: 17.8873 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 349/1000
2023-10-09 23:21:50.309 
Epoch 349/1000 
	 loss: 17.6834, MinusLogProbMetric: 17.6834, val_loss: 17.8391, val_MinusLogProbMetric: 17.8391

Epoch 349: val_loss did not improve from 17.79418
196/196 - 62s - loss: 17.6834 - MinusLogProbMetric: 17.6834 - val_loss: 17.8391 - val_MinusLogProbMetric: 17.8391 - lr: 1.6667e-04 - 62s/epoch - 317ms/step
Epoch 350/1000
2023-10-09 23:22:50.395 
Epoch 350/1000 
	 loss: 17.6791, MinusLogProbMetric: 17.6791, val_loss: 18.0022, val_MinusLogProbMetric: 18.0022

Epoch 350: val_loss did not improve from 17.79418
196/196 - 60s - loss: 17.6791 - MinusLogProbMetric: 17.6791 - val_loss: 18.0022 - val_MinusLogProbMetric: 18.0022 - lr: 1.6667e-04 - 60s/epoch - 307ms/step
Epoch 351/1000
2023-10-09 23:23:52.726 
Epoch 351/1000 
	 loss: 17.6797, MinusLogProbMetric: 17.6797, val_loss: 17.9101, val_MinusLogProbMetric: 17.9101

Epoch 351: val_loss did not improve from 17.79418
196/196 - 62s - loss: 17.6797 - MinusLogProbMetric: 17.6797 - val_loss: 17.9101 - val_MinusLogProbMetric: 17.9101 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 352/1000
2023-10-09 23:24:54.585 
Epoch 352/1000 
	 loss: 17.6666, MinusLogProbMetric: 17.6666, val_loss: 17.8332, val_MinusLogProbMetric: 17.8332

Epoch 352: val_loss did not improve from 17.79418
196/196 - 62s - loss: 17.6666 - MinusLogProbMetric: 17.6666 - val_loss: 17.8332 - val_MinusLogProbMetric: 17.8332 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 353/1000
2023-10-09 23:25:56.899 
Epoch 353/1000 
	 loss: 17.6726, MinusLogProbMetric: 17.6726, val_loss: 17.8935, val_MinusLogProbMetric: 17.8935

Epoch 353: val_loss did not improve from 17.79418
196/196 - 62s - loss: 17.6726 - MinusLogProbMetric: 17.6726 - val_loss: 17.8935 - val_MinusLogProbMetric: 17.8935 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 354/1000
2023-10-09 23:26:58.165 
Epoch 354/1000 
	 loss: 17.6782, MinusLogProbMetric: 17.6782, val_loss: 17.8925, val_MinusLogProbMetric: 17.8925

Epoch 354: val_loss did not improve from 17.79418
196/196 - 61s - loss: 17.6782 - MinusLogProbMetric: 17.6782 - val_loss: 17.8925 - val_MinusLogProbMetric: 17.8925 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 355/1000
2023-10-09 23:28:00.851 
Epoch 355/1000 
	 loss: 17.6380, MinusLogProbMetric: 17.6380, val_loss: 17.8144, val_MinusLogProbMetric: 17.8144

Epoch 355: val_loss did not improve from 17.79418
196/196 - 63s - loss: 17.6380 - MinusLogProbMetric: 17.6380 - val_loss: 17.8144 - val_MinusLogProbMetric: 17.8144 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 356/1000
2023-10-09 23:29:02.158 
Epoch 356/1000 
	 loss: 17.6675, MinusLogProbMetric: 17.6675, val_loss: 17.8557, val_MinusLogProbMetric: 17.8557

Epoch 356: val_loss did not improve from 17.79418
196/196 - 61s - loss: 17.6675 - MinusLogProbMetric: 17.6675 - val_loss: 17.8557 - val_MinusLogProbMetric: 17.8557 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 357/1000
2023-10-09 23:30:05.782 
Epoch 357/1000 
	 loss: 17.6334, MinusLogProbMetric: 17.6334, val_loss: 17.7479, val_MinusLogProbMetric: 17.7479

Epoch 357: val_loss improved from 17.79418 to 17.74790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 65s - loss: 17.6334 - MinusLogProbMetric: 17.6334 - val_loss: 17.7479 - val_MinusLogProbMetric: 17.7479 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 358/1000
2023-10-09 23:31:11.636 
Epoch 358/1000 
	 loss: 17.6365, MinusLogProbMetric: 17.6365, val_loss: 17.8699, val_MinusLogProbMetric: 17.8699

Epoch 358: val_loss did not improve from 17.74790
196/196 - 65s - loss: 17.6365 - MinusLogProbMetric: 17.6365 - val_loss: 17.8699 - val_MinusLogProbMetric: 17.8699 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 359/1000
2023-10-09 23:32:12.217 
Epoch 359/1000 
	 loss: 17.6478, MinusLogProbMetric: 17.6478, val_loss: 17.8635, val_MinusLogProbMetric: 17.8635

Epoch 359: val_loss did not improve from 17.74790
196/196 - 61s - loss: 17.6478 - MinusLogProbMetric: 17.6478 - val_loss: 17.8635 - val_MinusLogProbMetric: 17.8635 - lr: 1.6667e-04 - 61s/epoch - 309ms/step
Epoch 360/1000
2023-10-09 23:33:15.281 
Epoch 360/1000 
	 loss: 17.6308, MinusLogProbMetric: 17.6308, val_loss: 17.7563, val_MinusLogProbMetric: 17.7563

Epoch 360: val_loss did not improve from 17.74790
196/196 - 63s - loss: 17.6308 - MinusLogProbMetric: 17.6308 - val_loss: 17.7563 - val_MinusLogProbMetric: 17.7563 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 361/1000
2023-10-09 23:34:17.468 
Epoch 361/1000 
	 loss: 18.1640, MinusLogProbMetric: 18.1640, val_loss: 18.3188, val_MinusLogProbMetric: 18.3188

Epoch 361: val_loss did not improve from 17.74790
196/196 - 62s - loss: 18.1640 - MinusLogProbMetric: 18.1640 - val_loss: 18.3188 - val_MinusLogProbMetric: 18.3188 - lr: 1.6667e-04 - 62s/epoch - 317ms/step
Epoch 362/1000
2023-10-09 23:35:22.575 
Epoch 362/1000 
	 loss: 17.7040, MinusLogProbMetric: 17.7040, val_loss: 17.7536, val_MinusLogProbMetric: 17.7536

Epoch 362: val_loss did not improve from 17.74790
196/196 - 65s - loss: 17.7040 - MinusLogProbMetric: 17.7040 - val_loss: 17.7536 - val_MinusLogProbMetric: 17.7536 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 363/1000
2023-10-09 23:36:26.105 
Epoch 363/1000 
	 loss: 17.6277, MinusLogProbMetric: 17.6277, val_loss: 17.8199, val_MinusLogProbMetric: 17.8199

Epoch 363: val_loss did not improve from 17.74790
196/196 - 64s - loss: 17.6277 - MinusLogProbMetric: 17.6277 - val_loss: 17.8199 - val_MinusLogProbMetric: 17.8199 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 364/1000
2023-10-09 23:37:27.909 
Epoch 364/1000 
	 loss: 17.8243, MinusLogProbMetric: 17.8243, val_loss: 17.7847, val_MinusLogProbMetric: 17.7847

Epoch 364: val_loss did not improve from 17.74790
196/196 - 62s - loss: 17.8243 - MinusLogProbMetric: 17.8243 - val_loss: 17.7847 - val_MinusLogProbMetric: 17.7847 - lr: 1.6667e-04 - 62s/epoch - 315ms/step
Epoch 365/1000
2023-10-09 23:38:32.770 
Epoch 365/1000 
	 loss: 17.6273, MinusLogProbMetric: 17.6273, val_loss: 17.7672, val_MinusLogProbMetric: 17.7672

Epoch 365: val_loss did not improve from 17.74790
196/196 - 65s - loss: 17.6273 - MinusLogProbMetric: 17.6273 - val_loss: 17.7672 - val_MinusLogProbMetric: 17.7672 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 366/1000
2023-10-09 23:39:33.385 
Epoch 366/1000 
	 loss: 17.6362, MinusLogProbMetric: 17.6362, val_loss: 17.7682, val_MinusLogProbMetric: 17.7682

Epoch 366: val_loss did not improve from 17.74790
196/196 - 61s - loss: 17.6362 - MinusLogProbMetric: 17.6362 - val_loss: 17.7682 - val_MinusLogProbMetric: 17.7682 - lr: 1.6667e-04 - 61s/epoch - 309ms/step
Epoch 367/1000
2023-10-09 23:40:36.480 
Epoch 367/1000 
	 loss: 17.6324, MinusLogProbMetric: 17.6324, val_loss: 17.7723, val_MinusLogProbMetric: 17.7723

Epoch 367: val_loss did not improve from 17.74790
196/196 - 63s - loss: 17.6324 - MinusLogProbMetric: 17.6324 - val_loss: 17.7723 - val_MinusLogProbMetric: 17.7723 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 368/1000
2023-10-09 23:41:36.792 
Epoch 368/1000 
	 loss: 17.6202, MinusLogProbMetric: 17.6202, val_loss: 17.8256, val_MinusLogProbMetric: 17.8256

Epoch 368: val_loss did not improve from 17.74790
196/196 - 60s - loss: 17.6202 - MinusLogProbMetric: 17.6202 - val_loss: 17.8256 - val_MinusLogProbMetric: 17.8256 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 369/1000
2023-10-09 23:42:38.892 
Epoch 369/1000 
	 loss: 17.6250, MinusLogProbMetric: 17.6250, val_loss: 17.7595, val_MinusLogProbMetric: 17.7595

Epoch 369: val_loss did not improve from 17.74790
196/196 - 62s - loss: 17.6250 - MinusLogProbMetric: 17.6250 - val_loss: 17.7595 - val_MinusLogProbMetric: 17.7595 - lr: 1.6667e-04 - 62s/epoch - 317ms/step
Epoch 370/1000
2023-10-09 23:43:41.533 
Epoch 370/1000 
	 loss: 17.6224, MinusLogProbMetric: 17.6224, val_loss: 17.7728, val_MinusLogProbMetric: 17.7728

Epoch 370: val_loss did not improve from 17.74790
196/196 - 63s - loss: 17.6224 - MinusLogProbMetric: 17.6224 - val_loss: 17.7728 - val_MinusLogProbMetric: 17.7728 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 371/1000
2023-10-09 23:44:42.071 
Epoch 371/1000 
	 loss: 17.6051, MinusLogProbMetric: 17.6051, val_loss: 17.7023, val_MinusLogProbMetric: 17.7023

Epoch 371: val_loss improved from 17.74790 to 17.70230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 62s - loss: 17.6051 - MinusLogProbMetric: 17.6051 - val_loss: 17.7023 - val_MinusLogProbMetric: 17.7023 - lr: 1.6667e-04 - 62s/epoch - 315ms/step
Epoch 372/1000
2023-10-09 23:45:45.478 
Epoch 372/1000 
	 loss: 17.6203, MinusLogProbMetric: 17.6203, val_loss: 17.7737, val_MinusLogProbMetric: 17.7737

Epoch 372: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.6203 - MinusLogProbMetric: 17.6203 - val_loss: 17.7737 - val_MinusLogProbMetric: 17.7737 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 373/1000
2023-10-09 23:46:49.478 
Epoch 373/1000 
	 loss: 17.6047, MinusLogProbMetric: 17.6047, val_loss: 17.8514, val_MinusLogProbMetric: 17.8514

Epoch 373: val_loss did not improve from 17.70230
196/196 - 64s - loss: 17.6047 - MinusLogProbMetric: 17.6047 - val_loss: 17.8514 - val_MinusLogProbMetric: 17.8514 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 374/1000
2023-10-09 23:47:53.415 
Epoch 374/1000 
	 loss: 17.6223, MinusLogProbMetric: 17.6223, val_loss: 17.8356, val_MinusLogProbMetric: 17.8356

Epoch 374: val_loss did not improve from 17.70230
196/196 - 64s - loss: 17.6223 - MinusLogProbMetric: 17.6223 - val_loss: 17.8356 - val_MinusLogProbMetric: 17.8356 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 375/1000
2023-10-09 23:48:55.698 
Epoch 375/1000 
	 loss: 17.7192, MinusLogProbMetric: 17.7192, val_loss: 20.0389, val_MinusLogProbMetric: 20.0389

Epoch 375: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.7192 - MinusLogProbMetric: 17.7192 - val_loss: 20.0389 - val_MinusLogProbMetric: 20.0389 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 376/1000
2023-10-09 23:49:57.652 
Epoch 376/1000 
	 loss: 17.8188, MinusLogProbMetric: 17.8188, val_loss: 17.8575, val_MinusLogProbMetric: 17.8575

Epoch 376: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.8188 - MinusLogProbMetric: 17.8188 - val_loss: 17.8575 - val_MinusLogProbMetric: 17.8575 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 377/1000
2023-10-09 23:50:58.896 
Epoch 377/1000 
	 loss: 17.6091, MinusLogProbMetric: 17.6091, val_loss: 17.8604, val_MinusLogProbMetric: 17.8604

Epoch 377: val_loss did not improve from 17.70230
196/196 - 61s - loss: 17.6091 - MinusLogProbMetric: 17.6091 - val_loss: 17.8604 - val_MinusLogProbMetric: 17.8604 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 378/1000
2023-10-09 23:52:01.713 
Epoch 378/1000 
	 loss: 17.6084, MinusLogProbMetric: 17.6084, val_loss: 17.7590, val_MinusLogProbMetric: 17.7590

Epoch 378: val_loss did not improve from 17.70230
196/196 - 63s - loss: 17.6084 - MinusLogProbMetric: 17.6084 - val_loss: 17.7590 - val_MinusLogProbMetric: 17.7590 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 379/1000
2023-10-09 23:53:03.721 
Epoch 379/1000 
	 loss: 17.6218, MinusLogProbMetric: 17.6218, val_loss: 17.8523, val_MinusLogProbMetric: 17.8523

Epoch 379: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.6218 - MinusLogProbMetric: 17.6218 - val_loss: 17.8523 - val_MinusLogProbMetric: 17.8523 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 380/1000
2023-10-09 23:54:08.241 
Epoch 380/1000 
	 loss: 17.5959, MinusLogProbMetric: 17.5959, val_loss: 17.7159, val_MinusLogProbMetric: 17.7159

Epoch 380: val_loss did not improve from 17.70230
196/196 - 65s - loss: 17.5959 - MinusLogProbMetric: 17.5959 - val_loss: 17.7159 - val_MinusLogProbMetric: 17.7159 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 381/1000
2023-10-09 23:55:09.720 
Epoch 381/1000 
	 loss: 17.5937, MinusLogProbMetric: 17.5937, val_loss: 17.7816, val_MinusLogProbMetric: 17.7816

Epoch 381: val_loss did not improve from 17.70230
196/196 - 61s - loss: 17.5937 - MinusLogProbMetric: 17.5937 - val_loss: 17.7816 - val_MinusLogProbMetric: 17.7816 - lr: 1.6667e-04 - 61s/epoch - 314ms/step
Epoch 382/1000
2023-10-09 23:56:13.274 
Epoch 382/1000 
	 loss: 17.6049, MinusLogProbMetric: 17.6049, val_loss: 17.7739, val_MinusLogProbMetric: 17.7739

Epoch 382: val_loss did not improve from 17.70230
196/196 - 64s - loss: 17.6049 - MinusLogProbMetric: 17.6049 - val_loss: 17.7739 - val_MinusLogProbMetric: 17.7739 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 383/1000
2023-10-09 23:57:16.199 
Epoch 383/1000 
	 loss: 17.6005, MinusLogProbMetric: 17.6005, val_loss: 17.8662, val_MinusLogProbMetric: 17.8662

Epoch 383: val_loss did not improve from 17.70230
196/196 - 63s - loss: 17.6005 - MinusLogProbMetric: 17.6005 - val_loss: 17.8662 - val_MinusLogProbMetric: 17.8662 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 384/1000
2023-10-09 23:58:20.133 
Epoch 384/1000 
	 loss: 17.6027, MinusLogProbMetric: 17.6027, val_loss: 17.8426, val_MinusLogProbMetric: 17.8426

Epoch 384: val_loss did not improve from 17.70230
196/196 - 64s - loss: 17.6027 - MinusLogProbMetric: 17.6027 - val_loss: 17.8426 - val_MinusLogProbMetric: 17.8426 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 385/1000
2023-10-09 23:59:21.157 
Epoch 385/1000 
	 loss: 17.6058, MinusLogProbMetric: 17.6058, val_loss: 17.8261, val_MinusLogProbMetric: 17.8261

Epoch 385: val_loss did not improve from 17.70230
196/196 - 61s - loss: 17.6058 - MinusLogProbMetric: 17.6058 - val_loss: 17.8261 - val_MinusLogProbMetric: 17.8261 - lr: 1.6667e-04 - 61s/epoch - 311ms/step
Epoch 386/1000
2023-10-10 00:00:27.003 
Epoch 386/1000 
	 loss: 17.5905, MinusLogProbMetric: 17.5905, val_loss: 17.9278, val_MinusLogProbMetric: 17.9278

Epoch 386: val_loss did not improve from 17.70230
196/196 - 66s - loss: 17.5905 - MinusLogProbMetric: 17.5905 - val_loss: 17.9278 - val_MinusLogProbMetric: 17.9278 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 387/1000
2023-10-10 00:01:29.318 
Epoch 387/1000 
	 loss: 17.6075, MinusLogProbMetric: 17.6075, val_loss: 17.7759, val_MinusLogProbMetric: 17.7759

Epoch 387: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.6075 - MinusLogProbMetric: 17.6075 - val_loss: 17.7759 - val_MinusLogProbMetric: 17.7759 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 388/1000
2023-10-10 00:02:29.749 
Epoch 388/1000 
	 loss: 17.5934, MinusLogProbMetric: 17.5934, val_loss: 17.7872, val_MinusLogProbMetric: 17.7872

Epoch 388: val_loss did not improve from 17.70230
196/196 - 60s - loss: 17.5934 - MinusLogProbMetric: 17.5934 - val_loss: 17.7872 - val_MinusLogProbMetric: 17.7872 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 389/1000
2023-10-10 00:03:32.455 
Epoch 389/1000 
	 loss: 17.5792, MinusLogProbMetric: 17.5792, val_loss: 17.7593, val_MinusLogProbMetric: 17.7593

Epoch 389: val_loss did not improve from 17.70230
196/196 - 63s - loss: 17.5792 - MinusLogProbMetric: 17.5792 - val_loss: 17.7593 - val_MinusLogProbMetric: 17.7593 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 390/1000
2023-10-10 00:04:32.689 
Epoch 390/1000 
	 loss: 17.5976, MinusLogProbMetric: 17.5976, val_loss: 17.7263, val_MinusLogProbMetric: 17.7263

Epoch 390: val_loss did not improve from 17.70230
196/196 - 60s - loss: 17.5976 - MinusLogProbMetric: 17.5976 - val_loss: 17.7263 - val_MinusLogProbMetric: 17.7263 - lr: 1.6667e-04 - 60s/epoch - 307ms/step
Epoch 391/1000
2023-10-10 00:05:32.266 
Epoch 391/1000 
	 loss: 17.5669, MinusLogProbMetric: 17.5669, val_loss: 17.7032, val_MinusLogProbMetric: 17.7032

Epoch 391: val_loss did not improve from 17.70230
196/196 - 60s - loss: 17.5669 - MinusLogProbMetric: 17.5669 - val_loss: 17.7032 - val_MinusLogProbMetric: 17.7032 - lr: 1.6667e-04 - 60s/epoch - 304ms/step
Epoch 392/1000
2023-10-10 00:06:32.116 
Epoch 392/1000 
	 loss: 17.6770, MinusLogProbMetric: 17.6770, val_loss: 17.7159, val_MinusLogProbMetric: 17.7159

Epoch 392: val_loss did not improve from 17.70230
196/196 - 60s - loss: 17.6770 - MinusLogProbMetric: 17.6770 - val_loss: 17.7159 - val_MinusLogProbMetric: 17.7159 - lr: 1.6667e-04 - 60s/epoch - 305ms/step
Epoch 393/1000
2023-10-10 00:07:31.454 
Epoch 393/1000 
	 loss: 17.6241, MinusLogProbMetric: 17.6241, val_loss: 17.8034, val_MinusLogProbMetric: 17.8034

Epoch 393: val_loss did not improve from 17.70230
196/196 - 59s - loss: 17.6241 - MinusLogProbMetric: 17.6241 - val_loss: 17.8034 - val_MinusLogProbMetric: 17.8034 - lr: 1.6667e-04 - 59s/epoch - 303ms/step
Epoch 394/1000
2023-10-10 00:08:30.299 
Epoch 394/1000 
	 loss: 17.5785, MinusLogProbMetric: 17.5785, val_loss: 17.7455, val_MinusLogProbMetric: 17.7455

Epoch 394: val_loss did not improve from 17.70230
196/196 - 59s - loss: 17.5785 - MinusLogProbMetric: 17.5785 - val_loss: 17.7455 - val_MinusLogProbMetric: 17.7455 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 395/1000
2023-10-10 00:09:30.901 
Epoch 395/1000 
	 loss: 17.5651, MinusLogProbMetric: 17.5651, val_loss: 17.7715, val_MinusLogProbMetric: 17.7715

Epoch 395: val_loss did not improve from 17.70230
196/196 - 61s - loss: 17.5651 - MinusLogProbMetric: 17.5651 - val_loss: 17.7715 - val_MinusLogProbMetric: 17.7715 - lr: 1.6667e-04 - 61s/epoch - 309ms/step
Epoch 396/1000
2023-10-10 00:10:32.606 
Epoch 396/1000 
	 loss: 17.5693, MinusLogProbMetric: 17.5693, val_loss: 17.8025, val_MinusLogProbMetric: 17.8025

Epoch 396: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.5693 - MinusLogProbMetric: 17.5693 - val_loss: 17.8025 - val_MinusLogProbMetric: 17.8025 - lr: 1.6667e-04 - 62s/epoch - 315ms/step
Epoch 397/1000
2023-10-10 00:11:34.574 
Epoch 397/1000 
	 loss: 17.5995, MinusLogProbMetric: 17.5995, val_loss: 17.7804, val_MinusLogProbMetric: 17.7804

Epoch 397: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.5995 - MinusLogProbMetric: 17.5995 - val_loss: 17.7804 - val_MinusLogProbMetric: 17.7804 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 398/1000
2023-10-10 00:12:37.032 
Epoch 398/1000 
	 loss: 17.5709, MinusLogProbMetric: 17.5709, val_loss: 17.7258, val_MinusLogProbMetric: 17.7258

Epoch 398: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.5709 - MinusLogProbMetric: 17.5709 - val_loss: 17.7258 - val_MinusLogProbMetric: 17.7258 - lr: 1.6667e-04 - 62s/epoch - 319ms/step
Epoch 399/1000
2023-10-10 00:13:38.909 
Epoch 399/1000 
	 loss: 17.5769, MinusLogProbMetric: 17.5769, val_loss: 17.7047, val_MinusLogProbMetric: 17.7047

Epoch 399: val_loss did not improve from 17.70230
196/196 - 62s - loss: 17.5769 - MinusLogProbMetric: 17.5769 - val_loss: 17.7047 - val_MinusLogProbMetric: 17.7047 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 400/1000
2023-10-10 00:14:36.747 
Epoch 400/1000 
	 loss: 17.5844, MinusLogProbMetric: 17.5844, val_loss: 17.7223, val_MinusLogProbMetric: 17.7223

Epoch 400: val_loss did not improve from 17.70230
196/196 - 58s - loss: 17.5844 - MinusLogProbMetric: 17.5844 - val_loss: 17.7223 - val_MinusLogProbMetric: 17.7223 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 401/1000
2023-10-10 00:15:34.508 
Epoch 401/1000 
	 loss: 17.5619, MinusLogProbMetric: 17.5619, val_loss: 17.6881, val_MinusLogProbMetric: 17.6881

Epoch 401: val_loss improved from 17.70230 to 17.68807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.5619 - MinusLogProbMetric: 17.5619 - val_loss: 17.6881 - val_MinusLogProbMetric: 17.6881 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 402/1000
2023-10-10 00:16:32.847 
Epoch 402/1000 
	 loss: 17.5527, MinusLogProbMetric: 17.5527, val_loss: 17.7315, val_MinusLogProbMetric: 17.7315

Epoch 402: val_loss did not improve from 17.68807
196/196 - 57s - loss: 17.5527 - MinusLogProbMetric: 17.5527 - val_loss: 17.7315 - val_MinusLogProbMetric: 17.7315 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 403/1000
2023-10-10 00:17:31.379 
Epoch 403/1000 
	 loss: 17.5564, MinusLogProbMetric: 17.5564, val_loss: 17.8800, val_MinusLogProbMetric: 17.8800

Epoch 403: val_loss did not improve from 17.68807
196/196 - 59s - loss: 17.5564 - MinusLogProbMetric: 17.5564 - val_loss: 17.8800 - val_MinusLogProbMetric: 17.8800 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 404/1000
2023-10-10 00:18:29.965 
Epoch 404/1000 
	 loss: 17.5584, MinusLogProbMetric: 17.5584, val_loss: 17.6802, val_MinusLogProbMetric: 17.6802

Epoch 404: val_loss improved from 17.68807 to 17.68015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 17.5584 - MinusLogProbMetric: 17.5584 - val_loss: 17.6802 - val_MinusLogProbMetric: 17.6802 - lr: 1.6667e-04 - 60s/epoch - 304ms/step
Epoch 405/1000
2023-10-10 00:19:30.354 
Epoch 405/1000 
	 loss: 17.5568, MinusLogProbMetric: 17.5568, val_loss: 17.7382, val_MinusLogProbMetric: 17.7382

Epoch 405: val_loss did not improve from 17.68015
196/196 - 59s - loss: 17.5568 - MinusLogProbMetric: 17.5568 - val_loss: 17.7382 - val_MinusLogProbMetric: 17.7382 - lr: 1.6667e-04 - 59s/epoch - 303ms/step
Epoch 406/1000
2023-10-10 00:20:28.894 
Epoch 406/1000 
	 loss: 17.5646, MinusLogProbMetric: 17.5646, val_loss: 17.7033, val_MinusLogProbMetric: 17.7033

Epoch 406: val_loss did not improve from 17.68015
196/196 - 59s - loss: 17.5646 - MinusLogProbMetric: 17.5646 - val_loss: 17.7033 - val_MinusLogProbMetric: 17.7033 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 407/1000
2023-10-10 00:21:26.430 
Epoch 407/1000 
	 loss: 17.6080, MinusLogProbMetric: 17.6080, val_loss: 17.6636, val_MinusLogProbMetric: 17.6636

Epoch 407: val_loss improved from 17.68015 to 17.66361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.6080 - MinusLogProbMetric: 17.6080 - val_loss: 17.6636 - val_MinusLogProbMetric: 17.6636 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 408/1000
2023-10-10 00:22:25.278 
Epoch 408/1000 
	 loss: 17.5970, MinusLogProbMetric: 17.5970, val_loss: 17.8051, val_MinusLogProbMetric: 17.8051

Epoch 408: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5970 - MinusLogProbMetric: 17.5970 - val_loss: 17.8051 - val_MinusLogProbMetric: 17.8051 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 409/1000
2023-10-10 00:23:23.171 
Epoch 409/1000 
	 loss: 17.5488, MinusLogProbMetric: 17.5488, val_loss: 17.8516, val_MinusLogProbMetric: 17.8516

Epoch 409: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5488 - MinusLogProbMetric: 17.5488 - val_loss: 17.8516 - val_MinusLogProbMetric: 17.8516 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 410/1000
2023-10-10 00:24:20.196 
Epoch 410/1000 
	 loss: 17.5490, MinusLogProbMetric: 17.5490, val_loss: 17.6903, val_MinusLogProbMetric: 17.6903

Epoch 410: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5490 - MinusLogProbMetric: 17.5490 - val_loss: 17.6903 - val_MinusLogProbMetric: 17.6903 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 411/1000
2023-10-10 00:25:18.008 
Epoch 411/1000 
	 loss: 17.5486, MinusLogProbMetric: 17.5486, val_loss: 17.6886, val_MinusLogProbMetric: 17.6886

Epoch 411: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5486 - MinusLogProbMetric: 17.5486 - val_loss: 17.6886 - val_MinusLogProbMetric: 17.6886 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 412/1000
2023-10-10 00:26:15.390 
Epoch 412/1000 
	 loss: 17.5415, MinusLogProbMetric: 17.5415, val_loss: 17.7153, val_MinusLogProbMetric: 17.7153

Epoch 412: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5415 - MinusLogProbMetric: 17.5415 - val_loss: 17.7153 - val_MinusLogProbMetric: 17.7153 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 413/1000
2023-10-10 00:27:11.911 
Epoch 413/1000 
	 loss: 17.5399, MinusLogProbMetric: 17.5399, val_loss: 17.6694, val_MinusLogProbMetric: 17.6694

Epoch 413: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5399 - MinusLogProbMetric: 17.5399 - val_loss: 17.6694 - val_MinusLogProbMetric: 17.6694 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 414/1000
2023-10-10 00:28:10.472 
Epoch 414/1000 
	 loss: 17.5371, MinusLogProbMetric: 17.5371, val_loss: 17.6908, val_MinusLogProbMetric: 17.6908

Epoch 414: val_loss did not improve from 17.66361
196/196 - 59s - loss: 17.5371 - MinusLogProbMetric: 17.5371 - val_loss: 17.6908 - val_MinusLogProbMetric: 17.6908 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 415/1000
2023-10-10 00:29:08.037 
Epoch 415/1000 
	 loss: 17.5355, MinusLogProbMetric: 17.5355, val_loss: 17.7105, val_MinusLogProbMetric: 17.7105

Epoch 415: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5355 - MinusLogProbMetric: 17.5355 - val_loss: 17.7105 - val_MinusLogProbMetric: 17.7105 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 416/1000
2023-10-10 00:30:05.762 
Epoch 416/1000 
	 loss: 17.5393, MinusLogProbMetric: 17.5393, val_loss: 17.7241, val_MinusLogProbMetric: 17.7241

Epoch 416: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5393 - MinusLogProbMetric: 17.5393 - val_loss: 17.7241 - val_MinusLogProbMetric: 17.7241 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 417/1000
2023-10-10 00:31:03.185 
Epoch 417/1000 
	 loss: 17.5318, MinusLogProbMetric: 17.5318, val_loss: 17.7101, val_MinusLogProbMetric: 17.7101

Epoch 417: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5318 - MinusLogProbMetric: 17.5318 - val_loss: 17.7101 - val_MinusLogProbMetric: 17.7101 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 418/1000
2023-10-10 00:32:01.815 
Epoch 418/1000 
	 loss: 17.5722, MinusLogProbMetric: 17.5722, val_loss: 17.7054, val_MinusLogProbMetric: 17.7054

Epoch 418: val_loss did not improve from 17.66361
196/196 - 59s - loss: 17.5722 - MinusLogProbMetric: 17.5722 - val_loss: 17.7054 - val_MinusLogProbMetric: 17.7054 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 419/1000
2023-10-10 00:33:00.414 
Epoch 419/1000 
	 loss: 17.5431, MinusLogProbMetric: 17.5431, val_loss: 17.6840, val_MinusLogProbMetric: 17.6840

Epoch 419: val_loss did not improve from 17.66361
196/196 - 59s - loss: 17.5431 - MinusLogProbMetric: 17.5431 - val_loss: 17.6840 - val_MinusLogProbMetric: 17.6840 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 420/1000
2023-10-10 00:33:57.755 
Epoch 420/1000 
	 loss: 17.5225, MinusLogProbMetric: 17.5225, val_loss: 17.6712, val_MinusLogProbMetric: 17.6712

Epoch 420: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5225 - MinusLogProbMetric: 17.5225 - val_loss: 17.6712 - val_MinusLogProbMetric: 17.6712 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 421/1000
2023-10-10 00:34:54.377 
Epoch 421/1000 
	 loss: 17.5293, MinusLogProbMetric: 17.5293, val_loss: 17.7394, val_MinusLogProbMetric: 17.7394

Epoch 421: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5293 - MinusLogProbMetric: 17.5293 - val_loss: 17.7394 - val_MinusLogProbMetric: 17.7394 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 422/1000
2023-10-10 00:35:52.058 
Epoch 422/1000 
	 loss: 17.5330, MinusLogProbMetric: 17.5330, val_loss: 17.7053, val_MinusLogProbMetric: 17.7053

Epoch 422: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5330 - MinusLogProbMetric: 17.5330 - val_loss: 17.7053 - val_MinusLogProbMetric: 17.7053 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 423/1000
2023-10-10 00:36:49.574 
Epoch 423/1000 
	 loss: 17.5473, MinusLogProbMetric: 17.5473, val_loss: 17.6875, val_MinusLogProbMetric: 17.6875

Epoch 423: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5473 - MinusLogProbMetric: 17.5473 - val_loss: 17.6875 - val_MinusLogProbMetric: 17.6875 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 424/1000
2023-10-10 00:37:46.744 
Epoch 424/1000 
	 loss: 17.5181, MinusLogProbMetric: 17.5181, val_loss: 18.1155, val_MinusLogProbMetric: 18.1155

Epoch 424: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5181 - MinusLogProbMetric: 17.5181 - val_loss: 18.1155 - val_MinusLogProbMetric: 18.1155 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 425/1000
2023-10-10 00:38:44.117 
Epoch 425/1000 
	 loss: 17.5295, MinusLogProbMetric: 17.5295, val_loss: 17.6841, val_MinusLogProbMetric: 17.6841

Epoch 425: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5295 - MinusLogProbMetric: 17.5295 - val_loss: 17.6841 - val_MinusLogProbMetric: 17.6841 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 426/1000
2023-10-10 00:39:41.948 
Epoch 426/1000 
	 loss: 17.6919, MinusLogProbMetric: 17.6919, val_loss: 17.6837, val_MinusLogProbMetric: 17.6837

Epoch 426: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.6919 - MinusLogProbMetric: 17.6919 - val_loss: 17.6837 - val_MinusLogProbMetric: 17.6837 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 427/1000
2023-10-10 00:40:39.472 
Epoch 427/1000 
	 loss: 17.5107, MinusLogProbMetric: 17.5107, val_loss: 17.7880, val_MinusLogProbMetric: 17.7880

Epoch 427: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5107 - MinusLogProbMetric: 17.5107 - val_loss: 17.7880 - val_MinusLogProbMetric: 17.7880 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 428/1000
2023-10-10 00:41:37.337 
Epoch 428/1000 
	 loss: 17.5436, MinusLogProbMetric: 17.5436, val_loss: 17.7239, val_MinusLogProbMetric: 17.7239

Epoch 428: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5436 - MinusLogProbMetric: 17.5436 - val_loss: 17.7239 - val_MinusLogProbMetric: 17.7239 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 429/1000
2023-10-10 00:42:33.491 
Epoch 429/1000 
	 loss: 17.5233, MinusLogProbMetric: 17.5233, val_loss: 17.7407, val_MinusLogProbMetric: 17.7407

Epoch 429: val_loss did not improve from 17.66361
196/196 - 56s - loss: 17.5233 - MinusLogProbMetric: 17.5233 - val_loss: 17.7407 - val_MinusLogProbMetric: 17.7407 - lr: 1.6667e-04 - 56s/epoch - 286ms/step
Epoch 430/1000
2023-10-10 00:43:31.476 
Epoch 430/1000 
	 loss: 17.5153, MinusLogProbMetric: 17.5153, val_loss: 17.6945, val_MinusLogProbMetric: 17.6945

Epoch 430: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5153 - MinusLogProbMetric: 17.5153 - val_loss: 17.6945 - val_MinusLogProbMetric: 17.6945 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 431/1000
2023-10-10 00:44:29.216 
Epoch 431/1000 
	 loss: 17.5194, MinusLogProbMetric: 17.5194, val_loss: 17.7736, val_MinusLogProbMetric: 17.7736

Epoch 431: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5194 - MinusLogProbMetric: 17.5194 - val_loss: 17.7736 - val_MinusLogProbMetric: 17.7736 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 432/1000
2023-10-10 00:45:26.271 
Epoch 432/1000 
	 loss: 17.5380, MinusLogProbMetric: 17.5380, val_loss: 17.8309, val_MinusLogProbMetric: 17.8309

Epoch 432: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5380 - MinusLogProbMetric: 17.5380 - val_loss: 17.8309 - val_MinusLogProbMetric: 17.8309 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 433/1000
2023-10-10 00:46:23.966 
Epoch 433/1000 
	 loss: 17.5129, MinusLogProbMetric: 17.5129, val_loss: 17.6721, val_MinusLogProbMetric: 17.6721

Epoch 433: val_loss did not improve from 17.66361
196/196 - 58s - loss: 17.5129 - MinusLogProbMetric: 17.5129 - val_loss: 17.6721 - val_MinusLogProbMetric: 17.6721 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 434/1000
2023-10-10 00:47:20.635 
Epoch 434/1000 
	 loss: 17.5209, MinusLogProbMetric: 17.5209, val_loss: 17.7295, val_MinusLogProbMetric: 17.7295

Epoch 434: val_loss did not improve from 17.66361
196/196 - 57s - loss: 17.5209 - MinusLogProbMetric: 17.5209 - val_loss: 17.7295 - val_MinusLogProbMetric: 17.7295 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 435/1000
2023-10-10 00:48:17.904 
Epoch 435/1000 
	 loss: 17.6597, MinusLogProbMetric: 17.6597, val_loss: 17.6426, val_MinusLogProbMetric: 17.6426

Epoch 435: val_loss improved from 17.66361 to 17.64258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.6597 - MinusLogProbMetric: 17.6597 - val_loss: 17.6426 - val_MinusLogProbMetric: 17.6426 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 436/1000
2023-10-10 00:49:16.515 
Epoch 436/1000 
	 loss: 17.5274, MinusLogProbMetric: 17.5274, val_loss: 17.6732, val_MinusLogProbMetric: 17.6732

Epoch 436: val_loss did not improve from 17.64258
196/196 - 57s - loss: 17.5274 - MinusLogProbMetric: 17.5274 - val_loss: 17.6732 - val_MinusLogProbMetric: 17.6732 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 437/1000
2023-10-10 00:50:14.773 
Epoch 437/1000 
	 loss: 17.4913, MinusLogProbMetric: 17.4913, val_loss: 17.6857, val_MinusLogProbMetric: 17.6857

Epoch 437: val_loss did not improve from 17.64258
196/196 - 58s - loss: 17.4913 - MinusLogProbMetric: 17.4913 - val_loss: 17.6857 - val_MinusLogProbMetric: 17.6857 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 438/1000
2023-10-10 00:51:13.514 
Epoch 438/1000 
	 loss: 17.5209, MinusLogProbMetric: 17.5209, val_loss: 17.6474, val_MinusLogProbMetric: 17.6474

Epoch 438: val_loss did not improve from 17.64258
196/196 - 59s - loss: 17.5209 - MinusLogProbMetric: 17.5209 - val_loss: 17.6474 - val_MinusLogProbMetric: 17.6474 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 439/1000
2023-10-10 00:52:11.480 
Epoch 439/1000 
	 loss: 17.5119, MinusLogProbMetric: 17.5119, val_loss: 17.6611, val_MinusLogProbMetric: 17.6611

Epoch 439: val_loss did not improve from 17.64258
196/196 - 58s - loss: 17.5119 - MinusLogProbMetric: 17.5119 - val_loss: 17.6611 - val_MinusLogProbMetric: 17.6611 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 440/1000
2023-10-10 00:53:08.978 
Epoch 440/1000 
	 loss: 17.4991, MinusLogProbMetric: 17.4991, val_loss: 17.6675, val_MinusLogProbMetric: 17.6675

Epoch 440: val_loss did not improve from 17.64258
196/196 - 57s - loss: 17.4991 - MinusLogProbMetric: 17.4991 - val_loss: 17.6675 - val_MinusLogProbMetric: 17.6675 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 441/1000
2023-10-10 00:54:06.384 
Epoch 441/1000 
	 loss: 17.5153, MinusLogProbMetric: 17.5153, val_loss: 17.6449, val_MinusLogProbMetric: 17.6449

Epoch 441: val_loss did not improve from 17.64258
196/196 - 57s - loss: 17.5153 - MinusLogProbMetric: 17.5153 - val_loss: 17.6449 - val_MinusLogProbMetric: 17.6449 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 442/1000
2023-10-10 00:55:02.812 
Epoch 442/1000 
	 loss: 17.4939, MinusLogProbMetric: 17.4939, val_loss: 17.6968, val_MinusLogProbMetric: 17.6968

Epoch 442: val_loss did not improve from 17.64258
196/196 - 56s - loss: 17.4939 - MinusLogProbMetric: 17.4939 - val_loss: 17.6968 - val_MinusLogProbMetric: 17.6968 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 443/1000
2023-10-10 00:55:59.915 
Epoch 443/1000 
	 loss: 17.5105, MinusLogProbMetric: 17.5105, val_loss: 17.6747, val_MinusLogProbMetric: 17.6747

Epoch 443: val_loss did not improve from 17.64258
196/196 - 57s - loss: 17.5105 - MinusLogProbMetric: 17.5105 - val_loss: 17.6747 - val_MinusLogProbMetric: 17.6747 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 444/1000
2023-10-10 00:56:57.756 
Epoch 444/1000 
	 loss: 17.5022, MinusLogProbMetric: 17.5022, val_loss: 17.6376, val_MinusLogProbMetric: 17.6376

Epoch 444: val_loss improved from 17.64258 to 17.63764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.5022 - MinusLogProbMetric: 17.5022 - val_loss: 17.6376 - val_MinusLogProbMetric: 17.6376 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 445/1000
2023-10-10 00:57:56.526 
Epoch 445/1000 
	 loss: 17.6553, MinusLogProbMetric: 17.6553, val_loss: 17.7754, val_MinusLogProbMetric: 17.7754

Epoch 445: val_loss did not improve from 17.63764
196/196 - 58s - loss: 17.6553 - MinusLogProbMetric: 17.6553 - val_loss: 17.7754 - val_MinusLogProbMetric: 17.7754 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 446/1000
2023-10-10 00:58:53.954 
Epoch 446/1000 
	 loss: 17.4959, MinusLogProbMetric: 17.4959, val_loss: 17.6990, val_MinusLogProbMetric: 17.6990

Epoch 446: val_loss did not improve from 17.63764
196/196 - 57s - loss: 17.4959 - MinusLogProbMetric: 17.4959 - val_loss: 17.6990 - val_MinusLogProbMetric: 17.6990 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 447/1000
2023-10-10 00:59:51.665 
Epoch 447/1000 
	 loss: 17.4924, MinusLogProbMetric: 17.4924, val_loss: 17.6394, val_MinusLogProbMetric: 17.6394

Epoch 447: val_loss did not improve from 17.63764
196/196 - 58s - loss: 17.4924 - MinusLogProbMetric: 17.4924 - val_loss: 17.6394 - val_MinusLogProbMetric: 17.6394 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 448/1000
2023-10-10 01:00:49.313 
Epoch 448/1000 
	 loss: 17.5627, MinusLogProbMetric: 17.5627, val_loss: 17.6003, val_MinusLogProbMetric: 17.6003

Epoch 448: val_loss improved from 17.63764 to 17.60032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.5627 - MinusLogProbMetric: 17.5627 - val_loss: 17.6003 - val_MinusLogProbMetric: 17.6003 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 449/1000
2023-10-10 01:01:47.740 
Epoch 449/1000 
	 loss: 17.4834, MinusLogProbMetric: 17.4834, val_loss: 17.6516, val_MinusLogProbMetric: 17.6516

Epoch 449: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4834 - MinusLogProbMetric: 17.4834 - val_loss: 17.6516 - val_MinusLogProbMetric: 17.6516 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 450/1000
2023-10-10 01:02:45.729 
Epoch 450/1000 
	 loss: 17.4781, MinusLogProbMetric: 17.4781, val_loss: 17.6626, val_MinusLogProbMetric: 17.6626

Epoch 450: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4781 - MinusLogProbMetric: 17.4781 - val_loss: 17.6626 - val_MinusLogProbMetric: 17.6626 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 451/1000
2023-10-10 01:03:43.246 
Epoch 451/1000 
	 loss: 17.4768, MinusLogProbMetric: 17.4768, val_loss: 17.6329, val_MinusLogProbMetric: 17.6329

Epoch 451: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4768 - MinusLogProbMetric: 17.4768 - val_loss: 17.6329 - val_MinusLogProbMetric: 17.6329 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 452/1000
2023-10-10 01:04:41.177 
Epoch 452/1000 
	 loss: 17.4845, MinusLogProbMetric: 17.4845, val_loss: 17.7030, val_MinusLogProbMetric: 17.7030

Epoch 452: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4845 - MinusLogProbMetric: 17.4845 - val_loss: 17.7030 - val_MinusLogProbMetric: 17.7030 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 453/1000
2023-10-10 01:05:41.260 
Epoch 453/1000 
	 loss: 17.4889, MinusLogProbMetric: 17.4889, val_loss: 17.6767, val_MinusLogProbMetric: 17.6767

Epoch 453: val_loss did not improve from 17.60032
196/196 - 60s - loss: 17.4889 - MinusLogProbMetric: 17.4889 - val_loss: 17.6767 - val_MinusLogProbMetric: 17.6767 - lr: 1.6667e-04 - 60s/epoch - 307ms/step
Epoch 454/1000
2023-10-10 01:06:41.122 
Epoch 454/1000 
	 loss: 17.4940, MinusLogProbMetric: 17.4940, val_loss: 17.6544, val_MinusLogProbMetric: 17.6544

Epoch 454: val_loss did not improve from 17.60032
196/196 - 60s - loss: 17.4940 - MinusLogProbMetric: 17.4940 - val_loss: 17.6544 - val_MinusLogProbMetric: 17.6544 - lr: 1.6667e-04 - 60s/epoch - 305ms/step
Epoch 455/1000
2023-10-10 01:07:37.837 
Epoch 455/1000 
	 loss: 17.4783, MinusLogProbMetric: 17.4783, val_loss: 17.6161, val_MinusLogProbMetric: 17.6161

Epoch 455: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4783 - MinusLogProbMetric: 17.4783 - val_loss: 17.6161 - val_MinusLogProbMetric: 17.6161 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 456/1000
2023-10-10 01:08:34.746 
Epoch 456/1000 
	 loss: 17.5121, MinusLogProbMetric: 17.5121, val_loss: 17.6787, val_MinusLogProbMetric: 17.6787

Epoch 456: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.5121 - MinusLogProbMetric: 17.5121 - val_loss: 17.6787 - val_MinusLogProbMetric: 17.6787 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 457/1000
2023-10-10 01:09:34.072 
Epoch 457/1000 
	 loss: 17.4722, MinusLogProbMetric: 17.4722, val_loss: 17.7960, val_MinusLogProbMetric: 17.7960

Epoch 457: val_loss did not improve from 17.60032
196/196 - 59s - loss: 17.4722 - MinusLogProbMetric: 17.4722 - val_loss: 17.7960 - val_MinusLogProbMetric: 17.7960 - lr: 1.6667e-04 - 59s/epoch - 303ms/step
Epoch 458/1000
2023-10-10 01:10:30.269 
Epoch 458/1000 
	 loss: 17.4816, MinusLogProbMetric: 17.4816, val_loss: 17.6965, val_MinusLogProbMetric: 17.6965

Epoch 458: val_loss did not improve from 17.60032
196/196 - 56s - loss: 17.4816 - MinusLogProbMetric: 17.4816 - val_loss: 17.6965 - val_MinusLogProbMetric: 17.6965 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 459/1000
2023-10-10 01:11:27.284 
Epoch 459/1000 
	 loss: 17.5472, MinusLogProbMetric: 17.5472, val_loss: 17.6386, val_MinusLogProbMetric: 17.6386

Epoch 459: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.5472 - MinusLogProbMetric: 17.5472 - val_loss: 17.6386 - val_MinusLogProbMetric: 17.6386 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 460/1000
2023-10-10 01:12:23.299 
Epoch 460/1000 
	 loss: 17.4766, MinusLogProbMetric: 17.4766, val_loss: 17.6217, val_MinusLogProbMetric: 17.6217

Epoch 460: val_loss did not improve from 17.60032
196/196 - 56s - loss: 17.4766 - MinusLogProbMetric: 17.4766 - val_loss: 17.6217 - val_MinusLogProbMetric: 17.6217 - lr: 1.6667e-04 - 56s/epoch - 286ms/step
Epoch 461/1000
2023-10-10 01:13:19.833 
Epoch 461/1000 
	 loss: 17.4859, MinusLogProbMetric: 17.4859, val_loss: 17.6357, val_MinusLogProbMetric: 17.6357

Epoch 461: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4859 - MinusLogProbMetric: 17.4859 - val_loss: 17.6357 - val_MinusLogProbMetric: 17.6357 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 462/1000
2023-10-10 01:14:16.378 
Epoch 462/1000 
	 loss: 17.4783, MinusLogProbMetric: 17.4783, val_loss: 17.6047, val_MinusLogProbMetric: 17.6047

Epoch 462: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4783 - MinusLogProbMetric: 17.4783 - val_loss: 17.6047 - val_MinusLogProbMetric: 17.6047 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 463/1000
2023-10-10 01:15:15.152 
Epoch 463/1000 
	 loss: 17.4800, MinusLogProbMetric: 17.4800, val_loss: 17.6447, val_MinusLogProbMetric: 17.6447

Epoch 463: val_loss did not improve from 17.60032
196/196 - 59s - loss: 17.4800 - MinusLogProbMetric: 17.4800 - val_loss: 17.6447 - val_MinusLogProbMetric: 17.6447 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 464/1000
2023-10-10 01:16:12.152 
Epoch 464/1000 
	 loss: 17.4908, MinusLogProbMetric: 17.4908, val_loss: 17.6539, val_MinusLogProbMetric: 17.6539

Epoch 464: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4908 - MinusLogProbMetric: 17.4908 - val_loss: 17.6539 - val_MinusLogProbMetric: 17.6539 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 465/1000
2023-10-10 01:17:08.860 
Epoch 465/1000 
	 loss: 17.4729, MinusLogProbMetric: 17.4729, val_loss: 17.6184, val_MinusLogProbMetric: 17.6184

Epoch 465: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4729 - MinusLogProbMetric: 17.4729 - val_loss: 17.6184 - val_MinusLogProbMetric: 17.6184 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 466/1000
2023-10-10 01:18:04.980 
Epoch 466/1000 
	 loss: 17.7732, MinusLogProbMetric: 17.7732, val_loss: 17.6472, val_MinusLogProbMetric: 17.6472

Epoch 466: val_loss did not improve from 17.60032
196/196 - 56s - loss: 17.7732 - MinusLogProbMetric: 17.7732 - val_loss: 17.6472 - val_MinusLogProbMetric: 17.6472 - lr: 1.6667e-04 - 56s/epoch - 286ms/step
Epoch 467/1000
2023-10-10 01:19:02.755 
Epoch 467/1000 
	 loss: 17.4487, MinusLogProbMetric: 17.4487, val_loss: 17.6770, val_MinusLogProbMetric: 17.6770

Epoch 467: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4487 - MinusLogProbMetric: 17.4487 - val_loss: 17.6770 - val_MinusLogProbMetric: 17.6770 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 468/1000
2023-10-10 01:19:58.913 
Epoch 468/1000 
	 loss: 17.4637, MinusLogProbMetric: 17.4637, val_loss: 17.6798, val_MinusLogProbMetric: 17.6798

Epoch 468: val_loss did not improve from 17.60032
196/196 - 56s - loss: 17.4637 - MinusLogProbMetric: 17.4637 - val_loss: 17.6798 - val_MinusLogProbMetric: 17.6798 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 469/1000
2023-10-10 01:20:56.737 
Epoch 469/1000 
	 loss: 17.4545, MinusLogProbMetric: 17.4545, val_loss: 17.6675, val_MinusLogProbMetric: 17.6675

Epoch 469: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4545 - MinusLogProbMetric: 17.4545 - val_loss: 17.6675 - val_MinusLogProbMetric: 17.6675 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 470/1000
2023-10-10 01:21:53.886 
Epoch 470/1000 
	 loss: 17.4689, MinusLogProbMetric: 17.4689, val_loss: 17.6681, val_MinusLogProbMetric: 17.6681

Epoch 470: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4689 - MinusLogProbMetric: 17.4689 - val_loss: 17.6681 - val_MinusLogProbMetric: 17.6681 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 471/1000
2023-10-10 01:22:51.542 
Epoch 471/1000 
	 loss: 17.4671, MinusLogProbMetric: 17.4671, val_loss: 17.6943, val_MinusLogProbMetric: 17.6943

Epoch 471: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4671 - MinusLogProbMetric: 17.4671 - val_loss: 17.6943 - val_MinusLogProbMetric: 17.6943 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 472/1000
2023-10-10 01:23:49.731 
Epoch 472/1000 
	 loss: 17.5117, MinusLogProbMetric: 17.5117, val_loss: 17.7725, val_MinusLogProbMetric: 17.7725

Epoch 472: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.5117 - MinusLogProbMetric: 17.5117 - val_loss: 17.7725 - val_MinusLogProbMetric: 17.7725 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 473/1000
2023-10-10 01:24:47.762 
Epoch 473/1000 
	 loss: 17.4695, MinusLogProbMetric: 17.4695, val_loss: 17.7269, val_MinusLogProbMetric: 17.7269

Epoch 473: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4695 - MinusLogProbMetric: 17.4695 - val_loss: 17.7269 - val_MinusLogProbMetric: 17.7269 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 474/1000
2023-10-10 01:25:44.756 
Epoch 474/1000 
	 loss: 17.4555, MinusLogProbMetric: 17.4555, val_loss: 17.7136, val_MinusLogProbMetric: 17.7136

Epoch 474: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4555 - MinusLogProbMetric: 17.4555 - val_loss: 17.7136 - val_MinusLogProbMetric: 17.7136 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 475/1000
2023-10-10 01:26:41.632 
Epoch 475/1000 
	 loss: 17.4557, MinusLogProbMetric: 17.4557, val_loss: 17.6738, val_MinusLogProbMetric: 17.6738

Epoch 475: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4557 - MinusLogProbMetric: 17.4557 - val_loss: 17.6738 - val_MinusLogProbMetric: 17.6738 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 476/1000
2023-10-10 01:27:40.611 
Epoch 476/1000 
	 loss: 17.4591, MinusLogProbMetric: 17.4591, val_loss: 17.6224, val_MinusLogProbMetric: 17.6224

Epoch 476: val_loss did not improve from 17.60032
196/196 - 59s - loss: 17.4591 - MinusLogProbMetric: 17.4591 - val_loss: 17.6224 - val_MinusLogProbMetric: 17.6224 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 477/1000
2023-10-10 01:28:38.497 
Epoch 477/1000 
	 loss: 17.5083, MinusLogProbMetric: 17.5083, val_loss: 17.7188, val_MinusLogProbMetric: 17.7188

Epoch 477: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.5083 - MinusLogProbMetric: 17.5083 - val_loss: 17.7188 - val_MinusLogProbMetric: 17.7188 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 478/1000
2023-10-10 01:29:37.293 
Epoch 478/1000 
	 loss: 17.4482, MinusLogProbMetric: 17.4482, val_loss: 17.6020, val_MinusLogProbMetric: 17.6020

Epoch 478: val_loss did not improve from 17.60032
196/196 - 59s - loss: 17.4482 - MinusLogProbMetric: 17.4482 - val_loss: 17.6020 - val_MinusLogProbMetric: 17.6020 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 479/1000
2023-10-10 01:30:34.796 
Epoch 479/1000 
	 loss: 17.4570, MinusLogProbMetric: 17.4570, val_loss: 17.6182, val_MinusLogProbMetric: 17.6182

Epoch 479: val_loss did not improve from 17.60032
196/196 - 58s - loss: 17.4570 - MinusLogProbMetric: 17.4570 - val_loss: 17.6182 - val_MinusLogProbMetric: 17.6182 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 480/1000
2023-10-10 01:31:31.927 
Epoch 480/1000 
	 loss: 17.4558, MinusLogProbMetric: 17.4558, val_loss: 17.6211, val_MinusLogProbMetric: 17.6211

Epoch 480: val_loss did not improve from 17.60032
196/196 - 57s - loss: 17.4558 - MinusLogProbMetric: 17.4558 - val_loss: 17.6211 - val_MinusLogProbMetric: 17.6211 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 481/1000
2023-10-10 01:32:29.420 
Epoch 481/1000 
	 loss: 17.5467, MinusLogProbMetric: 17.5467, val_loss: 17.5996, val_MinusLogProbMetric: 17.5996

Epoch 481: val_loss improved from 17.60032 to 17.59962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.5467 - MinusLogProbMetric: 17.5467 - val_loss: 17.5996 - val_MinusLogProbMetric: 17.5996 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 482/1000
2023-10-10 01:33:28.167 
Epoch 482/1000 
	 loss: 17.4380, MinusLogProbMetric: 17.4380, val_loss: 17.5474, val_MinusLogProbMetric: 17.5474

Epoch 482: val_loss improved from 17.59962 to 17.54738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.4380 - MinusLogProbMetric: 17.4380 - val_loss: 17.5474 - val_MinusLogProbMetric: 17.5474 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 483/1000
2023-10-10 01:34:26.996 
Epoch 483/1000 
	 loss: 17.4411, MinusLogProbMetric: 17.4411, val_loss: 17.5877, val_MinusLogProbMetric: 17.5877

Epoch 483: val_loss did not improve from 17.54738
196/196 - 58s - loss: 17.4411 - MinusLogProbMetric: 17.4411 - val_loss: 17.5877 - val_MinusLogProbMetric: 17.5877 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 484/1000
2023-10-10 01:35:25.408 
Epoch 484/1000 
	 loss: 17.4502, MinusLogProbMetric: 17.4502, val_loss: 17.6749, val_MinusLogProbMetric: 17.6749

Epoch 484: val_loss did not improve from 17.54738
196/196 - 58s - loss: 17.4502 - MinusLogProbMetric: 17.4502 - val_loss: 17.6749 - val_MinusLogProbMetric: 17.6749 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 485/1000
2023-10-10 01:36:22.137 
Epoch 485/1000 
	 loss: 17.4417, MinusLogProbMetric: 17.4417, val_loss: 17.6620, val_MinusLogProbMetric: 17.6620

Epoch 485: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4417 - MinusLogProbMetric: 17.4417 - val_loss: 17.6620 - val_MinusLogProbMetric: 17.6620 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 486/1000
2023-10-10 01:37:19.351 
Epoch 486/1000 
	 loss: 17.4413, MinusLogProbMetric: 17.4413, val_loss: 17.6412, val_MinusLogProbMetric: 17.6412

Epoch 486: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4413 - MinusLogProbMetric: 17.4413 - val_loss: 17.6412 - val_MinusLogProbMetric: 17.6412 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 487/1000
2023-10-10 01:38:17.736 
Epoch 487/1000 
	 loss: 17.4279, MinusLogProbMetric: 17.4279, val_loss: 17.6461, val_MinusLogProbMetric: 17.6461

Epoch 487: val_loss did not improve from 17.54738
196/196 - 58s - loss: 17.4279 - MinusLogProbMetric: 17.4279 - val_loss: 17.6461 - val_MinusLogProbMetric: 17.6461 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 488/1000
2023-10-10 01:39:15.090 
Epoch 488/1000 
	 loss: 17.4206, MinusLogProbMetric: 17.4206, val_loss: 17.5707, val_MinusLogProbMetric: 17.5707

Epoch 488: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4206 - MinusLogProbMetric: 17.4206 - val_loss: 17.5707 - val_MinusLogProbMetric: 17.5707 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 489/1000
2023-10-10 01:40:12.903 
Epoch 489/1000 
	 loss: 17.4355, MinusLogProbMetric: 17.4355, val_loss: 17.5803, val_MinusLogProbMetric: 17.5803

Epoch 489: val_loss did not improve from 17.54738
196/196 - 58s - loss: 17.4355 - MinusLogProbMetric: 17.4355 - val_loss: 17.5803 - val_MinusLogProbMetric: 17.5803 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 490/1000
2023-10-10 01:41:10.175 
Epoch 490/1000 
	 loss: 17.4384, MinusLogProbMetric: 17.4384, val_loss: 17.6269, val_MinusLogProbMetric: 17.6269

Epoch 490: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4384 - MinusLogProbMetric: 17.4384 - val_loss: 17.6269 - val_MinusLogProbMetric: 17.6269 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 491/1000
2023-10-10 01:42:07.360 
Epoch 491/1000 
	 loss: 17.4278, MinusLogProbMetric: 17.4278, val_loss: 17.6156, val_MinusLogProbMetric: 17.6156

Epoch 491: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4278 - MinusLogProbMetric: 17.4278 - val_loss: 17.6156 - val_MinusLogProbMetric: 17.6156 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 492/1000
2023-10-10 01:43:04.577 
Epoch 492/1000 
	 loss: 17.4371, MinusLogProbMetric: 17.4371, val_loss: 17.6929, val_MinusLogProbMetric: 17.6929

Epoch 492: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4371 - MinusLogProbMetric: 17.4371 - val_loss: 17.6929 - val_MinusLogProbMetric: 17.6929 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 493/1000
2023-10-10 01:44:02.226 
Epoch 493/1000 
	 loss: 17.4772, MinusLogProbMetric: 17.4772, val_loss: 17.7490, val_MinusLogProbMetric: 17.7490

Epoch 493: val_loss did not improve from 17.54738
196/196 - 58s - loss: 17.4772 - MinusLogProbMetric: 17.4772 - val_loss: 17.7490 - val_MinusLogProbMetric: 17.7490 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 494/1000
2023-10-10 01:44:59.395 
Epoch 494/1000 
	 loss: 17.4339, MinusLogProbMetric: 17.4339, val_loss: 17.5913, val_MinusLogProbMetric: 17.5913

Epoch 494: val_loss did not improve from 17.54738
196/196 - 57s - loss: 17.4339 - MinusLogProbMetric: 17.4339 - val_loss: 17.5913 - val_MinusLogProbMetric: 17.5913 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 495/1000
2023-10-10 01:45:57.525 
Epoch 495/1000 
	 loss: 17.4234, MinusLogProbMetric: 17.4234, val_loss: 17.7394, val_MinusLogProbMetric: 17.7394

Epoch 495: val_loss did not improve from 17.54738
196/196 - 58s - loss: 17.4234 - MinusLogProbMetric: 17.4234 - val_loss: 17.7394 - val_MinusLogProbMetric: 17.7394 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 496/1000
2023-10-10 01:46:54.772 
Epoch 496/1000 
	 loss: 17.4250, MinusLogProbMetric: 17.4250, val_loss: 17.5374, val_MinusLogProbMetric: 17.5374

Epoch 496: val_loss improved from 17.54738 to 17.53741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.4250 - MinusLogProbMetric: 17.4250 - val_loss: 17.5374 - val_MinusLogProbMetric: 17.5374 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 497/1000
2023-10-10 01:47:53.399 
Epoch 497/1000 
	 loss: 17.4339, MinusLogProbMetric: 17.4339, val_loss: 17.5619, val_MinusLogProbMetric: 17.5619

Epoch 497: val_loss did not improve from 17.53741
196/196 - 58s - loss: 17.4339 - MinusLogProbMetric: 17.4339 - val_loss: 17.5619 - val_MinusLogProbMetric: 17.5619 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 498/1000
2023-10-10 01:48:50.568 
Epoch 498/1000 
	 loss: 17.4181, MinusLogProbMetric: 17.4181, val_loss: 17.5809, val_MinusLogProbMetric: 17.5809

Epoch 498: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4181 - MinusLogProbMetric: 17.4181 - val_loss: 17.5809 - val_MinusLogProbMetric: 17.5809 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 499/1000
2023-10-10 01:49:47.225 
Epoch 499/1000 
	 loss: 17.4842, MinusLogProbMetric: 17.4842, val_loss: 17.7861, val_MinusLogProbMetric: 17.7861

Epoch 499: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4842 - MinusLogProbMetric: 17.4842 - val_loss: 17.7861 - val_MinusLogProbMetric: 17.7861 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 500/1000
2023-10-10 01:50:43.673 
Epoch 500/1000 
	 loss: 17.4218, MinusLogProbMetric: 17.4218, val_loss: 17.6585, val_MinusLogProbMetric: 17.6585

Epoch 500: val_loss did not improve from 17.53741
196/196 - 56s - loss: 17.4218 - MinusLogProbMetric: 17.4218 - val_loss: 17.6585 - val_MinusLogProbMetric: 17.6585 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 501/1000
2023-10-10 01:51:41.719 
Epoch 501/1000 
	 loss: 17.4302, MinusLogProbMetric: 17.4302, val_loss: 17.6004, val_MinusLogProbMetric: 17.6004

Epoch 501: val_loss did not improve from 17.53741
196/196 - 58s - loss: 17.4302 - MinusLogProbMetric: 17.4302 - val_loss: 17.6004 - val_MinusLogProbMetric: 17.6004 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 502/1000
2023-10-10 01:52:38.362 
Epoch 502/1000 
	 loss: 17.4194, MinusLogProbMetric: 17.4194, val_loss: 17.6306, val_MinusLogProbMetric: 17.6306

Epoch 502: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4194 - MinusLogProbMetric: 17.4194 - val_loss: 17.6306 - val_MinusLogProbMetric: 17.6306 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 503/1000
2023-10-10 01:53:36.426 
Epoch 503/1000 
	 loss: 17.4328, MinusLogProbMetric: 17.4328, val_loss: 17.5713, val_MinusLogProbMetric: 17.5713

Epoch 503: val_loss did not improve from 17.53741
196/196 - 58s - loss: 17.4328 - MinusLogProbMetric: 17.4328 - val_loss: 17.5713 - val_MinusLogProbMetric: 17.5713 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 504/1000
2023-10-10 01:54:32.738 
Epoch 504/1000 
	 loss: 17.4556, MinusLogProbMetric: 17.4556, val_loss: 17.6299, val_MinusLogProbMetric: 17.6299

Epoch 504: val_loss did not improve from 17.53741
196/196 - 56s - loss: 17.4556 - MinusLogProbMetric: 17.4556 - val_loss: 17.6299 - val_MinusLogProbMetric: 17.6299 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 505/1000
2023-10-10 01:55:30.547 
Epoch 505/1000 
	 loss: 17.4301, MinusLogProbMetric: 17.4301, val_loss: 17.6008, val_MinusLogProbMetric: 17.6008

Epoch 505: val_loss did not improve from 17.53741
196/196 - 58s - loss: 17.4301 - MinusLogProbMetric: 17.4301 - val_loss: 17.6008 - val_MinusLogProbMetric: 17.6008 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 506/1000
2023-10-10 01:56:27.798 
Epoch 506/1000 
	 loss: 17.4152, MinusLogProbMetric: 17.4152, val_loss: 17.6889, val_MinusLogProbMetric: 17.6889

Epoch 506: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4152 - MinusLogProbMetric: 17.4152 - val_loss: 17.6889 - val_MinusLogProbMetric: 17.6889 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 507/1000
2023-10-10 01:57:24.499 
Epoch 507/1000 
	 loss: 17.4224, MinusLogProbMetric: 17.4224, val_loss: 17.6390, val_MinusLogProbMetric: 17.6390

Epoch 507: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4224 - MinusLogProbMetric: 17.4224 - val_loss: 17.6390 - val_MinusLogProbMetric: 17.6390 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 508/1000
2023-10-10 01:58:20.729 
Epoch 508/1000 
	 loss: 17.4235, MinusLogProbMetric: 17.4235, val_loss: 17.7385, val_MinusLogProbMetric: 17.7385

Epoch 508: val_loss did not improve from 17.53741
196/196 - 56s - loss: 17.4235 - MinusLogProbMetric: 17.4235 - val_loss: 17.7385 - val_MinusLogProbMetric: 17.7385 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 509/1000
2023-10-10 01:59:17.756 
Epoch 509/1000 
	 loss: 17.4257, MinusLogProbMetric: 17.4257, val_loss: 17.5953, val_MinusLogProbMetric: 17.5953

Epoch 509: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4257 - MinusLogProbMetric: 17.4257 - val_loss: 17.5953 - val_MinusLogProbMetric: 17.5953 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 510/1000
2023-10-10 02:00:14.466 
Epoch 510/1000 
	 loss: 17.4245, MinusLogProbMetric: 17.4245, val_loss: 17.6765, val_MinusLogProbMetric: 17.6765

Epoch 510: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4245 - MinusLogProbMetric: 17.4245 - val_loss: 17.6765 - val_MinusLogProbMetric: 17.6765 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 511/1000
2023-10-10 02:01:11.594 
Epoch 511/1000 
	 loss: 17.4327, MinusLogProbMetric: 17.4327, val_loss: 17.6472, val_MinusLogProbMetric: 17.6472

Epoch 511: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4327 - MinusLogProbMetric: 17.4327 - val_loss: 17.6472 - val_MinusLogProbMetric: 17.6472 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 512/1000
2023-10-10 02:02:08.652 
Epoch 512/1000 
	 loss: 17.4219, MinusLogProbMetric: 17.4219, val_loss: 17.6116, val_MinusLogProbMetric: 17.6116

Epoch 512: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4219 - MinusLogProbMetric: 17.4219 - val_loss: 17.6116 - val_MinusLogProbMetric: 17.6116 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 513/1000
2023-10-10 02:03:04.585 
Epoch 513/1000 
	 loss: 17.4750, MinusLogProbMetric: 17.4750, val_loss: 17.6551, val_MinusLogProbMetric: 17.6551

Epoch 513: val_loss did not improve from 17.53741
196/196 - 56s - loss: 17.4750 - MinusLogProbMetric: 17.4750 - val_loss: 17.6551 - val_MinusLogProbMetric: 17.6551 - lr: 1.6667e-04 - 56s/epoch - 285ms/step
Epoch 514/1000
2023-10-10 02:04:01.736 
Epoch 514/1000 
	 loss: 17.4035, MinusLogProbMetric: 17.4035, val_loss: 17.6375, val_MinusLogProbMetric: 17.6375

Epoch 514: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4035 - MinusLogProbMetric: 17.4035 - val_loss: 17.6375 - val_MinusLogProbMetric: 17.6375 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 515/1000
2023-10-10 02:04:58.477 
Epoch 515/1000 
	 loss: 17.4107, MinusLogProbMetric: 17.4107, val_loss: 17.6275, val_MinusLogProbMetric: 17.6275

Epoch 515: val_loss did not improve from 17.53741
196/196 - 57s - loss: 17.4107 - MinusLogProbMetric: 17.4107 - val_loss: 17.6275 - val_MinusLogProbMetric: 17.6275 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 516/1000
2023-10-10 02:05:56.092 
Epoch 516/1000 
	 loss: 17.3997, MinusLogProbMetric: 17.3997, val_loss: 17.5509, val_MinusLogProbMetric: 17.5509

Epoch 516: val_loss did not improve from 17.53741
196/196 - 58s - loss: 17.3997 - MinusLogProbMetric: 17.3997 - val_loss: 17.5509 - val_MinusLogProbMetric: 17.5509 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 517/1000
2023-10-10 02:06:53.496 
Epoch 517/1000 
	 loss: 17.3919, MinusLogProbMetric: 17.3919, val_loss: 17.5275, val_MinusLogProbMetric: 17.5275

Epoch 517: val_loss improved from 17.53741 to 17.52755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.3919 - MinusLogProbMetric: 17.3919 - val_loss: 17.5275 - val_MinusLogProbMetric: 17.5275 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 518/1000
2023-10-10 02:07:55.103 
Epoch 518/1000 
	 loss: 17.5021, MinusLogProbMetric: 17.5021, val_loss: 17.5510, val_MinusLogProbMetric: 17.5510

Epoch 518: val_loss did not improve from 17.52755
196/196 - 61s - loss: 17.5021 - MinusLogProbMetric: 17.5021 - val_loss: 17.5510 - val_MinusLogProbMetric: 17.5510 - lr: 1.6667e-04 - 61s/epoch - 309ms/step
Epoch 519/1000
2023-10-10 02:08:51.925 
Epoch 519/1000 
	 loss: 17.4013, MinusLogProbMetric: 17.4013, val_loss: 17.6939, val_MinusLogProbMetric: 17.6939

Epoch 519: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.4013 - MinusLogProbMetric: 17.4013 - val_loss: 17.6939 - val_MinusLogProbMetric: 17.6939 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 520/1000
2023-10-10 02:09:49.108 
Epoch 520/1000 
	 loss: 17.4225, MinusLogProbMetric: 17.4225, val_loss: 17.5921, val_MinusLogProbMetric: 17.5921

Epoch 520: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.4225 - MinusLogProbMetric: 17.4225 - val_loss: 17.5921 - val_MinusLogProbMetric: 17.5921 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 521/1000
2023-10-10 02:10:46.980 
Epoch 521/1000 
	 loss: 17.4286, MinusLogProbMetric: 17.4286, val_loss: 17.6675, val_MinusLogProbMetric: 17.6675

Epoch 521: val_loss did not improve from 17.52755
196/196 - 58s - loss: 17.4286 - MinusLogProbMetric: 17.4286 - val_loss: 17.6675 - val_MinusLogProbMetric: 17.6675 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 522/1000
2023-10-10 02:11:45.722 
Epoch 522/1000 
	 loss: 17.4035, MinusLogProbMetric: 17.4035, val_loss: 17.5819, val_MinusLogProbMetric: 17.5819

Epoch 522: val_loss did not improve from 17.52755
196/196 - 59s - loss: 17.4035 - MinusLogProbMetric: 17.4035 - val_loss: 17.5819 - val_MinusLogProbMetric: 17.5819 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 523/1000
2023-10-10 02:12:46.120 
Epoch 523/1000 
	 loss: 17.3868, MinusLogProbMetric: 17.3868, val_loss: 17.5940, val_MinusLogProbMetric: 17.5940

Epoch 523: val_loss did not improve from 17.52755
196/196 - 60s - loss: 17.3868 - MinusLogProbMetric: 17.3868 - val_loss: 17.5940 - val_MinusLogProbMetric: 17.5940 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 524/1000
2023-10-10 02:13:47.224 
Epoch 524/1000 
	 loss: 17.4034, MinusLogProbMetric: 17.4034, val_loss: 17.5435, val_MinusLogProbMetric: 17.5435

Epoch 524: val_loss did not improve from 17.52755
196/196 - 61s - loss: 17.4034 - MinusLogProbMetric: 17.4034 - val_loss: 17.5435 - val_MinusLogProbMetric: 17.5435 - lr: 1.6667e-04 - 61s/epoch - 312ms/step
Epoch 525/1000
2023-10-10 02:14:44.991 
Epoch 525/1000 
	 loss: 17.4009, MinusLogProbMetric: 17.4009, val_loss: 17.5288, val_MinusLogProbMetric: 17.5288

Epoch 525: val_loss did not improve from 17.52755
196/196 - 58s - loss: 17.4009 - MinusLogProbMetric: 17.4009 - val_loss: 17.5288 - val_MinusLogProbMetric: 17.5288 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 526/1000
2023-10-10 02:15:42.216 
Epoch 526/1000 
	 loss: 17.4013, MinusLogProbMetric: 17.4013, val_loss: 17.6637, val_MinusLogProbMetric: 17.6637

Epoch 526: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.4013 - MinusLogProbMetric: 17.4013 - val_loss: 17.6637 - val_MinusLogProbMetric: 17.6637 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 527/1000
2023-10-10 02:16:38.825 
Epoch 527/1000 
	 loss: 17.8296, MinusLogProbMetric: 17.8296, val_loss: 17.7931, val_MinusLogProbMetric: 17.7931

Epoch 527: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.8296 - MinusLogProbMetric: 17.8296 - val_loss: 17.7931 - val_MinusLogProbMetric: 17.7931 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 528/1000
2023-10-10 02:17:35.096 
Epoch 528/1000 
	 loss: 17.4279, MinusLogProbMetric: 17.4279, val_loss: 17.5318, val_MinusLogProbMetric: 17.5318

Epoch 528: val_loss did not improve from 17.52755
196/196 - 56s - loss: 17.4279 - MinusLogProbMetric: 17.4279 - val_loss: 17.5318 - val_MinusLogProbMetric: 17.5318 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 529/1000
2023-10-10 02:18:32.442 
Epoch 529/1000 
	 loss: 17.3764, MinusLogProbMetric: 17.3764, val_loss: 17.6575, val_MinusLogProbMetric: 17.6575

Epoch 529: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3764 - MinusLogProbMetric: 17.3764 - val_loss: 17.6575 - val_MinusLogProbMetric: 17.6575 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 530/1000
2023-10-10 02:19:29.100 
Epoch 530/1000 
	 loss: 17.4864, MinusLogProbMetric: 17.4864, val_loss: 17.5662, val_MinusLogProbMetric: 17.5662

Epoch 530: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.4864 - MinusLogProbMetric: 17.4864 - val_loss: 17.5662 - val_MinusLogProbMetric: 17.5662 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 531/1000
2023-10-10 02:20:26.579 
Epoch 531/1000 
	 loss: 17.3879, MinusLogProbMetric: 17.3879, val_loss: 17.5760, val_MinusLogProbMetric: 17.5760

Epoch 531: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3879 - MinusLogProbMetric: 17.3879 - val_loss: 17.5760 - val_MinusLogProbMetric: 17.5760 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 532/1000
2023-10-10 02:21:23.406 
Epoch 532/1000 
	 loss: 17.3751, MinusLogProbMetric: 17.3751, val_loss: 17.5512, val_MinusLogProbMetric: 17.5512

Epoch 532: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3751 - MinusLogProbMetric: 17.3751 - val_loss: 17.5512 - val_MinusLogProbMetric: 17.5512 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 533/1000
2023-10-10 02:22:20.236 
Epoch 533/1000 
	 loss: 17.3837, MinusLogProbMetric: 17.3837, val_loss: 17.6203, val_MinusLogProbMetric: 17.6203

Epoch 533: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3837 - MinusLogProbMetric: 17.3837 - val_loss: 17.6203 - val_MinusLogProbMetric: 17.6203 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 534/1000
2023-10-10 02:23:16.713 
Epoch 534/1000 
	 loss: 17.3881, MinusLogProbMetric: 17.3881, val_loss: 17.6485, val_MinusLogProbMetric: 17.6485

Epoch 534: val_loss did not improve from 17.52755
196/196 - 56s - loss: 17.3881 - MinusLogProbMetric: 17.3881 - val_loss: 17.6485 - val_MinusLogProbMetric: 17.6485 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 535/1000
2023-10-10 02:24:13.894 
Epoch 535/1000 
	 loss: 17.3976, MinusLogProbMetric: 17.3976, val_loss: 17.6018, val_MinusLogProbMetric: 17.6018

Epoch 535: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3976 - MinusLogProbMetric: 17.3976 - val_loss: 17.6018 - val_MinusLogProbMetric: 17.6018 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 536/1000
2023-10-10 02:25:11.981 
Epoch 536/1000 
	 loss: 17.3782, MinusLogProbMetric: 17.3782, val_loss: 17.5519, val_MinusLogProbMetric: 17.5519

Epoch 536: val_loss did not improve from 17.52755
196/196 - 58s - loss: 17.3782 - MinusLogProbMetric: 17.3782 - val_loss: 17.5519 - val_MinusLogProbMetric: 17.5519 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 537/1000
2023-10-10 02:26:10.441 
Epoch 537/1000 
	 loss: 17.3853, MinusLogProbMetric: 17.3853, val_loss: 17.6228, val_MinusLogProbMetric: 17.6228

Epoch 537: val_loss did not improve from 17.52755
196/196 - 58s - loss: 17.3853 - MinusLogProbMetric: 17.3853 - val_loss: 17.6228 - val_MinusLogProbMetric: 17.6228 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 538/1000
2023-10-10 02:27:08.371 
Epoch 538/1000 
	 loss: 17.3790, MinusLogProbMetric: 17.3790, val_loss: 17.5575, val_MinusLogProbMetric: 17.5575

Epoch 538: val_loss did not improve from 17.52755
196/196 - 58s - loss: 17.3790 - MinusLogProbMetric: 17.3790 - val_loss: 17.5575 - val_MinusLogProbMetric: 17.5575 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 539/1000
2023-10-10 02:28:06.296 
Epoch 539/1000 
	 loss: 17.3909, MinusLogProbMetric: 17.3909, val_loss: 17.6995, val_MinusLogProbMetric: 17.6995

Epoch 539: val_loss did not improve from 17.52755
196/196 - 58s - loss: 17.3909 - MinusLogProbMetric: 17.3909 - val_loss: 17.6995 - val_MinusLogProbMetric: 17.6995 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 540/1000
2023-10-10 02:29:03.145 
Epoch 540/1000 
	 loss: 17.3749, MinusLogProbMetric: 17.3749, val_loss: 17.5548, val_MinusLogProbMetric: 17.5548

Epoch 540: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3749 - MinusLogProbMetric: 17.3749 - val_loss: 17.5548 - val_MinusLogProbMetric: 17.5548 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 541/1000
2023-10-10 02:30:00.370 
Epoch 541/1000 
	 loss: 17.4179, MinusLogProbMetric: 17.4179, val_loss: 17.5362, val_MinusLogProbMetric: 17.5362

Epoch 541: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.4179 - MinusLogProbMetric: 17.4179 - val_loss: 17.5362 - val_MinusLogProbMetric: 17.5362 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 542/1000
2023-10-10 02:30:57.278 
Epoch 542/1000 
	 loss: 17.3606, MinusLogProbMetric: 17.3606, val_loss: 17.6599, val_MinusLogProbMetric: 17.6599

Epoch 542: val_loss did not improve from 17.52755
196/196 - 57s - loss: 17.3606 - MinusLogProbMetric: 17.3606 - val_loss: 17.6599 - val_MinusLogProbMetric: 17.6599 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 543/1000
2023-10-10 02:31:55.494 
Epoch 543/1000 
	 loss: 17.3788, MinusLogProbMetric: 17.3788, val_loss: 17.4978, val_MinusLogProbMetric: 17.4978

Epoch 543: val_loss improved from 17.52755 to 17.49776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.3788 - MinusLogProbMetric: 17.3788 - val_loss: 17.4978 - val_MinusLogProbMetric: 17.4978 - lr: 1.6667e-04 - 59s/epoch - 303ms/step
Epoch 544/1000
2023-10-10 02:32:54.692 
Epoch 544/1000 
	 loss: 17.3717, MinusLogProbMetric: 17.3717, val_loss: 17.5291, val_MinusLogProbMetric: 17.5291

Epoch 544: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3717 - MinusLogProbMetric: 17.3717 - val_loss: 17.5291 - val_MinusLogProbMetric: 17.5291 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 545/1000
2023-10-10 02:33:51.978 
Epoch 545/1000 
	 loss: 17.3704, MinusLogProbMetric: 17.3704, val_loss: 17.5590, val_MinusLogProbMetric: 17.5590

Epoch 545: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3704 - MinusLogProbMetric: 17.3704 - val_loss: 17.5590 - val_MinusLogProbMetric: 17.5590 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 546/1000
2023-10-10 02:34:48.698 
Epoch 546/1000 
	 loss: 17.3977, MinusLogProbMetric: 17.3977, val_loss: 17.7141, val_MinusLogProbMetric: 17.7141

Epoch 546: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3977 - MinusLogProbMetric: 17.3977 - val_loss: 17.7141 - val_MinusLogProbMetric: 17.7141 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 547/1000
2023-10-10 02:35:46.322 
Epoch 547/1000 
	 loss: 17.3764, MinusLogProbMetric: 17.3764, val_loss: 17.5385, val_MinusLogProbMetric: 17.5385

Epoch 547: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3764 - MinusLogProbMetric: 17.3764 - val_loss: 17.5385 - val_MinusLogProbMetric: 17.5385 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 548/1000
2023-10-10 02:36:43.861 
Epoch 548/1000 
	 loss: 17.3938, MinusLogProbMetric: 17.3938, val_loss: 17.5524, val_MinusLogProbMetric: 17.5524

Epoch 548: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3938 - MinusLogProbMetric: 17.3938 - val_loss: 17.5524 - val_MinusLogProbMetric: 17.5524 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 549/1000
2023-10-10 02:37:41.305 
Epoch 549/1000 
	 loss: 17.4353, MinusLogProbMetric: 17.4353, val_loss: 17.5439, val_MinusLogProbMetric: 17.5439

Epoch 549: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.4353 - MinusLogProbMetric: 17.4353 - val_loss: 17.5439 - val_MinusLogProbMetric: 17.5439 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 550/1000
2023-10-10 02:38:39.425 
Epoch 550/1000 
	 loss: 17.3522, MinusLogProbMetric: 17.3522, val_loss: 17.5022, val_MinusLogProbMetric: 17.5022

Epoch 550: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3522 - MinusLogProbMetric: 17.3522 - val_loss: 17.5022 - val_MinusLogProbMetric: 17.5022 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 551/1000
2023-10-10 02:39:38.043 
Epoch 551/1000 
	 loss: 17.3679, MinusLogProbMetric: 17.3679, val_loss: 17.5604, val_MinusLogProbMetric: 17.5604

Epoch 551: val_loss did not improve from 17.49776
196/196 - 59s - loss: 17.3679 - MinusLogProbMetric: 17.3679 - val_loss: 17.5604 - val_MinusLogProbMetric: 17.5604 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 552/1000
2023-10-10 02:40:37.072 
Epoch 552/1000 
	 loss: 17.3660, MinusLogProbMetric: 17.3660, val_loss: 17.5842, val_MinusLogProbMetric: 17.5842

Epoch 552: val_loss did not improve from 17.49776
196/196 - 59s - loss: 17.3660 - MinusLogProbMetric: 17.3660 - val_loss: 17.5842 - val_MinusLogProbMetric: 17.5842 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 553/1000
2023-10-10 02:41:37.834 
Epoch 553/1000 
	 loss: 17.4916, MinusLogProbMetric: 17.4916, val_loss: 17.5763, val_MinusLogProbMetric: 17.5763

Epoch 553: val_loss did not improve from 17.49776
196/196 - 61s - loss: 17.4916 - MinusLogProbMetric: 17.4916 - val_loss: 17.5763 - val_MinusLogProbMetric: 17.5763 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 554/1000
2023-10-10 02:42:38.557 
Epoch 554/1000 
	 loss: 17.3600, MinusLogProbMetric: 17.3600, val_loss: 17.6045, val_MinusLogProbMetric: 17.6045

Epoch 554: val_loss did not improve from 17.49776
196/196 - 61s - loss: 17.3600 - MinusLogProbMetric: 17.3600 - val_loss: 17.6045 - val_MinusLogProbMetric: 17.6045 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 555/1000
2023-10-10 02:43:36.506 
Epoch 555/1000 
	 loss: 17.3564, MinusLogProbMetric: 17.3564, val_loss: 17.5437, val_MinusLogProbMetric: 17.5437

Epoch 555: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3564 - MinusLogProbMetric: 17.3564 - val_loss: 17.5437 - val_MinusLogProbMetric: 17.5437 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 556/1000
2023-10-10 02:44:33.315 
Epoch 556/1000 
	 loss: 17.4614, MinusLogProbMetric: 17.4614, val_loss: 17.5629, val_MinusLogProbMetric: 17.5629

Epoch 556: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.4614 - MinusLogProbMetric: 17.4614 - val_loss: 17.5629 - val_MinusLogProbMetric: 17.5629 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 557/1000
2023-10-10 02:45:30.247 
Epoch 557/1000 
	 loss: 17.3797, MinusLogProbMetric: 17.3797, val_loss: 17.5998, val_MinusLogProbMetric: 17.5998

Epoch 557: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3797 - MinusLogProbMetric: 17.3797 - val_loss: 17.5998 - val_MinusLogProbMetric: 17.5998 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 558/1000
2023-10-10 02:46:28.875 
Epoch 558/1000 
	 loss: 17.3601, MinusLogProbMetric: 17.3601, val_loss: 17.5649, val_MinusLogProbMetric: 17.5649

Epoch 558: val_loss did not improve from 17.49776
196/196 - 59s - loss: 17.3601 - MinusLogProbMetric: 17.3601 - val_loss: 17.5649 - val_MinusLogProbMetric: 17.5649 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 559/1000
2023-10-10 02:47:26.063 
Epoch 559/1000 
	 loss: 17.3649, MinusLogProbMetric: 17.3649, val_loss: 17.5617, val_MinusLogProbMetric: 17.5617

Epoch 559: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3649 - MinusLogProbMetric: 17.3649 - val_loss: 17.5617 - val_MinusLogProbMetric: 17.5617 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 560/1000
2023-10-10 02:48:21.534 
Epoch 560/1000 
	 loss: 17.3472, MinusLogProbMetric: 17.3472, val_loss: 17.5233, val_MinusLogProbMetric: 17.5233

Epoch 560: val_loss did not improve from 17.49776
196/196 - 55s - loss: 17.3472 - MinusLogProbMetric: 17.3472 - val_loss: 17.5233 - val_MinusLogProbMetric: 17.5233 - lr: 1.6667e-04 - 55s/epoch - 283ms/step
Epoch 561/1000
2023-10-10 02:49:20.178 
Epoch 561/1000 
	 loss: 17.3793, MinusLogProbMetric: 17.3793, val_loss: 17.6311, val_MinusLogProbMetric: 17.6311

Epoch 561: val_loss did not improve from 17.49776
196/196 - 59s - loss: 17.3793 - MinusLogProbMetric: 17.3793 - val_loss: 17.6311 - val_MinusLogProbMetric: 17.6311 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 562/1000
2023-10-10 02:50:16.906 
Epoch 562/1000 
	 loss: 17.4114, MinusLogProbMetric: 17.4114, val_loss: 17.5447, val_MinusLogProbMetric: 17.5447

Epoch 562: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.4114 - MinusLogProbMetric: 17.4114 - val_loss: 17.5447 - val_MinusLogProbMetric: 17.5447 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 563/1000
2023-10-10 02:51:14.671 
Epoch 563/1000 
	 loss: 17.3636, MinusLogProbMetric: 17.3636, val_loss: 17.5837, val_MinusLogProbMetric: 17.5837

Epoch 563: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3636 - MinusLogProbMetric: 17.3636 - val_loss: 17.5837 - val_MinusLogProbMetric: 17.5837 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 564/1000
2023-10-10 02:52:11.772 
Epoch 564/1000 
	 loss: 17.3484, MinusLogProbMetric: 17.3484, val_loss: 17.5364, val_MinusLogProbMetric: 17.5364

Epoch 564: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3484 - MinusLogProbMetric: 17.3484 - val_loss: 17.5364 - val_MinusLogProbMetric: 17.5364 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 565/1000
2023-10-10 02:53:08.622 
Epoch 565/1000 
	 loss: 17.4682, MinusLogProbMetric: 17.4682, val_loss: 17.6040, val_MinusLogProbMetric: 17.6040

Epoch 565: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.4682 - MinusLogProbMetric: 17.4682 - val_loss: 17.6040 - val_MinusLogProbMetric: 17.6040 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 566/1000
2023-10-10 02:54:05.583 
Epoch 566/1000 
	 loss: 17.3818, MinusLogProbMetric: 17.3818, val_loss: 17.5459, val_MinusLogProbMetric: 17.5459

Epoch 566: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3818 - MinusLogProbMetric: 17.3818 - val_loss: 17.5459 - val_MinusLogProbMetric: 17.5459 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 567/1000
2023-10-10 02:55:02.448 
Epoch 567/1000 
	 loss: 17.3500, MinusLogProbMetric: 17.3500, val_loss: 17.5471, val_MinusLogProbMetric: 17.5471

Epoch 567: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3500 - MinusLogProbMetric: 17.3500 - val_loss: 17.5471 - val_MinusLogProbMetric: 17.5471 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 568/1000
2023-10-10 02:55:59.593 
Epoch 568/1000 
	 loss: 17.3425, MinusLogProbMetric: 17.3425, val_loss: 17.5653, val_MinusLogProbMetric: 17.5653

Epoch 568: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.3425 - MinusLogProbMetric: 17.3425 - val_loss: 17.5653 - val_MinusLogProbMetric: 17.5653 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 569/1000
2023-10-10 02:56:56.664 
Epoch 569/1000 
	 loss: 17.4632, MinusLogProbMetric: 17.4632, val_loss: 17.5962, val_MinusLogProbMetric: 17.5962

Epoch 569: val_loss did not improve from 17.49776
196/196 - 57s - loss: 17.4632 - MinusLogProbMetric: 17.4632 - val_loss: 17.5962 - val_MinusLogProbMetric: 17.5962 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 570/1000
2023-10-10 02:57:55.133 
Epoch 570/1000 
	 loss: 17.3316, MinusLogProbMetric: 17.3316, val_loss: 17.5482, val_MinusLogProbMetric: 17.5482

Epoch 570: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3316 - MinusLogProbMetric: 17.3316 - val_loss: 17.5482 - val_MinusLogProbMetric: 17.5482 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 571/1000
2023-10-10 02:58:50.975 
Epoch 571/1000 
	 loss: 17.3420, MinusLogProbMetric: 17.3420, val_loss: 17.5721, val_MinusLogProbMetric: 17.5721

Epoch 571: val_loss did not improve from 17.49776
196/196 - 56s - loss: 17.3420 - MinusLogProbMetric: 17.3420 - val_loss: 17.5721 - val_MinusLogProbMetric: 17.5721 - lr: 1.6667e-04 - 56s/epoch - 285ms/step
Epoch 572/1000
2023-10-10 02:59:48.706 
Epoch 572/1000 
	 loss: 17.3411, MinusLogProbMetric: 17.3411, val_loss: 17.5343, val_MinusLogProbMetric: 17.5343

Epoch 572: val_loss did not improve from 17.49776
196/196 - 58s - loss: 17.3411 - MinusLogProbMetric: 17.3411 - val_loss: 17.5343 - val_MinusLogProbMetric: 17.5343 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 573/1000
2023-10-10 03:00:46.803 
Epoch 573/1000 
	 loss: 17.3474, MinusLogProbMetric: 17.3474, val_loss: 17.4946, val_MinusLogProbMetric: 17.4946

Epoch 573: val_loss improved from 17.49776 to 17.49459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.3474 - MinusLogProbMetric: 17.3474 - val_loss: 17.4946 - val_MinusLogProbMetric: 17.4946 - lr: 1.6667e-04 - 59s/epoch - 302ms/step
Epoch 574/1000
2023-10-10 03:01:45.068 
Epoch 574/1000 
	 loss: 17.3402, MinusLogProbMetric: 17.3402, val_loss: 17.4843, val_MinusLogProbMetric: 17.4843

Epoch 574: val_loss improved from 17.49459 to 17.48426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.3402 - MinusLogProbMetric: 17.3402 - val_loss: 17.4843 - val_MinusLogProbMetric: 17.4843 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 575/1000
2023-10-10 03:02:43.616 
Epoch 575/1000 
	 loss: 17.3421, MinusLogProbMetric: 17.3421, val_loss: 17.4943, val_MinusLogProbMetric: 17.4943

Epoch 575: val_loss did not improve from 17.48426
196/196 - 58s - loss: 17.3421 - MinusLogProbMetric: 17.3421 - val_loss: 17.4943 - val_MinusLogProbMetric: 17.4943 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 576/1000
2023-10-10 03:03:45.165 
Epoch 576/1000 
	 loss: 17.3637, MinusLogProbMetric: 17.3637, val_loss: 17.6000, val_MinusLogProbMetric: 17.6000

Epoch 576: val_loss did not improve from 17.48426
196/196 - 62s - loss: 17.3637 - MinusLogProbMetric: 17.3637 - val_loss: 17.6000 - val_MinusLogProbMetric: 17.6000 - lr: 1.6667e-04 - 62s/epoch - 314ms/step
Epoch 577/1000
2023-10-10 03:04:44.181 
Epoch 577/1000 
	 loss: 17.3404, MinusLogProbMetric: 17.3404, val_loss: 17.4882, val_MinusLogProbMetric: 17.4882

Epoch 577: val_loss did not improve from 17.48426
196/196 - 59s - loss: 17.3404 - MinusLogProbMetric: 17.3404 - val_loss: 17.4882 - val_MinusLogProbMetric: 17.4882 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 578/1000
2023-10-10 03:05:42.700 
Epoch 578/1000 
	 loss: 17.3383, MinusLogProbMetric: 17.3383, val_loss: 17.4696, val_MinusLogProbMetric: 17.4696

Epoch 578: val_loss improved from 17.48426 to 17.46962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 17.3383 - MinusLogProbMetric: 17.3383 - val_loss: 17.4696 - val_MinusLogProbMetric: 17.4696 - lr: 1.6667e-04 - 60s/epoch - 304ms/step
Epoch 579/1000
2023-10-10 03:06:40.499 
Epoch 579/1000 
	 loss: 17.3386, MinusLogProbMetric: 17.3386, val_loss: 17.4791, val_MinusLogProbMetric: 17.4791

Epoch 579: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3386 - MinusLogProbMetric: 17.3386 - val_loss: 17.4791 - val_MinusLogProbMetric: 17.4791 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 580/1000
2023-10-10 03:07:37.690 
Epoch 580/1000 
	 loss: 17.3469, MinusLogProbMetric: 17.3469, val_loss: 17.5487, val_MinusLogProbMetric: 17.5487

Epoch 580: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3469 - MinusLogProbMetric: 17.3469 - val_loss: 17.5487 - val_MinusLogProbMetric: 17.5487 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 581/1000
2023-10-10 03:08:35.104 
Epoch 581/1000 
	 loss: 17.3403, MinusLogProbMetric: 17.3403, val_loss: 17.6105, val_MinusLogProbMetric: 17.6105

Epoch 581: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3403 - MinusLogProbMetric: 17.3403 - val_loss: 17.6105 - val_MinusLogProbMetric: 17.6105 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 582/1000
2023-10-10 03:09:32.886 
Epoch 582/1000 
	 loss: 17.4177, MinusLogProbMetric: 17.4177, val_loss: 17.5180, val_MinusLogProbMetric: 17.5180

Epoch 582: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.4177 - MinusLogProbMetric: 17.4177 - val_loss: 17.5180 - val_MinusLogProbMetric: 17.5180 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 583/1000
2023-10-10 03:10:31.051 
Epoch 583/1000 
	 loss: 17.3273, MinusLogProbMetric: 17.3273, val_loss: 17.5108, val_MinusLogProbMetric: 17.5108

Epoch 583: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3273 - MinusLogProbMetric: 17.3273 - val_loss: 17.5108 - val_MinusLogProbMetric: 17.5108 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 584/1000
2023-10-10 03:11:28.009 
Epoch 584/1000 
	 loss: 17.3257, MinusLogProbMetric: 17.3257, val_loss: 17.5468, val_MinusLogProbMetric: 17.5468

Epoch 584: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3257 - MinusLogProbMetric: 17.3257 - val_loss: 17.5468 - val_MinusLogProbMetric: 17.5468 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 585/1000
2023-10-10 03:12:24.992 
Epoch 585/1000 
	 loss: 17.3843, MinusLogProbMetric: 17.3843, val_loss: 17.5530, val_MinusLogProbMetric: 17.5530

Epoch 585: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3843 - MinusLogProbMetric: 17.3843 - val_loss: 17.5530 - val_MinusLogProbMetric: 17.5530 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 586/1000
2023-10-10 03:13:22.638 
Epoch 586/1000 
	 loss: 17.3308, MinusLogProbMetric: 17.3308, val_loss: 17.4943, val_MinusLogProbMetric: 17.4943

Epoch 586: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3308 - MinusLogProbMetric: 17.3308 - val_loss: 17.4943 - val_MinusLogProbMetric: 17.4943 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 587/1000
2023-10-10 03:14:20.671 
Epoch 587/1000 
	 loss: 17.3424, MinusLogProbMetric: 17.3424, val_loss: 17.5286, val_MinusLogProbMetric: 17.5286

Epoch 587: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3424 - MinusLogProbMetric: 17.3424 - val_loss: 17.5286 - val_MinusLogProbMetric: 17.5286 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 588/1000
2023-10-10 03:15:18.140 
Epoch 588/1000 
	 loss: 17.3347, MinusLogProbMetric: 17.3347, val_loss: 17.5494, val_MinusLogProbMetric: 17.5494

Epoch 588: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3347 - MinusLogProbMetric: 17.3347 - val_loss: 17.5494 - val_MinusLogProbMetric: 17.5494 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 589/1000
2023-10-10 03:16:15.478 
Epoch 589/1000 
	 loss: 17.3236, MinusLogProbMetric: 17.3236, val_loss: 17.5201, val_MinusLogProbMetric: 17.5201

Epoch 589: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3236 - MinusLogProbMetric: 17.3236 - val_loss: 17.5201 - val_MinusLogProbMetric: 17.5201 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 590/1000
2023-10-10 03:17:12.938 
Epoch 590/1000 
	 loss: 17.3282, MinusLogProbMetric: 17.3282, val_loss: 17.6039, val_MinusLogProbMetric: 17.6039

Epoch 590: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3282 - MinusLogProbMetric: 17.3282 - val_loss: 17.6039 - val_MinusLogProbMetric: 17.6039 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 591/1000
2023-10-10 03:18:09.723 
Epoch 591/1000 
	 loss: 17.3093, MinusLogProbMetric: 17.3093, val_loss: 17.5019, val_MinusLogProbMetric: 17.5019

Epoch 591: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3093 - MinusLogProbMetric: 17.3093 - val_loss: 17.5019 - val_MinusLogProbMetric: 17.5019 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 592/1000
2023-10-10 03:19:07.433 
Epoch 592/1000 
	 loss: 17.3271, MinusLogProbMetric: 17.3271, val_loss: 17.5579, val_MinusLogProbMetric: 17.5579

Epoch 592: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3271 - MinusLogProbMetric: 17.3271 - val_loss: 17.5579 - val_MinusLogProbMetric: 17.5579 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 593/1000
2023-10-10 03:20:04.117 
Epoch 593/1000 
	 loss: 17.3674, MinusLogProbMetric: 17.3674, val_loss: 17.4896, val_MinusLogProbMetric: 17.4896

Epoch 593: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3674 - MinusLogProbMetric: 17.3674 - val_loss: 17.4896 - val_MinusLogProbMetric: 17.4896 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 594/1000
2023-10-10 03:21:00.778 
Epoch 594/1000 
	 loss: 17.3319, MinusLogProbMetric: 17.3319, val_loss: 17.4841, val_MinusLogProbMetric: 17.4841

Epoch 594: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3319 - MinusLogProbMetric: 17.3319 - val_loss: 17.4841 - val_MinusLogProbMetric: 17.4841 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 595/1000
2023-10-10 03:21:57.023 
Epoch 595/1000 
	 loss: 17.3813, MinusLogProbMetric: 17.3813, val_loss: 17.5102, val_MinusLogProbMetric: 17.5102

Epoch 595: val_loss did not improve from 17.46962
196/196 - 56s - loss: 17.3813 - MinusLogProbMetric: 17.3813 - val_loss: 17.5102 - val_MinusLogProbMetric: 17.5102 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 596/1000
2023-10-10 03:22:54.429 
Epoch 596/1000 
	 loss: 17.3237, MinusLogProbMetric: 17.3237, val_loss: 17.4920, val_MinusLogProbMetric: 17.4920

Epoch 596: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3237 - MinusLogProbMetric: 17.3237 - val_loss: 17.4920 - val_MinusLogProbMetric: 17.4920 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 597/1000
2023-10-10 03:23:50.560 
Epoch 597/1000 
	 loss: 17.3506, MinusLogProbMetric: 17.3506, val_loss: 17.5051, val_MinusLogProbMetric: 17.5051

Epoch 597: val_loss did not improve from 17.46962
196/196 - 56s - loss: 17.3506 - MinusLogProbMetric: 17.3506 - val_loss: 17.5051 - val_MinusLogProbMetric: 17.5051 - lr: 1.6667e-04 - 56s/epoch - 286ms/step
Epoch 598/1000
2023-10-10 03:24:48.078 
Epoch 598/1000 
	 loss: 17.3119, MinusLogProbMetric: 17.3119, val_loss: 17.5003, val_MinusLogProbMetric: 17.5003

Epoch 598: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3119 - MinusLogProbMetric: 17.3119 - val_loss: 17.5003 - val_MinusLogProbMetric: 17.5003 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 599/1000
2023-10-10 03:25:46.669 
Epoch 599/1000 
	 loss: 17.3223, MinusLogProbMetric: 17.3223, val_loss: 17.5377, val_MinusLogProbMetric: 17.5377

Epoch 599: val_loss did not improve from 17.46962
196/196 - 59s - loss: 17.3223 - MinusLogProbMetric: 17.3223 - val_loss: 17.5377 - val_MinusLogProbMetric: 17.5377 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 600/1000
2023-10-10 03:26:43.374 
Epoch 600/1000 
	 loss: 17.3160, MinusLogProbMetric: 17.3160, val_loss: 17.5120, val_MinusLogProbMetric: 17.5120

Epoch 600: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3160 - MinusLogProbMetric: 17.3160 - val_loss: 17.5120 - val_MinusLogProbMetric: 17.5120 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 601/1000
2023-10-10 03:27:41.121 
Epoch 601/1000 
	 loss: 17.3194, MinusLogProbMetric: 17.3194, val_loss: 17.6403, val_MinusLogProbMetric: 17.6403

Epoch 601: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3194 - MinusLogProbMetric: 17.3194 - val_loss: 17.6403 - val_MinusLogProbMetric: 17.6403 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 602/1000
2023-10-10 03:28:38.802 
Epoch 602/1000 
	 loss: 17.3132, MinusLogProbMetric: 17.3132, val_loss: 17.5818, val_MinusLogProbMetric: 17.5818

Epoch 602: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3132 - MinusLogProbMetric: 17.3132 - val_loss: 17.5818 - val_MinusLogProbMetric: 17.5818 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 603/1000
2023-10-10 03:29:39.526 
Epoch 603/1000 
	 loss: 17.3283, MinusLogProbMetric: 17.3283, val_loss: 17.5621, val_MinusLogProbMetric: 17.5621

Epoch 603: val_loss did not improve from 17.46962
196/196 - 61s - loss: 17.3283 - MinusLogProbMetric: 17.3283 - val_loss: 17.5621 - val_MinusLogProbMetric: 17.5621 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 604/1000
2023-10-10 03:30:37.418 
Epoch 604/1000 
	 loss: 17.3580, MinusLogProbMetric: 17.3580, val_loss: 17.5211, val_MinusLogProbMetric: 17.5211

Epoch 604: val_loss did not improve from 17.46962
196/196 - 58s - loss: 17.3580 - MinusLogProbMetric: 17.3580 - val_loss: 17.5211 - val_MinusLogProbMetric: 17.5211 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 605/1000
2023-10-10 03:31:34.463 
Epoch 605/1000 
	 loss: 17.3125, MinusLogProbMetric: 17.3125, val_loss: 17.4713, val_MinusLogProbMetric: 17.4713

Epoch 605: val_loss did not improve from 17.46962
196/196 - 57s - loss: 17.3125 - MinusLogProbMetric: 17.3125 - val_loss: 17.4713 - val_MinusLogProbMetric: 17.4713 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 606/1000
2023-10-10 03:32:30.824 
Epoch 606/1000 
	 loss: 17.3243, MinusLogProbMetric: 17.3243, val_loss: 17.4681, val_MinusLogProbMetric: 17.4681

Epoch 606: val_loss improved from 17.46962 to 17.46811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 57s - loss: 17.3243 - MinusLogProbMetric: 17.3243 - val_loss: 17.4681 - val_MinusLogProbMetric: 17.4681 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 607/1000
2023-10-10 03:33:28.116 
Epoch 607/1000 
	 loss: 17.3126, MinusLogProbMetric: 17.3126, val_loss: 17.4931, val_MinusLogProbMetric: 17.4931

Epoch 607: val_loss did not improve from 17.46811
196/196 - 56s - loss: 17.3126 - MinusLogProbMetric: 17.3126 - val_loss: 17.4931 - val_MinusLogProbMetric: 17.4931 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 608/1000
2023-10-10 03:34:25.649 
Epoch 608/1000 
	 loss: 17.3154, MinusLogProbMetric: 17.3154, val_loss: 17.5294, val_MinusLogProbMetric: 17.5294

Epoch 608: val_loss did not improve from 17.46811
196/196 - 58s - loss: 17.3154 - MinusLogProbMetric: 17.3154 - val_loss: 17.5294 - val_MinusLogProbMetric: 17.5294 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 609/1000
2023-10-10 03:35:23.054 
Epoch 609/1000 
	 loss: 17.3236, MinusLogProbMetric: 17.3236, val_loss: 17.6190, val_MinusLogProbMetric: 17.6190

Epoch 609: val_loss did not improve from 17.46811
196/196 - 57s - loss: 17.3236 - MinusLogProbMetric: 17.3236 - val_loss: 17.6190 - val_MinusLogProbMetric: 17.6190 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 610/1000
2023-10-10 03:36:19.870 
Epoch 610/1000 
	 loss: 17.3246, MinusLogProbMetric: 17.3246, val_loss: 17.5391, val_MinusLogProbMetric: 17.5391

Epoch 610: val_loss did not improve from 17.46811
196/196 - 57s - loss: 17.3246 - MinusLogProbMetric: 17.3246 - val_loss: 17.5391 - val_MinusLogProbMetric: 17.5391 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 611/1000
2023-10-10 03:37:17.154 
Epoch 611/1000 
	 loss: 17.5303, MinusLogProbMetric: 17.5303, val_loss: 17.6460, val_MinusLogProbMetric: 17.6460

Epoch 611: val_loss did not improve from 17.46811
196/196 - 57s - loss: 17.5303 - MinusLogProbMetric: 17.5303 - val_loss: 17.6460 - val_MinusLogProbMetric: 17.6460 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 612/1000
2023-10-10 03:38:14.694 
Epoch 612/1000 
	 loss: 17.2977, MinusLogProbMetric: 17.2977, val_loss: 17.4732, val_MinusLogProbMetric: 17.4732

Epoch 612: val_loss did not improve from 17.46811
196/196 - 58s - loss: 17.2977 - MinusLogProbMetric: 17.2977 - val_loss: 17.4732 - val_MinusLogProbMetric: 17.4732 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 613/1000
2023-10-10 03:39:11.437 
Epoch 613/1000 
	 loss: 17.3010, MinusLogProbMetric: 17.3010, val_loss: 17.4562, val_MinusLogProbMetric: 17.4562

Epoch 613: val_loss improved from 17.46811 to 17.45617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.3010 - MinusLogProbMetric: 17.3010 - val_loss: 17.4562 - val_MinusLogProbMetric: 17.4562 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 614/1000
2023-10-10 03:40:10.123 
Epoch 614/1000 
	 loss: 17.2995, MinusLogProbMetric: 17.2995, val_loss: 17.4682, val_MinusLogProbMetric: 17.4682

Epoch 614: val_loss did not improve from 17.45617
196/196 - 58s - loss: 17.2995 - MinusLogProbMetric: 17.2995 - val_loss: 17.4682 - val_MinusLogProbMetric: 17.4682 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 615/1000
2023-10-10 03:41:08.161 
Epoch 615/1000 
	 loss: 17.3200, MinusLogProbMetric: 17.3200, val_loss: 17.4444, val_MinusLogProbMetric: 17.4444

Epoch 615: val_loss improved from 17.45617 to 17.44441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.3200 - MinusLogProbMetric: 17.3200 - val_loss: 17.4444 - val_MinusLogProbMetric: 17.4444 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 616/1000
2023-10-10 03:42:05.674 
Epoch 616/1000 
	 loss: 17.2931, MinusLogProbMetric: 17.2931, val_loss: 17.5329, val_MinusLogProbMetric: 17.5329

Epoch 616: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2931 - MinusLogProbMetric: 17.2931 - val_loss: 17.5329 - val_MinusLogProbMetric: 17.5329 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 617/1000
2023-10-10 03:43:03.231 
Epoch 617/1000 
	 loss: 17.3307, MinusLogProbMetric: 17.3307, val_loss: 17.4772, val_MinusLogProbMetric: 17.4772

Epoch 617: val_loss did not improve from 17.44441
196/196 - 58s - loss: 17.3307 - MinusLogProbMetric: 17.3307 - val_loss: 17.4772 - val_MinusLogProbMetric: 17.4772 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 618/1000
2023-10-10 03:44:00.004 
Epoch 618/1000 
	 loss: 17.3614, MinusLogProbMetric: 17.3614, val_loss: 17.4858, val_MinusLogProbMetric: 17.4858

Epoch 618: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.3614 - MinusLogProbMetric: 17.3614 - val_loss: 17.4858 - val_MinusLogProbMetric: 17.4858 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 619/1000
2023-10-10 03:44:57.723 
Epoch 619/1000 
	 loss: 17.2955, MinusLogProbMetric: 17.2955, val_loss: 17.4454, val_MinusLogProbMetric: 17.4454

Epoch 619: val_loss did not improve from 17.44441
196/196 - 58s - loss: 17.2955 - MinusLogProbMetric: 17.2955 - val_loss: 17.4454 - val_MinusLogProbMetric: 17.4454 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 620/1000
2023-10-10 03:45:54.414 
Epoch 620/1000 
	 loss: 17.3056, MinusLogProbMetric: 17.3056, val_loss: 17.4840, val_MinusLogProbMetric: 17.4840

Epoch 620: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.3056 - MinusLogProbMetric: 17.3056 - val_loss: 17.4840 - val_MinusLogProbMetric: 17.4840 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 621/1000
2023-10-10 03:46:51.735 
Epoch 621/1000 
	 loss: 17.2946, MinusLogProbMetric: 17.2946, val_loss: 17.4857, val_MinusLogProbMetric: 17.4857

Epoch 621: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2946 - MinusLogProbMetric: 17.2946 - val_loss: 17.4857 - val_MinusLogProbMetric: 17.4857 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 622/1000
2023-10-10 03:47:49.147 
Epoch 622/1000 
	 loss: 17.4390, MinusLogProbMetric: 17.4390, val_loss: 17.5468, val_MinusLogProbMetric: 17.5468

Epoch 622: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.4390 - MinusLogProbMetric: 17.4390 - val_loss: 17.5468 - val_MinusLogProbMetric: 17.5468 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 623/1000
2023-10-10 03:48:46.294 
Epoch 623/1000 
	 loss: 17.2988, MinusLogProbMetric: 17.2988, val_loss: 17.4566, val_MinusLogProbMetric: 17.4566

Epoch 623: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2988 - MinusLogProbMetric: 17.2988 - val_loss: 17.4566 - val_MinusLogProbMetric: 17.4566 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 624/1000
2023-10-10 03:49:43.416 
Epoch 624/1000 
	 loss: 17.3094, MinusLogProbMetric: 17.3094, val_loss: 17.5395, val_MinusLogProbMetric: 17.5395

Epoch 624: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.3094 - MinusLogProbMetric: 17.3094 - val_loss: 17.5395 - val_MinusLogProbMetric: 17.5395 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 625/1000
2023-10-10 03:50:40.292 
Epoch 625/1000 
	 loss: 17.2892, MinusLogProbMetric: 17.2892, val_loss: 17.5070, val_MinusLogProbMetric: 17.5070

Epoch 625: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2892 - MinusLogProbMetric: 17.2892 - val_loss: 17.5070 - val_MinusLogProbMetric: 17.5070 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 626/1000
2023-10-10 03:51:37.961 
Epoch 626/1000 
	 loss: 17.2929, MinusLogProbMetric: 17.2929, val_loss: 17.4484, val_MinusLogProbMetric: 17.4484

Epoch 626: val_loss did not improve from 17.44441
196/196 - 58s - loss: 17.2929 - MinusLogProbMetric: 17.2929 - val_loss: 17.4484 - val_MinusLogProbMetric: 17.4484 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 627/1000
2023-10-10 03:52:34.651 
Epoch 627/1000 
	 loss: 17.3044, MinusLogProbMetric: 17.3044, val_loss: 17.5252, val_MinusLogProbMetric: 17.5252

Epoch 627: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.3044 - MinusLogProbMetric: 17.3044 - val_loss: 17.5252 - val_MinusLogProbMetric: 17.5252 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 628/1000
2023-10-10 03:53:31.280 
Epoch 628/1000 
	 loss: 17.3163, MinusLogProbMetric: 17.3163, val_loss: 17.5408, val_MinusLogProbMetric: 17.5408

Epoch 628: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.3163 - MinusLogProbMetric: 17.3163 - val_loss: 17.5408 - val_MinusLogProbMetric: 17.5408 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 629/1000
2023-10-10 03:54:29.060 
Epoch 629/1000 
	 loss: 17.3802, MinusLogProbMetric: 17.3802, val_loss: 17.5090, val_MinusLogProbMetric: 17.5090

Epoch 629: val_loss did not improve from 17.44441
196/196 - 58s - loss: 17.3802 - MinusLogProbMetric: 17.3802 - val_loss: 17.5090 - val_MinusLogProbMetric: 17.5090 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 630/1000
2023-10-10 03:55:26.420 
Epoch 630/1000 
	 loss: 17.2835, MinusLogProbMetric: 17.2835, val_loss: 17.5012, val_MinusLogProbMetric: 17.5012

Epoch 630: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2835 - MinusLogProbMetric: 17.2835 - val_loss: 17.5012 - val_MinusLogProbMetric: 17.5012 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 631/1000
2023-10-10 03:56:23.790 
Epoch 631/1000 
	 loss: 17.3384, MinusLogProbMetric: 17.3384, val_loss: 17.5208, val_MinusLogProbMetric: 17.5208

Epoch 631: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.3384 - MinusLogProbMetric: 17.3384 - val_loss: 17.5208 - val_MinusLogProbMetric: 17.5208 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 632/1000
2023-10-10 03:57:21.194 
Epoch 632/1000 
	 loss: 17.2873, MinusLogProbMetric: 17.2873, val_loss: 17.4905, val_MinusLogProbMetric: 17.4905

Epoch 632: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2873 - MinusLogProbMetric: 17.2873 - val_loss: 17.4905 - val_MinusLogProbMetric: 17.4905 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 633/1000
2023-10-10 03:58:18.403 
Epoch 633/1000 
	 loss: 17.2828, MinusLogProbMetric: 17.2828, val_loss: 17.4498, val_MinusLogProbMetric: 17.4498

Epoch 633: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2828 - MinusLogProbMetric: 17.2828 - val_loss: 17.4498 - val_MinusLogProbMetric: 17.4498 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 634/1000
2023-10-10 03:59:14.852 
Epoch 634/1000 
	 loss: 17.2825, MinusLogProbMetric: 17.2825, val_loss: 17.5820, val_MinusLogProbMetric: 17.5820

Epoch 634: val_loss did not improve from 17.44441
196/196 - 56s - loss: 17.2825 - MinusLogProbMetric: 17.2825 - val_loss: 17.5820 - val_MinusLogProbMetric: 17.5820 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 635/1000
2023-10-10 04:00:11.178 
Epoch 635/1000 
	 loss: 17.2917, MinusLogProbMetric: 17.2917, val_loss: 17.5165, val_MinusLogProbMetric: 17.5165

Epoch 635: val_loss did not improve from 17.44441
196/196 - 56s - loss: 17.2917 - MinusLogProbMetric: 17.2917 - val_loss: 17.5165 - val_MinusLogProbMetric: 17.5165 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 636/1000
2023-10-10 04:01:08.569 
Epoch 636/1000 
	 loss: 17.2858, MinusLogProbMetric: 17.2858, val_loss: 17.4876, val_MinusLogProbMetric: 17.4876

Epoch 636: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2858 - MinusLogProbMetric: 17.2858 - val_loss: 17.4876 - val_MinusLogProbMetric: 17.4876 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 637/1000
2023-10-10 04:02:06.005 
Epoch 637/1000 
	 loss: 17.2746, MinusLogProbMetric: 17.2746, val_loss: 17.4996, val_MinusLogProbMetric: 17.4996

Epoch 637: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2746 - MinusLogProbMetric: 17.2746 - val_loss: 17.4996 - val_MinusLogProbMetric: 17.4996 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 638/1000
2023-10-10 04:03:03.189 
Epoch 638/1000 
	 loss: 17.2911, MinusLogProbMetric: 17.2911, val_loss: 17.4965, val_MinusLogProbMetric: 17.4965

Epoch 638: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2911 - MinusLogProbMetric: 17.2911 - val_loss: 17.4965 - val_MinusLogProbMetric: 17.4965 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 639/1000
2023-10-10 04:04:01.139 
Epoch 639/1000 
	 loss: 17.3910, MinusLogProbMetric: 17.3910, val_loss: 17.5226, val_MinusLogProbMetric: 17.5226

Epoch 639: val_loss did not improve from 17.44441
196/196 - 58s - loss: 17.3910 - MinusLogProbMetric: 17.3910 - val_loss: 17.5226 - val_MinusLogProbMetric: 17.5226 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 640/1000
2023-10-10 04:04:57.646 
Epoch 640/1000 
	 loss: 17.2893, MinusLogProbMetric: 17.2893, val_loss: 17.5380, val_MinusLogProbMetric: 17.5380

Epoch 640: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2893 - MinusLogProbMetric: 17.2893 - val_loss: 17.5380 - val_MinusLogProbMetric: 17.5380 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 641/1000
2023-10-10 04:05:54.460 
Epoch 641/1000 
	 loss: 17.2921, MinusLogProbMetric: 17.2921, val_loss: 17.4549, val_MinusLogProbMetric: 17.4549

Epoch 641: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2921 - MinusLogProbMetric: 17.2921 - val_loss: 17.4549 - val_MinusLogProbMetric: 17.4549 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 642/1000
2023-10-10 04:06:52.409 
Epoch 642/1000 
	 loss: 17.2806, MinusLogProbMetric: 17.2806, val_loss: 17.5928, val_MinusLogProbMetric: 17.5928

Epoch 642: val_loss did not improve from 17.44441
196/196 - 58s - loss: 17.2806 - MinusLogProbMetric: 17.2806 - val_loss: 17.5928 - val_MinusLogProbMetric: 17.5928 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 643/1000
2023-10-10 04:07:49.340 
Epoch 643/1000 
	 loss: 17.2865, MinusLogProbMetric: 17.2865, val_loss: 17.4804, val_MinusLogProbMetric: 17.4804

Epoch 643: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2865 - MinusLogProbMetric: 17.2865 - val_loss: 17.4804 - val_MinusLogProbMetric: 17.4804 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 644/1000
2023-10-10 04:08:46.777 
Epoch 644/1000 
	 loss: 17.2946, MinusLogProbMetric: 17.2946, val_loss: 17.5623, val_MinusLogProbMetric: 17.5623

Epoch 644: val_loss did not improve from 17.44441
196/196 - 57s - loss: 17.2946 - MinusLogProbMetric: 17.2946 - val_loss: 17.5623 - val_MinusLogProbMetric: 17.5623 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 645/1000
2023-10-10 04:09:43.984 
Epoch 645/1000 
	 loss: 17.2734, MinusLogProbMetric: 17.2734, val_loss: 17.4102, val_MinusLogProbMetric: 17.4102

Epoch 645: val_loss improved from 17.44441 to 17.41019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.2734 - MinusLogProbMetric: 17.2734 - val_loss: 17.4102 - val_MinusLogProbMetric: 17.4102 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 646/1000
2023-10-10 04:10:42.654 
Epoch 646/1000 
	 loss: 17.2868, MinusLogProbMetric: 17.2868, val_loss: 17.4495, val_MinusLogProbMetric: 17.4495

Epoch 646: val_loss did not improve from 17.41019
196/196 - 58s - loss: 17.2868 - MinusLogProbMetric: 17.2868 - val_loss: 17.4495 - val_MinusLogProbMetric: 17.4495 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 647/1000
2023-10-10 04:11:40.390 
Epoch 647/1000 
	 loss: 17.2844, MinusLogProbMetric: 17.2844, val_loss: 17.5125, val_MinusLogProbMetric: 17.5125

Epoch 647: val_loss did not improve from 17.41019
196/196 - 58s - loss: 17.2844 - MinusLogProbMetric: 17.2844 - val_loss: 17.5125 - val_MinusLogProbMetric: 17.5125 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 648/1000
2023-10-10 04:12:38.130 
Epoch 648/1000 
	 loss: 17.2899, MinusLogProbMetric: 17.2899, val_loss: 17.4033, val_MinusLogProbMetric: 17.4033

Epoch 648: val_loss improved from 17.41019 to 17.40326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.2899 - MinusLogProbMetric: 17.2899 - val_loss: 17.4033 - val_MinusLogProbMetric: 17.4033 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 649/1000
2023-10-10 04:13:36.603 
Epoch 649/1000 
	 loss: 17.2680, MinusLogProbMetric: 17.2680, val_loss: 17.4802, val_MinusLogProbMetric: 17.4802

Epoch 649: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2680 - MinusLogProbMetric: 17.2680 - val_loss: 17.4802 - val_MinusLogProbMetric: 17.4802 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 650/1000
2023-10-10 04:14:34.160 
Epoch 650/1000 
	 loss: 17.2680, MinusLogProbMetric: 17.2680, val_loss: 17.4464, val_MinusLogProbMetric: 17.4464

Epoch 650: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2680 - MinusLogProbMetric: 17.2680 - val_loss: 17.4464 - val_MinusLogProbMetric: 17.4464 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 651/1000
2023-10-10 04:15:31.888 
Epoch 651/1000 
	 loss: 17.2799, MinusLogProbMetric: 17.2799, val_loss: 17.4759, val_MinusLogProbMetric: 17.4759

Epoch 651: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2799 - MinusLogProbMetric: 17.2799 - val_loss: 17.4759 - val_MinusLogProbMetric: 17.4759 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 652/1000
2023-10-10 04:16:29.689 
Epoch 652/1000 
	 loss: 17.2903, MinusLogProbMetric: 17.2903, val_loss: 17.4763, val_MinusLogProbMetric: 17.4763

Epoch 652: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2903 - MinusLogProbMetric: 17.2903 - val_loss: 17.4763 - val_MinusLogProbMetric: 17.4763 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 653/1000
2023-10-10 04:17:26.436 
Epoch 653/1000 
	 loss: 17.2906, MinusLogProbMetric: 17.2906, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 653: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2906 - MinusLogProbMetric: 17.2906 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 654/1000
2023-10-10 04:18:24.511 
Epoch 654/1000 
	 loss: 17.2790, MinusLogProbMetric: 17.2790, val_loss: 17.4547, val_MinusLogProbMetric: 17.4547

Epoch 654: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2790 - MinusLogProbMetric: 17.2790 - val_loss: 17.4547 - val_MinusLogProbMetric: 17.4547 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 655/1000
2023-10-10 04:19:21.646 
Epoch 655/1000 
	 loss: 17.2722, MinusLogProbMetric: 17.2722, val_loss: 17.4619, val_MinusLogProbMetric: 17.4619

Epoch 655: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2722 - MinusLogProbMetric: 17.2722 - val_loss: 17.4619 - val_MinusLogProbMetric: 17.4619 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 656/1000
2023-10-10 04:20:19.682 
Epoch 656/1000 
	 loss: 17.2943, MinusLogProbMetric: 17.2943, val_loss: 17.5872, val_MinusLogProbMetric: 17.5872

Epoch 656: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2943 - MinusLogProbMetric: 17.2943 - val_loss: 17.5872 - val_MinusLogProbMetric: 17.5872 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 657/1000
2023-10-10 04:21:17.129 
Epoch 657/1000 
	 loss: 17.3571, MinusLogProbMetric: 17.3571, val_loss: 17.4413, val_MinusLogProbMetric: 17.4413

Epoch 657: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.3571 - MinusLogProbMetric: 17.3571 - val_loss: 17.4413 - val_MinusLogProbMetric: 17.4413 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 658/1000
2023-10-10 04:22:14.664 
Epoch 658/1000 
	 loss: 17.2660, MinusLogProbMetric: 17.2660, val_loss: 17.4379, val_MinusLogProbMetric: 17.4379

Epoch 658: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2660 - MinusLogProbMetric: 17.2660 - val_loss: 17.4379 - val_MinusLogProbMetric: 17.4379 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 659/1000
2023-10-10 04:23:13.043 
Epoch 659/1000 
	 loss: 17.2783, MinusLogProbMetric: 17.2783, val_loss: 17.5260, val_MinusLogProbMetric: 17.5260

Epoch 659: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2783 - MinusLogProbMetric: 17.2783 - val_loss: 17.5260 - val_MinusLogProbMetric: 17.5260 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 660/1000
2023-10-10 04:24:10.441 
Epoch 660/1000 
	 loss: 17.2527, MinusLogProbMetric: 17.2527, val_loss: 17.4767, val_MinusLogProbMetric: 17.4767

Epoch 660: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2527 - MinusLogProbMetric: 17.2527 - val_loss: 17.4767 - val_MinusLogProbMetric: 17.4767 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 661/1000
2023-10-10 04:25:07.521 
Epoch 661/1000 
	 loss: 17.2591, MinusLogProbMetric: 17.2591, val_loss: 17.5180, val_MinusLogProbMetric: 17.5180

Epoch 661: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2591 - MinusLogProbMetric: 17.2591 - val_loss: 17.5180 - val_MinusLogProbMetric: 17.5180 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 662/1000
2023-10-10 04:26:06.006 
Epoch 662/1000 
	 loss: 17.2595, MinusLogProbMetric: 17.2595, val_loss: 17.5200, val_MinusLogProbMetric: 17.5200

Epoch 662: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2595 - MinusLogProbMetric: 17.2595 - val_loss: 17.5200 - val_MinusLogProbMetric: 17.5200 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 663/1000
2023-10-10 04:27:03.967 
Epoch 663/1000 
	 loss: 17.2559, MinusLogProbMetric: 17.2559, val_loss: 17.4853, val_MinusLogProbMetric: 17.4853

Epoch 663: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2559 - MinusLogProbMetric: 17.2559 - val_loss: 17.4853 - val_MinusLogProbMetric: 17.4853 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 664/1000
2023-10-10 04:28:01.854 
Epoch 664/1000 
	 loss: 17.7122, MinusLogProbMetric: 17.7122, val_loss: 18.4464, val_MinusLogProbMetric: 18.4464

Epoch 664: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.7122 - MinusLogProbMetric: 17.7122 - val_loss: 18.4464 - val_MinusLogProbMetric: 18.4464 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 665/1000
2023-10-10 04:28:58.822 
Epoch 665/1000 
	 loss: 17.5920, MinusLogProbMetric: 17.5920, val_loss: 17.7045, val_MinusLogProbMetric: 17.7045

Epoch 665: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.5920 - MinusLogProbMetric: 17.5920 - val_loss: 17.7045 - val_MinusLogProbMetric: 17.7045 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 666/1000
2023-10-10 04:29:57.385 
Epoch 666/1000 
	 loss: 17.2870, MinusLogProbMetric: 17.2870, val_loss: 17.4838, val_MinusLogProbMetric: 17.4838

Epoch 666: val_loss did not improve from 17.40326
196/196 - 59s - loss: 17.2870 - MinusLogProbMetric: 17.2870 - val_loss: 17.4838 - val_MinusLogProbMetric: 17.4838 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 667/1000
2023-10-10 04:30:55.630 
Epoch 667/1000 
	 loss: 17.2565, MinusLogProbMetric: 17.2565, val_loss: 17.4911, val_MinusLogProbMetric: 17.4911

Epoch 667: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2565 - MinusLogProbMetric: 17.2565 - val_loss: 17.4911 - val_MinusLogProbMetric: 17.4911 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 668/1000
2023-10-10 04:31:54.160 
Epoch 668/1000 
	 loss: 17.2745, MinusLogProbMetric: 17.2745, val_loss: 17.4520, val_MinusLogProbMetric: 17.4520

Epoch 668: val_loss did not improve from 17.40326
196/196 - 59s - loss: 17.2745 - MinusLogProbMetric: 17.2745 - val_loss: 17.4520 - val_MinusLogProbMetric: 17.4520 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 669/1000
2023-10-10 04:32:53.571 
Epoch 669/1000 
	 loss: 17.2517, MinusLogProbMetric: 17.2517, val_loss: 17.4264, val_MinusLogProbMetric: 17.4264

Epoch 669: val_loss did not improve from 17.40326
196/196 - 59s - loss: 17.2517 - MinusLogProbMetric: 17.2517 - val_loss: 17.4264 - val_MinusLogProbMetric: 17.4264 - lr: 1.6667e-04 - 59s/epoch - 303ms/step
Epoch 670/1000
2023-10-10 04:33:51.316 
Epoch 670/1000 
	 loss: 17.2612, MinusLogProbMetric: 17.2612, val_loss: 17.4257, val_MinusLogProbMetric: 17.4257

Epoch 670: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2612 - MinusLogProbMetric: 17.2612 - val_loss: 17.4257 - val_MinusLogProbMetric: 17.4257 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 671/1000
2023-10-10 04:34:48.082 
Epoch 671/1000 
	 loss: 17.2802, MinusLogProbMetric: 17.2802, val_loss: 17.4807, val_MinusLogProbMetric: 17.4807

Epoch 671: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2802 - MinusLogProbMetric: 17.2802 - val_loss: 17.4807 - val_MinusLogProbMetric: 17.4807 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 672/1000
2023-10-10 04:35:45.981 
Epoch 672/1000 
	 loss: 17.2733, MinusLogProbMetric: 17.2733, val_loss: 17.4716, val_MinusLogProbMetric: 17.4716

Epoch 672: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2733 - MinusLogProbMetric: 17.2733 - val_loss: 17.4716 - val_MinusLogProbMetric: 17.4716 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 673/1000
2023-10-10 04:36:43.159 
Epoch 673/1000 
	 loss: 17.2753, MinusLogProbMetric: 17.2753, val_loss: 18.6606, val_MinusLogProbMetric: 18.6606

Epoch 673: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2753 - MinusLogProbMetric: 17.2753 - val_loss: 18.6606 - val_MinusLogProbMetric: 18.6606 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 674/1000
2023-10-10 04:37:40.360 
Epoch 674/1000 
	 loss: 17.2888, MinusLogProbMetric: 17.2888, val_loss: 17.4741, val_MinusLogProbMetric: 17.4741

Epoch 674: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2888 - MinusLogProbMetric: 17.2888 - val_loss: 17.4741 - val_MinusLogProbMetric: 17.4741 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 675/1000
2023-10-10 04:38:38.495 
Epoch 675/1000 
	 loss: 17.2582, MinusLogProbMetric: 17.2582, val_loss: 17.5611, val_MinusLogProbMetric: 17.5611

Epoch 675: val_loss did not improve from 17.40326
196/196 - 58s - loss: 17.2582 - MinusLogProbMetric: 17.2582 - val_loss: 17.5611 - val_MinusLogProbMetric: 17.5611 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 676/1000
2023-10-10 04:39:35.586 
Epoch 676/1000 
	 loss: 17.2602, MinusLogProbMetric: 17.2602, val_loss: 17.5491, val_MinusLogProbMetric: 17.5491

Epoch 676: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2602 - MinusLogProbMetric: 17.2602 - val_loss: 17.5491 - val_MinusLogProbMetric: 17.5491 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 677/1000
2023-10-10 04:40:31.698 
Epoch 677/1000 
	 loss: 17.2466, MinusLogProbMetric: 17.2466, val_loss: 17.4391, val_MinusLogProbMetric: 17.4391

Epoch 677: val_loss did not improve from 17.40326
196/196 - 56s - loss: 17.2466 - MinusLogProbMetric: 17.2466 - val_loss: 17.4391 - val_MinusLogProbMetric: 17.4391 - lr: 1.6667e-04 - 56s/epoch - 286ms/step
Epoch 678/1000
2023-10-10 04:41:28.410 
Epoch 678/1000 
	 loss: 17.2834, MinusLogProbMetric: 17.2834, val_loss: 17.4778, val_MinusLogProbMetric: 17.4778

Epoch 678: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2834 - MinusLogProbMetric: 17.2834 - val_loss: 17.4778 - val_MinusLogProbMetric: 17.4778 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 679/1000
2023-10-10 04:42:28.434 
Epoch 679/1000 
	 loss: 17.2467, MinusLogProbMetric: 17.2467, val_loss: 17.4504, val_MinusLogProbMetric: 17.4504

Epoch 679: val_loss did not improve from 17.40326
196/196 - 60s - loss: 17.2467 - MinusLogProbMetric: 17.2467 - val_loss: 17.4504 - val_MinusLogProbMetric: 17.4504 - lr: 1.6667e-04 - 60s/epoch - 306ms/step
Epoch 680/1000
2023-10-10 04:43:25.448 
Epoch 680/1000 
	 loss: 17.3788, MinusLogProbMetric: 17.3788, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 680: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.3788 - MinusLogProbMetric: 17.3788 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 681/1000
2023-10-10 04:44:22.666 
Epoch 681/1000 
	 loss: 17.2788, MinusLogProbMetric: 17.2788, val_loss: 17.4149, val_MinusLogProbMetric: 17.4149

Epoch 681: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2788 - MinusLogProbMetric: 17.2788 - val_loss: 17.4149 - val_MinusLogProbMetric: 17.4149 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 682/1000
2023-10-10 04:45:19.412 
Epoch 682/1000 
	 loss: 17.2418, MinusLogProbMetric: 17.2418, val_loss: 17.4247, val_MinusLogProbMetric: 17.4247

Epoch 682: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2418 - MinusLogProbMetric: 17.2418 - val_loss: 17.4247 - val_MinusLogProbMetric: 17.4247 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 683/1000
2023-10-10 04:46:15.569 
Epoch 683/1000 
	 loss: 17.2527, MinusLogProbMetric: 17.2527, val_loss: 17.4101, val_MinusLogProbMetric: 17.4101

Epoch 683: val_loss did not improve from 17.40326
196/196 - 56s - loss: 17.2527 - MinusLogProbMetric: 17.2527 - val_loss: 17.4101 - val_MinusLogProbMetric: 17.4101 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 684/1000
2023-10-10 04:47:12.564 
Epoch 684/1000 
	 loss: 17.2893, MinusLogProbMetric: 17.2893, val_loss: 17.4972, val_MinusLogProbMetric: 17.4972

Epoch 684: val_loss did not improve from 17.40326
196/196 - 57s - loss: 17.2893 - MinusLogProbMetric: 17.2893 - val_loss: 17.4972 - val_MinusLogProbMetric: 17.4972 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 685/1000
2023-10-10 04:48:09.922 
Epoch 685/1000 
	 loss: 17.2451, MinusLogProbMetric: 17.2451, val_loss: 17.4001, val_MinusLogProbMetric: 17.4001

Epoch 685: val_loss improved from 17.40326 to 17.40011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.2451 - MinusLogProbMetric: 17.2451 - val_loss: 17.4001 - val_MinusLogProbMetric: 17.4001 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 686/1000
2023-10-10 04:49:08.391 
Epoch 686/1000 
	 loss: 17.2421, MinusLogProbMetric: 17.2421, val_loss: 17.4677, val_MinusLogProbMetric: 17.4677

Epoch 686: val_loss did not improve from 17.40011
196/196 - 57s - loss: 17.2421 - MinusLogProbMetric: 17.2421 - val_loss: 17.4677 - val_MinusLogProbMetric: 17.4677 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 687/1000
2023-10-10 04:50:05.482 
Epoch 687/1000 
	 loss: 17.2485, MinusLogProbMetric: 17.2485, val_loss: 17.4231, val_MinusLogProbMetric: 17.4231

Epoch 687: val_loss did not improve from 17.40011
196/196 - 57s - loss: 17.2485 - MinusLogProbMetric: 17.2485 - val_loss: 17.4231 - val_MinusLogProbMetric: 17.4231 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 688/1000
2023-10-10 04:51:04.047 
Epoch 688/1000 
	 loss: 17.2530, MinusLogProbMetric: 17.2530, val_loss: 17.4830, val_MinusLogProbMetric: 17.4830

Epoch 688: val_loss did not improve from 17.40011
196/196 - 59s - loss: 17.2530 - MinusLogProbMetric: 17.2530 - val_loss: 17.4830 - val_MinusLogProbMetric: 17.4830 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 689/1000
2023-10-10 04:52:00.959 
Epoch 689/1000 
	 loss: 17.2304, MinusLogProbMetric: 17.2304, val_loss: 17.4767, val_MinusLogProbMetric: 17.4767

Epoch 689: val_loss did not improve from 17.40011
196/196 - 57s - loss: 17.2304 - MinusLogProbMetric: 17.2304 - val_loss: 17.4767 - val_MinusLogProbMetric: 17.4767 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 690/1000
2023-10-10 04:52:57.558 
Epoch 690/1000 
	 loss: 17.2766, MinusLogProbMetric: 17.2766, val_loss: 17.4849, val_MinusLogProbMetric: 17.4849

Epoch 690: val_loss did not improve from 17.40011
196/196 - 57s - loss: 17.2766 - MinusLogProbMetric: 17.2766 - val_loss: 17.4849 - val_MinusLogProbMetric: 17.4849 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 691/1000
2023-10-10 04:53:54.825 
Epoch 691/1000 
	 loss: 17.2468, MinusLogProbMetric: 17.2468, val_loss: 17.4416, val_MinusLogProbMetric: 17.4416

Epoch 691: val_loss did not improve from 17.40011
196/196 - 57s - loss: 17.2468 - MinusLogProbMetric: 17.2468 - val_loss: 17.4416 - val_MinusLogProbMetric: 17.4416 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 692/1000
2023-10-10 04:54:51.713 
Epoch 692/1000 
	 loss: 17.2432, MinusLogProbMetric: 17.2432, val_loss: 17.3881, val_MinusLogProbMetric: 17.3881

Epoch 692: val_loss improved from 17.40011 to 17.38811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.2432 - MinusLogProbMetric: 17.2432 - val_loss: 17.3881 - val_MinusLogProbMetric: 17.3881 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 693/1000
2023-10-10 04:55:50.394 
Epoch 693/1000 
	 loss: 17.2428, MinusLogProbMetric: 17.2428, val_loss: 17.4508, val_MinusLogProbMetric: 17.4508

Epoch 693: val_loss did not improve from 17.38811
196/196 - 58s - loss: 17.2428 - MinusLogProbMetric: 17.2428 - val_loss: 17.4508 - val_MinusLogProbMetric: 17.4508 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 694/1000
2023-10-10 04:56:48.725 
Epoch 694/1000 
	 loss: 17.2324, MinusLogProbMetric: 17.2324, val_loss: 17.4154, val_MinusLogProbMetric: 17.4154

Epoch 694: val_loss did not improve from 17.38811
196/196 - 58s - loss: 17.2324 - MinusLogProbMetric: 17.2324 - val_loss: 17.4154 - val_MinusLogProbMetric: 17.4154 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 695/1000
2023-10-10 04:57:46.642 
Epoch 695/1000 
	 loss: 17.2317, MinusLogProbMetric: 17.2317, val_loss: 17.4341, val_MinusLogProbMetric: 17.4341

Epoch 695: val_loss did not improve from 17.38811
196/196 - 58s - loss: 17.2317 - MinusLogProbMetric: 17.2317 - val_loss: 17.4341 - val_MinusLogProbMetric: 17.4341 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 696/1000
2023-10-10 04:58:43.153 
Epoch 696/1000 
	 loss: 17.2615, MinusLogProbMetric: 17.2615, val_loss: 17.4651, val_MinusLogProbMetric: 17.4651

Epoch 696: val_loss did not improve from 17.38811
196/196 - 57s - loss: 17.2615 - MinusLogProbMetric: 17.2615 - val_loss: 17.4651 - val_MinusLogProbMetric: 17.4651 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 697/1000
2023-10-10 04:59:40.246 
Epoch 697/1000 
	 loss: 17.2420, MinusLogProbMetric: 17.2420, val_loss: 17.3734, val_MinusLogProbMetric: 17.3734

Epoch 697: val_loss improved from 17.38811 to 17.37338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.2420 - MinusLogProbMetric: 17.2420 - val_loss: 17.3734 - val_MinusLogProbMetric: 17.3734 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 698/1000
2023-10-10 05:00:39.100 
Epoch 698/1000 
	 loss: 17.2846, MinusLogProbMetric: 17.2846, val_loss: 17.4572, val_MinusLogProbMetric: 17.4572

Epoch 698: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2846 - MinusLogProbMetric: 17.2846 - val_loss: 17.4572 - val_MinusLogProbMetric: 17.4572 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 699/1000
2023-10-10 05:01:37.530 
Epoch 699/1000 
	 loss: 17.2335, MinusLogProbMetric: 17.2335, val_loss: 17.4345, val_MinusLogProbMetric: 17.4345

Epoch 699: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2335 - MinusLogProbMetric: 17.2335 - val_loss: 17.4345 - val_MinusLogProbMetric: 17.4345 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 700/1000
2023-10-10 05:02:35.143 
Epoch 700/1000 
	 loss: 17.2477, MinusLogProbMetric: 17.2477, val_loss: 17.4466, val_MinusLogProbMetric: 17.4466

Epoch 700: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2477 - MinusLogProbMetric: 17.2477 - val_loss: 17.4466 - val_MinusLogProbMetric: 17.4466 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 701/1000
2023-10-10 05:03:33.515 
Epoch 701/1000 
	 loss: 17.2736, MinusLogProbMetric: 17.2736, val_loss: 17.5145, val_MinusLogProbMetric: 17.5145

Epoch 701: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2736 - MinusLogProbMetric: 17.2736 - val_loss: 17.5145 - val_MinusLogProbMetric: 17.5145 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 702/1000
2023-10-10 05:04:30.723 
Epoch 702/1000 
	 loss: 17.2432, MinusLogProbMetric: 17.2432, val_loss: 17.4999, val_MinusLogProbMetric: 17.4999

Epoch 702: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2432 - MinusLogProbMetric: 17.2432 - val_loss: 17.4999 - val_MinusLogProbMetric: 17.4999 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 703/1000
2023-10-10 05:05:27.356 
Epoch 703/1000 
	 loss: 17.2363, MinusLogProbMetric: 17.2363, val_loss: 17.4476, val_MinusLogProbMetric: 17.4476

Epoch 703: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2363 - MinusLogProbMetric: 17.2363 - val_loss: 17.4476 - val_MinusLogProbMetric: 17.4476 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 704/1000
2023-10-10 05:06:23.893 
Epoch 704/1000 
	 loss: 17.2318, MinusLogProbMetric: 17.2318, val_loss: 17.3987, val_MinusLogProbMetric: 17.3987

Epoch 704: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2318 - MinusLogProbMetric: 17.2318 - val_loss: 17.3987 - val_MinusLogProbMetric: 17.3987 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 705/1000
2023-10-10 05:07:21.377 
Epoch 705/1000 
	 loss: 17.2307, MinusLogProbMetric: 17.2307, val_loss: 17.4515, val_MinusLogProbMetric: 17.4515

Epoch 705: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2307 - MinusLogProbMetric: 17.2307 - val_loss: 17.4515 - val_MinusLogProbMetric: 17.4515 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 706/1000
2023-10-10 05:08:20.258 
Epoch 706/1000 
	 loss: 17.4158, MinusLogProbMetric: 17.4158, val_loss: 17.4033, val_MinusLogProbMetric: 17.4033

Epoch 706: val_loss did not improve from 17.37338
196/196 - 59s - loss: 17.4158 - MinusLogProbMetric: 17.4158 - val_loss: 17.4033 - val_MinusLogProbMetric: 17.4033 - lr: 1.6667e-04 - 59s/epoch - 300ms/step
Epoch 707/1000
2023-10-10 05:09:18.134 
Epoch 707/1000 
	 loss: 17.2306, MinusLogProbMetric: 17.2306, val_loss: 17.4446, val_MinusLogProbMetric: 17.4446

Epoch 707: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2306 - MinusLogProbMetric: 17.2306 - val_loss: 17.4446 - val_MinusLogProbMetric: 17.4446 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 708/1000
2023-10-10 05:10:15.033 
Epoch 708/1000 
	 loss: 17.2478, MinusLogProbMetric: 17.2478, val_loss: 17.3823, val_MinusLogProbMetric: 17.3823

Epoch 708: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2478 - MinusLogProbMetric: 17.2478 - val_loss: 17.3823 - val_MinusLogProbMetric: 17.3823 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 709/1000
2023-10-10 05:11:12.567 
Epoch 709/1000 
	 loss: 17.2459, MinusLogProbMetric: 17.2459, val_loss: 17.4490, val_MinusLogProbMetric: 17.4490

Epoch 709: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2459 - MinusLogProbMetric: 17.2459 - val_loss: 17.4490 - val_MinusLogProbMetric: 17.4490 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 710/1000
2023-10-10 05:12:11.011 
Epoch 710/1000 
	 loss: 17.2364, MinusLogProbMetric: 17.2364, val_loss: 17.4057, val_MinusLogProbMetric: 17.4057

Epoch 710: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2364 - MinusLogProbMetric: 17.2364 - val_loss: 17.4057 - val_MinusLogProbMetric: 17.4057 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 711/1000
2023-10-10 05:13:09.262 
Epoch 711/1000 
	 loss: 17.2455, MinusLogProbMetric: 17.2455, val_loss: 17.4660, val_MinusLogProbMetric: 17.4660

Epoch 711: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2455 - MinusLogProbMetric: 17.2455 - val_loss: 17.4660 - val_MinusLogProbMetric: 17.4660 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 712/1000
2023-10-10 05:14:07.818 
Epoch 712/1000 
	 loss: 17.2676, MinusLogProbMetric: 17.2676, val_loss: 17.4993, val_MinusLogProbMetric: 17.4993

Epoch 712: val_loss did not improve from 17.37338
196/196 - 59s - loss: 17.2676 - MinusLogProbMetric: 17.2676 - val_loss: 17.4993 - val_MinusLogProbMetric: 17.4993 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 713/1000
2023-10-10 05:15:05.436 
Epoch 713/1000 
	 loss: 17.2235, MinusLogProbMetric: 17.2235, val_loss: 17.3800, val_MinusLogProbMetric: 17.3800

Epoch 713: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2235 - MinusLogProbMetric: 17.2235 - val_loss: 17.3800 - val_MinusLogProbMetric: 17.3800 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 714/1000
2023-10-10 05:16:02.534 
Epoch 714/1000 
	 loss: 17.2142, MinusLogProbMetric: 17.2142, val_loss: 17.4832, val_MinusLogProbMetric: 17.4832

Epoch 714: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2142 - MinusLogProbMetric: 17.2142 - val_loss: 17.4832 - val_MinusLogProbMetric: 17.4832 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 715/1000
2023-10-10 05:16:59.297 
Epoch 715/1000 
	 loss: 17.2509, MinusLogProbMetric: 17.2509, val_loss: 17.4793, val_MinusLogProbMetric: 17.4793

Epoch 715: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2509 - MinusLogProbMetric: 17.2509 - val_loss: 17.4793 - val_MinusLogProbMetric: 17.4793 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 716/1000
2023-10-10 05:17:57.487 
Epoch 716/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.4185, val_MinusLogProbMetric: 17.4185

Epoch 716: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.4185 - val_MinusLogProbMetric: 17.4185 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 717/1000
2023-10-10 05:18:54.805 
Epoch 717/1000 
	 loss: 17.2346, MinusLogProbMetric: 17.2346, val_loss: 17.4565, val_MinusLogProbMetric: 17.4565

Epoch 717: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2346 - MinusLogProbMetric: 17.2346 - val_loss: 17.4565 - val_MinusLogProbMetric: 17.4565 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 718/1000
2023-10-10 05:19:51.493 
Epoch 718/1000 
	 loss: 17.3110, MinusLogProbMetric: 17.3110, val_loss: 17.3770, val_MinusLogProbMetric: 17.3770

Epoch 718: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.3110 - MinusLogProbMetric: 17.3110 - val_loss: 17.3770 - val_MinusLogProbMetric: 17.3770 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 719/1000
2023-10-10 05:20:48.842 
Epoch 719/1000 
	 loss: 17.2261, MinusLogProbMetric: 17.2261, val_loss: 17.5458, val_MinusLogProbMetric: 17.5458

Epoch 719: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2261 - MinusLogProbMetric: 17.2261 - val_loss: 17.5458 - val_MinusLogProbMetric: 17.5458 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 720/1000
2023-10-10 05:21:46.867 
Epoch 720/1000 
	 loss: 17.2222, MinusLogProbMetric: 17.2222, val_loss: 17.4127, val_MinusLogProbMetric: 17.4127

Epoch 720: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2222 - MinusLogProbMetric: 17.2222 - val_loss: 17.4127 - val_MinusLogProbMetric: 17.4127 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 721/1000
2023-10-10 05:22:43.958 
Epoch 721/1000 
	 loss: 17.2226, MinusLogProbMetric: 17.2226, val_loss: 17.4721, val_MinusLogProbMetric: 17.4721

Epoch 721: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2226 - MinusLogProbMetric: 17.2226 - val_loss: 17.4721 - val_MinusLogProbMetric: 17.4721 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 722/1000
2023-10-10 05:23:40.476 
Epoch 722/1000 
	 loss: 17.2429, MinusLogProbMetric: 17.2429, val_loss: 17.5169, val_MinusLogProbMetric: 17.5169

Epoch 722: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2429 - MinusLogProbMetric: 17.2429 - val_loss: 17.5169 - val_MinusLogProbMetric: 17.5169 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 723/1000
2023-10-10 05:24:37.317 
Epoch 723/1000 
	 loss: 17.2062, MinusLogProbMetric: 17.2062, val_loss: 17.4298, val_MinusLogProbMetric: 17.4298

Epoch 723: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2062 - MinusLogProbMetric: 17.2062 - val_loss: 17.4298 - val_MinusLogProbMetric: 17.4298 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 724/1000
2023-10-10 05:25:34.569 
Epoch 724/1000 
	 loss: 17.2494, MinusLogProbMetric: 17.2494, val_loss: 17.4989, val_MinusLogProbMetric: 17.4989

Epoch 724: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2494 - MinusLogProbMetric: 17.2494 - val_loss: 17.4989 - val_MinusLogProbMetric: 17.4989 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 725/1000
2023-10-10 05:26:32.062 
Epoch 725/1000 
	 loss: 17.2360, MinusLogProbMetric: 17.2360, val_loss: 17.3915, val_MinusLogProbMetric: 17.3915

Epoch 725: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2360 - MinusLogProbMetric: 17.2360 - val_loss: 17.3915 - val_MinusLogProbMetric: 17.3915 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 726/1000
2023-10-10 05:27:28.989 
Epoch 726/1000 
	 loss: 17.2636, MinusLogProbMetric: 17.2636, val_loss: 17.3877, val_MinusLogProbMetric: 17.3877

Epoch 726: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2636 - MinusLogProbMetric: 17.2636 - val_loss: 17.3877 - val_MinusLogProbMetric: 17.3877 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 727/1000
2023-10-10 05:28:26.774 
Epoch 727/1000 
	 loss: 17.2060, MinusLogProbMetric: 17.2060, val_loss: 17.4980, val_MinusLogProbMetric: 17.4980

Epoch 727: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2060 - MinusLogProbMetric: 17.2060 - val_loss: 17.4980 - val_MinusLogProbMetric: 17.4980 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 728/1000
2023-10-10 05:29:23.296 
Epoch 728/1000 
	 loss: 17.2659, MinusLogProbMetric: 17.2659, val_loss: 17.8167, val_MinusLogProbMetric: 17.8167

Epoch 728: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2659 - MinusLogProbMetric: 17.2659 - val_loss: 17.8167 - val_MinusLogProbMetric: 17.8167 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 729/1000
2023-10-10 05:30:20.257 
Epoch 729/1000 
	 loss: 17.2383, MinusLogProbMetric: 17.2383, val_loss: 17.4099, val_MinusLogProbMetric: 17.4099

Epoch 729: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2383 - MinusLogProbMetric: 17.2383 - val_loss: 17.4099 - val_MinusLogProbMetric: 17.4099 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 730/1000
2023-10-10 05:31:15.766 
Epoch 730/1000 
	 loss: 17.2244, MinusLogProbMetric: 17.2244, val_loss: 17.4315, val_MinusLogProbMetric: 17.4315

Epoch 730: val_loss did not improve from 17.37338
196/196 - 56s - loss: 17.2244 - MinusLogProbMetric: 17.2244 - val_loss: 17.4315 - val_MinusLogProbMetric: 17.4315 - lr: 1.6667e-04 - 56s/epoch - 283ms/step
Epoch 731/1000
2023-10-10 05:32:13.522 
Epoch 731/1000 
	 loss: 17.2221, MinusLogProbMetric: 17.2221, val_loss: 17.4256, val_MinusLogProbMetric: 17.4256

Epoch 731: val_loss did not improve from 17.37338
196/196 - 58s - loss: 17.2221 - MinusLogProbMetric: 17.2221 - val_loss: 17.4256 - val_MinusLogProbMetric: 17.4256 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 732/1000
2023-10-10 05:33:10.677 
Epoch 732/1000 
	 loss: 17.2109, MinusLogProbMetric: 17.2109, val_loss: 17.4216, val_MinusLogProbMetric: 17.4216

Epoch 732: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2109 - MinusLogProbMetric: 17.2109 - val_loss: 17.4216 - val_MinusLogProbMetric: 17.4216 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 733/1000
2023-10-10 05:34:07.904 
Epoch 733/1000 
	 loss: 17.2165, MinusLogProbMetric: 17.2165, val_loss: 17.4270, val_MinusLogProbMetric: 17.4270

Epoch 733: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2165 - MinusLogProbMetric: 17.2165 - val_loss: 17.4270 - val_MinusLogProbMetric: 17.4270 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 734/1000
2023-10-10 05:35:04.887 
Epoch 734/1000 
	 loss: 17.2394, MinusLogProbMetric: 17.2394, val_loss: 17.4324, val_MinusLogProbMetric: 17.4324

Epoch 734: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2394 - MinusLogProbMetric: 17.2394 - val_loss: 17.4324 - val_MinusLogProbMetric: 17.4324 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 735/1000
2023-10-10 05:36:02.287 
Epoch 735/1000 
	 loss: 17.2164, MinusLogProbMetric: 17.2164, val_loss: 17.5054, val_MinusLogProbMetric: 17.5054

Epoch 735: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2164 - MinusLogProbMetric: 17.2164 - val_loss: 17.5054 - val_MinusLogProbMetric: 17.5054 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 736/1000
2023-10-10 05:36:59.402 
Epoch 736/1000 
	 loss: 17.2328, MinusLogProbMetric: 17.2328, val_loss: 17.4110, val_MinusLogProbMetric: 17.4110

Epoch 736: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2328 - MinusLogProbMetric: 17.2328 - val_loss: 17.4110 - val_MinusLogProbMetric: 17.4110 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 737/1000
2023-10-10 05:37:56.403 
Epoch 737/1000 
	 loss: 17.3316, MinusLogProbMetric: 17.3316, val_loss: 17.7368, val_MinusLogProbMetric: 17.7368

Epoch 737: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.3316 - MinusLogProbMetric: 17.3316 - val_loss: 17.7368 - val_MinusLogProbMetric: 17.7368 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 738/1000
2023-10-10 05:38:52.692 
Epoch 738/1000 
	 loss: 17.2200, MinusLogProbMetric: 17.2200, val_loss: 17.3806, val_MinusLogProbMetric: 17.3806

Epoch 738: val_loss did not improve from 17.37338
196/196 - 56s - loss: 17.2200 - MinusLogProbMetric: 17.2200 - val_loss: 17.3806 - val_MinusLogProbMetric: 17.3806 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 739/1000
2023-10-10 05:39:49.726 
Epoch 739/1000 
	 loss: 17.2059, MinusLogProbMetric: 17.2059, val_loss: 17.4780, val_MinusLogProbMetric: 17.4780

Epoch 739: val_loss did not improve from 17.37338
196/196 - 57s - loss: 17.2059 - MinusLogProbMetric: 17.2059 - val_loss: 17.4780 - val_MinusLogProbMetric: 17.4780 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 740/1000
2023-10-10 05:40:47.058 
Epoch 740/1000 
	 loss: 17.2113, MinusLogProbMetric: 17.2113, val_loss: 17.3368, val_MinusLogProbMetric: 17.3368

Epoch 740: val_loss improved from 17.37338 to 17.33678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.2113 - MinusLogProbMetric: 17.2113 - val_loss: 17.3368 - val_MinusLogProbMetric: 17.3368 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 741/1000
2023-10-10 05:41:44.938 
Epoch 741/1000 
	 loss: 17.2206, MinusLogProbMetric: 17.2206, val_loss: 17.4343, val_MinusLogProbMetric: 17.4343

Epoch 741: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.2206 - MinusLogProbMetric: 17.2206 - val_loss: 17.4343 - val_MinusLogProbMetric: 17.4343 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 742/1000
2023-10-10 05:42:42.102 
Epoch 742/1000 
	 loss: 17.2279, MinusLogProbMetric: 17.2279, val_loss: 17.4152, val_MinusLogProbMetric: 17.4152

Epoch 742: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.2279 - MinusLogProbMetric: 17.2279 - val_loss: 17.4152 - val_MinusLogProbMetric: 17.4152 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 743/1000
2023-10-10 05:43:39.558 
Epoch 743/1000 
	 loss: 17.1928, MinusLogProbMetric: 17.1928, val_loss: 17.4779, val_MinusLogProbMetric: 17.4779

Epoch 743: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.1928 - MinusLogProbMetric: 17.1928 - val_loss: 17.4779 - val_MinusLogProbMetric: 17.4779 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 744/1000
2023-10-10 05:44:36.785 
Epoch 744/1000 
	 loss: 17.2261, MinusLogProbMetric: 17.2261, val_loss: 17.3919, val_MinusLogProbMetric: 17.3919

Epoch 744: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.2261 - MinusLogProbMetric: 17.2261 - val_loss: 17.3919 - val_MinusLogProbMetric: 17.3919 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 745/1000
2023-10-10 05:45:34.592 
Epoch 745/1000 
	 loss: 17.2052, MinusLogProbMetric: 17.2052, val_loss: 17.4326, val_MinusLogProbMetric: 17.4326

Epoch 745: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2052 - MinusLogProbMetric: 17.2052 - val_loss: 17.4326 - val_MinusLogProbMetric: 17.4326 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 746/1000
2023-10-10 05:46:31.734 
Epoch 746/1000 
	 loss: 17.2006, MinusLogProbMetric: 17.2006, val_loss: 17.4213, val_MinusLogProbMetric: 17.4213

Epoch 746: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.2006 - MinusLogProbMetric: 17.2006 - val_loss: 17.4213 - val_MinusLogProbMetric: 17.4213 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 747/1000
2023-10-10 05:47:29.223 
Epoch 747/1000 
	 loss: 17.2115, MinusLogProbMetric: 17.2115, val_loss: 17.5141, val_MinusLogProbMetric: 17.5141

Epoch 747: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.2115 - MinusLogProbMetric: 17.2115 - val_loss: 17.5141 - val_MinusLogProbMetric: 17.5141 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 748/1000
2023-10-10 05:48:26.755 
Epoch 748/1000 
	 loss: 17.2210, MinusLogProbMetric: 17.2210, val_loss: 17.4183, val_MinusLogProbMetric: 17.4183

Epoch 748: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2210 - MinusLogProbMetric: 17.2210 - val_loss: 17.4183 - val_MinusLogProbMetric: 17.4183 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 749/1000
2023-10-10 05:49:23.035 
Epoch 749/1000 
	 loss: 17.2070, MinusLogProbMetric: 17.2070, val_loss: 17.3966, val_MinusLogProbMetric: 17.3966

Epoch 749: val_loss did not improve from 17.33678
196/196 - 56s - loss: 17.2070 - MinusLogProbMetric: 17.2070 - val_loss: 17.3966 - val_MinusLogProbMetric: 17.3966 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 750/1000
2023-10-10 05:50:20.553 
Epoch 750/1000 
	 loss: 17.2636, MinusLogProbMetric: 17.2636, val_loss: 17.3528, val_MinusLogProbMetric: 17.3528

Epoch 750: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2636 - MinusLogProbMetric: 17.2636 - val_loss: 17.3528 - val_MinusLogProbMetric: 17.3528 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 751/1000
2023-10-10 05:51:18.920 
Epoch 751/1000 
	 loss: 17.2686, MinusLogProbMetric: 17.2686, val_loss: 17.4158, val_MinusLogProbMetric: 17.4158

Epoch 751: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2686 - MinusLogProbMetric: 17.2686 - val_loss: 17.4158 - val_MinusLogProbMetric: 17.4158 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 752/1000
2023-10-10 05:52:18.518 
Epoch 752/1000 
	 loss: 17.1966, MinusLogProbMetric: 17.1966, val_loss: 17.3838, val_MinusLogProbMetric: 17.3838

Epoch 752: val_loss did not improve from 17.33678
196/196 - 60s - loss: 17.1966 - MinusLogProbMetric: 17.1966 - val_loss: 17.3838 - val_MinusLogProbMetric: 17.3838 - lr: 1.6667e-04 - 60s/epoch - 304ms/step
Epoch 753/1000
2023-10-10 05:53:18.096 
Epoch 753/1000 
	 loss: 17.2347, MinusLogProbMetric: 17.2347, val_loss: 17.3961, val_MinusLogProbMetric: 17.3961

Epoch 753: val_loss did not improve from 17.33678
196/196 - 60s - loss: 17.2347 - MinusLogProbMetric: 17.2347 - val_loss: 17.3961 - val_MinusLogProbMetric: 17.3961 - lr: 1.6667e-04 - 60s/epoch - 304ms/step
Epoch 754/1000
2023-10-10 05:54:16.553 
Epoch 754/1000 
	 loss: 17.1899, MinusLogProbMetric: 17.1899, val_loss: 17.4171, val_MinusLogProbMetric: 17.4171

Epoch 754: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.1899 - MinusLogProbMetric: 17.1899 - val_loss: 17.4171 - val_MinusLogProbMetric: 17.4171 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 755/1000
2023-10-10 05:55:14.647 
Epoch 755/1000 
	 loss: 17.1964, MinusLogProbMetric: 17.1964, val_loss: 17.4009, val_MinusLogProbMetric: 17.4009

Epoch 755: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.1964 - MinusLogProbMetric: 17.1964 - val_loss: 17.4009 - val_MinusLogProbMetric: 17.4009 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 756/1000
2023-10-10 05:56:13.241 
Epoch 756/1000 
	 loss: 17.2066, MinusLogProbMetric: 17.2066, val_loss: 17.3487, val_MinusLogProbMetric: 17.3487

Epoch 756: val_loss did not improve from 17.33678
196/196 - 59s - loss: 17.2066 - MinusLogProbMetric: 17.2066 - val_loss: 17.3487 - val_MinusLogProbMetric: 17.3487 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 757/1000
2023-10-10 05:57:11.002 
Epoch 757/1000 
	 loss: 17.2032, MinusLogProbMetric: 17.2032, val_loss: 17.3888, val_MinusLogProbMetric: 17.3888

Epoch 757: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2032 - MinusLogProbMetric: 17.2032 - val_loss: 17.3888 - val_MinusLogProbMetric: 17.3888 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 758/1000
2023-10-10 05:58:07.909 
Epoch 758/1000 
	 loss: 17.1798, MinusLogProbMetric: 17.1798, val_loss: 17.5258, val_MinusLogProbMetric: 17.5258

Epoch 758: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.1798 - MinusLogProbMetric: 17.1798 - val_loss: 17.5258 - val_MinusLogProbMetric: 17.5258 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 759/1000
2023-10-10 05:59:06.162 
Epoch 759/1000 
	 loss: 17.2453, MinusLogProbMetric: 17.2453, val_loss: 17.3979, val_MinusLogProbMetric: 17.3979

Epoch 759: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2453 - MinusLogProbMetric: 17.2453 - val_loss: 17.3979 - val_MinusLogProbMetric: 17.3979 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 760/1000
2023-10-10 06:00:02.692 
Epoch 760/1000 
	 loss: 17.1964, MinusLogProbMetric: 17.1964, val_loss: 17.3830, val_MinusLogProbMetric: 17.3830

Epoch 760: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.1964 - MinusLogProbMetric: 17.1964 - val_loss: 17.3830 - val_MinusLogProbMetric: 17.3830 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 761/1000
2023-10-10 06:01:00.032 
Epoch 761/1000 
	 loss: 17.2066, MinusLogProbMetric: 17.2066, val_loss: 17.3956, val_MinusLogProbMetric: 17.3956

Epoch 761: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.2066 - MinusLogProbMetric: 17.2066 - val_loss: 17.3956 - val_MinusLogProbMetric: 17.3956 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 762/1000
2023-10-10 06:01:57.442 
Epoch 762/1000 
	 loss: 17.1811, MinusLogProbMetric: 17.1811, val_loss: 17.5128, val_MinusLogProbMetric: 17.5128

Epoch 762: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.1811 - MinusLogProbMetric: 17.1811 - val_loss: 17.5128 - val_MinusLogProbMetric: 17.5128 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 763/1000
2023-10-10 06:02:55.287 
Epoch 763/1000 
	 loss: 17.2012, MinusLogProbMetric: 17.2012, val_loss: 17.3699, val_MinusLogProbMetric: 17.3699

Epoch 763: val_loss did not improve from 17.33678
196/196 - 58s - loss: 17.2012 - MinusLogProbMetric: 17.2012 - val_loss: 17.3699 - val_MinusLogProbMetric: 17.3699 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 764/1000
2023-10-10 06:03:52.640 
Epoch 764/1000 
	 loss: 17.3296, MinusLogProbMetric: 17.3296, val_loss: 17.4960, val_MinusLogProbMetric: 17.4960

Epoch 764: val_loss did not improve from 17.33678
196/196 - 57s - loss: 17.3296 - MinusLogProbMetric: 17.3296 - val_loss: 17.4960 - val_MinusLogProbMetric: 17.4960 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 765/1000
2023-10-10 06:04:49.867 
Epoch 765/1000 
	 loss: 17.2059, MinusLogProbMetric: 17.2059, val_loss: 17.3236, val_MinusLogProbMetric: 17.3236

Epoch 765: val_loss improved from 17.33678 to 17.32364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.2059 - MinusLogProbMetric: 17.2059 - val_loss: 17.3236 - val_MinusLogProbMetric: 17.3236 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 766/1000
2023-10-10 06:05:47.958 
Epoch 766/1000 
	 loss: 17.2220, MinusLogProbMetric: 17.2220, val_loss: 17.4793, val_MinusLogProbMetric: 17.4793

Epoch 766: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2220 - MinusLogProbMetric: 17.2220 - val_loss: 17.4793 - val_MinusLogProbMetric: 17.4793 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 767/1000
2023-10-10 06:06:45.344 
Epoch 767/1000 
	 loss: 17.2076, MinusLogProbMetric: 17.2076, val_loss: 17.4624, val_MinusLogProbMetric: 17.4624

Epoch 767: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2076 - MinusLogProbMetric: 17.2076 - val_loss: 17.4624 - val_MinusLogProbMetric: 17.4624 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 768/1000
2023-10-10 06:07:43.772 
Epoch 768/1000 
	 loss: 17.1895, MinusLogProbMetric: 17.1895, val_loss: 17.5930, val_MinusLogProbMetric: 17.5930

Epoch 768: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1895 - MinusLogProbMetric: 17.1895 - val_loss: 17.5930 - val_MinusLogProbMetric: 17.5930 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 769/1000
2023-10-10 06:08:40.587 
Epoch 769/1000 
	 loss: 17.1967, MinusLogProbMetric: 17.1967, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 769: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1967 - MinusLogProbMetric: 17.1967 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 770/1000
2023-10-10 06:09:37.788 
Epoch 770/1000 
	 loss: 17.2016, MinusLogProbMetric: 17.2016, val_loss: 17.4061, val_MinusLogProbMetric: 17.4061

Epoch 770: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2016 - MinusLogProbMetric: 17.2016 - val_loss: 17.4061 - val_MinusLogProbMetric: 17.4061 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 771/1000
2023-10-10 06:10:34.875 
Epoch 771/1000 
	 loss: 17.1954, MinusLogProbMetric: 17.1954, val_loss: 17.3757, val_MinusLogProbMetric: 17.3757

Epoch 771: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1954 - MinusLogProbMetric: 17.1954 - val_loss: 17.3757 - val_MinusLogProbMetric: 17.3757 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 772/1000
2023-10-10 06:11:31.905 
Epoch 772/1000 
	 loss: 17.1823, MinusLogProbMetric: 17.1823, val_loss: 17.4928, val_MinusLogProbMetric: 17.4928

Epoch 772: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1823 - MinusLogProbMetric: 17.1823 - val_loss: 17.4928 - val_MinusLogProbMetric: 17.4928 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 773/1000
2023-10-10 06:12:29.721 
Epoch 773/1000 
	 loss: 17.1998, MinusLogProbMetric: 17.1998, val_loss: 17.3948, val_MinusLogProbMetric: 17.3948

Epoch 773: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1998 - MinusLogProbMetric: 17.1998 - val_loss: 17.3948 - val_MinusLogProbMetric: 17.3948 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 774/1000
2023-10-10 06:13:27.102 
Epoch 774/1000 
	 loss: 17.2163, MinusLogProbMetric: 17.2163, val_loss: 17.4457, val_MinusLogProbMetric: 17.4457

Epoch 774: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2163 - MinusLogProbMetric: 17.2163 - val_loss: 17.4457 - val_MinusLogProbMetric: 17.4457 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 775/1000
2023-10-10 06:14:24.967 
Epoch 775/1000 
	 loss: 17.2051, MinusLogProbMetric: 17.2051, val_loss: 17.4133, val_MinusLogProbMetric: 17.4133

Epoch 775: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.2051 - MinusLogProbMetric: 17.2051 - val_loss: 17.4133 - val_MinusLogProbMetric: 17.4133 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 776/1000
2023-10-10 06:15:22.067 
Epoch 776/1000 
	 loss: 17.1849, MinusLogProbMetric: 17.1849, val_loss: 17.3579, val_MinusLogProbMetric: 17.3579

Epoch 776: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1849 - MinusLogProbMetric: 17.1849 - val_loss: 17.3579 - val_MinusLogProbMetric: 17.3579 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 777/1000
2023-10-10 06:16:19.347 
Epoch 777/1000 
	 loss: 17.1649, MinusLogProbMetric: 17.1649, val_loss: 17.4483, val_MinusLogProbMetric: 17.4483

Epoch 777: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1649 - MinusLogProbMetric: 17.1649 - val_loss: 17.4483 - val_MinusLogProbMetric: 17.4483 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 778/1000
2023-10-10 06:17:15.749 
Epoch 778/1000 
	 loss: 17.1735, MinusLogProbMetric: 17.1735, val_loss: 17.6515, val_MinusLogProbMetric: 17.6515

Epoch 778: val_loss did not improve from 17.32364
196/196 - 56s - loss: 17.1735 - MinusLogProbMetric: 17.1735 - val_loss: 17.6515 - val_MinusLogProbMetric: 17.6515 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 779/1000
2023-10-10 06:18:12.565 
Epoch 779/1000 
	 loss: 17.2034, MinusLogProbMetric: 17.2034, val_loss: 17.4487, val_MinusLogProbMetric: 17.4487

Epoch 779: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2034 - MinusLogProbMetric: 17.2034 - val_loss: 17.4487 - val_MinusLogProbMetric: 17.4487 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 780/1000
2023-10-10 06:19:09.846 
Epoch 780/1000 
	 loss: 17.2170, MinusLogProbMetric: 17.2170, val_loss: 17.5165, val_MinusLogProbMetric: 17.5165

Epoch 780: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2170 - MinusLogProbMetric: 17.2170 - val_loss: 17.5165 - val_MinusLogProbMetric: 17.5165 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 781/1000
2023-10-10 06:20:08.400 
Epoch 781/1000 
	 loss: 17.2246, MinusLogProbMetric: 17.2246, val_loss: 17.4419, val_MinusLogProbMetric: 17.4419

Epoch 781: val_loss did not improve from 17.32364
196/196 - 59s - loss: 17.2246 - MinusLogProbMetric: 17.2246 - val_loss: 17.4419 - val_MinusLogProbMetric: 17.4419 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 782/1000
2023-10-10 06:21:05.232 
Epoch 782/1000 
	 loss: 17.1845, MinusLogProbMetric: 17.1845, val_loss: 17.4241, val_MinusLogProbMetric: 17.4241

Epoch 782: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1845 - MinusLogProbMetric: 17.1845 - val_loss: 17.4241 - val_MinusLogProbMetric: 17.4241 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 783/1000
2023-10-10 06:22:02.896 
Epoch 783/1000 
	 loss: 17.1757, MinusLogProbMetric: 17.1757, val_loss: 17.3595, val_MinusLogProbMetric: 17.3595

Epoch 783: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1757 - MinusLogProbMetric: 17.1757 - val_loss: 17.3595 - val_MinusLogProbMetric: 17.3595 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 784/1000
2023-10-10 06:23:00.255 
Epoch 784/1000 
	 loss: 17.1672, MinusLogProbMetric: 17.1672, val_loss: 17.3499, val_MinusLogProbMetric: 17.3499

Epoch 784: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1672 - MinusLogProbMetric: 17.1672 - val_loss: 17.3499 - val_MinusLogProbMetric: 17.3499 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 785/1000
2023-10-10 06:23:57.502 
Epoch 785/1000 
	 loss: 17.4734, MinusLogProbMetric: 17.4734, val_loss: 17.4274, val_MinusLogProbMetric: 17.4274

Epoch 785: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.4734 - MinusLogProbMetric: 17.4734 - val_loss: 17.4274 - val_MinusLogProbMetric: 17.4274 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 786/1000
2023-10-10 06:24:54.397 
Epoch 786/1000 
	 loss: 17.1982, MinusLogProbMetric: 17.1982, val_loss: 17.4292, val_MinusLogProbMetric: 17.4292

Epoch 786: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1982 - MinusLogProbMetric: 17.1982 - val_loss: 17.4292 - val_MinusLogProbMetric: 17.4292 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 787/1000
2023-10-10 06:25:51.705 
Epoch 787/1000 
	 loss: 17.1971, MinusLogProbMetric: 17.1971, val_loss: 17.3516, val_MinusLogProbMetric: 17.3516

Epoch 787: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1971 - MinusLogProbMetric: 17.1971 - val_loss: 17.3516 - val_MinusLogProbMetric: 17.3516 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 788/1000
2023-10-10 06:26:48.823 
Epoch 788/1000 
	 loss: 17.1893, MinusLogProbMetric: 17.1893, val_loss: 17.4073, val_MinusLogProbMetric: 17.4073

Epoch 788: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1893 - MinusLogProbMetric: 17.1893 - val_loss: 17.4073 - val_MinusLogProbMetric: 17.4073 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 789/1000
2023-10-10 06:27:45.685 
Epoch 789/1000 
	 loss: 17.1780, MinusLogProbMetric: 17.1780, val_loss: 17.3527, val_MinusLogProbMetric: 17.3527

Epoch 789: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1780 - MinusLogProbMetric: 17.1780 - val_loss: 17.3527 - val_MinusLogProbMetric: 17.3527 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 790/1000
2023-10-10 06:28:42.746 
Epoch 790/1000 
	 loss: 17.1795, MinusLogProbMetric: 17.1795, val_loss: 17.4201, val_MinusLogProbMetric: 17.4201

Epoch 790: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1795 - MinusLogProbMetric: 17.1795 - val_loss: 17.4201 - val_MinusLogProbMetric: 17.4201 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 791/1000
2023-10-10 06:29:39.629 
Epoch 791/1000 
	 loss: 17.1876, MinusLogProbMetric: 17.1876, val_loss: 17.3784, val_MinusLogProbMetric: 17.3784

Epoch 791: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1876 - MinusLogProbMetric: 17.1876 - val_loss: 17.3784 - val_MinusLogProbMetric: 17.3784 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 792/1000
2023-10-10 06:30:37.837 
Epoch 792/1000 
	 loss: 17.1810, MinusLogProbMetric: 17.1810, val_loss: 17.3660, val_MinusLogProbMetric: 17.3660

Epoch 792: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1810 - MinusLogProbMetric: 17.1810 - val_loss: 17.3660 - val_MinusLogProbMetric: 17.3660 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 793/1000
2023-10-10 06:31:35.554 
Epoch 793/1000 
	 loss: 17.1695, MinusLogProbMetric: 17.1695, val_loss: 17.3963, val_MinusLogProbMetric: 17.3963

Epoch 793: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1695 - MinusLogProbMetric: 17.1695 - val_loss: 17.3963 - val_MinusLogProbMetric: 17.3963 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 794/1000
2023-10-10 06:32:32.977 
Epoch 794/1000 
	 loss: 17.1774, MinusLogProbMetric: 17.1774, val_loss: 17.3511, val_MinusLogProbMetric: 17.3511

Epoch 794: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1774 - MinusLogProbMetric: 17.1774 - val_loss: 17.3511 - val_MinusLogProbMetric: 17.3511 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 795/1000
2023-10-10 06:33:29.958 
Epoch 795/1000 
	 loss: 17.2249, MinusLogProbMetric: 17.2249, val_loss: 17.3653, val_MinusLogProbMetric: 17.3653

Epoch 795: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2249 - MinusLogProbMetric: 17.2249 - val_loss: 17.3653 - val_MinusLogProbMetric: 17.3653 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 796/1000
2023-10-10 06:34:26.773 
Epoch 796/1000 
	 loss: 17.1868, MinusLogProbMetric: 17.1868, val_loss: 17.3741, val_MinusLogProbMetric: 17.3741

Epoch 796: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1868 - MinusLogProbMetric: 17.1868 - val_loss: 17.3741 - val_MinusLogProbMetric: 17.3741 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 797/1000
2023-10-10 06:35:23.689 
Epoch 797/1000 
	 loss: 17.1852, MinusLogProbMetric: 17.1852, val_loss: 17.3708, val_MinusLogProbMetric: 17.3708

Epoch 797: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1852 - MinusLogProbMetric: 17.1852 - val_loss: 17.3708 - val_MinusLogProbMetric: 17.3708 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 798/1000
2023-10-10 06:36:21.175 
Epoch 798/1000 
	 loss: 17.1777, MinusLogProbMetric: 17.1777, val_loss: 17.4401, val_MinusLogProbMetric: 17.4401

Epoch 798: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1777 - MinusLogProbMetric: 17.1777 - val_loss: 17.4401 - val_MinusLogProbMetric: 17.4401 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 799/1000
2023-10-10 06:37:18.511 
Epoch 799/1000 
	 loss: 17.2074, MinusLogProbMetric: 17.2074, val_loss: 17.3379, val_MinusLogProbMetric: 17.3379

Epoch 799: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2074 - MinusLogProbMetric: 17.2074 - val_loss: 17.3379 - val_MinusLogProbMetric: 17.3379 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 800/1000
2023-10-10 06:38:15.338 
Epoch 800/1000 
	 loss: 17.1716, MinusLogProbMetric: 17.1716, val_loss: 17.3792, val_MinusLogProbMetric: 17.3792

Epoch 800: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1716 - MinusLogProbMetric: 17.1716 - val_loss: 17.3792 - val_MinusLogProbMetric: 17.3792 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 801/1000
2023-10-10 06:39:11.953 
Epoch 801/1000 
	 loss: 17.1900, MinusLogProbMetric: 17.1900, val_loss: 17.4382, val_MinusLogProbMetric: 17.4382

Epoch 801: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1900 - MinusLogProbMetric: 17.1900 - val_loss: 17.4382 - val_MinusLogProbMetric: 17.4382 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 802/1000
2023-10-10 06:40:09.459 
Epoch 802/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.3877, val_MinusLogProbMetric: 17.3877

Epoch 802: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.3877 - val_MinusLogProbMetric: 17.3877 - lr: 1.6667e-04 - 58s/epoch - 293ms/step
Epoch 803/1000
2023-10-10 06:41:07.119 
Epoch 803/1000 
	 loss: 17.1729, MinusLogProbMetric: 17.1729, val_loss: 17.5337, val_MinusLogProbMetric: 17.5337

Epoch 803: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1729 - MinusLogProbMetric: 17.1729 - val_loss: 17.5337 - val_MinusLogProbMetric: 17.5337 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 804/1000
2023-10-10 06:42:04.195 
Epoch 804/1000 
	 loss: 17.1822, MinusLogProbMetric: 17.1822, val_loss: 17.4159, val_MinusLogProbMetric: 17.4159

Epoch 804: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1822 - MinusLogProbMetric: 17.1822 - val_loss: 17.4159 - val_MinusLogProbMetric: 17.4159 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 805/1000
2023-10-10 06:43:00.966 
Epoch 805/1000 
	 loss: 17.1713, MinusLogProbMetric: 17.1713, val_loss: 17.4039, val_MinusLogProbMetric: 17.4039

Epoch 805: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1713 - MinusLogProbMetric: 17.1713 - val_loss: 17.4039 - val_MinusLogProbMetric: 17.4039 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 806/1000
2023-10-10 06:43:58.131 
Epoch 806/1000 
	 loss: 17.1807, MinusLogProbMetric: 17.1807, val_loss: 17.3869, val_MinusLogProbMetric: 17.3869

Epoch 806: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1807 - MinusLogProbMetric: 17.1807 - val_loss: 17.3869 - val_MinusLogProbMetric: 17.3869 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 807/1000
2023-10-10 06:44:54.699 
Epoch 807/1000 
	 loss: 17.1782, MinusLogProbMetric: 17.1782, val_loss: 17.3735, val_MinusLogProbMetric: 17.3735

Epoch 807: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1782 - MinusLogProbMetric: 17.1782 - val_loss: 17.3735 - val_MinusLogProbMetric: 17.3735 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 808/1000
2023-10-10 06:45:52.040 
Epoch 808/1000 
	 loss: 17.2979, MinusLogProbMetric: 17.2979, val_loss: 17.4927, val_MinusLogProbMetric: 17.4927

Epoch 808: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.2979 - MinusLogProbMetric: 17.2979 - val_loss: 17.4927 - val_MinusLogProbMetric: 17.4927 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 809/1000
2023-10-10 06:46:49.693 
Epoch 809/1000 
	 loss: 17.1897, MinusLogProbMetric: 17.1897, val_loss: 17.3283, val_MinusLogProbMetric: 17.3283

Epoch 809: val_loss did not improve from 17.32364
196/196 - 58s - loss: 17.1897 - MinusLogProbMetric: 17.1897 - val_loss: 17.3283 - val_MinusLogProbMetric: 17.3283 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 810/1000
2023-10-10 06:47:47.092 
Epoch 810/1000 
	 loss: 17.1708, MinusLogProbMetric: 17.1708, val_loss: 17.5087, val_MinusLogProbMetric: 17.5087

Epoch 810: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1708 - MinusLogProbMetric: 17.1708 - val_loss: 17.5087 - val_MinusLogProbMetric: 17.5087 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 811/1000
2023-10-10 06:48:43.857 
Epoch 811/1000 
	 loss: 17.1793, MinusLogProbMetric: 17.1793, val_loss: 17.3626, val_MinusLogProbMetric: 17.3626

Epoch 811: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1793 - MinusLogProbMetric: 17.1793 - val_loss: 17.3626 - val_MinusLogProbMetric: 17.3626 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 812/1000
2023-10-10 06:49:41.029 
Epoch 812/1000 
	 loss: 17.1945, MinusLogProbMetric: 17.1945, val_loss: 17.5433, val_MinusLogProbMetric: 17.5433

Epoch 812: val_loss did not improve from 17.32364
196/196 - 57s - loss: 17.1945 - MinusLogProbMetric: 17.1945 - val_loss: 17.5433 - val_MinusLogProbMetric: 17.5433 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 813/1000
2023-10-10 06:50:37.277 
Epoch 813/1000 
	 loss: 17.1711, MinusLogProbMetric: 17.1711, val_loss: 17.3068, val_MinusLogProbMetric: 17.3068

Epoch 813: val_loss improved from 17.32364 to 17.30682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 57s - loss: 17.1711 - MinusLogProbMetric: 17.1711 - val_loss: 17.3068 - val_MinusLogProbMetric: 17.3068 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 814/1000
2023-10-10 06:51:35.310 
Epoch 814/1000 
	 loss: 17.1808, MinusLogProbMetric: 17.1808, val_loss: 17.5709, val_MinusLogProbMetric: 17.5709

Epoch 814: val_loss did not improve from 17.30682
196/196 - 57s - loss: 17.1808 - MinusLogProbMetric: 17.1808 - val_loss: 17.5709 - val_MinusLogProbMetric: 17.5709 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 815/1000
2023-10-10 06:52:32.331 
Epoch 815/1000 
	 loss: 17.1722, MinusLogProbMetric: 17.1722, val_loss: 17.4982, val_MinusLogProbMetric: 17.4982

Epoch 815: val_loss did not improve from 17.30682
196/196 - 57s - loss: 17.1722 - MinusLogProbMetric: 17.1722 - val_loss: 17.4982 - val_MinusLogProbMetric: 17.4982 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 816/1000
2023-10-10 06:53:29.095 
Epoch 816/1000 
	 loss: 17.4354, MinusLogProbMetric: 17.4354, val_loss: 17.4204, val_MinusLogProbMetric: 17.4204

Epoch 816: val_loss did not improve from 17.30682
196/196 - 57s - loss: 17.4354 - MinusLogProbMetric: 17.4354 - val_loss: 17.4204 - val_MinusLogProbMetric: 17.4204 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 817/1000
2023-10-10 06:54:27.187 
Epoch 817/1000 
	 loss: 17.1680, MinusLogProbMetric: 17.1680, val_loss: 17.2976, val_MinusLogProbMetric: 17.2976

Epoch 817: val_loss improved from 17.30682 to 17.29763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.1680 - MinusLogProbMetric: 17.1680 - val_loss: 17.2976 - val_MinusLogProbMetric: 17.2976 - lr: 1.6667e-04 - 59s/epoch - 302ms/step
Epoch 818/1000
2023-10-10 06:55:25.015 
Epoch 818/1000 
	 loss: 17.1688, MinusLogProbMetric: 17.1688, val_loss: 17.3907, val_MinusLogProbMetric: 17.3907

Epoch 818: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1688 - MinusLogProbMetric: 17.1688 - val_loss: 17.3907 - val_MinusLogProbMetric: 17.3907 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 819/1000
2023-10-10 06:56:21.379 
Epoch 819/1000 
	 loss: 17.1609, MinusLogProbMetric: 17.1609, val_loss: 17.3696, val_MinusLogProbMetric: 17.3696

Epoch 819: val_loss did not improve from 17.29763
196/196 - 56s - loss: 17.1609 - MinusLogProbMetric: 17.1609 - val_loss: 17.3696 - val_MinusLogProbMetric: 17.3696 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 820/1000
2023-10-10 06:57:17.965 
Epoch 820/1000 
	 loss: 17.1642, MinusLogProbMetric: 17.1642, val_loss: 17.3594, val_MinusLogProbMetric: 17.3594

Epoch 820: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1642 - MinusLogProbMetric: 17.1642 - val_loss: 17.3594 - val_MinusLogProbMetric: 17.3594 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 821/1000
2023-10-10 06:58:16.548 
Epoch 821/1000 
	 loss: 17.1655, MinusLogProbMetric: 17.1655, val_loss: 17.4984, val_MinusLogProbMetric: 17.4984

Epoch 821: val_loss did not improve from 17.29763
196/196 - 59s - loss: 17.1655 - MinusLogProbMetric: 17.1655 - val_loss: 17.4984 - val_MinusLogProbMetric: 17.4984 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 822/1000
2023-10-10 06:59:13.252 
Epoch 822/1000 
	 loss: 17.4616, MinusLogProbMetric: 17.4616, val_loss: 17.3839, val_MinusLogProbMetric: 17.3839

Epoch 822: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.4616 - MinusLogProbMetric: 17.4616 - val_loss: 17.3839 - val_MinusLogProbMetric: 17.3839 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 823/1000
2023-10-10 07:00:10.278 
Epoch 823/1000 
	 loss: 17.1602, MinusLogProbMetric: 17.1602, val_loss: 17.4260, val_MinusLogProbMetric: 17.4260

Epoch 823: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1602 - MinusLogProbMetric: 17.1602 - val_loss: 17.4260 - val_MinusLogProbMetric: 17.4260 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 824/1000
2023-10-10 07:01:08.761 
Epoch 824/1000 
	 loss: 17.1525, MinusLogProbMetric: 17.1525, val_loss: 17.4201, val_MinusLogProbMetric: 17.4201

Epoch 824: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1525 - MinusLogProbMetric: 17.1525 - val_loss: 17.4201 - val_MinusLogProbMetric: 17.4201 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 825/1000
2023-10-10 07:02:05.294 
Epoch 825/1000 
	 loss: 17.1726, MinusLogProbMetric: 17.1726, val_loss: 17.3997, val_MinusLogProbMetric: 17.3997

Epoch 825: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1726 - MinusLogProbMetric: 17.1726 - val_loss: 17.3997 - val_MinusLogProbMetric: 17.3997 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 826/1000
2023-10-10 07:03:01.940 
Epoch 826/1000 
	 loss: 17.1604, MinusLogProbMetric: 17.1604, val_loss: 17.3538, val_MinusLogProbMetric: 17.3538

Epoch 826: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1604 - MinusLogProbMetric: 17.1604 - val_loss: 17.3538 - val_MinusLogProbMetric: 17.3538 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 827/1000
2023-10-10 07:03:58.776 
Epoch 827/1000 
	 loss: 17.1632, MinusLogProbMetric: 17.1632, val_loss: 17.3123, val_MinusLogProbMetric: 17.3123

Epoch 827: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1632 - MinusLogProbMetric: 17.1632 - val_loss: 17.3123 - val_MinusLogProbMetric: 17.3123 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 828/1000
2023-10-10 07:04:56.434 
Epoch 828/1000 
	 loss: 17.1628, MinusLogProbMetric: 17.1628, val_loss: 17.3408, val_MinusLogProbMetric: 17.3408

Epoch 828: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1628 - MinusLogProbMetric: 17.1628 - val_loss: 17.3408 - val_MinusLogProbMetric: 17.3408 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 829/1000
2023-10-10 07:05:52.687 
Epoch 829/1000 
	 loss: 17.1728, MinusLogProbMetric: 17.1728, val_loss: 17.5506, val_MinusLogProbMetric: 17.5506

Epoch 829: val_loss did not improve from 17.29763
196/196 - 56s - loss: 17.1728 - MinusLogProbMetric: 17.1728 - val_loss: 17.5506 - val_MinusLogProbMetric: 17.5506 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 830/1000
2023-10-10 07:06:49.549 
Epoch 830/1000 
	 loss: 17.1472, MinusLogProbMetric: 17.1472, val_loss: 17.3614, val_MinusLogProbMetric: 17.3614

Epoch 830: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1472 - MinusLogProbMetric: 17.1472 - val_loss: 17.3614 - val_MinusLogProbMetric: 17.3614 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 831/1000
2023-10-10 07:07:46.694 
Epoch 831/1000 
	 loss: 17.1535, MinusLogProbMetric: 17.1535, val_loss: 17.3751, val_MinusLogProbMetric: 17.3751

Epoch 831: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1535 - MinusLogProbMetric: 17.1535 - val_loss: 17.3751 - val_MinusLogProbMetric: 17.3751 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 832/1000
2023-10-10 07:08:43.153 
Epoch 832/1000 
	 loss: 17.1594, MinusLogProbMetric: 17.1594, val_loss: 17.3465, val_MinusLogProbMetric: 17.3465

Epoch 832: val_loss did not improve from 17.29763
196/196 - 56s - loss: 17.1594 - MinusLogProbMetric: 17.1594 - val_loss: 17.3465 - val_MinusLogProbMetric: 17.3465 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 833/1000
2023-10-10 07:09:40.234 
Epoch 833/1000 
	 loss: 17.2081, MinusLogProbMetric: 17.2081, val_loss: 17.3586, val_MinusLogProbMetric: 17.3586

Epoch 833: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.2081 - MinusLogProbMetric: 17.2081 - val_loss: 17.3586 - val_MinusLogProbMetric: 17.3586 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 834/1000
2023-10-10 07:10:36.665 
Epoch 834/1000 
	 loss: 17.1693, MinusLogProbMetric: 17.1693, val_loss: 17.4349, val_MinusLogProbMetric: 17.4349

Epoch 834: val_loss did not improve from 17.29763
196/196 - 56s - loss: 17.1693 - MinusLogProbMetric: 17.1693 - val_loss: 17.4349 - val_MinusLogProbMetric: 17.4349 - lr: 1.6667e-04 - 56s/epoch - 288ms/step
Epoch 835/1000
2023-10-10 07:11:33.489 
Epoch 835/1000 
	 loss: 17.1512, MinusLogProbMetric: 17.1512, val_loss: 17.4199, val_MinusLogProbMetric: 17.4199

Epoch 835: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1512 - MinusLogProbMetric: 17.1512 - val_loss: 17.4199 - val_MinusLogProbMetric: 17.4199 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 836/1000
2023-10-10 07:12:31.614 
Epoch 836/1000 
	 loss: 17.1559, MinusLogProbMetric: 17.1559, val_loss: 17.3401, val_MinusLogProbMetric: 17.3401

Epoch 836: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1559 - MinusLogProbMetric: 17.1559 - val_loss: 17.3401 - val_MinusLogProbMetric: 17.3401 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 837/1000
2023-10-10 07:13:29.480 
Epoch 837/1000 
	 loss: 17.1425, MinusLogProbMetric: 17.1425, val_loss: 17.3301, val_MinusLogProbMetric: 17.3301

Epoch 837: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1425 - MinusLogProbMetric: 17.1425 - val_loss: 17.3301 - val_MinusLogProbMetric: 17.3301 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 838/1000
2023-10-10 07:14:26.846 
Epoch 838/1000 
	 loss: 17.1818, MinusLogProbMetric: 17.1818, val_loss: 17.3215, val_MinusLogProbMetric: 17.3215

Epoch 838: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1818 - MinusLogProbMetric: 17.1818 - val_loss: 17.3215 - val_MinusLogProbMetric: 17.3215 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 839/1000
2023-10-10 07:15:24.452 
Epoch 839/1000 
	 loss: 17.1574, MinusLogProbMetric: 17.1574, val_loss: 17.3642, val_MinusLogProbMetric: 17.3642

Epoch 839: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1574 - MinusLogProbMetric: 17.1574 - val_loss: 17.3642 - val_MinusLogProbMetric: 17.3642 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 840/1000
2023-10-10 07:16:21.485 
Epoch 840/1000 
	 loss: 17.2066, MinusLogProbMetric: 17.2066, val_loss: 17.3904, val_MinusLogProbMetric: 17.3904

Epoch 840: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.2066 - MinusLogProbMetric: 17.2066 - val_loss: 17.3904 - val_MinusLogProbMetric: 17.3904 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 841/1000
2023-10-10 07:17:18.921 
Epoch 841/1000 
	 loss: 17.1599, MinusLogProbMetric: 17.1599, val_loss: 17.3572, val_MinusLogProbMetric: 17.3572

Epoch 841: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1599 - MinusLogProbMetric: 17.1599 - val_loss: 17.3572 - val_MinusLogProbMetric: 17.3572 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 842/1000
2023-10-10 07:18:17.541 
Epoch 842/1000 
	 loss: 17.2131, MinusLogProbMetric: 17.2131, val_loss: 17.4954, val_MinusLogProbMetric: 17.4954

Epoch 842: val_loss did not improve from 17.29763
196/196 - 59s - loss: 17.2131 - MinusLogProbMetric: 17.2131 - val_loss: 17.4954 - val_MinusLogProbMetric: 17.4954 - lr: 1.6667e-04 - 59s/epoch - 299ms/step
Epoch 843/1000
2023-10-10 07:19:15.996 
Epoch 843/1000 
	 loss: 17.1489, MinusLogProbMetric: 17.1489, val_loss: 17.3552, val_MinusLogProbMetric: 17.3552

Epoch 843: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1489 - MinusLogProbMetric: 17.1489 - val_loss: 17.3552 - val_MinusLogProbMetric: 17.3552 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 844/1000
2023-10-10 07:20:12.926 
Epoch 844/1000 
	 loss: 17.1521, MinusLogProbMetric: 17.1521, val_loss: 17.3623, val_MinusLogProbMetric: 17.3623

Epoch 844: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1521 - MinusLogProbMetric: 17.1521 - val_loss: 17.3623 - val_MinusLogProbMetric: 17.3623 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 845/1000
2023-10-10 07:21:09.856 
Epoch 845/1000 
	 loss: 17.1502, MinusLogProbMetric: 17.1502, val_loss: 17.3879, val_MinusLogProbMetric: 17.3879

Epoch 845: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1502 - MinusLogProbMetric: 17.1502 - val_loss: 17.3879 - val_MinusLogProbMetric: 17.3879 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 846/1000
2023-10-10 07:22:07.233 
Epoch 846/1000 
	 loss: 17.1460, MinusLogProbMetric: 17.1460, val_loss: 17.3177, val_MinusLogProbMetric: 17.3177

Epoch 846: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1460 - MinusLogProbMetric: 17.1460 - val_loss: 17.3177 - val_MinusLogProbMetric: 17.3177 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 847/1000
2023-10-10 07:23:04.718 
Epoch 847/1000 
	 loss: 17.1546, MinusLogProbMetric: 17.1546, val_loss: 17.3863, val_MinusLogProbMetric: 17.3863

Epoch 847: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1546 - MinusLogProbMetric: 17.1546 - val_loss: 17.3863 - val_MinusLogProbMetric: 17.3863 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 848/1000
2023-10-10 07:24:01.238 
Epoch 848/1000 
	 loss: 17.1480, MinusLogProbMetric: 17.1480, val_loss: 17.3686, val_MinusLogProbMetric: 17.3686

Epoch 848: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1480 - MinusLogProbMetric: 17.1480 - val_loss: 17.3686 - val_MinusLogProbMetric: 17.3686 - lr: 1.6667e-04 - 57s/epoch - 288ms/step
Epoch 849/1000
2023-10-10 07:24:59.196 
Epoch 849/1000 
	 loss: 17.1647, MinusLogProbMetric: 17.1647, val_loss: 17.3647, val_MinusLogProbMetric: 17.3647

Epoch 849: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1647 - MinusLogProbMetric: 17.1647 - val_loss: 17.3647 - val_MinusLogProbMetric: 17.3647 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 850/1000
2023-10-10 07:25:57.101 
Epoch 850/1000 
	 loss: 17.1500, MinusLogProbMetric: 17.1500, val_loss: 17.3781, val_MinusLogProbMetric: 17.3781

Epoch 850: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1500 - MinusLogProbMetric: 17.1500 - val_loss: 17.3781 - val_MinusLogProbMetric: 17.3781 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 851/1000
2023-10-10 07:26:53.727 
Epoch 851/1000 
	 loss: 17.1607, MinusLogProbMetric: 17.1607, val_loss: 17.3365, val_MinusLogProbMetric: 17.3365

Epoch 851: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1607 - MinusLogProbMetric: 17.1607 - val_loss: 17.3365 - val_MinusLogProbMetric: 17.3365 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 852/1000
2023-10-10 07:27:51.100 
Epoch 852/1000 
	 loss: 17.1820, MinusLogProbMetric: 17.1820, val_loss: 17.3913, val_MinusLogProbMetric: 17.3913

Epoch 852: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1820 - MinusLogProbMetric: 17.1820 - val_loss: 17.3913 - val_MinusLogProbMetric: 17.3913 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 853/1000
2023-10-10 07:28:49.103 
Epoch 853/1000 
	 loss: 17.1582, MinusLogProbMetric: 17.1582, val_loss: 17.4851, val_MinusLogProbMetric: 17.4851

Epoch 853: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1582 - MinusLogProbMetric: 17.1582 - val_loss: 17.4851 - val_MinusLogProbMetric: 17.4851 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 854/1000
2023-10-10 07:29:48.050 
Epoch 854/1000 
	 loss: 17.2152, MinusLogProbMetric: 17.2152, val_loss: 17.3419, val_MinusLogProbMetric: 17.3419

Epoch 854: val_loss did not improve from 17.29763
196/196 - 59s - loss: 17.2152 - MinusLogProbMetric: 17.2152 - val_loss: 17.3419 - val_MinusLogProbMetric: 17.3419 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 855/1000
2023-10-10 07:30:45.818 
Epoch 855/1000 
	 loss: 17.1774, MinusLogProbMetric: 17.1774, val_loss: 17.3075, val_MinusLogProbMetric: 17.3075

Epoch 855: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1774 - MinusLogProbMetric: 17.1774 - val_loss: 17.3075 - val_MinusLogProbMetric: 17.3075 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 856/1000
2023-10-10 07:31:43.072 
Epoch 856/1000 
	 loss: 17.1485, MinusLogProbMetric: 17.1485, val_loss: 17.4015, val_MinusLogProbMetric: 17.4015

Epoch 856: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1485 - MinusLogProbMetric: 17.1485 - val_loss: 17.4015 - val_MinusLogProbMetric: 17.4015 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 857/1000
2023-10-10 07:32:39.966 
Epoch 857/1000 
	 loss: 17.1676, MinusLogProbMetric: 17.1676, val_loss: 17.3294, val_MinusLogProbMetric: 17.3294

Epoch 857: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1676 - MinusLogProbMetric: 17.1676 - val_loss: 17.3294 - val_MinusLogProbMetric: 17.3294 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 858/1000
2023-10-10 07:33:37.788 
Epoch 858/1000 
	 loss: 17.1725, MinusLogProbMetric: 17.1725, val_loss: 17.3604, val_MinusLogProbMetric: 17.3604

Epoch 858: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1725 - MinusLogProbMetric: 17.1725 - val_loss: 17.3604 - val_MinusLogProbMetric: 17.3604 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 859/1000
2023-10-10 07:34:35.641 
Epoch 859/1000 
	 loss: 17.2791, MinusLogProbMetric: 17.2791, val_loss: 17.3510, val_MinusLogProbMetric: 17.3510

Epoch 859: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.2791 - MinusLogProbMetric: 17.2791 - val_loss: 17.3510 - val_MinusLogProbMetric: 17.3510 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 860/1000
2023-10-10 07:35:32.877 
Epoch 860/1000 
	 loss: 17.1357, MinusLogProbMetric: 17.1357, val_loss: 17.4041, val_MinusLogProbMetric: 17.4041

Epoch 860: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1357 - MinusLogProbMetric: 17.1357 - val_loss: 17.4041 - val_MinusLogProbMetric: 17.4041 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 861/1000
2023-10-10 07:36:30.226 
Epoch 861/1000 
	 loss: 17.1613, MinusLogProbMetric: 17.1613, val_loss: 17.3097, val_MinusLogProbMetric: 17.3097

Epoch 861: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1613 - MinusLogProbMetric: 17.1613 - val_loss: 17.3097 - val_MinusLogProbMetric: 17.3097 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 862/1000
2023-10-10 07:37:26.779 
Epoch 862/1000 
	 loss: 17.4446, MinusLogProbMetric: 17.4446, val_loss: 17.4179, val_MinusLogProbMetric: 17.4179

Epoch 862: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.4446 - MinusLogProbMetric: 17.4446 - val_loss: 17.4179 - val_MinusLogProbMetric: 17.4179 - lr: 1.6667e-04 - 57s/epoch - 289ms/step
Epoch 863/1000
2023-10-10 07:38:23.765 
Epoch 863/1000 
	 loss: 17.1494, MinusLogProbMetric: 17.1494, val_loss: 17.4264, val_MinusLogProbMetric: 17.4264

Epoch 863: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1494 - MinusLogProbMetric: 17.1494 - val_loss: 17.4264 - val_MinusLogProbMetric: 17.4264 - lr: 1.6667e-04 - 57s/epoch - 291ms/step
Epoch 864/1000
2023-10-10 07:39:21.264 
Epoch 864/1000 
	 loss: 17.1315, MinusLogProbMetric: 17.1315, val_loss: 17.3688, val_MinusLogProbMetric: 17.3688

Epoch 864: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1315 - MinusLogProbMetric: 17.1315 - val_loss: 17.3688 - val_MinusLogProbMetric: 17.3688 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 865/1000
2023-10-10 07:40:20.231 
Epoch 865/1000 
	 loss: 17.1380, MinusLogProbMetric: 17.1380, val_loss: 17.3666, val_MinusLogProbMetric: 17.3666

Epoch 865: val_loss did not improve from 17.29763
196/196 - 59s - loss: 17.1380 - MinusLogProbMetric: 17.1380 - val_loss: 17.3666 - val_MinusLogProbMetric: 17.3666 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 866/1000
2023-10-10 07:41:18.606 
Epoch 866/1000 
	 loss: 17.1435, MinusLogProbMetric: 17.1435, val_loss: 17.3421, val_MinusLogProbMetric: 17.3421

Epoch 866: val_loss did not improve from 17.29763
196/196 - 58s - loss: 17.1435 - MinusLogProbMetric: 17.1435 - val_loss: 17.3421 - val_MinusLogProbMetric: 17.3421 - lr: 1.6667e-04 - 58s/epoch - 298ms/step
Epoch 867/1000
2023-10-10 07:42:15.747 
Epoch 867/1000 
	 loss: 17.1439, MinusLogProbMetric: 17.1439, val_loss: 17.4387, val_MinusLogProbMetric: 17.4387

Epoch 867: val_loss did not improve from 17.29763
196/196 - 57s - loss: 17.1439 - MinusLogProbMetric: 17.1439 - val_loss: 17.4387 - val_MinusLogProbMetric: 17.4387 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 868/1000
2023-10-10 07:43:12.855 
Epoch 868/1000 
	 loss: 17.0534, MinusLogProbMetric: 17.0534, val_loss: 17.2321, val_MinusLogProbMetric: 17.2321

Epoch 868: val_loss improved from 17.29763 to 17.23210, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.0534 - MinusLogProbMetric: 17.0534 - val_loss: 17.2321 - val_MinusLogProbMetric: 17.2321 - lr: 8.3333e-05 - 58s/epoch - 297ms/step
Epoch 869/1000
2023-10-10 07:44:11.902 
Epoch 869/1000 
	 loss: 17.0511, MinusLogProbMetric: 17.0511, val_loss: 17.2543, val_MinusLogProbMetric: 17.2543

Epoch 869: val_loss did not improve from 17.23210
196/196 - 58s - loss: 17.0511 - MinusLogProbMetric: 17.0511 - val_loss: 17.2543 - val_MinusLogProbMetric: 17.2543 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 870/1000
2023-10-10 07:45:09.288 
Epoch 870/1000 
	 loss: 17.0753, MinusLogProbMetric: 17.0753, val_loss: 17.2661, val_MinusLogProbMetric: 17.2661

Epoch 870: val_loss did not improve from 17.23210
196/196 - 57s - loss: 17.0753 - MinusLogProbMetric: 17.0753 - val_loss: 17.2661 - val_MinusLogProbMetric: 17.2661 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 871/1000
2023-10-10 07:46:06.678 
Epoch 871/1000 
	 loss: 17.0594, MinusLogProbMetric: 17.0594, val_loss: 17.3034, val_MinusLogProbMetric: 17.3034

Epoch 871: val_loss did not improve from 17.23210
196/196 - 57s - loss: 17.0594 - MinusLogProbMetric: 17.0594 - val_loss: 17.3034 - val_MinusLogProbMetric: 17.3034 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 872/1000
2023-10-10 07:47:03.789 
Epoch 872/1000 
	 loss: 17.0528, MinusLogProbMetric: 17.0528, val_loss: 17.2743, val_MinusLogProbMetric: 17.2743

Epoch 872: val_loss did not improve from 17.23210
196/196 - 57s - loss: 17.0528 - MinusLogProbMetric: 17.0528 - val_loss: 17.2743 - val_MinusLogProbMetric: 17.2743 - lr: 8.3333e-05 - 57s/epoch - 291ms/step
Epoch 873/1000
2023-10-10 07:48:00.397 
Epoch 873/1000 
	 loss: 17.0629, MinusLogProbMetric: 17.0629, val_loss: 17.2669, val_MinusLogProbMetric: 17.2669

Epoch 873: val_loss did not improve from 17.23210
196/196 - 57s - loss: 17.0629 - MinusLogProbMetric: 17.0629 - val_loss: 17.2669 - val_MinusLogProbMetric: 17.2669 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 874/1000
2023-10-10 07:48:57.414 
Epoch 874/1000 
	 loss: 17.0502, MinusLogProbMetric: 17.0502, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 874: val_loss did not improve from 17.23210
196/196 - 57s - loss: 17.0502 - MinusLogProbMetric: 17.0502 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 8.3333e-05 - 57s/epoch - 291ms/step
Epoch 875/1000
2023-10-10 07:49:54.939 
Epoch 875/1000 
	 loss: 17.0511, MinusLogProbMetric: 17.0511, val_loss: 17.2249, val_MinusLogProbMetric: 17.2249

Epoch 875: val_loss improved from 17.23210 to 17.22486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.0511 - MinusLogProbMetric: 17.0511 - val_loss: 17.2249 - val_MinusLogProbMetric: 17.2249 - lr: 8.3333e-05 - 59s/epoch - 299ms/step
Epoch 876/1000
2023-10-10 07:50:52.636 
Epoch 876/1000 
	 loss: 17.0472, MinusLogProbMetric: 17.0472, val_loss: 17.2424, val_MinusLogProbMetric: 17.2424

Epoch 876: val_loss did not improve from 17.22486
196/196 - 57s - loss: 17.0472 - MinusLogProbMetric: 17.0472 - val_loss: 17.2424 - val_MinusLogProbMetric: 17.2424 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 877/1000
2023-10-10 07:51:49.694 
Epoch 877/1000 
	 loss: 17.0697, MinusLogProbMetric: 17.0697, val_loss: 17.2507, val_MinusLogProbMetric: 17.2507

Epoch 877: val_loss did not improve from 17.22486
196/196 - 57s - loss: 17.0697 - MinusLogProbMetric: 17.0697 - val_loss: 17.2507 - val_MinusLogProbMetric: 17.2507 - lr: 8.3333e-05 - 57s/epoch - 291ms/step
Epoch 878/1000
2023-10-10 07:52:46.620 
Epoch 878/1000 
	 loss: 17.0438, MinusLogProbMetric: 17.0438, val_loss: 17.2238, val_MinusLogProbMetric: 17.2238

Epoch 878: val_loss improved from 17.22486 to 17.22379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 58s - loss: 17.0438 - MinusLogProbMetric: 17.0438 - val_loss: 17.2238 - val_MinusLogProbMetric: 17.2238 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 879/1000
2023-10-10 07:53:45.817 
Epoch 879/1000 
	 loss: 17.0626, MinusLogProbMetric: 17.0626, val_loss: 17.2724, val_MinusLogProbMetric: 17.2724

Epoch 879: val_loss did not improve from 17.22379
196/196 - 58s - loss: 17.0626 - MinusLogProbMetric: 17.0626 - val_loss: 17.2724 - val_MinusLogProbMetric: 17.2724 - lr: 8.3333e-05 - 58s/epoch - 297ms/step
Epoch 880/1000
2023-10-10 07:54:43.204 
Epoch 880/1000 
	 loss: 17.0529, MinusLogProbMetric: 17.0529, val_loss: 17.2591, val_MinusLogProbMetric: 17.2591

Epoch 880: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0529 - MinusLogProbMetric: 17.0529 - val_loss: 17.2591 - val_MinusLogProbMetric: 17.2591 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 881/1000
2023-10-10 07:55:41.320 
Epoch 881/1000 
	 loss: 17.0436, MinusLogProbMetric: 17.0436, val_loss: 17.2989, val_MinusLogProbMetric: 17.2989

Epoch 881: val_loss did not improve from 17.22379
196/196 - 58s - loss: 17.0436 - MinusLogProbMetric: 17.0436 - val_loss: 17.2989 - val_MinusLogProbMetric: 17.2989 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 882/1000
2023-10-10 07:56:37.222 
Epoch 882/1000 
	 loss: 17.0429, MinusLogProbMetric: 17.0429, val_loss: 17.2461, val_MinusLogProbMetric: 17.2461

Epoch 882: val_loss did not improve from 17.22379
196/196 - 56s - loss: 17.0429 - MinusLogProbMetric: 17.0429 - val_loss: 17.2461 - val_MinusLogProbMetric: 17.2461 - lr: 8.3333e-05 - 56s/epoch - 285ms/step
Epoch 883/1000
2023-10-10 07:57:33.978 
Epoch 883/1000 
	 loss: 17.1715, MinusLogProbMetric: 17.1715, val_loss: 17.2618, val_MinusLogProbMetric: 17.2618

Epoch 883: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.1715 - MinusLogProbMetric: 17.1715 - val_loss: 17.2618 - val_MinusLogProbMetric: 17.2618 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 884/1000
2023-10-10 07:58:31.259 
Epoch 884/1000 
	 loss: 17.0490, MinusLogProbMetric: 17.0490, val_loss: 17.2278, val_MinusLogProbMetric: 17.2278

Epoch 884: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0490 - MinusLogProbMetric: 17.0490 - val_loss: 17.2278 - val_MinusLogProbMetric: 17.2278 - lr: 8.3333e-05 - 57s/epoch - 292ms/step
Epoch 885/1000
2023-10-10 07:59:29.008 
Epoch 885/1000 
	 loss: 17.0468, MinusLogProbMetric: 17.0468, val_loss: 17.2491, val_MinusLogProbMetric: 17.2491

Epoch 885: val_loss did not improve from 17.22379
196/196 - 58s - loss: 17.0468 - MinusLogProbMetric: 17.0468 - val_loss: 17.2491 - val_MinusLogProbMetric: 17.2491 - lr: 8.3333e-05 - 58s/epoch - 295ms/step
Epoch 886/1000
2023-10-10 08:00:25.810 
Epoch 886/1000 
	 loss: 17.0537, MinusLogProbMetric: 17.0537, val_loss: 17.2425, val_MinusLogProbMetric: 17.2425

Epoch 886: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0537 - MinusLogProbMetric: 17.0537 - val_loss: 17.2425 - val_MinusLogProbMetric: 17.2425 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 887/1000
2023-10-10 08:01:22.832 
Epoch 887/1000 
	 loss: 17.0598, MinusLogProbMetric: 17.0598, val_loss: 17.2462, val_MinusLogProbMetric: 17.2462

Epoch 887: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0598 - MinusLogProbMetric: 17.0598 - val_loss: 17.2462 - val_MinusLogProbMetric: 17.2462 - lr: 8.3333e-05 - 57s/epoch - 291ms/step
Epoch 888/1000
2023-10-10 08:02:19.272 
Epoch 888/1000 
	 loss: 17.3690, MinusLogProbMetric: 17.3690, val_loss: 17.2809, val_MinusLogProbMetric: 17.2809

Epoch 888: val_loss did not improve from 17.22379
196/196 - 56s - loss: 17.3690 - MinusLogProbMetric: 17.3690 - val_loss: 17.2809 - val_MinusLogProbMetric: 17.2809 - lr: 8.3333e-05 - 56s/epoch - 288ms/step
Epoch 889/1000
2023-10-10 08:03:16.828 
Epoch 889/1000 
	 loss: 17.0595, MinusLogProbMetric: 17.0595, val_loss: 17.2540, val_MinusLogProbMetric: 17.2540

Epoch 889: val_loss did not improve from 17.22379
196/196 - 58s - loss: 17.0595 - MinusLogProbMetric: 17.0595 - val_loss: 17.2540 - val_MinusLogProbMetric: 17.2540 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 890/1000
2023-10-10 08:04:14.227 
Epoch 890/1000 
	 loss: 17.0484, MinusLogProbMetric: 17.0484, val_loss: 17.2394, val_MinusLogProbMetric: 17.2394

Epoch 890: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0484 - MinusLogProbMetric: 17.0484 - val_loss: 17.2394 - val_MinusLogProbMetric: 17.2394 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 891/1000
2023-10-10 08:05:12.993 
Epoch 891/1000 
	 loss: 17.0490, MinusLogProbMetric: 17.0490, val_loss: 17.2540, val_MinusLogProbMetric: 17.2540

Epoch 891: val_loss did not improve from 17.22379
196/196 - 59s - loss: 17.0490 - MinusLogProbMetric: 17.0490 - val_loss: 17.2540 - val_MinusLogProbMetric: 17.2540 - lr: 8.3333e-05 - 59s/epoch - 300ms/step
Epoch 892/1000
2023-10-10 08:06:11.528 
Epoch 892/1000 
	 loss: 17.0505, MinusLogProbMetric: 17.0505, val_loss: 17.2542, val_MinusLogProbMetric: 17.2542

Epoch 892: val_loss did not improve from 17.22379
196/196 - 59s - loss: 17.0505 - MinusLogProbMetric: 17.0505 - val_loss: 17.2542 - val_MinusLogProbMetric: 17.2542 - lr: 8.3333e-05 - 59s/epoch - 299ms/step
Epoch 893/1000
2023-10-10 08:07:09.172 
Epoch 893/1000 
	 loss: 17.0454, MinusLogProbMetric: 17.0454, val_loss: 17.2395, val_MinusLogProbMetric: 17.2395

Epoch 893: val_loss did not improve from 17.22379
196/196 - 58s - loss: 17.0454 - MinusLogProbMetric: 17.0454 - val_loss: 17.2395 - val_MinusLogProbMetric: 17.2395 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 894/1000
2023-10-10 08:08:06.043 
Epoch 894/1000 
	 loss: 17.0415, MinusLogProbMetric: 17.0415, val_loss: 17.2401, val_MinusLogProbMetric: 17.2401

Epoch 894: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0415 - MinusLogProbMetric: 17.0415 - val_loss: 17.2401 - val_MinusLogProbMetric: 17.2401 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 895/1000
2023-10-10 08:09:03.487 
Epoch 895/1000 
	 loss: 17.0431, MinusLogProbMetric: 17.0431, val_loss: 17.2289, val_MinusLogProbMetric: 17.2289

Epoch 895: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0431 - MinusLogProbMetric: 17.0431 - val_loss: 17.2289 - val_MinusLogProbMetric: 17.2289 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 896/1000
2023-10-10 08:10:00.828 
Epoch 896/1000 
	 loss: 17.0499, MinusLogProbMetric: 17.0499, val_loss: 17.2764, val_MinusLogProbMetric: 17.2764

Epoch 896: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0499 - MinusLogProbMetric: 17.0499 - val_loss: 17.2764 - val_MinusLogProbMetric: 17.2764 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 897/1000
2023-10-10 08:10:59.398 
Epoch 897/1000 
	 loss: 17.0641, MinusLogProbMetric: 17.0641, val_loss: 17.2391, val_MinusLogProbMetric: 17.2391

Epoch 897: val_loss did not improve from 17.22379
196/196 - 59s - loss: 17.0641 - MinusLogProbMetric: 17.0641 - val_loss: 17.2391 - val_MinusLogProbMetric: 17.2391 - lr: 8.3333e-05 - 59s/epoch - 299ms/step
Epoch 898/1000
2023-10-10 08:11:56.250 
Epoch 898/1000 
	 loss: 17.0490, MinusLogProbMetric: 17.0490, val_loss: 17.2557, val_MinusLogProbMetric: 17.2557

Epoch 898: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0490 - MinusLogProbMetric: 17.0490 - val_loss: 17.2557 - val_MinusLogProbMetric: 17.2557 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 899/1000
2023-10-10 08:12:53.039 
Epoch 899/1000 
	 loss: 17.0459, MinusLogProbMetric: 17.0459, val_loss: 17.2388, val_MinusLogProbMetric: 17.2388

Epoch 899: val_loss did not improve from 17.22379
196/196 - 57s - loss: 17.0459 - MinusLogProbMetric: 17.0459 - val_loss: 17.2388 - val_MinusLogProbMetric: 17.2388 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 900/1000
2023-10-10 08:13:51.164 
Epoch 900/1000 
	 loss: 17.0574, MinusLogProbMetric: 17.0574, val_loss: 17.2233, val_MinusLogProbMetric: 17.2233

Epoch 900: val_loss improved from 17.22379 to 17.22328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.0574 - MinusLogProbMetric: 17.0574 - val_loss: 17.2233 - val_MinusLogProbMetric: 17.2233 - lr: 8.3333e-05 - 59s/epoch - 302ms/step
Epoch 901/1000
2023-10-10 08:14:49.023 
Epoch 901/1000 
	 loss: 17.0852, MinusLogProbMetric: 17.0852, val_loss: 17.2395, val_MinusLogProbMetric: 17.2395

Epoch 901: val_loss did not improve from 17.22328
196/196 - 57s - loss: 17.0852 - MinusLogProbMetric: 17.0852 - val_loss: 17.2395 - val_MinusLogProbMetric: 17.2395 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 902/1000
2023-10-10 08:15:46.489 
Epoch 902/1000 
	 loss: 17.0446, MinusLogProbMetric: 17.0446, val_loss: 17.2339, val_MinusLogProbMetric: 17.2339

Epoch 902: val_loss did not improve from 17.22328
196/196 - 57s - loss: 17.0446 - MinusLogProbMetric: 17.0446 - val_loss: 17.2339 - val_MinusLogProbMetric: 17.2339 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 903/1000
2023-10-10 08:16:44.496 
Epoch 903/1000 
	 loss: 17.0374, MinusLogProbMetric: 17.0374, val_loss: 17.2446, val_MinusLogProbMetric: 17.2446

Epoch 903: val_loss did not improve from 17.22328
196/196 - 58s - loss: 17.0374 - MinusLogProbMetric: 17.0374 - val_loss: 17.2446 - val_MinusLogProbMetric: 17.2446 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 904/1000
2023-10-10 08:17:41.853 
Epoch 904/1000 
	 loss: 17.0513, MinusLogProbMetric: 17.0513, val_loss: 17.2481, val_MinusLogProbMetric: 17.2481

Epoch 904: val_loss did not improve from 17.22328
196/196 - 57s - loss: 17.0513 - MinusLogProbMetric: 17.0513 - val_loss: 17.2481 - val_MinusLogProbMetric: 17.2481 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 905/1000
2023-10-10 08:18:38.928 
Epoch 905/1000 
	 loss: 17.0480, MinusLogProbMetric: 17.0480, val_loss: 17.3116, val_MinusLogProbMetric: 17.3116

Epoch 905: val_loss did not improve from 17.22328
196/196 - 57s - loss: 17.0480 - MinusLogProbMetric: 17.0480 - val_loss: 17.3116 - val_MinusLogProbMetric: 17.3116 - lr: 8.3333e-05 - 57s/epoch - 291ms/step
Epoch 906/1000
2023-10-10 08:19:36.750 
Epoch 906/1000 
	 loss: 17.0406, MinusLogProbMetric: 17.0406, val_loss: 17.2314, val_MinusLogProbMetric: 17.2314

Epoch 906: val_loss did not improve from 17.22328
196/196 - 58s - loss: 17.0406 - MinusLogProbMetric: 17.0406 - val_loss: 17.2314 - val_MinusLogProbMetric: 17.2314 - lr: 8.3333e-05 - 58s/epoch - 295ms/step
Epoch 907/1000
2023-10-10 08:20:34.294 
Epoch 907/1000 
	 loss: 17.0469, MinusLogProbMetric: 17.0469, val_loss: 17.2316, val_MinusLogProbMetric: 17.2316

Epoch 907: val_loss did not improve from 17.22328
196/196 - 58s - loss: 17.0469 - MinusLogProbMetric: 17.0469 - val_loss: 17.2316 - val_MinusLogProbMetric: 17.2316 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 908/1000
2023-10-10 08:21:31.100 
Epoch 908/1000 
	 loss: 17.1336, MinusLogProbMetric: 17.1336, val_loss: 17.2334, val_MinusLogProbMetric: 17.2334

Epoch 908: val_loss did not improve from 17.22328
196/196 - 57s - loss: 17.1336 - MinusLogProbMetric: 17.1336 - val_loss: 17.2334 - val_MinusLogProbMetric: 17.2334 - lr: 8.3333e-05 - 57s/epoch - 290ms/step
Epoch 909/1000
2023-10-10 08:22:28.742 
Epoch 909/1000 
	 loss: 17.0420, MinusLogProbMetric: 17.0420, val_loss: 17.2544, val_MinusLogProbMetric: 17.2544

Epoch 909: val_loss did not improve from 17.22328
196/196 - 58s - loss: 17.0420 - MinusLogProbMetric: 17.0420 - val_loss: 17.2544 - val_MinusLogProbMetric: 17.2544 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 910/1000
2023-10-10 08:23:26.696 
Epoch 910/1000 
	 loss: 17.0461, MinusLogProbMetric: 17.0461, val_loss: 17.2138, val_MinusLogProbMetric: 17.2138

Epoch 910: val_loss improved from 17.22328 to 17.21378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 59s - loss: 17.0461 - MinusLogProbMetric: 17.0461 - val_loss: 17.2138 - val_MinusLogProbMetric: 17.2138 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 911/1000
2023-10-10 08:24:25.246 
Epoch 911/1000 
	 loss: 17.0468, MinusLogProbMetric: 17.0468, val_loss: 17.2600, val_MinusLogProbMetric: 17.2600

Epoch 911: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0468 - MinusLogProbMetric: 17.0468 - val_loss: 17.2600 - val_MinusLogProbMetric: 17.2600 - lr: 8.3333e-05 - 58s/epoch - 293ms/step
Epoch 912/1000
2023-10-10 08:25:23.475 
Epoch 912/1000 
	 loss: 17.0411, MinusLogProbMetric: 17.0411, val_loss: 17.2730, val_MinusLogProbMetric: 17.2730

Epoch 912: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0411 - MinusLogProbMetric: 17.0411 - val_loss: 17.2730 - val_MinusLogProbMetric: 17.2730 - lr: 8.3333e-05 - 58s/epoch - 297ms/step
Epoch 913/1000
2023-10-10 08:26:21.926 
Epoch 913/1000 
	 loss: 17.0444, MinusLogProbMetric: 17.0444, val_loss: 17.2508, val_MinusLogProbMetric: 17.2508

Epoch 913: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0444 - MinusLogProbMetric: 17.0444 - val_loss: 17.2508 - val_MinusLogProbMetric: 17.2508 - lr: 8.3333e-05 - 58s/epoch - 298ms/step
Epoch 914/1000
2023-10-10 08:27:19.764 
Epoch 914/1000 
	 loss: 17.0449, MinusLogProbMetric: 17.0449, val_loss: 17.2512, val_MinusLogProbMetric: 17.2512

Epoch 914: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0449 - MinusLogProbMetric: 17.0449 - val_loss: 17.2512 - val_MinusLogProbMetric: 17.2512 - lr: 8.3333e-05 - 58s/epoch - 295ms/step
Epoch 915/1000
2023-10-10 08:28:17.365 
Epoch 915/1000 
	 loss: 17.1043, MinusLogProbMetric: 17.1043, val_loss: 17.2416, val_MinusLogProbMetric: 17.2416

Epoch 915: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.1043 - MinusLogProbMetric: 17.1043 - val_loss: 17.2416 - val_MinusLogProbMetric: 17.2416 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 916/1000
2023-10-10 08:29:14.052 
Epoch 916/1000 
	 loss: 17.0386, MinusLogProbMetric: 17.0386, val_loss: 17.2767, val_MinusLogProbMetric: 17.2767

Epoch 916: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0386 - MinusLogProbMetric: 17.0386 - val_loss: 17.2767 - val_MinusLogProbMetric: 17.2767 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 917/1000
2023-10-10 08:30:10.116 
Epoch 917/1000 
	 loss: 17.0495, MinusLogProbMetric: 17.0495, val_loss: 17.2499, val_MinusLogProbMetric: 17.2499

Epoch 917: val_loss did not improve from 17.21378
196/196 - 56s - loss: 17.0495 - MinusLogProbMetric: 17.0495 - val_loss: 17.2499 - val_MinusLogProbMetric: 17.2499 - lr: 8.3333e-05 - 56s/epoch - 286ms/step
Epoch 918/1000
2023-10-10 08:31:06.455 
Epoch 918/1000 
	 loss: 17.0499, MinusLogProbMetric: 17.0499, val_loss: 17.2541, val_MinusLogProbMetric: 17.2541

Epoch 918: val_loss did not improve from 17.21378
196/196 - 56s - loss: 17.0499 - MinusLogProbMetric: 17.0499 - val_loss: 17.2541 - val_MinusLogProbMetric: 17.2541 - lr: 8.3333e-05 - 56s/epoch - 287ms/step
Epoch 919/1000
2023-10-10 08:32:02.826 
Epoch 919/1000 
	 loss: 17.0447, MinusLogProbMetric: 17.0447, val_loss: 17.2562, val_MinusLogProbMetric: 17.2562

Epoch 919: val_loss did not improve from 17.21378
196/196 - 56s - loss: 17.0447 - MinusLogProbMetric: 17.0447 - val_loss: 17.2562 - val_MinusLogProbMetric: 17.2562 - lr: 8.3333e-05 - 56s/epoch - 288ms/step
Epoch 920/1000
2023-10-10 08:32:59.510 
Epoch 920/1000 
	 loss: 17.0447, MinusLogProbMetric: 17.0447, val_loss: 17.2166, val_MinusLogProbMetric: 17.2166

Epoch 920: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0447 - MinusLogProbMetric: 17.0447 - val_loss: 17.2166 - val_MinusLogProbMetric: 17.2166 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 921/1000
2023-10-10 08:33:56.085 
Epoch 921/1000 
	 loss: 17.0392, MinusLogProbMetric: 17.0392, val_loss: 17.2495, val_MinusLogProbMetric: 17.2495

Epoch 921: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0392 - MinusLogProbMetric: 17.0392 - val_loss: 17.2495 - val_MinusLogProbMetric: 17.2495 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 922/1000
2023-10-10 08:34:52.708 
Epoch 922/1000 
	 loss: 17.0456, MinusLogProbMetric: 17.0456, val_loss: 17.2731, val_MinusLogProbMetric: 17.2731

Epoch 922: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0456 - MinusLogProbMetric: 17.0456 - val_loss: 17.2731 - val_MinusLogProbMetric: 17.2731 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 923/1000
2023-10-10 08:35:50.471 
Epoch 923/1000 
	 loss: 17.0435, MinusLogProbMetric: 17.0435, val_loss: 17.2696, val_MinusLogProbMetric: 17.2696

Epoch 923: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0435 - MinusLogProbMetric: 17.0435 - val_loss: 17.2696 - val_MinusLogProbMetric: 17.2696 - lr: 8.3333e-05 - 58s/epoch - 295ms/step
Epoch 924/1000
2023-10-10 08:36:48.404 
Epoch 924/1000 
	 loss: 17.0421, MinusLogProbMetric: 17.0421, val_loss: 17.2391, val_MinusLogProbMetric: 17.2391

Epoch 924: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0421 - MinusLogProbMetric: 17.0421 - val_loss: 17.2391 - val_MinusLogProbMetric: 17.2391 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 925/1000
2023-10-10 08:37:46.809 
Epoch 925/1000 
	 loss: 17.0570, MinusLogProbMetric: 17.0570, val_loss: 17.2723, val_MinusLogProbMetric: 17.2723

Epoch 925: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0570 - MinusLogProbMetric: 17.0570 - val_loss: 17.2723 - val_MinusLogProbMetric: 17.2723 - lr: 8.3333e-05 - 58s/epoch - 298ms/step
Epoch 926/1000
2023-10-10 08:38:46.377 
Epoch 926/1000 
	 loss: 17.0408, MinusLogProbMetric: 17.0408, val_loss: 17.2289, val_MinusLogProbMetric: 17.2289

Epoch 926: val_loss did not improve from 17.21378
196/196 - 60s - loss: 17.0408 - MinusLogProbMetric: 17.0408 - val_loss: 17.2289 - val_MinusLogProbMetric: 17.2289 - lr: 8.3333e-05 - 60s/epoch - 304ms/step
Epoch 927/1000
2023-10-10 08:39:44.553 
Epoch 927/1000 
	 loss: 17.0368, MinusLogProbMetric: 17.0368, val_loss: 17.2517, val_MinusLogProbMetric: 17.2517

Epoch 927: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0368 - MinusLogProbMetric: 17.0368 - val_loss: 17.2517 - val_MinusLogProbMetric: 17.2517 - lr: 8.3333e-05 - 58s/epoch - 297ms/step
Epoch 928/1000
2023-10-10 08:40:42.837 
Epoch 928/1000 
	 loss: 17.0362, MinusLogProbMetric: 17.0362, val_loss: 17.2488, val_MinusLogProbMetric: 17.2488

Epoch 928: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0362 - MinusLogProbMetric: 17.0362 - val_loss: 17.2488 - val_MinusLogProbMetric: 17.2488 - lr: 8.3333e-05 - 58s/epoch - 297ms/step
Epoch 929/1000
2023-10-10 08:41:40.930 
Epoch 929/1000 
	 loss: 17.0498, MinusLogProbMetric: 17.0498, val_loss: 17.2450, val_MinusLogProbMetric: 17.2450

Epoch 929: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0498 - MinusLogProbMetric: 17.0498 - val_loss: 17.2450 - val_MinusLogProbMetric: 17.2450 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 930/1000
2023-10-10 08:42:38.854 
Epoch 930/1000 
	 loss: 17.0379, MinusLogProbMetric: 17.0379, val_loss: 17.2327, val_MinusLogProbMetric: 17.2327

Epoch 930: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0379 - MinusLogProbMetric: 17.0379 - val_loss: 17.2327 - val_MinusLogProbMetric: 17.2327 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 931/1000
2023-10-10 08:43:36.572 
Epoch 931/1000 
	 loss: 17.0403, MinusLogProbMetric: 17.0403, val_loss: 17.2571, val_MinusLogProbMetric: 17.2571

Epoch 931: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0403 - MinusLogProbMetric: 17.0403 - val_loss: 17.2571 - val_MinusLogProbMetric: 17.2571 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 932/1000
2023-10-10 08:44:34.040 
Epoch 932/1000 
	 loss: 17.0382, MinusLogProbMetric: 17.0382, val_loss: 17.2278, val_MinusLogProbMetric: 17.2278

Epoch 932: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0382 - MinusLogProbMetric: 17.0382 - val_loss: 17.2278 - val_MinusLogProbMetric: 17.2278 - lr: 8.3333e-05 - 57s/epoch - 293ms/step
Epoch 933/1000
2023-10-10 08:45:32.115 
Epoch 933/1000 
	 loss: 17.0338, MinusLogProbMetric: 17.0338, val_loss: 17.2401, val_MinusLogProbMetric: 17.2401

Epoch 933: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0338 - MinusLogProbMetric: 17.0338 - val_loss: 17.2401 - val_MinusLogProbMetric: 17.2401 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 934/1000
2023-10-10 08:46:30.025 
Epoch 934/1000 
	 loss: 17.0588, MinusLogProbMetric: 17.0588, val_loss: 17.2157, val_MinusLogProbMetric: 17.2157

Epoch 934: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0588 - MinusLogProbMetric: 17.0588 - val_loss: 17.2157 - val_MinusLogProbMetric: 17.2157 - lr: 8.3333e-05 - 58s/epoch - 295ms/step
Epoch 935/1000
2023-10-10 08:47:26.078 
Epoch 935/1000 
	 loss: 17.0421, MinusLogProbMetric: 17.0421, val_loss: 17.2631, val_MinusLogProbMetric: 17.2631

Epoch 935: val_loss did not improve from 17.21378
196/196 - 56s - loss: 17.0421 - MinusLogProbMetric: 17.0421 - val_loss: 17.2631 - val_MinusLogProbMetric: 17.2631 - lr: 8.3333e-05 - 56s/epoch - 286ms/step
Epoch 936/1000
2023-10-10 08:48:22.639 
Epoch 936/1000 
	 loss: 17.0347, MinusLogProbMetric: 17.0347, val_loss: 17.2269, val_MinusLogProbMetric: 17.2269

Epoch 936: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0347 - MinusLogProbMetric: 17.0347 - val_loss: 17.2269 - val_MinusLogProbMetric: 17.2269 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 937/1000
2023-10-10 08:49:20.491 
Epoch 937/1000 
	 loss: 17.0435, MinusLogProbMetric: 17.0435, val_loss: 17.2303, val_MinusLogProbMetric: 17.2303

Epoch 937: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.0435 - MinusLogProbMetric: 17.0435 - val_loss: 17.2303 - val_MinusLogProbMetric: 17.2303 - lr: 8.3333e-05 - 58s/epoch - 295ms/step
Epoch 938/1000
2023-10-10 08:50:18.088 
Epoch 938/1000 
	 loss: 17.4302, MinusLogProbMetric: 17.4302, val_loss: 17.2715, val_MinusLogProbMetric: 17.2715

Epoch 938: val_loss did not improve from 17.21378
196/196 - 58s - loss: 17.4302 - MinusLogProbMetric: 17.4302 - val_loss: 17.2715 - val_MinusLogProbMetric: 17.2715 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 939/1000
2023-10-10 08:51:18.054 
Epoch 939/1000 
	 loss: 17.0522, MinusLogProbMetric: 17.0522, val_loss: 17.2201, val_MinusLogProbMetric: 17.2201

Epoch 939: val_loss did not improve from 17.21378
196/196 - 60s - loss: 17.0522 - MinusLogProbMetric: 17.0522 - val_loss: 17.2201 - val_MinusLogProbMetric: 17.2201 - lr: 8.3333e-05 - 60s/epoch - 306ms/step
Epoch 940/1000
2023-10-10 08:52:16.940 
Epoch 940/1000 
	 loss: 17.0420, MinusLogProbMetric: 17.0420, val_loss: 17.2513, val_MinusLogProbMetric: 17.2513

Epoch 940: val_loss did not improve from 17.21378
196/196 - 59s - loss: 17.0420 - MinusLogProbMetric: 17.0420 - val_loss: 17.2513 - val_MinusLogProbMetric: 17.2513 - lr: 8.3333e-05 - 59s/epoch - 300ms/step
Epoch 941/1000
2023-10-10 08:53:16.845 
Epoch 941/1000 
	 loss: 17.0469, MinusLogProbMetric: 17.0469, val_loss: 17.2379, val_MinusLogProbMetric: 17.2379

Epoch 941: val_loss did not improve from 17.21378
196/196 - 60s - loss: 17.0469 - MinusLogProbMetric: 17.0469 - val_loss: 17.2379 - val_MinusLogProbMetric: 17.2379 - lr: 8.3333e-05 - 60s/epoch - 306ms/step
Epoch 942/1000
2023-10-10 08:54:15.387 
Epoch 942/1000 
	 loss: 17.0422, MinusLogProbMetric: 17.0422, val_loss: 17.2510, val_MinusLogProbMetric: 17.2510

Epoch 942: val_loss did not improve from 17.21378
196/196 - 59s - loss: 17.0422 - MinusLogProbMetric: 17.0422 - val_loss: 17.2510 - val_MinusLogProbMetric: 17.2510 - lr: 8.3333e-05 - 59s/epoch - 299ms/step
Epoch 943/1000
2023-10-10 08:55:12.541 
Epoch 943/1000 
	 loss: 17.0388, MinusLogProbMetric: 17.0388, val_loss: 17.2649, val_MinusLogProbMetric: 17.2649

Epoch 943: val_loss did not improve from 17.21378
196/196 - 57s - loss: 17.0388 - MinusLogProbMetric: 17.0388 - val_loss: 17.2649 - val_MinusLogProbMetric: 17.2649 - lr: 8.3333e-05 - 57s/epoch - 292ms/step
Epoch 944/1000
2023-10-10 08:56:11.302 
Epoch 944/1000 
	 loss: 17.0316, MinusLogProbMetric: 17.0316, val_loss: 17.2300, val_MinusLogProbMetric: 17.2300

Epoch 944: val_loss did not improve from 17.21378
196/196 - 59s - loss: 17.0316 - MinusLogProbMetric: 17.0316 - val_loss: 17.2300 - val_MinusLogProbMetric: 17.2300 - lr: 8.3333e-05 - 59s/epoch - 300ms/step
Epoch 945/1000
2023-10-10 08:57:10.944 
Epoch 945/1000 
	 loss: 17.0528, MinusLogProbMetric: 17.0528, val_loss: 17.2567, val_MinusLogProbMetric: 17.2567

Epoch 945: val_loss did not improve from 17.21378
196/196 - 60s - loss: 17.0528 - MinusLogProbMetric: 17.0528 - val_loss: 17.2567 - val_MinusLogProbMetric: 17.2567 - lr: 8.3333e-05 - 60s/epoch - 304ms/step
Epoch 946/1000
2023-10-10 08:58:09.960 
Epoch 946/1000 
	 loss: 17.0414, MinusLogProbMetric: 17.0414, val_loss: 17.2051, val_MinusLogProbMetric: 17.2051

Epoch 946: val_loss improved from 17.21378 to 17.20510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 60s - loss: 17.0414 - MinusLogProbMetric: 17.0414 - val_loss: 17.2051 - val_MinusLogProbMetric: 17.2051 - lr: 8.3333e-05 - 60s/epoch - 307ms/step
Epoch 947/1000
2023-10-10 08:59:10.272 
Epoch 947/1000 
	 loss: 17.0318, MinusLogProbMetric: 17.0318, val_loss: 17.2394, val_MinusLogProbMetric: 17.2394

Epoch 947: val_loss did not improve from 17.20510
196/196 - 59s - loss: 17.0318 - MinusLogProbMetric: 17.0318 - val_loss: 17.2394 - val_MinusLogProbMetric: 17.2394 - lr: 8.3333e-05 - 59s/epoch - 302ms/step
Epoch 948/1000
2023-10-10 09:00:09.308 
Epoch 948/1000 
	 loss: 17.0397, MinusLogProbMetric: 17.0397, val_loss: 17.2856, val_MinusLogProbMetric: 17.2856

Epoch 948: val_loss did not improve from 17.20510
196/196 - 59s - loss: 17.0397 - MinusLogProbMetric: 17.0397 - val_loss: 17.2856 - val_MinusLogProbMetric: 17.2856 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 949/1000
2023-10-10 09:01:10.745 
Epoch 949/1000 
	 loss: 17.0355, MinusLogProbMetric: 17.0355, val_loss: 17.2253, val_MinusLogProbMetric: 17.2253

Epoch 949: val_loss did not improve from 17.20510
196/196 - 61s - loss: 17.0355 - MinusLogProbMetric: 17.0355 - val_loss: 17.2253 - val_MinusLogProbMetric: 17.2253 - lr: 8.3333e-05 - 61s/epoch - 313ms/step
Epoch 950/1000
2023-10-10 09:02:15.829 
Epoch 950/1000 
	 loss: 17.0324, MinusLogProbMetric: 17.0324, val_loss: 17.2401, val_MinusLogProbMetric: 17.2401

Epoch 950: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0324 - MinusLogProbMetric: 17.0324 - val_loss: 17.2401 - val_MinusLogProbMetric: 17.2401 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 951/1000
2023-10-10 09:03:21.119 
Epoch 951/1000 
	 loss: 17.0402, MinusLogProbMetric: 17.0402, val_loss: 17.2325, val_MinusLogProbMetric: 17.2325

Epoch 951: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0402 - MinusLogProbMetric: 17.0402 - val_loss: 17.2325 - val_MinusLogProbMetric: 17.2325 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 952/1000
2023-10-10 09:04:25.959 
Epoch 952/1000 
	 loss: 17.0301, MinusLogProbMetric: 17.0301, val_loss: 17.2292, val_MinusLogProbMetric: 17.2292

Epoch 952: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0301 - MinusLogProbMetric: 17.0301 - val_loss: 17.2292 - val_MinusLogProbMetric: 17.2292 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 953/1000
2023-10-10 09:05:31.273 
Epoch 953/1000 
	 loss: 17.0448, MinusLogProbMetric: 17.0448, val_loss: 17.2565, val_MinusLogProbMetric: 17.2565

Epoch 953: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0448 - MinusLogProbMetric: 17.0448 - val_loss: 17.2565 - val_MinusLogProbMetric: 17.2565 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 954/1000
2023-10-10 09:06:36.243 
Epoch 954/1000 
	 loss: 17.0497, MinusLogProbMetric: 17.0497, val_loss: 17.2820, val_MinusLogProbMetric: 17.2820

Epoch 954: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0497 - MinusLogProbMetric: 17.0497 - val_loss: 17.2820 - val_MinusLogProbMetric: 17.2820 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 955/1000
2023-10-10 09:07:46.420 
Epoch 955/1000 
	 loss: 17.0417, MinusLogProbMetric: 17.0417, val_loss: 17.2478, val_MinusLogProbMetric: 17.2478

Epoch 955: val_loss did not improve from 17.20510
196/196 - 70s - loss: 17.0417 - MinusLogProbMetric: 17.0417 - val_loss: 17.2478 - val_MinusLogProbMetric: 17.2478 - lr: 8.3333e-05 - 70s/epoch - 358ms/step
Epoch 956/1000
2023-10-10 09:08:52.626 
Epoch 956/1000 
	 loss: 17.0580, MinusLogProbMetric: 17.0580, val_loss: 17.2239, val_MinusLogProbMetric: 17.2239

Epoch 956: val_loss did not improve from 17.20510
196/196 - 66s - loss: 17.0580 - MinusLogProbMetric: 17.0580 - val_loss: 17.2239 - val_MinusLogProbMetric: 17.2239 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 957/1000
2023-10-10 09:09:58.221 
Epoch 957/1000 
	 loss: 17.0331, MinusLogProbMetric: 17.0331, val_loss: 17.2288, val_MinusLogProbMetric: 17.2288

Epoch 957: val_loss did not improve from 17.20510
196/196 - 66s - loss: 17.0331 - MinusLogProbMetric: 17.0331 - val_loss: 17.2288 - val_MinusLogProbMetric: 17.2288 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 958/1000
2023-10-10 09:11:02.738 
Epoch 958/1000 
	 loss: 17.0429, MinusLogProbMetric: 17.0429, val_loss: 17.2468, val_MinusLogProbMetric: 17.2468

Epoch 958: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0429 - MinusLogProbMetric: 17.0429 - val_loss: 17.2468 - val_MinusLogProbMetric: 17.2468 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 959/1000
2023-10-10 09:12:07.673 
Epoch 959/1000 
	 loss: 17.0433, MinusLogProbMetric: 17.0433, val_loss: 17.2125, val_MinusLogProbMetric: 17.2125

Epoch 959: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0433 - MinusLogProbMetric: 17.0433 - val_loss: 17.2125 - val_MinusLogProbMetric: 17.2125 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 960/1000
2023-10-10 09:13:12.724 
Epoch 960/1000 
	 loss: 17.0362, MinusLogProbMetric: 17.0362, val_loss: 17.2675, val_MinusLogProbMetric: 17.2675

Epoch 960: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0362 - MinusLogProbMetric: 17.0362 - val_loss: 17.2675 - val_MinusLogProbMetric: 17.2675 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 961/1000
2023-10-10 09:14:19.703 
Epoch 961/1000 
	 loss: 17.0427, MinusLogProbMetric: 17.0427, val_loss: 17.2316, val_MinusLogProbMetric: 17.2316

Epoch 961: val_loss did not improve from 17.20510
196/196 - 67s - loss: 17.0427 - MinusLogProbMetric: 17.0427 - val_loss: 17.2316 - val_MinusLogProbMetric: 17.2316 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 962/1000
2023-10-10 09:15:25.853 
Epoch 962/1000 
	 loss: 17.0357, MinusLogProbMetric: 17.0357, val_loss: 17.2249, val_MinusLogProbMetric: 17.2249

Epoch 962: val_loss did not improve from 17.20510
196/196 - 66s - loss: 17.0357 - MinusLogProbMetric: 17.0357 - val_loss: 17.2249 - val_MinusLogProbMetric: 17.2249 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 963/1000
2023-10-10 09:16:30.556 
Epoch 963/1000 
	 loss: 17.0594, MinusLogProbMetric: 17.0594, val_loss: 17.2248, val_MinusLogProbMetric: 17.2248

Epoch 963: val_loss did not improve from 17.20510
196/196 - 65s - loss: 17.0594 - MinusLogProbMetric: 17.0594 - val_loss: 17.2248 - val_MinusLogProbMetric: 17.2248 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 964/1000
2023-10-10 09:17:36.512 
Epoch 964/1000 
	 loss: 17.0401, MinusLogProbMetric: 17.0401, val_loss: 17.2241, val_MinusLogProbMetric: 17.2241

Epoch 964: val_loss did not improve from 17.20510
196/196 - 66s - loss: 17.0401 - MinusLogProbMetric: 17.0401 - val_loss: 17.2241 - val_MinusLogProbMetric: 17.2241 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 965/1000
2023-10-10 09:18:45.005 
Epoch 965/1000 
	 loss: 17.0290, MinusLogProbMetric: 17.0290, val_loss: 17.1972, val_MinusLogProbMetric: 17.1972

Epoch 965: val_loss improved from 17.20510 to 17.19723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_247/weights/best_weights.h5
196/196 - 70s - loss: 17.0290 - MinusLogProbMetric: 17.0290 - val_loss: 17.1972 - val_MinusLogProbMetric: 17.1972 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 966/1000
2023-10-10 09:19:54.573 
Epoch 966/1000 
	 loss: 17.0365, MinusLogProbMetric: 17.0365, val_loss: 17.3371, val_MinusLogProbMetric: 17.3371

Epoch 966: val_loss did not improve from 17.19723
196/196 - 68s - loss: 17.0365 - MinusLogProbMetric: 17.0365 - val_loss: 17.3371 - val_MinusLogProbMetric: 17.3371 - lr: 8.3333e-05 - 68s/epoch - 349ms/step
Epoch 967/1000
2023-10-10 09:21:00.430 
Epoch 967/1000 
	 loss: 17.1891, MinusLogProbMetric: 17.1891, val_loss: 17.2220, val_MinusLogProbMetric: 17.2220

Epoch 967: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.1891 - MinusLogProbMetric: 17.1891 - val_loss: 17.2220 - val_MinusLogProbMetric: 17.2220 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 968/1000
2023-10-10 09:22:05.177 
Epoch 968/1000 
	 loss: 17.0348, MinusLogProbMetric: 17.0348, val_loss: 17.2595, val_MinusLogProbMetric: 17.2595

Epoch 968: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0348 - MinusLogProbMetric: 17.0348 - val_loss: 17.2595 - val_MinusLogProbMetric: 17.2595 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 969/1000
2023-10-10 09:23:09.733 
Epoch 969/1000 
	 loss: 17.0316, MinusLogProbMetric: 17.0316, val_loss: 17.2359, val_MinusLogProbMetric: 17.2359

Epoch 969: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0316 - MinusLogProbMetric: 17.0316 - val_loss: 17.2359 - val_MinusLogProbMetric: 17.2359 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 970/1000
2023-10-10 09:24:14.299 
Epoch 970/1000 
	 loss: 17.0378, MinusLogProbMetric: 17.0378, val_loss: 17.2314, val_MinusLogProbMetric: 17.2314

Epoch 970: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0378 - MinusLogProbMetric: 17.0378 - val_loss: 17.2314 - val_MinusLogProbMetric: 17.2314 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 971/1000
2023-10-10 09:25:19.119 
Epoch 971/1000 
	 loss: 17.0381, MinusLogProbMetric: 17.0381, val_loss: 17.2574, val_MinusLogProbMetric: 17.2574

Epoch 971: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0381 - MinusLogProbMetric: 17.0381 - val_loss: 17.2574 - val_MinusLogProbMetric: 17.2574 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 972/1000
2023-10-10 09:26:25.609 
Epoch 972/1000 
	 loss: 17.0367, MinusLogProbMetric: 17.0367, val_loss: 17.2346, val_MinusLogProbMetric: 17.2346

Epoch 972: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0367 - MinusLogProbMetric: 17.0367 - val_loss: 17.2346 - val_MinusLogProbMetric: 17.2346 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 973/1000
2023-10-10 09:27:31.442 
Epoch 973/1000 
	 loss: 17.0329, MinusLogProbMetric: 17.0329, val_loss: 17.2124, val_MinusLogProbMetric: 17.2124

Epoch 973: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0329 - MinusLogProbMetric: 17.0329 - val_loss: 17.2124 - val_MinusLogProbMetric: 17.2124 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 974/1000
2023-10-10 09:28:35.712 
Epoch 974/1000 
	 loss: 17.0403, MinusLogProbMetric: 17.0403, val_loss: 17.2468, val_MinusLogProbMetric: 17.2468

Epoch 974: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0403 - MinusLogProbMetric: 17.0403 - val_loss: 17.2468 - val_MinusLogProbMetric: 17.2468 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 975/1000
2023-10-10 09:29:40.661 
Epoch 975/1000 
	 loss: 17.0312, MinusLogProbMetric: 17.0312, val_loss: 17.2645, val_MinusLogProbMetric: 17.2645

Epoch 975: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0312 - MinusLogProbMetric: 17.0312 - val_loss: 17.2645 - val_MinusLogProbMetric: 17.2645 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 976/1000
2023-10-10 09:30:49.667 
Epoch 976/1000 
	 loss: 17.0338, MinusLogProbMetric: 17.0338, val_loss: 17.2347, val_MinusLogProbMetric: 17.2347

Epoch 976: val_loss did not improve from 17.19723
196/196 - 69s - loss: 17.0338 - MinusLogProbMetric: 17.0338 - val_loss: 17.2347 - val_MinusLogProbMetric: 17.2347 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 977/1000
2023-10-10 09:32:02.378 
Epoch 977/1000 
	 loss: 17.0252, MinusLogProbMetric: 17.0252, val_loss: 17.2786, val_MinusLogProbMetric: 17.2786

Epoch 977: val_loss did not improve from 17.19723
196/196 - 73s - loss: 17.0252 - MinusLogProbMetric: 17.0252 - val_loss: 17.2786 - val_MinusLogProbMetric: 17.2786 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 978/1000
2023-10-10 09:33:08.531 
Epoch 978/1000 
	 loss: 17.0223, MinusLogProbMetric: 17.0223, val_loss: 17.2070, val_MinusLogProbMetric: 17.2070

Epoch 978: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0223 - MinusLogProbMetric: 17.0223 - val_loss: 17.2070 - val_MinusLogProbMetric: 17.2070 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 979/1000
2023-10-10 09:34:14.198 
Epoch 979/1000 
	 loss: 17.0304, MinusLogProbMetric: 17.0304, val_loss: 17.2467, val_MinusLogProbMetric: 17.2467

Epoch 979: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0304 - MinusLogProbMetric: 17.0304 - val_loss: 17.2467 - val_MinusLogProbMetric: 17.2467 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 980/1000
2023-10-10 09:35:19.056 
Epoch 980/1000 
	 loss: 17.0225, MinusLogProbMetric: 17.0225, val_loss: 17.2174, val_MinusLogProbMetric: 17.2174

Epoch 980: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0225 - MinusLogProbMetric: 17.0225 - val_loss: 17.2174 - val_MinusLogProbMetric: 17.2174 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 981/1000
2023-10-10 09:36:23.720 
Epoch 981/1000 
	 loss: 17.0352, MinusLogProbMetric: 17.0352, val_loss: 17.2467, val_MinusLogProbMetric: 17.2467

Epoch 981: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0352 - MinusLogProbMetric: 17.0352 - val_loss: 17.2467 - val_MinusLogProbMetric: 17.2467 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 982/1000
2023-10-10 09:37:30.434 
Epoch 982/1000 
	 loss: 17.0890, MinusLogProbMetric: 17.0890, val_loss: 17.2016, val_MinusLogProbMetric: 17.2016

Epoch 982: val_loss did not improve from 17.19723
196/196 - 67s - loss: 17.0890 - MinusLogProbMetric: 17.0890 - val_loss: 17.2016 - val_MinusLogProbMetric: 17.2016 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 983/1000
2023-10-10 09:38:34.853 
Epoch 983/1000 
	 loss: 17.0245, MinusLogProbMetric: 17.0245, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 983: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0245 - MinusLogProbMetric: 17.0245 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 984/1000
2023-10-10 09:39:41.995 
Epoch 984/1000 
	 loss: 17.0327, MinusLogProbMetric: 17.0327, val_loss: 17.2156, val_MinusLogProbMetric: 17.2156

Epoch 984: val_loss did not improve from 17.19723
196/196 - 67s - loss: 17.0327 - MinusLogProbMetric: 17.0327 - val_loss: 17.2156 - val_MinusLogProbMetric: 17.2156 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 985/1000
2023-10-10 09:40:46.102 
Epoch 985/1000 
	 loss: 17.0274, MinusLogProbMetric: 17.0274, val_loss: 17.2391, val_MinusLogProbMetric: 17.2391

Epoch 985: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0274 - MinusLogProbMetric: 17.0274 - val_loss: 17.2391 - val_MinusLogProbMetric: 17.2391 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 986/1000
2023-10-10 09:41:51.208 
Epoch 986/1000 
	 loss: 17.0253, MinusLogProbMetric: 17.0253, val_loss: 17.2355, val_MinusLogProbMetric: 17.2355

Epoch 986: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0253 - MinusLogProbMetric: 17.0253 - val_loss: 17.2355 - val_MinusLogProbMetric: 17.2355 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 987/1000
2023-10-10 09:42:55.374 
Epoch 987/1000 
	 loss: 17.0395, MinusLogProbMetric: 17.0395, val_loss: 17.2293, val_MinusLogProbMetric: 17.2293

Epoch 987: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0395 - MinusLogProbMetric: 17.0395 - val_loss: 17.2293 - val_MinusLogProbMetric: 17.2293 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 988/1000
2023-10-10 09:43:59.952 
Epoch 988/1000 
	 loss: 17.0250, MinusLogProbMetric: 17.0250, val_loss: 17.3750, val_MinusLogProbMetric: 17.3750

Epoch 988: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0250 - MinusLogProbMetric: 17.0250 - val_loss: 17.3750 - val_MinusLogProbMetric: 17.3750 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 989/1000
2023-10-10 09:45:05.438 
Epoch 989/1000 
	 loss: 17.0353, MinusLogProbMetric: 17.0353, val_loss: 17.2609, val_MinusLogProbMetric: 17.2609

Epoch 989: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0353 - MinusLogProbMetric: 17.0353 - val_loss: 17.2609 - val_MinusLogProbMetric: 17.2609 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 990/1000
2023-10-10 09:46:10.722 
Epoch 990/1000 
	 loss: 17.0399, MinusLogProbMetric: 17.0399, val_loss: 17.2396, val_MinusLogProbMetric: 17.2396

Epoch 990: val_loss did not improve from 17.19723
196/196 - 65s - loss: 17.0399 - MinusLogProbMetric: 17.0399 - val_loss: 17.2396 - val_MinusLogProbMetric: 17.2396 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 991/1000
2023-10-10 09:47:18.280 
Epoch 991/1000 
	 loss: 17.0250, MinusLogProbMetric: 17.0250, val_loss: 17.2627, val_MinusLogProbMetric: 17.2627

Epoch 991: val_loss did not improve from 17.19723
196/196 - 68s - loss: 17.0250 - MinusLogProbMetric: 17.0250 - val_loss: 17.2627 - val_MinusLogProbMetric: 17.2627 - lr: 8.3333e-05 - 68s/epoch - 345ms/step
Epoch 992/1000
2023-10-10 09:48:24.222 
Epoch 992/1000 
	 loss: 17.0352, MinusLogProbMetric: 17.0352, val_loss: 17.2170, val_MinusLogProbMetric: 17.2170

Epoch 992: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0352 - MinusLogProbMetric: 17.0352 - val_loss: 17.2170 - val_MinusLogProbMetric: 17.2170 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 993/1000
2023-10-10 09:49:37.959 
Epoch 993/1000 
	 loss: 17.0445, MinusLogProbMetric: 17.0445, val_loss: 17.2360, val_MinusLogProbMetric: 17.2360

Epoch 993: val_loss did not improve from 17.19723
196/196 - 74s - loss: 17.0445 - MinusLogProbMetric: 17.0445 - val_loss: 17.2360 - val_MinusLogProbMetric: 17.2360 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 994/1000
2023-10-10 09:50:45.812 
Epoch 994/1000 
	 loss: 17.0240, MinusLogProbMetric: 17.0240, val_loss: 17.2209, val_MinusLogProbMetric: 17.2209

Epoch 994: val_loss did not improve from 17.19723
196/196 - 68s - loss: 17.0240 - MinusLogProbMetric: 17.0240 - val_loss: 17.2209 - val_MinusLogProbMetric: 17.2209 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 995/1000
2023-10-10 09:51:52.250 
Epoch 995/1000 
	 loss: 17.0553, MinusLogProbMetric: 17.0553, val_loss: 17.2426, val_MinusLogProbMetric: 17.2426

Epoch 995: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0553 - MinusLogProbMetric: 17.0553 - val_loss: 17.2426 - val_MinusLogProbMetric: 17.2426 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 996/1000
2023-10-10 09:52:57.771 
Epoch 996/1000 
	 loss: 17.0234, MinusLogProbMetric: 17.0234, val_loss: 17.2462, val_MinusLogProbMetric: 17.2462

Epoch 996: val_loss did not improve from 17.19723
196/196 - 66s - loss: 17.0234 - MinusLogProbMetric: 17.0234 - val_loss: 17.2462 - val_MinusLogProbMetric: 17.2462 - lr: 8.3333e-05 - 66s/epoch - 334ms/step
Epoch 997/1000
2023-10-10 09:54:01.716 
Epoch 997/1000 
	 loss: 17.0478, MinusLogProbMetric: 17.0478, val_loss: 17.2378, val_MinusLogProbMetric: 17.2378

Epoch 997: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0478 - MinusLogProbMetric: 17.0478 - val_loss: 17.2378 - val_MinusLogProbMetric: 17.2378 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 998/1000
2023-10-10 09:55:05.805 
Epoch 998/1000 
	 loss: 17.0231, MinusLogProbMetric: 17.0231, val_loss: 17.2140, val_MinusLogProbMetric: 17.2140

Epoch 998: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0231 - MinusLogProbMetric: 17.0231 - val_loss: 17.2140 - val_MinusLogProbMetric: 17.2140 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 999/1000
2023-10-10 09:56:09.751 
Epoch 999/1000 
	 loss: 17.0270, MinusLogProbMetric: 17.0270, val_loss: 17.2407, val_MinusLogProbMetric: 17.2407

Epoch 999: val_loss did not improve from 17.19723
196/196 - 64s - loss: 17.0270 - MinusLogProbMetric: 17.0270 - val_loss: 17.2407 - val_MinusLogProbMetric: 17.2407 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 1000/1000
2023-10-10 09:57:18.188 
Epoch 1000/1000 
	 loss: 17.0219, MinusLogProbMetric: 17.0219, val_loss: 17.2439, val_MinusLogProbMetric: 17.2439

Epoch 1000: val_loss did not improve from 17.19723
196/196 - 68s - loss: 17.0219 - MinusLogProbMetric: 17.0219 - val_loss: 17.2439 - val_MinusLogProbMetric: 17.2439 - lr: 8.3333e-05 - 68s/epoch - 349ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 28.980866762809455 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 17.098125267075375 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 12.177345715928823 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 12.802087512798607 seconds.
Training succeeded with seed 0.
Model trained in 59998.95 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 73.15 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 73.63 s.
===========
Run 247/720 done in 60259.67 s.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

===========
Generating train data for run 269.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f3cb0576f20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3cc4227ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3cc4227ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3cb06826e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cb05a63e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cb05a6950>, <keras.callbacks.ModelCheckpoint object at 0x7f3cb05a6a10>, <keras.callbacks.EarlyStopping object at 0x7f3cb05a6c80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cb05a6cb0>, <keras.callbacks.TerminateOnNaN object at 0x7f3cb05a68f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 09:58:40.128388
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:00:42.068 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 122s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 122s/epoch - 622ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 269.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f3c8d556d70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c8d0494b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c8d0494b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c8cc1db40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c8cc4cf10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c8cc4d480>, <keras.callbacks.ModelCheckpoint object at 0x7f3c8cc4d540>, <keras.callbacks.EarlyStopping object at 0x7f3c8cc4d7b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c8cc4d7e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c8cc4d420>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:00:49.433028
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:02:54.007 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 124s/epoch - 634ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 269.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f3af434bf70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c8cc584f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c8cc584f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3b20548a00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3b20549cc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3b2054a230>, <keras.callbacks.ModelCheckpoint object at 0x7f3b2054a2f0>, <keras.callbacks.EarlyStopping object at 0x7f3b2054a560>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3b2054a590>, <keras.callbacks.TerminateOnNaN object at 0x7f3b2054a1d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:03:03.589212
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:05:27.080 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 143s/epoch - 730ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 269.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f4358cc7400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3b903cacb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3b903cacb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3b2028d630>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ac816bc70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ac816a0e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3ac81699f0>, <keras.callbacks.EarlyStopping object at 0x7f3ac816ace0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ac816aa40>, <keras.callbacks.TerminateOnNaN object at 0x7f3ac816bc10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:05:35.503531
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 196007 calls to <function Model.make_train_function.<locals>.train_function at 0x7f3cb02d5000> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:08:06.623 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 151s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 151s/epoch - 770ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 269.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f3a74f4fa60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3abd6aa1d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3abd6aa1d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ad101b670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c8c31b130>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c8c31b6a0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c8c31b760>, <keras.callbacks.EarlyStopping object at 0x7f3c8c31b9d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c8c31ba00>, <keras.callbacks.TerminateOnNaN object at 0x7f3c8c31b640>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:08:16.122418
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 196009 calls to <function Model.make_train_function.<locals>.train_function at 0x7f3adc401510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:10:50.874 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 155s/epoch - 788ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 269.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f3cb0213d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ad112fd00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ad112fd00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3b206e5f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3b106627d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3b10662d40>, <keras.callbacks.ModelCheckpoint object at 0x7f3b10662e00>, <keras.callbacks.EarlyStopping object at 0x7f3b10663070>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3b106630a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3b10662ce0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:10:59.285028
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:13:29.324 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 150s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 150s/epoch - 764ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 269.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f3abdd47f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a782dace0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a782dace0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f43582fa4a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ac90bc760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ac90bd180>, <keras.callbacks.ModelCheckpoint object at 0x7f3ac90bd030>, <keras.callbacks.EarlyStopping object at 0x7f3ac90be6e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ac90bf010>, <keras.callbacks.TerminateOnNaN object at 0x7f3ac90bcf40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:13:37.529731
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:15:59.588 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 142s/epoch - 724ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 269.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f3b082151b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3bdc791390>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3bdc791390>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a74900340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3b082bb820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3b082bbd90>, <keras.callbacks.ModelCheckpoint object at 0x7f3b082bbe50>, <keras.callbacks.EarlyStopping object at 0x7f3b082bbfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3b082bbee0>, <keras.callbacks.TerminateOnNaN object at 0x7f3b082bbf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:16:06.774217
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:18:29.615 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 143s/epoch - 728ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 269.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f3a745bded0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3abc16fdf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3abc16fdf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3b902d7100>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ae44b7d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ae44b78e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3ae44b76a0>, <keras.callbacks.EarlyStopping object at 0x7f3ae44b6bc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ae44b7460>, <keras.callbacks.TerminateOnNaN object at 0x7f3ae44b79d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:18:38.245358
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:21:18.654 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 160s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 160s/epoch - 818ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 269.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_131"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_132 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f3bfc11f700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f433da2dea0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f433da2dea0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3adc758c10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f433d9d6bf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f433d9d7160>, <keras.callbacks.ModelCheckpoint object at 0x7f433d9d7220>, <keras.callbacks.EarlyStopping object at 0x7f433d9d7490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f433d9d74c0>, <keras.callbacks.TerminateOnNaN object at 0x7f433d9d7100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:21:27.144360
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:24:00.375 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 153s/epoch - 781ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 269.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_142"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_143 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f3ac9a43070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3bdc427a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3bdc427a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ac9f35810>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a788a8c10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a788a9180>, <keras.callbacks.ModelCheckpoint object at 0x7f3a788a9240>, <keras.callbacks.EarlyStopping object at 0x7f3a788a94b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a788a94e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3a788a9120>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-10 10:24:08.818921
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:26:33.769 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 145s/epoch - 739ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 269/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

===========
Generating train data for run 309.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_153"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_154 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f3ac86aeec0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ac86ee6e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ac86ee6e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a79c056c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3b47e269b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3b47e26f20>, <keras.callbacks.ModelCheckpoint object at 0x7f3b47e26fe0>, <keras.callbacks.EarlyStopping object at 0x7f3b47e27250>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3b47e27280>, <keras.callbacks.TerminateOnNaN object at 0x7f3b47e26ec0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_309/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-10-10 10:26:43.940595
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 75: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:29:23.296 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1526.2714, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 1526.2714 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 159s/epoch - 812ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 309.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_164"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_165 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f3a24173d30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a1da22e00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a1da22e00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a04645f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c775e2320>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c775e2890>, <keras.callbacks.ModelCheckpoint object at 0x7f3c775e2950>, <keras.callbacks.EarlyStopping object at 0x7f3c775e2bc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c775e2bf0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c775e2830>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_309/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-10-10 10:29:31.665229
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-10-10 10:32:46.173 
Epoch 1/1000 
	 loss: 1010.3918, MinusLogProbMetric: 1010.3918, val_loss: 389.1003, val_MinusLogProbMetric: 389.1003

Epoch 1: val_loss improved from inf to 389.10031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 195s - loss: 1010.3918 - MinusLogProbMetric: 1010.3918 - val_loss: 389.1003 - val_MinusLogProbMetric: 389.1003 - lr: 3.3333e-04 - 195s/epoch - 996ms/step
Epoch 2/1000
2023-10-10 10:33:41.016 
Epoch 2/1000 
	 loss: 362.4630, MinusLogProbMetric: 362.4630, val_loss: 242.6197, val_MinusLogProbMetric: 242.6197

Epoch 2: val_loss improved from 389.10031 to 242.61969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 362.4630 - MinusLogProbMetric: 362.4630 - val_loss: 242.6197 - val_MinusLogProbMetric: 242.6197 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 3/1000
2023-10-10 10:34:35.717 
Epoch 3/1000 
	 loss: 218.7462, MinusLogProbMetric: 218.7462, val_loss: 183.3880, val_MinusLogProbMetric: 183.3880

Epoch 3: val_loss improved from 242.61969 to 183.38800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 218.7462 - MinusLogProbMetric: 218.7462 - val_loss: 183.3880 - val_MinusLogProbMetric: 183.3880 - lr: 3.3333e-04 - 55s/epoch - 279ms/step
Epoch 4/1000
2023-10-10 10:35:31.454 
Epoch 4/1000 
	 loss: 205.2031, MinusLogProbMetric: 205.2031, val_loss: 317.8603, val_MinusLogProbMetric: 317.8603

Epoch 4: val_loss did not improve from 183.38800
196/196 - 55s - loss: 205.2031 - MinusLogProbMetric: 205.2031 - val_loss: 317.8603 - val_MinusLogProbMetric: 317.8603 - lr: 3.3333e-04 - 55s/epoch - 280ms/step
Epoch 5/1000
2023-10-10 10:36:28.048 
Epoch 5/1000 
	 loss: 263.1856, MinusLogProbMetric: 263.1856, val_loss: 211.3664, val_MinusLogProbMetric: 211.3664

Epoch 5: val_loss did not improve from 183.38800
196/196 - 57s - loss: 263.1856 - MinusLogProbMetric: 263.1856 - val_loss: 211.3664 - val_MinusLogProbMetric: 211.3664 - lr: 3.3333e-04 - 57s/epoch - 289ms/step
Epoch 6/1000
2023-10-10 10:37:23.028 
Epoch 6/1000 
	 loss: 193.9667, MinusLogProbMetric: 193.9667, val_loss: 180.6976, val_MinusLogProbMetric: 180.6976

Epoch 6: val_loss improved from 183.38800 to 180.69760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 193.9667 - MinusLogProbMetric: 193.9667 - val_loss: 180.6976 - val_MinusLogProbMetric: 180.6976 - lr: 3.3333e-04 - 56s/epoch - 285ms/step
Epoch 7/1000
2023-10-10 10:38:18.554 
Epoch 7/1000 
	 loss: 167.5199, MinusLogProbMetric: 167.5199, val_loss: 148.8618, val_MinusLogProbMetric: 148.8618

Epoch 7: val_loss improved from 180.69760 to 148.86182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 167.5199 - MinusLogProbMetric: 167.5199 - val_loss: 148.8618 - val_MinusLogProbMetric: 148.8618 - lr: 3.3333e-04 - 56s/epoch - 283ms/step
Epoch 8/1000
2023-10-10 10:39:13.704 
Epoch 8/1000 
	 loss: 235.9695, MinusLogProbMetric: 235.9695, val_loss: 273.2455, val_MinusLogProbMetric: 273.2455

Epoch 8: val_loss did not improve from 148.86182
196/196 - 54s - loss: 235.9695 - MinusLogProbMetric: 235.9695 - val_loss: 273.2455 - val_MinusLogProbMetric: 273.2455 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 9/1000
2023-10-10 10:40:07.719 
Epoch 9/1000 
	 loss: 228.6720, MinusLogProbMetric: 228.6720, val_loss: 203.6309, val_MinusLogProbMetric: 203.6309

Epoch 9: val_loss did not improve from 148.86182
196/196 - 54s - loss: 228.6720 - MinusLogProbMetric: 228.6720 - val_loss: 203.6309 - val_MinusLogProbMetric: 203.6309 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 10/1000
2023-10-10 10:41:03.827 
Epoch 10/1000 
	 loss: 196.3182, MinusLogProbMetric: 196.3182, val_loss: 195.1934, val_MinusLogProbMetric: 195.1934

Epoch 10: val_loss did not improve from 148.86182
196/196 - 56s - loss: 196.3182 - MinusLogProbMetric: 196.3182 - val_loss: 195.1934 - val_MinusLogProbMetric: 195.1934 - lr: 3.3333e-04 - 56s/epoch - 286ms/step
Epoch 11/1000
2023-10-10 10:41:58.084 
Epoch 11/1000 
	 loss: 178.3229, MinusLogProbMetric: 178.3229, val_loss: 219.8976, val_MinusLogProbMetric: 219.8976

Epoch 11: val_loss did not improve from 148.86182
196/196 - 54s - loss: 178.3229 - MinusLogProbMetric: 178.3229 - val_loss: 219.8976 - val_MinusLogProbMetric: 219.8976 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 12/1000
2023-10-10 10:42:52.866 
Epoch 12/1000 
	 loss: 169.6493, MinusLogProbMetric: 169.6493, val_loss: 157.3141, val_MinusLogProbMetric: 157.3141

Epoch 12: val_loss did not improve from 148.86182
196/196 - 55s - loss: 169.6493 - MinusLogProbMetric: 169.6493 - val_loss: 157.3141 - val_MinusLogProbMetric: 157.3141 - lr: 3.3333e-04 - 55s/epoch - 279ms/step
Epoch 13/1000
2023-10-10 10:43:47.244 
Epoch 13/1000 
	 loss: 240.6128, MinusLogProbMetric: 240.6128, val_loss: 263.8029, val_MinusLogProbMetric: 263.8029

Epoch 13: val_loss did not improve from 148.86182
196/196 - 54s - loss: 240.6128 - MinusLogProbMetric: 240.6128 - val_loss: 263.8029 - val_MinusLogProbMetric: 263.8029 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 14/1000
2023-10-10 10:44:40.492 
Epoch 14/1000 
	 loss: 217.7417, MinusLogProbMetric: 217.7417, val_loss: 185.2203, val_MinusLogProbMetric: 185.2203

Epoch 14: val_loss did not improve from 148.86182
196/196 - 53s - loss: 217.7417 - MinusLogProbMetric: 217.7417 - val_loss: 185.2203 - val_MinusLogProbMetric: 185.2203 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 15/1000
2023-10-10 10:45:33.342 
Epoch 15/1000 
	 loss: 181.8384, MinusLogProbMetric: 181.8384, val_loss: 167.1772, val_MinusLogProbMetric: 167.1772

Epoch 15: val_loss did not improve from 148.86182
196/196 - 53s - loss: 181.8384 - MinusLogProbMetric: 181.8384 - val_loss: 167.1772 - val_MinusLogProbMetric: 167.1772 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 16/1000
2023-10-10 10:46:27.382 
Epoch 16/1000 
	 loss: 164.0182, MinusLogProbMetric: 164.0182, val_loss: 164.0403, val_MinusLogProbMetric: 164.0403

Epoch 16: val_loss did not improve from 148.86182
196/196 - 54s - loss: 164.0182 - MinusLogProbMetric: 164.0182 - val_loss: 164.0403 - val_MinusLogProbMetric: 164.0403 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 17/1000
2023-10-10 10:47:21.796 
Epoch 17/1000 
	 loss: 164.9407, MinusLogProbMetric: 164.9407, val_loss: 149.2642, val_MinusLogProbMetric: 149.2642

Epoch 17: val_loss did not improve from 148.86182
196/196 - 54s - loss: 164.9407 - MinusLogProbMetric: 164.9407 - val_loss: 149.2642 - val_MinusLogProbMetric: 149.2642 - lr: 3.3333e-04 - 54s/epoch - 278ms/step
Epoch 18/1000
2023-10-10 10:48:15.333 
Epoch 18/1000 
	 loss: 168.7121, MinusLogProbMetric: 168.7121, val_loss: 245.2737, val_MinusLogProbMetric: 245.2737

Epoch 18: val_loss did not improve from 148.86182
196/196 - 54s - loss: 168.7121 - MinusLogProbMetric: 168.7121 - val_loss: 245.2737 - val_MinusLogProbMetric: 245.2737 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 19/1000
2023-10-10 10:49:08.760 
Epoch 19/1000 
	 loss: 204.6204, MinusLogProbMetric: 204.6204, val_loss: 177.2765, val_MinusLogProbMetric: 177.2765

Epoch 19: val_loss did not improve from 148.86182
196/196 - 53s - loss: 204.6204 - MinusLogProbMetric: 204.6204 - val_loss: 177.2765 - val_MinusLogProbMetric: 177.2765 - lr: 3.3333e-04 - 53s/epoch - 273ms/step
Epoch 20/1000
2023-10-10 10:50:01.953 
Epoch 20/1000 
	 loss: 162.6261, MinusLogProbMetric: 162.6261, val_loss: 155.8050, val_MinusLogProbMetric: 155.8050

Epoch 20: val_loss did not improve from 148.86182
196/196 - 53s - loss: 162.6261 - MinusLogProbMetric: 162.6261 - val_loss: 155.8050 - val_MinusLogProbMetric: 155.8050 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 21/1000
2023-10-10 10:50:55.449 
Epoch 21/1000 
	 loss: 154.0243, MinusLogProbMetric: 154.0243, val_loss: 166.5795, val_MinusLogProbMetric: 166.5795

Epoch 21: val_loss did not improve from 148.86182
196/196 - 53s - loss: 154.0243 - MinusLogProbMetric: 154.0243 - val_loss: 166.5795 - val_MinusLogProbMetric: 166.5795 - lr: 3.3333e-04 - 53s/epoch - 273ms/step
Epoch 22/1000
2023-10-10 10:51:48.621 
Epoch 22/1000 
	 loss: 148.7031, MinusLogProbMetric: 148.7031, val_loss: 135.4666, val_MinusLogProbMetric: 135.4666

Epoch 22: val_loss improved from 148.86182 to 135.46657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 148.7031 - MinusLogProbMetric: 148.7031 - val_loss: 135.4666 - val_MinusLogProbMetric: 135.4666 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 23/1000
2023-10-10 10:52:42.757 
Epoch 23/1000 
	 loss: 132.4823, MinusLogProbMetric: 132.4823, val_loss: 130.8323, val_MinusLogProbMetric: 130.8323

Epoch 23: val_loss improved from 135.46657 to 130.83229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 132.4823 - MinusLogProbMetric: 132.4823 - val_loss: 130.8323 - val_MinusLogProbMetric: 130.8323 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 24/1000
2023-10-10 10:53:36.732 
Epoch 24/1000 
	 loss: 158.3689, MinusLogProbMetric: 158.3689, val_loss: 144.2986, val_MinusLogProbMetric: 144.2986

Epoch 24: val_loss did not improve from 130.83229
196/196 - 53s - loss: 158.3689 - MinusLogProbMetric: 158.3689 - val_loss: 144.2986 - val_MinusLogProbMetric: 144.2986 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 25/1000
2023-10-10 10:54:30.089 
Epoch 25/1000 
	 loss: 134.9471, MinusLogProbMetric: 134.9471, val_loss: 129.2942, val_MinusLogProbMetric: 129.2942

Epoch 25: val_loss improved from 130.83229 to 129.29424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 134.9471 - MinusLogProbMetric: 134.9471 - val_loss: 129.2942 - val_MinusLogProbMetric: 129.2942 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 26/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 154: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 10:55:13.591 
Epoch 26/1000 
	 loss: nan, MinusLogProbMetric: 236.1880, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 26: val_loss did not improve from 129.29424
196/196 - 43s - loss: nan - MinusLogProbMetric: 236.1880 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 43s/epoch - 218ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 309.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_175"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_176 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f3a1c07f460>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3abd814eb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3abd814eb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a1ca98580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a1cb95e40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a1cb963b0>, <keras.callbacks.ModelCheckpoint object at 0x7f3a1cb96470>, <keras.callbacks.EarlyStopping object at 0x7f3a1cb966e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a1cb96710>, <keras.callbacks.TerminateOnNaN object at 0x7f3a1cb96350>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-10-10 10:55:22.495345
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-10-10 10:58:22.492 
Epoch 1/1000 
	 loss: 157.9066, MinusLogProbMetric: 157.9066, val_loss: 133.8793, val_MinusLogProbMetric: 133.8793

Epoch 1: val_loss improved from inf to 133.87927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 180s - loss: 157.9066 - MinusLogProbMetric: 157.9066 - val_loss: 133.8793 - val_MinusLogProbMetric: 133.8793 - lr: 1.1111e-04 - 180s/epoch - 921ms/step
Epoch 2/1000
2023-10-10 10:59:17.132 
Epoch 2/1000 
	 loss: 129.0596, MinusLogProbMetric: 129.0596, val_loss: 125.7829, val_MinusLogProbMetric: 125.7829

Epoch 2: val_loss improved from 133.87927 to 125.78293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 129.0596 - MinusLogProbMetric: 129.0596 - val_loss: 125.7829 - val_MinusLogProbMetric: 125.7829 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 3/1000
2023-10-10 11:00:11.079 
Epoch 3/1000 
	 loss: 107.9885, MinusLogProbMetric: 107.9885, val_loss: 50.6405, val_MinusLogProbMetric: 50.6405

Epoch 3: val_loss improved from 125.78293 to 50.64045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 107.9885 - MinusLogProbMetric: 107.9885 - val_loss: 50.6405 - val_MinusLogProbMetric: 50.6405 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 4/1000
2023-10-10 11:01:05.418 
Epoch 4/1000 
	 loss: 43.9587, MinusLogProbMetric: 43.9587, val_loss: 39.2365, val_MinusLogProbMetric: 39.2365

Epoch 4: val_loss improved from 50.64045 to 39.23645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 43.9587 - MinusLogProbMetric: 43.9587 - val_loss: 39.2365 - val_MinusLogProbMetric: 39.2365 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 5/1000
2023-10-10 11:01:59.056 
Epoch 5/1000 
	 loss: 36.1837, MinusLogProbMetric: 36.1837, val_loss: 34.7393, val_MinusLogProbMetric: 34.7393

Epoch 5: val_loss improved from 39.23645 to 34.73929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 36.1837 - MinusLogProbMetric: 36.1837 - val_loss: 34.7393 - val_MinusLogProbMetric: 34.7393 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 6/1000
2023-10-10 11:02:54.229 
Epoch 6/1000 
	 loss: 32.5406, MinusLogProbMetric: 32.5406, val_loss: 31.1563, val_MinusLogProbMetric: 31.1563

Epoch 6: val_loss improved from 34.73929 to 31.15627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 32.5406 - MinusLogProbMetric: 32.5406 - val_loss: 31.1563 - val_MinusLogProbMetric: 31.1563 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 7/1000
2023-10-10 11:03:48.535 
Epoch 7/1000 
	 loss: 76.1094, MinusLogProbMetric: 76.1094, val_loss: 72.8134, val_MinusLogProbMetric: 72.8134

Epoch 7: val_loss did not improve from 31.15627
196/196 - 53s - loss: 76.1094 - MinusLogProbMetric: 76.1094 - val_loss: 72.8134 - val_MinusLogProbMetric: 72.8134 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 8/1000
2023-10-10 11:04:42.667 
Epoch 8/1000 
	 loss: 53.9250, MinusLogProbMetric: 53.9250, val_loss: 43.5117, val_MinusLogProbMetric: 43.5117

Epoch 8: val_loss did not improve from 31.15627
196/196 - 54s - loss: 53.9250 - MinusLogProbMetric: 53.9250 - val_loss: 43.5117 - val_MinusLogProbMetric: 43.5117 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 9/1000
2023-10-10 11:05:36.093 
Epoch 9/1000 
	 loss: 39.2983, MinusLogProbMetric: 39.2983, val_loss: 36.3502, val_MinusLogProbMetric: 36.3502

Epoch 9: val_loss did not improve from 31.15627
196/196 - 53s - loss: 39.2983 - MinusLogProbMetric: 39.2983 - val_loss: 36.3502 - val_MinusLogProbMetric: 36.3502 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 10/1000
2023-10-10 11:06:29.501 
Epoch 10/1000 
	 loss: 34.6667, MinusLogProbMetric: 34.6667, val_loss: 32.5424, val_MinusLogProbMetric: 32.5424

Epoch 10: val_loss did not improve from 31.15627
196/196 - 53s - loss: 34.6667 - MinusLogProbMetric: 34.6667 - val_loss: 32.5424 - val_MinusLogProbMetric: 32.5424 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 11/1000
2023-10-10 11:07:23.607 
Epoch 11/1000 
	 loss: 31.5257, MinusLogProbMetric: 31.5257, val_loss: 30.7668, val_MinusLogProbMetric: 30.7668

Epoch 11: val_loss improved from 31.15627 to 30.76683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 31.5257 - MinusLogProbMetric: 31.5257 - val_loss: 30.7668 - val_MinusLogProbMetric: 30.7668 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 12/1000
2023-10-10 11:08:17.749 
Epoch 12/1000 
	 loss: 29.4296, MinusLogProbMetric: 29.4296, val_loss: 28.6882, val_MinusLogProbMetric: 28.6882

Epoch 12: val_loss improved from 30.76683 to 28.68815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 29.4296 - MinusLogProbMetric: 29.4296 - val_loss: 28.6882 - val_MinusLogProbMetric: 28.6882 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 13/1000
2023-10-10 11:09:12.104 
Epoch 13/1000 
	 loss: 28.4717, MinusLogProbMetric: 28.4717, val_loss: 27.3269, val_MinusLogProbMetric: 27.3269

Epoch 13: val_loss improved from 28.68815 to 27.32688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 28.4717 - MinusLogProbMetric: 28.4717 - val_loss: 27.3269 - val_MinusLogProbMetric: 27.3269 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 14/1000
2023-10-10 11:10:06.225 
Epoch 14/1000 
	 loss: 26.7971, MinusLogProbMetric: 26.7971, val_loss: 26.2752, val_MinusLogProbMetric: 26.2752

Epoch 14: val_loss improved from 27.32688 to 26.27518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 26.7971 - MinusLogProbMetric: 26.7971 - val_loss: 26.2752 - val_MinusLogProbMetric: 26.2752 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 15/1000
2023-10-10 11:11:01.822 
Epoch 15/1000 
	 loss: 25.8625, MinusLogProbMetric: 25.8625, val_loss: 25.3614, val_MinusLogProbMetric: 25.3614

Epoch 15: val_loss improved from 26.27518 to 25.36140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 25.8625 - MinusLogProbMetric: 25.8625 - val_loss: 25.3614 - val_MinusLogProbMetric: 25.3614 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 16/1000
2023-10-10 11:11:56.466 
Epoch 16/1000 
	 loss: 25.0096, MinusLogProbMetric: 25.0096, val_loss: 24.8095, val_MinusLogProbMetric: 24.8095

Epoch 16: val_loss improved from 25.36140 to 24.80946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 25.0096 - MinusLogProbMetric: 25.0096 - val_loss: 24.8095 - val_MinusLogProbMetric: 24.8095 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 17/1000
2023-10-10 11:12:50.740 
Epoch 17/1000 
	 loss: 24.4594, MinusLogProbMetric: 24.4594, val_loss: 24.6711, val_MinusLogProbMetric: 24.6711

Epoch 17: val_loss improved from 24.80946 to 24.67107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 24.4594 - MinusLogProbMetric: 24.4594 - val_loss: 24.6711 - val_MinusLogProbMetric: 24.6711 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 18/1000
2023-10-10 11:13:45.269 
Epoch 18/1000 
	 loss: 23.9405, MinusLogProbMetric: 23.9405, val_loss: 23.6992, val_MinusLogProbMetric: 23.6992

Epoch 18: val_loss improved from 24.67107 to 23.69923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 23.9405 - MinusLogProbMetric: 23.9405 - val_loss: 23.6992 - val_MinusLogProbMetric: 23.6992 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 19/1000
2023-10-10 11:14:40.332 
Epoch 19/1000 
	 loss: 23.4893, MinusLogProbMetric: 23.4893, val_loss: 24.1751, val_MinusLogProbMetric: 24.1751

Epoch 19: val_loss did not improve from 23.69923
196/196 - 54s - loss: 23.4893 - MinusLogProbMetric: 23.4893 - val_loss: 24.1751 - val_MinusLogProbMetric: 24.1751 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 20/1000
2023-10-10 11:15:34.460 
Epoch 20/1000 
	 loss: 23.1177, MinusLogProbMetric: 23.1177, val_loss: 23.1222, val_MinusLogProbMetric: 23.1222

Epoch 20: val_loss improved from 23.69923 to 23.12224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 23.1177 - MinusLogProbMetric: 23.1177 - val_loss: 23.1222 - val_MinusLogProbMetric: 23.1222 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 21/1000
2023-10-10 11:16:29.090 
Epoch 21/1000 
	 loss: 22.8551, MinusLogProbMetric: 22.8551, val_loss: 22.5739, val_MinusLogProbMetric: 22.5739

Epoch 21: val_loss improved from 23.12224 to 22.57391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 22.8551 - MinusLogProbMetric: 22.8551 - val_loss: 22.5739 - val_MinusLogProbMetric: 22.5739 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 22/1000
2023-10-10 11:17:24.100 
Epoch 22/1000 
	 loss: 22.5049, MinusLogProbMetric: 22.5049, val_loss: 22.2287, val_MinusLogProbMetric: 22.2287

Epoch 22: val_loss improved from 22.57391 to 22.22872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 22.5049 - MinusLogProbMetric: 22.5049 - val_loss: 22.2287 - val_MinusLogProbMetric: 22.2287 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 23/1000
2023-10-10 11:18:18.986 
Epoch 23/1000 
	 loss: 27.0312, MinusLogProbMetric: 27.0312, val_loss: 116.5792, val_MinusLogProbMetric: 116.5792

Epoch 23: val_loss did not improve from 22.22872
196/196 - 54s - loss: 27.0312 - MinusLogProbMetric: 27.0312 - val_loss: 116.5792 - val_MinusLogProbMetric: 116.5792 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 24/1000
2023-10-10 11:19:12.844 
Epoch 24/1000 
	 loss: 55.0362, MinusLogProbMetric: 55.0362, val_loss: 31.8994, val_MinusLogProbMetric: 31.8994

Epoch 24: val_loss did not improve from 22.22872
196/196 - 54s - loss: 55.0362 - MinusLogProbMetric: 55.0362 - val_loss: 31.8994 - val_MinusLogProbMetric: 31.8994 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 25/1000
2023-10-10 11:20:07.480 
Epoch 25/1000 
	 loss: 28.3155, MinusLogProbMetric: 28.3155, val_loss: 26.4546, val_MinusLogProbMetric: 26.4546

Epoch 25: val_loss did not improve from 22.22872
196/196 - 55s - loss: 28.3155 - MinusLogProbMetric: 28.3155 - val_loss: 26.4546 - val_MinusLogProbMetric: 26.4546 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 26/1000
2023-10-10 11:21:01.010 
Epoch 26/1000 
	 loss: 25.4057, MinusLogProbMetric: 25.4057, val_loss: 24.6708, val_MinusLogProbMetric: 24.6708

Epoch 26: val_loss did not improve from 22.22872
196/196 - 54s - loss: 25.4057 - MinusLogProbMetric: 25.4057 - val_loss: 24.6708 - val_MinusLogProbMetric: 24.6708 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 27/1000
2023-10-10 11:21:54.957 
Epoch 27/1000 
	 loss: 24.0644, MinusLogProbMetric: 24.0644, val_loss: 23.9936, val_MinusLogProbMetric: 23.9936

Epoch 27: val_loss did not improve from 22.22872
196/196 - 54s - loss: 24.0644 - MinusLogProbMetric: 24.0644 - val_loss: 23.9936 - val_MinusLogProbMetric: 23.9936 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 28/1000
2023-10-10 11:22:48.644 
Epoch 28/1000 
	 loss: 23.2114, MinusLogProbMetric: 23.2114, val_loss: 23.2430, val_MinusLogProbMetric: 23.2430

Epoch 28: val_loss did not improve from 22.22872
196/196 - 54s - loss: 23.2114 - MinusLogProbMetric: 23.2114 - val_loss: 23.2430 - val_MinusLogProbMetric: 23.2430 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 29/1000
2023-10-10 11:23:42.840 
Epoch 29/1000 
	 loss: 22.6475, MinusLogProbMetric: 22.6475, val_loss: 22.3436, val_MinusLogProbMetric: 22.3436

Epoch 29: val_loss did not improve from 22.22872
196/196 - 54s - loss: 22.6475 - MinusLogProbMetric: 22.6475 - val_loss: 22.3436 - val_MinusLogProbMetric: 22.3436 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 30/1000
2023-10-10 11:24:37.799 
Epoch 30/1000 
	 loss: 22.1506, MinusLogProbMetric: 22.1506, val_loss: 22.4105, val_MinusLogProbMetric: 22.4105

Epoch 30: val_loss did not improve from 22.22872
196/196 - 55s - loss: 22.1506 - MinusLogProbMetric: 22.1506 - val_loss: 22.4105 - val_MinusLogProbMetric: 22.4105 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 31/1000
2023-10-10 11:25:31.792 
Epoch 31/1000 
	 loss: 21.7336, MinusLogProbMetric: 21.7336, val_loss: 21.7804, val_MinusLogProbMetric: 21.7804

Epoch 31: val_loss improved from 22.22872 to 21.78044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 21.7336 - MinusLogProbMetric: 21.7336 - val_loss: 21.7804 - val_MinusLogProbMetric: 21.7804 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 32/1000
2023-10-10 11:26:25.954 
Epoch 32/1000 
	 loss: 21.5503, MinusLogProbMetric: 21.5503, val_loss: 21.5730, val_MinusLogProbMetric: 21.5730

Epoch 32: val_loss improved from 21.78044 to 21.57305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 21.5503 - MinusLogProbMetric: 21.5503 - val_loss: 21.5730 - val_MinusLogProbMetric: 21.5730 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 33/1000
2023-10-10 11:27:20.817 
Epoch 33/1000 
	 loss: 21.3003, MinusLogProbMetric: 21.3003, val_loss: 21.2685, val_MinusLogProbMetric: 21.2685

Epoch 33: val_loss improved from 21.57305 to 21.26847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 21.3003 - MinusLogProbMetric: 21.3003 - val_loss: 21.2685 - val_MinusLogProbMetric: 21.2685 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 34/1000
2023-10-10 11:28:15.673 
Epoch 34/1000 
	 loss: 21.2004, MinusLogProbMetric: 21.2004, val_loss: 21.0672, val_MinusLogProbMetric: 21.0672

Epoch 34: val_loss improved from 21.26847 to 21.06724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 21.2004 - MinusLogProbMetric: 21.2004 - val_loss: 21.0672 - val_MinusLogProbMetric: 21.0672 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 35/1000
2023-10-10 11:29:10.494 
Epoch 35/1000 
	 loss: 20.9409, MinusLogProbMetric: 20.9409, val_loss: 20.8657, val_MinusLogProbMetric: 20.8657

Epoch 35: val_loss improved from 21.06724 to 20.86570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 20.9409 - MinusLogProbMetric: 20.9409 - val_loss: 20.8657 - val_MinusLogProbMetric: 20.8657 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 36/1000
2023-10-10 11:30:05.358 
Epoch 36/1000 
	 loss: 20.7822, MinusLogProbMetric: 20.7822, val_loss: 20.9888, val_MinusLogProbMetric: 20.9888

Epoch 36: val_loss did not improve from 20.86570
196/196 - 54s - loss: 20.7822 - MinusLogProbMetric: 20.7822 - val_loss: 20.9888 - val_MinusLogProbMetric: 20.9888 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 37/1000
2023-10-10 11:30:59.097 
Epoch 37/1000 
	 loss: 20.7886, MinusLogProbMetric: 20.7886, val_loss: 20.8929, val_MinusLogProbMetric: 20.8929

Epoch 37: val_loss did not improve from 20.86570
196/196 - 54s - loss: 20.7886 - MinusLogProbMetric: 20.7886 - val_loss: 20.8929 - val_MinusLogProbMetric: 20.8929 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 38/1000
2023-10-10 11:31:53.504 
Epoch 38/1000 
	 loss: 20.6120, MinusLogProbMetric: 20.6120, val_loss: 20.6601, val_MinusLogProbMetric: 20.6601

Epoch 38: val_loss improved from 20.86570 to 20.66015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 20.6120 - MinusLogProbMetric: 20.6120 - val_loss: 20.6601 - val_MinusLogProbMetric: 20.6601 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 39/1000
2023-10-10 11:32:47.955 
Epoch 39/1000 
	 loss: 25.4714, MinusLogProbMetric: 25.4714, val_loss: 21.3389, val_MinusLogProbMetric: 21.3389

Epoch 39: val_loss did not improve from 20.66015
196/196 - 54s - loss: 25.4714 - MinusLogProbMetric: 25.4714 - val_loss: 21.3389 - val_MinusLogProbMetric: 21.3389 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 40/1000
2023-10-10 11:33:42.186 
Epoch 40/1000 
	 loss: 20.5325, MinusLogProbMetric: 20.5325, val_loss: 20.5028, val_MinusLogProbMetric: 20.5028

Epoch 40: val_loss improved from 20.66015 to 20.50276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 20.5325 - MinusLogProbMetric: 20.5325 - val_loss: 20.5028 - val_MinusLogProbMetric: 20.5028 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 41/1000
2023-10-10 11:34:37.680 
Epoch 41/1000 
	 loss: 20.3014, MinusLogProbMetric: 20.3014, val_loss: 20.2890, val_MinusLogProbMetric: 20.2890

Epoch 41: val_loss improved from 20.50276 to 20.28904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 20.3014 - MinusLogProbMetric: 20.3014 - val_loss: 20.2890 - val_MinusLogProbMetric: 20.2890 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 42/1000
2023-10-10 11:35:32.758 
Epoch 42/1000 
	 loss: 20.1931, MinusLogProbMetric: 20.1931, val_loss: 146.7654, val_MinusLogProbMetric: 146.7654

Epoch 42: val_loss did not improve from 20.28904
196/196 - 54s - loss: 20.1931 - MinusLogProbMetric: 20.1931 - val_loss: 146.7654 - val_MinusLogProbMetric: 146.7654 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 43/1000
2023-10-10 11:36:25.800 
Epoch 43/1000 
	 loss: 43.6997, MinusLogProbMetric: 43.6997, val_loss: 28.3555, val_MinusLogProbMetric: 28.3555

Epoch 43: val_loss did not improve from 20.28904
196/196 - 53s - loss: 43.6997 - MinusLogProbMetric: 43.6997 - val_loss: 28.3555 - val_MinusLogProbMetric: 28.3555 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 44/1000
2023-10-10 11:37:20.015 
Epoch 44/1000 
	 loss: 25.3792, MinusLogProbMetric: 25.3792, val_loss: 23.9569, val_MinusLogProbMetric: 23.9569

Epoch 44: val_loss did not improve from 20.28904
196/196 - 54s - loss: 25.3792 - MinusLogProbMetric: 25.3792 - val_loss: 23.9569 - val_MinusLogProbMetric: 23.9569 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 45/1000
2023-10-10 11:38:13.751 
Epoch 45/1000 
	 loss: 23.2362, MinusLogProbMetric: 23.2362, val_loss: 22.6420, val_MinusLogProbMetric: 22.6420

Epoch 45: val_loss did not improve from 20.28904
196/196 - 54s - loss: 23.2362 - MinusLogProbMetric: 23.2362 - val_loss: 22.6420 - val_MinusLogProbMetric: 22.6420 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 46/1000
2023-10-10 11:39:07.786 
Epoch 46/1000 
	 loss: 22.3212, MinusLogProbMetric: 22.3212, val_loss: 22.0706, val_MinusLogProbMetric: 22.0706

Epoch 46: val_loss did not improve from 20.28904
196/196 - 54s - loss: 22.3212 - MinusLogProbMetric: 22.3212 - val_loss: 22.0706 - val_MinusLogProbMetric: 22.0706 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 47/1000
2023-10-10 11:40:01.624 
Epoch 47/1000 
	 loss: 21.8524, MinusLogProbMetric: 21.8524, val_loss: 22.1813, val_MinusLogProbMetric: 22.1813

Epoch 47: val_loss did not improve from 20.28904
196/196 - 54s - loss: 21.8524 - MinusLogProbMetric: 21.8524 - val_loss: 22.1813 - val_MinusLogProbMetric: 22.1813 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 48/1000
2023-10-10 11:40:55.233 
Epoch 48/1000 
	 loss: 21.4513, MinusLogProbMetric: 21.4513, val_loss: 21.5907, val_MinusLogProbMetric: 21.5907

Epoch 48: val_loss did not improve from 20.28904
196/196 - 54s - loss: 21.4513 - MinusLogProbMetric: 21.4513 - val_loss: 21.5907 - val_MinusLogProbMetric: 21.5907 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 49/1000
2023-10-10 11:41:48.548 
Epoch 49/1000 
	 loss: 21.2431, MinusLogProbMetric: 21.2431, val_loss: 20.9832, val_MinusLogProbMetric: 20.9832

Epoch 49: val_loss did not improve from 20.28904
196/196 - 53s - loss: 21.2431 - MinusLogProbMetric: 21.2431 - val_loss: 20.9832 - val_MinusLogProbMetric: 20.9832 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 50/1000
2023-10-10 11:42:42.657 
Epoch 50/1000 
	 loss: 21.0213, MinusLogProbMetric: 21.0213, val_loss: 21.0922, val_MinusLogProbMetric: 21.0922

Epoch 50: val_loss did not improve from 20.28904
196/196 - 54s - loss: 21.0213 - MinusLogProbMetric: 21.0213 - val_loss: 21.0922 - val_MinusLogProbMetric: 21.0922 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 51/1000
2023-10-10 11:43:36.297 
Epoch 51/1000 
	 loss: 25.9568, MinusLogProbMetric: 25.9568, val_loss: 22.4949, val_MinusLogProbMetric: 22.4949

Epoch 51: val_loss did not improve from 20.28904
196/196 - 54s - loss: 25.9568 - MinusLogProbMetric: 25.9568 - val_loss: 22.4949 - val_MinusLogProbMetric: 22.4949 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 52/1000
2023-10-10 11:44:29.780 
Epoch 52/1000 
	 loss: 21.3207, MinusLogProbMetric: 21.3207, val_loss: 20.7519, val_MinusLogProbMetric: 20.7519

Epoch 52: val_loss did not improve from 20.28904
196/196 - 53s - loss: 21.3207 - MinusLogProbMetric: 21.3207 - val_loss: 20.7519 - val_MinusLogProbMetric: 20.7519 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 53/1000
2023-10-10 11:45:23.759 
Epoch 53/1000 
	 loss: 20.7737, MinusLogProbMetric: 20.7737, val_loss: 20.8290, val_MinusLogProbMetric: 20.8290

Epoch 53: val_loss did not improve from 20.28904
196/196 - 54s - loss: 20.7737 - MinusLogProbMetric: 20.7737 - val_loss: 20.8290 - val_MinusLogProbMetric: 20.8290 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 54/1000
2023-10-10 11:46:17.077 
Epoch 54/1000 
	 loss: 20.5609, MinusLogProbMetric: 20.5609, val_loss: 20.6126, val_MinusLogProbMetric: 20.6126

Epoch 54: val_loss did not improve from 20.28904
196/196 - 53s - loss: 20.5609 - MinusLogProbMetric: 20.5609 - val_loss: 20.6126 - val_MinusLogProbMetric: 20.6126 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 55/1000
2023-10-10 11:47:11.346 
Epoch 55/1000 
	 loss: 20.4862, MinusLogProbMetric: 20.4862, val_loss: 20.8600, val_MinusLogProbMetric: 20.8600

Epoch 55: val_loss did not improve from 20.28904
196/196 - 54s - loss: 20.4862 - MinusLogProbMetric: 20.4862 - val_loss: 20.8600 - val_MinusLogProbMetric: 20.8600 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 56/1000
2023-10-10 11:48:06.915 
Epoch 56/1000 
	 loss: 21.4337, MinusLogProbMetric: 21.4337, val_loss: 20.3953, val_MinusLogProbMetric: 20.3953

Epoch 56: val_loss did not improve from 20.28904
196/196 - 56s - loss: 21.4337 - MinusLogProbMetric: 21.4337 - val_loss: 20.3953 - val_MinusLogProbMetric: 20.3953 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 57/1000
2023-10-10 11:49:03.194 
Epoch 57/1000 
	 loss: 20.2840, MinusLogProbMetric: 20.2840, val_loss: 20.3708, val_MinusLogProbMetric: 20.3708

Epoch 57: val_loss did not improve from 20.28904
196/196 - 56s - loss: 20.2840 - MinusLogProbMetric: 20.2840 - val_loss: 20.3708 - val_MinusLogProbMetric: 20.3708 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 58/1000
2023-10-10 11:49:57.525 
Epoch 58/1000 
	 loss: 20.2001, MinusLogProbMetric: 20.2001, val_loss: 20.4304, val_MinusLogProbMetric: 20.4304

Epoch 58: val_loss did not improve from 20.28904
196/196 - 54s - loss: 20.2001 - MinusLogProbMetric: 20.2001 - val_loss: 20.4304 - val_MinusLogProbMetric: 20.4304 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 59/1000
2023-10-10 11:50:51.616 
Epoch 59/1000 
	 loss: 20.2492, MinusLogProbMetric: 20.2492, val_loss: 20.3989, val_MinusLogProbMetric: 20.3989

Epoch 59: val_loss did not improve from 20.28904
196/196 - 54s - loss: 20.2492 - MinusLogProbMetric: 20.2492 - val_loss: 20.3989 - val_MinusLogProbMetric: 20.3989 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 60/1000
2023-10-10 11:51:45.451 
Epoch 60/1000 
	 loss: 20.1477, MinusLogProbMetric: 20.1477, val_loss: 21.1648, val_MinusLogProbMetric: 21.1648

Epoch 60: val_loss did not improve from 20.28904
196/196 - 54s - loss: 20.1477 - MinusLogProbMetric: 20.1477 - val_loss: 21.1648 - val_MinusLogProbMetric: 21.1648 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 61/1000
2023-10-10 11:52:39.957 
Epoch 61/1000 
	 loss: 20.4615, MinusLogProbMetric: 20.4615, val_loss: 22.6764, val_MinusLogProbMetric: 22.6764

Epoch 61: val_loss did not improve from 20.28904
196/196 - 55s - loss: 20.4615 - MinusLogProbMetric: 20.4615 - val_loss: 22.6764 - val_MinusLogProbMetric: 22.6764 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 62/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 122: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 11:53:14.993 
Epoch 62/1000 
	 loss: nan, MinusLogProbMetric: 37.1472, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 62: val_loss did not improve from 20.28904
196/196 - 35s - loss: nan - MinusLogProbMetric: 37.1472 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 35s/epoch - 179ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 3.703703703703703e-05.
===========
Generating train data for run 309.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f3abd327430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c8c702e90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c8c702e90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a6c20e050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a27465900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a27465e70>, <keras.callbacks.ModelCheckpoint object at 0x7f3a27465f30>, <keras.callbacks.EarlyStopping object at 0x7f3a274661a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a274661d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3a27465e10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-10-10 11:53:22.969258
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-10-10 11:56:26.142 
Epoch 1/1000 
	 loss: 21.7048, MinusLogProbMetric: 21.7048, val_loss: 20.0017, val_MinusLogProbMetric: 20.0017

Epoch 1: val_loss improved from inf to 20.00167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 184s - loss: 21.7048 - MinusLogProbMetric: 21.7048 - val_loss: 20.0017 - val_MinusLogProbMetric: 20.0017 - lr: 3.7037e-05 - 184s/epoch - 937ms/step
Epoch 2/1000
2023-10-10 11:57:21.105 
Epoch 2/1000 
	 loss: 19.8638, MinusLogProbMetric: 19.8638, val_loss: 20.1636, val_MinusLogProbMetric: 20.1636

Epoch 2: val_loss did not improve from 20.00167
196/196 - 54s - loss: 19.8638 - MinusLogProbMetric: 19.8638 - val_loss: 20.1636 - val_MinusLogProbMetric: 20.1636 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 3/1000
2023-10-10 11:58:14.962 
Epoch 3/1000 
	 loss: 19.7838, MinusLogProbMetric: 19.7838, val_loss: 20.3800, val_MinusLogProbMetric: 20.3800

Epoch 3: val_loss did not improve from 20.00167
196/196 - 54s - loss: 19.7838 - MinusLogProbMetric: 19.7838 - val_loss: 20.3800 - val_MinusLogProbMetric: 20.3800 - lr: 3.7037e-05 - 54s/epoch - 275ms/step
Epoch 4/1000
2023-10-10 11:59:09.260 
Epoch 4/1000 
	 loss: 19.7118, MinusLogProbMetric: 19.7118, val_loss: 19.8534, val_MinusLogProbMetric: 19.8534

Epoch 4: val_loss improved from 20.00167 to 19.85338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.7118 - MinusLogProbMetric: 19.7118 - val_loss: 19.8534 - val_MinusLogProbMetric: 19.8534 - lr: 3.7037e-05 - 55s/epoch - 281ms/step
Epoch 5/1000
2023-10-10 12:00:04.585 
Epoch 5/1000 
	 loss: 22.1310, MinusLogProbMetric: 22.1310, val_loss: 112.1165, val_MinusLogProbMetric: 112.1165

Epoch 5: val_loss did not improve from 19.85338
196/196 - 55s - loss: 22.1310 - MinusLogProbMetric: 22.1310 - val_loss: 112.1165 - val_MinusLogProbMetric: 112.1165 - lr: 3.7037e-05 - 55s/epoch - 278ms/step
Epoch 6/1000
2023-10-10 12:00:59.079 
Epoch 6/1000 
	 loss: 50.8989, MinusLogProbMetric: 50.8989, val_loss: 35.5510, val_MinusLogProbMetric: 35.5510

Epoch 6: val_loss did not improve from 19.85338
196/196 - 54s - loss: 50.8989 - MinusLogProbMetric: 50.8989 - val_loss: 35.5510 - val_MinusLogProbMetric: 35.5510 - lr: 3.7037e-05 - 54s/epoch - 278ms/step
Epoch 7/1000
2023-10-10 12:01:53.531 
Epoch 7/1000 
	 loss: 30.5590, MinusLogProbMetric: 30.5590, val_loss: 28.1161, val_MinusLogProbMetric: 28.1161

Epoch 7: val_loss did not improve from 19.85338
196/196 - 54s - loss: 30.5590 - MinusLogProbMetric: 30.5590 - val_loss: 28.1161 - val_MinusLogProbMetric: 28.1161 - lr: 3.7037e-05 - 54s/epoch - 278ms/step
Epoch 8/1000
2023-10-10 12:02:47.885 
Epoch 8/1000 
	 loss: 26.9098, MinusLogProbMetric: 26.9098, val_loss: 26.1843, val_MinusLogProbMetric: 26.1843

Epoch 8: val_loss did not improve from 19.85338
196/196 - 54s - loss: 26.9098 - MinusLogProbMetric: 26.9098 - val_loss: 26.1843 - val_MinusLogProbMetric: 26.1843 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 9/1000
2023-10-10 12:03:41.667 
Epoch 9/1000 
	 loss: 25.4308, MinusLogProbMetric: 25.4308, val_loss: 25.0122, val_MinusLogProbMetric: 25.0122

Epoch 9: val_loss did not improve from 19.85338
196/196 - 54s - loss: 25.4308 - MinusLogProbMetric: 25.4308 - val_loss: 25.0122 - val_MinusLogProbMetric: 25.0122 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 10/1000
2023-10-10 12:04:35.405 
Epoch 10/1000 
	 loss: 24.4396, MinusLogProbMetric: 24.4396, val_loss: 24.1140, val_MinusLogProbMetric: 24.1140

Epoch 10: val_loss did not improve from 19.85338
196/196 - 54s - loss: 24.4396 - MinusLogProbMetric: 24.4396 - val_loss: 24.1140 - val_MinusLogProbMetric: 24.1140 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 11/1000
2023-10-10 12:05:29.222 
Epoch 11/1000 
	 loss: 23.7902, MinusLogProbMetric: 23.7902, val_loss: 23.5089, val_MinusLogProbMetric: 23.5089

Epoch 11: val_loss did not improve from 19.85338
196/196 - 54s - loss: 23.7902 - MinusLogProbMetric: 23.7902 - val_loss: 23.5089 - val_MinusLogProbMetric: 23.5089 - lr: 3.7037e-05 - 54s/epoch - 275ms/step
Epoch 12/1000
2023-10-10 12:06:23.437 
Epoch 12/1000 
	 loss: 23.0735, MinusLogProbMetric: 23.0735, val_loss: 22.8965, val_MinusLogProbMetric: 22.8965

Epoch 12: val_loss did not improve from 19.85338
196/196 - 54s - loss: 23.0735 - MinusLogProbMetric: 23.0735 - val_loss: 22.8965 - val_MinusLogProbMetric: 22.8965 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 13/1000
2023-10-10 12:07:17.154 
Epoch 13/1000 
	 loss: 22.5460, MinusLogProbMetric: 22.5460, val_loss: 22.4099, val_MinusLogProbMetric: 22.4099

Epoch 13: val_loss did not improve from 19.85338
196/196 - 54s - loss: 22.5460 - MinusLogProbMetric: 22.5460 - val_loss: 22.4099 - val_MinusLogProbMetric: 22.4099 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 14/1000
2023-10-10 12:08:11.428 
Epoch 14/1000 
	 loss: 25.8618, MinusLogProbMetric: 25.8618, val_loss: 23.9740, val_MinusLogProbMetric: 23.9740

Epoch 14: val_loss did not improve from 19.85338
196/196 - 54s - loss: 25.8618 - MinusLogProbMetric: 25.8618 - val_loss: 23.9740 - val_MinusLogProbMetric: 23.9740 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 15/1000
2023-10-10 12:09:05.540 
Epoch 15/1000 
	 loss: 22.5208, MinusLogProbMetric: 22.5208, val_loss: 21.9748, val_MinusLogProbMetric: 21.9748

Epoch 15: val_loss did not improve from 19.85338
196/196 - 54s - loss: 22.5208 - MinusLogProbMetric: 22.5208 - val_loss: 21.9748 - val_MinusLogProbMetric: 21.9748 - lr: 3.7037e-05 - 54s/epoch - 276ms/step
Epoch 16/1000
2023-10-10 12:09:59.020 
Epoch 16/1000 
	 loss: 21.6593, MinusLogProbMetric: 21.6593, val_loss: 21.6482, val_MinusLogProbMetric: 21.6482

Epoch 16: val_loss did not improve from 19.85338
196/196 - 53s - loss: 21.6593 - MinusLogProbMetric: 21.6593 - val_loss: 21.6482 - val_MinusLogProbMetric: 21.6482 - lr: 3.7037e-05 - 53s/epoch - 273ms/step
Epoch 17/1000
2023-10-10 12:10:53.072 
Epoch 17/1000 
	 loss: 21.3478, MinusLogProbMetric: 21.3478, val_loss: 21.3332, val_MinusLogProbMetric: 21.3332

Epoch 17: val_loss did not improve from 19.85338
196/196 - 54s - loss: 21.3478 - MinusLogProbMetric: 21.3478 - val_loss: 21.3332 - val_MinusLogProbMetric: 21.3332 - lr: 3.7037e-05 - 54s/epoch - 276ms/step
Epoch 18/1000
2023-10-10 12:11:47.548 
Epoch 18/1000 
	 loss: 21.0889, MinusLogProbMetric: 21.0889, val_loss: 21.0956, val_MinusLogProbMetric: 21.0956

Epoch 18: val_loss did not improve from 19.85338
196/196 - 54s - loss: 21.0889 - MinusLogProbMetric: 21.0889 - val_loss: 21.0956 - val_MinusLogProbMetric: 21.0956 - lr: 3.7037e-05 - 54s/epoch - 278ms/step
Epoch 19/1000
2023-10-10 12:12:42.207 
Epoch 19/1000 
	 loss: 20.8995, MinusLogProbMetric: 20.8995, val_loss: 20.8841, val_MinusLogProbMetric: 20.8841

Epoch 19: val_loss did not improve from 19.85338
196/196 - 55s - loss: 20.8995 - MinusLogProbMetric: 20.8995 - val_loss: 20.8841 - val_MinusLogProbMetric: 20.8841 - lr: 3.7037e-05 - 55s/epoch - 279ms/step
Epoch 20/1000
2023-10-10 12:13:35.738 
Epoch 20/1000 
	 loss: 20.7113, MinusLogProbMetric: 20.7113, val_loss: 20.6559, val_MinusLogProbMetric: 20.6559

Epoch 20: val_loss did not improve from 19.85338
196/196 - 54s - loss: 20.7113 - MinusLogProbMetric: 20.7113 - val_loss: 20.6559 - val_MinusLogProbMetric: 20.6559 - lr: 3.7037e-05 - 54s/epoch - 273ms/step
Epoch 21/1000
2023-10-10 12:14:30.971 
Epoch 21/1000 
	 loss: 20.5613, MinusLogProbMetric: 20.5613, val_loss: 20.5847, val_MinusLogProbMetric: 20.5847

Epoch 21: val_loss did not improve from 19.85338
196/196 - 55s - loss: 20.5613 - MinusLogProbMetric: 20.5613 - val_loss: 20.5847 - val_MinusLogProbMetric: 20.5847 - lr: 3.7037e-05 - 55s/epoch - 282ms/step
Epoch 22/1000
2023-10-10 12:15:27.036 
Epoch 22/1000 
	 loss: 20.4257, MinusLogProbMetric: 20.4257, val_loss: 20.4525, val_MinusLogProbMetric: 20.4525

Epoch 22: val_loss did not improve from 19.85338
196/196 - 56s - loss: 20.4257 - MinusLogProbMetric: 20.4257 - val_loss: 20.4525 - val_MinusLogProbMetric: 20.4525 - lr: 3.7037e-05 - 56s/epoch - 286ms/step
Epoch 23/1000
2023-10-10 12:16:20.771 
Epoch 23/1000 
	 loss: 20.3046, MinusLogProbMetric: 20.3046, val_loss: 20.3084, val_MinusLogProbMetric: 20.3084

Epoch 23: val_loss did not improve from 19.85338
196/196 - 54s - loss: 20.3046 - MinusLogProbMetric: 20.3046 - val_loss: 20.3084 - val_MinusLogProbMetric: 20.3084 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 24/1000
2023-10-10 12:17:14.817 
Epoch 24/1000 
	 loss: 20.1749, MinusLogProbMetric: 20.1749, val_loss: 20.1922, val_MinusLogProbMetric: 20.1922

Epoch 24: val_loss did not improve from 19.85338
196/196 - 54s - loss: 20.1749 - MinusLogProbMetric: 20.1749 - val_loss: 20.1922 - val_MinusLogProbMetric: 20.1922 - lr: 3.7037e-05 - 54s/epoch - 276ms/step
Epoch 25/1000
2023-10-10 12:18:08.553 
Epoch 25/1000 
	 loss: 20.0744, MinusLogProbMetric: 20.0744, val_loss: 20.0516, val_MinusLogProbMetric: 20.0516

Epoch 25: val_loss did not improve from 19.85338
196/196 - 54s - loss: 20.0744 - MinusLogProbMetric: 20.0744 - val_loss: 20.0516 - val_MinusLogProbMetric: 20.0516 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 26/1000
2023-10-10 12:19:03.497 
Epoch 26/1000 
	 loss: 19.9591, MinusLogProbMetric: 19.9591, val_loss: 20.1006, val_MinusLogProbMetric: 20.1006

Epoch 26: val_loss did not improve from 19.85338
196/196 - 55s - loss: 19.9591 - MinusLogProbMetric: 19.9591 - val_loss: 20.1006 - val_MinusLogProbMetric: 20.1006 - lr: 3.7037e-05 - 55s/epoch - 280ms/step
Epoch 27/1000
2023-10-10 12:19:58.267 
Epoch 27/1000 
	 loss: 19.9986, MinusLogProbMetric: 19.9986, val_loss: 26.9044, val_MinusLogProbMetric: 26.9044

Epoch 27: val_loss did not improve from 19.85338
196/196 - 55s - loss: 19.9986 - MinusLogProbMetric: 19.9986 - val_loss: 26.9044 - val_MinusLogProbMetric: 26.9044 - lr: 3.7037e-05 - 55s/epoch - 279ms/step
Epoch 28/1000
2023-10-10 12:20:52.545 
Epoch 28/1000 
	 loss: 20.1135, MinusLogProbMetric: 20.1135, val_loss: 19.8957, val_MinusLogProbMetric: 19.8957

Epoch 28: val_loss did not improve from 19.85338
196/196 - 54s - loss: 20.1135 - MinusLogProbMetric: 20.1135 - val_loss: 19.8957 - val_MinusLogProbMetric: 19.8957 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 29/1000
2023-10-10 12:21:46.363 
Epoch 29/1000 
	 loss: 19.7301, MinusLogProbMetric: 19.7301, val_loss: 19.9938, val_MinusLogProbMetric: 19.9938

Epoch 29: val_loss did not improve from 19.85338
196/196 - 54s - loss: 19.7301 - MinusLogProbMetric: 19.7301 - val_loss: 19.9938 - val_MinusLogProbMetric: 19.9938 - lr: 3.7037e-05 - 54s/epoch - 275ms/step
Epoch 30/1000
2023-10-10 12:22:40.392 
Epoch 30/1000 
	 loss: 19.6402, MinusLogProbMetric: 19.6402, val_loss: 19.8883, val_MinusLogProbMetric: 19.8883

Epoch 30: val_loss did not improve from 19.85338
196/196 - 54s - loss: 19.6402 - MinusLogProbMetric: 19.6402 - val_loss: 19.8883 - val_MinusLogProbMetric: 19.8883 - lr: 3.7037e-05 - 54s/epoch - 276ms/step
Epoch 31/1000
2023-10-10 12:23:34.135 
Epoch 31/1000 
	 loss: 19.5479, MinusLogProbMetric: 19.5479, val_loss: 19.5675, val_MinusLogProbMetric: 19.5675

Epoch 31: val_loss improved from 19.85338 to 19.56749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.5479 - MinusLogProbMetric: 19.5479 - val_loss: 19.5675 - val_MinusLogProbMetric: 19.5675 - lr: 3.7037e-05 - 55s/epoch - 278ms/step
Epoch 32/1000
2023-10-10 12:24:28.924 
Epoch 32/1000 
	 loss: 19.4746, MinusLogProbMetric: 19.4746, val_loss: 19.5437, val_MinusLogProbMetric: 19.5437

Epoch 32: val_loss improved from 19.56749 to 19.54366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.4746 - MinusLogProbMetric: 19.4746 - val_loss: 19.5437 - val_MinusLogProbMetric: 19.5437 - lr: 3.7037e-05 - 55s/epoch - 280ms/step
Epoch 33/1000
2023-10-10 12:25:24.781 
Epoch 33/1000 
	 loss: 19.3917, MinusLogProbMetric: 19.3917, val_loss: 19.4748, val_MinusLogProbMetric: 19.4748

Epoch 33: val_loss improved from 19.54366 to 19.47479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 19.3917 - MinusLogProbMetric: 19.3917 - val_loss: 19.4748 - val_MinusLogProbMetric: 19.4748 - lr: 3.7037e-05 - 56s/epoch - 285ms/step
Epoch 34/1000
2023-10-10 12:26:19.940 
Epoch 34/1000 
	 loss: 19.3391, MinusLogProbMetric: 19.3391, val_loss: 19.3855, val_MinusLogProbMetric: 19.3855

Epoch 34: val_loss improved from 19.47479 to 19.38548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.3391 - MinusLogProbMetric: 19.3391 - val_loss: 19.3855 - val_MinusLogProbMetric: 19.3855 - lr: 3.7037e-05 - 55s/epoch - 281ms/step
Epoch 35/1000
2023-10-10 12:27:14.756 
Epoch 35/1000 
	 loss: 19.3108, MinusLogProbMetric: 19.3108, val_loss: 19.3768, val_MinusLogProbMetric: 19.3768

Epoch 35: val_loss improved from 19.38548 to 19.37683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.3108 - MinusLogProbMetric: 19.3108 - val_loss: 19.3768 - val_MinusLogProbMetric: 19.3768 - lr: 3.7037e-05 - 55s/epoch - 279ms/step
Epoch 36/1000
2023-10-10 12:28:10.341 
Epoch 36/1000 
	 loss: 19.2202, MinusLogProbMetric: 19.2202, val_loss: 19.4157, val_MinusLogProbMetric: 19.4157

Epoch 36: val_loss did not improve from 19.37683
196/196 - 55s - loss: 19.2202 - MinusLogProbMetric: 19.2202 - val_loss: 19.4157 - val_MinusLogProbMetric: 19.4157 - lr: 3.7037e-05 - 55s/epoch - 279ms/step
Epoch 37/1000
2023-10-10 12:29:04.293 
Epoch 37/1000 
	 loss: 19.2040, MinusLogProbMetric: 19.2040, val_loss: 20.8141, val_MinusLogProbMetric: 20.8141

Epoch 37: val_loss did not improve from 19.37683
196/196 - 54s - loss: 19.2040 - MinusLogProbMetric: 19.2040 - val_loss: 20.8141 - val_MinusLogProbMetric: 20.8141 - lr: 3.7037e-05 - 54s/epoch - 275ms/step
Epoch 38/1000
2023-10-10 12:29:58.035 
Epoch 38/1000 
	 loss: 19.1907, MinusLogProbMetric: 19.1907, val_loss: 19.2128, val_MinusLogProbMetric: 19.2128

Epoch 38: val_loss improved from 19.37683 to 19.21277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.1907 - MinusLogProbMetric: 19.1907 - val_loss: 19.2128 - val_MinusLogProbMetric: 19.2128 - lr: 3.7037e-05 - 55s/epoch - 279ms/step
Epoch 39/1000
2023-10-10 12:30:53.093 
Epoch 39/1000 
	 loss: 19.3800, MinusLogProbMetric: 19.3800, val_loss: 19.2392, val_MinusLogProbMetric: 19.2392

Epoch 39: val_loss did not improve from 19.21277
196/196 - 54s - loss: 19.3800 - MinusLogProbMetric: 19.3800 - val_loss: 19.2392 - val_MinusLogProbMetric: 19.2392 - lr: 3.7037e-05 - 54s/epoch - 276ms/step
Epoch 40/1000
2023-10-10 12:31:46.719 
Epoch 40/1000 
	 loss: 19.0434, MinusLogProbMetric: 19.0434, val_loss: 19.2085, val_MinusLogProbMetric: 19.2085

Epoch 40: val_loss improved from 19.21277 to 19.20849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 19.0434 - MinusLogProbMetric: 19.0434 - val_loss: 19.2085 - val_MinusLogProbMetric: 19.2085 - lr: 3.7037e-05 - 54s/epoch - 278ms/step
Epoch 41/1000
2023-10-10 12:32:41.634 
Epoch 41/1000 
	 loss: 19.0146, MinusLogProbMetric: 19.0146, val_loss: 19.1302, val_MinusLogProbMetric: 19.1302

Epoch 41: val_loss improved from 19.20849 to 19.13023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.0146 - MinusLogProbMetric: 19.0146 - val_loss: 19.1302 - val_MinusLogProbMetric: 19.1302 - lr: 3.7037e-05 - 55s/epoch - 281ms/step
Epoch 42/1000
2023-10-10 12:33:36.550 
Epoch 42/1000 
	 loss: 19.0350, MinusLogProbMetric: 19.0350, val_loss: 19.0388, val_MinusLogProbMetric: 19.0388

Epoch 42: val_loss improved from 19.13023 to 19.03875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.0350 - MinusLogProbMetric: 19.0350 - val_loss: 19.0388 - val_MinusLogProbMetric: 19.0388 - lr: 3.7037e-05 - 55s/epoch - 280ms/step
Epoch 43/1000
2023-10-10 12:34:31.854 
Epoch 43/1000 
	 loss: 19.0277, MinusLogProbMetric: 19.0277, val_loss: 18.9343, val_MinusLogProbMetric: 18.9343

Epoch 43: val_loss improved from 19.03875 to 18.93430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 19.0277 - MinusLogProbMetric: 19.0277 - val_loss: 18.9343 - val_MinusLogProbMetric: 18.9343 - lr: 3.7037e-05 - 55s/epoch - 282ms/step
Epoch 44/1000
2023-10-10 12:35:27.918 
Epoch 44/1000 
	 loss: 18.9397, MinusLogProbMetric: 18.9397, val_loss: 19.1500, val_MinusLogProbMetric: 19.1500

Epoch 44: val_loss did not improve from 18.93430
196/196 - 55s - loss: 18.9397 - MinusLogProbMetric: 18.9397 - val_loss: 19.1500 - val_MinusLogProbMetric: 19.1500 - lr: 3.7037e-05 - 55s/epoch - 281ms/step
Epoch 45/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 121: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 12:36:03.166 
Epoch 45/1000 
	 loss: nan, MinusLogProbMetric: 18.9110, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 45: val_loss did not improve from 18.93430
196/196 - 35s - loss: nan - MinusLogProbMetric: 18.9110 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 35s/epoch - 180ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.2345679012345677e-05.
===========
Generating train data for run 309.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f3a0c24c4c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a25c068f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a25c068f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a1542b700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a1544d930>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a1544e4d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3a1544e9e0>, <keras.callbacks.EarlyStopping object at 0x7f3a1544ed40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a1544ef80>, <keras.callbacks.TerminateOnNaN object at 0x7f3a1544e500>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-10-10 12:36:10.013003
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-10-10 12:39:05.123 
Epoch 1/1000 
	 loss: 19.1155, MinusLogProbMetric: 19.1155, val_loss: 19.0243, val_MinusLogProbMetric: 19.0243

Epoch 1: val_loss improved from inf to 19.02426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 176s - loss: 19.1155 - MinusLogProbMetric: 19.1155 - val_loss: 19.0243 - val_MinusLogProbMetric: 19.0243 - lr: 1.2346e-05 - 176s/epoch - 897ms/step
Epoch 2/1000
2023-10-10 12:40:02.586 
Epoch 2/1000 
	 loss: 19.8550, MinusLogProbMetric: 19.8550, val_loss: 20.0599, val_MinusLogProbMetric: 20.0599

Epoch 2: val_loss did not improve from 19.02426
196/196 - 56s - loss: 19.8550 - MinusLogProbMetric: 19.8550 - val_loss: 20.0599 - val_MinusLogProbMetric: 20.0599 - lr: 1.2346e-05 - 56s/epoch - 287ms/step
Epoch 3/1000
2023-10-10 12:40:57.231 
Epoch 3/1000 
	 loss: 19.4648, MinusLogProbMetric: 19.4648, val_loss: 19.3284, val_MinusLogProbMetric: 19.3284

Epoch 3: val_loss did not improve from 19.02426
196/196 - 55s - loss: 19.4648 - MinusLogProbMetric: 19.4648 - val_loss: 19.3284 - val_MinusLogProbMetric: 19.3284 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 4/1000
2023-10-10 12:41:50.852 
Epoch 4/1000 
	 loss: 19.1776, MinusLogProbMetric: 19.1776, val_loss: 19.2281, val_MinusLogProbMetric: 19.2281

Epoch 4: val_loss did not improve from 19.02426
196/196 - 54s - loss: 19.1776 - MinusLogProbMetric: 19.1776 - val_loss: 19.2281 - val_MinusLogProbMetric: 19.2281 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 5/1000
2023-10-10 12:42:44.457 
Epoch 5/1000 
	 loss: 19.0687, MinusLogProbMetric: 19.0687, val_loss: 19.1079, val_MinusLogProbMetric: 19.1079

Epoch 5: val_loss did not improve from 19.02426
196/196 - 54s - loss: 19.0687 - MinusLogProbMetric: 19.0687 - val_loss: 19.1079 - val_MinusLogProbMetric: 19.1079 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 6/1000
2023-10-10 12:43:37.933 
Epoch 6/1000 
	 loss: 18.9808, MinusLogProbMetric: 18.9808, val_loss: 19.0601, val_MinusLogProbMetric: 19.0601

Epoch 6: val_loss did not improve from 19.02426
196/196 - 53s - loss: 18.9808 - MinusLogProbMetric: 18.9808 - val_loss: 19.0601 - val_MinusLogProbMetric: 19.0601 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 7/1000
2023-10-10 12:44:31.807 
Epoch 7/1000 
	 loss: 18.9142, MinusLogProbMetric: 18.9142, val_loss: 18.9644, val_MinusLogProbMetric: 18.9644

Epoch 7: val_loss improved from 19.02426 to 18.96439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.9142 - MinusLogProbMetric: 18.9142 - val_loss: 18.9644 - val_MinusLogProbMetric: 18.9644 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 8/1000
2023-10-10 12:45:26.751 
Epoch 8/1000 
	 loss: 18.8533, MinusLogProbMetric: 18.8533, val_loss: 18.9355, val_MinusLogProbMetric: 18.9355

Epoch 8: val_loss improved from 18.96439 to 18.93551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.8533 - MinusLogProbMetric: 18.8533 - val_loss: 18.9355 - val_MinusLogProbMetric: 18.9355 - lr: 1.2346e-05 - 55s/epoch - 281ms/step
Epoch 9/1000
2023-10-10 12:46:22.398 
Epoch 9/1000 
	 loss: 18.8216, MinusLogProbMetric: 18.8216, val_loss: 18.9065, val_MinusLogProbMetric: 18.9065

Epoch 9: val_loss improved from 18.93551 to 18.90655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 18.8216 - MinusLogProbMetric: 18.8216 - val_loss: 18.9065 - val_MinusLogProbMetric: 18.9065 - lr: 1.2346e-05 - 56s/epoch - 284ms/step
Epoch 10/1000
2023-10-10 12:47:16.702 
Epoch 10/1000 
	 loss: 18.7915, MinusLogProbMetric: 18.7915, val_loss: 18.8624, val_MinusLogProbMetric: 18.8624

Epoch 10: val_loss improved from 18.90655 to 18.86236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.7915 - MinusLogProbMetric: 18.7915 - val_loss: 18.8624 - val_MinusLogProbMetric: 18.8624 - lr: 1.2346e-05 - 55s/epoch - 281ms/step
Epoch 11/1000
2023-10-10 12:48:12.186 
Epoch 11/1000 
	 loss: 18.7662, MinusLogProbMetric: 18.7662, val_loss: 18.8614, val_MinusLogProbMetric: 18.8614

Epoch 11: val_loss improved from 18.86236 to 18.86138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.7662 - MinusLogProbMetric: 18.7662 - val_loss: 18.8614 - val_MinusLogProbMetric: 18.8614 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 12/1000
2023-10-10 12:49:06.024 
Epoch 12/1000 
	 loss: 18.7501, MinusLogProbMetric: 18.7501, val_loss: 18.7755, val_MinusLogProbMetric: 18.7755

Epoch 12: val_loss improved from 18.86138 to 18.77546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.7501 - MinusLogProbMetric: 18.7501 - val_loss: 18.7755 - val_MinusLogProbMetric: 18.7755 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 13/1000
2023-10-10 12:50:00.490 
Epoch 13/1000 
	 loss: 18.6917, MinusLogProbMetric: 18.6917, val_loss: 18.7121, val_MinusLogProbMetric: 18.7121

Epoch 13: val_loss improved from 18.77546 to 18.71210, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.6917 - MinusLogProbMetric: 18.6917 - val_loss: 18.7121 - val_MinusLogProbMetric: 18.7121 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 14/1000
2023-10-10 12:50:55.209 
Epoch 14/1000 
	 loss: 18.6759, MinusLogProbMetric: 18.6759, val_loss: 18.7338, val_MinusLogProbMetric: 18.7338

Epoch 14: val_loss did not improve from 18.71210
196/196 - 53s - loss: 18.6759 - MinusLogProbMetric: 18.6759 - val_loss: 18.7338 - val_MinusLogProbMetric: 18.7338 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 15/1000
2023-10-10 12:51:49.307 
Epoch 15/1000 
	 loss: 18.6436, MinusLogProbMetric: 18.6436, val_loss: 18.7547, val_MinusLogProbMetric: 18.7547

Epoch 15: val_loss did not improve from 18.71210
196/196 - 54s - loss: 18.6436 - MinusLogProbMetric: 18.6436 - val_loss: 18.7547 - val_MinusLogProbMetric: 18.7547 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 16/1000
2023-10-10 12:52:43.461 
Epoch 16/1000 
	 loss: 18.6199, MinusLogProbMetric: 18.6199, val_loss: 18.6848, val_MinusLogProbMetric: 18.6848

Epoch 16: val_loss improved from 18.71210 to 18.68484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.6199 - MinusLogProbMetric: 18.6199 - val_loss: 18.6848 - val_MinusLogProbMetric: 18.6848 - lr: 1.2346e-05 - 55s/epoch - 281ms/step
Epoch 17/1000
2023-10-10 12:53:38.523 
Epoch 17/1000 
	 loss: 18.6110, MinusLogProbMetric: 18.6110, val_loss: 18.7329, val_MinusLogProbMetric: 18.7329

Epoch 17: val_loss did not improve from 18.68484
196/196 - 54s - loss: 18.6110 - MinusLogProbMetric: 18.6110 - val_loss: 18.7329 - val_MinusLogProbMetric: 18.7329 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 18/1000
2023-10-10 12:54:31.913 
Epoch 18/1000 
	 loss: 18.5754, MinusLogProbMetric: 18.5754, val_loss: 18.5914, val_MinusLogProbMetric: 18.5914

Epoch 18: val_loss improved from 18.68484 to 18.59139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.5754 - MinusLogProbMetric: 18.5754 - val_loss: 18.5914 - val_MinusLogProbMetric: 18.5914 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 19/1000
2023-10-10 12:55:25.947 
Epoch 19/1000 
	 loss: 18.5669, MinusLogProbMetric: 18.5669, val_loss: 18.5781, val_MinusLogProbMetric: 18.5781

Epoch 19: val_loss improved from 18.59139 to 18.57807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.5669 - MinusLogProbMetric: 18.5669 - val_loss: 18.5781 - val_MinusLogProbMetric: 18.5781 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 20/1000
2023-10-10 12:56:20.472 
Epoch 20/1000 
	 loss: 18.5318, MinusLogProbMetric: 18.5318, val_loss: 18.6347, val_MinusLogProbMetric: 18.6347

Epoch 20: val_loss did not improve from 18.57807
196/196 - 54s - loss: 18.5318 - MinusLogProbMetric: 18.5318 - val_loss: 18.6347 - val_MinusLogProbMetric: 18.6347 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 21/1000
2023-10-10 12:57:14.081 
Epoch 21/1000 
	 loss: 18.5171, MinusLogProbMetric: 18.5171, val_loss: 18.6124, val_MinusLogProbMetric: 18.6124

Epoch 21: val_loss did not improve from 18.57807
196/196 - 54s - loss: 18.5171 - MinusLogProbMetric: 18.5171 - val_loss: 18.6124 - val_MinusLogProbMetric: 18.6124 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 22/1000
2023-10-10 12:58:07.050 
Epoch 22/1000 
	 loss: 18.5176, MinusLogProbMetric: 18.5176, val_loss: 18.6451, val_MinusLogProbMetric: 18.6451

Epoch 22: val_loss did not improve from 18.57807
196/196 - 53s - loss: 18.5176 - MinusLogProbMetric: 18.5176 - val_loss: 18.6451 - val_MinusLogProbMetric: 18.6451 - lr: 1.2346e-05 - 53s/epoch - 270ms/step
Epoch 23/1000
2023-10-10 12:59:00.271 
Epoch 23/1000 
	 loss: 18.4979, MinusLogProbMetric: 18.4979, val_loss: 18.6227, val_MinusLogProbMetric: 18.6227

Epoch 23: val_loss did not improve from 18.57807
196/196 - 53s - loss: 18.4979 - MinusLogProbMetric: 18.4979 - val_loss: 18.6227 - val_MinusLogProbMetric: 18.6227 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 24/1000
2023-10-10 12:59:53.846 
Epoch 24/1000 
	 loss: 18.4693, MinusLogProbMetric: 18.4693, val_loss: 18.5437, val_MinusLogProbMetric: 18.5437

Epoch 24: val_loss improved from 18.57807 to 18.54367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.4693 - MinusLogProbMetric: 18.4693 - val_loss: 18.5437 - val_MinusLogProbMetric: 18.5437 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 25/1000
2023-10-10 13:00:48.590 
Epoch 25/1000 
	 loss: 18.4726, MinusLogProbMetric: 18.4726, val_loss: 18.6226, val_MinusLogProbMetric: 18.6226

Epoch 25: val_loss did not improve from 18.54367
196/196 - 54s - loss: 18.4726 - MinusLogProbMetric: 18.4726 - val_loss: 18.6226 - val_MinusLogProbMetric: 18.6226 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 26/1000
2023-10-10 13:01:42.176 
Epoch 26/1000 
	 loss: 18.4398, MinusLogProbMetric: 18.4398, val_loss: 18.6182, val_MinusLogProbMetric: 18.6182

Epoch 26: val_loss did not improve from 18.54367
196/196 - 54s - loss: 18.4398 - MinusLogProbMetric: 18.4398 - val_loss: 18.6182 - val_MinusLogProbMetric: 18.6182 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 27/1000
2023-10-10 13:02:36.662 
Epoch 27/1000 
	 loss: 18.4285, MinusLogProbMetric: 18.4285, val_loss: 18.5647, val_MinusLogProbMetric: 18.5647

Epoch 27: val_loss did not improve from 18.54367
196/196 - 54s - loss: 18.4285 - MinusLogProbMetric: 18.4285 - val_loss: 18.5647 - val_MinusLogProbMetric: 18.5647 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 28/1000
2023-10-10 13:03:31.571 
Epoch 28/1000 
	 loss: 18.4126, MinusLogProbMetric: 18.4126, val_loss: 18.4871, val_MinusLogProbMetric: 18.4871

Epoch 28: val_loss improved from 18.54367 to 18.48713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 18.4126 - MinusLogProbMetric: 18.4126 - val_loss: 18.4871 - val_MinusLogProbMetric: 18.4871 - lr: 1.2346e-05 - 56s/epoch - 285ms/step
Epoch 29/1000
2023-10-10 13:04:27.069 
Epoch 29/1000 
	 loss: 18.3875, MinusLogProbMetric: 18.3875, val_loss: 18.7266, val_MinusLogProbMetric: 18.7266

Epoch 29: val_loss did not improve from 18.48713
196/196 - 55s - loss: 18.3875 - MinusLogProbMetric: 18.3875 - val_loss: 18.7266 - val_MinusLogProbMetric: 18.7266 - lr: 1.2346e-05 - 55s/epoch - 278ms/step
Epoch 30/1000
2023-10-10 13:05:20.937 
Epoch 30/1000 
	 loss: 18.5155, MinusLogProbMetric: 18.5155, val_loss: 18.4515, val_MinusLogProbMetric: 18.4515

Epoch 30: val_loss improved from 18.48713 to 18.45148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.5155 - MinusLogProbMetric: 18.5155 - val_loss: 18.4515 - val_MinusLogProbMetric: 18.4515 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 31/1000
2023-10-10 13:06:15.980 
Epoch 31/1000 
	 loss: 18.3644, MinusLogProbMetric: 18.3644, val_loss: 18.4506, val_MinusLogProbMetric: 18.4506

Epoch 31: val_loss improved from 18.45148 to 18.45057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.3644 - MinusLogProbMetric: 18.3644 - val_loss: 18.4506 - val_MinusLogProbMetric: 18.4506 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 32/1000
2023-10-10 13:07:10.086 
Epoch 32/1000 
	 loss: 18.3691, MinusLogProbMetric: 18.3691, val_loss: 18.4289, val_MinusLogProbMetric: 18.4289

Epoch 32: val_loss improved from 18.45057 to 18.42894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.3691 - MinusLogProbMetric: 18.3691 - val_loss: 18.4289 - val_MinusLogProbMetric: 18.4289 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 33/1000
2023-10-10 13:08:04.445 
Epoch 33/1000 
	 loss: 18.3316, MinusLogProbMetric: 18.3316, val_loss: 18.4853, val_MinusLogProbMetric: 18.4853

Epoch 33: val_loss did not improve from 18.42894
196/196 - 53s - loss: 18.3316 - MinusLogProbMetric: 18.3316 - val_loss: 18.4853 - val_MinusLogProbMetric: 18.4853 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 34/1000
2023-10-10 13:08:57.764 
Epoch 34/1000 
	 loss: 18.3414, MinusLogProbMetric: 18.3414, val_loss: 18.3943, val_MinusLogProbMetric: 18.3943

Epoch 34: val_loss improved from 18.42894 to 18.39434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.3414 - MinusLogProbMetric: 18.3414 - val_loss: 18.3943 - val_MinusLogProbMetric: 18.3943 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 35/1000
2023-10-10 13:09:52.311 
Epoch 35/1000 
	 loss: 18.3187, MinusLogProbMetric: 18.3187, val_loss: 18.3780, val_MinusLogProbMetric: 18.3780

Epoch 35: val_loss improved from 18.39434 to 18.37803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.3187 - MinusLogProbMetric: 18.3187 - val_loss: 18.3780 - val_MinusLogProbMetric: 18.3780 - lr: 1.2346e-05 - 55s/epoch - 278ms/step
Epoch 36/1000
2023-10-10 13:10:46.098 
Epoch 36/1000 
	 loss: 18.3139, MinusLogProbMetric: 18.3139, val_loss: 18.3654, val_MinusLogProbMetric: 18.3654

Epoch 36: val_loss improved from 18.37803 to 18.36538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.3139 - MinusLogProbMetric: 18.3139 - val_loss: 18.3654 - val_MinusLogProbMetric: 18.3654 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 37/1000
2023-10-10 13:11:40.574 
Epoch 37/1000 
	 loss: 18.2943, MinusLogProbMetric: 18.2943, val_loss: 18.3741, val_MinusLogProbMetric: 18.3741

Epoch 37: val_loss did not improve from 18.36538
196/196 - 54s - loss: 18.2943 - MinusLogProbMetric: 18.2943 - val_loss: 18.3741 - val_MinusLogProbMetric: 18.3741 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 38/1000
2023-10-10 13:12:34.500 
Epoch 38/1000 
	 loss: 18.2939, MinusLogProbMetric: 18.2939, val_loss: 18.3352, val_MinusLogProbMetric: 18.3352

Epoch 38: val_loss improved from 18.36538 to 18.33519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.2939 - MinusLogProbMetric: 18.2939 - val_loss: 18.3352 - val_MinusLogProbMetric: 18.3352 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 39/1000
2023-10-10 13:13:28.855 
Epoch 39/1000 
	 loss: 18.2719, MinusLogProbMetric: 18.2719, val_loss: 18.3628, val_MinusLogProbMetric: 18.3628

Epoch 39: val_loss did not improve from 18.33519
196/196 - 54s - loss: 18.2719 - MinusLogProbMetric: 18.2719 - val_loss: 18.3628 - val_MinusLogProbMetric: 18.3628 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 40/1000
2023-10-10 13:14:21.970 
Epoch 40/1000 
	 loss: 18.2613, MinusLogProbMetric: 18.2613, val_loss: 18.3537, val_MinusLogProbMetric: 18.3537

Epoch 40: val_loss did not improve from 18.33519
196/196 - 53s - loss: 18.2613 - MinusLogProbMetric: 18.2613 - val_loss: 18.3537 - val_MinusLogProbMetric: 18.3537 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 41/1000
2023-10-10 13:15:17.778 
Epoch 41/1000 
	 loss: 18.2571, MinusLogProbMetric: 18.2571, val_loss: 18.3558, val_MinusLogProbMetric: 18.3558

Epoch 41: val_loss did not improve from 18.33519
196/196 - 56s - loss: 18.2571 - MinusLogProbMetric: 18.2571 - val_loss: 18.3558 - val_MinusLogProbMetric: 18.3558 - lr: 1.2346e-05 - 56s/epoch - 285ms/step
Epoch 42/1000
2023-10-10 13:16:11.880 
Epoch 42/1000 
	 loss: 18.2534, MinusLogProbMetric: 18.2534, val_loss: 18.3692, val_MinusLogProbMetric: 18.3692

Epoch 42: val_loss did not improve from 18.33519
196/196 - 54s - loss: 18.2534 - MinusLogProbMetric: 18.2534 - val_loss: 18.3692 - val_MinusLogProbMetric: 18.3692 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 43/1000
2023-10-10 13:17:05.491 
Epoch 43/1000 
	 loss: 18.2267, MinusLogProbMetric: 18.2267, val_loss: 18.3157, val_MinusLogProbMetric: 18.3157

Epoch 43: val_loss improved from 18.33519 to 18.31565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.2267 - MinusLogProbMetric: 18.2267 - val_loss: 18.3157 - val_MinusLogProbMetric: 18.3157 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 44/1000
2023-10-10 13:18:00.318 
Epoch 44/1000 
	 loss: 18.2309, MinusLogProbMetric: 18.2309, val_loss: 18.3224, val_MinusLogProbMetric: 18.3224

Epoch 44: val_loss did not improve from 18.31565
196/196 - 54s - loss: 18.2309 - MinusLogProbMetric: 18.2309 - val_loss: 18.3224 - val_MinusLogProbMetric: 18.3224 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 45/1000
2023-10-10 13:18:53.976 
Epoch 45/1000 
	 loss: 18.2140, MinusLogProbMetric: 18.2140, val_loss: 18.3854, val_MinusLogProbMetric: 18.3854

Epoch 45: val_loss did not improve from 18.31565
196/196 - 54s - loss: 18.2140 - MinusLogProbMetric: 18.2140 - val_loss: 18.3854 - val_MinusLogProbMetric: 18.3854 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 46/1000
2023-10-10 13:19:47.414 
Epoch 46/1000 
	 loss: 18.2025, MinusLogProbMetric: 18.2025, val_loss: 18.2296, val_MinusLogProbMetric: 18.2296

Epoch 46: val_loss improved from 18.31565 to 18.22956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.2025 - MinusLogProbMetric: 18.2025 - val_loss: 18.2296 - val_MinusLogProbMetric: 18.2296 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 47/1000
2023-10-10 13:20:41.860 
Epoch 47/1000 
	 loss: 18.2767, MinusLogProbMetric: 18.2767, val_loss: 18.3077, val_MinusLogProbMetric: 18.3077

Epoch 47: val_loss did not improve from 18.22956
196/196 - 54s - loss: 18.2767 - MinusLogProbMetric: 18.2767 - val_loss: 18.3077 - val_MinusLogProbMetric: 18.3077 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 48/1000
2023-10-10 13:21:35.388 
Epoch 48/1000 
	 loss: 18.1884, MinusLogProbMetric: 18.1884, val_loss: 18.3159, val_MinusLogProbMetric: 18.3159

Epoch 48: val_loss did not improve from 18.22956
196/196 - 54s - loss: 18.1884 - MinusLogProbMetric: 18.1884 - val_loss: 18.3159 - val_MinusLogProbMetric: 18.3159 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 49/1000
2023-10-10 13:22:28.663 
Epoch 49/1000 
	 loss: 18.1840, MinusLogProbMetric: 18.1840, val_loss: 18.3213, val_MinusLogProbMetric: 18.3213

Epoch 49: val_loss did not improve from 18.22956
196/196 - 53s - loss: 18.1840 - MinusLogProbMetric: 18.1840 - val_loss: 18.3213 - val_MinusLogProbMetric: 18.3213 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 50/1000
2023-10-10 13:23:22.976 
Epoch 50/1000 
	 loss: 18.1712, MinusLogProbMetric: 18.1712, val_loss: 18.3106, val_MinusLogProbMetric: 18.3106

Epoch 50: val_loss did not improve from 18.22956
196/196 - 54s - loss: 18.1712 - MinusLogProbMetric: 18.1712 - val_loss: 18.3106 - val_MinusLogProbMetric: 18.3106 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 51/1000
2023-10-10 13:24:17.700 
Epoch 51/1000 
	 loss: 18.1692, MinusLogProbMetric: 18.1692, val_loss: 18.2592, val_MinusLogProbMetric: 18.2592

Epoch 51: val_loss did not improve from 18.22956
196/196 - 55s - loss: 18.1692 - MinusLogProbMetric: 18.1692 - val_loss: 18.2592 - val_MinusLogProbMetric: 18.2592 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 52/1000
2023-10-10 13:25:11.711 
Epoch 52/1000 
	 loss: 18.1615, MinusLogProbMetric: 18.1615, val_loss: 18.3722, val_MinusLogProbMetric: 18.3722

Epoch 52: val_loss did not improve from 18.22956
196/196 - 54s - loss: 18.1615 - MinusLogProbMetric: 18.1615 - val_loss: 18.3722 - val_MinusLogProbMetric: 18.3722 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 53/1000
2023-10-10 13:26:05.143 
Epoch 53/1000 
	 loss: 18.2080, MinusLogProbMetric: 18.2080, val_loss: 18.3827, val_MinusLogProbMetric: 18.3827

Epoch 53: val_loss did not improve from 18.22956
196/196 - 53s - loss: 18.2080 - MinusLogProbMetric: 18.2080 - val_loss: 18.3827 - val_MinusLogProbMetric: 18.3827 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 54/1000
2023-10-10 13:27:00.113 
Epoch 54/1000 
	 loss: 18.1372, MinusLogProbMetric: 18.1372, val_loss: 18.1717, val_MinusLogProbMetric: 18.1717

Epoch 54: val_loss improved from 18.22956 to 18.17167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 18.1372 - MinusLogProbMetric: 18.1372 - val_loss: 18.1717 - val_MinusLogProbMetric: 18.1717 - lr: 1.2346e-05 - 56s/epoch - 284ms/step
Epoch 55/1000
2023-10-10 13:27:54.870 
Epoch 55/1000 
	 loss: 18.1181, MinusLogProbMetric: 18.1181, val_loss: 18.2256, val_MinusLogProbMetric: 18.2256

Epoch 55: val_loss did not improve from 18.17167
196/196 - 54s - loss: 18.1181 - MinusLogProbMetric: 18.1181 - val_loss: 18.2256 - val_MinusLogProbMetric: 18.2256 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 56/1000
2023-10-10 13:28:48.693 
Epoch 56/1000 
	 loss: 18.1201, MinusLogProbMetric: 18.1201, val_loss: 18.2119, val_MinusLogProbMetric: 18.2119

Epoch 56: val_loss did not improve from 18.17167
196/196 - 54s - loss: 18.1201 - MinusLogProbMetric: 18.1201 - val_loss: 18.2119 - val_MinusLogProbMetric: 18.2119 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 57/1000
2023-10-10 13:29:42.315 
Epoch 57/1000 
	 loss: 18.1053, MinusLogProbMetric: 18.1053, val_loss: 18.1734, val_MinusLogProbMetric: 18.1734

Epoch 57: val_loss did not improve from 18.17167
196/196 - 54s - loss: 18.1053 - MinusLogProbMetric: 18.1053 - val_loss: 18.1734 - val_MinusLogProbMetric: 18.1734 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 58/1000
2023-10-10 13:30:35.891 
Epoch 58/1000 
	 loss: 18.1063, MinusLogProbMetric: 18.1063, val_loss: 18.1324, val_MinusLogProbMetric: 18.1324

Epoch 58: val_loss improved from 18.17167 to 18.13236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.1063 - MinusLogProbMetric: 18.1063 - val_loss: 18.1324 - val_MinusLogProbMetric: 18.1324 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 59/1000
2023-10-10 13:31:30.649 
Epoch 59/1000 
	 loss: 18.0991, MinusLogProbMetric: 18.0991, val_loss: 18.2881, val_MinusLogProbMetric: 18.2881

Epoch 59: val_loss did not improve from 18.13236
196/196 - 54s - loss: 18.0991 - MinusLogProbMetric: 18.0991 - val_loss: 18.2881 - val_MinusLogProbMetric: 18.2881 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 60/1000
2023-10-10 13:32:24.131 
Epoch 60/1000 
	 loss: 18.1028, MinusLogProbMetric: 18.1028, val_loss: 18.4046, val_MinusLogProbMetric: 18.4046

Epoch 60: val_loss did not improve from 18.13236
196/196 - 53s - loss: 18.1028 - MinusLogProbMetric: 18.1028 - val_loss: 18.4046 - val_MinusLogProbMetric: 18.4046 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 61/1000
2023-10-10 13:33:17.808 
Epoch 61/1000 
	 loss: 18.0930, MinusLogProbMetric: 18.0930, val_loss: 18.1876, val_MinusLogProbMetric: 18.1876

Epoch 61: val_loss did not improve from 18.13236
196/196 - 54s - loss: 18.0930 - MinusLogProbMetric: 18.0930 - val_loss: 18.1876 - val_MinusLogProbMetric: 18.1876 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 62/1000
2023-10-10 13:34:12.162 
Epoch 62/1000 
	 loss: 18.0977, MinusLogProbMetric: 18.0977, val_loss: 18.1672, val_MinusLogProbMetric: 18.1672

Epoch 62: val_loss did not improve from 18.13236
196/196 - 54s - loss: 18.0977 - MinusLogProbMetric: 18.0977 - val_loss: 18.1672 - val_MinusLogProbMetric: 18.1672 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 63/1000
2023-10-10 13:35:05.641 
Epoch 63/1000 
	 loss: 18.0730, MinusLogProbMetric: 18.0730, val_loss: 18.1000, val_MinusLogProbMetric: 18.1000

Epoch 63: val_loss improved from 18.13236 to 18.10003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 18.0730 - MinusLogProbMetric: 18.0730 - val_loss: 18.1000 - val_MinusLogProbMetric: 18.1000 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 64/1000
2023-10-10 13:36:01.101 
Epoch 64/1000 
	 loss: 18.0655, MinusLogProbMetric: 18.0655, val_loss: 18.1402, val_MinusLogProbMetric: 18.1402

Epoch 64: val_loss did not improve from 18.10003
196/196 - 55s - loss: 18.0655 - MinusLogProbMetric: 18.0655 - val_loss: 18.1402 - val_MinusLogProbMetric: 18.1402 - lr: 1.2346e-05 - 55s/epoch - 278ms/step
Epoch 65/1000
2023-10-10 13:36:54.696 
Epoch 65/1000 
	 loss: 18.0450, MinusLogProbMetric: 18.0450, val_loss: 18.1792, val_MinusLogProbMetric: 18.1792

Epoch 65: val_loss did not improve from 18.10003
196/196 - 54s - loss: 18.0450 - MinusLogProbMetric: 18.0450 - val_loss: 18.1792 - val_MinusLogProbMetric: 18.1792 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 66/1000
2023-10-10 13:37:48.491 
Epoch 66/1000 
	 loss: 18.0509, MinusLogProbMetric: 18.0509, val_loss: 18.0697, val_MinusLogProbMetric: 18.0697

Epoch 66: val_loss improved from 18.10003 to 18.06974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.0509 - MinusLogProbMetric: 18.0509 - val_loss: 18.0697 - val_MinusLogProbMetric: 18.0697 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 67/1000
2023-10-10 13:38:43.025 
Epoch 67/1000 
	 loss: 18.0415, MinusLogProbMetric: 18.0415, val_loss: 18.1136, val_MinusLogProbMetric: 18.1136

Epoch 67: val_loss did not improve from 18.06974
196/196 - 54s - loss: 18.0415 - MinusLogProbMetric: 18.0415 - val_loss: 18.1136 - val_MinusLogProbMetric: 18.1136 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 68/1000
2023-10-10 13:39:36.775 
Epoch 68/1000 
	 loss: 18.0428, MinusLogProbMetric: 18.0428, val_loss: 18.0580, val_MinusLogProbMetric: 18.0580

Epoch 68: val_loss improved from 18.06974 to 18.05805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.0428 - MinusLogProbMetric: 18.0428 - val_loss: 18.0580 - val_MinusLogProbMetric: 18.0580 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 69/1000
2023-10-10 13:40:31.071 
Epoch 69/1000 
	 loss: 18.0143, MinusLogProbMetric: 18.0143, val_loss: 18.1575, val_MinusLogProbMetric: 18.1575

Epoch 69: val_loss did not improve from 18.05805
196/196 - 53s - loss: 18.0143 - MinusLogProbMetric: 18.0143 - val_loss: 18.1575 - val_MinusLogProbMetric: 18.1575 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 70/1000
2023-10-10 13:41:24.932 
Epoch 70/1000 
	 loss: 18.0297, MinusLogProbMetric: 18.0297, val_loss: 18.0618, val_MinusLogProbMetric: 18.0618

Epoch 70: val_loss did not improve from 18.05805
196/196 - 54s - loss: 18.0297 - MinusLogProbMetric: 18.0297 - val_loss: 18.0618 - val_MinusLogProbMetric: 18.0618 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 71/1000
2023-10-10 13:42:18.153 
Epoch 71/1000 
	 loss: 18.0142, MinusLogProbMetric: 18.0142, val_loss: 18.0932, val_MinusLogProbMetric: 18.0932

Epoch 71: val_loss did not improve from 18.05805
196/196 - 53s - loss: 18.0142 - MinusLogProbMetric: 18.0142 - val_loss: 18.0932 - val_MinusLogProbMetric: 18.0932 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 72/1000
2023-10-10 13:43:12.220 
Epoch 72/1000 
	 loss: 18.0232, MinusLogProbMetric: 18.0232, val_loss: 18.0837, val_MinusLogProbMetric: 18.0837

Epoch 72: val_loss did not improve from 18.05805
196/196 - 54s - loss: 18.0232 - MinusLogProbMetric: 18.0232 - val_loss: 18.0837 - val_MinusLogProbMetric: 18.0837 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 73/1000
2023-10-10 13:44:06.006 
Epoch 73/1000 
	 loss: 18.0074, MinusLogProbMetric: 18.0074, val_loss: 18.2166, val_MinusLogProbMetric: 18.2166

Epoch 73: val_loss did not improve from 18.05805
196/196 - 54s - loss: 18.0074 - MinusLogProbMetric: 18.0074 - val_loss: 18.2166 - val_MinusLogProbMetric: 18.2166 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 74/1000
2023-10-10 13:44:59.831 
Epoch 74/1000 
	 loss: 18.0130, MinusLogProbMetric: 18.0130, val_loss: 18.0562, val_MinusLogProbMetric: 18.0562

Epoch 74: val_loss improved from 18.05805 to 18.05620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 18.0130 - MinusLogProbMetric: 18.0130 - val_loss: 18.0562 - val_MinusLogProbMetric: 18.0562 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 75/1000
2023-10-10 13:45:53.940 
Epoch 75/1000 
	 loss: 18.0059, MinusLogProbMetric: 18.0059, val_loss: 18.0961, val_MinusLogProbMetric: 18.0961

Epoch 75: val_loss did not improve from 18.05620
196/196 - 53s - loss: 18.0059 - MinusLogProbMetric: 18.0059 - val_loss: 18.0961 - val_MinusLogProbMetric: 18.0961 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 76/1000
2023-10-10 13:46:47.396 
Epoch 76/1000 
	 loss: 17.9783, MinusLogProbMetric: 17.9783, val_loss: 18.0707, val_MinusLogProbMetric: 18.0707

Epoch 76: val_loss did not improve from 18.05620
196/196 - 53s - loss: 17.9783 - MinusLogProbMetric: 17.9783 - val_loss: 18.0707 - val_MinusLogProbMetric: 18.0707 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 77/1000
2023-10-10 13:47:41.093 
Epoch 77/1000 
	 loss: 17.9860, MinusLogProbMetric: 17.9860, val_loss: 18.0095, val_MinusLogProbMetric: 18.0095

Epoch 77: val_loss improved from 18.05620 to 18.00949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 17.9860 - MinusLogProbMetric: 17.9860 - val_loss: 18.0095 - val_MinusLogProbMetric: 18.0095 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 78/1000
2023-10-10 13:48:35.112 
Epoch 78/1000 
	 loss: 17.9692, MinusLogProbMetric: 17.9692, val_loss: 18.0821, val_MinusLogProbMetric: 18.0821

Epoch 78: val_loss did not improve from 18.00949
196/196 - 53s - loss: 17.9692 - MinusLogProbMetric: 17.9692 - val_loss: 18.0821 - val_MinusLogProbMetric: 18.0821 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 79/1000
2023-10-10 13:49:28.469 
Epoch 79/1000 
	 loss: 17.9652, MinusLogProbMetric: 17.9652, val_loss: 18.1403, val_MinusLogProbMetric: 18.1403

Epoch 79: val_loss did not improve from 18.00949
196/196 - 53s - loss: 17.9652 - MinusLogProbMetric: 17.9652 - val_loss: 18.1403 - val_MinusLogProbMetric: 18.1403 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 80/1000
2023-10-10 13:50:22.203 
Epoch 80/1000 
	 loss: 17.9645, MinusLogProbMetric: 17.9645, val_loss: 18.0732, val_MinusLogProbMetric: 18.0732

Epoch 80: val_loss did not improve from 18.00949
196/196 - 54s - loss: 17.9645 - MinusLogProbMetric: 17.9645 - val_loss: 18.0732 - val_MinusLogProbMetric: 18.0732 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 81/1000
2023-10-10 13:51:15.880 
Epoch 81/1000 
	 loss: 17.9681, MinusLogProbMetric: 17.9681, val_loss: 18.0177, val_MinusLogProbMetric: 18.0177

Epoch 81: val_loss did not improve from 18.00949
196/196 - 54s - loss: 17.9681 - MinusLogProbMetric: 17.9681 - val_loss: 18.0177 - val_MinusLogProbMetric: 18.0177 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 82/1000
2023-10-10 13:52:09.705 
Epoch 82/1000 
	 loss: 17.9532, MinusLogProbMetric: 17.9532, val_loss: 18.0289, val_MinusLogProbMetric: 18.0289

Epoch 82: val_loss did not improve from 18.00949
196/196 - 54s - loss: 17.9532 - MinusLogProbMetric: 17.9532 - val_loss: 18.0289 - val_MinusLogProbMetric: 18.0289 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 83/1000
2023-10-10 13:53:03.422 
Epoch 83/1000 
	 loss: 17.9408, MinusLogProbMetric: 17.9408, val_loss: 18.0543, val_MinusLogProbMetric: 18.0543

Epoch 83: val_loss did not improve from 18.00949
196/196 - 54s - loss: 17.9408 - MinusLogProbMetric: 17.9408 - val_loss: 18.0543 - val_MinusLogProbMetric: 18.0543 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 84/1000
2023-10-10 13:53:57.424 
Epoch 84/1000 
	 loss: 17.9316, MinusLogProbMetric: 17.9316, val_loss: 18.0012, val_MinusLogProbMetric: 18.0012

Epoch 84: val_loss improved from 18.00949 to 18.00118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 17.9316 - MinusLogProbMetric: 17.9316 - val_loss: 18.0012 - val_MinusLogProbMetric: 18.0012 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 85/1000
2023-10-10 13:54:51.872 
Epoch 85/1000 
	 loss: 17.9479, MinusLogProbMetric: 17.9479, val_loss: 17.9777, val_MinusLogProbMetric: 17.9777

Epoch 85: val_loss improved from 18.00118 to 17.97769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 17.9479 - MinusLogProbMetric: 17.9479 - val_loss: 17.9777 - val_MinusLogProbMetric: 17.9777 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 86/1000
2023-10-10 13:55:46.422 
Epoch 86/1000 
	 loss: 17.9305, MinusLogProbMetric: 17.9305, val_loss: 18.0629, val_MinusLogProbMetric: 18.0629

Epoch 86: val_loss did not improve from 17.97769
196/196 - 54s - loss: 17.9305 - MinusLogProbMetric: 17.9305 - val_loss: 18.0629 - val_MinusLogProbMetric: 18.0629 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 87/1000
2023-10-10 13:56:40.454 
Epoch 87/1000 
	 loss: 17.9408, MinusLogProbMetric: 17.9408, val_loss: 18.0568, val_MinusLogProbMetric: 18.0568

Epoch 87: val_loss did not improve from 17.97769
196/196 - 54s - loss: 17.9408 - MinusLogProbMetric: 17.9408 - val_loss: 18.0568 - val_MinusLogProbMetric: 18.0568 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 88/1000
2023-10-10 13:57:33.747 
Epoch 88/1000 
	 loss: 17.9149, MinusLogProbMetric: 17.9149, val_loss: 18.0430, val_MinusLogProbMetric: 18.0430

Epoch 88: val_loss did not improve from 17.97769
196/196 - 53s - loss: 17.9149 - MinusLogProbMetric: 17.9149 - val_loss: 18.0430 - val_MinusLogProbMetric: 18.0430 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 89/1000
2023-10-10 13:58:26.938 
Epoch 89/1000 
	 loss: 17.9151, MinusLogProbMetric: 17.9151, val_loss: 18.0398, val_MinusLogProbMetric: 18.0398

Epoch 89: val_loss did not improve from 17.97769
196/196 - 53s - loss: 17.9151 - MinusLogProbMetric: 17.9151 - val_loss: 18.0398 - val_MinusLogProbMetric: 18.0398 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 90/1000
2023-10-10 13:59:20.231 
Epoch 90/1000 
	 loss: 17.9080, MinusLogProbMetric: 17.9080, val_loss: 17.9816, val_MinusLogProbMetric: 17.9816

Epoch 90: val_loss did not improve from 17.97769
196/196 - 53s - loss: 17.9080 - MinusLogProbMetric: 17.9080 - val_loss: 17.9816 - val_MinusLogProbMetric: 17.9816 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 91/1000
2023-10-10 14:00:13.667 
Epoch 91/1000 
	 loss: 17.9018, MinusLogProbMetric: 17.9018, val_loss: 18.1424, val_MinusLogProbMetric: 18.1424

Epoch 91: val_loss did not improve from 17.97769
196/196 - 53s - loss: 17.9018 - MinusLogProbMetric: 17.9018 - val_loss: 18.1424 - val_MinusLogProbMetric: 18.1424 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 92/1000
2023-10-10 14:01:06.897 
Epoch 92/1000 
	 loss: 17.9085, MinusLogProbMetric: 17.9085, val_loss: 17.9184, val_MinusLogProbMetric: 17.9184

Epoch 92: val_loss improved from 17.97769 to 17.91838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 17.9085 - MinusLogProbMetric: 17.9085 - val_loss: 17.9184 - val_MinusLogProbMetric: 17.9184 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 93/1000
2023-10-10 14:02:01.421 
Epoch 93/1000 
	 loss: 18.6284, MinusLogProbMetric: 18.6284, val_loss: 18.0374, val_MinusLogProbMetric: 18.0374

Epoch 93: val_loss did not improve from 17.91838
196/196 - 54s - loss: 18.6284 - MinusLogProbMetric: 18.6284 - val_loss: 18.0374 - val_MinusLogProbMetric: 18.0374 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 94/1000
2023-10-10 14:02:55.061 
Epoch 94/1000 
	 loss: 17.9037, MinusLogProbMetric: 17.9037, val_loss: 17.9387, val_MinusLogProbMetric: 17.9387

Epoch 94: val_loss did not improve from 17.91838
196/196 - 54s - loss: 17.9037 - MinusLogProbMetric: 17.9037 - val_loss: 17.9387 - val_MinusLogProbMetric: 17.9387 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 95/1000
2023-10-10 14:03:48.677 
Epoch 95/1000 
	 loss: 17.8712, MinusLogProbMetric: 17.8712, val_loss: 17.9929, val_MinusLogProbMetric: 17.9929

Epoch 95: val_loss did not improve from 17.91838
196/196 - 54s - loss: 17.8712 - MinusLogProbMetric: 17.8712 - val_loss: 17.9929 - val_MinusLogProbMetric: 17.9929 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 96/1000
2023-10-10 14:04:42.323 
Epoch 96/1000 
	 loss: 17.8762, MinusLogProbMetric: 17.8762, val_loss: 17.9954, val_MinusLogProbMetric: 17.9954

Epoch 96: val_loss did not improve from 17.91838
196/196 - 54s - loss: 17.8762 - MinusLogProbMetric: 17.8762 - val_loss: 17.9954 - val_MinusLogProbMetric: 17.9954 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 97/1000
2023-10-10 14:05:35.614 
Epoch 97/1000 
	 loss: 17.8736, MinusLogProbMetric: 17.8736, val_loss: 17.9960, val_MinusLogProbMetric: 17.9960

Epoch 97: val_loss did not improve from 17.91838
196/196 - 53s - loss: 17.8736 - MinusLogProbMetric: 17.8736 - val_loss: 17.9960 - val_MinusLogProbMetric: 17.9960 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 98/1000
2023-10-10 14:06:29.175 
Epoch 98/1000 
	 loss: 17.8604, MinusLogProbMetric: 17.8604, val_loss: 17.9240, val_MinusLogProbMetric: 17.9240

Epoch 98: val_loss did not improve from 17.91838
196/196 - 54s - loss: 17.8604 - MinusLogProbMetric: 17.8604 - val_loss: 17.9240 - val_MinusLogProbMetric: 17.9240 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 99/1000
2023-10-10 14:07:22.869 
Epoch 99/1000 
	 loss: 17.8531, MinusLogProbMetric: 17.8531, val_loss: 17.9816, val_MinusLogProbMetric: 17.9816

Epoch 99: val_loss did not improve from 17.91838
196/196 - 54s - loss: 17.8531 - MinusLogProbMetric: 17.8531 - val_loss: 17.9816 - val_MinusLogProbMetric: 17.9816 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 100/1000
2023-10-10 14:08:17.962 
Epoch 100/1000 
	 loss: 17.8526, MinusLogProbMetric: 17.8526, val_loss: 17.9093, val_MinusLogProbMetric: 17.9093

Epoch 100: val_loss improved from 17.91838 to 17.90932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 56s - loss: 17.8526 - MinusLogProbMetric: 17.8526 - val_loss: 17.9093 - val_MinusLogProbMetric: 17.9093 - lr: 1.2346e-05 - 56s/epoch - 286ms/step
Epoch 101/1000
2023-10-10 14:09:15.844 
Epoch 101/1000 
	 loss: 17.8511, MinusLogProbMetric: 17.8511, val_loss: 17.8948, val_MinusLogProbMetric: 17.8948

Epoch 101: val_loss improved from 17.90932 to 17.89485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 58s - loss: 17.8511 - MinusLogProbMetric: 17.8511 - val_loss: 17.8948 - val_MinusLogProbMetric: 17.8948 - lr: 1.2346e-05 - 58s/epoch - 296ms/step
Epoch 102/1000
2023-10-10 14:10:12.693 
Epoch 102/1000 
	 loss: 17.8522, MinusLogProbMetric: 17.8522, val_loss: 18.0317, val_MinusLogProbMetric: 18.0317

Epoch 102: val_loss did not improve from 17.89485
196/196 - 56s - loss: 17.8522 - MinusLogProbMetric: 17.8522 - val_loss: 18.0317 - val_MinusLogProbMetric: 18.0317 - lr: 1.2346e-05 - 56s/epoch - 285ms/step
Epoch 103/1000
2023-10-10 14:11:09.217 
Epoch 103/1000 
	 loss: 17.8323, MinusLogProbMetric: 17.8323, val_loss: 17.9326, val_MinusLogProbMetric: 17.9326

Epoch 103: val_loss did not improve from 17.89485
196/196 - 56s - loss: 17.8323 - MinusLogProbMetric: 17.8323 - val_loss: 17.9326 - val_MinusLogProbMetric: 17.9326 - lr: 1.2346e-05 - 56s/epoch - 288ms/step
Epoch 104/1000
2023-10-10 14:12:04.261 
Epoch 104/1000 
	 loss: 17.8353, MinusLogProbMetric: 17.8353, val_loss: 17.9148, val_MinusLogProbMetric: 17.9148

Epoch 104: val_loss did not improve from 17.89485
196/196 - 55s - loss: 17.8353 - MinusLogProbMetric: 17.8353 - val_loss: 17.9148 - val_MinusLogProbMetric: 17.9148 - lr: 1.2346e-05 - 55s/epoch - 281ms/step
Epoch 105/1000
2023-10-10 14:13:00.687 
Epoch 105/1000 
	 loss: 17.8330, MinusLogProbMetric: 17.8330, val_loss: 17.9795, val_MinusLogProbMetric: 17.9795

Epoch 105: val_loss did not improve from 17.89485
196/196 - 56s - loss: 17.8330 - MinusLogProbMetric: 17.8330 - val_loss: 17.9795 - val_MinusLogProbMetric: 17.9795 - lr: 1.2346e-05 - 56s/epoch - 288ms/step
Epoch 106/1000
2023-10-10 14:13:57.040 
Epoch 106/1000 
	 loss: 17.8402, MinusLogProbMetric: 17.8402, val_loss: 17.8747, val_MinusLogProbMetric: 17.8747

Epoch 106: val_loss improved from 17.89485 to 17.87465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 57s - loss: 17.8402 - MinusLogProbMetric: 17.8402 - val_loss: 17.8747 - val_MinusLogProbMetric: 17.8747 - lr: 1.2346e-05 - 57s/epoch - 292ms/step
Epoch 107/1000
2023-10-10 14:14:53.590 
Epoch 107/1000 
	 loss: 17.8316, MinusLogProbMetric: 17.8316, val_loss: 17.9129, val_MinusLogProbMetric: 17.9129

Epoch 107: val_loss did not improve from 17.87465
196/196 - 56s - loss: 17.8316 - MinusLogProbMetric: 17.8316 - val_loss: 17.9129 - val_MinusLogProbMetric: 17.9129 - lr: 1.2346e-05 - 56s/epoch - 284ms/step
Epoch 108/1000
2023-10-10 14:15:49.684 
Epoch 108/1000 
	 loss: 17.8225, MinusLogProbMetric: 17.8225, val_loss: 17.8781, val_MinusLogProbMetric: 17.8781

Epoch 108: val_loss did not improve from 17.87465
196/196 - 56s - loss: 17.8225 - MinusLogProbMetric: 17.8225 - val_loss: 17.8781 - val_MinusLogProbMetric: 17.8781 - lr: 1.2346e-05 - 56s/epoch - 286ms/step
Epoch 109/1000
2023-10-10 14:16:43.779 
Epoch 109/1000 
	 loss: 17.8190, MinusLogProbMetric: 17.8190, val_loss: 17.9328, val_MinusLogProbMetric: 17.9328

Epoch 109: val_loss did not improve from 17.87465
196/196 - 54s - loss: 17.8190 - MinusLogProbMetric: 17.8190 - val_loss: 17.9328 - val_MinusLogProbMetric: 17.9328 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 110/1000
2023-10-10 14:17:36.922 
Epoch 110/1000 
	 loss: 17.8098, MinusLogProbMetric: 17.8098, val_loss: 17.8528, val_MinusLogProbMetric: 17.8528

Epoch 110: val_loss improved from 17.87465 to 17.85280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 54s - loss: 17.8098 - MinusLogProbMetric: 17.8098 - val_loss: 17.8528 - val_MinusLogProbMetric: 17.8528 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 111/1000
2023-10-10 14:18:31.465 
Epoch 111/1000 
	 loss: 17.8066, MinusLogProbMetric: 17.8066, val_loss: 17.8555, val_MinusLogProbMetric: 17.8555

Epoch 111: val_loss did not improve from 17.85280
196/196 - 54s - loss: 17.8066 - MinusLogProbMetric: 17.8066 - val_loss: 17.8555 - val_MinusLogProbMetric: 17.8555 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 112/1000
2023-10-10 14:19:24.960 
Epoch 112/1000 
	 loss: 17.8128, MinusLogProbMetric: 17.8128, val_loss: 17.8610, val_MinusLogProbMetric: 17.8610

Epoch 112: val_loss did not improve from 17.85280
196/196 - 53s - loss: 17.8128 - MinusLogProbMetric: 17.8128 - val_loss: 17.8610 - val_MinusLogProbMetric: 17.8610 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 113/1000
2023-10-10 14:20:18.191 
Epoch 113/1000 
	 loss: 17.7987, MinusLogProbMetric: 17.7987, val_loss: 17.8930, val_MinusLogProbMetric: 17.8930

Epoch 113: val_loss did not improve from 17.85280
196/196 - 53s - loss: 17.7987 - MinusLogProbMetric: 17.7987 - val_loss: 17.8930 - val_MinusLogProbMetric: 17.8930 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 114/1000
2023-10-10 14:21:11.247 
Epoch 114/1000 
	 loss: 17.7896, MinusLogProbMetric: 17.7896, val_loss: 17.8605, val_MinusLogProbMetric: 17.8605

Epoch 114: val_loss did not improve from 17.85280
196/196 - 53s - loss: 17.7896 - MinusLogProbMetric: 17.7896 - val_loss: 17.8605 - val_MinusLogProbMetric: 17.8605 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 115/1000
2023-10-10 14:22:04.952 
Epoch 115/1000 
	 loss: 17.7978, MinusLogProbMetric: 17.7978, val_loss: 17.8857, val_MinusLogProbMetric: 17.8857

Epoch 115: val_loss did not improve from 17.85280
196/196 - 54s - loss: 17.7978 - MinusLogProbMetric: 17.7978 - val_loss: 17.8857 - val_MinusLogProbMetric: 17.8857 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 116/1000
2023-10-10 14:22:58.058 
Epoch 116/1000 
	 loss: 17.7868, MinusLogProbMetric: 17.7868, val_loss: 17.8543, val_MinusLogProbMetric: 17.8543

Epoch 116: val_loss did not improve from 17.85280
196/196 - 53s - loss: 17.7868 - MinusLogProbMetric: 17.7868 - val_loss: 17.8543 - val_MinusLogProbMetric: 17.8543 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 117/1000
2023-10-10 14:23:52.133 
Epoch 117/1000 
	 loss: 17.7892, MinusLogProbMetric: 17.7892, val_loss: 17.9095, val_MinusLogProbMetric: 17.9095

Epoch 117: val_loss did not improve from 17.85280
196/196 - 54s - loss: 17.7892 - MinusLogProbMetric: 17.7892 - val_loss: 17.9095 - val_MinusLogProbMetric: 17.9095 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 118/1000
2023-10-10 14:24:45.726 
Epoch 118/1000 
	 loss: 17.8605, MinusLogProbMetric: 17.8605, val_loss: 18.2730, val_MinusLogProbMetric: 18.2730

Epoch 118: val_loss did not improve from 17.85280
196/196 - 54s - loss: 17.8605 - MinusLogProbMetric: 17.8605 - val_loss: 18.2730 - val_MinusLogProbMetric: 18.2730 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 119/1000
2023-10-10 14:25:39.038 
Epoch 119/1000 
	 loss: 17.8017, MinusLogProbMetric: 17.8017, val_loss: 17.8741, val_MinusLogProbMetric: 17.8741

Epoch 119: val_loss did not improve from 17.85280
196/196 - 53s - loss: 17.8017 - MinusLogProbMetric: 17.8017 - val_loss: 17.8741 - val_MinusLogProbMetric: 17.8741 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 120/1000
2023-10-10 14:26:32.958 
Epoch 120/1000 
	 loss: 17.9358, MinusLogProbMetric: 17.9358, val_loss: 17.9510, val_MinusLogProbMetric: 17.9510

Epoch 120: val_loss did not improve from 17.85280
196/196 - 54s - loss: 17.9358 - MinusLogProbMetric: 17.9358 - val_loss: 17.9510 - val_MinusLogProbMetric: 17.9510 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 121/1000
2023-10-10 14:27:26.796 
Epoch 121/1000 
	 loss: 17.7771, MinusLogProbMetric: 17.7771, val_loss: 17.8297, val_MinusLogProbMetric: 17.8297

Epoch 121: val_loss improved from 17.85280 to 17.82974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 55s - loss: 17.7771 - MinusLogProbMetric: 17.7771 - val_loss: 17.8297 - val_MinusLogProbMetric: 17.8297 - lr: 1.2346e-05 - 55s/epoch - 279ms/step
Epoch 122/1000
2023-10-10 14:28:21.235 
Epoch 122/1000 
	 loss: 17.7780, MinusLogProbMetric: 17.7780, val_loss: 18.2535, val_MinusLogProbMetric: 18.2535

Epoch 122: val_loss did not improve from 17.82974
196/196 - 54s - loss: 17.7780 - MinusLogProbMetric: 17.7780 - val_loss: 18.2535 - val_MinusLogProbMetric: 18.2535 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 123/1000
2023-10-10 14:29:14.599 
Epoch 123/1000 
	 loss: 27.3640, MinusLogProbMetric: 27.3640, val_loss: 29.4814, val_MinusLogProbMetric: 29.4814

Epoch 123: val_loss did not improve from 17.82974
196/196 - 53s - loss: 27.3640 - MinusLogProbMetric: 27.3640 - val_loss: 29.4814 - val_MinusLogProbMetric: 29.4814 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 124/1000
2023-10-10 14:30:07.256 
Epoch 124/1000 
	 loss: 26.3298, MinusLogProbMetric: 26.3298, val_loss: 23.7448, val_MinusLogProbMetric: 23.7448

Epoch 124: val_loss did not improve from 17.82974
196/196 - 53s - loss: 26.3298 - MinusLogProbMetric: 26.3298 - val_loss: 23.7448 - val_MinusLogProbMetric: 23.7448 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 125/1000
2023-10-10 14:30:59.947 
Epoch 125/1000 
	 loss: 24.0053, MinusLogProbMetric: 24.0053, val_loss: 23.7971, val_MinusLogProbMetric: 23.7971

Epoch 125: val_loss did not improve from 17.82974
196/196 - 53s - loss: 24.0053 - MinusLogProbMetric: 24.0053 - val_loss: 23.7971 - val_MinusLogProbMetric: 23.7971 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 126/1000
2023-10-10 14:31:53.489 
Epoch 126/1000 
	 loss: 26.9113, MinusLogProbMetric: 26.9113, val_loss: 26.0355, val_MinusLogProbMetric: 26.0355

Epoch 126: val_loss did not improve from 17.82974
196/196 - 54s - loss: 26.9113 - MinusLogProbMetric: 26.9113 - val_loss: 26.0355 - val_MinusLogProbMetric: 26.0355 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 127/1000
2023-10-10 14:32:47.148 
Epoch 127/1000 
	 loss: 24.9900, MinusLogProbMetric: 24.9900, val_loss: 24.1905, val_MinusLogProbMetric: 24.1905

Epoch 127: val_loss did not improve from 17.82974
196/196 - 54s - loss: 24.9900 - MinusLogProbMetric: 24.9900 - val_loss: 24.1905 - val_MinusLogProbMetric: 24.1905 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 128/1000
2023-10-10 14:33:40.601 
Epoch 128/1000 
	 loss: 23.5975, MinusLogProbMetric: 23.5975, val_loss: 23.1686, val_MinusLogProbMetric: 23.1686

Epoch 128: val_loss did not improve from 17.82974
196/196 - 53s - loss: 23.5975 - MinusLogProbMetric: 23.5975 - val_loss: 23.1686 - val_MinusLogProbMetric: 23.1686 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 129/1000
2023-10-10 14:34:32.960 
Epoch 129/1000 
	 loss: 22.7110, MinusLogProbMetric: 22.7110, val_loss: 22.4712, val_MinusLogProbMetric: 22.4712

Epoch 129: val_loss did not improve from 17.82974
196/196 - 52s - loss: 22.7110 - MinusLogProbMetric: 22.7110 - val_loss: 22.4712 - val_MinusLogProbMetric: 22.4712 - lr: 1.2346e-05 - 52s/epoch - 267ms/step
Epoch 130/1000
2023-10-10 14:35:25.901 
Epoch 130/1000 
	 loss: 22.1300, MinusLogProbMetric: 22.1300, val_loss: 22.0047, val_MinusLogProbMetric: 22.0047

Epoch 130: val_loss did not improve from 17.82974
196/196 - 53s - loss: 22.1300 - MinusLogProbMetric: 22.1300 - val_loss: 22.0047 - val_MinusLogProbMetric: 22.0047 - lr: 1.2346e-05 - 53s/epoch - 270ms/step
Epoch 131/1000
2023-10-10 14:36:18.663 
Epoch 131/1000 
	 loss: 21.7486, MinusLogProbMetric: 21.7486, val_loss: 21.5741, val_MinusLogProbMetric: 21.5741

Epoch 131: val_loss did not improve from 17.82974
196/196 - 53s - loss: 21.7486 - MinusLogProbMetric: 21.7486 - val_loss: 21.5741 - val_MinusLogProbMetric: 21.5741 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 132/1000
2023-10-10 14:37:11.831 
Epoch 132/1000 
	 loss: 21.3750, MinusLogProbMetric: 21.3750, val_loss: 21.2705, val_MinusLogProbMetric: 21.2705

Epoch 132: val_loss did not improve from 17.82974
196/196 - 53s - loss: 21.3750 - MinusLogProbMetric: 21.3750 - val_loss: 21.2705 - val_MinusLogProbMetric: 21.2705 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 133/1000
2023-10-10 14:38:05.503 
Epoch 133/1000 
	 loss: 21.0986, MinusLogProbMetric: 21.0986, val_loss: 21.0237, val_MinusLogProbMetric: 21.0237

Epoch 133: val_loss did not improve from 17.82974
196/196 - 54s - loss: 21.0986 - MinusLogProbMetric: 21.0986 - val_loss: 21.0237 - val_MinusLogProbMetric: 21.0237 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 134/1000
2023-10-10 14:38:58.225 
Epoch 134/1000 
	 loss: 20.8851, MinusLogProbMetric: 20.8851, val_loss: 20.8469, val_MinusLogProbMetric: 20.8469

Epoch 134: val_loss did not improve from 17.82974
196/196 - 53s - loss: 20.8851 - MinusLogProbMetric: 20.8851 - val_loss: 20.8469 - val_MinusLogProbMetric: 20.8469 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 135/1000
2023-10-10 14:39:51.392 
Epoch 135/1000 
	 loss: 20.6848, MinusLogProbMetric: 20.6848, val_loss: 20.6414, val_MinusLogProbMetric: 20.6414

Epoch 135: val_loss did not improve from 17.82974
196/196 - 53s - loss: 20.6848 - MinusLogProbMetric: 20.6848 - val_loss: 20.6414 - val_MinusLogProbMetric: 20.6414 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 136/1000
2023-10-10 14:40:44.483 
Epoch 136/1000 
	 loss: 21.3019, MinusLogProbMetric: 21.3019, val_loss: 22.3366, val_MinusLogProbMetric: 22.3366

Epoch 136: val_loss did not improve from 17.82974
196/196 - 53s - loss: 21.3019 - MinusLogProbMetric: 21.3019 - val_loss: 22.3366 - val_MinusLogProbMetric: 22.3366 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 137/1000
2023-10-10 14:41:37.554 
Epoch 137/1000 
	 loss: 20.6196, MinusLogProbMetric: 20.6196, val_loss: 20.4228, val_MinusLogProbMetric: 20.4228

Epoch 137: val_loss did not improve from 17.82974
196/196 - 53s - loss: 20.6196 - MinusLogProbMetric: 20.6196 - val_loss: 20.4228 - val_MinusLogProbMetric: 20.4228 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 138/1000
2023-10-10 14:42:31.274 
Epoch 138/1000 
	 loss: 20.3313, MinusLogProbMetric: 20.3313, val_loss: 20.3469, val_MinusLogProbMetric: 20.3469

Epoch 138: val_loss did not improve from 17.82974
196/196 - 54s - loss: 20.3313 - MinusLogProbMetric: 20.3313 - val_loss: 20.3469 - val_MinusLogProbMetric: 20.3469 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 139/1000
2023-10-10 14:43:25.029 
Epoch 139/1000 
	 loss: 20.4223, MinusLogProbMetric: 20.4223, val_loss: 20.1808, val_MinusLogProbMetric: 20.1808

Epoch 139: val_loss did not improve from 17.82974
196/196 - 54s - loss: 20.4223 - MinusLogProbMetric: 20.4223 - val_loss: 20.1808 - val_MinusLogProbMetric: 20.1808 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 140/1000
2023-10-10 14:44:18.939 
Epoch 140/1000 
	 loss: 20.1496, MinusLogProbMetric: 20.1496, val_loss: 20.1206, val_MinusLogProbMetric: 20.1206

Epoch 140: val_loss did not improve from 17.82974
196/196 - 54s - loss: 20.1496 - MinusLogProbMetric: 20.1496 - val_loss: 20.1206 - val_MinusLogProbMetric: 20.1206 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 141/1000
2023-10-10 14:45:12.241 
Epoch 141/1000 
	 loss: 20.0243, MinusLogProbMetric: 20.0243, val_loss: 20.0317, val_MinusLogProbMetric: 20.0317

Epoch 141: val_loss did not improve from 17.82974
196/196 - 53s - loss: 20.0243 - MinusLogProbMetric: 20.0243 - val_loss: 20.0317 - val_MinusLogProbMetric: 20.0317 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 142/1000
2023-10-10 14:46:05.779 
Epoch 142/1000 
	 loss: 19.9235, MinusLogProbMetric: 19.9235, val_loss: 19.9535, val_MinusLogProbMetric: 19.9535

Epoch 142: val_loss did not improve from 17.82974
196/196 - 54s - loss: 19.9235 - MinusLogProbMetric: 19.9235 - val_loss: 19.9535 - val_MinusLogProbMetric: 19.9535 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 143/1000
2023-10-10 14:46:59.925 
Epoch 143/1000 
	 loss: 19.8438, MinusLogProbMetric: 19.8438, val_loss: 20.0108, val_MinusLogProbMetric: 20.0108

Epoch 143: val_loss did not improve from 17.82974
196/196 - 54s - loss: 19.8438 - MinusLogProbMetric: 19.8438 - val_loss: 20.0108 - val_MinusLogProbMetric: 20.0108 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 144/1000
2023-10-10 14:47:53.363 
Epoch 144/1000 
	 loss: 19.8136, MinusLogProbMetric: 19.8136, val_loss: 19.9074, val_MinusLogProbMetric: 19.9074

Epoch 144: val_loss did not improve from 17.82974
196/196 - 53s - loss: 19.8136 - MinusLogProbMetric: 19.8136 - val_loss: 19.9074 - val_MinusLogProbMetric: 19.9074 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 145/1000
2023-10-10 14:48:48.708 
Epoch 145/1000 
	 loss: 19.9439, MinusLogProbMetric: 19.9439, val_loss: 20.1179, val_MinusLogProbMetric: 20.1179

Epoch 145: val_loss did not improve from 17.82974
196/196 - 55s - loss: 19.9439 - MinusLogProbMetric: 19.9439 - val_loss: 20.1179 - val_MinusLogProbMetric: 20.1179 - lr: 1.2346e-05 - 55s/epoch - 282ms/step
Epoch 146/1000
2023-10-10 14:49:41.916 
Epoch 146/1000 
	 loss: 20.1277, MinusLogProbMetric: 20.1277, val_loss: 20.2775, val_MinusLogProbMetric: 20.2775

Epoch 146: val_loss did not improve from 17.82974
196/196 - 53s - loss: 20.1277 - MinusLogProbMetric: 20.1277 - val_loss: 20.2775 - val_MinusLogProbMetric: 20.2775 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 147/1000
2023-10-10 14:50:35.009 
Epoch 147/1000 
	 loss: 19.7524, MinusLogProbMetric: 19.7524, val_loss: 19.8598, val_MinusLogProbMetric: 19.8598

Epoch 147: val_loss did not improve from 17.82974
196/196 - 53s - loss: 19.7524 - MinusLogProbMetric: 19.7524 - val_loss: 19.8598 - val_MinusLogProbMetric: 19.8598 - lr: 1.2346e-05 - 53s/epoch - 271ms/step
Epoch 148/1000
2023-10-10 14:51:28.655 
Epoch 148/1000 
	 loss: 19.6818, MinusLogProbMetric: 19.6818, val_loss: 19.7115, val_MinusLogProbMetric: 19.7115

Epoch 148: val_loss did not improve from 17.82974
196/196 - 54s - loss: 19.6818 - MinusLogProbMetric: 19.6818 - val_loss: 19.7115 - val_MinusLogProbMetric: 19.7115 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 149/1000
2023-10-10 14:52:22.265 
Epoch 149/1000 
	 loss: 19.5575, MinusLogProbMetric: 19.5575, val_loss: 19.6618, val_MinusLogProbMetric: 19.6618

Epoch 149: val_loss did not improve from 17.82974
196/196 - 54s - loss: 19.5575 - MinusLogProbMetric: 19.5575 - val_loss: 19.6618 - val_MinusLogProbMetric: 19.6618 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 150/1000
2023-10-10 14:53:15.131 
Epoch 150/1000 
	 loss: 21.3024, MinusLogProbMetric: 21.3024, val_loss: 21.0305, val_MinusLogProbMetric: 21.0305

Epoch 150: val_loss did not improve from 17.82974
196/196 - 53s - loss: 21.3024 - MinusLogProbMetric: 21.3024 - val_loss: 21.0305 - val_MinusLogProbMetric: 21.0305 - lr: 1.2346e-05 - 53s/epoch - 270ms/step
Epoch 151/1000
2023-10-10 14:54:09.042 
Epoch 151/1000 
	 loss: 20.5643, MinusLogProbMetric: 20.5643, val_loss: 19.9174, val_MinusLogProbMetric: 19.9174

Epoch 151: val_loss did not improve from 17.82974
196/196 - 54s - loss: 20.5643 - MinusLogProbMetric: 20.5643 - val_loss: 19.9174 - val_MinusLogProbMetric: 19.9174 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 152/1000
2023-10-10 14:55:02.485 
Epoch 152/1000 
	 loss: 19.6916, MinusLogProbMetric: 19.6916, val_loss: 19.5650, val_MinusLogProbMetric: 19.5650

Epoch 152: val_loss did not improve from 17.82974
196/196 - 53s - loss: 19.6916 - MinusLogProbMetric: 19.6916 - val_loss: 19.5650 - val_MinusLogProbMetric: 19.5650 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 153/1000
2023-10-10 14:55:56.056 
Epoch 153/1000 
	 loss: 19.4613, MinusLogProbMetric: 19.4613, val_loss: 19.3759, val_MinusLogProbMetric: 19.3759

Epoch 153: val_loss did not improve from 17.82974
196/196 - 54s - loss: 19.4613 - MinusLogProbMetric: 19.4613 - val_loss: 19.3759 - val_MinusLogProbMetric: 19.3759 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 154/1000
2023-10-10 14:56:49.756 
Epoch 154/1000 
	 loss: 19.2972, MinusLogProbMetric: 19.2972, val_loss: 19.2634, val_MinusLogProbMetric: 19.2634

Epoch 154: val_loss did not improve from 17.82974
196/196 - 54s - loss: 19.2972 - MinusLogProbMetric: 19.2972 - val_loss: 19.2634 - val_MinusLogProbMetric: 19.2634 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 155/1000
2023-10-10 14:57:41.160 
Epoch 155/1000 
	 loss: 19.1832, MinusLogProbMetric: 19.1832, val_loss: 19.1916, val_MinusLogProbMetric: 19.1916

Epoch 155: val_loss did not improve from 17.82974
196/196 - 51s - loss: 19.1832 - MinusLogProbMetric: 19.1832 - val_loss: 19.1916 - val_MinusLogProbMetric: 19.1916 - lr: 1.2346e-05 - 51s/epoch - 262ms/step
Epoch 156/1000
2023-10-10 14:58:31.898 
Epoch 156/1000 
	 loss: 19.1280, MinusLogProbMetric: 19.1280, val_loss: 19.0503, val_MinusLogProbMetric: 19.0503

Epoch 156: val_loss did not improve from 17.82974
196/196 - 51s - loss: 19.1280 - MinusLogProbMetric: 19.1280 - val_loss: 19.0503 - val_MinusLogProbMetric: 19.0503 - lr: 1.2346e-05 - 51s/epoch - 259ms/step
Epoch 157/1000
2023-10-10 14:59:22.672 
Epoch 157/1000 
	 loss: 18.9555, MinusLogProbMetric: 18.9555, val_loss: 18.9464, val_MinusLogProbMetric: 18.9464

Epoch 157: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.9555 - MinusLogProbMetric: 18.9555 - val_loss: 18.9464 - val_MinusLogProbMetric: 18.9464 - lr: 1.2346e-05 - 51s/epoch - 259ms/step
Epoch 158/1000
2023-10-10 15:00:14.765 
Epoch 158/1000 
	 loss: 18.9103, MinusLogProbMetric: 18.9103, val_loss: 18.8499, val_MinusLogProbMetric: 18.8499

Epoch 158: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.9103 - MinusLogProbMetric: 18.9103 - val_loss: 18.8499 - val_MinusLogProbMetric: 18.8499 - lr: 1.2346e-05 - 52s/epoch - 266ms/step
Epoch 159/1000
2023-10-10 15:01:06.152 
Epoch 159/1000 
	 loss: 18.7841, MinusLogProbMetric: 18.7841, val_loss: 18.9449, val_MinusLogProbMetric: 18.9449

Epoch 159: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.7841 - MinusLogProbMetric: 18.7841 - val_loss: 18.9449 - val_MinusLogProbMetric: 18.9449 - lr: 1.2346e-05 - 51s/epoch - 262ms/step
Epoch 160/1000
2023-10-10 15:01:57.372 
Epoch 160/1000 
	 loss: 18.7497, MinusLogProbMetric: 18.7497, val_loss: 18.7756, val_MinusLogProbMetric: 18.7756

Epoch 160: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.7497 - MinusLogProbMetric: 18.7497 - val_loss: 18.7756 - val_MinusLogProbMetric: 18.7756 - lr: 1.2346e-05 - 51s/epoch - 261ms/step
Epoch 161/1000
2023-10-10 15:02:49.062 
Epoch 161/1000 
	 loss: 18.6900, MinusLogProbMetric: 18.6900, val_loss: 18.7197, val_MinusLogProbMetric: 18.7197

Epoch 161: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.6900 - MinusLogProbMetric: 18.6900 - val_loss: 18.7197 - val_MinusLogProbMetric: 18.7197 - lr: 1.2346e-05 - 52s/epoch - 264ms/step
Epoch 162/1000
2023-10-10 15:03:41.511 
Epoch 162/1000 
	 loss: 18.6669, MinusLogProbMetric: 18.6669, val_loss: 18.7631, val_MinusLogProbMetric: 18.7631

Epoch 162: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.6669 - MinusLogProbMetric: 18.6669 - val_loss: 18.7631 - val_MinusLogProbMetric: 18.7631 - lr: 1.2346e-05 - 53s/epoch - 268ms/step
Epoch 163/1000
2023-10-10 15:04:33.328 
Epoch 163/1000 
	 loss: 18.6255, MinusLogProbMetric: 18.6255, val_loss: 18.6304, val_MinusLogProbMetric: 18.6304

Epoch 163: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.6255 - MinusLogProbMetric: 18.6255 - val_loss: 18.6304 - val_MinusLogProbMetric: 18.6304 - lr: 1.2346e-05 - 52s/epoch - 264ms/step
Epoch 164/1000
2023-10-10 15:05:24.348 
Epoch 164/1000 
	 loss: 18.5871, MinusLogProbMetric: 18.5871, val_loss: 18.6186, val_MinusLogProbMetric: 18.6186

Epoch 164: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.5871 - MinusLogProbMetric: 18.5871 - val_loss: 18.6186 - val_MinusLogProbMetric: 18.6186 - lr: 1.2346e-05 - 51s/epoch - 260ms/step
Epoch 165/1000
2023-10-10 15:06:15.126 
Epoch 165/1000 
	 loss: 18.5584, MinusLogProbMetric: 18.5584, val_loss: 18.5872, val_MinusLogProbMetric: 18.5872

Epoch 165: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.5584 - MinusLogProbMetric: 18.5584 - val_loss: 18.5872 - val_MinusLogProbMetric: 18.5872 - lr: 1.2346e-05 - 51s/epoch - 259ms/step
Epoch 166/1000
2023-10-10 15:07:05.653 
Epoch 166/1000 
	 loss: 18.5302, MinusLogProbMetric: 18.5302, val_loss: 18.5491, val_MinusLogProbMetric: 18.5491

Epoch 166: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.5302 - MinusLogProbMetric: 18.5302 - val_loss: 18.5491 - val_MinusLogProbMetric: 18.5491 - lr: 1.2346e-05 - 51s/epoch - 258ms/step
Epoch 167/1000
2023-10-10 15:07:57.462 
Epoch 167/1000 
	 loss: 18.5050, MinusLogProbMetric: 18.5050, val_loss: 18.5712, val_MinusLogProbMetric: 18.5712

Epoch 167: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.5050 - MinusLogProbMetric: 18.5050 - val_loss: 18.5712 - val_MinusLogProbMetric: 18.5712 - lr: 1.2346e-05 - 52s/epoch - 264ms/step
Epoch 168/1000
2023-10-10 15:08:48.850 
Epoch 168/1000 
	 loss: 18.4887, MinusLogProbMetric: 18.4887, val_loss: 18.5252, val_MinusLogProbMetric: 18.5252

Epoch 168: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.4887 - MinusLogProbMetric: 18.4887 - val_loss: 18.5252 - val_MinusLogProbMetric: 18.5252 - lr: 1.2346e-05 - 51s/epoch - 262ms/step
Epoch 169/1000
2023-10-10 15:09:40.111 
Epoch 169/1000 
	 loss: 18.4580, MinusLogProbMetric: 18.4580, val_loss: 18.4979, val_MinusLogProbMetric: 18.4979

Epoch 169: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.4580 - MinusLogProbMetric: 18.4580 - val_loss: 18.4979 - val_MinusLogProbMetric: 18.4979 - lr: 1.2346e-05 - 51s/epoch - 262ms/step
Epoch 170/1000
2023-10-10 15:10:31.648 
Epoch 170/1000 
	 loss: 18.4401, MinusLogProbMetric: 18.4401, val_loss: 18.4744, val_MinusLogProbMetric: 18.4744

Epoch 170: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.4401 - MinusLogProbMetric: 18.4401 - val_loss: 18.4744 - val_MinusLogProbMetric: 18.4744 - lr: 1.2346e-05 - 52s/epoch - 263ms/step
Epoch 171/1000
2023-10-10 15:11:24.052 
Epoch 171/1000 
	 loss: 18.4158, MinusLogProbMetric: 18.4158, val_loss: 18.4879, val_MinusLogProbMetric: 18.4879

Epoch 171: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.4158 - MinusLogProbMetric: 18.4158 - val_loss: 18.4879 - val_MinusLogProbMetric: 18.4879 - lr: 1.2346e-05 - 52s/epoch - 267ms/step
Epoch 172/1000
2023-10-10 15:12:16.016 
Epoch 172/1000 
	 loss: 18.3614, MinusLogProbMetric: 18.3614, val_loss: 18.4231, val_MinusLogProbMetric: 18.4231

Epoch 172: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3614 - MinusLogProbMetric: 18.3614 - val_loss: 18.4231 - val_MinusLogProbMetric: 18.4231 - lr: 6.1728e-06 - 52s/epoch - 265ms/step
Epoch 173/1000
2023-10-10 15:13:07.390 
Epoch 173/1000 
	 loss: 18.3482, MinusLogProbMetric: 18.3482, val_loss: 18.3872, val_MinusLogProbMetric: 18.3872

Epoch 173: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.3482 - MinusLogProbMetric: 18.3482 - val_loss: 18.3872 - val_MinusLogProbMetric: 18.3872 - lr: 6.1728e-06 - 51s/epoch - 262ms/step
Epoch 174/1000
2023-10-10 15:13:58.790 
Epoch 174/1000 
	 loss: 18.3339, MinusLogProbMetric: 18.3339, val_loss: 18.3803, val_MinusLogProbMetric: 18.3803

Epoch 174: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.3339 - MinusLogProbMetric: 18.3339 - val_loss: 18.3803 - val_MinusLogProbMetric: 18.3803 - lr: 6.1728e-06 - 51s/epoch - 262ms/step
Epoch 175/1000
2023-10-10 15:14:50.808 
Epoch 175/1000 
	 loss: 18.3283, MinusLogProbMetric: 18.3283, val_loss: 18.3790, val_MinusLogProbMetric: 18.3790

Epoch 175: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3283 - MinusLogProbMetric: 18.3283 - val_loss: 18.3790 - val_MinusLogProbMetric: 18.3790 - lr: 6.1728e-06 - 52s/epoch - 265ms/step
Epoch 176/1000
2023-10-10 15:15:43.100 
Epoch 176/1000 
	 loss: 18.3190, MinusLogProbMetric: 18.3190, val_loss: 18.3626, val_MinusLogProbMetric: 18.3626

Epoch 176: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3190 - MinusLogProbMetric: 18.3190 - val_loss: 18.3626 - val_MinusLogProbMetric: 18.3626 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 177/1000
2023-10-10 15:16:35.182 
Epoch 177/1000 
	 loss: 18.3072, MinusLogProbMetric: 18.3072, val_loss: 18.3515, val_MinusLogProbMetric: 18.3515

Epoch 177: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3072 - MinusLogProbMetric: 18.3072 - val_loss: 18.3515 - val_MinusLogProbMetric: 18.3515 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 178/1000
2023-10-10 15:17:27.288 
Epoch 178/1000 
	 loss: 18.2997, MinusLogProbMetric: 18.2997, val_loss: 18.3595, val_MinusLogProbMetric: 18.3595

Epoch 178: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.2997 - MinusLogProbMetric: 18.2997 - val_loss: 18.3595 - val_MinusLogProbMetric: 18.3595 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 179/1000
2023-10-10 15:18:19.728 
Epoch 179/1000 
	 loss: 18.2927, MinusLogProbMetric: 18.2927, val_loss: 18.3251, val_MinusLogProbMetric: 18.3251

Epoch 179: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.2927 - MinusLogProbMetric: 18.2927 - val_loss: 18.3251 - val_MinusLogProbMetric: 18.3251 - lr: 6.1728e-06 - 52s/epoch - 268ms/step
Epoch 180/1000
2023-10-10 15:19:12.371 
Epoch 180/1000 
	 loss: 18.2896, MinusLogProbMetric: 18.2896, val_loss: 18.3201, val_MinusLogProbMetric: 18.3201

Epoch 180: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.2896 - MinusLogProbMetric: 18.2896 - val_loss: 18.3201 - val_MinusLogProbMetric: 18.3201 - lr: 6.1728e-06 - 53s/epoch - 268ms/step
Epoch 181/1000
2023-10-10 15:20:04.568 
Epoch 181/1000 
	 loss: 18.6886, MinusLogProbMetric: 18.6886, val_loss: 24.9336, val_MinusLogProbMetric: 24.9336

Epoch 181: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.6886 - MinusLogProbMetric: 18.6886 - val_loss: 24.9336 - val_MinusLogProbMetric: 24.9336 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 182/1000
2023-10-10 15:20:56.925 
Epoch 182/1000 
	 loss: 20.0379, MinusLogProbMetric: 20.0379, val_loss: 19.2627, val_MinusLogProbMetric: 19.2627

Epoch 182: val_loss did not improve from 17.82974
196/196 - 52s - loss: 20.0379 - MinusLogProbMetric: 20.0379 - val_loss: 19.2627 - val_MinusLogProbMetric: 19.2627 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 183/1000
2023-10-10 15:21:48.441 
Epoch 183/1000 
	 loss: 19.0392, MinusLogProbMetric: 19.0392, val_loss: 18.9303, val_MinusLogProbMetric: 18.9303

Epoch 183: val_loss did not improve from 17.82974
196/196 - 52s - loss: 19.0392 - MinusLogProbMetric: 19.0392 - val_loss: 18.9303 - val_MinusLogProbMetric: 18.9303 - lr: 6.1728e-06 - 52s/epoch - 263ms/step
Epoch 184/1000
2023-10-10 15:22:41.295 
Epoch 184/1000 
	 loss: 18.8198, MinusLogProbMetric: 18.8198, val_loss: 18.8296, val_MinusLogProbMetric: 18.8296

Epoch 184: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.8198 - MinusLogProbMetric: 18.8198 - val_loss: 18.8296 - val_MinusLogProbMetric: 18.8296 - lr: 6.1728e-06 - 53s/epoch - 270ms/step
Epoch 185/1000
2023-10-10 15:23:32.914 
Epoch 185/1000 
	 loss: 18.6846, MinusLogProbMetric: 18.6846, val_loss: 18.6939, val_MinusLogProbMetric: 18.6939

Epoch 185: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.6846 - MinusLogProbMetric: 18.6846 - val_loss: 18.6939 - val_MinusLogProbMetric: 18.6939 - lr: 6.1728e-06 - 52s/epoch - 263ms/step
Epoch 186/1000
2023-10-10 15:24:24.204 
Epoch 186/1000 
	 loss: 18.5889, MinusLogProbMetric: 18.5889, val_loss: 18.6078, val_MinusLogProbMetric: 18.6078

Epoch 186: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.5889 - MinusLogProbMetric: 18.5889 - val_loss: 18.6078 - val_MinusLogProbMetric: 18.6078 - lr: 6.1728e-06 - 51s/epoch - 262ms/step
Epoch 187/1000
2023-10-10 15:25:16.883 
Epoch 187/1000 
	 loss: 18.5328, MinusLogProbMetric: 18.5328, val_loss: 18.5407, val_MinusLogProbMetric: 18.5407

Epoch 187: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.5328 - MinusLogProbMetric: 18.5328 - val_loss: 18.5407 - val_MinusLogProbMetric: 18.5407 - lr: 6.1728e-06 - 53s/epoch - 269ms/step
Epoch 188/1000
2023-10-10 15:26:09.873 
Epoch 188/1000 
	 loss: 18.4673, MinusLogProbMetric: 18.4673, val_loss: 18.5121, val_MinusLogProbMetric: 18.5121

Epoch 188: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.4673 - MinusLogProbMetric: 18.4673 - val_loss: 18.5121 - val_MinusLogProbMetric: 18.5121 - lr: 6.1728e-06 - 53s/epoch - 270ms/step
Epoch 189/1000
2023-10-10 15:27:02.185 
Epoch 189/1000 
	 loss: 18.5310, MinusLogProbMetric: 18.5310, val_loss: 18.4375, val_MinusLogProbMetric: 18.4375

Epoch 189: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.5310 - MinusLogProbMetric: 18.5310 - val_loss: 18.4375 - val_MinusLogProbMetric: 18.4375 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 190/1000
2023-10-10 15:27:53.350 
Epoch 190/1000 
	 loss: 18.3953, MinusLogProbMetric: 18.3953, val_loss: 18.4252, val_MinusLogProbMetric: 18.4252

Epoch 190: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.3953 - MinusLogProbMetric: 18.3953 - val_loss: 18.4252 - val_MinusLogProbMetric: 18.4252 - lr: 6.1728e-06 - 51s/epoch - 261ms/step
Epoch 191/1000
2023-10-10 15:28:45.361 
Epoch 191/1000 
	 loss: 33.2928, MinusLogProbMetric: 33.2928, val_loss: 23.8916, val_MinusLogProbMetric: 23.8916

Epoch 191: val_loss did not improve from 17.82974
196/196 - 52s - loss: 33.2928 - MinusLogProbMetric: 33.2928 - val_loss: 23.8916 - val_MinusLogProbMetric: 23.8916 - lr: 6.1728e-06 - 52s/epoch - 265ms/step
Epoch 192/1000
2023-10-10 15:29:37.215 
Epoch 192/1000 
	 loss: 23.2835, MinusLogProbMetric: 23.2835, val_loss: 22.7799, val_MinusLogProbMetric: 22.7799

Epoch 192: val_loss did not improve from 17.82974
196/196 - 52s - loss: 23.2835 - MinusLogProbMetric: 23.2835 - val_loss: 22.7799 - val_MinusLogProbMetric: 22.7799 - lr: 6.1728e-06 - 52s/epoch - 265ms/step
Epoch 193/1000
2023-10-10 15:30:30.077 
Epoch 193/1000 
	 loss: 22.6334, MinusLogProbMetric: 22.6334, val_loss: 22.0601, val_MinusLogProbMetric: 22.0601

Epoch 193: val_loss did not improve from 17.82974
196/196 - 53s - loss: 22.6334 - MinusLogProbMetric: 22.6334 - val_loss: 22.0601 - val_MinusLogProbMetric: 22.0601 - lr: 6.1728e-06 - 53s/epoch - 270ms/step
Epoch 194/1000
2023-10-10 15:31:21.351 
Epoch 194/1000 
	 loss: 21.3786, MinusLogProbMetric: 21.3786, val_loss: 20.8901, val_MinusLogProbMetric: 20.8901

Epoch 194: val_loss did not improve from 17.82974
196/196 - 51s - loss: 21.3786 - MinusLogProbMetric: 21.3786 - val_loss: 20.8901 - val_MinusLogProbMetric: 20.8901 - lr: 6.1728e-06 - 51s/epoch - 261ms/step
Epoch 195/1000
2023-10-10 15:32:13.758 
Epoch 195/1000 
	 loss: 20.1245, MinusLogProbMetric: 20.1245, val_loss: 19.5407, val_MinusLogProbMetric: 19.5407

Epoch 195: val_loss did not improve from 17.82974
196/196 - 52s - loss: 20.1245 - MinusLogProbMetric: 20.1245 - val_loss: 19.5407 - val_MinusLogProbMetric: 19.5407 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 196/1000
2023-10-10 15:33:05.294 
Epoch 196/1000 
	 loss: 19.3451, MinusLogProbMetric: 19.3451, val_loss: 19.2447, val_MinusLogProbMetric: 19.2447

Epoch 196: val_loss did not improve from 17.82974
196/196 - 52s - loss: 19.3451 - MinusLogProbMetric: 19.3451 - val_loss: 19.2447 - val_MinusLogProbMetric: 19.2447 - lr: 6.1728e-06 - 52s/epoch - 263ms/step
Epoch 197/1000
2023-10-10 15:33:57.441 
Epoch 197/1000 
	 loss: 19.3994, MinusLogProbMetric: 19.3994, val_loss: 19.2532, val_MinusLogProbMetric: 19.2532

Epoch 197: val_loss did not improve from 17.82974
196/196 - 52s - loss: 19.3994 - MinusLogProbMetric: 19.3994 - val_loss: 19.2532 - val_MinusLogProbMetric: 19.2532 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 198/1000
2023-10-10 15:34:50.682 
Epoch 198/1000 
	 loss: 19.0670, MinusLogProbMetric: 19.0670, val_loss: 19.0223, val_MinusLogProbMetric: 19.0223

Epoch 198: val_loss did not improve from 17.82974
196/196 - 53s - loss: 19.0670 - MinusLogProbMetric: 19.0670 - val_loss: 19.0223 - val_MinusLogProbMetric: 19.0223 - lr: 6.1728e-06 - 53s/epoch - 271ms/step
Epoch 199/1000
2023-10-10 15:35:42.915 
Epoch 199/1000 
	 loss: 18.9216, MinusLogProbMetric: 18.9216, val_loss: 18.9126, val_MinusLogProbMetric: 18.9126

Epoch 199: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.9216 - MinusLogProbMetric: 18.9216 - val_loss: 18.9126 - val_MinusLogProbMetric: 18.9126 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 200/1000
2023-10-10 15:36:34.627 
Epoch 200/1000 
	 loss: 18.8186, MinusLogProbMetric: 18.8186, val_loss: 18.8195, val_MinusLogProbMetric: 18.8195

Epoch 200: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.8186 - MinusLogProbMetric: 18.8186 - val_loss: 18.8195 - val_MinusLogProbMetric: 18.8195 - lr: 6.1728e-06 - 52s/epoch - 264ms/step
Epoch 201/1000
2023-10-10 15:37:25.753 
Epoch 201/1000 
	 loss: 18.7351, MinusLogProbMetric: 18.7351, val_loss: 18.7360, val_MinusLogProbMetric: 18.7360

Epoch 201: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.7351 - MinusLogProbMetric: 18.7351 - val_loss: 18.7360 - val_MinusLogProbMetric: 18.7360 - lr: 6.1728e-06 - 51s/epoch - 261ms/step
Epoch 202/1000
2023-10-10 15:38:17.987 
Epoch 202/1000 
	 loss: 18.6621, MinusLogProbMetric: 18.6621, val_loss: 18.6675, val_MinusLogProbMetric: 18.6675

Epoch 202: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.6621 - MinusLogProbMetric: 18.6621 - val_loss: 18.6675 - val_MinusLogProbMetric: 18.6675 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 203/1000
2023-10-10 15:39:10.654 
Epoch 203/1000 
	 loss: 18.5964, MinusLogProbMetric: 18.5964, val_loss: 18.6024, val_MinusLogProbMetric: 18.6024

Epoch 203: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.5964 - MinusLogProbMetric: 18.5964 - val_loss: 18.6024 - val_MinusLogProbMetric: 18.6024 - lr: 6.1728e-06 - 53s/epoch - 269ms/step
Epoch 204/1000
2023-10-10 15:40:01.990 
Epoch 204/1000 
	 loss: 18.5348, MinusLogProbMetric: 18.5348, val_loss: 18.5551, val_MinusLogProbMetric: 18.5551

Epoch 204: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.5348 - MinusLogProbMetric: 18.5348 - val_loss: 18.5551 - val_MinusLogProbMetric: 18.5551 - lr: 6.1728e-06 - 51s/epoch - 262ms/step
Epoch 205/1000
2023-10-10 15:40:53.799 
Epoch 205/1000 
	 loss: 18.4861, MinusLogProbMetric: 18.4861, val_loss: 18.5056, val_MinusLogProbMetric: 18.5056

Epoch 205: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.4861 - MinusLogProbMetric: 18.4861 - val_loss: 18.5056 - val_MinusLogProbMetric: 18.5056 - lr: 6.1728e-06 - 52s/epoch - 264ms/step
Epoch 206/1000
2023-10-10 15:41:46.149 
Epoch 206/1000 
	 loss: 18.4397, MinusLogProbMetric: 18.4397, val_loss: 18.4670, val_MinusLogProbMetric: 18.4670

Epoch 206: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.4397 - MinusLogProbMetric: 18.4397 - val_loss: 18.4670 - val_MinusLogProbMetric: 18.4670 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 207/1000
2023-10-10 15:42:38.618 
Epoch 207/1000 
	 loss: 18.3968, MinusLogProbMetric: 18.3968, val_loss: 18.4260, val_MinusLogProbMetric: 18.4260

Epoch 207: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3968 - MinusLogProbMetric: 18.3968 - val_loss: 18.4260 - val_MinusLogProbMetric: 18.4260 - lr: 6.1728e-06 - 52s/epoch - 268ms/step
Epoch 208/1000
2023-10-10 15:43:30.455 
Epoch 208/1000 
	 loss: 18.3637, MinusLogProbMetric: 18.3637, val_loss: 18.3927, val_MinusLogProbMetric: 18.3927

Epoch 208: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3637 - MinusLogProbMetric: 18.3637 - val_loss: 18.3927 - val_MinusLogProbMetric: 18.3927 - lr: 6.1728e-06 - 52s/epoch - 264ms/step
Epoch 209/1000
2023-10-10 15:44:22.527 
Epoch 209/1000 
	 loss: 18.3332, MinusLogProbMetric: 18.3332, val_loss: 18.3683, val_MinusLogProbMetric: 18.3683

Epoch 209: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3332 - MinusLogProbMetric: 18.3332 - val_loss: 18.3683 - val_MinusLogProbMetric: 18.3683 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 210/1000
2023-10-10 15:45:14.898 
Epoch 210/1000 
	 loss: 18.3066, MinusLogProbMetric: 18.3066, val_loss: 18.3432, val_MinusLogProbMetric: 18.3432

Epoch 210: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.3066 - MinusLogProbMetric: 18.3066 - val_loss: 18.3432 - val_MinusLogProbMetric: 18.3432 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 211/1000
2023-10-10 15:46:07.054 
Epoch 211/1000 
	 loss: 18.2809, MinusLogProbMetric: 18.2809, val_loss: 18.3169, val_MinusLogProbMetric: 18.3169

Epoch 211: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.2809 - MinusLogProbMetric: 18.2809 - val_loss: 18.3169 - val_MinusLogProbMetric: 18.3169 - lr: 6.1728e-06 - 52s/epoch - 266ms/step
Epoch 212/1000
2023-10-10 15:46:59.649 
Epoch 212/1000 
	 loss: 18.2601, MinusLogProbMetric: 18.2601, val_loss: 18.2944, val_MinusLogProbMetric: 18.2944

Epoch 212: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.2601 - MinusLogProbMetric: 18.2601 - val_loss: 18.2944 - val_MinusLogProbMetric: 18.2944 - lr: 6.1728e-06 - 53s/epoch - 268ms/step
Epoch 213/1000
2023-10-10 15:47:51.128 
Epoch 213/1000 
	 loss: 18.2427, MinusLogProbMetric: 18.2427, val_loss: 18.2856, val_MinusLogProbMetric: 18.2856

Epoch 213: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.2427 - MinusLogProbMetric: 18.2427 - val_loss: 18.2856 - val_MinusLogProbMetric: 18.2856 - lr: 6.1728e-06 - 51s/epoch - 263ms/step
Epoch 214/1000
2023-10-10 15:48:43.467 
Epoch 214/1000 
	 loss: 18.2249, MinusLogProbMetric: 18.2249, val_loss: 18.2630, val_MinusLogProbMetric: 18.2630

Epoch 214: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.2249 - MinusLogProbMetric: 18.2249 - val_loss: 18.2630 - val_MinusLogProbMetric: 18.2630 - lr: 6.1728e-06 - 52s/epoch - 267ms/step
Epoch 215/1000
2023-10-10 15:49:36.097 
Epoch 215/1000 
	 loss: 18.2107, MinusLogProbMetric: 18.2107, val_loss: 18.2529, val_MinusLogProbMetric: 18.2529

Epoch 215: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.2107 - MinusLogProbMetric: 18.2107 - val_loss: 18.2529 - val_MinusLogProbMetric: 18.2529 - lr: 6.1728e-06 - 53s/epoch - 269ms/step
Epoch 216/1000
2023-10-10 15:50:27.863 
Epoch 216/1000 
	 loss: 18.1950, MinusLogProbMetric: 18.1950, val_loss: 18.2432, val_MinusLogProbMetric: 18.2432

Epoch 216: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.1950 - MinusLogProbMetric: 18.1950 - val_loss: 18.2432 - val_MinusLogProbMetric: 18.2432 - lr: 6.1728e-06 - 52s/epoch - 264ms/step
Epoch 217/1000
2023-10-10 15:51:19.150 
Epoch 217/1000 
	 loss: 18.1872, MinusLogProbMetric: 18.1872, val_loss: 18.2252, val_MinusLogProbMetric: 18.2252

Epoch 217: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.1872 - MinusLogProbMetric: 18.1872 - val_loss: 18.2252 - val_MinusLogProbMetric: 18.2252 - lr: 6.1728e-06 - 51s/epoch - 262ms/step
Epoch 218/1000
2023-10-10 15:52:10.529 
Epoch 218/1000 
	 loss: 18.1732, MinusLogProbMetric: 18.1732, val_loss: 18.2208, val_MinusLogProbMetric: 18.2208

Epoch 218: val_loss did not improve from 17.82974
196/196 - 51s - loss: 18.1732 - MinusLogProbMetric: 18.1732 - val_loss: 18.2208 - val_MinusLogProbMetric: 18.2208 - lr: 6.1728e-06 - 51s/epoch - 262ms/step
Epoch 219/1000
2023-10-10 15:53:03.161 
Epoch 219/1000 
	 loss: 18.1629, MinusLogProbMetric: 18.1629, val_loss: 18.2092, val_MinusLogProbMetric: 18.2092

Epoch 219: val_loss did not improve from 17.82974
196/196 - 53s - loss: 18.1629 - MinusLogProbMetric: 18.1629 - val_loss: 18.2092 - val_MinusLogProbMetric: 18.2092 - lr: 6.1728e-06 - 53s/epoch - 269ms/step
Epoch 220/1000
2023-10-10 15:53:55.194 
Epoch 220/1000 
	 loss: 18.1531, MinusLogProbMetric: 18.1531, val_loss: 18.2010, val_MinusLogProbMetric: 18.2010

Epoch 220: val_loss did not improve from 17.82974
196/196 - 52s - loss: 18.1531 - MinusLogProbMetric: 18.1531 - val_loss: 18.2010 - val_MinusLogProbMetric: 18.2010 - lr: 6.1728e-06 - 52s/epoch - 265ms/step
Epoch 221/1000
2023-10-10 15:54:47.583 
Epoch 221/1000 
	 loss: 18.1429, MinusLogProbMetric: 18.1429, val_loss: 18.1992, val_MinusLogProbMetric: 18.1992

Epoch 221: val_loss did not improve from 17.82974
Restoring model weights from the end of the best epoch: 121.
196/196 - 53s - loss: 18.1429 - MinusLogProbMetric: 18.1429 - val_loss: 18.1992 - val_MinusLogProbMetric: 18.1992 - lr: 6.1728e-06 - 53s/epoch - 270ms/step
Epoch 221: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 20.623568810988218 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 11.072194827953354 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 8.42185040190816 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 9.402961870189756 seconds.
Training succeeded with seed 926.
Model trained in 11918.15 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 50.74 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 50.94 s.
===========
Run 309/720 done in 19743.67 s.
===========

===========
Generating train data for run 310.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_310
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  2305120   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f3c6f13bcd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c6f8add80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c6f8add80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c6fd77b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c6f16ee60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c6f16f3d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c6f16f490>, <keras.callbacks.EarlyStopping object at 0x7f3c6f16f700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c6f16f730>, <keras.callbacks.TerminateOnNaN object at 0x7f3c6f16f370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_310/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 310/720 with hyperparameters:
timestamp = 2023-10-10 15:55:46.445530
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 50: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 15:58:07.690 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1188.5935, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 1188.5935 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 141s/epoch - 720ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 310.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_310
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  2305120   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f3d1410ea70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3df43c5870>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3df43c5870>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c8ca16080>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3abdc724a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3abdc72a10>, <keras.callbacks.ModelCheckpoint object at 0x7f3abdc72ad0>, <keras.callbacks.EarlyStopping object at 0x7f3abdc72d40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3abdc72d70>, <keras.callbacks.TerminateOnNaN object at 0x7f3abdc729b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_310/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 310/720 with hyperparameters:
timestamp = 2023-10-10 15:58:14.224457
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-10-10 16:01:07.120 
Epoch 1/1000 
	 loss: 300.7418, MinusLogProbMetric: 300.7418, val_loss: 82.6179, val_MinusLogProbMetric: 82.6179

Epoch 1: val_loss improved from inf to 82.61790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 174s - loss: 300.7418 - MinusLogProbMetric: 300.7418 - val_loss: 82.6179 - val_MinusLogProbMetric: 82.6179 - lr: 3.3333e-04 - 174s/epoch - 886ms/step
Epoch 2/1000
2023-10-10 16:02:00.119 
Epoch 2/1000 
	 loss: 56.6022, MinusLogProbMetric: 56.6022, val_loss: 45.3248, val_MinusLogProbMetric: 45.3248

Epoch 2: val_loss improved from 82.61790 to 45.32484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 56.6022 - MinusLogProbMetric: 56.6022 - val_loss: 45.3248 - val_MinusLogProbMetric: 45.3248 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 3/1000
2023-10-10 16:02:52.915 
Epoch 3/1000 
	 loss: 39.0979, MinusLogProbMetric: 39.0979, val_loss: 37.4782, val_MinusLogProbMetric: 37.4782

Epoch 3: val_loss improved from 45.32484 to 37.47818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 39.0979 - MinusLogProbMetric: 39.0979 - val_loss: 37.4782 - val_MinusLogProbMetric: 37.4782 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 4/1000
2023-10-10 16:03:45.340 
Epoch 4/1000 
	 loss: 33.0678, MinusLogProbMetric: 33.0678, val_loss: 33.6918, val_MinusLogProbMetric: 33.6918

Epoch 4: val_loss improved from 37.47818 to 33.69185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 33.0678 - MinusLogProbMetric: 33.0678 - val_loss: 33.6918 - val_MinusLogProbMetric: 33.6918 - lr: 3.3333e-04 - 53s/epoch - 268ms/step
Epoch 5/1000
2023-10-10 16:04:38.867 
Epoch 5/1000 
	 loss: 29.8769, MinusLogProbMetric: 29.8769, val_loss: 29.4378, val_MinusLogProbMetric: 29.4378

Epoch 5: val_loss improved from 33.69185 to 29.43782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 29.8769 - MinusLogProbMetric: 29.8769 - val_loss: 29.4378 - val_MinusLogProbMetric: 29.4378 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 6/1000
2023-10-10 16:05:31.309 
Epoch 6/1000 
	 loss: 27.7214, MinusLogProbMetric: 27.7214, val_loss: 28.2797, val_MinusLogProbMetric: 28.2797

Epoch 6: val_loss improved from 29.43782 to 28.27974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 52s - loss: 27.7214 - MinusLogProbMetric: 27.7214 - val_loss: 28.2797 - val_MinusLogProbMetric: 28.2797 - lr: 3.3333e-04 - 52s/epoch - 268ms/step
Epoch 7/1000
2023-10-10 16:06:24.504 
Epoch 7/1000 
	 loss: 26.3997, MinusLogProbMetric: 26.3997, val_loss: 25.5851, val_MinusLogProbMetric: 25.5851

Epoch 7: val_loss improved from 28.27974 to 25.58509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 26.3997 - MinusLogProbMetric: 26.3997 - val_loss: 25.5851 - val_MinusLogProbMetric: 25.5851 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 8/1000
2023-10-10 16:07:18.426 
Epoch 8/1000 
	 loss: 25.2657, MinusLogProbMetric: 25.2657, val_loss: 28.1992, val_MinusLogProbMetric: 28.1992

Epoch 8: val_loss did not improve from 25.58509
196/196 - 53s - loss: 25.2657 - MinusLogProbMetric: 25.2657 - val_loss: 28.1992 - val_MinusLogProbMetric: 28.1992 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 9/1000
2023-10-10 16:08:10.293 
Epoch 9/1000 
	 loss: 24.9155, MinusLogProbMetric: 24.9155, val_loss: 24.4053, val_MinusLogProbMetric: 24.4053

Epoch 9: val_loss improved from 25.58509 to 24.40527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 24.9155 - MinusLogProbMetric: 24.9155 - val_loss: 24.4053 - val_MinusLogProbMetric: 24.4053 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 10/1000
2023-10-10 16:09:03.213 
Epoch 10/1000 
	 loss: 23.8115, MinusLogProbMetric: 23.8115, val_loss: 24.1797, val_MinusLogProbMetric: 24.1797

Epoch 10: val_loss improved from 24.40527 to 24.17972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 23.8115 - MinusLogProbMetric: 23.8115 - val_loss: 24.1797 - val_MinusLogProbMetric: 24.1797 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 11/1000
2023-10-10 16:09:56.247 
Epoch 11/1000 
	 loss: 23.4513, MinusLogProbMetric: 23.4513, val_loss: 23.4682, val_MinusLogProbMetric: 23.4682

Epoch 11: val_loss improved from 24.17972 to 23.46818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 23.4513 - MinusLogProbMetric: 23.4513 - val_loss: 23.4682 - val_MinusLogProbMetric: 23.4682 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 12/1000
2023-10-10 16:10:50.048 
Epoch 12/1000 
	 loss: 23.0319, MinusLogProbMetric: 23.0319, val_loss: 23.6408, val_MinusLogProbMetric: 23.6408

Epoch 12: val_loss did not improve from 23.46818
196/196 - 53s - loss: 23.0319 - MinusLogProbMetric: 23.0319 - val_loss: 23.6408 - val_MinusLogProbMetric: 23.6408 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 13/1000
2023-10-10 16:11:42.386 
Epoch 13/1000 
	 loss: 22.7486, MinusLogProbMetric: 22.7486, val_loss: 22.7054, val_MinusLogProbMetric: 22.7054

Epoch 13: val_loss improved from 23.46818 to 22.70543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 22.7486 - MinusLogProbMetric: 22.7486 - val_loss: 22.7054 - val_MinusLogProbMetric: 22.7054 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 14/1000
2023-10-10 16:12:35.561 
Epoch 14/1000 
	 loss: 22.5768, MinusLogProbMetric: 22.5768, val_loss: 23.1433, val_MinusLogProbMetric: 23.1433

Epoch 14: val_loss did not improve from 22.70543
196/196 - 52s - loss: 22.5768 - MinusLogProbMetric: 22.5768 - val_loss: 23.1433 - val_MinusLogProbMetric: 23.1433 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 15/1000
2023-10-10 16:13:28.902 
Epoch 15/1000 
	 loss: 22.2570, MinusLogProbMetric: 22.2570, val_loss: 22.0196, val_MinusLogProbMetric: 22.0196

Epoch 15: val_loss improved from 22.70543 to 22.01962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 22.2570 - MinusLogProbMetric: 22.2570 - val_loss: 22.0196 - val_MinusLogProbMetric: 22.0196 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 16/1000
2023-10-10 16:14:23.208 
Epoch 16/1000 
	 loss: 21.7136, MinusLogProbMetric: 21.7136, val_loss: 23.2872, val_MinusLogProbMetric: 23.2872

Epoch 16: val_loss did not improve from 22.01962
196/196 - 54s - loss: 21.7136 - MinusLogProbMetric: 21.7136 - val_loss: 23.2872 - val_MinusLogProbMetric: 23.2872 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 17/1000
2023-10-10 16:15:16.234 
Epoch 17/1000 
	 loss: 21.8814, MinusLogProbMetric: 21.8814, val_loss: 22.2374, val_MinusLogProbMetric: 22.2374

Epoch 17: val_loss did not improve from 22.01962
196/196 - 53s - loss: 21.8814 - MinusLogProbMetric: 21.8814 - val_loss: 22.2374 - val_MinusLogProbMetric: 22.2374 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 18/1000
2023-10-10 16:16:08.700 
Epoch 18/1000 
	 loss: 21.4029, MinusLogProbMetric: 21.4029, val_loss: 21.2168, val_MinusLogProbMetric: 21.2168

Epoch 18: val_loss improved from 22.01962 to 21.21684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 21.4029 - MinusLogProbMetric: 21.4029 - val_loss: 21.2168 - val_MinusLogProbMetric: 21.2168 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 19/1000
2023-10-10 16:17:01.732 
Epoch 19/1000 
	 loss: 21.5863, MinusLogProbMetric: 21.5863, val_loss: 21.3764, val_MinusLogProbMetric: 21.3764

Epoch 19: val_loss did not improve from 21.21684
196/196 - 52s - loss: 21.5863 - MinusLogProbMetric: 21.5863 - val_loss: 21.3764 - val_MinusLogProbMetric: 21.3764 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 20/1000
2023-10-10 16:17:54.022 
Epoch 20/1000 
	 loss: 21.1184, MinusLogProbMetric: 21.1184, val_loss: 22.8024, val_MinusLogProbMetric: 22.8024

Epoch 20: val_loss did not improve from 21.21684
196/196 - 52s - loss: 21.1184 - MinusLogProbMetric: 21.1184 - val_loss: 22.8024 - val_MinusLogProbMetric: 22.8024 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 21/1000
2023-10-10 16:18:46.351 
Epoch 21/1000 
	 loss: 21.0685, MinusLogProbMetric: 21.0685, val_loss: 21.0234, val_MinusLogProbMetric: 21.0234

Epoch 21: val_loss improved from 21.21684 to 21.02340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 21.0685 - MinusLogProbMetric: 21.0685 - val_loss: 21.0234 - val_MinusLogProbMetric: 21.0234 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 22/1000
2023-10-10 16:19:40.684 
Epoch 22/1000 
	 loss: 21.0335, MinusLogProbMetric: 21.0335, val_loss: 20.4826, val_MinusLogProbMetric: 20.4826

Epoch 22: val_loss improved from 21.02340 to 20.48261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 21.0335 - MinusLogProbMetric: 21.0335 - val_loss: 20.4826 - val_MinusLogProbMetric: 20.4826 - lr: 3.3333e-04 - 54s/epoch - 278ms/step
Epoch 23/1000
2023-10-10 16:20:34.590 
Epoch 23/1000 
	 loss: 21.0830, MinusLogProbMetric: 21.0830, val_loss: 21.1323, val_MinusLogProbMetric: 21.1323

Epoch 23: val_loss did not improve from 20.48261
196/196 - 53s - loss: 21.0830 - MinusLogProbMetric: 21.0830 - val_loss: 21.1323 - val_MinusLogProbMetric: 21.1323 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 24/1000
2023-10-10 16:21:27.010 
Epoch 24/1000 
	 loss: 20.6696, MinusLogProbMetric: 20.6696, val_loss: 20.6826, val_MinusLogProbMetric: 20.6826

Epoch 24: val_loss did not improve from 20.48261
196/196 - 52s - loss: 20.6696 - MinusLogProbMetric: 20.6696 - val_loss: 20.6826 - val_MinusLogProbMetric: 20.6826 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 25/1000
2023-10-10 16:22:19.938 
Epoch 25/1000 
	 loss: 20.6355, MinusLogProbMetric: 20.6355, val_loss: 21.8432, val_MinusLogProbMetric: 21.8432

Epoch 25: val_loss did not improve from 20.48261
196/196 - 53s - loss: 20.6355 - MinusLogProbMetric: 20.6355 - val_loss: 21.8432 - val_MinusLogProbMetric: 21.8432 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 26/1000
2023-10-10 16:23:12.588 
Epoch 26/1000 
	 loss: 20.6555, MinusLogProbMetric: 20.6555, val_loss: 19.9668, val_MinusLogProbMetric: 19.9668

Epoch 26: val_loss improved from 20.48261 to 19.96680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 20.6555 - MinusLogProbMetric: 20.6555 - val_loss: 19.9668 - val_MinusLogProbMetric: 19.9668 - lr: 3.3333e-04 - 53s/epoch - 273ms/step
Epoch 27/1000
2023-10-10 16:24:06.211 
Epoch 27/1000 
	 loss: 20.5115, MinusLogProbMetric: 20.5115, val_loss: 20.0074, val_MinusLogProbMetric: 20.0074

Epoch 27: val_loss did not improve from 19.96680
196/196 - 53s - loss: 20.5115 - MinusLogProbMetric: 20.5115 - val_loss: 20.0074 - val_MinusLogProbMetric: 20.0074 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 28/1000
2023-10-10 16:24:58.919 
Epoch 28/1000 
	 loss: 20.3754, MinusLogProbMetric: 20.3754, val_loss: 20.7261, val_MinusLogProbMetric: 20.7261

Epoch 28: val_loss did not improve from 19.96680
196/196 - 53s - loss: 20.3754 - MinusLogProbMetric: 20.3754 - val_loss: 20.7261 - val_MinusLogProbMetric: 20.7261 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 29/1000
2023-10-10 16:25:51.475 
Epoch 29/1000 
	 loss: 20.1665, MinusLogProbMetric: 20.1665, val_loss: 19.6375, val_MinusLogProbMetric: 19.6375

Epoch 29: val_loss improved from 19.96680 to 19.63751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 20.1665 - MinusLogProbMetric: 20.1665 - val_loss: 19.6375 - val_MinusLogProbMetric: 19.6375 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 30/1000
2023-10-10 16:26:44.426 
Epoch 30/1000 
	 loss: 20.1483, MinusLogProbMetric: 20.1483, val_loss: 21.3778, val_MinusLogProbMetric: 21.3778

Epoch 30: val_loss did not improve from 19.63751
196/196 - 52s - loss: 20.1483 - MinusLogProbMetric: 20.1483 - val_loss: 21.3778 - val_MinusLogProbMetric: 21.3778 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 31/1000
2023-10-10 16:27:36.607 
Epoch 31/1000 
	 loss: 20.1284, MinusLogProbMetric: 20.1284, val_loss: 19.9803, val_MinusLogProbMetric: 19.9803

Epoch 31: val_loss did not improve from 19.63751
196/196 - 52s - loss: 20.1284 - MinusLogProbMetric: 20.1284 - val_loss: 19.9803 - val_MinusLogProbMetric: 19.9803 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 32/1000
2023-10-10 16:28:29.199 
Epoch 32/1000 
	 loss: 19.9758, MinusLogProbMetric: 19.9758, val_loss: 20.8001, val_MinusLogProbMetric: 20.8001

Epoch 32: val_loss did not improve from 19.63751
196/196 - 53s - loss: 19.9758 - MinusLogProbMetric: 19.9758 - val_loss: 20.8001 - val_MinusLogProbMetric: 20.8001 - lr: 3.3333e-04 - 53s/epoch - 268ms/step
Epoch 33/1000
2023-10-10 16:29:22.185 
Epoch 33/1000 
	 loss: 20.1094, MinusLogProbMetric: 20.1094, val_loss: 20.9477, val_MinusLogProbMetric: 20.9477

Epoch 33: val_loss did not improve from 19.63751
196/196 - 53s - loss: 20.1094 - MinusLogProbMetric: 20.1094 - val_loss: 20.9477 - val_MinusLogProbMetric: 20.9477 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 34/1000
2023-10-10 16:30:14.910 
Epoch 34/1000 
	 loss: 19.9930, MinusLogProbMetric: 19.9930, val_loss: 20.5216, val_MinusLogProbMetric: 20.5216

Epoch 34: val_loss did not improve from 19.63751
196/196 - 53s - loss: 19.9930 - MinusLogProbMetric: 19.9930 - val_loss: 20.5216 - val_MinusLogProbMetric: 20.5216 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 35/1000
2023-10-10 16:31:08.344 
Epoch 35/1000 
	 loss: 19.8557, MinusLogProbMetric: 19.8557, val_loss: 19.4537, val_MinusLogProbMetric: 19.4537

Epoch 35: val_loss improved from 19.63751 to 19.45365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 19.8557 - MinusLogProbMetric: 19.8557 - val_loss: 19.4537 - val_MinusLogProbMetric: 19.4537 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 36/1000
2023-10-10 16:32:01.707 
Epoch 36/1000 
	 loss: 19.8964, MinusLogProbMetric: 19.8964, val_loss: 20.1682, val_MinusLogProbMetric: 20.1682

Epoch 36: val_loss did not improve from 19.45365
196/196 - 53s - loss: 19.8964 - MinusLogProbMetric: 19.8964 - val_loss: 20.1682 - val_MinusLogProbMetric: 20.1682 - lr: 3.3333e-04 - 53s/epoch - 268ms/step
Epoch 37/1000
2023-10-10 16:32:54.118 
Epoch 37/1000 
	 loss: 19.7504, MinusLogProbMetric: 19.7504, val_loss: 19.8134, val_MinusLogProbMetric: 19.8134

Epoch 37: val_loss did not improve from 19.45365
196/196 - 52s - loss: 19.7504 - MinusLogProbMetric: 19.7504 - val_loss: 19.8134 - val_MinusLogProbMetric: 19.8134 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 38/1000
2023-10-10 16:33:46.186 
Epoch 38/1000 
	 loss: 19.7146, MinusLogProbMetric: 19.7146, val_loss: 19.5545, val_MinusLogProbMetric: 19.5545

Epoch 38: val_loss did not improve from 19.45365
196/196 - 52s - loss: 19.7146 - MinusLogProbMetric: 19.7146 - val_loss: 19.5545 - val_MinusLogProbMetric: 19.5545 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 39/1000
2023-10-10 16:34:39.223 
Epoch 39/1000 
	 loss: 19.7368, MinusLogProbMetric: 19.7368, val_loss: 19.7066, val_MinusLogProbMetric: 19.7066

Epoch 39: val_loss did not improve from 19.45365
196/196 - 53s - loss: 19.7368 - MinusLogProbMetric: 19.7368 - val_loss: 19.7066 - val_MinusLogProbMetric: 19.7066 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 40/1000
2023-10-10 16:35:31.050 
Epoch 40/1000 
	 loss: 19.6831, MinusLogProbMetric: 19.6831, val_loss: 19.5307, val_MinusLogProbMetric: 19.5307

Epoch 40: val_loss did not improve from 19.45365
196/196 - 52s - loss: 19.6831 - MinusLogProbMetric: 19.6831 - val_loss: 19.5307 - val_MinusLogProbMetric: 19.5307 - lr: 3.3333e-04 - 52s/epoch - 264ms/step
Epoch 41/1000
2023-10-10 16:36:23.078 
Epoch 41/1000 
	 loss: 19.5758, MinusLogProbMetric: 19.5758, val_loss: 19.5081, val_MinusLogProbMetric: 19.5081

Epoch 41: val_loss did not improve from 19.45365
196/196 - 52s - loss: 19.5758 - MinusLogProbMetric: 19.5758 - val_loss: 19.5081 - val_MinusLogProbMetric: 19.5081 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 42/1000
2023-10-10 16:37:16.951 
Epoch 42/1000 
	 loss: 19.5796, MinusLogProbMetric: 19.5796, val_loss: 19.2336, val_MinusLogProbMetric: 19.2336

Epoch 42: val_loss improved from 19.45365 to 19.23357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 55s - loss: 19.5796 - MinusLogProbMetric: 19.5796 - val_loss: 19.2336 - val_MinusLogProbMetric: 19.2336 - lr: 3.3333e-04 - 55s/epoch - 279ms/step
Epoch 43/1000
2023-10-10 16:38:11.029 
Epoch 43/1000 
	 loss: 19.6172, MinusLogProbMetric: 19.6172, val_loss: 20.1761, val_MinusLogProbMetric: 20.1761

Epoch 43: val_loss did not improve from 19.23357
196/196 - 53s - loss: 19.6172 - MinusLogProbMetric: 19.6172 - val_loss: 20.1761 - val_MinusLogProbMetric: 20.1761 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 44/1000
2023-10-10 16:39:03.832 
Epoch 44/1000 
	 loss: 19.4702, MinusLogProbMetric: 19.4702, val_loss: 19.8581, val_MinusLogProbMetric: 19.8581

Epoch 44: val_loss did not improve from 19.23357
196/196 - 53s - loss: 19.4702 - MinusLogProbMetric: 19.4702 - val_loss: 19.8581 - val_MinusLogProbMetric: 19.8581 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 45/1000
2023-10-10 16:39:56.008 
Epoch 45/1000 
	 loss: 19.4005, MinusLogProbMetric: 19.4005, val_loss: 20.8000, val_MinusLogProbMetric: 20.8000

Epoch 45: val_loss did not improve from 19.23357
196/196 - 52s - loss: 19.4005 - MinusLogProbMetric: 19.4005 - val_loss: 20.8000 - val_MinusLogProbMetric: 20.8000 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 46/1000
2023-10-10 16:40:48.831 
Epoch 46/1000 
	 loss: 19.4842, MinusLogProbMetric: 19.4842, val_loss: 20.1807, val_MinusLogProbMetric: 20.1807

Epoch 46: val_loss did not improve from 19.23357
196/196 - 53s - loss: 19.4842 - MinusLogProbMetric: 19.4842 - val_loss: 20.1807 - val_MinusLogProbMetric: 20.1807 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 47/1000
2023-10-10 16:41:41.874 
Epoch 47/1000 
	 loss: 19.2937, MinusLogProbMetric: 19.2937, val_loss: 19.1195, val_MinusLogProbMetric: 19.1195

Epoch 47: val_loss improved from 19.23357 to 19.11949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 19.2937 - MinusLogProbMetric: 19.2937 - val_loss: 19.1195 - val_MinusLogProbMetric: 19.1195 - lr: 3.3333e-04 - 54s/epoch - 275ms/step
Epoch 48/1000
2023-10-10 16:42:36.310 
Epoch 48/1000 
	 loss: 19.5200, MinusLogProbMetric: 19.5200, val_loss: 19.7241, val_MinusLogProbMetric: 19.7241

Epoch 48: val_loss did not improve from 19.11949
196/196 - 54s - loss: 19.5200 - MinusLogProbMetric: 19.5200 - val_loss: 19.7241 - val_MinusLogProbMetric: 19.7241 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 49/1000
2023-10-10 16:43:28.819 
Epoch 49/1000 
	 loss: 19.2791, MinusLogProbMetric: 19.2791, val_loss: 19.3976, val_MinusLogProbMetric: 19.3976

Epoch 49: val_loss did not improve from 19.11949
196/196 - 52s - loss: 19.2791 - MinusLogProbMetric: 19.2791 - val_loss: 19.3976 - val_MinusLogProbMetric: 19.3976 - lr: 3.3333e-04 - 52s/epoch - 268ms/step
Epoch 50/1000
2023-10-10 16:44:22.012 
Epoch 50/1000 
	 loss: 19.3286, MinusLogProbMetric: 19.3286, val_loss: 19.3103, val_MinusLogProbMetric: 19.3103

Epoch 50: val_loss did not improve from 19.11949
196/196 - 53s - loss: 19.3286 - MinusLogProbMetric: 19.3286 - val_loss: 19.3103 - val_MinusLogProbMetric: 19.3103 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 51/1000
2023-10-10 16:45:14.131 
Epoch 51/1000 
	 loss: 19.1284, MinusLogProbMetric: 19.1284, val_loss: 19.2845, val_MinusLogProbMetric: 19.2845

Epoch 51: val_loss did not improve from 19.11949
196/196 - 52s - loss: 19.1284 - MinusLogProbMetric: 19.1284 - val_loss: 19.2845 - val_MinusLogProbMetric: 19.2845 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 52/1000
2023-10-10 16:46:07.643 
Epoch 52/1000 
	 loss: 19.8153, MinusLogProbMetric: 19.8153, val_loss: 19.7756, val_MinusLogProbMetric: 19.7756

Epoch 52: val_loss did not improve from 19.11949
196/196 - 54s - loss: 19.8153 - MinusLogProbMetric: 19.8153 - val_loss: 19.7756 - val_MinusLogProbMetric: 19.7756 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 53/1000
2023-10-10 16:47:00.458 
Epoch 53/1000 
	 loss: 19.1052, MinusLogProbMetric: 19.1052, val_loss: 19.4687, val_MinusLogProbMetric: 19.4687

Epoch 53: val_loss did not improve from 19.11949
196/196 - 53s - loss: 19.1052 - MinusLogProbMetric: 19.1052 - val_loss: 19.4687 - val_MinusLogProbMetric: 19.4687 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 54/1000
2023-10-10 16:47:53.016 
Epoch 54/1000 
	 loss: 19.1792, MinusLogProbMetric: 19.1792, val_loss: 19.0350, val_MinusLogProbMetric: 19.0350

Epoch 54: val_loss improved from 19.11949 to 19.03498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 19.1792 - MinusLogProbMetric: 19.1792 - val_loss: 19.0350 - val_MinusLogProbMetric: 19.0350 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 55/1000
2023-10-10 16:48:46.388 
Epoch 55/1000 
	 loss: 19.2397, MinusLogProbMetric: 19.2397, val_loss: 20.7967, val_MinusLogProbMetric: 20.7967

Epoch 55: val_loss did not improve from 19.03498
196/196 - 53s - loss: 19.2397 - MinusLogProbMetric: 19.2397 - val_loss: 20.7967 - val_MinusLogProbMetric: 20.7967 - lr: 3.3333e-04 - 53s/epoch - 268ms/step
Epoch 56/1000
2023-10-10 16:49:39.193 
Epoch 56/1000 
	 loss: 19.1409, MinusLogProbMetric: 19.1409, val_loss: 22.1671, val_MinusLogProbMetric: 22.1671

Epoch 56: val_loss did not improve from 19.03498
196/196 - 53s - loss: 19.1409 - MinusLogProbMetric: 19.1409 - val_loss: 22.1671 - val_MinusLogProbMetric: 22.1671 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 57/1000
2023-10-10 16:50:30.979 
Epoch 57/1000 
	 loss: 19.2660, MinusLogProbMetric: 19.2660, val_loss: 20.2947, val_MinusLogProbMetric: 20.2947

Epoch 57: val_loss did not improve from 19.03498
196/196 - 52s - loss: 19.2660 - MinusLogProbMetric: 19.2660 - val_loss: 20.2947 - val_MinusLogProbMetric: 20.2947 - lr: 3.3333e-04 - 52s/epoch - 264ms/step
Epoch 58/1000
2023-10-10 16:51:23.421 
Epoch 58/1000 
	 loss: 19.2164, MinusLogProbMetric: 19.2164, val_loss: 19.5166, val_MinusLogProbMetric: 19.5166

Epoch 58: val_loss did not improve from 19.03498
196/196 - 52s - loss: 19.2164 - MinusLogProbMetric: 19.2164 - val_loss: 19.5166 - val_MinusLogProbMetric: 19.5166 - lr: 3.3333e-04 - 52s/epoch - 268ms/step
Epoch 59/1000
2023-10-10 16:52:15.669 
Epoch 59/1000 
	 loss: 18.9624, MinusLogProbMetric: 18.9624, val_loss: 19.1404, val_MinusLogProbMetric: 19.1404

Epoch 59: val_loss did not improve from 19.03498
196/196 - 52s - loss: 18.9624 - MinusLogProbMetric: 18.9624 - val_loss: 19.1404 - val_MinusLogProbMetric: 19.1404 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 60/1000
2023-10-10 16:53:07.785 
Epoch 60/1000 
	 loss: 19.2616, MinusLogProbMetric: 19.2616, val_loss: 19.4224, val_MinusLogProbMetric: 19.4224

Epoch 60: val_loss did not improve from 19.03498
196/196 - 52s - loss: 19.2616 - MinusLogProbMetric: 19.2616 - val_loss: 19.4224 - val_MinusLogProbMetric: 19.4224 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 61/1000
2023-10-10 16:54:00.328 
Epoch 61/1000 
	 loss: 19.0131, MinusLogProbMetric: 19.0131, val_loss: 19.8825, val_MinusLogProbMetric: 19.8825

Epoch 61: val_loss did not improve from 19.03498
196/196 - 53s - loss: 19.0131 - MinusLogProbMetric: 19.0131 - val_loss: 19.8825 - val_MinusLogProbMetric: 19.8825 - lr: 3.3333e-04 - 53s/epoch - 268ms/step
Epoch 62/1000
2023-10-10 16:54:52.636 
Epoch 62/1000 
	 loss: 18.9704, MinusLogProbMetric: 18.9704, val_loss: 19.3660, val_MinusLogProbMetric: 19.3660

Epoch 62: val_loss did not improve from 19.03498
196/196 - 52s - loss: 18.9704 - MinusLogProbMetric: 18.9704 - val_loss: 19.3660 - val_MinusLogProbMetric: 19.3660 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 63/1000
2023-10-10 16:55:44.633 
Epoch 63/1000 
	 loss: 19.1540, MinusLogProbMetric: 19.1540, val_loss: 19.2911, val_MinusLogProbMetric: 19.2911

Epoch 63: val_loss did not improve from 19.03498
196/196 - 52s - loss: 19.1540 - MinusLogProbMetric: 19.1540 - val_loss: 19.2911 - val_MinusLogProbMetric: 19.2911 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 64/1000
2023-10-10 16:56:36.467 
Epoch 64/1000 
	 loss: 19.0709, MinusLogProbMetric: 19.0709, val_loss: 19.0058, val_MinusLogProbMetric: 19.0058

Epoch 64: val_loss improved from 19.03498 to 19.00582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 53s - loss: 19.0709 - MinusLogProbMetric: 19.0709 - val_loss: 19.0058 - val_MinusLogProbMetric: 19.0058 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 65/1000
2023-10-10 16:57:30.379 
Epoch 65/1000 
	 loss: 18.8723, MinusLogProbMetric: 18.8723, val_loss: 19.3079, val_MinusLogProbMetric: 19.3079

Epoch 65: val_loss did not improve from 19.00582
196/196 - 53s - loss: 18.8723 - MinusLogProbMetric: 18.8723 - val_loss: 19.3079 - val_MinusLogProbMetric: 19.3079 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 66/1000
2023-10-10 16:58:22.533 
Epoch 66/1000 
	 loss: 18.8416, MinusLogProbMetric: 18.8416, val_loss: 19.1515, val_MinusLogProbMetric: 19.1515

Epoch 66: val_loss did not improve from 19.00582
196/196 - 52s - loss: 18.8416 - MinusLogProbMetric: 18.8416 - val_loss: 19.1515 - val_MinusLogProbMetric: 19.1515 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 67/1000
2023-10-10 16:59:15.957 
Epoch 67/1000 
	 loss: 18.9885, MinusLogProbMetric: 18.9885, val_loss: 18.8969, val_MinusLogProbMetric: 18.8969

Epoch 67: val_loss improved from 19.00582 to 18.89692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 18.9885 - MinusLogProbMetric: 18.9885 - val_loss: 18.8969 - val_MinusLogProbMetric: 18.8969 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 68/1000
2023-10-10 17:00:08.416 
Epoch 68/1000 
	 loss: 18.7872, MinusLogProbMetric: 18.7872, val_loss: 20.2266, val_MinusLogProbMetric: 20.2266

Epoch 68: val_loss did not improve from 18.89692
196/196 - 52s - loss: 18.7872 - MinusLogProbMetric: 18.7872 - val_loss: 20.2266 - val_MinusLogProbMetric: 20.2266 - lr: 3.3333e-04 - 52s/epoch - 263ms/step
Epoch 69/1000
2023-10-10 17:01:01.483 
Epoch 69/1000 
	 loss: 18.9621, MinusLogProbMetric: 18.9621, val_loss: 18.7865, val_MinusLogProbMetric: 18.7865

Epoch 69: val_loss improved from 18.89692 to 18.78646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 18.9621 - MinusLogProbMetric: 18.9621 - val_loss: 18.7865 - val_MinusLogProbMetric: 18.7865 - lr: 3.3333e-04 - 54s/epoch - 275ms/step
Epoch 70/1000
2023-10-10 17:01:55.290 
Epoch 70/1000 
	 loss: 18.7080, MinusLogProbMetric: 18.7080, val_loss: 18.7892, val_MinusLogProbMetric: 18.7892

Epoch 70: val_loss did not improve from 18.78646
196/196 - 53s - loss: 18.7080 - MinusLogProbMetric: 18.7080 - val_loss: 18.7892 - val_MinusLogProbMetric: 18.7892 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 71/1000
2023-10-10 17:02:48.164 
Epoch 71/1000 
	 loss: 18.7282, MinusLogProbMetric: 18.7282, val_loss: 20.1425, val_MinusLogProbMetric: 20.1425

Epoch 71: val_loss did not improve from 18.78646
196/196 - 53s - loss: 18.7282 - MinusLogProbMetric: 18.7282 - val_loss: 20.1425 - val_MinusLogProbMetric: 20.1425 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 72/1000
2023-10-10 17:03:41.487 
Epoch 72/1000 
	 loss: 18.8534, MinusLogProbMetric: 18.8534, val_loss: 18.5780, val_MinusLogProbMetric: 18.5780

Epoch 72: val_loss improved from 18.78646 to 18.57795, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 18.8534 - MinusLogProbMetric: 18.8534 - val_loss: 18.5780 - val_MinusLogProbMetric: 18.5780 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 73/1000
2023-10-10 17:04:36.286 
Epoch 73/1000 
	 loss: 18.8263, MinusLogProbMetric: 18.8263, val_loss: 18.6330, val_MinusLogProbMetric: 18.6330

Epoch 73: val_loss did not improve from 18.57795
196/196 - 54s - loss: 18.8263 - MinusLogProbMetric: 18.8263 - val_loss: 18.6330 - val_MinusLogProbMetric: 18.6330 - lr: 3.3333e-04 - 54s/epoch - 274ms/step
Epoch 74/1000
2023-10-10 17:05:29.855 
Epoch 74/1000 
	 loss: 18.7703, MinusLogProbMetric: 18.7703, val_loss: 18.8302, val_MinusLogProbMetric: 18.8302

Epoch 74: val_loss did not improve from 18.57795
196/196 - 54s - loss: 18.7703 - MinusLogProbMetric: 18.7703 - val_loss: 18.8302 - val_MinusLogProbMetric: 18.8302 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 75/1000
2023-10-10 17:06:21.828 
Epoch 75/1000 
	 loss: 18.6534, MinusLogProbMetric: 18.6534, val_loss: 18.6807, val_MinusLogProbMetric: 18.6807

Epoch 75: val_loss did not improve from 18.57795
196/196 - 52s - loss: 18.6534 - MinusLogProbMetric: 18.6534 - val_loss: 18.6807 - val_MinusLogProbMetric: 18.6807 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 76/1000
2023-10-10 17:07:15.159 
Epoch 76/1000 
	 loss: 18.7841, MinusLogProbMetric: 18.7841, val_loss: 18.8661, val_MinusLogProbMetric: 18.8661

Epoch 76: val_loss did not improve from 18.57795
196/196 - 53s - loss: 18.7841 - MinusLogProbMetric: 18.7841 - val_loss: 18.8661 - val_MinusLogProbMetric: 18.8661 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 77/1000
2023-10-10 17:08:08.123 
Epoch 77/1000 
	 loss: 18.7423, MinusLogProbMetric: 18.7423, val_loss: 18.7146, val_MinusLogProbMetric: 18.7146

Epoch 77: val_loss did not improve from 18.57795
196/196 - 53s - loss: 18.7423 - MinusLogProbMetric: 18.7423 - val_loss: 18.7146 - val_MinusLogProbMetric: 18.7146 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 78/1000
2023-10-10 17:09:00.120 
Epoch 78/1000 
	 loss: 18.7235, MinusLogProbMetric: 18.7235, val_loss: 18.6638, val_MinusLogProbMetric: 18.6638

Epoch 78: val_loss did not improve from 18.57795
196/196 - 52s - loss: 18.7235 - MinusLogProbMetric: 18.7235 - val_loss: 18.6638 - val_MinusLogProbMetric: 18.6638 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 79/1000
2023-10-10 17:09:52.306 
Epoch 79/1000 
	 loss: 18.7368, MinusLogProbMetric: 18.7368, val_loss: 18.7970, val_MinusLogProbMetric: 18.7970

Epoch 79: val_loss did not improve from 18.57795
196/196 - 52s - loss: 18.7368 - MinusLogProbMetric: 18.7368 - val_loss: 18.7970 - val_MinusLogProbMetric: 18.7970 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 80/1000
2023-10-10 17:10:44.719 
Epoch 80/1000 
	 loss: 18.5783, MinusLogProbMetric: 18.5783, val_loss: 19.1788, val_MinusLogProbMetric: 19.1788

Epoch 80: val_loss did not improve from 18.57795
196/196 - 52s - loss: 18.5783 - MinusLogProbMetric: 18.5783 - val_loss: 19.1788 - val_MinusLogProbMetric: 19.1788 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 81/1000
2023-10-10 17:11:37.779 
Epoch 81/1000 
	 loss: 18.7429, MinusLogProbMetric: 18.7429, val_loss: 19.0642, val_MinusLogProbMetric: 19.0642

Epoch 81: val_loss did not improve from 18.57795
196/196 - 53s - loss: 18.7429 - MinusLogProbMetric: 18.7429 - val_loss: 19.0642 - val_MinusLogProbMetric: 19.0642 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 82/1000
2023-10-10 17:12:30.947 
Epoch 82/1000 
	 loss: 18.5831, MinusLogProbMetric: 18.5831, val_loss: 19.0856, val_MinusLogProbMetric: 19.0856

Epoch 82: val_loss did not improve from 18.57795
196/196 - 53s - loss: 18.5831 - MinusLogProbMetric: 18.5831 - val_loss: 19.0856 - val_MinusLogProbMetric: 19.0856 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 83/1000
2023-10-10 17:13:22.860 
Epoch 83/1000 
	 loss: 18.6163, MinusLogProbMetric: 18.6163, val_loss: 19.5768, val_MinusLogProbMetric: 19.5768

Epoch 83: val_loss did not improve from 18.57795
196/196 - 52s - loss: 18.6163 - MinusLogProbMetric: 18.6163 - val_loss: 19.5768 - val_MinusLogProbMetric: 19.5768 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 84/1000
2023-10-10 17:14:16.845 
Epoch 84/1000 
	 loss: 18.6955, MinusLogProbMetric: 18.6955, val_loss: 18.9317, val_MinusLogProbMetric: 18.9317

Epoch 84: val_loss did not improve from 18.57795
196/196 - 54s - loss: 18.6955 - MinusLogProbMetric: 18.6955 - val_loss: 18.9317 - val_MinusLogProbMetric: 18.9317 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 85/1000
2023-10-10 17:15:11.492 
Epoch 85/1000 
	 loss: 18.7373, MinusLogProbMetric: 18.7373, val_loss: 19.3140, val_MinusLogProbMetric: 19.3140

Epoch 85: val_loss did not improve from 18.57795
196/196 - 55s - loss: 18.7373 - MinusLogProbMetric: 18.7373 - val_loss: 19.3140 - val_MinusLogProbMetric: 19.3140 - lr: 3.3333e-04 - 55s/epoch - 279ms/step
Epoch 86/1000
2023-10-10 17:16:09.989 
Epoch 86/1000 
	 loss: 18.5330, MinusLogProbMetric: 18.5330, val_loss: 18.6216, val_MinusLogProbMetric: 18.6216

Epoch 86: val_loss did not improve from 18.57795
196/196 - 58s - loss: 18.5330 - MinusLogProbMetric: 18.5330 - val_loss: 18.6216 - val_MinusLogProbMetric: 18.6216 - lr: 3.3333e-04 - 58s/epoch - 298ms/step
Epoch 87/1000
2023-10-10 17:17:05.219 
Epoch 87/1000 
	 loss: 18.6130, MinusLogProbMetric: 18.6130, val_loss: 18.6731, val_MinusLogProbMetric: 18.6731

Epoch 87: val_loss did not improve from 18.57795
196/196 - 55s - loss: 18.6130 - MinusLogProbMetric: 18.6130 - val_loss: 18.6731 - val_MinusLogProbMetric: 18.6731 - lr: 3.3333e-04 - 55s/epoch - 282ms/step
Epoch 88/1000
2023-10-10 17:17:59.450 
Epoch 88/1000 
	 loss: 18.6105, MinusLogProbMetric: 18.6105, val_loss: 18.3793, val_MinusLogProbMetric: 18.3793

Epoch 88: val_loss improved from 18.57795 to 18.37932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 55s - loss: 18.6105 - MinusLogProbMetric: 18.6105 - val_loss: 18.3793 - val_MinusLogProbMetric: 18.3793 - lr: 3.3333e-04 - 55s/epoch - 281ms/step
Epoch 89/1000
2023-10-10 17:18:55.236 
Epoch 89/1000 
	 loss: 18.5486, MinusLogProbMetric: 18.5486, val_loss: 18.4664, val_MinusLogProbMetric: 18.4664

Epoch 89: val_loss did not improve from 18.37932
196/196 - 55s - loss: 18.5486 - MinusLogProbMetric: 18.5486 - val_loss: 18.4664 - val_MinusLogProbMetric: 18.4664 - lr: 3.3333e-04 - 55s/epoch - 280ms/step
Epoch 90/1000
2023-10-10 17:19:49.486 
Epoch 90/1000 
	 loss: 18.4873, MinusLogProbMetric: 18.4873, val_loss: 18.7521, val_MinusLogProbMetric: 18.7521

Epoch 90: val_loss did not improve from 18.37932
196/196 - 54s - loss: 18.4873 - MinusLogProbMetric: 18.4873 - val_loss: 18.7521 - val_MinusLogProbMetric: 18.7521 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 91/1000
2023-10-10 17:20:43.509 
Epoch 91/1000 
	 loss: 18.4866, MinusLogProbMetric: 18.4866, val_loss: 18.7860, val_MinusLogProbMetric: 18.7860

Epoch 91: val_loss did not improve from 18.37932
196/196 - 54s - loss: 18.4866 - MinusLogProbMetric: 18.4866 - val_loss: 18.7860 - val_MinusLogProbMetric: 18.7860 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 92/1000
2023-10-10 17:21:36.799 
Epoch 92/1000 
	 loss: 18.5108, MinusLogProbMetric: 18.5108, val_loss: 18.6328, val_MinusLogProbMetric: 18.6328

Epoch 92: val_loss did not improve from 18.37932
196/196 - 53s - loss: 18.5108 - MinusLogProbMetric: 18.5108 - val_loss: 18.6328 - val_MinusLogProbMetric: 18.6328 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 93/1000
2023-10-10 17:22:29.824 
Epoch 93/1000 
	 loss: 18.5074, MinusLogProbMetric: 18.5074, val_loss: 18.8001, val_MinusLogProbMetric: 18.8001

Epoch 93: val_loss did not improve from 18.37932
196/196 - 53s - loss: 18.5074 - MinusLogProbMetric: 18.5074 - val_loss: 18.8001 - val_MinusLogProbMetric: 18.8001 - lr: 3.3333e-04 - 53s/epoch - 271ms/step
Epoch 94/1000
2023-10-10 17:23:21.704 
Epoch 94/1000 
	 loss: 18.5903, MinusLogProbMetric: 18.5903, val_loss: 18.5617, val_MinusLogProbMetric: 18.5617

Epoch 94: val_loss did not improve from 18.37932
196/196 - 52s - loss: 18.5903 - MinusLogProbMetric: 18.5903 - val_loss: 18.5617 - val_MinusLogProbMetric: 18.5617 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 95/1000
2023-10-10 17:24:14.516 
Epoch 95/1000 
	 loss: 18.3829, MinusLogProbMetric: 18.3829, val_loss: 19.1942, val_MinusLogProbMetric: 19.1942

Epoch 95: val_loss did not improve from 18.37932
196/196 - 53s - loss: 18.3829 - MinusLogProbMetric: 18.3829 - val_loss: 19.1942 - val_MinusLogProbMetric: 19.1942 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 96/1000
2023-10-10 17:25:07.981 
Epoch 96/1000 
	 loss: 18.4573, MinusLogProbMetric: 18.4573, val_loss: 18.4159, val_MinusLogProbMetric: 18.4159

Epoch 96: val_loss did not improve from 18.37932
196/196 - 53s - loss: 18.4573 - MinusLogProbMetric: 18.4573 - val_loss: 18.4159 - val_MinusLogProbMetric: 18.4159 - lr: 3.3333e-04 - 53s/epoch - 273ms/step
Epoch 97/1000
2023-10-10 17:26:01.881 
Epoch 97/1000 
	 loss: 18.3743, MinusLogProbMetric: 18.3743, val_loss: 18.7123, val_MinusLogProbMetric: 18.7123

Epoch 97: val_loss did not improve from 18.37932
196/196 - 54s - loss: 18.3743 - MinusLogProbMetric: 18.3743 - val_loss: 18.7123 - val_MinusLogProbMetric: 18.7123 - lr: 3.3333e-04 - 54s/epoch - 275ms/step
Epoch 98/1000
2023-10-10 17:26:53.538 
Epoch 98/1000 
	 loss: 18.3885, MinusLogProbMetric: 18.3885, val_loss: 18.9277, val_MinusLogProbMetric: 18.9277

Epoch 98: val_loss did not improve from 18.37932
196/196 - 52s - loss: 18.3885 - MinusLogProbMetric: 18.3885 - val_loss: 18.9277 - val_MinusLogProbMetric: 18.9277 - lr: 3.3333e-04 - 52s/epoch - 264ms/step
Epoch 99/1000
2023-10-10 17:27:45.783 
Epoch 99/1000 
	 loss: 18.3244, MinusLogProbMetric: 18.3244, val_loss: 19.1044, val_MinusLogProbMetric: 19.1044

Epoch 99: val_loss did not improve from 18.37932
196/196 - 52s - loss: 18.3244 - MinusLogProbMetric: 18.3244 - val_loss: 19.1044 - val_MinusLogProbMetric: 19.1044 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 100/1000
2023-10-10 17:28:39.125 
Epoch 100/1000 
	 loss: 18.5863, MinusLogProbMetric: 18.5863, val_loss: 18.6611, val_MinusLogProbMetric: 18.6611

Epoch 100: val_loss did not improve from 18.37932
196/196 - 53s - loss: 18.5863 - MinusLogProbMetric: 18.5863 - val_loss: 18.6611 - val_MinusLogProbMetric: 18.6611 - lr: 3.3333e-04 - 53s/epoch - 272ms/step
Epoch 101/1000
2023-10-10 17:29:31.834 
Epoch 101/1000 
	 loss: 18.3459, MinusLogProbMetric: 18.3459, val_loss: 18.3763, val_MinusLogProbMetric: 18.3763

Epoch 101: val_loss improved from 18.37932 to 18.37630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 18.3459 - MinusLogProbMetric: 18.3459 - val_loss: 18.3763 - val_MinusLogProbMetric: 18.3763 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 102/1000
2023-10-10 17:30:24.641 
Epoch 102/1000 
	 loss: 18.3775, MinusLogProbMetric: 18.3775, val_loss: 18.6957, val_MinusLogProbMetric: 18.6957

Epoch 102: val_loss did not improve from 18.37630
196/196 - 52s - loss: 18.3775 - MinusLogProbMetric: 18.3775 - val_loss: 18.6957 - val_MinusLogProbMetric: 18.6957 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 103/1000
2023-10-10 17:31:17.710 
Epoch 103/1000 
	 loss: 18.3716, MinusLogProbMetric: 18.3716, val_loss: 18.1366, val_MinusLogProbMetric: 18.1366

Epoch 103: val_loss improved from 18.37630 to 18.13660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 54s - loss: 18.3716 - MinusLogProbMetric: 18.3716 - val_loss: 18.1366 - val_MinusLogProbMetric: 18.1366 - lr: 3.3333e-04 - 54s/epoch - 275ms/step
Epoch 104/1000
2023-10-10 17:32:10.651 
Epoch 104/1000 
	 loss: 18.4787, MinusLogProbMetric: 18.4787, val_loss: 18.4598, val_MinusLogProbMetric: 18.4598

Epoch 104: val_loss did not improve from 18.13660
196/196 - 52s - loss: 18.4787 - MinusLogProbMetric: 18.4787 - val_loss: 18.4598 - val_MinusLogProbMetric: 18.4598 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 105/1000
2023-10-10 17:33:03.665 
Epoch 105/1000 
	 loss: 18.4619, MinusLogProbMetric: 18.4619, val_loss: 18.5441, val_MinusLogProbMetric: 18.5441

Epoch 105: val_loss did not improve from 18.13660
196/196 - 53s - loss: 18.4619 - MinusLogProbMetric: 18.4619 - val_loss: 18.5441 - val_MinusLogProbMetric: 18.5441 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 106/1000
2023-10-10 17:33:55.975 
Epoch 106/1000 
	 loss: 18.3252, MinusLogProbMetric: 18.3252, val_loss: 18.6203, val_MinusLogProbMetric: 18.6203

Epoch 106: val_loss did not improve from 18.13660
196/196 - 52s - loss: 18.3252 - MinusLogProbMetric: 18.3252 - val_loss: 18.6203 - val_MinusLogProbMetric: 18.6203 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 107/1000
2023-10-10 17:34:48.752 
Epoch 107/1000 
	 loss: 18.3936, MinusLogProbMetric: 18.3936, val_loss: 18.8019, val_MinusLogProbMetric: 18.8019

Epoch 107: val_loss did not improve from 18.13660
196/196 - 53s - loss: 18.3936 - MinusLogProbMetric: 18.3936 - val_loss: 18.8019 - val_MinusLogProbMetric: 18.8019 - lr: 3.3333e-04 - 53s/epoch - 269ms/step
Epoch 108/1000
2023-10-10 17:35:40.819 
Epoch 108/1000 
	 loss: 18.2887, MinusLogProbMetric: 18.2887, val_loss: 18.6392, val_MinusLogProbMetric: 18.6392

Epoch 108: val_loss did not improve from 18.13660
196/196 - 52s - loss: 18.2887 - MinusLogProbMetric: 18.2887 - val_loss: 18.6392 - val_MinusLogProbMetric: 18.6392 - lr: 3.3333e-04 - 52s/epoch - 266ms/step
Epoch 109/1000
2023-10-10 17:36:33.784 
Epoch 109/1000 
	 loss: 18.4824, MinusLogProbMetric: 18.4824, val_loss: 18.7893, val_MinusLogProbMetric: 18.7893

Epoch 109: val_loss did not improve from 18.13660
196/196 - 53s - loss: 18.4824 - MinusLogProbMetric: 18.4824 - val_loss: 18.7893 - val_MinusLogProbMetric: 18.7893 - lr: 3.3333e-04 - 53s/epoch - 270ms/step
Epoch 110/1000
2023-10-10 17:37:25.477 
Epoch 110/1000 
	 loss: 18.2439, MinusLogProbMetric: 18.2439, val_loss: 18.3123, val_MinusLogProbMetric: 18.3123

Epoch 110: val_loss did not improve from 18.13660
196/196 - 52s - loss: 18.2439 - MinusLogProbMetric: 18.2439 - val_loss: 18.3123 - val_MinusLogProbMetric: 18.3123 - lr: 3.3333e-04 - 52s/epoch - 264ms/step
Epoch 111/1000
2023-10-10 17:38:16.400 
Epoch 111/1000 
	 loss: 18.2529, MinusLogProbMetric: 18.2529, val_loss: 18.2662, val_MinusLogProbMetric: 18.2662

Epoch 111: val_loss did not improve from 18.13660
196/196 - 51s - loss: 18.2529 - MinusLogProbMetric: 18.2529 - val_loss: 18.2662 - val_MinusLogProbMetric: 18.2662 - lr: 3.3333e-04 - 51s/epoch - 260ms/step
Epoch 112/1000
2023-10-10 17:39:23.673 
Epoch 112/1000 
	 loss: 18.2392, MinusLogProbMetric: 18.2392, val_loss: 18.2466, val_MinusLogProbMetric: 18.2466

Epoch 112: val_loss did not improve from 18.13660
196/196 - 67s - loss: 18.2392 - MinusLogProbMetric: 18.2392 - val_loss: 18.2466 - val_MinusLogProbMetric: 18.2466 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 113/1000
2023-10-10 17:40:44.714 
Epoch 113/1000 
	 loss: 18.3077, MinusLogProbMetric: 18.3077, val_loss: 19.1684, val_MinusLogProbMetric: 19.1684

Epoch 113: val_loss did not improve from 18.13660
196/196 - 81s - loss: 18.3077 - MinusLogProbMetric: 18.3077 - val_loss: 19.1684 - val_MinusLogProbMetric: 19.1684 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 114/1000
2023-10-10 17:42:01.664 
Epoch 114/1000 
	 loss: 18.2399, MinusLogProbMetric: 18.2399, val_loss: 18.2739, val_MinusLogProbMetric: 18.2739

Epoch 114: val_loss did not improve from 18.13660
196/196 - 77s - loss: 18.2399 - MinusLogProbMetric: 18.2399 - val_loss: 18.2739 - val_MinusLogProbMetric: 18.2739 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 115/1000
2023-10-10 17:43:17.642 
Epoch 115/1000 
	 loss: 18.2593, MinusLogProbMetric: 18.2593, val_loss: 19.4103, val_MinusLogProbMetric: 19.4103

Epoch 115: val_loss did not improve from 18.13660
196/196 - 76s - loss: 18.2593 - MinusLogProbMetric: 18.2593 - val_loss: 19.4103 - val_MinusLogProbMetric: 19.4103 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 116/1000
2023-10-10 17:44:30.032 
Epoch 116/1000 
	 loss: 18.2954, MinusLogProbMetric: 18.2954, val_loss: 18.1285, val_MinusLogProbMetric: 18.1285

Epoch 116: val_loss improved from 18.13660 to 18.12853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 74s - loss: 18.2954 - MinusLogProbMetric: 18.2954 - val_loss: 18.1285 - val_MinusLogProbMetric: 18.1285 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 117/1000
2023-10-10 17:45:44.320 
Epoch 117/1000 
	 loss: 18.2246, MinusLogProbMetric: 18.2246, val_loss: 18.5622, val_MinusLogProbMetric: 18.5622

Epoch 117: val_loss did not improve from 18.12853
196/196 - 73s - loss: 18.2246 - MinusLogProbMetric: 18.2246 - val_loss: 18.5622 - val_MinusLogProbMetric: 18.5622 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 118/1000
2023-10-10 17:47:00.385 
Epoch 118/1000 
	 loss: 18.3274, MinusLogProbMetric: 18.3274, val_loss: 18.2758, val_MinusLogProbMetric: 18.2758

Epoch 118: val_loss did not improve from 18.12853
196/196 - 76s - loss: 18.3274 - MinusLogProbMetric: 18.3274 - val_loss: 18.2758 - val_MinusLogProbMetric: 18.2758 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 119/1000
2023-10-10 17:48:15.088 
Epoch 119/1000 
	 loss: 18.2435, MinusLogProbMetric: 18.2435, val_loss: 18.2120, val_MinusLogProbMetric: 18.2120

Epoch 119: val_loss did not improve from 18.12853
196/196 - 75s - loss: 18.2435 - MinusLogProbMetric: 18.2435 - val_loss: 18.2120 - val_MinusLogProbMetric: 18.2120 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 120/1000
2023-10-10 17:49:28.434 
Epoch 120/1000 
	 loss: 18.1195, MinusLogProbMetric: 18.1195, val_loss: 18.3023, val_MinusLogProbMetric: 18.3023

Epoch 120: val_loss did not improve from 18.12853
196/196 - 73s - loss: 18.1195 - MinusLogProbMetric: 18.1195 - val_loss: 18.3023 - val_MinusLogProbMetric: 18.3023 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 121/1000
2023-10-10 17:50:42.828 
Epoch 121/1000 
	 loss: 18.2923, MinusLogProbMetric: 18.2923, val_loss: 19.1696, val_MinusLogProbMetric: 19.1696

Epoch 121: val_loss did not improve from 18.12853
196/196 - 74s - loss: 18.2923 - MinusLogProbMetric: 18.2923 - val_loss: 19.1696 - val_MinusLogProbMetric: 19.1696 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 122/1000
2023-10-10 17:51:57.245 
Epoch 122/1000 
	 loss: 18.1095, MinusLogProbMetric: 18.1095, val_loss: 18.1872, val_MinusLogProbMetric: 18.1872

Epoch 122: val_loss did not improve from 18.12853
196/196 - 74s - loss: 18.1095 - MinusLogProbMetric: 18.1095 - val_loss: 18.1872 - val_MinusLogProbMetric: 18.1872 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 123/1000
2023-10-10 17:53:10.702 
Epoch 123/1000 
	 loss: 18.2777, MinusLogProbMetric: 18.2777, val_loss: 18.4146, val_MinusLogProbMetric: 18.4146

Epoch 123: val_loss did not improve from 18.12853
196/196 - 73s - loss: 18.2777 - MinusLogProbMetric: 18.2777 - val_loss: 18.4146 - val_MinusLogProbMetric: 18.4146 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 124/1000
2023-10-10 17:54:24.144 
Epoch 124/1000 
	 loss: 18.1736, MinusLogProbMetric: 18.1736, val_loss: 18.2541, val_MinusLogProbMetric: 18.2541

Epoch 124: val_loss did not improve from 18.12853
196/196 - 73s - loss: 18.1736 - MinusLogProbMetric: 18.1736 - val_loss: 18.2541 - val_MinusLogProbMetric: 18.2541 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 125/1000
2023-10-10 17:55:39.540 
Epoch 125/1000 
	 loss: 18.1646, MinusLogProbMetric: 18.1646, val_loss: 19.5489, val_MinusLogProbMetric: 19.5489

Epoch 125: val_loss did not improve from 18.12853
196/196 - 75s - loss: 18.1646 - MinusLogProbMetric: 18.1646 - val_loss: 19.5489 - val_MinusLogProbMetric: 19.5489 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 126/1000
2023-10-10 17:56:52.403 
Epoch 126/1000 
	 loss: 18.1972, MinusLogProbMetric: 18.1972, val_loss: 18.4432, val_MinusLogProbMetric: 18.4432

Epoch 126: val_loss did not improve from 18.12853
196/196 - 73s - loss: 18.1972 - MinusLogProbMetric: 18.1972 - val_loss: 18.4432 - val_MinusLogProbMetric: 18.4432 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 127/1000
2023-10-10 17:58:04.109 
Epoch 127/1000 
	 loss: 18.2156, MinusLogProbMetric: 18.2156, val_loss: 18.9361, val_MinusLogProbMetric: 18.9361

Epoch 127: val_loss did not improve from 18.12853
196/196 - 72s - loss: 18.2156 - MinusLogProbMetric: 18.2156 - val_loss: 18.9361 - val_MinusLogProbMetric: 18.9361 - lr: 3.3333e-04 - 72s/epoch - 366ms/step
Epoch 128/1000
2023-10-10 17:59:17.196 
Epoch 128/1000 
	 loss: 18.1444, MinusLogProbMetric: 18.1444, val_loss: 18.2181, val_MinusLogProbMetric: 18.2181

Epoch 128: val_loss did not improve from 18.12853
196/196 - 73s - loss: 18.1444 - MinusLogProbMetric: 18.1444 - val_loss: 18.2181 - val_MinusLogProbMetric: 18.2181 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 129/1000
2023-10-10 18:00:36.494 
Epoch 129/1000 
	 loss: 18.1496, MinusLogProbMetric: 18.1496, val_loss: 18.6123, val_MinusLogProbMetric: 18.6123

Epoch 129: val_loss did not improve from 18.12853
196/196 - 79s - loss: 18.1496 - MinusLogProbMetric: 18.1496 - val_loss: 18.6123 - val_MinusLogProbMetric: 18.6123 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 130/1000
2023-10-10 18:01:50.012 
Epoch 130/1000 
	 loss: 18.1856, MinusLogProbMetric: 18.1856, val_loss: 20.1526, val_MinusLogProbMetric: 20.1526

Epoch 130: val_loss did not improve from 18.12853
196/196 - 74s - loss: 18.1856 - MinusLogProbMetric: 18.1856 - val_loss: 20.1526 - val_MinusLogProbMetric: 20.1526 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 131/1000
2023-10-10 18:03:08.870 
Epoch 131/1000 
	 loss: 18.1227, MinusLogProbMetric: 18.1227, val_loss: 18.4296, val_MinusLogProbMetric: 18.4296

Epoch 131: val_loss did not improve from 18.12853
196/196 - 79s - loss: 18.1227 - MinusLogProbMetric: 18.1227 - val_loss: 18.4296 - val_MinusLogProbMetric: 18.4296 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 132/1000
2023-10-10 18:04:25.696 
Epoch 132/1000 
	 loss: 18.1279, MinusLogProbMetric: 18.1279, val_loss: 17.9247, val_MinusLogProbMetric: 17.9247

Epoch 132: val_loss improved from 18.12853 to 17.92469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 77s - loss: 18.1279 - MinusLogProbMetric: 18.1279 - val_loss: 17.9247 - val_MinusLogProbMetric: 17.9247 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 133/1000
2023-10-10 18:05:39.442 
Epoch 133/1000 
	 loss: 18.1241, MinusLogProbMetric: 18.1241, val_loss: 18.5726, val_MinusLogProbMetric: 18.5726

Epoch 133: val_loss did not improve from 17.92469
196/196 - 73s - loss: 18.1241 - MinusLogProbMetric: 18.1241 - val_loss: 18.5726 - val_MinusLogProbMetric: 18.5726 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 134/1000
2023-10-10 18:06:56.476 
Epoch 134/1000 
	 loss: 18.1101, MinusLogProbMetric: 18.1101, val_loss: 18.1598, val_MinusLogProbMetric: 18.1598

Epoch 134: val_loss did not improve from 17.92469
196/196 - 77s - loss: 18.1101 - MinusLogProbMetric: 18.1101 - val_loss: 18.1598 - val_MinusLogProbMetric: 18.1598 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 135/1000
2023-10-10 18:08:16.726 
Epoch 135/1000 
	 loss: 18.1236, MinusLogProbMetric: 18.1236, val_loss: 18.6408, val_MinusLogProbMetric: 18.6408

Epoch 135: val_loss did not improve from 17.92469
196/196 - 80s - loss: 18.1236 - MinusLogProbMetric: 18.1236 - val_loss: 18.6408 - val_MinusLogProbMetric: 18.6408 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 136/1000
2023-10-10 18:09:30.399 
Epoch 136/1000 
	 loss: 18.0771, MinusLogProbMetric: 18.0771, val_loss: 18.5508, val_MinusLogProbMetric: 18.5508

Epoch 136: val_loss did not improve from 17.92469
196/196 - 74s - loss: 18.0771 - MinusLogProbMetric: 18.0771 - val_loss: 18.5508 - val_MinusLogProbMetric: 18.5508 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 137/1000
2023-10-10 18:10:48.000 
Epoch 137/1000 
	 loss: 18.1401, MinusLogProbMetric: 18.1401, val_loss: 17.9979, val_MinusLogProbMetric: 17.9979

Epoch 137: val_loss did not improve from 17.92469
196/196 - 78s - loss: 18.1401 - MinusLogProbMetric: 18.1401 - val_loss: 17.9979 - val_MinusLogProbMetric: 17.9979 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 138/1000
2023-10-10 18:12:07.929 
Epoch 138/1000 
	 loss: 18.1619, MinusLogProbMetric: 18.1619, val_loss: 18.3810, val_MinusLogProbMetric: 18.3810

Epoch 138: val_loss did not improve from 17.92469
196/196 - 80s - loss: 18.1619 - MinusLogProbMetric: 18.1619 - val_loss: 18.3810 - val_MinusLogProbMetric: 18.3810 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 139/1000
2023-10-10 18:13:21.849 
Epoch 139/1000 
	 loss: 18.0871, MinusLogProbMetric: 18.0871, val_loss: 18.2747, val_MinusLogProbMetric: 18.2747

Epoch 139: val_loss did not improve from 17.92469
196/196 - 74s - loss: 18.0871 - MinusLogProbMetric: 18.0871 - val_loss: 18.2747 - val_MinusLogProbMetric: 18.2747 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 140/1000
2023-10-10 18:14:35.680 
Epoch 140/1000 
	 loss: 18.0163, MinusLogProbMetric: 18.0163, val_loss: 18.6494, val_MinusLogProbMetric: 18.6494

Epoch 140: val_loss did not improve from 17.92469
196/196 - 74s - loss: 18.0163 - MinusLogProbMetric: 18.0163 - val_loss: 18.6494 - val_MinusLogProbMetric: 18.6494 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 141/1000
2023-10-10 18:15:52.771 
Epoch 141/1000 
	 loss: 18.0602, MinusLogProbMetric: 18.0602, val_loss: 18.1080, val_MinusLogProbMetric: 18.1080

Epoch 141: val_loss did not improve from 17.92469
196/196 - 77s - loss: 18.0602 - MinusLogProbMetric: 18.0602 - val_loss: 18.1080 - val_MinusLogProbMetric: 18.1080 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 142/1000
2023-10-10 18:17:05.889 
Epoch 142/1000 
	 loss: 18.0430, MinusLogProbMetric: 18.0430, val_loss: 18.2038, val_MinusLogProbMetric: 18.2038

Epoch 142: val_loss did not improve from 17.92469
196/196 - 73s - loss: 18.0430 - MinusLogProbMetric: 18.0430 - val_loss: 18.2038 - val_MinusLogProbMetric: 18.2038 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 143/1000
2023-10-10 18:18:24.520 
Epoch 143/1000 
	 loss: 17.9931, MinusLogProbMetric: 17.9931, val_loss: 18.5177, val_MinusLogProbMetric: 18.5177

Epoch 143: val_loss did not improve from 17.92469
196/196 - 79s - loss: 17.9931 - MinusLogProbMetric: 17.9931 - val_loss: 18.5177 - val_MinusLogProbMetric: 18.5177 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 144/1000
2023-10-10 18:19:37.182 
Epoch 144/1000 
	 loss: 18.1642, MinusLogProbMetric: 18.1642, val_loss: 18.3593, val_MinusLogProbMetric: 18.3593

Epoch 144: val_loss did not improve from 17.92469
196/196 - 73s - loss: 18.1642 - MinusLogProbMetric: 18.1642 - val_loss: 18.3593 - val_MinusLogProbMetric: 18.3593 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 145/1000
2023-10-10 18:20:55.866 
Epoch 145/1000 
	 loss: 18.0333, MinusLogProbMetric: 18.0333, val_loss: 18.2854, val_MinusLogProbMetric: 18.2854

Epoch 145: val_loss did not improve from 17.92469
196/196 - 79s - loss: 18.0333 - MinusLogProbMetric: 18.0333 - val_loss: 18.2854 - val_MinusLogProbMetric: 18.2854 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 146/1000
2023-10-10 18:22:13.364 
Epoch 146/1000 
	 loss: 17.9466, MinusLogProbMetric: 17.9466, val_loss: 18.3120, val_MinusLogProbMetric: 18.3120

Epoch 146: val_loss did not improve from 17.92469
196/196 - 78s - loss: 17.9466 - MinusLogProbMetric: 17.9466 - val_loss: 18.3120 - val_MinusLogProbMetric: 18.3120 - lr: 3.3333e-04 - 78s/epoch - 395ms/step
Epoch 147/1000
2023-10-10 18:23:28.380 
Epoch 147/1000 
	 loss: 17.9876, MinusLogProbMetric: 17.9876, val_loss: 19.0178, val_MinusLogProbMetric: 19.0178

Epoch 147: val_loss did not improve from 17.92469
196/196 - 75s - loss: 17.9876 - MinusLogProbMetric: 17.9876 - val_loss: 19.0178 - val_MinusLogProbMetric: 19.0178 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 148/1000
2023-10-10 18:24:46.800 
Epoch 148/1000 
	 loss: 18.1234, MinusLogProbMetric: 18.1234, val_loss: 18.1156, val_MinusLogProbMetric: 18.1156

Epoch 148: val_loss did not improve from 17.92469
196/196 - 78s - loss: 18.1234 - MinusLogProbMetric: 18.1234 - val_loss: 18.1156 - val_MinusLogProbMetric: 18.1156 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 149/1000
2023-10-10 18:26:05.454 
Epoch 149/1000 
	 loss: 18.0882, MinusLogProbMetric: 18.0882, val_loss: 18.2771, val_MinusLogProbMetric: 18.2771

Epoch 149: val_loss did not improve from 17.92469
196/196 - 79s - loss: 18.0882 - MinusLogProbMetric: 18.0882 - val_loss: 18.2771 - val_MinusLogProbMetric: 18.2771 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 150/1000
2023-10-10 18:27:20.277 
Epoch 150/1000 
	 loss: 18.0017, MinusLogProbMetric: 18.0017, val_loss: 19.0286, val_MinusLogProbMetric: 19.0286

Epoch 150: val_loss did not improve from 17.92469
196/196 - 75s - loss: 18.0017 - MinusLogProbMetric: 18.0017 - val_loss: 19.0286 - val_MinusLogProbMetric: 19.0286 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 151/1000
2023-10-10 18:28:38.598 
Epoch 151/1000 
	 loss: 18.0477, MinusLogProbMetric: 18.0477, val_loss: 18.2044, val_MinusLogProbMetric: 18.2044

Epoch 151: val_loss did not improve from 17.92469
196/196 - 78s - loss: 18.0477 - MinusLogProbMetric: 18.0477 - val_loss: 18.2044 - val_MinusLogProbMetric: 18.2044 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 152/1000
2023-10-10 18:29:53.384 
Epoch 152/1000 
	 loss: 17.9269, MinusLogProbMetric: 17.9269, val_loss: 18.3422, val_MinusLogProbMetric: 18.3422

Epoch 152: val_loss did not improve from 17.92469
196/196 - 75s - loss: 17.9269 - MinusLogProbMetric: 17.9269 - val_loss: 18.3422 - val_MinusLogProbMetric: 18.3422 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 153/1000
2023-10-10 18:31:09.903 
Epoch 153/1000 
	 loss: 17.9522, MinusLogProbMetric: 17.9522, val_loss: 18.4364, val_MinusLogProbMetric: 18.4364

Epoch 153: val_loss did not improve from 17.92469
196/196 - 77s - loss: 17.9522 - MinusLogProbMetric: 17.9522 - val_loss: 18.4364 - val_MinusLogProbMetric: 18.4364 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 154/1000
2023-10-10 18:32:26.291 
Epoch 154/1000 
	 loss: 17.9638, MinusLogProbMetric: 17.9638, val_loss: 18.0551, val_MinusLogProbMetric: 18.0551

Epoch 154: val_loss did not improve from 17.92469
196/196 - 76s - loss: 17.9638 - MinusLogProbMetric: 17.9638 - val_loss: 18.0551 - val_MinusLogProbMetric: 18.0551 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 155/1000
2023-10-10 18:33:45.225 
Epoch 155/1000 
	 loss: 18.0735, MinusLogProbMetric: 18.0735, val_loss: 18.2335, val_MinusLogProbMetric: 18.2335

Epoch 155: val_loss did not improve from 17.92469
196/196 - 79s - loss: 18.0735 - MinusLogProbMetric: 18.0735 - val_loss: 18.2335 - val_MinusLogProbMetric: 18.2335 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 156/1000
2023-10-10 18:35:00.040 
Epoch 156/1000 
	 loss: 17.9813, MinusLogProbMetric: 17.9813, val_loss: 18.7131, val_MinusLogProbMetric: 18.7131

Epoch 156: val_loss did not improve from 17.92469
196/196 - 75s - loss: 17.9813 - MinusLogProbMetric: 17.9813 - val_loss: 18.7131 - val_MinusLogProbMetric: 18.7131 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 157/1000
2023-10-10 18:36:12.351 
Epoch 157/1000 
	 loss: 17.9438, MinusLogProbMetric: 17.9438, val_loss: 18.1866, val_MinusLogProbMetric: 18.1866

Epoch 157: val_loss did not improve from 17.92469
196/196 - 72s - loss: 17.9438 - MinusLogProbMetric: 17.9438 - val_loss: 18.1866 - val_MinusLogProbMetric: 18.1866 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 158/1000
2023-10-10 18:37:25.317 
Epoch 158/1000 
	 loss: 17.9708, MinusLogProbMetric: 17.9708, val_loss: 18.5573, val_MinusLogProbMetric: 18.5573

Epoch 158: val_loss did not improve from 17.92469
196/196 - 73s - loss: 17.9708 - MinusLogProbMetric: 17.9708 - val_loss: 18.5573 - val_MinusLogProbMetric: 18.5573 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 159/1000
2023-10-10 18:38:39.435 
Epoch 159/1000 
	 loss: 18.0183, MinusLogProbMetric: 18.0183, val_loss: 18.8101, val_MinusLogProbMetric: 18.8101

Epoch 159: val_loss did not improve from 17.92469
196/196 - 74s - loss: 18.0183 - MinusLogProbMetric: 18.0183 - val_loss: 18.8101 - val_MinusLogProbMetric: 18.8101 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 160/1000
2023-10-10 18:39:56.096 
Epoch 160/1000 
	 loss: 17.9910, MinusLogProbMetric: 17.9910, val_loss: 18.1152, val_MinusLogProbMetric: 18.1152

Epoch 160: val_loss did not improve from 17.92469
196/196 - 77s - loss: 17.9910 - MinusLogProbMetric: 17.9910 - val_loss: 18.1152 - val_MinusLogProbMetric: 18.1152 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 161/1000
2023-10-10 18:41:10.848 
Epoch 161/1000 
	 loss: 17.9224, MinusLogProbMetric: 17.9224, val_loss: 18.3427, val_MinusLogProbMetric: 18.3427

Epoch 161: val_loss did not improve from 17.92469
196/196 - 75s - loss: 17.9224 - MinusLogProbMetric: 17.9224 - val_loss: 18.3427 - val_MinusLogProbMetric: 18.3427 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 162/1000
2023-10-10 18:42:30.848 
Epoch 162/1000 
	 loss: 17.9004, MinusLogProbMetric: 17.9004, val_loss: 18.6348, val_MinusLogProbMetric: 18.6348

Epoch 162: val_loss did not improve from 17.92469
196/196 - 80s - loss: 17.9004 - MinusLogProbMetric: 17.9004 - val_loss: 18.6348 - val_MinusLogProbMetric: 18.6348 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 163/1000
2023-10-10 18:43:45.146 
Epoch 163/1000 
	 loss: 17.9026, MinusLogProbMetric: 17.9026, val_loss: 18.8934, val_MinusLogProbMetric: 18.8934

Epoch 163: val_loss did not improve from 17.92469
196/196 - 74s - loss: 17.9026 - MinusLogProbMetric: 17.9026 - val_loss: 18.8934 - val_MinusLogProbMetric: 18.8934 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 164/1000
2023-10-10 18:45:05.102 
Epoch 164/1000 
	 loss: 17.9122, MinusLogProbMetric: 17.9122, val_loss: 18.6806, val_MinusLogProbMetric: 18.6806

Epoch 164: val_loss did not improve from 17.92469
196/196 - 80s - loss: 17.9122 - MinusLogProbMetric: 17.9122 - val_loss: 18.6806 - val_MinusLogProbMetric: 18.6806 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 165/1000
2023-10-10 18:46:25.820 
Epoch 165/1000 
	 loss: 17.9600, MinusLogProbMetric: 17.9600, val_loss: 17.9741, val_MinusLogProbMetric: 17.9741

Epoch 165: val_loss did not improve from 17.92469
196/196 - 81s - loss: 17.9600 - MinusLogProbMetric: 17.9600 - val_loss: 17.9741 - val_MinusLogProbMetric: 17.9741 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 166/1000
2023-10-10 18:47:40.712 
Epoch 166/1000 
	 loss: 17.9351, MinusLogProbMetric: 17.9351, val_loss: 18.8392, val_MinusLogProbMetric: 18.8392

Epoch 166: val_loss did not improve from 17.92469
196/196 - 75s - loss: 17.9351 - MinusLogProbMetric: 17.9351 - val_loss: 18.8392 - val_MinusLogProbMetric: 18.8392 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 167/1000
2023-10-10 18:48:57.601 
Epoch 167/1000 
	 loss: 17.9539, MinusLogProbMetric: 17.9539, val_loss: 18.2585, val_MinusLogProbMetric: 18.2585

Epoch 167: val_loss did not improve from 17.92469
196/196 - 77s - loss: 17.9539 - MinusLogProbMetric: 17.9539 - val_loss: 18.2585 - val_MinusLogProbMetric: 18.2585 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 168/1000
2023-10-10 18:50:13.105 
Epoch 168/1000 
	 loss: 18.0407, MinusLogProbMetric: 18.0407, val_loss: 18.4958, val_MinusLogProbMetric: 18.4958

Epoch 168: val_loss did not improve from 17.92469
196/196 - 75s - loss: 18.0407 - MinusLogProbMetric: 18.0407 - val_loss: 18.4958 - val_MinusLogProbMetric: 18.4958 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 169/1000
2023-10-10 18:51:25.844 
Epoch 169/1000 
	 loss: 17.8897, MinusLogProbMetric: 17.8897, val_loss: 18.0528, val_MinusLogProbMetric: 18.0528

Epoch 169: val_loss did not improve from 17.92469
196/196 - 73s - loss: 17.8897 - MinusLogProbMetric: 17.8897 - val_loss: 18.0528 - val_MinusLogProbMetric: 18.0528 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 170/1000
2023-10-10 18:52:42.000 
Epoch 170/1000 
	 loss: 17.9508, MinusLogProbMetric: 17.9508, val_loss: 18.5589, val_MinusLogProbMetric: 18.5589

Epoch 170: val_loss did not improve from 17.92469
196/196 - 76s - loss: 17.9508 - MinusLogProbMetric: 17.9508 - val_loss: 18.5589 - val_MinusLogProbMetric: 18.5589 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 171/1000
2023-10-10 18:53:56.203 
Epoch 171/1000 
	 loss: 17.9370, MinusLogProbMetric: 17.9370, val_loss: 18.2997, val_MinusLogProbMetric: 18.2997

Epoch 171: val_loss did not improve from 17.92469
196/196 - 74s - loss: 17.9370 - MinusLogProbMetric: 17.9370 - val_loss: 18.2997 - val_MinusLogProbMetric: 18.2997 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 172/1000
2023-10-10 18:55:13.960 
Epoch 172/1000 
	 loss: 17.8341, MinusLogProbMetric: 17.8341, val_loss: 17.8398, val_MinusLogProbMetric: 17.8398

Epoch 172: val_loss improved from 17.92469 to 17.83975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 80s - loss: 17.8341 - MinusLogProbMetric: 17.8341 - val_loss: 17.8398 - val_MinusLogProbMetric: 17.8398 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 173/1000
2023-10-10 18:56:32.675 
Epoch 173/1000 
	 loss: 17.8809, MinusLogProbMetric: 17.8809, val_loss: 19.1636, val_MinusLogProbMetric: 19.1636

Epoch 173: val_loss did not improve from 17.83975
196/196 - 77s - loss: 17.8809 - MinusLogProbMetric: 17.8809 - val_loss: 19.1636 - val_MinusLogProbMetric: 19.1636 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 174/1000
2023-10-10 18:57:47.934 
Epoch 174/1000 
	 loss: 17.9002, MinusLogProbMetric: 17.9002, val_loss: 18.2588, val_MinusLogProbMetric: 18.2588

Epoch 174: val_loss did not improve from 17.83975
196/196 - 75s - loss: 17.9002 - MinusLogProbMetric: 17.9002 - val_loss: 18.2588 - val_MinusLogProbMetric: 18.2588 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 175/1000
2023-10-10 18:59:00.520 
Epoch 175/1000 
	 loss: 17.8223, MinusLogProbMetric: 17.8223, val_loss: 18.2512, val_MinusLogProbMetric: 18.2512

Epoch 175: val_loss did not improve from 17.83975
196/196 - 73s - loss: 17.8223 - MinusLogProbMetric: 17.8223 - val_loss: 18.2512 - val_MinusLogProbMetric: 18.2512 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 176/1000
2023-10-10 19:00:15.531 
Epoch 176/1000 
	 loss: 17.8502, MinusLogProbMetric: 17.8502, val_loss: 18.5555, val_MinusLogProbMetric: 18.5555

Epoch 176: val_loss did not improve from 17.83975
196/196 - 75s - loss: 17.8502 - MinusLogProbMetric: 17.8502 - val_loss: 18.5555 - val_MinusLogProbMetric: 18.5555 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 177/1000
2023-10-10 19:01:28.843 
Epoch 177/1000 
	 loss: 17.8398, MinusLogProbMetric: 17.8398, val_loss: 18.0049, val_MinusLogProbMetric: 18.0049

Epoch 177: val_loss did not improve from 17.83975
196/196 - 73s - loss: 17.8398 - MinusLogProbMetric: 17.8398 - val_loss: 18.0049 - val_MinusLogProbMetric: 18.0049 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 178/1000
2023-10-10 19:02:42.305 
Epoch 178/1000 
	 loss: 17.8309, MinusLogProbMetric: 17.8309, val_loss: 18.0987, val_MinusLogProbMetric: 18.0987

Epoch 178: val_loss did not improve from 17.83975
196/196 - 73s - loss: 17.8309 - MinusLogProbMetric: 17.8309 - val_loss: 18.0987 - val_MinusLogProbMetric: 18.0987 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 179/1000
2023-10-10 19:03:56.049 
Epoch 179/1000 
	 loss: 17.8331, MinusLogProbMetric: 17.8331, val_loss: 18.8959, val_MinusLogProbMetric: 18.8959

Epoch 179: val_loss did not improve from 17.83975
196/196 - 74s - loss: 17.8331 - MinusLogProbMetric: 17.8331 - val_loss: 18.8959 - val_MinusLogProbMetric: 18.8959 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 180/1000
2023-10-10 19:05:11.269 
Epoch 180/1000 
	 loss: 17.8490, MinusLogProbMetric: 17.8490, val_loss: 18.3556, val_MinusLogProbMetric: 18.3556

Epoch 180: val_loss did not improve from 17.83975
196/196 - 75s - loss: 17.8490 - MinusLogProbMetric: 17.8490 - val_loss: 18.3556 - val_MinusLogProbMetric: 18.3556 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 181/1000
2023-10-10 19:06:24.679 
Epoch 181/1000 
	 loss: 17.9215, MinusLogProbMetric: 17.9215, val_loss: 17.8638, val_MinusLogProbMetric: 17.8638

Epoch 181: val_loss did not improve from 17.83975
196/196 - 73s - loss: 17.9215 - MinusLogProbMetric: 17.9215 - val_loss: 17.8638 - val_MinusLogProbMetric: 17.8638 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 182/1000
2023-10-10 19:07:37.751 
Epoch 182/1000 
	 loss: 17.9099, MinusLogProbMetric: 17.9099, val_loss: 17.9702, val_MinusLogProbMetric: 17.9702

Epoch 182: val_loss did not improve from 17.83975
196/196 - 73s - loss: 17.9099 - MinusLogProbMetric: 17.9099 - val_loss: 17.9702 - val_MinusLogProbMetric: 17.9702 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 183/1000
2023-10-10 19:08:54.078 
Epoch 183/1000 
	 loss: 17.7784, MinusLogProbMetric: 17.7784, val_loss: 18.0072, val_MinusLogProbMetric: 18.0072

Epoch 183: val_loss did not improve from 17.83975
196/196 - 76s - loss: 17.7784 - MinusLogProbMetric: 17.7784 - val_loss: 18.0072 - val_MinusLogProbMetric: 18.0072 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 184/1000
2023-10-10 19:10:08.308 
Epoch 184/1000 
	 loss: 17.8261, MinusLogProbMetric: 17.8261, val_loss: 17.6666, val_MinusLogProbMetric: 17.6666

Epoch 184: val_loss improved from 17.83975 to 17.66662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 77s - loss: 17.8261 - MinusLogProbMetric: 17.8261 - val_loss: 17.6666 - val_MinusLogProbMetric: 17.6666 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 185/1000
2023-10-10 19:11:26.507 
Epoch 185/1000 
	 loss: 17.8195, MinusLogProbMetric: 17.8195, val_loss: 18.4222, val_MinusLogProbMetric: 18.4222

Epoch 185: val_loss did not improve from 17.66662
196/196 - 75s - loss: 17.8195 - MinusLogProbMetric: 17.8195 - val_loss: 18.4222 - val_MinusLogProbMetric: 18.4222 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 186/1000
2023-10-10 19:12:39.576 
Epoch 186/1000 
	 loss: 17.7645, MinusLogProbMetric: 17.7645, val_loss: 18.0264, val_MinusLogProbMetric: 18.0264

Epoch 186: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.7645 - MinusLogProbMetric: 17.7645 - val_loss: 18.0264 - val_MinusLogProbMetric: 18.0264 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 187/1000
2023-10-10 19:13:50.717 
Epoch 187/1000 
	 loss: 17.9339, MinusLogProbMetric: 17.9339, val_loss: 18.1174, val_MinusLogProbMetric: 18.1174

Epoch 187: val_loss did not improve from 17.66662
196/196 - 71s - loss: 17.9339 - MinusLogProbMetric: 17.9339 - val_loss: 18.1174 - val_MinusLogProbMetric: 18.1174 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 188/1000
2023-10-10 19:15:03.152 
Epoch 188/1000 
	 loss: 17.8436, MinusLogProbMetric: 17.8436, val_loss: 19.5982, val_MinusLogProbMetric: 19.5982

Epoch 188: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.8436 - MinusLogProbMetric: 17.8436 - val_loss: 19.5982 - val_MinusLogProbMetric: 19.5982 - lr: 3.3333e-04 - 72s/epoch - 370ms/step
Epoch 189/1000
2023-10-10 19:16:17.408 
Epoch 189/1000 
	 loss: 17.7918, MinusLogProbMetric: 17.7918, val_loss: 20.0057, val_MinusLogProbMetric: 20.0057

Epoch 189: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.7918 - MinusLogProbMetric: 17.7918 - val_loss: 20.0057 - val_MinusLogProbMetric: 20.0057 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 190/1000
2023-10-10 19:17:30.149 
Epoch 190/1000 
	 loss: 17.7994, MinusLogProbMetric: 17.7994, val_loss: 17.9526, val_MinusLogProbMetric: 17.9526

Epoch 190: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.7994 - MinusLogProbMetric: 17.7994 - val_loss: 17.9526 - val_MinusLogProbMetric: 17.9526 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 191/1000
2023-10-10 19:18:43.486 
Epoch 191/1000 
	 loss: 17.8351, MinusLogProbMetric: 17.8351, val_loss: 17.9716, val_MinusLogProbMetric: 17.9716

Epoch 191: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.8351 - MinusLogProbMetric: 17.8351 - val_loss: 17.9716 - val_MinusLogProbMetric: 17.9716 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 192/1000
2023-10-10 19:19:57.769 
Epoch 192/1000 
	 loss: 17.6984, MinusLogProbMetric: 17.6984, val_loss: 17.6703, val_MinusLogProbMetric: 17.6703

Epoch 192: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.6984 - MinusLogProbMetric: 17.6984 - val_loss: 17.6703 - val_MinusLogProbMetric: 17.6703 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 193/1000
2023-10-10 19:21:11.610 
Epoch 193/1000 
	 loss: 17.8285, MinusLogProbMetric: 17.8285, val_loss: 17.9072, val_MinusLogProbMetric: 17.9072

Epoch 193: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.8285 - MinusLogProbMetric: 17.8285 - val_loss: 17.9072 - val_MinusLogProbMetric: 17.9072 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 194/1000
2023-10-10 19:22:23.876 
Epoch 194/1000 
	 loss: 17.7139, MinusLogProbMetric: 17.7139, val_loss: 18.6119, val_MinusLogProbMetric: 18.6119

Epoch 194: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.7139 - MinusLogProbMetric: 17.7139 - val_loss: 18.6119 - val_MinusLogProbMetric: 18.6119 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 195/1000
2023-10-10 19:23:37.521 
Epoch 195/1000 
	 loss: 17.8534, MinusLogProbMetric: 17.8534, val_loss: 18.1290, val_MinusLogProbMetric: 18.1290

Epoch 195: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.8534 - MinusLogProbMetric: 17.8534 - val_loss: 18.1290 - val_MinusLogProbMetric: 18.1290 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 196/1000
2023-10-10 19:24:49.693 
Epoch 196/1000 
	 loss: 17.7302, MinusLogProbMetric: 17.7302, val_loss: 18.0833, val_MinusLogProbMetric: 18.0833

Epoch 196: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.7302 - MinusLogProbMetric: 17.7302 - val_loss: 18.0833 - val_MinusLogProbMetric: 18.0833 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 197/1000
2023-10-10 19:26:01.982 
Epoch 197/1000 
	 loss: 17.7176, MinusLogProbMetric: 17.7176, val_loss: 17.7267, val_MinusLogProbMetric: 17.7267

Epoch 197: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.7176 - MinusLogProbMetric: 17.7176 - val_loss: 17.7267 - val_MinusLogProbMetric: 17.7267 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 198/1000
2023-10-10 19:27:15.470 
Epoch 198/1000 
	 loss: 17.7621, MinusLogProbMetric: 17.7621, val_loss: 18.0998, val_MinusLogProbMetric: 18.0998

Epoch 198: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.7621 - MinusLogProbMetric: 17.7621 - val_loss: 18.0998 - val_MinusLogProbMetric: 18.0998 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 199/1000
2023-10-10 19:28:28.594 
Epoch 199/1000 
	 loss: 17.7189, MinusLogProbMetric: 17.7189, val_loss: 20.3380, val_MinusLogProbMetric: 20.3380

Epoch 199: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.7189 - MinusLogProbMetric: 17.7189 - val_loss: 20.3380 - val_MinusLogProbMetric: 20.3380 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 200/1000
2023-10-10 19:29:41.888 
Epoch 200/1000 
	 loss: 17.7212, MinusLogProbMetric: 17.7212, val_loss: 21.0980, val_MinusLogProbMetric: 21.0980

Epoch 200: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.7212 - MinusLogProbMetric: 17.7212 - val_loss: 21.0980 - val_MinusLogProbMetric: 21.0980 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 201/1000
2023-10-10 19:30:54.201 
Epoch 201/1000 
	 loss: 17.7199, MinusLogProbMetric: 17.7199, val_loss: 17.8933, val_MinusLogProbMetric: 17.8933

Epoch 201: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.7199 - MinusLogProbMetric: 17.7199 - val_loss: 17.8933 - val_MinusLogProbMetric: 17.8933 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 202/1000
2023-10-10 19:32:08.729 
Epoch 202/1000 
	 loss: 17.7369, MinusLogProbMetric: 17.7369, val_loss: 22.7559, val_MinusLogProbMetric: 22.7559

Epoch 202: val_loss did not improve from 17.66662
196/196 - 75s - loss: 17.7369 - MinusLogProbMetric: 17.7369 - val_loss: 22.7559 - val_MinusLogProbMetric: 22.7559 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 203/1000
2023-10-10 19:33:22.261 
Epoch 203/1000 
	 loss: 17.9028, MinusLogProbMetric: 17.9028, val_loss: 18.8119, val_MinusLogProbMetric: 18.8119

Epoch 203: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.9028 - MinusLogProbMetric: 17.9028 - val_loss: 18.8119 - val_MinusLogProbMetric: 18.8119 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 204/1000
2023-10-10 19:34:35.938 
Epoch 204/1000 
	 loss: 17.6679, MinusLogProbMetric: 17.6679, val_loss: 18.7824, val_MinusLogProbMetric: 18.7824

Epoch 204: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.6679 - MinusLogProbMetric: 17.6679 - val_loss: 18.7824 - val_MinusLogProbMetric: 18.7824 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 205/1000
2023-10-10 19:35:48.976 
Epoch 205/1000 
	 loss: 17.7459, MinusLogProbMetric: 17.7459, val_loss: 18.1637, val_MinusLogProbMetric: 18.1637

Epoch 205: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.7459 - MinusLogProbMetric: 17.7459 - val_loss: 18.1637 - val_MinusLogProbMetric: 18.1637 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 206/1000
2023-10-10 19:37:00.424 
Epoch 206/1000 
	 loss: 17.6875, MinusLogProbMetric: 17.6875, val_loss: 19.4674, val_MinusLogProbMetric: 19.4674

Epoch 206: val_loss did not improve from 17.66662
196/196 - 71s - loss: 17.6875 - MinusLogProbMetric: 17.6875 - val_loss: 19.4674 - val_MinusLogProbMetric: 19.4674 - lr: 3.3333e-04 - 71s/epoch - 365ms/step
Epoch 207/1000
2023-10-10 19:38:13.073 
Epoch 207/1000 
	 loss: 17.8115, MinusLogProbMetric: 17.8115, val_loss: 18.4075, val_MinusLogProbMetric: 18.4075

Epoch 207: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.8115 - MinusLogProbMetric: 17.8115 - val_loss: 18.4075 - val_MinusLogProbMetric: 18.4075 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 208/1000
2023-10-10 19:39:24.263 
Epoch 208/1000 
	 loss: 17.6743, MinusLogProbMetric: 17.6743, val_loss: 18.5891, val_MinusLogProbMetric: 18.5891

Epoch 208: val_loss did not improve from 17.66662
196/196 - 71s - loss: 17.6743 - MinusLogProbMetric: 17.6743 - val_loss: 18.5891 - val_MinusLogProbMetric: 18.5891 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 209/1000
2023-10-10 19:40:36.349 
Epoch 209/1000 
	 loss: 17.7134, MinusLogProbMetric: 17.7134, val_loss: 17.7269, val_MinusLogProbMetric: 17.7269

Epoch 209: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.7134 - MinusLogProbMetric: 17.7134 - val_loss: 17.7269 - val_MinusLogProbMetric: 17.7269 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 210/1000
2023-10-10 19:41:50.188 
Epoch 210/1000 
	 loss: 17.7016, MinusLogProbMetric: 17.7016, val_loss: 18.4756, val_MinusLogProbMetric: 18.4756

Epoch 210: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.7016 - MinusLogProbMetric: 17.7016 - val_loss: 18.4756 - val_MinusLogProbMetric: 18.4756 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 211/1000
2023-10-10 19:43:06.173 
Epoch 211/1000 
	 loss: 17.7333, MinusLogProbMetric: 17.7333, val_loss: 17.8479, val_MinusLogProbMetric: 17.8479

Epoch 211: val_loss did not improve from 17.66662
196/196 - 76s - loss: 17.7333 - MinusLogProbMetric: 17.7333 - val_loss: 17.8479 - val_MinusLogProbMetric: 17.8479 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 212/1000
2023-10-10 19:44:22.340 
Epoch 212/1000 
	 loss: 17.6751, MinusLogProbMetric: 17.6751, val_loss: 17.9926, val_MinusLogProbMetric: 17.9926

Epoch 212: val_loss did not improve from 17.66662
196/196 - 76s - loss: 17.6751 - MinusLogProbMetric: 17.6751 - val_loss: 17.9926 - val_MinusLogProbMetric: 17.9926 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 213/1000
2023-10-10 19:45:37.446 
Epoch 213/1000 
	 loss: 17.6627, MinusLogProbMetric: 17.6627, val_loss: 17.8881, val_MinusLogProbMetric: 17.8881

Epoch 213: val_loss did not improve from 17.66662
196/196 - 75s - loss: 17.6627 - MinusLogProbMetric: 17.6627 - val_loss: 17.8881 - val_MinusLogProbMetric: 17.8881 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 214/1000
2023-10-10 19:46:51.117 
Epoch 214/1000 
	 loss: 17.8391, MinusLogProbMetric: 17.8391, val_loss: 18.0284, val_MinusLogProbMetric: 18.0284

Epoch 214: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.8391 - MinusLogProbMetric: 17.8391 - val_loss: 18.0284 - val_MinusLogProbMetric: 18.0284 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 215/1000
2023-10-10 19:48:05.617 
Epoch 215/1000 
	 loss: 17.6231, MinusLogProbMetric: 17.6231, val_loss: 17.6981, val_MinusLogProbMetric: 17.6981

Epoch 215: val_loss did not improve from 17.66662
196/196 - 75s - loss: 17.6231 - MinusLogProbMetric: 17.6231 - val_loss: 17.6981 - val_MinusLogProbMetric: 17.6981 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 216/1000
2023-10-10 19:49:18.405 
Epoch 216/1000 
	 loss: 17.6959, MinusLogProbMetric: 17.6959, val_loss: 18.5332, val_MinusLogProbMetric: 18.5332

Epoch 216: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.6959 - MinusLogProbMetric: 17.6959 - val_loss: 18.5332 - val_MinusLogProbMetric: 18.5332 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 217/1000
2023-10-10 19:50:31.056 
Epoch 217/1000 
	 loss: 17.6905, MinusLogProbMetric: 17.6905, val_loss: 18.3021, val_MinusLogProbMetric: 18.3021

Epoch 217: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.6905 - MinusLogProbMetric: 17.6905 - val_loss: 18.3021 - val_MinusLogProbMetric: 18.3021 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 218/1000
2023-10-10 19:51:45.139 
Epoch 218/1000 
	 loss: 17.7081, MinusLogProbMetric: 17.7081, val_loss: 18.9431, val_MinusLogProbMetric: 18.9431

Epoch 218: val_loss did not improve from 17.66662
196/196 - 74s - loss: 17.7081 - MinusLogProbMetric: 17.7081 - val_loss: 18.9431 - val_MinusLogProbMetric: 18.9431 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 219/1000
2023-10-10 19:52:57.434 
Epoch 219/1000 
	 loss: 17.6153, MinusLogProbMetric: 17.6153, val_loss: 18.1050, val_MinusLogProbMetric: 18.1050

Epoch 219: val_loss did not improve from 17.66662
196/196 - 72s - loss: 17.6153 - MinusLogProbMetric: 17.6153 - val_loss: 18.1050 - val_MinusLogProbMetric: 18.1050 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 220/1000
2023-10-10 19:54:12.387 
Epoch 220/1000 
	 loss: 17.6964, MinusLogProbMetric: 17.6964, val_loss: 18.1873, val_MinusLogProbMetric: 18.1873

Epoch 220: val_loss did not improve from 17.66662
196/196 - 75s - loss: 17.6964 - MinusLogProbMetric: 17.6964 - val_loss: 18.1873 - val_MinusLogProbMetric: 18.1873 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 221/1000
2023-10-10 19:55:25.704 
Epoch 221/1000 
	 loss: 17.6675, MinusLogProbMetric: 17.6675, val_loss: 18.0734, val_MinusLogProbMetric: 18.0734

Epoch 221: val_loss did not improve from 17.66662
196/196 - 73s - loss: 17.6675 - MinusLogProbMetric: 17.6675 - val_loss: 18.0734 - val_MinusLogProbMetric: 18.0734 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 222/1000
2023-10-10 19:56:39.531 
Epoch 222/1000 
	 loss: 17.6518, MinusLogProbMetric: 17.6518, val_loss: 17.6254, val_MinusLogProbMetric: 17.6254

Epoch 222: val_loss improved from 17.66662 to 17.62538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 74s - loss: 17.6518 - MinusLogProbMetric: 17.6518 - val_loss: 17.6254 - val_MinusLogProbMetric: 17.6254 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 223/1000
2023-10-10 19:57:52.371 
Epoch 223/1000 
	 loss: 17.7195, MinusLogProbMetric: 17.7195, val_loss: 18.6943, val_MinusLogProbMetric: 18.6943

Epoch 223: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.7195 - MinusLogProbMetric: 17.7195 - val_loss: 18.6943 - val_MinusLogProbMetric: 18.6943 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 224/1000
2023-10-10 19:59:07.439 
Epoch 224/1000 
	 loss: 17.6313, MinusLogProbMetric: 17.6313, val_loss: 19.0995, val_MinusLogProbMetric: 19.0995

Epoch 224: val_loss did not improve from 17.62538
196/196 - 75s - loss: 17.6313 - MinusLogProbMetric: 17.6313 - val_loss: 19.0995 - val_MinusLogProbMetric: 19.0995 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 225/1000
2023-10-10 20:00:21.548 
Epoch 225/1000 
	 loss: 17.6743, MinusLogProbMetric: 17.6743, val_loss: 18.5036, val_MinusLogProbMetric: 18.5036

Epoch 225: val_loss did not improve from 17.62538
196/196 - 74s - loss: 17.6743 - MinusLogProbMetric: 17.6743 - val_loss: 18.5036 - val_MinusLogProbMetric: 18.5036 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 226/1000
2023-10-10 20:01:35.258 
Epoch 226/1000 
	 loss: 17.5828, MinusLogProbMetric: 17.5828, val_loss: 17.8557, val_MinusLogProbMetric: 17.8557

Epoch 226: val_loss did not improve from 17.62538
196/196 - 74s - loss: 17.5828 - MinusLogProbMetric: 17.5828 - val_loss: 17.8557 - val_MinusLogProbMetric: 17.8557 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 227/1000
2023-10-10 20:02:50.410 
Epoch 227/1000 
	 loss: 17.7222, MinusLogProbMetric: 17.7222, val_loss: 18.2322, val_MinusLogProbMetric: 18.2322

Epoch 227: val_loss did not improve from 17.62538
196/196 - 75s - loss: 17.7222 - MinusLogProbMetric: 17.7222 - val_loss: 18.2322 - val_MinusLogProbMetric: 18.2322 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 228/1000
2023-10-10 20:04:03.160 
Epoch 228/1000 
	 loss: 17.6119, MinusLogProbMetric: 17.6119, val_loss: 17.9211, val_MinusLogProbMetric: 17.9211

Epoch 228: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.6119 - MinusLogProbMetric: 17.6119 - val_loss: 17.9211 - val_MinusLogProbMetric: 17.9211 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 229/1000
2023-10-10 20:05:16.319 
Epoch 229/1000 
	 loss: 17.6053, MinusLogProbMetric: 17.6053, val_loss: 18.2933, val_MinusLogProbMetric: 18.2933

Epoch 229: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.6053 - MinusLogProbMetric: 17.6053 - val_loss: 18.2933 - val_MinusLogProbMetric: 18.2933 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 230/1000
2023-10-10 20:06:29.451 
Epoch 230/1000 
	 loss: 17.6615, MinusLogProbMetric: 17.6615, val_loss: 17.9352, val_MinusLogProbMetric: 17.9352

Epoch 230: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.6615 - MinusLogProbMetric: 17.6615 - val_loss: 17.9352 - val_MinusLogProbMetric: 17.9352 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 231/1000
2023-10-10 20:07:41.750 
Epoch 231/1000 
	 loss: 17.5575, MinusLogProbMetric: 17.5575, val_loss: 19.0699, val_MinusLogProbMetric: 19.0699

Epoch 231: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5575 - MinusLogProbMetric: 17.5575 - val_loss: 19.0699 - val_MinusLogProbMetric: 19.0699 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 232/1000
2023-10-10 20:08:57.167 
Epoch 232/1000 
	 loss: 17.6480, MinusLogProbMetric: 17.6480, val_loss: 18.1669, val_MinusLogProbMetric: 18.1669

Epoch 232: val_loss did not improve from 17.62538
196/196 - 75s - loss: 17.6480 - MinusLogProbMetric: 17.6480 - val_loss: 18.1669 - val_MinusLogProbMetric: 18.1669 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 233/1000
2023-10-10 20:10:09.628 
Epoch 233/1000 
	 loss: 17.5891, MinusLogProbMetric: 17.5891, val_loss: 19.2454, val_MinusLogProbMetric: 19.2454

Epoch 233: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5891 - MinusLogProbMetric: 17.5891 - val_loss: 19.2454 - val_MinusLogProbMetric: 19.2454 - lr: 3.3333e-04 - 72s/epoch - 370ms/step
Epoch 234/1000
2023-10-10 20:11:20.718 
Epoch 234/1000 
	 loss: 17.6153, MinusLogProbMetric: 17.6153, val_loss: 18.9693, val_MinusLogProbMetric: 18.9693

Epoch 234: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.6153 - MinusLogProbMetric: 17.6153 - val_loss: 18.9693 - val_MinusLogProbMetric: 18.9693 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 235/1000
2023-10-10 20:12:33.049 
Epoch 235/1000 
	 loss: 17.5991, MinusLogProbMetric: 17.5991, val_loss: 18.4676, val_MinusLogProbMetric: 18.4676

Epoch 235: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5991 - MinusLogProbMetric: 17.5991 - val_loss: 18.4676 - val_MinusLogProbMetric: 18.4676 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 236/1000
2023-10-10 20:13:46.020 
Epoch 236/1000 
	 loss: 17.5777, MinusLogProbMetric: 17.5777, val_loss: 17.6834, val_MinusLogProbMetric: 17.6834

Epoch 236: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.5777 - MinusLogProbMetric: 17.5777 - val_loss: 17.6834 - val_MinusLogProbMetric: 17.6834 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 237/1000
2023-10-10 20:14:57.949 
Epoch 237/1000 
	 loss: 17.6266, MinusLogProbMetric: 17.6266, val_loss: 18.0560, val_MinusLogProbMetric: 18.0560

Epoch 237: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.6266 - MinusLogProbMetric: 17.6266 - val_loss: 18.0560 - val_MinusLogProbMetric: 18.0560 - lr: 3.3333e-04 - 72s/epoch - 367ms/step
Epoch 238/1000
2023-10-10 20:16:10.269 
Epoch 238/1000 
	 loss: 17.5220, MinusLogProbMetric: 17.5220, val_loss: 18.6880, val_MinusLogProbMetric: 18.6880

Epoch 238: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5220 - MinusLogProbMetric: 17.5220 - val_loss: 18.6880 - val_MinusLogProbMetric: 18.6880 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 239/1000
2023-10-10 20:17:21.839 
Epoch 239/1000 
	 loss: 17.7357, MinusLogProbMetric: 17.7357, val_loss: 17.9026, val_MinusLogProbMetric: 17.9026

Epoch 239: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.7357 - MinusLogProbMetric: 17.7357 - val_loss: 17.9026 - val_MinusLogProbMetric: 17.9026 - lr: 3.3333e-04 - 72s/epoch - 365ms/step
Epoch 240/1000
2023-10-10 20:18:29.883 
Epoch 240/1000 
	 loss: 17.5507, MinusLogProbMetric: 17.5507, val_loss: 17.8771, val_MinusLogProbMetric: 17.8771

Epoch 240: val_loss did not improve from 17.62538
196/196 - 68s - loss: 17.5507 - MinusLogProbMetric: 17.5507 - val_loss: 17.8771 - val_MinusLogProbMetric: 17.8771 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 241/1000
2023-10-10 20:19:41.357 
Epoch 241/1000 
	 loss: 17.6072, MinusLogProbMetric: 17.6072, val_loss: 17.9808, val_MinusLogProbMetric: 17.9808

Epoch 241: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.6072 - MinusLogProbMetric: 17.6072 - val_loss: 17.9808 - val_MinusLogProbMetric: 17.9808 - lr: 3.3333e-04 - 71s/epoch - 365ms/step
Epoch 242/1000
2023-10-10 20:20:52.509 
Epoch 242/1000 
	 loss: 17.4828, MinusLogProbMetric: 17.4828, val_loss: 17.8188, val_MinusLogProbMetric: 17.8188

Epoch 242: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.4828 - MinusLogProbMetric: 17.4828 - val_loss: 17.8188 - val_MinusLogProbMetric: 17.8188 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 243/1000
2023-10-10 20:22:05.951 
Epoch 243/1000 
	 loss: 17.6477, MinusLogProbMetric: 17.6477, val_loss: 18.1247, val_MinusLogProbMetric: 18.1247

Epoch 243: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.6477 - MinusLogProbMetric: 17.6477 - val_loss: 18.1247 - val_MinusLogProbMetric: 18.1247 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 244/1000
2023-10-10 20:23:19.178 
Epoch 244/1000 
	 loss: 17.6097, MinusLogProbMetric: 17.6097, val_loss: 19.4502, val_MinusLogProbMetric: 19.4502

Epoch 244: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.6097 - MinusLogProbMetric: 17.6097 - val_loss: 19.4502 - val_MinusLogProbMetric: 19.4502 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 245/1000
2023-10-10 20:24:31.145 
Epoch 245/1000 
	 loss: 17.5773, MinusLogProbMetric: 17.5773, val_loss: 17.9838, val_MinusLogProbMetric: 17.9838

Epoch 245: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5773 - MinusLogProbMetric: 17.5773 - val_loss: 17.9838 - val_MinusLogProbMetric: 17.9838 - lr: 3.3333e-04 - 72s/epoch - 367ms/step
Epoch 246/1000
2023-10-10 20:25:42.890 
Epoch 246/1000 
	 loss: 17.5420, MinusLogProbMetric: 17.5420, val_loss: 18.0958, val_MinusLogProbMetric: 18.0958

Epoch 246: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5420 - MinusLogProbMetric: 17.5420 - val_loss: 18.0958 - val_MinusLogProbMetric: 18.0958 - lr: 3.3333e-04 - 72s/epoch - 366ms/step
Epoch 247/1000
2023-10-10 20:26:55.096 
Epoch 247/1000 
	 loss: 17.5631, MinusLogProbMetric: 17.5631, val_loss: 18.0974, val_MinusLogProbMetric: 18.0974

Epoch 247: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5631 - MinusLogProbMetric: 17.5631 - val_loss: 18.0974 - val_MinusLogProbMetric: 18.0974 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 248/1000
2023-10-10 20:28:07.138 
Epoch 248/1000 
	 loss: 17.5526, MinusLogProbMetric: 17.5526, val_loss: 18.6712, val_MinusLogProbMetric: 18.6712

Epoch 248: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5526 - MinusLogProbMetric: 17.5526 - val_loss: 18.6712 - val_MinusLogProbMetric: 18.6712 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 249/1000
2023-10-10 20:29:19.276 
Epoch 249/1000 
	 loss: 17.6600, MinusLogProbMetric: 17.6600, val_loss: 18.6368, val_MinusLogProbMetric: 18.6368

Epoch 249: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.6600 - MinusLogProbMetric: 17.6600 - val_loss: 18.6368 - val_MinusLogProbMetric: 18.6368 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 250/1000
2023-10-10 20:30:29.358 
Epoch 250/1000 
	 loss: 17.5210, MinusLogProbMetric: 17.5210, val_loss: 17.8860, val_MinusLogProbMetric: 17.8860

Epoch 250: val_loss did not improve from 17.62538
196/196 - 70s - loss: 17.5210 - MinusLogProbMetric: 17.5210 - val_loss: 17.8860 - val_MinusLogProbMetric: 17.8860 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 251/1000
2023-10-10 20:31:42.679 
Epoch 251/1000 
	 loss: 17.5231, MinusLogProbMetric: 17.5231, val_loss: 17.8211, val_MinusLogProbMetric: 17.8211

Epoch 251: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.5231 - MinusLogProbMetric: 17.5231 - val_loss: 17.8211 - val_MinusLogProbMetric: 17.8211 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 252/1000
2023-10-10 20:32:54.290 
Epoch 252/1000 
	 loss: 17.5745, MinusLogProbMetric: 17.5745, val_loss: 17.8794, val_MinusLogProbMetric: 17.8794

Epoch 252: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.5745 - MinusLogProbMetric: 17.5745 - val_loss: 17.8794 - val_MinusLogProbMetric: 17.8794 - lr: 3.3333e-04 - 72s/epoch - 365ms/step
Epoch 253/1000
2023-10-10 20:34:04.815 
Epoch 253/1000 
	 loss: 17.6321, MinusLogProbMetric: 17.6321, val_loss: 17.6321, val_MinusLogProbMetric: 17.6321

Epoch 253: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.6321 - MinusLogProbMetric: 17.6321 - val_loss: 17.6321 - val_MinusLogProbMetric: 17.6321 - lr: 3.3333e-04 - 71s/epoch - 360ms/step
Epoch 254/1000
2023-10-10 20:35:15.893 
Epoch 254/1000 
	 loss: 17.5785, MinusLogProbMetric: 17.5785, val_loss: 17.8188, val_MinusLogProbMetric: 17.8188

Epoch 254: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.5785 - MinusLogProbMetric: 17.5785 - val_loss: 17.8188 - val_MinusLogProbMetric: 17.8188 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 255/1000
2023-10-10 20:36:26.853 
Epoch 255/1000 
	 loss: 17.5522, MinusLogProbMetric: 17.5522, val_loss: 17.6696, val_MinusLogProbMetric: 17.6696

Epoch 255: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.5522 - MinusLogProbMetric: 17.5522 - val_loss: 17.6696 - val_MinusLogProbMetric: 17.6696 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 256/1000
2023-10-10 20:37:37.079 
Epoch 256/1000 
	 loss: 17.5339, MinusLogProbMetric: 17.5339, val_loss: 17.9360, val_MinusLogProbMetric: 17.9360

Epoch 256: val_loss did not improve from 17.62538
196/196 - 70s - loss: 17.5339 - MinusLogProbMetric: 17.5339 - val_loss: 17.9360 - val_MinusLogProbMetric: 17.9360 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 257/1000
2023-10-10 20:38:48.380 
Epoch 257/1000 
	 loss: 17.4710, MinusLogProbMetric: 17.4710, val_loss: 18.4338, val_MinusLogProbMetric: 18.4338

Epoch 257: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.4710 - MinusLogProbMetric: 17.4710 - val_loss: 18.4338 - val_MinusLogProbMetric: 18.4338 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 258/1000
2023-10-10 20:40:01.568 
Epoch 258/1000 
	 loss: 17.5037, MinusLogProbMetric: 17.5037, val_loss: 17.6997, val_MinusLogProbMetric: 17.6997

Epoch 258: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.5037 - MinusLogProbMetric: 17.5037 - val_loss: 17.6997 - val_MinusLogProbMetric: 17.6997 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 259/1000
2023-10-10 20:41:13.050 
Epoch 259/1000 
	 loss: 17.5094, MinusLogProbMetric: 17.5094, val_loss: 18.0251, val_MinusLogProbMetric: 18.0251

Epoch 259: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.5094 - MinusLogProbMetric: 17.5094 - val_loss: 18.0251 - val_MinusLogProbMetric: 18.0251 - lr: 3.3333e-04 - 71s/epoch - 365ms/step
Epoch 260/1000
2023-10-10 20:42:25.815 
Epoch 260/1000 
	 loss: 17.5185, MinusLogProbMetric: 17.5185, val_loss: 17.8999, val_MinusLogProbMetric: 17.8999

Epoch 260: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.5185 - MinusLogProbMetric: 17.5185 - val_loss: 17.8999 - val_MinusLogProbMetric: 17.8999 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 261/1000
2023-10-10 20:43:40.202 
Epoch 261/1000 
	 loss: 17.5162, MinusLogProbMetric: 17.5162, val_loss: 17.8898, val_MinusLogProbMetric: 17.8898

Epoch 261: val_loss did not improve from 17.62538
196/196 - 74s - loss: 17.5162 - MinusLogProbMetric: 17.5162 - val_loss: 17.8898 - val_MinusLogProbMetric: 17.8898 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 262/1000
2023-10-10 20:44:54.551 
Epoch 262/1000 
	 loss: 17.5238, MinusLogProbMetric: 17.5238, val_loss: 17.8161, val_MinusLogProbMetric: 17.8161

Epoch 262: val_loss did not improve from 17.62538
196/196 - 74s - loss: 17.5238 - MinusLogProbMetric: 17.5238 - val_loss: 17.8161 - val_MinusLogProbMetric: 17.8161 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 263/1000
2023-10-10 20:46:09.150 
Epoch 263/1000 
	 loss: 17.5164, MinusLogProbMetric: 17.5164, val_loss: 18.0003, val_MinusLogProbMetric: 18.0003

Epoch 263: val_loss did not improve from 17.62538
196/196 - 75s - loss: 17.5164 - MinusLogProbMetric: 17.5164 - val_loss: 18.0003 - val_MinusLogProbMetric: 18.0003 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 264/1000
2023-10-10 20:47:18.720 
Epoch 264/1000 
	 loss: 17.6115, MinusLogProbMetric: 17.6115, val_loss: 18.3164, val_MinusLogProbMetric: 18.3164

Epoch 264: val_loss did not improve from 17.62538
196/196 - 70s - loss: 17.6115 - MinusLogProbMetric: 17.6115 - val_loss: 18.3164 - val_MinusLogProbMetric: 18.3164 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 265/1000
2023-10-10 20:48:31.197 
Epoch 265/1000 
	 loss: 17.6098, MinusLogProbMetric: 17.6098, val_loss: 17.7844, val_MinusLogProbMetric: 17.7844

Epoch 265: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.6098 - MinusLogProbMetric: 17.6098 - val_loss: 17.7844 - val_MinusLogProbMetric: 17.7844 - lr: 3.3333e-04 - 72s/epoch - 370ms/step
Epoch 266/1000
2023-10-10 20:49:43.504 
Epoch 266/1000 
	 loss: 17.4528, MinusLogProbMetric: 17.4528, val_loss: 18.1507, val_MinusLogProbMetric: 18.1507

Epoch 266: val_loss did not improve from 17.62538
196/196 - 72s - loss: 17.4528 - MinusLogProbMetric: 17.4528 - val_loss: 18.1507 - val_MinusLogProbMetric: 18.1507 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 267/1000
2023-10-10 20:50:57.792 
Epoch 267/1000 
	 loss: 17.4673, MinusLogProbMetric: 17.4673, val_loss: 17.7258, val_MinusLogProbMetric: 17.7258

Epoch 267: val_loss did not improve from 17.62538
196/196 - 74s - loss: 17.4673 - MinusLogProbMetric: 17.4673 - val_loss: 17.7258 - val_MinusLogProbMetric: 17.7258 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 268/1000
2023-10-10 20:52:09.185 
Epoch 268/1000 
	 loss: 17.4967, MinusLogProbMetric: 17.4967, val_loss: 17.9947, val_MinusLogProbMetric: 17.9947

Epoch 268: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.4967 - MinusLogProbMetric: 17.4967 - val_loss: 17.9947 - val_MinusLogProbMetric: 17.9947 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 269/1000
2023-10-10 20:53:20.201 
Epoch 269/1000 
	 loss: 17.4666, MinusLogProbMetric: 17.4666, val_loss: 17.6397, val_MinusLogProbMetric: 17.6397

Epoch 269: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.4666 - MinusLogProbMetric: 17.4666 - val_loss: 17.6397 - val_MinusLogProbMetric: 17.6397 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 270/1000
2023-10-10 20:54:30.019 
Epoch 270/1000 
	 loss: 17.4626, MinusLogProbMetric: 17.4626, val_loss: 17.8318, val_MinusLogProbMetric: 17.8318

Epoch 270: val_loss did not improve from 17.62538
196/196 - 70s - loss: 17.4626 - MinusLogProbMetric: 17.4626 - val_loss: 17.8318 - val_MinusLogProbMetric: 17.8318 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 271/1000
2023-10-10 20:55:40.690 
Epoch 271/1000 
	 loss: 17.4903, MinusLogProbMetric: 17.4903, val_loss: 17.9002, val_MinusLogProbMetric: 17.9002

Epoch 271: val_loss did not improve from 17.62538
196/196 - 71s - loss: 17.4903 - MinusLogProbMetric: 17.4903 - val_loss: 17.9002 - val_MinusLogProbMetric: 17.9002 - lr: 3.3333e-04 - 71s/epoch - 360ms/step
Epoch 272/1000
2023-10-10 20:56:53.592 
Epoch 272/1000 
	 loss: 17.5324, MinusLogProbMetric: 17.5324, val_loss: 18.0966, val_MinusLogProbMetric: 18.0966

Epoch 272: val_loss did not improve from 17.62538
196/196 - 73s - loss: 17.5324 - MinusLogProbMetric: 17.5324 - val_loss: 18.0966 - val_MinusLogProbMetric: 18.0966 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 273/1000
2023-10-10 20:58:06.497 
Epoch 273/1000 
	 loss: 16.9658, MinusLogProbMetric: 16.9658, val_loss: 17.3097, val_MinusLogProbMetric: 17.3097

Epoch 273: val_loss improved from 17.62538 to 17.30973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 74s - loss: 16.9658 - MinusLogProbMetric: 16.9658 - val_loss: 17.3097 - val_MinusLogProbMetric: 17.3097 - lr: 1.6667e-04 - 74s/epoch - 375ms/step
Epoch 274/1000
2023-10-10 20:59:19.942 
Epoch 274/1000 
	 loss: 16.9493, MinusLogProbMetric: 16.9493, val_loss: 17.6271, val_MinusLogProbMetric: 17.6271

Epoch 274: val_loss did not improve from 17.30973
196/196 - 73s - loss: 16.9493 - MinusLogProbMetric: 16.9493 - val_loss: 17.6271 - val_MinusLogProbMetric: 17.6271 - lr: 1.6667e-04 - 73s/epoch - 371ms/step
Epoch 275/1000
2023-10-10 21:00:32.438 
Epoch 275/1000 
	 loss: 16.9447, MinusLogProbMetric: 16.9447, val_loss: 17.2544, val_MinusLogProbMetric: 17.2544

Epoch 275: val_loss improved from 17.30973 to 17.25445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 73s - loss: 16.9447 - MinusLogProbMetric: 16.9447 - val_loss: 17.2544 - val_MinusLogProbMetric: 17.2544 - lr: 1.6667e-04 - 73s/epoch - 373ms/step
Epoch 276/1000
2023-10-10 21:01:45.989 
Epoch 276/1000 
	 loss: 16.9729, MinusLogProbMetric: 16.9729, val_loss: 17.2279, val_MinusLogProbMetric: 17.2279

Epoch 276: val_loss improved from 17.25445 to 17.22792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 73s - loss: 16.9729 - MinusLogProbMetric: 16.9729 - val_loss: 17.2279 - val_MinusLogProbMetric: 17.2279 - lr: 1.6667e-04 - 73s/epoch - 375ms/step
Epoch 277/1000
2023-10-10 21:02:58.558 
Epoch 277/1000 
	 loss: 16.9452, MinusLogProbMetric: 16.9452, val_loss: 17.3779, val_MinusLogProbMetric: 17.3779

Epoch 277: val_loss did not improve from 17.22792
196/196 - 72s - loss: 16.9452 - MinusLogProbMetric: 16.9452 - val_loss: 17.3779 - val_MinusLogProbMetric: 17.3779 - lr: 1.6667e-04 - 72s/epoch - 367ms/step
Epoch 278/1000
2023-10-10 21:04:11.473 
Epoch 278/1000 
	 loss: 16.9930, MinusLogProbMetric: 16.9930, val_loss: 17.4097, val_MinusLogProbMetric: 17.4097

Epoch 278: val_loss did not improve from 17.22792
196/196 - 73s - loss: 16.9930 - MinusLogProbMetric: 16.9930 - val_loss: 17.4097 - val_MinusLogProbMetric: 17.4097 - lr: 1.6667e-04 - 73s/epoch - 372ms/step
Epoch 279/1000
2023-10-10 21:05:24.207 
Epoch 279/1000 
	 loss: 16.9593, MinusLogProbMetric: 16.9593, val_loss: 18.1738, val_MinusLogProbMetric: 18.1738

Epoch 279: val_loss did not improve from 17.22792
196/196 - 73s - loss: 16.9593 - MinusLogProbMetric: 16.9593 - val_loss: 18.1738 - val_MinusLogProbMetric: 18.1738 - lr: 1.6667e-04 - 73s/epoch - 371ms/step
Epoch 280/1000
2023-10-10 21:06:36.696 
Epoch 280/1000 
	 loss: 17.0365, MinusLogProbMetric: 17.0365, val_loss: 17.2298, val_MinusLogProbMetric: 17.2298

Epoch 280: val_loss did not improve from 17.22792
196/196 - 72s - loss: 17.0365 - MinusLogProbMetric: 17.0365 - val_loss: 17.2298 - val_MinusLogProbMetric: 17.2298 - lr: 1.6667e-04 - 72s/epoch - 370ms/step
Epoch 281/1000
2023-10-10 21:07:48.431 
Epoch 281/1000 
	 loss: 16.9706, MinusLogProbMetric: 16.9706, val_loss: 17.2812, val_MinusLogProbMetric: 17.2812

Epoch 281: val_loss did not improve from 17.22792
196/196 - 72s - loss: 16.9706 - MinusLogProbMetric: 16.9706 - val_loss: 17.2812 - val_MinusLogProbMetric: 17.2812 - lr: 1.6667e-04 - 72s/epoch - 366ms/step
Epoch 282/1000
2023-10-10 21:09:02.245 
Epoch 282/1000 
	 loss: 16.9102, MinusLogProbMetric: 16.9102, val_loss: 17.2882, val_MinusLogProbMetric: 17.2882

Epoch 282: val_loss did not improve from 17.22792
196/196 - 74s - loss: 16.9102 - MinusLogProbMetric: 16.9102 - val_loss: 17.2882 - val_MinusLogProbMetric: 17.2882 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 283/1000
2023-10-10 21:10:14.799 
Epoch 283/1000 
	 loss: 16.9707, MinusLogProbMetric: 16.9707, val_loss: 17.3406, val_MinusLogProbMetric: 17.3406

Epoch 283: val_loss did not improve from 17.22792
196/196 - 73s - loss: 16.9707 - MinusLogProbMetric: 16.9707 - val_loss: 17.3406 - val_MinusLogProbMetric: 17.3406 - lr: 1.6667e-04 - 73s/epoch - 370ms/step
Epoch 284/1000
2023-10-10 21:11:27.057 
Epoch 284/1000 
	 loss: 16.9937, MinusLogProbMetric: 16.9937, val_loss: 17.4896, val_MinusLogProbMetric: 17.4896

Epoch 284: val_loss did not improve from 17.22792
196/196 - 72s - loss: 16.9937 - MinusLogProbMetric: 16.9937 - val_loss: 17.4896 - val_MinusLogProbMetric: 17.4896 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 285/1000
2023-10-10 21:12:41.121 
Epoch 285/1000 
	 loss: 16.9563, MinusLogProbMetric: 16.9563, val_loss: 17.4115, val_MinusLogProbMetric: 17.4115

Epoch 285: val_loss did not improve from 17.22792
196/196 - 74s - loss: 16.9563 - MinusLogProbMetric: 16.9563 - val_loss: 17.4115 - val_MinusLogProbMetric: 17.4115 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 286/1000
2023-10-10 21:13:53.910 
Epoch 286/1000 
	 loss: 16.9602, MinusLogProbMetric: 16.9602, val_loss: 17.2082, val_MinusLogProbMetric: 17.2082

Epoch 286: val_loss improved from 17.22792 to 17.20823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 73s - loss: 16.9602 - MinusLogProbMetric: 16.9602 - val_loss: 17.2082 - val_MinusLogProbMetric: 17.2082 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 287/1000
2023-10-10 21:15:06.938 
Epoch 287/1000 
	 loss: 17.0014, MinusLogProbMetric: 17.0014, val_loss: 17.4564, val_MinusLogProbMetric: 17.4564

Epoch 287: val_loss did not improve from 17.20823
196/196 - 72s - loss: 17.0014 - MinusLogProbMetric: 17.0014 - val_loss: 17.4564 - val_MinusLogProbMetric: 17.4564 - lr: 1.6667e-04 - 72s/epoch - 370ms/step
Epoch 288/1000
2023-10-10 21:16:19.387 
Epoch 288/1000 
	 loss: 16.9477, MinusLogProbMetric: 16.9477, val_loss: 17.3223, val_MinusLogProbMetric: 17.3223

Epoch 288: val_loss did not improve from 17.20823
196/196 - 72s - loss: 16.9477 - MinusLogProbMetric: 16.9477 - val_loss: 17.3223 - val_MinusLogProbMetric: 17.3223 - lr: 1.6667e-04 - 72s/epoch - 370ms/step
Epoch 289/1000
2023-10-10 21:17:32.784 
Epoch 289/1000 
	 loss: 16.9179, MinusLogProbMetric: 16.9179, val_loss: 17.2694, val_MinusLogProbMetric: 17.2694

Epoch 289: val_loss did not improve from 17.20823
196/196 - 73s - loss: 16.9179 - MinusLogProbMetric: 16.9179 - val_loss: 17.2694 - val_MinusLogProbMetric: 17.2694 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 290/1000
2023-10-10 21:18:44.932 
Epoch 290/1000 
	 loss: 16.9841, MinusLogProbMetric: 16.9841, val_loss: 17.2645, val_MinusLogProbMetric: 17.2645

Epoch 290: val_loss did not improve from 17.20823
196/196 - 72s - loss: 16.9841 - MinusLogProbMetric: 16.9841 - val_loss: 17.2645 - val_MinusLogProbMetric: 17.2645 - lr: 1.6667e-04 - 72s/epoch - 368ms/step
Epoch 291/1000
2023-10-10 21:19:57.162 
Epoch 291/1000 
	 loss: 17.0058, MinusLogProbMetric: 17.0058, val_loss: 17.6042, val_MinusLogProbMetric: 17.6042

Epoch 291: val_loss did not improve from 17.20823
196/196 - 72s - loss: 17.0058 - MinusLogProbMetric: 17.0058 - val_loss: 17.6042 - val_MinusLogProbMetric: 17.6042 - lr: 1.6667e-04 - 72s/epoch - 368ms/step
Epoch 292/1000
2023-10-10 21:21:09.383 
Epoch 292/1000 
	 loss: 16.9742, MinusLogProbMetric: 16.9742, val_loss: 17.4445, val_MinusLogProbMetric: 17.4445

Epoch 292: val_loss did not improve from 17.20823
196/196 - 72s - loss: 16.9742 - MinusLogProbMetric: 16.9742 - val_loss: 17.4445 - val_MinusLogProbMetric: 17.4445 - lr: 1.6667e-04 - 72s/epoch - 368ms/step
Epoch 293/1000
2023-10-10 21:22:21.611 
Epoch 293/1000 
	 loss: 16.9375, MinusLogProbMetric: 16.9375, val_loss: 17.9291, val_MinusLogProbMetric: 17.9291

Epoch 293: val_loss did not improve from 17.20823
196/196 - 72s - loss: 16.9375 - MinusLogProbMetric: 16.9375 - val_loss: 17.9291 - val_MinusLogProbMetric: 17.9291 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 294/1000
2023-10-10 21:23:34.326 
Epoch 294/1000 
	 loss: 16.9042, MinusLogProbMetric: 16.9042, val_loss: 17.1603, val_MinusLogProbMetric: 17.1603

Epoch 294: val_loss improved from 17.20823 to 17.16025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 74s - loss: 16.9042 - MinusLogProbMetric: 16.9042 - val_loss: 17.1603 - val_MinusLogProbMetric: 17.1603 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 295/1000
2023-10-10 21:24:48.227 
Epoch 295/1000 
	 loss: 16.9809, MinusLogProbMetric: 16.9809, val_loss: 17.2487, val_MinusLogProbMetric: 17.2487

Epoch 295: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9809 - MinusLogProbMetric: 16.9809 - val_loss: 17.2487 - val_MinusLogProbMetric: 17.2487 - lr: 1.6667e-04 - 73s/epoch - 372ms/step
Epoch 296/1000
2023-10-10 21:26:00.717 
Epoch 296/1000 
	 loss: 16.9702, MinusLogProbMetric: 16.9702, val_loss: 17.3998, val_MinusLogProbMetric: 17.3998

Epoch 296: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9702 - MinusLogProbMetric: 16.9702 - val_loss: 17.3998 - val_MinusLogProbMetric: 17.3998 - lr: 1.6667e-04 - 72s/epoch - 370ms/step
Epoch 297/1000
2023-10-10 21:27:12.945 
Epoch 297/1000 
	 loss: 16.9704, MinusLogProbMetric: 16.9704, val_loss: 17.2749, val_MinusLogProbMetric: 17.2749

Epoch 297: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9704 - MinusLogProbMetric: 16.9704 - val_loss: 17.2749 - val_MinusLogProbMetric: 17.2749 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 298/1000
2023-10-10 21:28:24.190 
Epoch 298/1000 
	 loss: 16.9993, MinusLogProbMetric: 16.9993, val_loss: 17.3451, val_MinusLogProbMetric: 17.3451

Epoch 298: val_loss did not improve from 17.16025
196/196 - 71s - loss: 16.9993 - MinusLogProbMetric: 16.9993 - val_loss: 17.3451 - val_MinusLogProbMetric: 17.3451 - lr: 1.6667e-04 - 71s/epoch - 364ms/step
Epoch 299/1000
2023-10-10 21:29:36.468 
Epoch 299/1000 
	 loss: 16.8979, MinusLogProbMetric: 16.8979, val_loss: 17.3101, val_MinusLogProbMetric: 17.3101

Epoch 299: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.8979 - MinusLogProbMetric: 16.8979 - val_loss: 17.3101 - val_MinusLogProbMetric: 17.3101 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 300/1000
2023-10-10 21:30:48.137 
Epoch 300/1000 
	 loss: 16.9335, MinusLogProbMetric: 16.9335, val_loss: 17.2668, val_MinusLogProbMetric: 17.2668

Epoch 300: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9335 - MinusLogProbMetric: 16.9335 - val_loss: 17.2668 - val_MinusLogProbMetric: 17.2668 - lr: 1.6667e-04 - 72s/epoch - 366ms/step
Epoch 301/1000
2023-10-10 21:31:57.509 
Epoch 301/1000 
	 loss: 16.9629, MinusLogProbMetric: 16.9629, val_loss: 17.2772, val_MinusLogProbMetric: 17.2772

Epoch 301: val_loss did not improve from 17.16025
196/196 - 69s - loss: 16.9629 - MinusLogProbMetric: 16.9629 - val_loss: 17.2772 - val_MinusLogProbMetric: 17.2772 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 302/1000
2023-10-10 21:33:09.130 
Epoch 302/1000 
	 loss: 16.9317, MinusLogProbMetric: 16.9317, val_loss: 17.3255, val_MinusLogProbMetric: 17.3255

Epoch 302: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9317 - MinusLogProbMetric: 16.9317 - val_loss: 17.3255 - val_MinusLogProbMetric: 17.3255 - lr: 1.6667e-04 - 72s/epoch - 365ms/step
Epoch 303/1000
2023-10-10 21:34:22.231 
Epoch 303/1000 
	 loss: 16.9675, MinusLogProbMetric: 16.9675, val_loss: 17.4365, val_MinusLogProbMetric: 17.4365

Epoch 303: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9675 - MinusLogProbMetric: 16.9675 - val_loss: 17.4365 - val_MinusLogProbMetric: 17.4365 - lr: 1.6667e-04 - 73s/epoch - 373ms/step
Epoch 304/1000
2023-10-10 21:35:36.702 
Epoch 304/1000 
	 loss: 16.9416, MinusLogProbMetric: 16.9416, val_loss: 17.2495, val_MinusLogProbMetric: 17.2495

Epoch 304: val_loss did not improve from 17.16025
196/196 - 74s - loss: 16.9416 - MinusLogProbMetric: 16.9416 - val_loss: 17.2495 - val_MinusLogProbMetric: 17.2495 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 305/1000
2023-10-10 21:36:54.313 
Epoch 305/1000 
	 loss: 16.9220, MinusLogProbMetric: 16.9220, val_loss: 17.5710, val_MinusLogProbMetric: 17.5710

Epoch 305: val_loss did not improve from 17.16025
196/196 - 78s - loss: 16.9220 - MinusLogProbMetric: 16.9220 - val_loss: 17.5710 - val_MinusLogProbMetric: 17.5710 - lr: 1.6667e-04 - 78s/epoch - 396ms/step
Epoch 306/1000
2023-10-10 21:38:12.034 
Epoch 306/1000 
	 loss: 16.9749, MinusLogProbMetric: 16.9749, val_loss: 18.0496, val_MinusLogProbMetric: 18.0496

Epoch 306: val_loss did not improve from 17.16025
196/196 - 78s - loss: 16.9749 - MinusLogProbMetric: 16.9749 - val_loss: 18.0496 - val_MinusLogProbMetric: 18.0496 - lr: 1.6667e-04 - 78s/epoch - 396ms/step
Epoch 307/1000
2023-10-10 21:39:26.301 
Epoch 307/1000 
	 loss: 16.9882, MinusLogProbMetric: 16.9882, val_loss: 17.4647, val_MinusLogProbMetric: 17.4647

Epoch 307: val_loss did not improve from 17.16025
196/196 - 74s - loss: 16.9882 - MinusLogProbMetric: 16.9882 - val_loss: 17.4647 - val_MinusLogProbMetric: 17.4647 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 308/1000
2023-10-10 21:40:40.767 
Epoch 308/1000 
	 loss: 16.9332, MinusLogProbMetric: 16.9332, val_loss: 17.2789, val_MinusLogProbMetric: 17.2789

Epoch 308: val_loss did not improve from 17.16025
196/196 - 74s - loss: 16.9332 - MinusLogProbMetric: 16.9332 - val_loss: 17.2789 - val_MinusLogProbMetric: 17.2789 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 309/1000
2023-10-10 21:41:58.263 
Epoch 309/1000 
	 loss: 16.9240, MinusLogProbMetric: 16.9240, val_loss: 17.4351, val_MinusLogProbMetric: 17.4351

Epoch 309: val_loss did not improve from 17.16025
196/196 - 77s - loss: 16.9240 - MinusLogProbMetric: 16.9240 - val_loss: 17.4351 - val_MinusLogProbMetric: 17.4351 - lr: 1.6667e-04 - 77s/epoch - 395ms/step
Epoch 310/1000
2023-10-10 21:43:10.635 
Epoch 310/1000 
	 loss: 16.9484, MinusLogProbMetric: 16.9484, val_loss: 17.3560, val_MinusLogProbMetric: 17.3560

Epoch 310: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9484 - MinusLogProbMetric: 16.9484 - val_loss: 17.3560 - val_MinusLogProbMetric: 17.3560 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 311/1000
2023-10-10 21:44:25.719 
Epoch 311/1000 
	 loss: 16.9341, MinusLogProbMetric: 16.9341, val_loss: 17.3909, val_MinusLogProbMetric: 17.3909

Epoch 311: val_loss did not improve from 17.16025
196/196 - 75s - loss: 16.9341 - MinusLogProbMetric: 16.9341 - val_loss: 17.3909 - val_MinusLogProbMetric: 17.3909 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 312/1000
2023-10-10 21:45:40.592 
Epoch 312/1000 
	 loss: 16.9677, MinusLogProbMetric: 16.9677, val_loss: 17.3274, val_MinusLogProbMetric: 17.3274

Epoch 312: val_loss did not improve from 17.16025
196/196 - 75s - loss: 16.9677 - MinusLogProbMetric: 16.9677 - val_loss: 17.3274 - val_MinusLogProbMetric: 17.3274 - lr: 1.6667e-04 - 75s/epoch - 382ms/step
Epoch 313/1000
2023-10-10 21:46:54.334 
Epoch 313/1000 
	 loss: 16.8989, MinusLogProbMetric: 16.8989, val_loss: 17.3188, val_MinusLogProbMetric: 17.3188

Epoch 313: val_loss did not improve from 17.16025
196/196 - 74s - loss: 16.8989 - MinusLogProbMetric: 16.8989 - val_loss: 17.3188 - val_MinusLogProbMetric: 17.3188 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 314/1000
2023-10-10 21:48:11.075 
Epoch 314/1000 
	 loss: 17.0191, MinusLogProbMetric: 17.0191, val_loss: 17.6478, val_MinusLogProbMetric: 17.6478

Epoch 314: val_loss did not improve from 17.16025
196/196 - 77s - loss: 17.0191 - MinusLogProbMetric: 17.0191 - val_loss: 17.6478 - val_MinusLogProbMetric: 17.6478 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 315/1000
2023-10-10 21:49:23.784 
Epoch 315/1000 
	 loss: 16.9450, MinusLogProbMetric: 16.9450, val_loss: 17.2579, val_MinusLogProbMetric: 17.2579

Epoch 315: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9450 - MinusLogProbMetric: 16.9450 - val_loss: 17.2579 - val_MinusLogProbMetric: 17.2579 - lr: 1.6667e-04 - 73s/epoch - 371ms/step
Epoch 316/1000
2023-10-10 21:50:39.912 
Epoch 316/1000 
	 loss: 16.9562, MinusLogProbMetric: 16.9562, val_loss: 17.4850, val_MinusLogProbMetric: 17.4850

Epoch 316: val_loss did not improve from 17.16025
196/196 - 76s - loss: 16.9562 - MinusLogProbMetric: 16.9562 - val_loss: 17.4850 - val_MinusLogProbMetric: 17.4850 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 317/1000
2023-10-10 21:51:54.423 
Epoch 317/1000 
	 loss: 16.9483, MinusLogProbMetric: 16.9483, val_loss: 17.2996, val_MinusLogProbMetric: 17.2996

Epoch 317: val_loss did not improve from 17.16025
196/196 - 75s - loss: 16.9483 - MinusLogProbMetric: 16.9483 - val_loss: 17.2996 - val_MinusLogProbMetric: 17.2996 - lr: 1.6667e-04 - 75s/epoch - 380ms/step
Epoch 318/1000
2023-10-10 21:53:07.733 
Epoch 318/1000 
	 loss: 16.9699, MinusLogProbMetric: 16.9699, val_loss: 17.2303, val_MinusLogProbMetric: 17.2303

Epoch 318: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9699 - MinusLogProbMetric: 16.9699 - val_loss: 17.2303 - val_MinusLogProbMetric: 17.2303 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 319/1000
2023-10-10 21:54:21.112 
Epoch 319/1000 
	 loss: 16.9420, MinusLogProbMetric: 16.9420, val_loss: 17.3105, val_MinusLogProbMetric: 17.3105

Epoch 319: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9420 - MinusLogProbMetric: 16.9420 - val_loss: 17.3105 - val_MinusLogProbMetric: 17.3105 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 320/1000
2023-10-10 21:55:33.567 
Epoch 320/1000 
	 loss: 16.8944, MinusLogProbMetric: 16.8944, val_loss: 17.3801, val_MinusLogProbMetric: 17.3801

Epoch 320: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.8944 - MinusLogProbMetric: 16.8944 - val_loss: 17.3801 - val_MinusLogProbMetric: 17.3801 - lr: 1.6667e-04 - 72s/epoch - 370ms/step
Epoch 321/1000
2023-10-10 21:56:46.903 
Epoch 321/1000 
	 loss: 16.9367, MinusLogProbMetric: 16.9367, val_loss: 17.3562, val_MinusLogProbMetric: 17.3562

Epoch 321: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9367 - MinusLogProbMetric: 16.9367 - val_loss: 17.3562 - val_MinusLogProbMetric: 17.3562 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 322/1000
2023-10-10 21:57:59.825 
Epoch 322/1000 
	 loss: 16.9776, MinusLogProbMetric: 16.9776, val_loss: 17.2454, val_MinusLogProbMetric: 17.2454

Epoch 322: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9776 - MinusLogProbMetric: 16.9776 - val_loss: 17.2454 - val_MinusLogProbMetric: 17.2454 - lr: 1.6667e-04 - 73s/epoch - 372ms/step
Epoch 323/1000
2023-10-10 21:59:12.109 
Epoch 323/1000 
	 loss: 16.8704, MinusLogProbMetric: 16.8704, val_loss: 17.4487, val_MinusLogProbMetric: 17.4487

Epoch 323: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.8704 - MinusLogProbMetric: 16.8704 - val_loss: 17.4487 - val_MinusLogProbMetric: 17.4487 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 324/1000
2023-10-10 22:00:25.150 
Epoch 324/1000 
	 loss: 16.9561, MinusLogProbMetric: 16.9561, val_loss: 17.2613, val_MinusLogProbMetric: 17.2613

Epoch 324: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9561 - MinusLogProbMetric: 16.9561 - val_loss: 17.2613 - val_MinusLogProbMetric: 17.2613 - lr: 1.6667e-04 - 73s/epoch - 373ms/step
Epoch 325/1000
2023-10-10 22:01:38.493 
Epoch 325/1000 
	 loss: 16.9689, MinusLogProbMetric: 16.9689, val_loss: 17.4277, val_MinusLogProbMetric: 17.4277

Epoch 325: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9689 - MinusLogProbMetric: 16.9689 - val_loss: 17.4277 - val_MinusLogProbMetric: 17.4277 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 326/1000
2023-10-10 22:02:50.974 
Epoch 326/1000 
	 loss: 16.9076, MinusLogProbMetric: 16.9076, val_loss: 17.4553, val_MinusLogProbMetric: 17.4553

Epoch 326: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9076 - MinusLogProbMetric: 16.9076 - val_loss: 17.4553 - val_MinusLogProbMetric: 17.4553 - lr: 1.6667e-04 - 72s/epoch - 370ms/step
Epoch 327/1000
2023-10-10 22:04:03.208 
Epoch 327/1000 
	 loss: 16.9555, MinusLogProbMetric: 16.9555, val_loss: 17.2649, val_MinusLogProbMetric: 17.2649

Epoch 327: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9555 - MinusLogProbMetric: 16.9555 - val_loss: 17.2649 - val_MinusLogProbMetric: 17.2649 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 328/1000
2023-10-10 22:05:16.437 
Epoch 328/1000 
	 loss: 16.9116, MinusLogProbMetric: 16.9116, val_loss: 17.5637, val_MinusLogProbMetric: 17.5637

Epoch 328: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9116 - MinusLogProbMetric: 16.9116 - val_loss: 17.5637 - val_MinusLogProbMetric: 17.5637 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 329/1000
2023-10-10 22:06:33.753 
Epoch 329/1000 
	 loss: 16.9269, MinusLogProbMetric: 16.9269, val_loss: 17.5470, val_MinusLogProbMetric: 17.5470

Epoch 329: val_loss did not improve from 17.16025
196/196 - 77s - loss: 16.9269 - MinusLogProbMetric: 16.9269 - val_loss: 17.5470 - val_MinusLogProbMetric: 17.5470 - lr: 1.6667e-04 - 77s/epoch - 394ms/step
Epoch 330/1000
2023-10-10 22:07:49.112 
Epoch 330/1000 
	 loss: 16.9097, MinusLogProbMetric: 16.9097, val_loss: 17.2352, val_MinusLogProbMetric: 17.2352

Epoch 330: val_loss did not improve from 17.16025
196/196 - 75s - loss: 16.9097 - MinusLogProbMetric: 16.9097 - val_loss: 17.2352 - val_MinusLogProbMetric: 17.2352 - lr: 1.6667e-04 - 75s/epoch - 384ms/step
Epoch 331/1000
2023-10-10 22:09:02.003 
Epoch 331/1000 
	 loss: 16.9086, MinusLogProbMetric: 16.9086, val_loss: 17.3835, val_MinusLogProbMetric: 17.3835

Epoch 331: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9086 - MinusLogProbMetric: 16.9086 - val_loss: 17.3835 - val_MinusLogProbMetric: 17.3835 - lr: 1.6667e-04 - 73s/epoch - 372ms/step
Epoch 332/1000
2023-10-10 22:10:11.757 
Epoch 332/1000 
	 loss: 16.9621, MinusLogProbMetric: 16.9621, val_loss: 17.4660, val_MinusLogProbMetric: 17.4660

Epoch 332: val_loss did not improve from 17.16025
196/196 - 70s - loss: 16.9621 - MinusLogProbMetric: 16.9621 - val_loss: 17.4660 - val_MinusLogProbMetric: 17.4660 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 333/1000
2023-10-10 22:11:24.677 
Epoch 333/1000 
	 loss: 16.9554, MinusLogProbMetric: 16.9554, val_loss: 17.4380, val_MinusLogProbMetric: 17.4380

Epoch 333: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9554 - MinusLogProbMetric: 16.9554 - val_loss: 17.4380 - val_MinusLogProbMetric: 17.4380 - lr: 1.6667e-04 - 73s/epoch - 372ms/step
Epoch 334/1000
2023-10-10 22:12:37.935 
Epoch 334/1000 
	 loss: 16.9241, MinusLogProbMetric: 16.9241, val_loss: 17.3116, val_MinusLogProbMetric: 17.3116

Epoch 334: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9241 - MinusLogProbMetric: 16.9241 - val_loss: 17.3116 - val_MinusLogProbMetric: 17.3116 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 335/1000
2023-10-10 22:13:50.153 
Epoch 335/1000 
	 loss: 16.9875, MinusLogProbMetric: 16.9875, val_loss: 17.2890, val_MinusLogProbMetric: 17.2890

Epoch 335: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9875 - MinusLogProbMetric: 16.9875 - val_loss: 17.2890 - val_MinusLogProbMetric: 17.2890 - lr: 1.6667e-04 - 72s/epoch - 368ms/step
Epoch 336/1000
2023-10-10 22:15:02.140 
Epoch 336/1000 
	 loss: 16.9366, MinusLogProbMetric: 16.9366, val_loss: 17.3963, val_MinusLogProbMetric: 17.3963

Epoch 336: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9366 - MinusLogProbMetric: 16.9366 - val_loss: 17.3963 - val_MinusLogProbMetric: 17.3963 - lr: 1.6667e-04 - 72s/epoch - 367ms/step
Epoch 337/1000
2023-10-10 22:16:12.120 
Epoch 337/1000 
	 loss: 16.9304, MinusLogProbMetric: 16.9304, val_loss: 17.3555, val_MinusLogProbMetric: 17.3555

Epoch 337: val_loss did not improve from 17.16025
196/196 - 70s - loss: 16.9304 - MinusLogProbMetric: 16.9304 - val_loss: 17.3555 - val_MinusLogProbMetric: 17.3555 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 338/1000
2023-10-10 22:17:23.581 
Epoch 338/1000 
	 loss: 16.9146, MinusLogProbMetric: 16.9146, val_loss: 17.4211, val_MinusLogProbMetric: 17.4211

Epoch 338: val_loss did not improve from 17.16025
196/196 - 71s - loss: 16.9146 - MinusLogProbMetric: 16.9146 - val_loss: 17.4211 - val_MinusLogProbMetric: 17.4211 - lr: 1.6667e-04 - 71s/epoch - 365ms/step
Epoch 339/1000
2023-10-10 22:18:36.682 
Epoch 339/1000 
	 loss: 16.9344, MinusLogProbMetric: 16.9344, val_loss: 17.3559, val_MinusLogProbMetric: 17.3559

Epoch 339: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9344 - MinusLogProbMetric: 16.9344 - val_loss: 17.3559 - val_MinusLogProbMetric: 17.3559 - lr: 1.6667e-04 - 73s/epoch - 373ms/step
Epoch 340/1000
2023-10-10 22:19:48.917 
Epoch 340/1000 
	 loss: 16.9061, MinusLogProbMetric: 16.9061, val_loss: 17.2680, val_MinusLogProbMetric: 17.2680

Epoch 340: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9061 - MinusLogProbMetric: 16.9061 - val_loss: 17.2680 - val_MinusLogProbMetric: 17.2680 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 341/1000
2023-10-10 22:21:03.175 
Epoch 341/1000 
	 loss: 16.9362, MinusLogProbMetric: 16.9362, val_loss: 17.6956, val_MinusLogProbMetric: 17.6956

Epoch 341: val_loss did not improve from 17.16025
196/196 - 74s - loss: 16.9362 - MinusLogProbMetric: 16.9362 - val_loss: 17.6956 - val_MinusLogProbMetric: 17.6956 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 342/1000
2023-10-10 22:22:16.356 
Epoch 342/1000 
	 loss: 16.8813, MinusLogProbMetric: 16.8813, val_loss: 17.4864, val_MinusLogProbMetric: 17.4864

Epoch 342: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.8813 - MinusLogProbMetric: 16.8813 - val_loss: 17.4864 - val_MinusLogProbMetric: 17.4864 - lr: 1.6667e-04 - 73s/epoch - 373ms/step
Epoch 343/1000
2023-10-10 22:23:28.042 
Epoch 343/1000 
	 loss: 16.9335, MinusLogProbMetric: 16.9335, val_loss: 17.2552, val_MinusLogProbMetric: 17.2552

Epoch 343: val_loss did not improve from 17.16025
196/196 - 72s - loss: 16.9335 - MinusLogProbMetric: 16.9335 - val_loss: 17.2552 - val_MinusLogProbMetric: 17.2552 - lr: 1.6667e-04 - 72s/epoch - 366ms/step
Epoch 344/1000
2023-10-10 22:24:41.257 
Epoch 344/1000 
	 loss: 16.9347, MinusLogProbMetric: 16.9347, val_loss: 17.7185, val_MinusLogProbMetric: 17.7185

Epoch 344: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.9347 - MinusLogProbMetric: 16.9347 - val_loss: 17.7185 - val_MinusLogProbMetric: 17.7185 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 345/1000
2023-10-10 22:25:54.150 
Epoch 345/1000 
	 loss: 16.7029, MinusLogProbMetric: 16.7029, val_loss: 17.2035, val_MinusLogProbMetric: 17.2035

Epoch 345: val_loss did not improve from 17.16025
196/196 - 73s - loss: 16.7029 - MinusLogProbMetric: 16.7029 - val_loss: 17.2035 - val_MinusLogProbMetric: 17.2035 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 346/1000
2023-10-10 22:27:06.431 
Epoch 346/1000 
	 loss: 16.6837, MinusLogProbMetric: 16.6837, val_loss: 17.1551, val_MinusLogProbMetric: 17.1551

Epoch 346: val_loss improved from 17.16025 to 17.15508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 73s - loss: 16.6837 - MinusLogProbMetric: 16.6837 - val_loss: 17.1551 - val_MinusLogProbMetric: 17.1551 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 347/1000
2023-10-10 22:28:23.984 
Epoch 347/1000 
	 loss: 16.6671, MinusLogProbMetric: 16.6671, val_loss: 17.0743, val_MinusLogProbMetric: 17.0743

Epoch 347: val_loss improved from 17.15508 to 17.07427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 79s - loss: 16.6671 - MinusLogProbMetric: 16.6671 - val_loss: 17.0743 - val_MinusLogProbMetric: 17.0743 - lr: 8.3333e-05 - 79s/epoch - 402ms/step
Epoch 348/1000
2023-10-10 22:29:37.778 
Epoch 348/1000 
	 loss: 16.6852, MinusLogProbMetric: 16.6852, val_loss: 17.0891, val_MinusLogProbMetric: 17.0891

Epoch 348: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6852 - MinusLogProbMetric: 16.6852 - val_loss: 17.0891 - val_MinusLogProbMetric: 17.0891 - lr: 8.3333e-05 - 72s/epoch - 366ms/step
Epoch 349/1000
2023-10-10 22:30:50.080 
Epoch 349/1000 
	 loss: 16.6745, MinusLogProbMetric: 16.6745, val_loss: 17.1091, val_MinusLogProbMetric: 17.1091

Epoch 349: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6745 - MinusLogProbMetric: 16.6745 - val_loss: 17.1091 - val_MinusLogProbMetric: 17.1091 - lr: 8.3333e-05 - 72s/epoch - 369ms/step
Epoch 350/1000
2023-10-10 22:32:01.648 
Epoch 350/1000 
	 loss: 16.6650, MinusLogProbMetric: 16.6650, val_loss: 17.1164, val_MinusLogProbMetric: 17.1164

Epoch 350: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6650 - MinusLogProbMetric: 16.6650 - val_loss: 17.1164 - val_MinusLogProbMetric: 17.1164 - lr: 8.3333e-05 - 72s/epoch - 365ms/step
Epoch 351/1000
2023-10-10 22:33:13.676 
Epoch 351/1000 
	 loss: 16.6801, MinusLogProbMetric: 16.6801, val_loss: 17.1893, val_MinusLogProbMetric: 17.1893

Epoch 351: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6801 - MinusLogProbMetric: 16.6801 - val_loss: 17.1893 - val_MinusLogProbMetric: 17.1893 - lr: 8.3333e-05 - 72s/epoch - 367ms/step
Epoch 352/1000
2023-10-10 22:34:25.764 
Epoch 352/1000 
	 loss: 16.6868, MinusLogProbMetric: 16.6868, val_loss: 17.0932, val_MinusLogProbMetric: 17.0932

Epoch 352: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6868 - MinusLogProbMetric: 16.6868 - val_loss: 17.0932 - val_MinusLogProbMetric: 17.0932 - lr: 8.3333e-05 - 72s/epoch - 368ms/step
Epoch 353/1000
2023-10-10 22:35:38.293 
Epoch 353/1000 
	 loss: 16.6612, MinusLogProbMetric: 16.6612, val_loss: 17.0964, val_MinusLogProbMetric: 17.0964

Epoch 353: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6612 - MinusLogProbMetric: 16.6612 - val_loss: 17.0964 - val_MinusLogProbMetric: 17.0964 - lr: 8.3333e-05 - 72s/epoch - 370ms/step
Epoch 354/1000
2023-10-10 22:36:49.624 
Epoch 354/1000 
	 loss: 16.6660, MinusLogProbMetric: 16.6660, val_loss: 17.2017, val_MinusLogProbMetric: 17.2017

Epoch 354: val_loss did not improve from 17.07427
196/196 - 71s - loss: 16.6660 - MinusLogProbMetric: 16.6660 - val_loss: 17.2017 - val_MinusLogProbMetric: 17.2017 - lr: 8.3333e-05 - 71s/epoch - 364ms/step
Epoch 355/1000
2023-10-10 22:38:01.803 
Epoch 355/1000 
	 loss: 16.6646, MinusLogProbMetric: 16.6646, val_loss: 17.1104, val_MinusLogProbMetric: 17.1104

Epoch 355: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6646 - MinusLogProbMetric: 16.6646 - val_loss: 17.1104 - val_MinusLogProbMetric: 17.1104 - lr: 8.3333e-05 - 72s/epoch - 368ms/step
Epoch 356/1000
2023-10-10 22:39:17.720 
Epoch 356/1000 
	 loss: 16.6634, MinusLogProbMetric: 16.6634, val_loss: 17.1970, val_MinusLogProbMetric: 17.1970

Epoch 356: val_loss did not improve from 17.07427
196/196 - 76s - loss: 16.6634 - MinusLogProbMetric: 16.6634 - val_loss: 17.1970 - val_MinusLogProbMetric: 17.1970 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 357/1000
2023-10-10 22:40:41.789 
Epoch 357/1000 
	 loss: 16.6674, MinusLogProbMetric: 16.6674, val_loss: 17.2110, val_MinusLogProbMetric: 17.2110

Epoch 357: val_loss did not improve from 17.07427
196/196 - 84s - loss: 16.6674 - MinusLogProbMetric: 16.6674 - val_loss: 17.2110 - val_MinusLogProbMetric: 17.2110 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 358/1000
2023-10-10 22:42:00.972 
Epoch 358/1000 
	 loss: 16.6596, MinusLogProbMetric: 16.6596, val_loss: 17.1581, val_MinusLogProbMetric: 17.1581

Epoch 358: val_loss did not improve from 17.07427
196/196 - 79s - loss: 16.6596 - MinusLogProbMetric: 16.6596 - val_loss: 17.1581 - val_MinusLogProbMetric: 17.1581 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 359/1000
2023-10-10 22:43:16.137 
Epoch 359/1000 
	 loss: 16.6565, MinusLogProbMetric: 16.6565, val_loss: 17.1084, val_MinusLogProbMetric: 17.1084

Epoch 359: val_loss did not improve from 17.07427
196/196 - 75s - loss: 16.6565 - MinusLogProbMetric: 16.6565 - val_loss: 17.1084 - val_MinusLogProbMetric: 17.1084 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 360/1000
2023-10-10 22:44:33.698 
Epoch 360/1000 
	 loss: 16.6803, MinusLogProbMetric: 16.6803, val_loss: 17.1488, val_MinusLogProbMetric: 17.1488

Epoch 360: val_loss did not improve from 17.07427
196/196 - 78s - loss: 16.6803 - MinusLogProbMetric: 16.6803 - val_loss: 17.1488 - val_MinusLogProbMetric: 17.1488 - lr: 8.3333e-05 - 78s/epoch - 396ms/step
Epoch 361/1000
2023-10-10 22:45:46.525 
Epoch 361/1000 
	 loss: 16.6788, MinusLogProbMetric: 16.6788, val_loss: 17.1048, val_MinusLogProbMetric: 17.1048

Epoch 361: val_loss did not improve from 17.07427
196/196 - 73s - loss: 16.6788 - MinusLogProbMetric: 16.6788 - val_loss: 17.1048 - val_MinusLogProbMetric: 17.1048 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 362/1000
2023-10-10 22:47:04.176 
Epoch 362/1000 
	 loss: 16.7138, MinusLogProbMetric: 16.7138, val_loss: 17.1229, val_MinusLogProbMetric: 17.1229

Epoch 362: val_loss did not improve from 17.07427
196/196 - 78s - loss: 16.7138 - MinusLogProbMetric: 16.7138 - val_loss: 17.1229 - val_MinusLogProbMetric: 17.1229 - lr: 8.3333e-05 - 78s/epoch - 396ms/step
Epoch 363/1000
2023-10-10 22:48:17.912 
Epoch 363/1000 
	 loss: 16.6600, MinusLogProbMetric: 16.6600, val_loss: 17.3354, val_MinusLogProbMetric: 17.3354

Epoch 363: val_loss did not improve from 17.07427
196/196 - 74s - loss: 16.6600 - MinusLogProbMetric: 16.6600 - val_loss: 17.3354 - val_MinusLogProbMetric: 17.3354 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 364/1000
2023-10-10 22:49:35.030 
Epoch 364/1000 
	 loss: 16.6904, MinusLogProbMetric: 16.6904, val_loss: 17.0940, val_MinusLogProbMetric: 17.0940

Epoch 364: val_loss did not improve from 17.07427
196/196 - 77s - loss: 16.6904 - MinusLogProbMetric: 16.6904 - val_loss: 17.0940 - val_MinusLogProbMetric: 17.0940 - lr: 8.3333e-05 - 77s/epoch - 393ms/step
Epoch 365/1000
2023-10-10 22:50:49.772 
Epoch 365/1000 
	 loss: 16.6877, MinusLogProbMetric: 16.6877, val_loss: 17.0962, val_MinusLogProbMetric: 17.0962

Epoch 365: val_loss did not improve from 17.07427
196/196 - 75s - loss: 16.6877 - MinusLogProbMetric: 16.6877 - val_loss: 17.0962 - val_MinusLogProbMetric: 17.0962 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 366/1000
2023-10-10 22:52:02.236 
Epoch 366/1000 
	 loss: 16.6781, MinusLogProbMetric: 16.6781, val_loss: 17.1011, val_MinusLogProbMetric: 17.1011

Epoch 366: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6781 - MinusLogProbMetric: 16.6781 - val_loss: 17.1011 - val_MinusLogProbMetric: 17.1011 - lr: 8.3333e-05 - 72s/epoch - 370ms/step
Epoch 367/1000
2023-10-10 22:53:16.363 
Epoch 367/1000 
	 loss: 16.6774, MinusLogProbMetric: 16.6774, val_loss: 17.2521, val_MinusLogProbMetric: 17.2521

Epoch 367: val_loss did not improve from 17.07427
196/196 - 74s - loss: 16.6774 - MinusLogProbMetric: 16.6774 - val_loss: 17.2521 - val_MinusLogProbMetric: 17.2521 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 368/1000
2023-10-10 22:54:28.619 
Epoch 368/1000 
	 loss: 16.6766, MinusLogProbMetric: 16.6766, val_loss: 17.1804, val_MinusLogProbMetric: 17.1804

Epoch 368: val_loss did not improve from 17.07427
196/196 - 72s - loss: 16.6766 - MinusLogProbMetric: 16.6766 - val_loss: 17.1804 - val_MinusLogProbMetric: 17.1804 - lr: 8.3333e-05 - 72s/epoch - 369ms/step
Epoch 369/1000
2023-10-10 22:55:41.305 
Epoch 369/1000 
	 loss: 16.6751, MinusLogProbMetric: 16.6751, val_loss: 17.2892, val_MinusLogProbMetric: 17.2892

Epoch 369: val_loss did not improve from 17.07427
196/196 - 73s - loss: 16.6751 - MinusLogProbMetric: 16.6751 - val_loss: 17.2892 - val_MinusLogProbMetric: 17.2892 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 370/1000
2023-10-10 22:56:57.771 
Epoch 370/1000 
	 loss: 16.6904, MinusLogProbMetric: 16.6904, val_loss: 17.0689, val_MinusLogProbMetric: 17.0689

Epoch 370: val_loss improved from 17.07427 to 17.06891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 78s - loss: 16.6904 - MinusLogProbMetric: 16.6904 - val_loss: 17.0689 - val_MinusLogProbMetric: 17.0689 - lr: 8.3333e-05 - 78s/epoch - 398ms/step
Epoch 371/1000
2023-10-10 22:58:10.196 
Epoch 371/1000 
	 loss: 16.6706, MinusLogProbMetric: 16.6706, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 371: val_loss did not improve from 17.06891
196/196 - 71s - loss: 16.6706 - MinusLogProbMetric: 16.6706 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 8.3333e-05 - 71s/epoch - 361ms/step
Epoch 372/1000
2023-10-10 22:59:25.305 
Epoch 372/1000 
	 loss: 16.6723, MinusLogProbMetric: 16.6723, val_loss: 17.1351, val_MinusLogProbMetric: 17.1351

Epoch 372: val_loss did not improve from 17.06891
196/196 - 75s - loss: 16.6723 - MinusLogProbMetric: 16.6723 - val_loss: 17.1351 - val_MinusLogProbMetric: 17.1351 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 373/1000
2023-10-10 23:00:39.511 
Epoch 373/1000 
	 loss: 16.6614, MinusLogProbMetric: 16.6614, val_loss: 17.1176, val_MinusLogProbMetric: 17.1176

Epoch 373: val_loss did not improve from 17.06891
196/196 - 74s - loss: 16.6614 - MinusLogProbMetric: 16.6614 - val_loss: 17.1176 - val_MinusLogProbMetric: 17.1176 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 374/1000
2023-10-10 23:01:52.744 
Epoch 374/1000 
	 loss: 16.6790, MinusLogProbMetric: 16.6790, val_loss: 17.0976, val_MinusLogProbMetric: 17.0976

Epoch 374: val_loss did not improve from 17.06891
196/196 - 73s - loss: 16.6790 - MinusLogProbMetric: 16.6790 - val_loss: 17.0976 - val_MinusLogProbMetric: 17.0976 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 375/1000
2023-10-10 23:03:09.969 
Epoch 375/1000 
	 loss: 16.6433, MinusLogProbMetric: 16.6433, val_loss: 17.1857, val_MinusLogProbMetric: 17.1857

Epoch 375: val_loss did not improve from 17.06891
196/196 - 77s - loss: 16.6433 - MinusLogProbMetric: 16.6433 - val_loss: 17.1857 - val_MinusLogProbMetric: 17.1857 - lr: 8.3333e-05 - 77s/epoch - 394ms/step
Epoch 376/1000
2023-10-10 23:04:22.965 
Epoch 376/1000 
	 loss: 16.6865, MinusLogProbMetric: 16.6865, val_loss: 17.1765, val_MinusLogProbMetric: 17.1765

Epoch 376: val_loss did not improve from 17.06891
196/196 - 73s - loss: 16.6865 - MinusLogProbMetric: 16.6865 - val_loss: 17.1765 - val_MinusLogProbMetric: 17.1765 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 377/1000
2023-10-10 23:05:35.007 
Epoch 377/1000 
	 loss: 16.6698, MinusLogProbMetric: 16.6698, val_loss: 17.1978, val_MinusLogProbMetric: 17.1978

Epoch 377: val_loss did not improve from 17.06891
196/196 - 72s - loss: 16.6698 - MinusLogProbMetric: 16.6698 - val_loss: 17.1978 - val_MinusLogProbMetric: 17.1978 - lr: 8.3333e-05 - 72s/epoch - 367ms/step
Epoch 378/1000
2023-10-10 23:06:48.401 
Epoch 378/1000 
	 loss: 16.6534, MinusLogProbMetric: 16.6534, val_loss: 17.2286, val_MinusLogProbMetric: 17.2286

Epoch 378: val_loss did not improve from 17.06891
196/196 - 73s - loss: 16.6534 - MinusLogProbMetric: 16.6534 - val_loss: 17.2286 - val_MinusLogProbMetric: 17.2286 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 379/1000
2023-10-10 23:08:03.318 
Epoch 379/1000 
	 loss: 16.6587, MinusLogProbMetric: 16.6587, val_loss: 17.2163, val_MinusLogProbMetric: 17.2163

Epoch 379: val_loss did not improve from 17.06891
196/196 - 75s - loss: 16.6587 - MinusLogProbMetric: 16.6587 - val_loss: 17.2163 - val_MinusLogProbMetric: 17.2163 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 380/1000
2023-10-10 23:09:15.920 
Epoch 380/1000 
	 loss: 16.6481, MinusLogProbMetric: 16.6481, val_loss: 17.0883, val_MinusLogProbMetric: 17.0883

Epoch 380: val_loss did not improve from 17.06891
196/196 - 73s - loss: 16.6481 - MinusLogProbMetric: 16.6481 - val_loss: 17.0883 - val_MinusLogProbMetric: 17.0883 - lr: 8.3333e-05 - 73s/epoch - 370ms/step
Epoch 381/1000
2023-10-10 23:10:29.193 
Epoch 381/1000 
	 loss: 16.6451, MinusLogProbMetric: 16.6451, val_loss: 17.1194, val_MinusLogProbMetric: 17.1194

Epoch 381: val_loss did not improve from 17.06891
196/196 - 73s - loss: 16.6451 - MinusLogProbMetric: 16.6451 - val_loss: 17.1194 - val_MinusLogProbMetric: 17.1194 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 382/1000
2023-10-10 23:11:45.058 
Epoch 382/1000 
	 loss: 16.6736, MinusLogProbMetric: 16.6736, val_loss: 17.2210, val_MinusLogProbMetric: 17.2210

Epoch 382: val_loss did not improve from 17.06891
196/196 - 76s - loss: 16.6736 - MinusLogProbMetric: 16.6736 - val_loss: 17.2210 - val_MinusLogProbMetric: 17.2210 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 383/1000
2023-10-10 23:12:59.212 
Epoch 383/1000 
	 loss: 16.6413, MinusLogProbMetric: 16.6413, val_loss: 17.1134, val_MinusLogProbMetric: 17.1134

Epoch 383: val_loss did not improve from 17.06891
196/196 - 74s - loss: 16.6413 - MinusLogProbMetric: 16.6413 - val_loss: 17.1134 - val_MinusLogProbMetric: 17.1134 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 384/1000
2023-10-10 23:14:13.490 
Epoch 384/1000 
	 loss: 16.6805, MinusLogProbMetric: 16.6805, val_loss: 17.1127, val_MinusLogProbMetric: 17.1127

Epoch 384: val_loss did not improve from 17.06891
196/196 - 74s - loss: 16.6805 - MinusLogProbMetric: 16.6805 - val_loss: 17.1127 - val_MinusLogProbMetric: 17.1127 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 385/1000
2023-10-10 23:15:27.091 
Epoch 385/1000 
	 loss: 16.6575, MinusLogProbMetric: 16.6575, val_loss: 17.1007, val_MinusLogProbMetric: 17.1007

Epoch 385: val_loss did not improve from 17.06891
196/196 - 74s - loss: 16.6575 - MinusLogProbMetric: 16.6575 - val_loss: 17.1007 - val_MinusLogProbMetric: 17.1007 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 386/1000
2023-10-10 23:16:36.606 
Epoch 386/1000 
	 loss: 16.6622, MinusLogProbMetric: 16.6622, val_loss: 17.0580, val_MinusLogProbMetric: 17.0580

Epoch 386: val_loss improved from 17.06891 to 17.05805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 72s - loss: 16.6622 - MinusLogProbMetric: 16.6622 - val_loss: 17.0580 - val_MinusLogProbMetric: 17.0580 - lr: 8.3333e-05 - 72s/epoch - 368ms/step
Epoch 387/1000
2023-10-10 23:17:53.092 
Epoch 387/1000 
	 loss: 16.6503, MinusLogProbMetric: 16.6503, val_loss: 17.1324, val_MinusLogProbMetric: 17.1324

Epoch 387: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.6503 - MinusLogProbMetric: 16.6503 - val_loss: 17.1324 - val_MinusLogProbMetric: 17.1324 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 388/1000
2023-10-10 23:19:05.935 
Epoch 388/1000 
	 loss: 16.6677, MinusLogProbMetric: 16.6677, val_loss: 17.2933, val_MinusLogProbMetric: 17.2933

Epoch 388: val_loss did not improve from 17.05805
196/196 - 73s - loss: 16.6677 - MinusLogProbMetric: 16.6677 - val_loss: 17.2933 - val_MinusLogProbMetric: 17.2933 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 389/1000
2023-10-10 23:20:18.987 
Epoch 389/1000 
	 loss: 16.6715, MinusLogProbMetric: 16.6715, val_loss: 17.1708, val_MinusLogProbMetric: 17.1708

Epoch 389: val_loss did not improve from 17.05805
196/196 - 73s - loss: 16.6715 - MinusLogProbMetric: 16.6715 - val_loss: 17.1708 - val_MinusLogProbMetric: 17.1708 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 390/1000
2023-10-10 23:21:34.934 
Epoch 390/1000 
	 loss: 16.6706, MinusLogProbMetric: 16.6706, val_loss: 17.0610, val_MinusLogProbMetric: 17.0610

Epoch 390: val_loss did not improve from 17.05805
196/196 - 76s - loss: 16.6706 - MinusLogProbMetric: 16.6706 - val_loss: 17.0610 - val_MinusLogProbMetric: 17.0610 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 391/1000
2023-10-10 23:22:49.732 
Epoch 391/1000 
	 loss: 16.6662, MinusLogProbMetric: 16.6662, val_loss: 17.1736, val_MinusLogProbMetric: 17.1736

Epoch 391: val_loss did not improve from 17.05805
196/196 - 75s - loss: 16.6662 - MinusLogProbMetric: 16.6662 - val_loss: 17.1736 - val_MinusLogProbMetric: 17.1736 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 392/1000
2023-10-10 23:24:01.923 
Epoch 392/1000 
	 loss: 16.6743, MinusLogProbMetric: 16.6743, val_loss: 17.1349, val_MinusLogProbMetric: 17.1349

Epoch 392: val_loss did not improve from 17.05805
196/196 - 72s - loss: 16.6743 - MinusLogProbMetric: 16.6743 - val_loss: 17.1349 - val_MinusLogProbMetric: 17.1349 - lr: 8.3333e-05 - 72s/epoch - 368ms/step
Epoch 393/1000
2023-10-10 23:25:14.191 
Epoch 393/1000 
	 loss: 16.6427, MinusLogProbMetric: 16.6427, val_loss: 17.1298, val_MinusLogProbMetric: 17.1298

Epoch 393: val_loss did not improve from 17.05805
196/196 - 72s - loss: 16.6427 - MinusLogProbMetric: 16.6427 - val_loss: 17.1298 - val_MinusLogProbMetric: 17.1298 - lr: 8.3333e-05 - 72s/epoch - 369ms/step
Epoch 394/1000
2023-10-10 23:26:27.687 
Epoch 394/1000 
	 loss: 16.6320, MinusLogProbMetric: 16.6320, val_loss: 17.1552, val_MinusLogProbMetric: 17.1552

Epoch 394: val_loss did not improve from 17.05805
196/196 - 73s - loss: 16.6320 - MinusLogProbMetric: 16.6320 - val_loss: 17.1552 - val_MinusLogProbMetric: 17.1552 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 395/1000
2023-10-10 23:27:41.828 
Epoch 395/1000 
	 loss: 16.6649, MinusLogProbMetric: 16.6649, val_loss: 17.3651, val_MinusLogProbMetric: 17.3651

Epoch 395: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.6649 - MinusLogProbMetric: 16.6649 - val_loss: 17.3651 - val_MinusLogProbMetric: 17.3651 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 396/1000
2023-10-10 23:28:55.851 
Epoch 396/1000 
	 loss: 16.7113, MinusLogProbMetric: 16.7113, val_loss: 17.1665, val_MinusLogProbMetric: 17.1665

Epoch 396: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.7113 - MinusLogProbMetric: 16.7113 - val_loss: 17.1665 - val_MinusLogProbMetric: 17.1665 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 397/1000
2023-10-10 23:30:10.311 
Epoch 397/1000 
	 loss: 16.6652, MinusLogProbMetric: 16.6652, val_loss: 17.0736, val_MinusLogProbMetric: 17.0736

Epoch 397: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.6652 - MinusLogProbMetric: 16.6652 - val_loss: 17.0736 - val_MinusLogProbMetric: 17.0736 - lr: 8.3333e-05 - 74s/epoch - 380ms/step
Epoch 398/1000
2023-10-10 23:31:25.333 
Epoch 398/1000 
	 loss: 16.6694, MinusLogProbMetric: 16.6694, val_loss: 17.1893, val_MinusLogProbMetric: 17.1893

Epoch 398: val_loss did not improve from 17.05805
196/196 - 75s - loss: 16.6694 - MinusLogProbMetric: 16.6694 - val_loss: 17.1893 - val_MinusLogProbMetric: 17.1893 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 399/1000
2023-10-10 23:32:39.586 
Epoch 399/1000 
	 loss: 16.6717, MinusLogProbMetric: 16.6717, val_loss: 17.2173, val_MinusLogProbMetric: 17.2173

Epoch 399: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.6717 - MinusLogProbMetric: 16.6717 - val_loss: 17.2173 - val_MinusLogProbMetric: 17.2173 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 400/1000
2023-10-10 23:33:53.428 
Epoch 400/1000 
	 loss: 16.6435, MinusLogProbMetric: 16.6435, val_loss: 17.1603, val_MinusLogProbMetric: 17.1603

Epoch 400: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.6435 - MinusLogProbMetric: 16.6435 - val_loss: 17.1603 - val_MinusLogProbMetric: 17.1603 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 401/1000
2023-10-10 23:35:05.914 
Epoch 401/1000 
	 loss: 16.6721, MinusLogProbMetric: 16.6721, val_loss: 17.2274, val_MinusLogProbMetric: 17.2274

Epoch 401: val_loss did not improve from 17.05805
196/196 - 72s - loss: 16.6721 - MinusLogProbMetric: 16.6721 - val_loss: 17.2274 - val_MinusLogProbMetric: 17.2274 - lr: 8.3333e-05 - 72s/epoch - 370ms/step
Epoch 402/1000
2023-10-10 23:36:19.139 
Epoch 402/1000 
	 loss: 16.6504, MinusLogProbMetric: 16.6504, val_loss: 17.2071, val_MinusLogProbMetric: 17.2071

Epoch 402: val_loss did not improve from 17.05805
196/196 - 73s - loss: 16.6504 - MinusLogProbMetric: 16.6504 - val_loss: 17.2071 - val_MinusLogProbMetric: 17.2071 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 403/1000
2023-10-10 23:37:34.986 
Epoch 403/1000 
	 loss: 16.6711, MinusLogProbMetric: 16.6711, val_loss: 17.1191, val_MinusLogProbMetric: 17.1191

Epoch 403: val_loss did not improve from 17.05805
196/196 - 76s - loss: 16.6711 - MinusLogProbMetric: 16.6711 - val_loss: 17.1191 - val_MinusLogProbMetric: 17.1191 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 404/1000
2023-10-10 23:38:48.299 
Epoch 404/1000 
	 loss: 16.6594, MinusLogProbMetric: 16.6594, val_loss: 17.2068, val_MinusLogProbMetric: 17.2068

Epoch 404: val_loss did not improve from 17.05805
196/196 - 73s - loss: 16.6594 - MinusLogProbMetric: 16.6594 - val_loss: 17.2068 - val_MinusLogProbMetric: 17.2068 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 405/1000
2023-10-10 23:40:01.627 
Epoch 405/1000 
	 loss: 16.6739, MinusLogProbMetric: 16.6739, val_loss: 17.3080, val_MinusLogProbMetric: 17.3080

Epoch 405: val_loss did not improve from 17.05805
196/196 - 73s - loss: 16.6739 - MinusLogProbMetric: 16.6739 - val_loss: 17.3080 - val_MinusLogProbMetric: 17.3080 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 406/1000
2023-10-10 23:41:13.651 
Epoch 406/1000 
	 loss: 16.6573, MinusLogProbMetric: 16.6573, val_loss: 17.2774, val_MinusLogProbMetric: 17.2774

Epoch 406: val_loss did not improve from 17.05805
196/196 - 72s - loss: 16.6573 - MinusLogProbMetric: 16.6573 - val_loss: 17.2774 - val_MinusLogProbMetric: 17.2774 - lr: 8.3333e-05 - 72s/epoch - 367ms/step
Epoch 407/1000
2023-10-10 23:42:28.001 
Epoch 407/1000 
	 loss: 16.6888, MinusLogProbMetric: 16.6888, val_loss: 17.1373, val_MinusLogProbMetric: 17.1373

Epoch 407: val_loss did not improve from 17.05805
196/196 - 74s - loss: 16.6888 - MinusLogProbMetric: 16.6888 - val_loss: 17.1373 - val_MinusLogProbMetric: 17.1373 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 408/1000
2023-10-10 23:44:00.258 
Epoch 408/1000 
	 loss: 16.6486, MinusLogProbMetric: 16.6486, val_loss: 17.0605, val_MinusLogProbMetric: 17.0605

Epoch 408: val_loss did not improve from 17.05805
196/196 - 92s - loss: 16.6486 - MinusLogProbMetric: 16.6486 - val_loss: 17.0605 - val_MinusLogProbMetric: 17.0605 - lr: 8.3333e-05 - 92s/epoch - 471ms/step
Epoch 409/1000
2023-10-10 23:45:53.787 
Epoch 409/1000 
	 loss: 16.6620, MinusLogProbMetric: 16.6620, val_loss: 17.1620, val_MinusLogProbMetric: 17.1620

Epoch 409: val_loss did not improve from 17.05805
196/196 - 114s - loss: 16.6620 - MinusLogProbMetric: 16.6620 - val_loss: 17.1620 - val_MinusLogProbMetric: 17.1620 - lr: 8.3333e-05 - 114s/epoch - 579ms/step
Epoch 410/1000
2023-10-10 23:47:40.907 
Epoch 410/1000 
	 loss: 16.6356, MinusLogProbMetric: 16.6356, val_loss: 17.3237, val_MinusLogProbMetric: 17.3237

Epoch 410: val_loss did not improve from 17.05805
196/196 - 107s - loss: 16.6356 - MinusLogProbMetric: 16.6356 - val_loss: 17.3237 - val_MinusLogProbMetric: 17.3237 - lr: 8.3333e-05 - 107s/epoch - 547ms/step
Epoch 411/1000
2023-10-10 23:49:33.794 
Epoch 411/1000 
	 loss: 16.6605, MinusLogProbMetric: 16.6605, val_loss: 17.2451, val_MinusLogProbMetric: 17.2451

Epoch 411: val_loss did not improve from 17.05805
196/196 - 113s - loss: 16.6605 - MinusLogProbMetric: 16.6605 - val_loss: 17.2451 - val_MinusLogProbMetric: 17.2451 - lr: 8.3333e-05 - 113s/epoch - 576ms/step
Epoch 412/1000
2023-10-10 23:51:27.233 
Epoch 412/1000 
	 loss: 16.6518, MinusLogProbMetric: 16.6518, val_loss: 17.0689, val_MinusLogProbMetric: 17.0689

Epoch 412: val_loss did not improve from 17.05805
196/196 - 113s - loss: 16.6518 - MinusLogProbMetric: 16.6518 - val_loss: 17.0689 - val_MinusLogProbMetric: 17.0689 - lr: 8.3333e-05 - 113s/epoch - 579ms/step
Epoch 413/1000
2023-10-10 23:53:19.674 
Epoch 413/1000 
	 loss: 16.6628, MinusLogProbMetric: 16.6628, val_loss: 17.0573, val_MinusLogProbMetric: 17.0573

Epoch 413: val_loss improved from 17.05805 to 17.05727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 114s - loss: 16.6628 - MinusLogProbMetric: 16.6628 - val_loss: 17.0573 - val_MinusLogProbMetric: 17.0573 - lr: 8.3333e-05 - 114s/epoch - 582ms/step
Epoch 414/1000
2023-10-10 23:55:16.325 
Epoch 414/1000 
	 loss: 16.6568, MinusLogProbMetric: 16.6568, val_loss: 17.1598, val_MinusLogProbMetric: 17.1598

Epoch 414: val_loss did not improve from 17.05727
196/196 - 115s - loss: 16.6568 - MinusLogProbMetric: 16.6568 - val_loss: 17.1598 - val_MinusLogProbMetric: 17.1598 - lr: 8.3333e-05 - 115s/epoch - 586ms/step
Epoch 415/1000
2023-10-10 23:57:05.735 
Epoch 415/1000 
	 loss: 16.6417, MinusLogProbMetric: 16.6417, val_loss: 17.1093, val_MinusLogProbMetric: 17.1093

Epoch 415: val_loss did not improve from 17.05727
196/196 - 109s - loss: 16.6417 - MinusLogProbMetric: 16.6417 - val_loss: 17.1093 - val_MinusLogProbMetric: 17.1093 - lr: 8.3333e-05 - 109s/epoch - 558ms/step
Epoch 416/1000
2023-10-10 23:58:52.901 
Epoch 416/1000 
	 loss: 16.6603, MinusLogProbMetric: 16.6603, val_loss: 17.1848, val_MinusLogProbMetric: 17.1848

Epoch 416: val_loss did not improve from 17.05727
196/196 - 107s - loss: 16.6603 - MinusLogProbMetric: 16.6603 - val_loss: 17.1848 - val_MinusLogProbMetric: 17.1848 - lr: 8.3333e-05 - 107s/epoch - 547ms/step
Epoch 417/1000
2023-10-11 00:00:42.776 
Epoch 417/1000 
	 loss: 16.6532, MinusLogProbMetric: 16.6532, val_loss: 17.1930, val_MinusLogProbMetric: 17.1930

Epoch 417: val_loss did not improve from 17.05727
196/196 - 110s - loss: 16.6532 - MinusLogProbMetric: 16.6532 - val_loss: 17.1930 - val_MinusLogProbMetric: 17.1930 - lr: 8.3333e-05 - 110s/epoch - 561ms/step
Epoch 418/1000
2023-10-11 00:02:38.738 
Epoch 418/1000 
	 loss: 16.6727, MinusLogProbMetric: 16.6727, val_loss: 17.0387, val_MinusLogProbMetric: 17.0387

Epoch 418: val_loss improved from 17.05727 to 17.03872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 118s - loss: 16.6727 - MinusLogProbMetric: 16.6727 - val_loss: 17.0387 - val_MinusLogProbMetric: 17.0387 - lr: 8.3333e-05 - 118s/epoch - 604ms/step
Epoch 419/1000
2023-10-11 00:04:34.825 
Epoch 419/1000 
	 loss: 16.6470, MinusLogProbMetric: 16.6470, val_loss: 17.1160, val_MinusLogProbMetric: 17.1160

Epoch 419: val_loss did not improve from 17.03872
196/196 - 114s - loss: 16.6470 - MinusLogProbMetric: 16.6470 - val_loss: 17.1160 - val_MinusLogProbMetric: 17.1160 - lr: 8.3333e-05 - 114s/epoch - 580ms/step
Epoch 420/1000
2023-10-11 00:06:30.268 
Epoch 420/1000 
	 loss: 16.6223, MinusLogProbMetric: 16.6223, val_loss: 17.1780, val_MinusLogProbMetric: 17.1780

Epoch 420: val_loss did not improve from 17.03872
196/196 - 115s - loss: 16.6223 - MinusLogProbMetric: 16.6223 - val_loss: 17.1780 - val_MinusLogProbMetric: 17.1780 - lr: 8.3333e-05 - 115s/epoch - 589ms/step
Epoch 421/1000
2023-10-11 00:08:21.940 
Epoch 421/1000 
	 loss: 16.6795, MinusLogProbMetric: 16.6795, val_loss: 17.2209, val_MinusLogProbMetric: 17.2209

Epoch 421: val_loss did not improve from 17.03872
196/196 - 112s - loss: 16.6795 - MinusLogProbMetric: 16.6795 - val_loss: 17.2209 - val_MinusLogProbMetric: 17.2209 - lr: 8.3333e-05 - 112s/epoch - 570ms/step
Epoch 422/1000
2023-10-11 00:10:15.432 
Epoch 422/1000 
	 loss: 16.6527, MinusLogProbMetric: 16.6527, val_loss: 17.0549, val_MinusLogProbMetric: 17.0549

Epoch 422: val_loss did not improve from 17.03872
196/196 - 114s - loss: 16.6527 - MinusLogProbMetric: 16.6527 - val_loss: 17.0549 - val_MinusLogProbMetric: 17.0549 - lr: 8.3333e-05 - 114s/epoch - 579ms/step
Epoch 423/1000
2023-10-11 00:12:02.911 
Epoch 423/1000 
	 loss: 16.6574, MinusLogProbMetric: 16.6574, val_loss: 17.2047, val_MinusLogProbMetric: 17.2047

Epoch 423: val_loss did not improve from 17.03872
196/196 - 107s - loss: 16.6574 - MinusLogProbMetric: 16.6574 - val_loss: 17.2047 - val_MinusLogProbMetric: 17.2047 - lr: 8.3333e-05 - 107s/epoch - 548ms/step
Epoch 424/1000
2023-10-11 00:13:58.557 
Epoch 424/1000 
	 loss: 16.6365, MinusLogProbMetric: 16.6365, val_loss: 17.0517, val_MinusLogProbMetric: 17.0517

Epoch 424: val_loss did not improve from 17.03872
196/196 - 116s - loss: 16.6365 - MinusLogProbMetric: 16.6365 - val_loss: 17.0517 - val_MinusLogProbMetric: 17.0517 - lr: 8.3333e-05 - 116s/epoch - 590ms/step
Epoch 425/1000
2023-10-11 00:15:53.113 
Epoch 425/1000 
	 loss: 16.6683, MinusLogProbMetric: 16.6683, val_loss: 17.1374, val_MinusLogProbMetric: 17.1374

Epoch 425: val_loss did not improve from 17.03872
196/196 - 115s - loss: 16.6683 - MinusLogProbMetric: 16.6683 - val_loss: 17.1374 - val_MinusLogProbMetric: 17.1374 - lr: 8.3333e-05 - 115s/epoch - 585ms/step
Epoch 426/1000
2023-10-11 00:17:46.060 
Epoch 426/1000 
	 loss: 16.6627, MinusLogProbMetric: 16.6627, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 426: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6627 - MinusLogProbMetric: 16.6627 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 8.3333e-05 - 113s/epoch - 576ms/step
Epoch 427/1000
2023-10-11 00:19:40.313 
Epoch 427/1000 
	 loss: 16.6348, MinusLogProbMetric: 16.6348, val_loss: 17.1923, val_MinusLogProbMetric: 17.1923

Epoch 427: val_loss did not improve from 17.03872
196/196 - 114s - loss: 16.6348 - MinusLogProbMetric: 16.6348 - val_loss: 17.1923 - val_MinusLogProbMetric: 17.1923 - lr: 8.3333e-05 - 114s/epoch - 583ms/step
Epoch 428/1000
2023-10-11 00:21:36.653 
Epoch 428/1000 
	 loss: 16.6642, MinusLogProbMetric: 16.6642, val_loss: 17.2338, val_MinusLogProbMetric: 17.2338

Epoch 428: val_loss did not improve from 17.03872
196/196 - 116s - loss: 16.6642 - MinusLogProbMetric: 16.6642 - val_loss: 17.2338 - val_MinusLogProbMetric: 17.2338 - lr: 8.3333e-05 - 116s/epoch - 594ms/step
Epoch 429/1000
2023-10-11 00:23:25.605 
Epoch 429/1000 
	 loss: 16.6519, MinusLogProbMetric: 16.6519, val_loss: 17.0693, val_MinusLogProbMetric: 17.0693

Epoch 429: val_loss did not improve from 17.03872
196/196 - 109s - loss: 16.6519 - MinusLogProbMetric: 16.6519 - val_loss: 17.0693 - val_MinusLogProbMetric: 17.0693 - lr: 8.3333e-05 - 109s/epoch - 556ms/step
Epoch 430/1000
2023-10-11 00:25:19.085 
Epoch 430/1000 
	 loss: 16.6833, MinusLogProbMetric: 16.6833, val_loss: 17.1488, val_MinusLogProbMetric: 17.1488

Epoch 430: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6833 - MinusLogProbMetric: 16.6833 - val_loss: 17.1488 - val_MinusLogProbMetric: 17.1488 - lr: 8.3333e-05 - 113s/epoch - 579ms/step
Epoch 431/1000
2023-10-11 00:27:13.471 
Epoch 431/1000 
	 loss: 16.6422, MinusLogProbMetric: 16.6422, val_loss: 17.1811, val_MinusLogProbMetric: 17.1811

Epoch 431: val_loss did not improve from 17.03872
196/196 - 114s - loss: 16.6422 - MinusLogProbMetric: 16.6422 - val_loss: 17.1811 - val_MinusLogProbMetric: 17.1811 - lr: 8.3333e-05 - 114s/epoch - 584ms/step
Epoch 432/1000
2023-10-11 00:29:07.051 
Epoch 432/1000 
	 loss: 16.6598, MinusLogProbMetric: 16.6598, val_loss: 17.1661, val_MinusLogProbMetric: 17.1661

Epoch 432: val_loss did not improve from 17.03872
196/196 - 114s - loss: 16.6598 - MinusLogProbMetric: 16.6598 - val_loss: 17.1661 - val_MinusLogProbMetric: 17.1661 - lr: 8.3333e-05 - 114s/epoch - 579ms/step
Epoch 433/1000
2023-10-11 00:30:56.793 
Epoch 433/1000 
	 loss: 16.6489, MinusLogProbMetric: 16.6489, val_loss: 17.0540, val_MinusLogProbMetric: 17.0540

Epoch 433: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6489 - MinusLogProbMetric: 16.6489 - val_loss: 17.0540 - val_MinusLogProbMetric: 17.0540 - lr: 8.3333e-05 - 110s/epoch - 560ms/step
Epoch 434/1000
2023-10-11 00:32:48.450 
Epoch 434/1000 
	 loss: 16.6391, MinusLogProbMetric: 16.6391, val_loss: 17.1684, val_MinusLogProbMetric: 17.1684

Epoch 434: val_loss did not improve from 17.03872
196/196 - 112s - loss: 16.6391 - MinusLogProbMetric: 16.6391 - val_loss: 17.1684 - val_MinusLogProbMetric: 17.1684 - lr: 8.3333e-05 - 112s/epoch - 570ms/step
Epoch 435/1000
2023-10-11 00:34:36.097 
Epoch 435/1000 
	 loss: 16.6240, MinusLogProbMetric: 16.6240, val_loss: 17.1519, val_MinusLogProbMetric: 17.1519

Epoch 435: val_loss did not improve from 17.03872
196/196 - 108s - loss: 16.6240 - MinusLogProbMetric: 16.6240 - val_loss: 17.1519 - val_MinusLogProbMetric: 17.1519 - lr: 8.3333e-05 - 108s/epoch - 549ms/step
Epoch 436/1000
2023-10-11 00:36:24.659 
Epoch 436/1000 
	 loss: 16.6308, MinusLogProbMetric: 16.6308, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 436: val_loss did not improve from 17.03872
196/196 - 109s - loss: 16.6308 - MinusLogProbMetric: 16.6308 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 8.3333e-05 - 109s/epoch - 554ms/step
Epoch 437/1000
2023-10-11 00:38:15.362 
Epoch 437/1000 
	 loss: 16.6575, MinusLogProbMetric: 16.6575, val_loss: 17.0844, val_MinusLogProbMetric: 17.0844

Epoch 437: val_loss did not improve from 17.03872
196/196 - 111s - loss: 16.6575 - MinusLogProbMetric: 16.6575 - val_loss: 17.0844 - val_MinusLogProbMetric: 17.0844 - lr: 8.3333e-05 - 111s/epoch - 565ms/step
Epoch 438/1000
2023-10-11 00:40:03.365 
Epoch 438/1000 
	 loss: 16.6584, MinusLogProbMetric: 16.6584, val_loss: 17.0869, val_MinusLogProbMetric: 17.0869

Epoch 438: val_loss did not improve from 17.03872
196/196 - 108s - loss: 16.6584 - MinusLogProbMetric: 16.6584 - val_loss: 17.0869 - val_MinusLogProbMetric: 17.0869 - lr: 8.3333e-05 - 108s/epoch - 551ms/step
Epoch 439/1000
2023-10-11 00:41:53.173 
Epoch 439/1000 
	 loss: 16.6612, MinusLogProbMetric: 16.6612, val_loss: 17.1090, val_MinusLogProbMetric: 17.1090

Epoch 439: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6612 - MinusLogProbMetric: 16.6612 - val_loss: 17.1090 - val_MinusLogProbMetric: 17.1090 - lr: 8.3333e-05 - 110s/epoch - 560ms/step
Epoch 440/1000
2023-10-11 00:43:45.997 
Epoch 440/1000 
	 loss: 16.6409, MinusLogProbMetric: 16.6409, val_loss: 17.4443, val_MinusLogProbMetric: 17.4443

Epoch 440: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6409 - MinusLogProbMetric: 16.6409 - val_loss: 17.4443 - val_MinusLogProbMetric: 17.4443 - lr: 8.3333e-05 - 113s/epoch - 576ms/step
Epoch 441/1000
2023-10-11 00:45:31.791 
Epoch 441/1000 
	 loss: 16.6607, MinusLogProbMetric: 16.6607, val_loss: 17.2176, val_MinusLogProbMetric: 17.2176

Epoch 441: val_loss did not improve from 17.03872
196/196 - 106s - loss: 16.6607 - MinusLogProbMetric: 16.6607 - val_loss: 17.2176 - val_MinusLogProbMetric: 17.2176 - lr: 8.3333e-05 - 106s/epoch - 540ms/step
Epoch 442/1000
2023-10-11 00:47:20.225 
Epoch 442/1000 
	 loss: 16.6509, MinusLogProbMetric: 16.6509, val_loss: 17.1974, val_MinusLogProbMetric: 17.1974

Epoch 442: val_loss did not improve from 17.03872
196/196 - 108s - loss: 16.6509 - MinusLogProbMetric: 16.6509 - val_loss: 17.1974 - val_MinusLogProbMetric: 17.1974 - lr: 8.3333e-05 - 108s/epoch - 553ms/step
Epoch 443/1000
2023-10-11 00:49:09.943 
Epoch 443/1000 
	 loss: 16.6397, MinusLogProbMetric: 16.6397, val_loss: 17.1483, val_MinusLogProbMetric: 17.1483

Epoch 443: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6397 - MinusLogProbMetric: 16.6397 - val_loss: 17.1483 - val_MinusLogProbMetric: 17.1483 - lr: 8.3333e-05 - 110s/epoch - 560ms/step
Epoch 444/1000
2023-10-11 00:51:01.532 
Epoch 444/1000 
	 loss: 16.6466, MinusLogProbMetric: 16.6466, val_loss: 17.2906, val_MinusLogProbMetric: 17.2906

Epoch 444: val_loss did not improve from 17.03872
196/196 - 112s - loss: 16.6466 - MinusLogProbMetric: 16.6466 - val_loss: 17.2906 - val_MinusLogProbMetric: 17.2906 - lr: 8.3333e-05 - 112s/epoch - 569ms/step
Epoch 445/1000
2023-10-11 00:52:54.227 
Epoch 445/1000 
	 loss: 16.6588, MinusLogProbMetric: 16.6588, val_loss: 17.1535, val_MinusLogProbMetric: 17.1535

Epoch 445: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6588 - MinusLogProbMetric: 16.6588 - val_loss: 17.1535 - val_MinusLogProbMetric: 17.1535 - lr: 8.3333e-05 - 113s/epoch - 575ms/step
Epoch 446/1000
2023-10-11 00:54:50.738 
Epoch 446/1000 
	 loss: 16.6380, MinusLogProbMetric: 16.6380, val_loss: 17.1945, val_MinusLogProbMetric: 17.1945

Epoch 446: val_loss did not improve from 17.03872
196/196 - 116s - loss: 16.6380 - MinusLogProbMetric: 16.6380 - val_loss: 17.1945 - val_MinusLogProbMetric: 17.1945 - lr: 8.3333e-05 - 116s/epoch - 594ms/step
Epoch 447/1000
2023-10-11 00:56:44.668 
Epoch 447/1000 
	 loss: 16.6488, MinusLogProbMetric: 16.6488, val_loss: 17.3026, val_MinusLogProbMetric: 17.3026

Epoch 447: val_loss did not improve from 17.03872
196/196 - 114s - loss: 16.6488 - MinusLogProbMetric: 16.6488 - val_loss: 17.3026 - val_MinusLogProbMetric: 17.3026 - lr: 8.3333e-05 - 114s/epoch - 581ms/step
Epoch 448/1000
2023-10-11 00:58:37.953 
Epoch 448/1000 
	 loss: 16.6694, MinusLogProbMetric: 16.6694, val_loss: 17.2599, val_MinusLogProbMetric: 17.2599

Epoch 448: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6694 - MinusLogProbMetric: 16.6694 - val_loss: 17.2599 - val_MinusLogProbMetric: 17.2599 - lr: 8.3333e-05 - 113s/epoch - 578ms/step
Epoch 449/1000
2023-10-11 01:00:27.780 
Epoch 449/1000 
	 loss: 16.6466, MinusLogProbMetric: 16.6466, val_loss: 17.1499, val_MinusLogProbMetric: 17.1499

Epoch 449: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6466 - MinusLogProbMetric: 16.6466 - val_loss: 17.1499 - val_MinusLogProbMetric: 17.1499 - lr: 8.3333e-05 - 110s/epoch - 560ms/step
Epoch 450/1000
2023-10-11 01:02:20.571 
Epoch 450/1000 
	 loss: 16.6312, MinusLogProbMetric: 16.6312, val_loss: 17.1647, val_MinusLogProbMetric: 17.1647

Epoch 450: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6312 - MinusLogProbMetric: 16.6312 - val_loss: 17.1647 - val_MinusLogProbMetric: 17.1647 - lr: 8.3333e-05 - 113s/epoch - 576ms/step
Epoch 451/1000
2023-10-11 01:04:05.041 
Epoch 451/1000 
	 loss: 16.6440, MinusLogProbMetric: 16.6440, val_loss: 17.1425, val_MinusLogProbMetric: 17.1425

Epoch 451: val_loss did not improve from 17.03872
196/196 - 104s - loss: 16.6440 - MinusLogProbMetric: 16.6440 - val_loss: 17.1425 - val_MinusLogProbMetric: 17.1425 - lr: 8.3333e-05 - 104s/epoch - 533ms/step
Epoch 452/1000
2023-10-11 01:05:55.463 
Epoch 452/1000 
	 loss: 16.6422, MinusLogProbMetric: 16.6422, val_loss: 17.0981, val_MinusLogProbMetric: 17.0981

Epoch 452: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6422 - MinusLogProbMetric: 16.6422 - val_loss: 17.0981 - val_MinusLogProbMetric: 17.0981 - lr: 8.3333e-05 - 110s/epoch - 563ms/step
Epoch 453/1000
2023-10-11 01:07:47.404 
Epoch 453/1000 
	 loss: 16.6605, MinusLogProbMetric: 16.6605, val_loss: 17.1263, val_MinusLogProbMetric: 17.1263

Epoch 453: val_loss did not improve from 17.03872
196/196 - 112s - loss: 16.6605 - MinusLogProbMetric: 16.6605 - val_loss: 17.1263 - val_MinusLogProbMetric: 17.1263 - lr: 8.3333e-05 - 112s/epoch - 571ms/step
Epoch 454/1000
2023-10-11 01:09:36.991 
Epoch 454/1000 
	 loss: 16.6250, MinusLogProbMetric: 16.6250, val_loss: 17.1005, val_MinusLogProbMetric: 17.1005

Epoch 454: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6250 - MinusLogProbMetric: 16.6250 - val_loss: 17.1005 - val_MinusLogProbMetric: 17.1005 - lr: 8.3333e-05 - 110s/epoch - 559ms/step
Epoch 455/1000
2023-10-11 01:11:26.282 
Epoch 455/1000 
	 loss: 16.6216, MinusLogProbMetric: 16.6216, val_loss: 17.1388, val_MinusLogProbMetric: 17.1388

Epoch 455: val_loss did not improve from 17.03872
196/196 - 109s - loss: 16.6216 - MinusLogProbMetric: 16.6216 - val_loss: 17.1388 - val_MinusLogProbMetric: 17.1388 - lr: 8.3333e-05 - 109s/epoch - 558ms/step
Epoch 456/1000
2023-10-11 01:13:12.582 
Epoch 456/1000 
	 loss: 16.6424, MinusLogProbMetric: 16.6424, val_loss: 17.0889, val_MinusLogProbMetric: 17.0889

Epoch 456: val_loss did not improve from 17.03872
196/196 - 106s - loss: 16.6424 - MinusLogProbMetric: 16.6424 - val_loss: 17.0889 - val_MinusLogProbMetric: 17.0889 - lr: 8.3333e-05 - 106s/epoch - 542ms/step
Epoch 457/1000
2023-10-11 01:14:57.686 
Epoch 457/1000 
	 loss: 16.6296, MinusLogProbMetric: 16.6296, val_loss: 17.1529, val_MinusLogProbMetric: 17.1529

Epoch 457: val_loss did not improve from 17.03872
196/196 - 105s - loss: 16.6296 - MinusLogProbMetric: 16.6296 - val_loss: 17.1529 - val_MinusLogProbMetric: 17.1529 - lr: 8.3333e-05 - 105s/epoch - 536ms/step
Epoch 458/1000
2023-10-11 01:16:46.562 
Epoch 458/1000 
	 loss: 16.6452, MinusLogProbMetric: 16.6452, val_loss: 17.2716, val_MinusLogProbMetric: 17.2716

Epoch 458: val_loss did not improve from 17.03872
196/196 - 109s - loss: 16.6452 - MinusLogProbMetric: 16.6452 - val_loss: 17.2716 - val_MinusLogProbMetric: 17.2716 - lr: 8.3333e-05 - 109s/epoch - 555ms/step
Epoch 459/1000
2023-10-11 01:18:37.633 
Epoch 459/1000 
	 loss: 16.7103, MinusLogProbMetric: 16.7103, val_loss: 17.1380, val_MinusLogProbMetric: 17.1380

Epoch 459: val_loss did not improve from 17.03872
196/196 - 111s - loss: 16.7103 - MinusLogProbMetric: 16.7103 - val_loss: 17.1380 - val_MinusLogProbMetric: 17.1380 - lr: 8.3333e-05 - 111s/epoch - 567ms/step
Epoch 460/1000
2023-10-11 01:20:27.813 
Epoch 460/1000 
	 loss: 16.6658, MinusLogProbMetric: 16.6658, val_loss: 17.2593, val_MinusLogProbMetric: 17.2593

Epoch 460: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.6658 - MinusLogProbMetric: 16.6658 - val_loss: 17.2593 - val_MinusLogProbMetric: 17.2593 - lr: 8.3333e-05 - 110s/epoch - 562ms/step
Epoch 461/1000
2023-10-11 01:22:20.781 
Epoch 461/1000 
	 loss: 16.6684, MinusLogProbMetric: 16.6684, val_loss: 17.1295, val_MinusLogProbMetric: 17.1295

Epoch 461: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6684 - MinusLogProbMetric: 16.6684 - val_loss: 17.1295 - val_MinusLogProbMetric: 17.1295 - lr: 8.3333e-05 - 113s/epoch - 576ms/step
Epoch 462/1000
2023-10-11 01:24:10.033 
Epoch 462/1000 
	 loss: 16.6188, MinusLogProbMetric: 16.6188, val_loss: 18.3043, val_MinusLogProbMetric: 18.3043

Epoch 462: val_loss did not improve from 17.03872
196/196 - 109s - loss: 16.6188 - MinusLogProbMetric: 16.6188 - val_loss: 18.3043 - val_MinusLogProbMetric: 18.3043 - lr: 8.3333e-05 - 109s/epoch - 558ms/step
Epoch 463/1000
2023-10-11 01:25:58.288 
Epoch 463/1000 
	 loss: 16.6910, MinusLogProbMetric: 16.6910, val_loss: 17.1495, val_MinusLogProbMetric: 17.1495

Epoch 463: val_loss did not improve from 17.03872
196/196 - 108s - loss: 16.6910 - MinusLogProbMetric: 16.6910 - val_loss: 17.1495 - val_MinusLogProbMetric: 17.1495 - lr: 8.3333e-05 - 108s/epoch - 552ms/step
Epoch 464/1000
2023-10-11 01:27:45.643 
Epoch 464/1000 
	 loss: 16.6226, MinusLogProbMetric: 16.6226, val_loss: 17.0543, val_MinusLogProbMetric: 17.0543

Epoch 464: val_loss did not improve from 17.03872
196/196 - 107s - loss: 16.6226 - MinusLogProbMetric: 16.6226 - val_loss: 17.0543 - val_MinusLogProbMetric: 17.0543 - lr: 8.3333e-05 - 107s/epoch - 548ms/step
Epoch 465/1000
2023-10-11 01:29:37.636 
Epoch 465/1000 
	 loss: 16.6281, MinusLogProbMetric: 16.6281, val_loss: 17.2011, val_MinusLogProbMetric: 17.2011

Epoch 465: val_loss did not improve from 17.03872
196/196 - 112s - loss: 16.6281 - MinusLogProbMetric: 16.6281 - val_loss: 17.2011 - val_MinusLogProbMetric: 17.2011 - lr: 8.3333e-05 - 112s/epoch - 571ms/step
Epoch 466/1000
2023-10-11 01:31:30.212 
Epoch 466/1000 
	 loss: 16.6292, MinusLogProbMetric: 16.6292, val_loss: 17.0788, val_MinusLogProbMetric: 17.0788

Epoch 466: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6292 - MinusLogProbMetric: 16.6292 - val_loss: 17.0788 - val_MinusLogProbMetric: 17.0788 - lr: 8.3333e-05 - 113s/epoch - 574ms/step
Epoch 467/1000
2023-10-11 01:33:23.592 
Epoch 467/1000 
	 loss: 16.6471, MinusLogProbMetric: 16.6471, val_loss: 17.1960, val_MinusLogProbMetric: 17.1960

Epoch 467: val_loss did not improve from 17.03872
196/196 - 113s - loss: 16.6471 - MinusLogProbMetric: 16.6471 - val_loss: 17.1960 - val_MinusLogProbMetric: 17.1960 - lr: 8.3333e-05 - 113s/epoch - 578ms/step
Epoch 468/1000
2023-10-11 01:35:09.742 
Epoch 468/1000 
	 loss: 16.6275, MinusLogProbMetric: 16.6275, val_loss: 17.0908, val_MinusLogProbMetric: 17.0908

Epoch 468: val_loss did not improve from 17.03872
196/196 - 106s - loss: 16.6275 - MinusLogProbMetric: 16.6275 - val_loss: 17.0908 - val_MinusLogProbMetric: 17.0908 - lr: 8.3333e-05 - 106s/epoch - 542ms/step
Epoch 469/1000
2023-10-11 01:36:59.635 
Epoch 469/1000 
	 loss: 16.5304, MinusLogProbMetric: 16.5304, val_loss: 17.0474, val_MinusLogProbMetric: 17.0474

Epoch 469: val_loss did not improve from 17.03872
196/196 - 110s - loss: 16.5304 - MinusLogProbMetric: 16.5304 - val_loss: 17.0474 - val_MinusLogProbMetric: 17.0474 - lr: 4.1667e-05 - 110s/epoch - 561ms/step
Epoch 470/1000
2023-10-11 01:38:45.756 
Epoch 470/1000 
	 loss: 16.5308, MinusLogProbMetric: 16.5308, val_loss: 17.0509, val_MinusLogProbMetric: 17.0509

Epoch 470: val_loss did not improve from 17.03872
196/196 - 106s - loss: 16.5308 - MinusLogProbMetric: 16.5308 - val_loss: 17.0509 - val_MinusLogProbMetric: 17.0509 - lr: 4.1667e-05 - 106s/epoch - 541ms/step
Epoch 471/1000
2023-10-11 01:40:41.055 
Epoch 471/1000 
	 loss: 16.5346, MinusLogProbMetric: 16.5346, val_loss: 17.0701, val_MinusLogProbMetric: 17.0701

Epoch 471: val_loss did not improve from 17.03872
196/196 - 115s - loss: 16.5346 - MinusLogProbMetric: 16.5346 - val_loss: 17.0701 - val_MinusLogProbMetric: 17.0701 - lr: 4.1667e-05 - 115s/epoch - 588ms/step
Epoch 472/1000
2023-10-11 01:42:31.760 
Epoch 472/1000 
	 loss: 16.5323, MinusLogProbMetric: 16.5323, val_loss: 17.0542, val_MinusLogProbMetric: 17.0542

Epoch 472: val_loss did not improve from 17.03872
196/196 - 111s - loss: 16.5323 - MinusLogProbMetric: 16.5323 - val_loss: 17.0542 - val_MinusLogProbMetric: 17.0542 - lr: 4.1667e-05 - 111s/epoch - 565ms/step
Epoch 473/1000
2023-10-11 01:44:18.504 
Epoch 473/1000 
	 loss: 16.5404, MinusLogProbMetric: 16.5404, val_loss: 17.0440, val_MinusLogProbMetric: 17.0440

Epoch 473: val_loss did not improve from 17.03872
196/196 - 107s - loss: 16.5404 - MinusLogProbMetric: 16.5404 - val_loss: 17.0440 - val_MinusLogProbMetric: 17.0440 - lr: 4.1667e-05 - 107s/epoch - 545ms/step
Epoch 474/1000
2023-10-11 01:46:10.374 
Epoch 474/1000 
	 loss: 16.5320, MinusLogProbMetric: 16.5320, val_loss: 17.0287, val_MinusLogProbMetric: 17.0287

Epoch 474: val_loss improved from 17.03872 to 17.02868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 114s - loss: 16.5320 - MinusLogProbMetric: 16.5320 - val_loss: 17.0287 - val_MinusLogProbMetric: 17.0287 - lr: 4.1667e-05 - 114s/epoch - 584ms/step
Epoch 475/1000
2023-10-11 01:48:08.933 
Epoch 475/1000 
	 loss: 16.5301, MinusLogProbMetric: 16.5301, val_loss: 17.0569, val_MinusLogProbMetric: 17.0569

Epoch 475: val_loss did not improve from 17.02868
196/196 - 116s - loss: 16.5301 - MinusLogProbMetric: 16.5301 - val_loss: 17.0569 - val_MinusLogProbMetric: 17.0569 - lr: 4.1667e-05 - 116s/epoch - 592ms/step
Epoch 476/1000
2023-10-11 01:50:02.958 
Epoch 476/1000 
	 loss: 16.5439, MinusLogProbMetric: 16.5439, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 476: val_loss did not improve from 17.02868
196/196 - 114s - loss: 16.5439 - MinusLogProbMetric: 16.5439 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 4.1667e-05 - 114s/epoch - 582ms/step
Epoch 477/1000
2023-10-11 01:51:50.750 
Epoch 477/1000 
	 loss: 16.5276, MinusLogProbMetric: 16.5276, val_loss: 17.0522, val_MinusLogProbMetric: 17.0522

Epoch 477: val_loss did not improve from 17.02868
196/196 - 108s - loss: 16.5276 - MinusLogProbMetric: 16.5276 - val_loss: 17.0522 - val_MinusLogProbMetric: 17.0522 - lr: 4.1667e-05 - 108s/epoch - 550ms/step
Epoch 478/1000
2023-10-11 01:53:45.530 
Epoch 478/1000 
	 loss: 16.5420, MinusLogProbMetric: 16.5420, val_loss: 17.0289, val_MinusLogProbMetric: 17.0289

Epoch 478: val_loss did not improve from 17.02868
196/196 - 115s - loss: 16.5420 - MinusLogProbMetric: 16.5420 - val_loss: 17.0289 - val_MinusLogProbMetric: 17.0289 - lr: 4.1667e-05 - 115s/epoch - 586ms/step
Epoch 479/1000
2023-10-11 01:55:40.252 
Epoch 479/1000 
	 loss: 16.5336, MinusLogProbMetric: 16.5336, val_loss: 17.1111, val_MinusLogProbMetric: 17.1111

Epoch 479: val_loss did not improve from 17.02868
196/196 - 115s - loss: 16.5336 - MinusLogProbMetric: 16.5336 - val_loss: 17.1111 - val_MinusLogProbMetric: 17.1111 - lr: 4.1667e-05 - 115s/epoch - 585ms/step
Epoch 480/1000
2023-10-11 01:57:35.828 
Epoch 480/1000 
	 loss: 16.5410, MinusLogProbMetric: 16.5410, val_loss: 17.0367, val_MinusLogProbMetric: 17.0367

Epoch 480: val_loss did not improve from 17.02868
196/196 - 116s - loss: 16.5410 - MinusLogProbMetric: 16.5410 - val_loss: 17.0367 - val_MinusLogProbMetric: 17.0367 - lr: 4.1667e-05 - 116s/epoch - 590ms/step
Epoch 481/1000
2023-10-11 01:59:40.165 
Epoch 481/1000 
	 loss: 16.5357, MinusLogProbMetric: 16.5357, val_loss: 17.0222, val_MinusLogProbMetric: 17.0222

Epoch 481: val_loss improved from 17.02868 to 17.02215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 127s - loss: 16.5357 - MinusLogProbMetric: 16.5357 - val_loss: 17.0222 - val_MinusLogProbMetric: 17.0222 - lr: 4.1667e-05 - 127s/epoch - 646ms/step
Epoch 482/1000
2023-10-11 02:01:40.276 
Epoch 482/1000 
	 loss: 16.5294, MinusLogProbMetric: 16.5294, val_loss: 17.0266, val_MinusLogProbMetric: 17.0266

Epoch 482: val_loss did not improve from 17.02215
196/196 - 118s - loss: 16.5294 - MinusLogProbMetric: 16.5294 - val_loss: 17.0266 - val_MinusLogProbMetric: 17.0266 - lr: 4.1667e-05 - 118s/epoch - 602ms/step
Epoch 483/1000
2023-10-11 02:03:38.961 
Epoch 483/1000 
	 loss: 16.5271, MinusLogProbMetric: 16.5271, val_loss: 17.0490, val_MinusLogProbMetric: 17.0490

Epoch 483: val_loss did not improve from 17.02215
196/196 - 119s - loss: 16.5271 - MinusLogProbMetric: 16.5271 - val_loss: 17.0490 - val_MinusLogProbMetric: 17.0490 - lr: 4.1667e-05 - 119s/epoch - 606ms/step
Epoch 484/1000
2023-10-11 02:05:34.910 
Epoch 484/1000 
	 loss: 16.5429, MinusLogProbMetric: 16.5429, val_loss: 17.0249, val_MinusLogProbMetric: 17.0249

Epoch 484: val_loss did not improve from 17.02215
196/196 - 116s - loss: 16.5429 - MinusLogProbMetric: 16.5429 - val_loss: 17.0249 - val_MinusLogProbMetric: 17.0249 - lr: 4.1667e-05 - 116s/epoch - 592ms/step
Epoch 485/1000
2023-10-11 02:07:29.613 
Epoch 485/1000 
	 loss: 16.5292, MinusLogProbMetric: 16.5292, val_loss: 17.0536, val_MinusLogProbMetric: 17.0536

Epoch 485: val_loss did not improve from 17.02215
196/196 - 115s - loss: 16.5292 - MinusLogProbMetric: 16.5292 - val_loss: 17.0536 - val_MinusLogProbMetric: 17.0536 - lr: 4.1667e-05 - 115s/epoch - 585ms/step
Epoch 486/1000
2023-10-11 02:09:18.584 
Epoch 486/1000 
	 loss: 16.5334, MinusLogProbMetric: 16.5334, val_loss: 17.1350, val_MinusLogProbMetric: 17.1350

Epoch 486: val_loss did not improve from 17.02215
196/196 - 109s - loss: 16.5334 - MinusLogProbMetric: 16.5334 - val_loss: 17.1350 - val_MinusLogProbMetric: 17.1350 - lr: 4.1667e-05 - 109s/epoch - 556ms/step
Epoch 487/1000
2023-10-11 02:11:18.279 
Epoch 487/1000 
	 loss: 16.5372, MinusLogProbMetric: 16.5372, val_loss: 17.0103, val_MinusLogProbMetric: 17.0103

Epoch 487: val_loss improved from 17.02215 to 17.01031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 126s - loss: 16.5372 - MinusLogProbMetric: 16.5372 - val_loss: 17.0103 - val_MinusLogProbMetric: 17.0103 - lr: 4.1667e-05 - 126s/epoch - 640ms/step
Epoch 488/1000
2023-10-11 02:13:10.593 
Epoch 488/1000 
	 loss: 16.5276, MinusLogProbMetric: 16.5276, val_loss: 17.0655, val_MinusLogProbMetric: 17.0655

Epoch 488: val_loss did not improve from 17.01031
196/196 - 106s - loss: 16.5276 - MinusLogProbMetric: 16.5276 - val_loss: 17.0655 - val_MinusLogProbMetric: 17.0655 - lr: 4.1667e-05 - 106s/epoch - 543ms/step
Epoch 489/1000
2023-10-11 02:14:58.804 
Epoch 489/1000 
	 loss: 16.5419, MinusLogProbMetric: 16.5419, val_loss: 17.0527, val_MinusLogProbMetric: 17.0527

Epoch 489: val_loss did not improve from 17.01031
196/196 - 108s - loss: 16.5419 - MinusLogProbMetric: 16.5419 - val_loss: 17.0527 - val_MinusLogProbMetric: 17.0527 - lr: 4.1667e-05 - 108s/epoch - 552ms/step
Epoch 490/1000
2023-10-11 02:16:52.496 
Epoch 490/1000 
	 loss: 16.5288, MinusLogProbMetric: 16.5288, val_loss: 17.0626, val_MinusLogProbMetric: 17.0626

Epoch 490: val_loss did not improve from 17.01031
196/196 - 114s - loss: 16.5288 - MinusLogProbMetric: 16.5288 - val_loss: 17.0626 - val_MinusLogProbMetric: 17.0626 - lr: 4.1667e-05 - 114s/epoch - 580ms/step
Epoch 491/1000
2023-10-11 02:18:42.299 
Epoch 491/1000 
	 loss: 16.5330, MinusLogProbMetric: 16.5330, val_loss: 17.0299, val_MinusLogProbMetric: 17.0299

Epoch 491: val_loss did not improve from 17.01031
196/196 - 110s - loss: 16.5330 - MinusLogProbMetric: 16.5330 - val_loss: 17.0299 - val_MinusLogProbMetric: 17.0299 - lr: 4.1667e-05 - 110s/epoch - 560ms/step
Epoch 492/1000
2023-10-11 02:20:38.291 
Epoch 492/1000 
	 loss: 16.5437, MinusLogProbMetric: 16.5437, val_loss: 17.0236, val_MinusLogProbMetric: 17.0236

Epoch 492: val_loss did not improve from 17.01031
196/196 - 116s - loss: 16.5437 - MinusLogProbMetric: 16.5437 - val_loss: 17.0236 - val_MinusLogProbMetric: 17.0236 - lr: 4.1667e-05 - 116s/epoch - 592ms/step
Epoch 493/1000
2023-10-11 02:22:29.891 
Epoch 493/1000 
	 loss: 16.5172, MinusLogProbMetric: 16.5172, val_loss: 17.0159, val_MinusLogProbMetric: 17.0159

Epoch 493: val_loss did not improve from 17.01031
196/196 - 112s - loss: 16.5172 - MinusLogProbMetric: 16.5172 - val_loss: 17.0159 - val_MinusLogProbMetric: 17.0159 - lr: 4.1667e-05 - 112s/epoch - 569ms/step
Epoch 494/1000
2023-10-11 02:24:20.792 
Epoch 494/1000 
	 loss: 16.5373, MinusLogProbMetric: 16.5373, val_loss: 17.0078, val_MinusLogProbMetric: 17.0078

Epoch 494: val_loss improved from 17.01031 to 17.00776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 114s - loss: 16.5373 - MinusLogProbMetric: 16.5373 - val_loss: 17.0078 - val_MinusLogProbMetric: 17.0078 - lr: 4.1667e-05 - 114s/epoch - 581ms/step
Epoch 495/1000
2023-10-11 02:26:13.481 
Epoch 495/1000 
	 loss: 16.5287, MinusLogProbMetric: 16.5287, val_loss: 17.0351, val_MinusLogProbMetric: 17.0351

Epoch 495: val_loss did not improve from 17.00776
196/196 - 110s - loss: 16.5287 - MinusLogProbMetric: 16.5287 - val_loss: 17.0351 - val_MinusLogProbMetric: 17.0351 - lr: 4.1667e-05 - 110s/epoch - 560ms/step
Epoch 496/1000
2023-10-11 02:28:04.642 
Epoch 496/1000 
	 loss: 16.5384, MinusLogProbMetric: 16.5384, val_loss: 17.0808, val_MinusLogProbMetric: 17.0808

Epoch 496: val_loss did not improve from 17.00776
196/196 - 111s - loss: 16.5384 - MinusLogProbMetric: 16.5384 - val_loss: 17.0808 - val_MinusLogProbMetric: 17.0808 - lr: 4.1667e-05 - 111s/epoch - 567ms/step
Epoch 497/1000
2023-10-11 02:29:56.692 
Epoch 497/1000 
	 loss: 16.5279, MinusLogProbMetric: 16.5279, val_loss: 17.0189, val_MinusLogProbMetric: 17.0189

Epoch 497: val_loss did not improve from 17.00776
196/196 - 112s - loss: 16.5279 - MinusLogProbMetric: 16.5279 - val_loss: 17.0189 - val_MinusLogProbMetric: 17.0189 - lr: 4.1667e-05 - 112s/epoch - 572ms/step
Epoch 498/1000
2023-10-11 02:31:49.932 
Epoch 498/1000 
	 loss: 16.5298, MinusLogProbMetric: 16.5298, val_loss: 17.0208, val_MinusLogProbMetric: 17.0208

Epoch 498: val_loss did not improve from 17.00776
196/196 - 113s - loss: 16.5298 - MinusLogProbMetric: 16.5298 - val_loss: 17.0208 - val_MinusLogProbMetric: 17.0208 - lr: 4.1667e-05 - 113s/epoch - 578ms/step
Epoch 499/1000
2023-10-11 02:33:37.612 
Epoch 499/1000 
	 loss: 16.5347, MinusLogProbMetric: 16.5347, val_loss: 17.0097, val_MinusLogProbMetric: 17.0097

Epoch 499: val_loss did not improve from 17.00776
196/196 - 108s - loss: 16.5347 - MinusLogProbMetric: 16.5347 - val_loss: 17.0097 - val_MinusLogProbMetric: 17.0097 - lr: 4.1667e-05 - 108s/epoch - 549ms/step
Epoch 500/1000
2023-10-11 02:35:29.263 
Epoch 500/1000 
	 loss: 16.5254, MinusLogProbMetric: 16.5254, val_loss: 17.0245, val_MinusLogProbMetric: 17.0245

Epoch 500: val_loss did not improve from 17.00776
196/196 - 112s - loss: 16.5254 - MinusLogProbMetric: 16.5254 - val_loss: 17.0245 - val_MinusLogProbMetric: 17.0245 - lr: 4.1667e-05 - 112s/epoch - 570ms/step
Epoch 501/1000
2023-10-11 02:37:23.293 
Epoch 501/1000 
	 loss: 16.5368, MinusLogProbMetric: 16.5368, val_loss: 17.0565, val_MinusLogProbMetric: 17.0565

Epoch 501: val_loss did not improve from 17.00776
196/196 - 114s - loss: 16.5368 - MinusLogProbMetric: 16.5368 - val_loss: 17.0565 - val_MinusLogProbMetric: 17.0565 - lr: 4.1667e-05 - 114s/epoch - 582ms/step
Epoch 502/1000
2023-10-11 02:39:11.942 
Epoch 502/1000 
	 loss: 16.5333, MinusLogProbMetric: 16.5333, val_loss: 17.0239, val_MinusLogProbMetric: 17.0239

Epoch 502: val_loss did not improve from 17.00776
196/196 - 109s - loss: 16.5333 - MinusLogProbMetric: 16.5333 - val_loss: 17.0239 - val_MinusLogProbMetric: 17.0239 - lr: 4.1667e-05 - 109s/epoch - 554ms/step
Epoch 503/1000
2023-10-11 02:41:02.251 
Epoch 503/1000 
	 loss: 16.5233, MinusLogProbMetric: 16.5233, val_loss: 17.0228, val_MinusLogProbMetric: 17.0228

Epoch 503: val_loss did not improve from 17.00776
196/196 - 110s - loss: 16.5233 - MinusLogProbMetric: 16.5233 - val_loss: 17.0228 - val_MinusLogProbMetric: 17.0228 - lr: 4.1667e-05 - 110s/epoch - 563ms/step
Epoch 504/1000
2023-10-11 02:42:51.645 
Epoch 504/1000 
	 loss: 16.5268, MinusLogProbMetric: 16.5268, val_loss: 17.0336, val_MinusLogProbMetric: 17.0336

Epoch 504: val_loss did not improve from 17.00776
196/196 - 109s - loss: 16.5268 - MinusLogProbMetric: 16.5268 - val_loss: 17.0336 - val_MinusLogProbMetric: 17.0336 - lr: 4.1667e-05 - 109s/epoch - 558ms/step
Epoch 505/1000
2023-10-11 02:44:39.406 
Epoch 505/1000 
	 loss: 16.5413, MinusLogProbMetric: 16.5413, val_loss: 17.0480, val_MinusLogProbMetric: 17.0480

Epoch 505: val_loss did not improve from 17.00776
196/196 - 108s - loss: 16.5413 - MinusLogProbMetric: 16.5413 - val_loss: 17.0480 - val_MinusLogProbMetric: 17.0480 - lr: 4.1667e-05 - 108s/epoch - 550ms/step
Epoch 506/1000
2023-10-11 02:46:29.346 
Epoch 506/1000 
	 loss: 16.5280, MinusLogProbMetric: 16.5280, val_loss: 17.0724, val_MinusLogProbMetric: 17.0724

Epoch 506: val_loss did not improve from 17.00776
196/196 - 110s - loss: 16.5280 - MinusLogProbMetric: 16.5280 - val_loss: 17.0724 - val_MinusLogProbMetric: 17.0724 - lr: 4.1667e-05 - 110s/epoch - 561ms/step
Epoch 507/1000
2023-10-11 02:48:20.867 
Epoch 507/1000 
	 loss: 16.5271, MinusLogProbMetric: 16.5271, val_loss: 17.0044, val_MinusLogProbMetric: 17.0044

Epoch 507: val_loss improved from 17.00776 to 17.00442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 114s - loss: 16.5271 - MinusLogProbMetric: 16.5271 - val_loss: 17.0044 - val_MinusLogProbMetric: 17.0044 - lr: 4.1667e-05 - 114s/epoch - 583ms/step
Epoch 508/1000
2023-10-11 02:50:10.625 
Epoch 508/1000 
	 loss: 16.5199, MinusLogProbMetric: 16.5199, val_loss: 17.1537, val_MinusLogProbMetric: 17.1537

Epoch 508: val_loss did not improve from 17.00442
196/196 - 107s - loss: 16.5199 - MinusLogProbMetric: 16.5199 - val_loss: 17.1537 - val_MinusLogProbMetric: 17.1537 - lr: 4.1667e-05 - 107s/epoch - 546ms/step
Epoch 509/1000
2023-10-11 02:51:57.626 
Epoch 509/1000 
	 loss: 16.5299, MinusLogProbMetric: 16.5299, val_loss: 17.0062, val_MinusLogProbMetric: 17.0062

Epoch 509: val_loss did not improve from 17.00442
196/196 - 107s - loss: 16.5299 - MinusLogProbMetric: 16.5299 - val_loss: 17.0062 - val_MinusLogProbMetric: 17.0062 - lr: 4.1667e-05 - 107s/epoch - 546ms/step
Epoch 510/1000
2023-10-11 02:53:42.972 
Epoch 510/1000 
	 loss: 16.5240, MinusLogProbMetric: 16.5240, val_loss: 17.0208, val_MinusLogProbMetric: 17.0208

Epoch 510: val_loss did not improve from 17.00442
196/196 - 105s - loss: 16.5240 - MinusLogProbMetric: 16.5240 - val_loss: 17.0208 - val_MinusLogProbMetric: 17.0208 - lr: 4.1667e-05 - 105s/epoch - 537ms/step
Epoch 511/1000
2023-10-11 02:55:31.972 
Epoch 511/1000 
	 loss: 16.5362, MinusLogProbMetric: 16.5362, val_loss: 17.0973, val_MinusLogProbMetric: 17.0973

Epoch 511: val_loss did not improve from 17.00442
196/196 - 109s - loss: 16.5362 - MinusLogProbMetric: 16.5362 - val_loss: 17.0973 - val_MinusLogProbMetric: 17.0973 - lr: 4.1667e-05 - 109s/epoch - 556ms/step
Epoch 512/1000
2023-10-11 02:57:22.671 
Epoch 512/1000 
	 loss: 16.5243, MinusLogProbMetric: 16.5243, val_loss: 17.0412, val_MinusLogProbMetric: 17.0412

Epoch 512: val_loss did not improve from 17.00442
196/196 - 111s - loss: 16.5243 - MinusLogProbMetric: 16.5243 - val_loss: 17.0412 - val_MinusLogProbMetric: 17.0412 - lr: 4.1667e-05 - 111s/epoch - 565ms/step
Epoch 513/1000
2023-10-11 02:59:06.898 
Epoch 513/1000 
	 loss: 16.5250, MinusLogProbMetric: 16.5250, val_loss: 17.0384, val_MinusLogProbMetric: 17.0384

Epoch 513: val_loss did not improve from 17.00442
196/196 - 104s - loss: 16.5250 - MinusLogProbMetric: 16.5250 - val_loss: 17.0384 - val_MinusLogProbMetric: 17.0384 - lr: 4.1667e-05 - 104s/epoch - 532ms/step
Epoch 514/1000
2023-10-11 03:00:54.556 
Epoch 514/1000 
	 loss: 16.5253, MinusLogProbMetric: 16.5253, val_loss: 17.0857, val_MinusLogProbMetric: 17.0857

Epoch 514: val_loss did not improve from 17.00442
196/196 - 108s - loss: 16.5253 - MinusLogProbMetric: 16.5253 - val_loss: 17.0857 - val_MinusLogProbMetric: 17.0857 - lr: 4.1667e-05 - 108s/epoch - 549ms/step
Epoch 515/1000
2023-10-11 03:02:42.888 
Epoch 515/1000 
	 loss: 16.5271, MinusLogProbMetric: 16.5271, val_loss: 17.0117, val_MinusLogProbMetric: 17.0117

Epoch 515: val_loss did not improve from 17.00442
196/196 - 108s - loss: 16.5271 - MinusLogProbMetric: 16.5271 - val_loss: 17.0117 - val_MinusLogProbMetric: 17.0117 - lr: 4.1667e-05 - 108s/epoch - 553ms/step
Epoch 516/1000
2023-10-11 03:04:29.191 
Epoch 516/1000 
	 loss: 16.5212, MinusLogProbMetric: 16.5212, val_loss: 17.0327, val_MinusLogProbMetric: 17.0327

Epoch 516: val_loss did not improve from 17.00442
196/196 - 106s - loss: 16.5212 - MinusLogProbMetric: 16.5212 - val_loss: 17.0327 - val_MinusLogProbMetric: 17.0327 - lr: 4.1667e-05 - 106s/epoch - 542ms/step
Epoch 517/1000
2023-10-11 03:06:19.099 
Epoch 517/1000 
	 loss: 16.5183, MinusLogProbMetric: 16.5183, val_loss: 17.0037, val_MinusLogProbMetric: 17.0037

Epoch 517: val_loss improved from 17.00442 to 17.00366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 111s - loss: 16.5183 - MinusLogProbMetric: 16.5183 - val_loss: 17.0037 - val_MinusLogProbMetric: 17.0037 - lr: 4.1667e-05 - 111s/epoch - 568ms/step
Epoch 518/1000
2023-10-11 03:08:07.180 
Epoch 518/1000 
	 loss: 16.5266, MinusLogProbMetric: 16.5266, val_loss: 16.9977, val_MinusLogProbMetric: 16.9977

Epoch 518: val_loss improved from 17.00366 to 16.99773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 108s - loss: 16.5266 - MinusLogProbMetric: 16.5266 - val_loss: 16.9977 - val_MinusLogProbMetric: 16.9977 - lr: 4.1667e-05 - 108s/epoch - 553ms/step
Epoch 519/1000
2023-10-11 03:09:57.861 
Epoch 519/1000 
	 loss: 16.5193, MinusLogProbMetric: 16.5193, val_loss: 17.0540, val_MinusLogProbMetric: 17.0540

Epoch 519: val_loss did not improve from 16.99773
196/196 - 109s - loss: 16.5193 - MinusLogProbMetric: 16.5193 - val_loss: 17.0540 - val_MinusLogProbMetric: 17.0540 - lr: 4.1667e-05 - 109s/epoch - 555ms/step
Epoch 520/1000
2023-10-11 03:11:44.553 
Epoch 520/1000 
	 loss: 16.5374, MinusLogProbMetric: 16.5374, val_loss: 17.0736, val_MinusLogProbMetric: 17.0736

Epoch 520: val_loss did not improve from 16.99773
196/196 - 107s - loss: 16.5374 - MinusLogProbMetric: 16.5374 - val_loss: 17.0736 - val_MinusLogProbMetric: 17.0736 - lr: 4.1667e-05 - 107s/epoch - 544ms/step
Epoch 521/1000
2023-10-11 03:13:26.992 
Epoch 521/1000 
	 loss: 16.5122, MinusLogProbMetric: 16.5122, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 521: val_loss did not improve from 16.99773
196/196 - 102s - loss: 16.5122 - MinusLogProbMetric: 16.5122 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 4.1667e-05 - 102s/epoch - 523ms/step
Epoch 522/1000
2023-10-11 03:15:11.128 
Epoch 522/1000 
	 loss: 16.5243, MinusLogProbMetric: 16.5243, val_loss: 17.0356, val_MinusLogProbMetric: 17.0356

Epoch 522: val_loss did not improve from 16.99773
196/196 - 104s - loss: 16.5243 - MinusLogProbMetric: 16.5243 - val_loss: 17.0356 - val_MinusLogProbMetric: 17.0356 - lr: 4.1667e-05 - 104s/epoch - 531ms/step
Epoch 523/1000
2023-10-11 03:17:02.922 
Epoch 523/1000 
	 loss: 16.5152, MinusLogProbMetric: 16.5152, val_loss: 17.0115, val_MinusLogProbMetric: 17.0115

Epoch 523: val_loss did not improve from 16.99773
196/196 - 112s - loss: 16.5152 - MinusLogProbMetric: 16.5152 - val_loss: 17.0115 - val_MinusLogProbMetric: 17.0115 - lr: 4.1667e-05 - 112s/epoch - 570ms/step
Epoch 524/1000
2023-10-11 03:18:46.573 
Epoch 524/1000 
	 loss: 16.5264, MinusLogProbMetric: 16.5264, val_loss: 17.0018, val_MinusLogProbMetric: 17.0018

Epoch 524: val_loss did not improve from 16.99773
196/196 - 104s - loss: 16.5264 - MinusLogProbMetric: 16.5264 - val_loss: 17.0018 - val_MinusLogProbMetric: 17.0018 - lr: 4.1667e-05 - 104s/epoch - 529ms/step
Epoch 525/1000
2023-10-11 03:20:35.170 
Epoch 525/1000 
	 loss: 16.5269, MinusLogProbMetric: 16.5269, val_loss: 17.0457, val_MinusLogProbMetric: 17.0457

Epoch 525: val_loss did not improve from 16.99773
196/196 - 109s - loss: 16.5269 - MinusLogProbMetric: 16.5269 - val_loss: 17.0457 - val_MinusLogProbMetric: 17.0457 - lr: 4.1667e-05 - 109s/epoch - 554ms/step
Epoch 526/1000
2023-10-11 03:22:26.510 
Epoch 526/1000 
	 loss: 16.5258, MinusLogProbMetric: 16.5258, val_loss: 17.0383, val_MinusLogProbMetric: 17.0383

Epoch 526: val_loss did not improve from 16.99773
196/196 - 111s - loss: 16.5258 - MinusLogProbMetric: 16.5258 - val_loss: 17.0383 - val_MinusLogProbMetric: 17.0383 - lr: 4.1667e-05 - 111s/epoch - 568ms/step
Epoch 527/1000
2023-10-11 03:24:12.607 
Epoch 527/1000 
	 loss: 16.5363, MinusLogProbMetric: 16.5363, val_loss: 17.0514, val_MinusLogProbMetric: 17.0514

Epoch 527: val_loss did not improve from 16.99773
196/196 - 106s - loss: 16.5363 - MinusLogProbMetric: 16.5363 - val_loss: 17.0514 - val_MinusLogProbMetric: 17.0514 - lr: 4.1667e-05 - 106s/epoch - 541ms/step
Epoch 528/1000
2023-10-11 03:25:58.575 
Epoch 528/1000 
	 loss: 16.5418, MinusLogProbMetric: 16.5418, val_loss: 17.1516, val_MinusLogProbMetric: 17.1516

Epoch 528: val_loss did not improve from 16.99773
196/196 - 106s - loss: 16.5418 - MinusLogProbMetric: 16.5418 - val_loss: 17.1516 - val_MinusLogProbMetric: 17.1516 - lr: 4.1667e-05 - 106s/epoch - 541ms/step
Epoch 529/1000
2023-10-11 03:27:46.243 
Epoch 529/1000 
	 loss: 16.5386, MinusLogProbMetric: 16.5386, val_loss: 17.1022, val_MinusLogProbMetric: 17.1022

Epoch 529: val_loss did not improve from 16.99773
196/196 - 108s - loss: 16.5386 - MinusLogProbMetric: 16.5386 - val_loss: 17.1022 - val_MinusLogProbMetric: 17.1022 - lr: 4.1667e-05 - 108s/epoch - 549ms/step
Epoch 530/1000
2023-10-11 03:29:33.203 
Epoch 530/1000 
	 loss: 16.5259, MinusLogProbMetric: 16.5259, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 530: val_loss did not improve from 16.99773
196/196 - 107s - loss: 16.5259 - MinusLogProbMetric: 16.5259 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 4.1667e-05 - 107s/epoch - 546ms/step
Epoch 531/1000
2023-10-11 03:31:24.087 
Epoch 531/1000 
	 loss: 16.5150, MinusLogProbMetric: 16.5150, val_loss: 17.0751, val_MinusLogProbMetric: 17.0751

Epoch 531: val_loss did not improve from 16.99773
196/196 - 111s - loss: 16.5150 - MinusLogProbMetric: 16.5150 - val_loss: 17.0751 - val_MinusLogProbMetric: 17.0751 - lr: 4.1667e-05 - 111s/epoch - 566ms/step
Epoch 532/1000
2023-10-11 03:33:13.290 
Epoch 532/1000 
	 loss: 16.5318, MinusLogProbMetric: 16.5318, val_loss: 17.0634, val_MinusLogProbMetric: 17.0634

Epoch 532: val_loss did not improve from 16.99773
196/196 - 109s - loss: 16.5318 - MinusLogProbMetric: 16.5318 - val_loss: 17.0634 - val_MinusLogProbMetric: 17.0634 - lr: 4.1667e-05 - 109s/epoch - 557ms/step
Epoch 533/1000
2023-10-11 03:35:00.612 
Epoch 533/1000 
	 loss: 16.5324, MinusLogProbMetric: 16.5324, val_loss: 17.0071, val_MinusLogProbMetric: 17.0071

Epoch 533: val_loss did not improve from 16.99773
196/196 - 107s - loss: 16.5324 - MinusLogProbMetric: 16.5324 - val_loss: 17.0071 - val_MinusLogProbMetric: 17.0071 - lr: 4.1667e-05 - 107s/epoch - 547ms/step
Epoch 534/1000
2023-10-11 03:36:48.537 
Epoch 534/1000 
	 loss: 16.5303, MinusLogProbMetric: 16.5303, val_loss: 17.0720, val_MinusLogProbMetric: 17.0720

Epoch 534: val_loss did not improve from 16.99773
196/196 - 108s - loss: 16.5303 - MinusLogProbMetric: 16.5303 - val_loss: 17.0720 - val_MinusLogProbMetric: 17.0720 - lr: 4.1667e-05 - 108s/epoch - 551ms/step
Epoch 535/1000
2023-10-11 03:38:39.025 
Epoch 535/1000 
	 loss: 16.5054, MinusLogProbMetric: 16.5054, val_loss: 17.0945, val_MinusLogProbMetric: 17.0945

Epoch 535: val_loss did not improve from 16.99773
196/196 - 110s - loss: 16.5054 - MinusLogProbMetric: 16.5054 - val_loss: 17.0945 - val_MinusLogProbMetric: 17.0945 - lr: 4.1667e-05 - 110s/epoch - 564ms/step
Epoch 536/1000
2023-10-11 03:40:30.527 
Epoch 536/1000 
	 loss: 16.5171, MinusLogProbMetric: 16.5171, val_loss: 17.1046, val_MinusLogProbMetric: 17.1046

Epoch 536: val_loss did not improve from 16.99773
196/196 - 111s - loss: 16.5171 - MinusLogProbMetric: 16.5171 - val_loss: 17.1046 - val_MinusLogProbMetric: 17.1046 - lr: 4.1667e-05 - 111s/epoch - 569ms/step
Epoch 537/1000
2023-10-11 03:42:29.980 
Epoch 537/1000 
	 loss: 16.5257, MinusLogProbMetric: 16.5257, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 537: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5257 - MinusLogProbMetric: 16.5257 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 4.1667e-05 - 119s/epoch - 609ms/step
Epoch 538/1000
2023-10-11 03:44:23.789 
Epoch 538/1000 
	 loss: 16.5231, MinusLogProbMetric: 16.5231, val_loss: 17.0119, val_MinusLogProbMetric: 17.0119

Epoch 538: val_loss did not improve from 16.99773
196/196 - 114s - loss: 16.5231 - MinusLogProbMetric: 16.5231 - val_loss: 17.0119 - val_MinusLogProbMetric: 17.0119 - lr: 4.1667e-05 - 114s/epoch - 581ms/step
Epoch 539/1000
2023-10-11 03:46:22.602 
Epoch 539/1000 
	 loss: 16.5269, MinusLogProbMetric: 16.5269, val_loss: 17.0159, val_MinusLogProbMetric: 17.0159

Epoch 539: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5269 - MinusLogProbMetric: 16.5269 - val_loss: 17.0159 - val_MinusLogProbMetric: 17.0159 - lr: 4.1667e-05 - 119s/epoch - 606ms/step
Epoch 540/1000
2023-10-11 03:48:21.361 
Epoch 540/1000 
	 loss: 16.5078, MinusLogProbMetric: 16.5078, val_loss: 17.0690, val_MinusLogProbMetric: 17.0690

Epoch 540: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5078 - MinusLogProbMetric: 16.5078 - val_loss: 17.0690 - val_MinusLogProbMetric: 17.0690 - lr: 4.1667e-05 - 119s/epoch - 606ms/step
Epoch 541/1000
2023-10-11 03:50:18.344 
Epoch 541/1000 
	 loss: 16.5449, MinusLogProbMetric: 16.5449, val_loss: 17.0211, val_MinusLogProbMetric: 17.0211

Epoch 541: val_loss did not improve from 16.99773
196/196 - 117s - loss: 16.5449 - MinusLogProbMetric: 16.5449 - val_loss: 17.0211 - val_MinusLogProbMetric: 17.0211 - lr: 4.1667e-05 - 117s/epoch - 597ms/step
Epoch 542/1000
2023-10-11 03:52:18.873 
Epoch 542/1000 
	 loss: 16.5227, MinusLogProbMetric: 16.5227, val_loss: 17.0512, val_MinusLogProbMetric: 17.0512

Epoch 542: val_loss did not improve from 16.99773
196/196 - 121s - loss: 16.5227 - MinusLogProbMetric: 16.5227 - val_loss: 17.0512 - val_MinusLogProbMetric: 17.0512 - lr: 4.1667e-05 - 121s/epoch - 615ms/step
Epoch 543/1000
2023-10-11 03:54:17.453 
Epoch 543/1000 
	 loss: 16.5314, MinusLogProbMetric: 16.5314, val_loss: 17.0674, val_MinusLogProbMetric: 17.0674

Epoch 543: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5314 - MinusLogProbMetric: 16.5314 - val_loss: 17.0674 - val_MinusLogProbMetric: 17.0674 - lr: 4.1667e-05 - 119s/epoch - 605ms/step
Epoch 544/1000
2023-10-11 03:56:14.394 
Epoch 544/1000 
	 loss: 16.5214, MinusLogProbMetric: 16.5214, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 544: val_loss did not improve from 16.99773
196/196 - 117s - loss: 16.5214 - MinusLogProbMetric: 16.5214 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 4.1667e-05 - 117s/epoch - 597ms/step
Epoch 545/1000
2023-10-11 03:58:15.154 
Epoch 545/1000 
	 loss: 16.5167, MinusLogProbMetric: 16.5167, val_loss: 17.0376, val_MinusLogProbMetric: 17.0376

Epoch 545: val_loss did not improve from 16.99773
196/196 - 121s - loss: 16.5167 - MinusLogProbMetric: 16.5167 - val_loss: 17.0376 - val_MinusLogProbMetric: 17.0376 - lr: 4.1667e-05 - 121s/epoch - 616ms/step
Epoch 546/1000
2023-10-11 04:00:14.729 
Epoch 546/1000 
	 loss: 16.5163, MinusLogProbMetric: 16.5163, val_loss: 17.0114, val_MinusLogProbMetric: 17.0114

Epoch 546: val_loss did not improve from 16.99773
196/196 - 120s - loss: 16.5163 - MinusLogProbMetric: 16.5163 - val_loss: 17.0114 - val_MinusLogProbMetric: 17.0114 - lr: 4.1667e-05 - 120s/epoch - 610ms/step
Epoch 547/1000
2023-10-11 04:02:14.618 
Epoch 547/1000 
	 loss: 16.5183, MinusLogProbMetric: 16.5183, val_loss: 17.0678, val_MinusLogProbMetric: 17.0678

Epoch 547: val_loss did not improve from 16.99773
196/196 - 120s - loss: 16.5183 - MinusLogProbMetric: 16.5183 - val_loss: 17.0678 - val_MinusLogProbMetric: 17.0678 - lr: 4.1667e-05 - 120s/epoch - 612ms/step
Epoch 548/1000
2023-10-11 04:04:17.373 
Epoch 548/1000 
	 loss: 16.5291, MinusLogProbMetric: 16.5291, val_loss: 17.0345, val_MinusLogProbMetric: 17.0345

Epoch 548: val_loss did not improve from 16.99773
196/196 - 123s - loss: 16.5291 - MinusLogProbMetric: 16.5291 - val_loss: 17.0345 - val_MinusLogProbMetric: 17.0345 - lr: 4.1667e-05 - 123s/epoch - 626ms/step
Epoch 549/1000
2023-10-11 04:06:16.352 
Epoch 549/1000 
	 loss: 16.5132, MinusLogProbMetric: 16.5132, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 549: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5132 - MinusLogProbMetric: 16.5132 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 4.1667e-05 - 119s/epoch - 607ms/step
Epoch 550/1000
2023-10-11 04:08:17.298 
Epoch 550/1000 
	 loss: 16.5474, MinusLogProbMetric: 16.5474, val_loss: 17.0918, val_MinusLogProbMetric: 17.0918

Epoch 550: val_loss did not improve from 16.99773
196/196 - 121s - loss: 16.5474 - MinusLogProbMetric: 16.5474 - val_loss: 17.0918 - val_MinusLogProbMetric: 17.0918 - lr: 4.1667e-05 - 121s/epoch - 617ms/step
Epoch 551/1000
2023-10-11 04:10:16.840 
Epoch 551/1000 
	 loss: 16.5175, MinusLogProbMetric: 16.5175, val_loss: 17.0175, val_MinusLogProbMetric: 17.0175

Epoch 551: val_loss did not improve from 16.99773
196/196 - 120s - loss: 16.5175 - MinusLogProbMetric: 16.5175 - val_loss: 17.0175 - val_MinusLogProbMetric: 17.0175 - lr: 4.1667e-05 - 120s/epoch - 610ms/step
Epoch 552/1000
2023-10-11 04:12:13.153 
Epoch 552/1000 
	 loss: 16.5136, MinusLogProbMetric: 16.5136, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 552: val_loss did not improve from 16.99773
196/196 - 116s - loss: 16.5136 - MinusLogProbMetric: 16.5136 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 4.1667e-05 - 116s/epoch - 594ms/step
Epoch 553/1000
2023-10-11 04:14:13.682 
Epoch 553/1000 
	 loss: 16.5334, MinusLogProbMetric: 16.5334, val_loss: 17.0795, val_MinusLogProbMetric: 17.0795

Epoch 553: val_loss did not improve from 16.99773
196/196 - 120s - loss: 16.5334 - MinusLogProbMetric: 16.5334 - val_loss: 17.0795 - val_MinusLogProbMetric: 17.0795 - lr: 4.1667e-05 - 120s/epoch - 615ms/step
Epoch 554/1000
2023-10-11 04:16:14.553 
Epoch 554/1000 
	 loss: 16.5247, MinusLogProbMetric: 16.5247, val_loss: 17.0748, val_MinusLogProbMetric: 17.0748

Epoch 554: val_loss did not improve from 16.99773
196/196 - 121s - loss: 16.5247 - MinusLogProbMetric: 16.5247 - val_loss: 17.0748 - val_MinusLogProbMetric: 17.0748 - lr: 4.1667e-05 - 121s/epoch - 617ms/step
Epoch 555/1000
2023-10-11 04:18:09.968 
Epoch 555/1000 
	 loss: 16.5239, MinusLogProbMetric: 16.5239, val_loss: 17.0641, val_MinusLogProbMetric: 17.0641

Epoch 555: val_loss did not improve from 16.99773
196/196 - 115s - loss: 16.5239 - MinusLogProbMetric: 16.5239 - val_loss: 17.0641 - val_MinusLogProbMetric: 17.0641 - lr: 4.1667e-05 - 115s/epoch - 589ms/step
Epoch 556/1000
2023-10-11 04:20:08.670 
Epoch 556/1000 
	 loss: 16.5207, MinusLogProbMetric: 16.5207, val_loss: 17.0596, val_MinusLogProbMetric: 17.0596

Epoch 556: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5207 - MinusLogProbMetric: 16.5207 - val_loss: 17.0596 - val_MinusLogProbMetric: 17.0596 - lr: 4.1667e-05 - 119s/epoch - 606ms/step
Epoch 557/1000
2023-10-11 04:22:05.707 
Epoch 557/1000 
	 loss: 16.5328, MinusLogProbMetric: 16.5328, val_loss: 17.2575, val_MinusLogProbMetric: 17.2575

Epoch 557: val_loss did not improve from 16.99773
196/196 - 117s - loss: 16.5328 - MinusLogProbMetric: 16.5328 - val_loss: 17.2575 - val_MinusLogProbMetric: 17.2575 - lr: 4.1667e-05 - 117s/epoch - 597ms/step
Epoch 558/1000
2023-10-11 04:24:00.965 
Epoch 558/1000 
	 loss: 16.5160, MinusLogProbMetric: 16.5160, val_loss: 17.0294, val_MinusLogProbMetric: 17.0294

Epoch 558: val_loss did not improve from 16.99773
196/196 - 115s - loss: 16.5160 - MinusLogProbMetric: 16.5160 - val_loss: 17.0294 - val_MinusLogProbMetric: 17.0294 - lr: 4.1667e-05 - 115s/epoch - 588ms/step
Epoch 559/1000
2023-10-11 04:25:54.172 
Epoch 559/1000 
	 loss: 16.5076, MinusLogProbMetric: 16.5076, val_loss: 17.0544, val_MinusLogProbMetric: 17.0544

Epoch 559: val_loss did not improve from 16.99773
196/196 - 113s - loss: 16.5076 - MinusLogProbMetric: 16.5076 - val_loss: 17.0544 - val_MinusLogProbMetric: 17.0544 - lr: 4.1667e-05 - 113s/epoch - 578ms/step
Epoch 560/1000
2023-10-11 04:27:52.404 
Epoch 560/1000 
	 loss: 16.5492, MinusLogProbMetric: 16.5492, val_loss: 17.0323, val_MinusLogProbMetric: 17.0323

Epoch 560: val_loss did not improve from 16.99773
196/196 - 118s - loss: 16.5492 - MinusLogProbMetric: 16.5492 - val_loss: 17.0323 - val_MinusLogProbMetric: 17.0323 - lr: 4.1667e-05 - 118s/epoch - 603ms/step
Epoch 561/1000
2023-10-11 04:29:45.332 
Epoch 561/1000 
	 loss: 16.5139, MinusLogProbMetric: 16.5139, val_loss: 17.0633, val_MinusLogProbMetric: 17.0633

Epoch 561: val_loss did not improve from 16.99773
196/196 - 113s - loss: 16.5139 - MinusLogProbMetric: 16.5139 - val_loss: 17.0633 - val_MinusLogProbMetric: 17.0633 - lr: 4.1667e-05 - 113s/epoch - 576ms/step
Epoch 562/1000
2023-10-11 04:31:44.463 
Epoch 562/1000 
	 loss: 16.5458, MinusLogProbMetric: 16.5458, val_loss: 17.0785, val_MinusLogProbMetric: 17.0785

Epoch 562: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5458 - MinusLogProbMetric: 16.5458 - val_loss: 17.0785 - val_MinusLogProbMetric: 17.0785 - lr: 4.1667e-05 - 119s/epoch - 608ms/step
Epoch 563/1000
2023-10-11 04:33:41.152 
Epoch 563/1000 
	 loss: 16.5225, MinusLogProbMetric: 16.5225, val_loss: 17.0118, val_MinusLogProbMetric: 17.0118

Epoch 563: val_loss did not improve from 16.99773
196/196 - 117s - loss: 16.5225 - MinusLogProbMetric: 16.5225 - val_loss: 17.0118 - val_MinusLogProbMetric: 17.0118 - lr: 4.1667e-05 - 117s/epoch - 595ms/step
Epoch 564/1000
2023-10-11 04:35:39.787 
Epoch 564/1000 
	 loss: 16.5235, MinusLogProbMetric: 16.5235, val_loss: 17.0208, val_MinusLogProbMetric: 17.0208

Epoch 564: val_loss did not improve from 16.99773
196/196 - 119s - loss: 16.5235 - MinusLogProbMetric: 16.5235 - val_loss: 17.0208 - val_MinusLogProbMetric: 17.0208 - lr: 4.1667e-05 - 119s/epoch - 605ms/step
Epoch 565/1000
2023-10-11 04:37:40.167 
Epoch 565/1000 
	 loss: 16.5145, MinusLogProbMetric: 16.5145, val_loss: 17.0214, val_MinusLogProbMetric: 17.0214

Epoch 565: val_loss did not improve from 16.99773
196/196 - 120s - loss: 16.5145 - MinusLogProbMetric: 16.5145 - val_loss: 17.0214 - val_MinusLogProbMetric: 17.0214 - lr: 4.1667e-05 - 120s/epoch - 614ms/step
Epoch 566/1000
2023-10-11 04:39:38.391 
Epoch 566/1000 
	 loss: 16.5315, MinusLogProbMetric: 16.5315, val_loss: 17.0169, val_MinusLogProbMetric: 17.0169

Epoch 566: val_loss did not improve from 16.99773
196/196 - 118s - loss: 16.5315 - MinusLogProbMetric: 16.5315 - val_loss: 17.0169 - val_MinusLogProbMetric: 17.0169 - lr: 4.1667e-05 - 118s/epoch - 603ms/step
Epoch 567/1000
2023-10-11 04:41:35.917 
Epoch 567/1000 
	 loss: 16.5015, MinusLogProbMetric: 16.5015, val_loss: 17.0028, val_MinusLogProbMetric: 17.0028

Epoch 567: val_loss did not improve from 16.99773
196/196 - 118s - loss: 16.5015 - MinusLogProbMetric: 16.5015 - val_loss: 17.0028 - val_MinusLogProbMetric: 17.0028 - lr: 4.1667e-05 - 118s/epoch - 600ms/step
Epoch 568/1000
2023-10-11 04:43:33.441 
Epoch 568/1000 
	 loss: 16.5147, MinusLogProbMetric: 16.5147, val_loss: 17.0434, val_MinusLogProbMetric: 17.0434

Epoch 568: val_loss did not improve from 16.99773
196/196 - 118s - loss: 16.5147 - MinusLogProbMetric: 16.5147 - val_loss: 17.0434 - val_MinusLogProbMetric: 17.0434 - lr: 4.1667e-05 - 118s/epoch - 600ms/step
Epoch 569/1000
2023-10-11 04:45:32.959 
Epoch 569/1000 
	 loss: 16.4714, MinusLogProbMetric: 16.4714, val_loss: 16.9831, val_MinusLogProbMetric: 16.9831

Epoch 569: val_loss improved from 16.99773 to 16.98315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 124s - loss: 16.4714 - MinusLogProbMetric: 16.4714 - val_loss: 16.9831 - val_MinusLogProbMetric: 16.9831 - lr: 2.0833e-05 - 124s/epoch - 632ms/step
Epoch 570/1000
2023-10-11 04:47:38.829 
Epoch 570/1000 
	 loss: 16.4670, MinusLogProbMetric: 16.4670, val_loss: 17.0073, val_MinusLogProbMetric: 17.0073

Epoch 570: val_loss did not improve from 16.98315
196/196 - 121s - loss: 16.4670 - MinusLogProbMetric: 16.4670 - val_loss: 17.0073 - val_MinusLogProbMetric: 17.0073 - lr: 2.0833e-05 - 121s/epoch - 620ms/step
Epoch 571/1000
2023-10-11 04:49:39.439 
Epoch 571/1000 
	 loss: 16.4691, MinusLogProbMetric: 16.4691, val_loss: 16.9877, val_MinusLogProbMetric: 16.9877

Epoch 571: val_loss did not improve from 16.98315
196/196 - 121s - loss: 16.4691 - MinusLogProbMetric: 16.4691 - val_loss: 16.9877 - val_MinusLogProbMetric: 16.9877 - lr: 2.0833e-05 - 121s/epoch - 615ms/step
Epoch 572/1000
2023-10-11 04:51:40.832 
Epoch 572/1000 
	 loss: 16.4688, MinusLogProbMetric: 16.4688, val_loss: 16.9828, val_MinusLogProbMetric: 16.9828

Epoch 572: val_loss improved from 16.98315 to 16.98280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 123s - loss: 16.4688 - MinusLogProbMetric: 16.4688 - val_loss: 16.9828 - val_MinusLogProbMetric: 16.9828 - lr: 2.0833e-05 - 123s/epoch - 626ms/step
Epoch 573/1000
2023-10-11 04:53:37.173 
Epoch 573/1000 
	 loss: 16.4692, MinusLogProbMetric: 16.4692, val_loss: 16.9811, val_MinusLogProbMetric: 16.9811

Epoch 573: val_loss improved from 16.98280 to 16.98106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 116s - loss: 16.4692 - MinusLogProbMetric: 16.4692 - val_loss: 16.9811 - val_MinusLogProbMetric: 16.9811 - lr: 2.0833e-05 - 116s/epoch - 593ms/step
Epoch 574/1000
2023-10-11 04:55:35.741 
Epoch 574/1000 
	 loss: 16.4646, MinusLogProbMetric: 16.4646, val_loss: 16.9900, val_MinusLogProbMetric: 16.9900

Epoch 574: val_loss did not improve from 16.98106
196/196 - 117s - loss: 16.4646 - MinusLogProbMetric: 16.4646 - val_loss: 16.9900 - val_MinusLogProbMetric: 16.9900 - lr: 2.0833e-05 - 117s/epoch - 599ms/step
Epoch 575/1000
2023-10-11 04:57:29.361 
Epoch 575/1000 
	 loss: 16.4689, MinusLogProbMetric: 16.4689, val_loss: 16.9880, val_MinusLogProbMetric: 16.9880

Epoch 575: val_loss did not improve from 16.98106
196/196 - 114s - loss: 16.4689 - MinusLogProbMetric: 16.4689 - val_loss: 16.9880 - val_MinusLogProbMetric: 16.9880 - lr: 2.0833e-05 - 114s/epoch - 580ms/step
Epoch 576/1000
2023-10-11 04:59:21.906 
Epoch 576/1000 
	 loss: 16.4692, MinusLogProbMetric: 16.4692, val_loss: 16.9993, val_MinusLogProbMetric: 16.9993

Epoch 576: val_loss did not improve from 16.98106
196/196 - 113s - loss: 16.4692 - MinusLogProbMetric: 16.4692 - val_loss: 16.9993 - val_MinusLogProbMetric: 16.9993 - lr: 2.0833e-05 - 113s/epoch - 574ms/step
Epoch 577/1000
2023-10-11 05:01:10.462 
Epoch 577/1000 
	 loss: 16.4692, MinusLogProbMetric: 16.4692, val_loss: 16.9892, val_MinusLogProbMetric: 16.9892

Epoch 577: val_loss did not improve from 16.98106
196/196 - 109s - loss: 16.4692 - MinusLogProbMetric: 16.4692 - val_loss: 16.9892 - val_MinusLogProbMetric: 16.9892 - lr: 2.0833e-05 - 109s/epoch - 554ms/step
Epoch 578/1000
2023-10-11 05:03:09.233 
Epoch 578/1000 
	 loss: 16.4672, MinusLogProbMetric: 16.4672, val_loss: 17.0383, val_MinusLogProbMetric: 17.0383

Epoch 578: val_loss did not improve from 16.98106
196/196 - 119s - loss: 16.4672 - MinusLogProbMetric: 16.4672 - val_loss: 17.0383 - val_MinusLogProbMetric: 17.0383 - lr: 2.0833e-05 - 119s/epoch - 606ms/step
Epoch 579/1000
2023-10-11 05:04:55.820 
Epoch 579/1000 
	 loss: 16.4808, MinusLogProbMetric: 16.4808, val_loss: 17.0146, val_MinusLogProbMetric: 17.0146

Epoch 579: val_loss did not improve from 16.98106
196/196 - 107s - loss: 16.4808 - MinusLogProbMetric: 16.4808 - val_loss: 17.0146 - val_MinusLogProbMetric: 17.0146 - lr: 2.0833e-05 - 107s/epoch - 544ms/step
Epoch 580/1000
2023-10-11 05:06:39.478 
Epoch 580/1000 
	 loss: 16.4705, MinusLogProbMetric: 16.4705, val_loss: 16.9864, val_MinusLogProbMetric: 16.9864

Epoch 580: val_loss did not improve from 16.98106
196/196 - 104s - loss: 16.4705 - MinusLogProbMetric: 16.4705 - val_loss: 16.9864 - val_MinusLogProbMetric: 16.9864 - lr: 2.0833e-05 - 104s/epoch - 529ms/step
Epoch 581/1000
2023-10-11 05:08:36.742 
Epoch 581/1000 
	 loss: 16.4638, MinusLogProbMetric: 16.4638, val_loss: 16.9856, val_MinusLogProbMetric: 16.9856

Epoch 581: val_loss did not improve from 16.98106
196/196 - 117s - loss: 16.4638 - MinusLogProbMetric: 16.4638 - val_loss: 16.9856 - val_MinusLogProbMetric: 16.9856 - lr: 2.0833e-05 - 117s/epoch - 598ms/step
Epoch 582/1000
2023-10-11 05:10:25.014 
Epoch 582/1000 
	 loss: 16.4666, MinusLogProbMetric: 16.4666, val_loss: 17.0002, val_MinusLogProbMetric: 17.0002

Epoch 582: val_loss did not improve from 16.98106
196/196 - 108s - loss: 16.4666 - MinusLogProbMetric: 16.4666 - val_loss: 17.0002 - val_MinusLogProbMetric: 17.0002 - lr: 2.0833e-05 - 108s/epoch - 552ms/step
Epoch 583/1000
2023-10-11 05:12:20.243 
Epoch 583/1000 
	 loss: 16.4655, MinusLogProbMetric: 16.4655, val_loss: 16.9875, val_MinusLogProbMetric: 16.9875

Epoch 583: val_loss did not improve from 16.98106
196/196 - 115s - loss: 16.4655 - MinusLogProbMetric: 16.4655 - val_loss: 16.9875 - val_MinusLogProbMetric: 16.9875 - lr: 2.0833e-05 - 115s/epoch - 588ms/step
Epoch 584/1000
2023-10-11 05:14:12.023 
Epoch 584/1000 
	 loss: 16.4661, MinusLogProbMetric: 16.4661, val_loss: 17.0257, val_MinusLogProbMetric: 17.0257

Epoch 584: val_loss did not improve from 16.98106
196/196 - 112s - loss: 16.4661 - MinusLogProbMetric: 16.4661 - val_loss: 17.0257 - val_MinusLogProbMetric: 17.0257 - lr: 2.0833e-05 - 112s/epoch - 570ms/step
Epoch 585/1000
2023-10-11 05:16:02.334 
Epoch 585/1000 
	 loss: 16.4695, MinusLogProbMetric: 16.4695, val_loss: 17.0106, val_MinusLogProbMetric: 17.0106

Epoch 585: val_loss did not improve from 16.98106
196/196 - 110s - loss: 16.4695 - MinusLogProbMetric: 16.4695 - val_loss: 17.0106 - val_MinusLogProbMetric: 17.0106 - lr: 2.0833e-05 - 110s/epoch - 563ms/step
Epoch 586/1000
2023-10-11 05:17:50.987 
Epoch 586/1000 
	 loss: 16.4709, MinusLogProbMetric: 16.4709, val_loss: 16.9926, val_MinusLogProbMetric: 16.9926

Epoch 586: val_loss did not improve from 16.98106
196/196 - 109s - loss: 16.4709 - MinusLogProbMetric: 16.4709 - val_loss: 16.9926 - val_MinusLogProbMetric: 16.9926 - lr: 2.0833e-05 - 109s/epoch - 554ms/step
Epoch 587/1000
2023-10-11 05:19:42.612 
Epoch 587/1000 
	 loss: 16.4688, MinusLogProbMetric: 16.4688, val_loss: 17.0029, val_MinusLogProbMetric: 17.0029

Epoch 587: val_loss did not improve from 16.98106
196/196 - 112s - loss: 16.4688 - MinusLogProbMetric: 16.4688 - val_loss: 17.0029 - val_MinusLogProbMetric: 17.0029 - lr: 2.0833e-05 - 112s/epoch - 569ms/step
Epoch 588/1000
2023-10-11 05:21:31.831 
Epoch 588/1000 
	 loss: 16.4662, MinusLogProbMetric: 16.4662, val_loss: 16.9960, val_MinusLogProbMetric: 16.9960

Epoch 588: val_loss did not improve from 16.98106
196/196 - 109s - loss: 16.4662 - MinusLogProbMetric: 16.4662 - val_loss: 16.9960 - val_MinusLogProbMetric: 16.9960 - lr: 2.0833e-05 - 109s/epoch - 557ms/step
Epoch 589/1000
2023-10-11 05:23:20.791 
Epoch 589/1000 
	 loss: 16.4679, MinusLogProbMetric: 16.4679, val_loss: 16.9998, val_MinusLogProbMetric: 16.9998

Epoch 589: val_loss did not improve from 16.98106
196/196 - 109s - loss: 16.4679 - MinusLogProbMetric: 16.4679 - val_loss: 16.9998 - val_MinusLogProbMetric: 16.9998 - lr: 2.0833e-05 - 109s/epoch - 556ms/step
Epoch 590/1000
2023-10-11 05:25:12.080 
Epoch 590/1000 
	 loss: 16.4775, MinusLogProbMetric: 16.4775, val_loss: 16.9870, val_MinusLogProbMetric: 16.9870

Epoch 590: val_loss did not improve from 16.98106
196/196 - 111s - loss: 16.4775 - MinusLogProbMetric: 16.4775 - val_loss: 16.9870 - val_MinusLogProbMetric: 16.9870 - lr: 2.0833e-05 - 111s/epoch - 568ms/step
Epoch 591/1000
2023-10-11 05:27:05.315 
Epoch 591/1000 
	 loss: 16.4621, MinusLogProbMetric: 16.4621, val_loss: 17.0071, val_MinusLogProbMetric: 17.0071

Epoch 591: val_loss did not improve from 16.98106
196/196 - 113s - loss: 16.4621 - MinusLogProbMetric: 16.4621 - val_loss: 17.0071 - val_MinusLogProbMetric: 17.0071 - lr: 2.0833e-05 - 113s/epoch - 578ms/step
Epoch 592/1000
2023-10-11 05:28:51.042 
Epoch 592/1000 
	 loss: 16.4698, MinusLogProbMetric: 16.4698, val_loss: 16.9938, val_MinusLogProbMetric: 16.9938

Epoch 592: val_loss did not improve from 16.98106
196/196 - 106s - loss: 16.4698 - MinusLogProbMetric: 16.4698 - val_loss: 16.9938 - val_MinusLogProbMetric: 16.9938 - lr: 2.0833e-05 - 106s/epoch - 539ms/step
Epoch 593/1000
2023-10-11 05:30:33.998 
Epoch 593/1000 
	 loss: 16.4637, MinusLogProbMetric: 16.4637, val_loss: 16.9995, val_MinusLogProbMetric: 16.9995

Epoch 593: val_loss did not improve from 16.98106
196/196 - 103s - loss: 16.4637 - MinusLogProbMetric: 16.4637 - val_loss: 16.9995 - val_MinusLogProbMetric: 16.9995 - lr: 2.0833e-05 - 103s/epoch - 525ms/step
Epoch 594/1000
2023-10-11 05:32:14.939 
Epoch 594/1000 
	 loss: 16.4664, MinusLogProbMetric: 16.4664, val_loss: 16.9933, val_MinusLogProbMetric: 16.9933

Epoch 594: val_loss did not improve from 16.98106
196/196 - 101s - loss: 16.4664 - MinusLogProbMetric: 16.4664 - val_loss: 16.9933 - val_MinusLogProbMetric: 16.9933 - lr: 2.0833e-05 - 101s/epoch - 515ms/step
Epoch 595/1000
2023-10-11 05:34:00.049 
Epoch 595/1000 
	 loss: 16.4695, MinusLogProbMetric: 16.4695, val_loss: 16.9910, val_MinusLogProbMetric: 16.9910

Epoch 595: val_loss did not improve from 16.98106
196/196 - 105s - loss: 16.4695 - MinusLogProbMetric: 16.4695 - val_loss: 16.9910 - val_MinusLogProbMetric: 16.9910 - lr: 2.0833e-05 - 105s/epoch - 536ms/step
Epoch 596/1000
2023-10-11 05:35:42.972 
Epoch 596/1000 
	 loss: 16.4652, MinusLogProbMetric: 16.4652, val_loss: 16.9886, val_MinusLogProbMetric: 16.9886

Epoch 596: val_loss did not improve from 16.98106
196/196 - 103s - loss: 16.4652 - MinusLogProbMetric: 16.4652 - val_loss: 16.9886 - val_MinusLogProbMetric: 16.9886 - lr: 2.0833e-05 - 103s/epoch - 525ms/step
Epoch 597/1000
2023-10-11 05:37:27.324 
Epoch 597/1000 
	 loss: 16.4611, MinusLogProbMetric: 16.4611, val_loss: 16.9905, val_MinusLogProbMetric: 16.9905

Epoch 597: val_loss did not improve from 16.98106
196/196 - 104s - loss: 16.4611 - MinusLogProbMetric: 16.4611 - val_loss: 16.9905 - val_MinusLogProbMetric: 16.9905 - lr: 2.0833e-05 - 104s/epoch - 532ms/step
Epoch 598/1000
2023-10-11 05:39:13.778 
Epoch 598/1000 
	 loss: 16.4626, MinusLogProbMetric: 16.4626, val_loss: 16.9826, val_MinusLogProbMetric: 16.9826

Epoch 598: val_loss did not improve from 16.98106
196/196 - 106s - loss: 16.4626 - MinusLogProbMetric: 16.4626 - val_loss: 16.9826 - val_MinusLogProbMetric: 16.9826 - lr: 2.0833e-05 - 106s/epoch - 543ms/step
Epoch 599/1000
2023-10-11 05:41:01.408 
Epoch 599/1000 
	 loss: 16.4761, MinusLogProbMetric: 16.4761, val_loss: 16.9766, val_MinusLogProbMetric: 16.9766

Epoch 599: val_loss improved from 16.98106 to 16.97665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 109s - loss: 16.4761 - MinusLogProbMetric: 16.4761 - val_loss: 16.9766 - val_MinusLogProbMetric: 16.9766 - lr: 2.0833e-05 - 109s/epoch - 555ms/step
Epoch 600/1000
2023-10-11 05:42:48.073 
Epoch 600/1000 
	 loss: 16.4604, MinusLogProbMetric: 16.4604, val_loss: 17.0102, val_MinusLogProbMetric: 17.0102

Epoch 600: val_loss did not improve from 16.97665
196/196 - 105s - loss: 16.4604 - MinusLogProbMetric: 16.4604 - val_loss: 17.0102 - val_MinusLogProbMetric: 17.0102 - lr: 2.0833e-05 - 105s/epoch - 538ms/step
Epoch 601/1000
2023-10-11 05:44:30.050 
Epoch 601/1000 
	 loss: 16.4628, MinusLogProbMetric: 16.4628, val_loss: 16.9790, val_MinusLogProbMetric: 16.9790

Epoch 601: val_loss did not improve from 16.97665
196/196 - 102s - loss: 16.4628 - MinusLogProbMetric: 16.4628 - val_loss: 16.9790 - val_MinusLogProbMetric: 16.9790 - lr: 2.0833e-05 - 102s/epoch - 520ms/step
Epoch 602/1000
2023-10-11 05:46:12.424 
Epoch 602/1000 
	 loss: 16.4684, MinusLogProbMetric: 16.4684, val_loss: 17.0584, val_MinusLogProbMetric: 17.0584

Epoch 602: val_loss did not improve from 16.97665
196/196 - 102s - loss: 16.4684 - MinusLogProbMetric: 16.4684 - val_loss: 17.0584 - val_MinusLogProbMetric: 17.0584 - lr: 2.0833e-05 - 102s/epoch - 522ms/step
Epoch 603/1000
2023-10-11 05:48:06.613 
Epoch 603/1000 
	 loss: 16.4648, MinusLogProbMetric: 16.4648, val_loss: 17.0287, val_MinusLogProbMetric: 17.0287

Epoch 603: val_loss did not improve from 16.97665
196/196 - 114s - loss: 16.4648 - MinusLogProbMetric: 16.4648 - val_loss: 17.0287 - val_MinusLogProbMetric: 17.0287 - lr: 2.0833e-05 - 114s/epoch - 583ms/step
Epoch 604/1000
2023-10-11 05:49:55.321 
Epoch 604/1000 
	 loss: 16.4684, MinusLogProbMetric: 16.4684, val_loss: 17.0018, val_MinusLogProbMetric: 17.0018

Epoch 604: val_loss did not improve from 16.97665
196/196 - 109s - loss: 16.4684 - MinusLogProbMetric: 16.4684 - val_loss: 17.0018 - val_MinusLogProbMetric: 17.0018 - lr: 2.0833e-05 - 109s/epoch - 554ms/step
Epoch 605/1000
2023-10-11 05:51:42.569 
Epoch 605/1000 
	 loss: 16.4693, MinusLogProbMetric: 16.4693, val_loss: 16.9895, val_MinusLogProbMetric: 16.9895

Epoch 605: val_loss did not improve from 16.97665
196/196 - 107s - loss: 16.4693 - MinusLogProbMetric: 16.4693 - val_loss: 16.9895 - val_MinusLogProbMetric: 16.9895 - lr: 2.0833e-05 - 107s/epoch - 547ms/step
Epoch 606/1000
2023-10-11 05:53:31.372 
Epoch 606/1000 
	 loss: 16.4614, MinusLogProbMetric: 16.4614, val_loss: 16.9921, val_MinusLogProbMetric: 16.9921

Epoch 606: val_loss did not improve from 16.97665
196/196 - 109s - loss: 16.4614 - MinusLogProbMetric: 16.4614 - val_loss: 16.9921 - val_MinusLogProbMetric: 16.9921 - lr: 2.0833e-05 - 109s/epoch - 555ms/step
Epoch 607/1000
2023-10-11 05:55:18.078 
Epoch 607/1000 
	 loss: 16.4665, MinusLogProbMetric: 16.4665, val_loss: 16.9940, val_MinusLogProbMetric: 16.9940

Epoch 607: val_loss did not improve from 16.97665
196/196 - 107s - loss: 16.4665 - MinusLogProbMetric: 16.4665 - val_loss: 16.9940 - val_MinusLogProbMetric: 16.9940 - lr: 2.0833e-05 - 107s/epoch - 544ms/step
Epoch 608/1000
2023-10-11 05:57:06.806 
Epoch 608/1000 
	 loss: 16.4648, MinusLogProbMetric: 16.4648, val_loss: 17.0103, val_MinusLogProbMetric: 17.0103

Epoch 608: val_loss did not improve from 16.97665
196/196 - 109s - loss: 16.4648 - MinusLogProbMetric: 16.4648 - val_loss: 17.0103 - val_MinusLogProbMetric: 17.0103 - lr: 2.0833e-05 - 109s/epoch - 555ms/step
Epoch 609/1000
2023-10-11 05:58:57.809 
Epoch 609/1000 
	 loss: 16.4616, MinusLogProbMetric: 16.4616, val_loss: 16.9852, val_MinusLogProbMetric: 16.9852

Epoch 609: val_loss did not improve from 16.97665
196/196 - 111s - loss: 16.4616 - MinusLogProbMetric: 16.4616 - val_loss: 16.9852 - val_MinusLogProbMetric: 16.9852 - lr: 2.0833e-05 - 111s/epoch - 566ms/step
Epoch 610/1000
2023-10-11 06:00:45.174 
Epoch 610/1000 
	 loss: 16.4637, MinusLogProbMetric: 16.4637, val_loss: 17.0159, val_MinusLogProbMetric: 17.0159

Epoch 610: val_loss did not improve from 16.97665
196/196 - 107s - loss: 16.4637 - MinusLogProbMetric: 16.4637 - val_loss: 17.0159 - val_MinusLogProbMetric: 17.0159 - lr: 2.0833e-05 - 107s/epoch - 548ms/step
Epoch 611/1000
2023-10-11 06:02:31.555 
Epoch 611/1000 
	 loss: 16.4619, MinusLogProbMetric: 16.4619, val_loss: 17.0045, val_MinusLogProbMetric: 17.0045

Epoch 611: val_loss did not improve from 16.97665
196/196 - 106s - loss: 16.4619 - MinusLogProbMetric: 16.4619 - val_loss: 17.0045 - val_MinusLogProbMetric: 17.0045 - lr: 2.0833e-05 - 106s/epoch - 543ms/step
Epoch 612/1000
2023-10-11 06:04:21.984 
Epoch 612/1000 
	 loss: 16.4697, MinusLogProbMetric: 16.4697, val_loss: 17.0104, val_MinusLogProbMetric: 17.0104

Epoch 612: val_loss did not improve from 16.97665
196/196 - 110s - loss: 16.4697 - MinusLogProbMetric: 16.4697 - val_loss: 17.0104 - val_MinusLogProbMetric: 17.0104 - lr: 2.0833e-05 - 110s/epoch - 564ms/step
Epoch 613/1000
2023-10-11 06:06:08.304 
Epoch 613/1000 
	 loss: 16.4603, MinusLogProbMetric: 16.4603, val_loss: 16.9987, val_MinusLogProbMetric: 16.9987

Epoch 613: val_loss did not improve from 16.97665
196/196 - 106s - loss: 16.4603 - MinusLogProbMetric: 16.4603 - val_loss: 16.9987 - val_MinusLogProbMetric: 16.9987 - lr: 2.0833e-05 - 106s/epoch - 542ms/step
Epoch 614/1000
2023-10-11 06:08:05.130 
Epoch 614/1000 
	 loss: 16.4671, MinusLogProbMetric: 16.4671, val_loss: 16.9899, val_MinusLogProbMetric: 16.9899

Epoch 614: val_loss did not improve from 16.97665
196/196 - 117s - loss: 16.4671 - MinusLogProbMetric: 16.4671 - val_loss: 16.9899 - val_MinusLogProbMetric: 16.9899 - lr: 2.0833e-05 - 117s/epoch - 596ms/step
Epoch 615/1000
2023-10-11 06:10:02.710 
Epoch 615/1000 
	 loss: 16.4631, MinusLogProbMetric: 16.4631, val_loss: 17.0150, val_MinusLogProbMetric: 17.0150

Epoch 615: val_loss did not improve from 16.97665
196/196 - 118s - loss: 16.4631 - MinusLogProbMetric: 16.4631 - val_loss: 17.0150 - val_MinusLogProbMetric: 17.0150 - lr: 2.0833e-05 - 118s/epoch - 600ms/step
Epoch 616/1000
2023-10-11 06:11:55.371 
Epoch 616/1000 
	 loss: 16.4667, MinusLogProbMetric: 16.4667, val_loss: 16.9905, val_MinusLogProbMetric: 16.9905

Epoch 616: val_loss did not improve from 16.97665
196/196 - 113s - loss: 16.4667 - MinusLogProbMetric: 16.4667 - val_loss: 16.9905 - val_MinusLogProbMetric: 16.9905 - lr: 2.0833e-05 - 113s/epoch - 575ms/step
Epoch 617/1000
2023-10-11 06:13:41.078 
Epoch 617/1000 
	 loss: 16.4642, MinusLogProbMetric: 16.4642, val_loss: 17.0009, val_MinusLogProbMetric: 17.0009

Epoch 617: val_loss did not improve from 16.97665
196/196 - 106s - loss: 16.4642 - MinusLogProbMetric: 16.4642 - val_loss: 17.0009 - val_MinusLogProbMetric: 17.0009 - lr: 2.0833e-05 - 106s/epoch - 539ms/step
Epoch 618/1000
2023-10-11 06:15:33.598 
Epoch 618/1000 
	 loss: 16.4594, MinusLogProbMetric: 16.4594, val_loss: 16.9998, val_MinusLogProbMetric: 16.9998

Epoch 618: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4594 - MinusLogProbMetric: 16.4594 - val_loss: 16.9998 - val_MinusLogProbMetric: 16.9998 - lr: 2.0833e-05 - 112s/epoch - 574ms/step
Epoch 619/1000
2023-10-11 06:17:20.522 
Epoch 619/1000 
	 loss: 16.4633, MinusLogProbMetric: 16.4633, val_loss: 16.9915, val_MinusLogProbMetric: 16.9915

Epoch 619: val_loss did not improve from 16.97665
196/196 - 107s - loss: 16.4633 - MinusLogProbMetric: 16.4633 - val_loss: 16.9915 - val_MinusLogProbMetric: 16.9915 - lr: 2.0833e-05 - 107s/epoch - 546ms/step
Epoch 620/1000
2023-10-11 06:19:16.863 
Epoch 620/1000 
	 loss: 16.4638, MinusLogProbMetric: 16.4638, val_loss: 16.9967, val_MinusLogProbMetric: 16.9967

Epoch 620: val_loss did not improve from 16.97665
196/196 - 116s - loss: 16.4638 - MinusLogProbMetric: 16.4638 - val_loss: 16.9967 - val_MinusLogProbMetric: 16.9967 - lr: 2.0833e-05 - 116s/epoch - 593ms/step
Epoch 621/1000
2023-10-11 06:21:06.475 
Epoch 621/1000 
	 loss: 16.4626, MinusLogProbMetric: 16.4626, val_loss: 16.9782, val_MinusLogProbMetric: 16.9782

Epoch 621: val_loss did not improve from 16.97665
196/196 - 110s - loss: 16.4626 - MinusLogProbMetric: 16.4626 - val_loss: 16.9782 - val_MinusLogProbMetric: 16.9782 - lr: 2.0833e-05 - 110s/epoch - 559ms/step
Epoch 622/1000
2023-10-11 06:22:57.451 
Epoch 622/1000 
	 loss: 16.4598, MinusLogProbMetric: 16.4598, val_loss: 16.9946, val_MinusLogProbMetric: 16.9946

Epoch 622: val_loss did not improve from 16.97665
196/196 - 111s - loss: 16.4598 - MinusLogProbMetric: 16.4598 - val_loss: 16.9946 - val_MinusLogProbMetric: 16.9946 - lr: 2.0833e-05 - 111s/epoch - 566ms/step
Epoch 623/1000
2023-10-11 06:24:46.031 
Epoch 623/1000 
	 loss: 16.4584, MinusLogProbMetric: 16.4584, val_loss: 17.0122, val_MinusLogProbMetric: 17.0122

Epoch 623: val_loss did not improve from 16.97665
196/196 - 109s - loss: 16.4584 - MinusLogProbMetric: 16.4584 - val_loss: 17.0122 - val_MinusLogProbMetric: 17.0122 - lr: 2.0833e-05 - 109s/epoch - 554ms/step
Epoch 624/1000
2023-10-11 06:26:44.661 
Epoch 624/1000 
	 loss: 16.4601, MinusLogProbMetric: 16.4601, val_loss: 16.9846, val_MinusLogProbMetric: 16.9846

Epoch 624: val_loss did not improve from 16.97665
196/196 - 119s - loss: 16.4601 - MinusLogProbMetric: 16.4601 - val_loss: 16.9846 - val_MinusLogProbMetric: 16.9846 - lr: 2.0833e-05 - 119s/epoch - 605ms/step
Epoch 625/1000
2023-10-11 06:28:36.688 
Epoch 625/1000 
	 loss: 16.4587, MinusLogProbMetric: 16.4587, val_loss: 16.9876, val_MinusLogProbMetric: 16.9876

Epoch 625: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4587 - MinusLogProbMetric: 16.4587 - val_loss: 16.9876 - val_MinusLogProbMetric: 16.9876 - lr: 2.0833e-05 - 112s/epoch - 571ms/step
Epoch 626/1000
2023-10-11 06:30:27.330 
Epoch 626/1000 
	 loss: 16.4554, MinusLogProbMetric: 16.4554, val_loss: 16.9891, val_MinusLogProbMetric: 16.9891

Epoch 626: val_loss did not improve from 16.97665
196/196 - 111s - loss: 16.4554 - MinusLogProbMetric: 16.4554 - val_loss: 16.9891 - val_MinusLogProbMetric: 16.9891 - lr: 2.0833e-05 - 111s/epoch - 565ms/step
Epoch 627/1000
2023-10-11 06:32:15.061 
Epoch 627/1000 
	 loss: 16.4563, MinusLogProbMetric: 16.4563, val_loss: 16.9990, val_MinusLogProbMetric: 16.9990

Epoch 627: val_loss did not improve from 16.97665
196/196 - 108s - loss: 16.4563 - MinusLogProbMetric: 16.4563 - val_loss: 16.9990 - val_MinusLogProbMetric: 16.9990 - lr: 2.0833e-05 - 108s/epoch - 550ms/step
Epoch 628/1000
2023-10-11 06:34:06.420 
Epoch 628/1000 
	 loss: 16.4572, MinusLogProbMetric: 16.4572, val_loss: 16.9929, val_MinusLogProbMetric: 16.9929

Epoch 628: val_loss did not improve from 16.97665
196/196 - 111s - loss: 16.4572 - MinusLogProbMetric: 16.4572 - val_loss: 16.9929 - val_MinusLogProbMetric: 16.9929 - lr: 2.0833e-05 - 111s/epoch - 568ms/step
Epoch 629/1000
2023-10-11 06:35:51.993 
Epoch 629/1000 
	 loss: 16.4591, MinusLogProbMetric: 16.4591, val_loss: 16.9996, val_MinusLogProbMetric: 16.9996

Epoch 629: val_loss did not improve from 16.97665
196/196 - 106s - loss: 16.4591 - MinusLogProbMetric: 16.4591 - val_loss: 16.9996 - val_MinusLogProbMetric: 16.9996 - lr: 2.0833e-05 - 106s/epoch - 539ms/step
Epoch 630/1000
2023-10-11 06:37:43.480 
Epoch 630/1000 
	 loss: 16.4653, MinusLogProbMetric: 16.4653, val_loss: 17.0037, val_MinusLogProbMetric: 17.0037

Epoch 630: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4653 - MinusLogProbMetric: 16.4653 - val_loss: 17.0037 - val_MinusLogProbMetric: 17.0037 - lr: 2.0833e-05 - 112s/epoch - 569ms/step
Epoch 631/1000
2023-10-11 06:39:28.726 
Epoch 631/1000 
	 loss: 16.4615, MinusLogProbMetric: 16.4615, val_loss: 16.9786, val_MinusLogProbMetric: 16.9786

Epoch 631: val_loss did not improve from 16.97665
196/196 - 105s - loss: 16.4615 - MinusLogProbMetric: 16.4615 - val_loss: 16.9786 - val_MinusLogProbMetric: 16.9786 - lr: 2.0833e-05 - 105s/epoch - 537ms/step
Epoch 632/1000
2023-10-11 06:41:21.337 
Epoch 632/1000 
	 loss: 16.4637, MinusLogProbMetric: 16.4637, val_loss: 17.0323, val_MinusLogProbMetric: 17.0323

Epoch 632: val_loss did not improve from 16.97665
196/196 - 113s - loss: 16.4637 - MinusLogProbMetric: 16.4637 - val_loss: 17.0323 - val_MinusLogProbMetric: 17.0323 - lr: 2.0833e-05 - 113s/epoch - 575ms/step
Epoch 633/1000
2023-10-11 06:43:16.769 
Epoch 633/1000 
	 loss: 16.4659, MinusLogProbMetric: 16.4659, val_loss: 16.9905, val_MinusLogProbMetric: 16.9905

Epoch 633: val_loss did not improve from 16.97665
196/196 - 115s - loss: 16.4659 - MinusLogProbMetric: 16.4659 - val_loss: 16.9905 - val_MinusLogProbMetric: 16.9905 - lr: 2.0833e-05 - 115s/epoch - 589ms/step
Epoch 634/1000
2023-10-11 06:45:09.089 
Epoch 634/1000 
	 loss: 16.4628, MinusLogProbMetric: 16.4628, val_loss: 17.0031, val_MinusLogProbMetric: 17.0031

Epoch 634: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4628 - MinusLogProbMetric: 16.4628 - val_loss: 17.0031 - val_MinusLogProbMetric: 17.0031 - lr: 2.0833e-05 - 112s/epoch - 573ms/step
Epoch 635/1000
2023-10-11 06:46:53.718 
Epoch 635/1000 
	 loss: 16.4680, MinusLogProbMetric: 16.4680, val_loss: 17.0281, val_MinusLogProbMetric: 17.0281

Epoch 635: val_loss did not improve from 16.97665
196/196 - 105s - loss: 16.4680 - MinusLogProbMetric: 16.4680 - val_loss: 17.0281 - val_MinusLogProbMetric: 17.0281 - lr: 2.0833e-05 - 105s/epoch - 534ms/step
Epoch 636/1000
2023-10-11 06:48:45.484 
Epoch 636/1000 
	 loss: 16.4677, MinusLogProbMetric: 16.4677, val_loss: 17.0270, val_MinusLogProbMetric: 17.0270

Epoch 636: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4677 - MinusLogProbMetric: 16.4677 - val_loss: 17.0270 - val_MinusLogProbMetric: 17.0270 - lr: 2.0833e-05 - 112s/epoch - 570ms/step
Epoch 637/1000
2023-10-11 06:50:37.952 
Epoch 637/1000 
	 loss: 16.4676, MinusLogProbMetric: 16.4676, val_loss: 16.9861, val_MinusLogProbMetric: 16.9861

Epoch 637: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4676 - MinusLogProbMetric: 16.4676 - val_loss: 16.9861 - val_MinusLogProbMetric: 16.9861 - lr: 2.0833e-05 - 112s/epoch - 574ms/step
Epoch 638/1000
2023-10-11 06:52:28.569 
Epoch 638/1000 
	 loss: 16.4618, MinusLogProbMetric: 16.4618, val_loss: 16.9976, val_MinusLogProbMetric: 16.9976

Epoch 638: val_loss did not improve from 16.97665
196/196 - 111s - loss: 16.4618 - MinusLogProbMetric: 16.4618 - val_loss: 16.9976 - val_MinusLogProbMetric: 16.9976 - lr: 2.0833e-05 - 111s/epoch - 564ms/step
Epoch 639/1000
2023-10-11 06:54:12.595 
Epoch 639/1000 
	 loss: 16.4576, MinusLogProbMetric: 16.4576, val_loss: 17.0066, val_MinusLogProbMetric: 17.0066

Epoch 639: val_loss did not improve from 16.97665
196/196 - 104s - loss: 16.4576 - MinusLogProbMetric: 16.4576 - val_loss: 17.0066 - val_MinusLogProbMetric: 17.0066 - lr: 2.0833e-05 - 104s/epoch - 531ms/step
Epoch 640/1000
2023-10-11 06:56:03.880 
Epoch 640/1000 
	 loss: 16.4641, MinusLogProbMetric: 16.4641, val_loss: 16.9845, val_MinusLogProbMetric: 16.9845

Epoch 640: val_loss did not improve from 16.97665
196/196 - 111s - loss: 16.4641 - MinusLogProbMetric: 16.4641 - val_loss: 16.9845 - val_MinusLogProbMetric: 16.9845 - lr: 2.0833e-05 - 111s/epoch - 568ms/step
Epoch 641/1000
2023-10-11 06:57:50.739 
Epoch 641/1000 
	 loss: 16.4638, MinusLogProbMetric: 16.4638, val_loss: 16.9817, val_MinusLogProbMetric: 16.9817

Epoch 641: val_loss did not improve from 16.97665
196/196 - 107s - loss: 16.4638 - MinusLogProbMetric: 16.4638 - val_loss: 16.9817 - val_MinusLogProbMetric: 16.9817 - lr: 2.0833e-05 - 107s/epoch - 545ms/step
Epoch 642/1000
2023-10-11 06:59:40.403 
Epoch 642/1000 
	 loss: 16.4566, MinusLogProbMetric: 16.4566, val_loss: 16.9897, val_MinusLogProbMetric: 16.9897

Epoch 642: val_loss did not improve from 16.97665
196/196 - 110s - loss: 16.4566 - MinusLogProbMetric: 16.4566 - val_loss: 16.9897 - val_MinusLogProbMetric: 16.9897 - lr: 2.0833e-05 - 110s/epoch - 560ms/step
Epoch 643/1000
2023-10-11 07:01:25.563 
Epoch 643/1000 
	 loss: 16.4624, MinusLogProbMetric: 16.4624, val_loss: 17.0087, val_MinusLogProbMetric: 17.0087

Epoch 643: val_loss did not improve from 16.97665
196/196 - 105s - loss: 16.4624 - MinusLogProbMetric: 16.4624 - val_loss: 17.0087 - val_MinusLogProbMetric: 17.0087 - lr: 2.0833e-05 - 105s/epoch - 536ms/step
Epoch 644/1000
2023-10-11 07:03:10.358 
Epoch 644/1000 
	 loss: 16.4610, MinusLogProbMetric: 16.4610, val_loss: 17.0739, val_MinusLogProbMetric: 17.0739

Epoch 644: val_loss did not improve from 16.97665
196/196 - 105s - loss: 16.4610 - MinusLogProbMetric: 16.4610 - val_loss: 17.0739 - val_MinusLogProbMetric: 17.0739 - lr: 2.0833e-05 - 105s/epoch - 535ms/step
Epoch 645/1000
2023-10-11 07:05:06.867 
Epoch 645/1000 
	 loss: 16.4636, MinusLogProbMetric: 16.4636, val_loss: 17.0759, val_MinusLogProbMetric: 17.0759

Epoch 645: val_loss did not improve from 16.97665
196/196 - 116s - loss: 16.4636 - MinusLogProbMetric: 16.4636 - val_loss: 17.0759 - val_MinusLogProbMetric: 17.0759 - lr: 2.0833e-05 - 116s/epoch - 594ms/step
Epoch 646/1000
2023-10-11 07:06:58.677 
Epoch 646/1000 
	 loss: 16.4658, MinusLogProbMetric: 16.4658, val_loss: 16.9896, val_MinusLogProbMetric: 16.9896

Epoch 646: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4658 - MinusLogProbMetric: 16.4658 - val_loss: 16.9896 - val_MinusLogProbMetric: 16.9896 - lr: 2.0833e-05 - 112s/epoch - 571ms/step
Epoch 647/1000
2023-10-11 07:08:45.948 
Epoch 647/1000 
	 loss: 16.4651, MinusLogProbMetric: 16.4651, val_loss: 17.0145, val_MinusLogProbMetric: 17.0145

Epoch 647: val_loss did not improve from 16.97665
196/196 - 107s - loss: 16.4651 - MinusLogProbMetric: 16.4651 - val_loss: 17.0145 - val_MinusLogProbMetric: 17.0145 - lr: 2.0833e-05 - 107s/epoch - 547ms/step
Epoch 648/1000
2023-10-11 07:10:38.102 
Epoch 648/1000 
	 loss: 16.4568, MinusLogProbMetric: 16.4568, val_loss: 17.0194, val_MinusLogProbMetric: 17.0194

Epoch 648: val_loss did not improve from 16.97665
196/196 - 112s - loss: 16.4568 - MinusLogProbMetric: 16.4568 - val_loss: 17.0194 - val_MinusLogProbMetric: 17.0194 - lr: 2.0833e-05 - 112s/epoch - 572ms/step
Epoch 649/1000
2023-10-11 07:12:27.960 
Epoch 649/1000 
	 loss: 16.4608, MinusLogProbMetric: 16.4608, val_loss: 16.9771, val_MinusLogProbMetric: 16.9771

Epoch 649: val_loss did not improve from 16.97665
196/196 - 110s - loss: 16.4608 - MinusLogProbMetric: 16.4608 - val_loss: 16.9771 - val_MinusLogProbMetric: 16.9771 - lr: 2.0833e-05 - 110s/epoch - 561ms/step
Epoch 650/1000
2023-10-11 07:14:19.159 
Epoch 650/1000 
	 loss: 16.4389, MinusLogProbMetric: 16.4389, val_loss: 16.9724, val_MinusLogProbMetric: 16.9724

Epoch 650: val_loss improved from 16.97665 to 16.97237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 112s - loss: 16.4389 - MinusLogProbMetric: 16.4389 - val_loss: 16.9724 - val_MinusLogProbMetric: 16.9724 - lr: 1.0417e-05 - 112s/epoch - 570ms/step
Epoch 651/1000
2023-10-11 07:16:12.842 
Epoch 651/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 16.9685, val_MinusLogProbMetric: 16.9685

Epoch 651: val_loss improved from 16.97237 to 16.96847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 115s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 16.9685 - val_MinusLogProbMetric: 16.9685 - lr: 1.0417e-05 - 115s/epoch - 585ms/step
Epoch 652/1000
2023-10-11 07:18:03.175 
Epoch 652/1000 
	 loss: 16.4387, MinusLogProbMetric: 16.4387, val_loss: 16.9794, val_MinusLogProbMetric: 16.9794

Epoch 652: val_loss did not improve from 16.96847
196/196 - 109s - loss: 16.4387 - MinusLogProbMetric: 16.4387 - val_loss: 16.9794 - val_MinusLogProbMetric: 16.9794 - lr: 1.0417e-05 - 109s/epoch - 555ms/step
Epoch 653/1000
2023-10-11 07:19:51.307 
Epoch 653/1000 
	 loss: 16.4405, MinusLogProbMetric: 16.4405, val_loss: 16.9751, val_MinusLogProbMetric: 16.9751

Epoch 653: val_loss did not improve from 16.96847
196/196 - 108s - loss: 16.4405 - MinusLogProbMetric: 16.4405 - val_loss: 16.9751 - val_MinusLogProbMetric: 16.9751 - lr: 1.0417e-05 - 108s/epoch - 552ms/step
Epoch 654/1000
2023-10-11 07:21:40.332 
Epoch 654/1000 
	 loss: 16.4395, MinusLogProbMetric: 16.4395, val_loss: 16.9682, val_MinusLogProbMetric: 16.9682

Epoch 654: val_loss improved from 16.96847 to 16.96825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 111s - loss: 16.4395 - MinusLogProbMetric: 16.4395 - val_loss: 16.9682 - val_MinusLogProbMetric: 16.9682 - lr: 1.0417e-05 - 111s/epoch - 569ms/step
Epoch 655/1000
2023-10-11 07:23:29.837 
Epoch 655/1000 
	 loss: 16.4376, MinusLogProbMetric: 16.4376, val_loss: 16.9882, val_MinusLogProbMetric: 16.9882

Epoch 655: val_loss did not improve from 16.96825
196/196 - 107s - loss: 16.4376 - MinusLogProbMetric: 16.4376 - val_loss: 16.9882 - val_MinusLogProbMetric: 16.9882 - lr: 1.0417e-05 - 107s/epoch - 546ms/step
Epoch 656/1000
2023-10-11 07:25:19.895 
Epoch 656/1000 
	 loss: 16.4389, MinusLogProbMetric: 16.4389, val_loss: 16.9791, val_MinusLogProbMetric: 16.9791

Epoch 656: val_loss did not improve from 16.96825
196/196 - 110s - loss: 16.4389 - MinusLogProbMetric: 16.4389 - val_loss: 16.9791 - val_MinusLogProbMetric: 16.9791 - lr: 1.0417e-05 - 110s/epoch - 561ms/step
Epoch 657/1000
2023-10-11 07:27:10.198 
Epoch 657/1000 
	 loss: 16.4378, MinusLogProbMetric: 16.4378, val_loss: 16.9715, val_MinusLogProbMetric: 16.9715

Epoch 657: val_loss did not improve from 16.96825
196/196 - 110s - loss: 16.4378 - MinusLogProbMetric: 16.4378 - val_loss: 16.9715 - val_MinusLogProbMetric: 16.9715 - lr: 1.0417e-05 - 110s/epoch - 563ms/step
Epoch 658/1000
2023-10-11 07:28:59.363 
Epoch 658/1000 
	 loss: 16.4362, MinusLogProbMetric: 16.4362, val_loss: 16.9809, val_MinusLogProbMetric: 16.9809

Epoch 658: val_loss did not improve from 16.96825
196/196 - 109s - loss: 16.4362 - MinusLogProbMetric: 16.4362 - val_loss: 16.9809 - val_MinusLogProbMetric: 16.9809 - lr: 1.0417e-05 - 109s/epoch - 557ms/step
Epoch 659/1000
2023-10-11 07:30:49.659 
Epoch 659/1000 
	 loss: 16.4368, MinusLogProbMetric: 16.4368, val_loss: 16.9743, val_MinusLogProbMetric: 16.9743

Epoch 659: val_loss did not improve from 16.96825
196/196 - 110s - loss: 16.4368 - MinusLogProbMetric: 16.4368 - val_loss: 16.9743 - val_MinusLogProbMetric: 16.9743 - lr: 1.0417e-05 - 110s/epoch - 563ms/step
Epoch 660/1000
2023-10-11 07:32:41.290 
Epoch 660/1000 
	 loss: 16.4387, MinusLogProbMetric: 16.4387, val_loss: 16.9719, val_MinusLogProbMetric: 16.9719

Epoch 660: val_loss did not improve from 16.96825
196/196 - 112s - loss: 16.4387 - MinusLogProbMetric: 16.4387 - val_loss: 16.9719 - val_MinusLogProbMetric: 16.9719 - lr: 1.0417e-05 - 112s/epoch - 570ms/step
Epoch 661/1000
2023-10-11 07:34:28.127 
Epoch 661/1000 
	 loss: 16.4393, MinusLogProbMetric: 16.4393, val_loss: 16.9783, val_MinusLogProbMetric: 16.9783

Epoch 661: val_loss did not improve from 16.96825
196/196 - 107s - loss: 16.4393 - MinusLogProbMetric: 16.4393 - val_loss: 16.9783 - val_MinusLogProbMetric: 16.9783 - lr: 1.0417e-05 - 107s/epoch - 545ms/step
Epoch 662/1000
2023-10-11 07:36:18.954 
Epoch 662/1000 
	 loss: 16.4372, MinusLogProbMetric: 16.4372, val_loss: 16.9916, val_MinusLogProbMetric: 16.9916

Epoch 662: val_loss did not improve from 16.96825
196/196 - 111s - loss: 16.4372 - MinusLogProbMetric: 16.4372 - val_loss: 16.9916 - val_MinusLogProbMetric: 16.9916 - lr: 1.0417e-05 - 111s/epoch - 565ms/step
Epoch 663/1000
2023-10-11 07:38:07.129 
Epoch 663/1000 
	 loss: 16.4371, MinusLogProbMetric: 16.4371, val_loss: 16.9713, val_MinusLogProbMetric: 16.9713

Epoch 663: val_loss did not improve from 16.96825
196/196 - 108s - loss: 16.4371 - MinusLogProbMetric: 16.4371 - val_loss: 16.9713 - val_MinusLogProbMetric: 16.9713 - lr: 1.0417e-05 - 108s/epoch - 552ms/step
Epoch 664/1000
2023-10-11 07:40:00.675 
Epoch 664/1000 
	 loss: 16.4350, MinusLogProbMetric: 16.4350, val_loss: 16.9754, val_MinusLogProbMetric: 16.9754

Epoch 664: val_loss did not improve from 16.96825
196/196 - 114s - loss: 16.4350 - MinusLogProbMetric: 16.4350 - val_loss: 16.9754 - val_MinusLogProbMetric: 16.9754 - lr: 1.0417e-05 - 114s/epoch - 579ms/step
Epoch 665/1000
2023-10-11 07:41:47.679 
Epoch 665/1000 
	 loss: 16.4366, MinusLogProbMetric: 16.4366, val_loss: 16.9677, val_MinusLogProbMetric: 16.9677

Epoch 665: val_loss improved from 16.96825 to 16.96770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 110s - loss: 16.4366 - MinusLogProbMetric: 16.4366 - val_loss: 16.9677 - val_MinusLogProbMetric: 16.9677 - lr: 1.0417e-05 - 110s/epoch - 559ms/step
Epoch 666/1000
2023-10-11 07:43:39.865 
Epoch 666/1000 
	 loss: 16.4358, MinusLogProbMetric: 16.4358, val_loss: 16.9771, val_MinusLogProbMetric: 16.9771

Epoch 666: val_loss did not improve from 16.96770
196/196 - 110s - loss: 16.4358 - MinusLogProbMetric: 16.4358 - val_loss: 16.9771 - val_MinusLogProbMetric: 16.9771 - lr: 1.0417e-05 - 110s/epoch - 559ms/step
Epoch 667/1000
2023-10-11 07:45:27.658 
Epoch 667/1000 
	 loss: 16.4356, MinusLogProbMetric: 16.4356, val_loss: 16.9730, val_MinusLogProbMetric: 16.9730

Epoch 667: val_loss did not improve from 16.96770
196/196 - 108s - loss: 16.4356 - MinusLogProbMetric: 16.4356 - val_loss: 16.9730 - val_MinusLogProbMetric: 16.9730 - lr: 1.0417e-05 - 108s/epoch - 550ms/step
Epoch 668/1000
2023-10-11 07:47:21.043 
Epoch 668/1000 
	 loss: 16.4391, MinusLogProbMetric: 16.4391, val_loss: 16.9730, val_MinusLogProbMetric: 16.9730

Epoch 668: val_loss did not improve from 16.96770
196/196 - 113s - loss: 16.4391 - MinusLogProbMetric: 16.4391 - val_loss: 16.9730 - val_MinusLogProbMetric: 16.9730 - lr: 1.0417e-05 - 113s/epoch - 578ms/step
Epoch 669/1000
2023-10-11 07:49:12.870 
Epoch 669/1000 
	 loss: 16.4374, MinusLogProbMetric: 16.4374, val_loss: 16.9715, val_MinusLogProbMetric: 16.9715

Epoch 669: val_loss did not improve from 16.96770
196/196 - 112s - loss: 16.4374 - MinusLogProbMetric: 16.4374 - val_loss: 16.9715 - val_MinusLogProbMetric: 16.9715 - lr: 1.0417e-05 - 112s/epoch - 571ms/step
Epoch 670/1000
2023-10-11 07:51:03.071 
Epoch 670/1000 
	 loss: 16.4364, MinusLogProbMetric: 16.4364, val_loss: 16.9746, val_MinusLogProbMetric: 16.9746

Epoch 670: val_loss did not improve from 16.96770
196/196 - 110s - loss: 16.4364 - MinusLogProbMetric: 16.4364 - val_loss: 16.9746 - val_MinusLogProbMetric: 16.9746 - lr: 1.0417e-05 - 110s/epoch - 562ms/step
Epoch 671/1000
2023-10-11 07:52:51.032 
Epoch 671/1000 
	 loss: 16.4387, MinusLogProbMetric: 16.4387, val_loss: 16.9820, val_MinusLogProbMetric: 16.9820

Epoch 671: val_loss did not improve from 16.96770
196/196 - 108s - loss: 16.4387 - MinusLogProbMetric: 16.4387 - val_loss: 16.9820 - val_MinusLogProbMetric: 16.9820 - lr: 1.0417e-05 - 108s/epoch - 551ms/step
Epoch 672/1000
2023-10-11 07:54:36.697 
Epoch 672/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 16.9909, val_MinusLogProbMetric: 16.9909

Epoch 672: val_loss did not improve from 16.96770
196/196 - 106s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 16.9909 - val_MinusLogProbMetric: 16.9909 - lr: 1.0417e-05 - 106s/epoch - 539ms/step
Epoch 673/1000
2023-10-11 07:56:26.287 
Epoch 673/1000 
	 loss: 16.4391, MinusLogProbMetric: 16.4391, val_loss: 16.9699, val_MinusLogProbMetric: 16.9699

Epoch 673: val_loss did not improve from 16.96770
196/196 - 110s - loss: 16.4391 - MinusLogProbMetric: 16.4391 - val_loss: 16.9699 - val_MinusLogProbMetric: 16.9699 - lr: 1.0417e-05 - 110s/epoch - 559ms/step
Epoch 674/1000
2023-10-11 07:58:16.948 
Epoch 674/1000 
	 loss: 16.4371, MinusLogProbMetric: 16.4371, val_loss: 16.9718, val_MinusLogProbMetric: 16.9718

Epoch 674: val_loss did not improve from 16.96770
196/196 - 111s - loss: 16.4371 - MinusLogProbMetric: 16.4371 - val_loss: 16.9718 - val_MinusLogProbMetric: 16.9718 - lr: 1.0417e-05 - 111s/epoch - 565ms/step
Epoch 675/1000
2023-10-11 08:00:02.031 
Epoch 675/1000 
	 loss: 16.4384, MinusLogProbMetric: 16.4384, val_loss: 16.9780, val_MinusLogProbMetric: 16.9780

Epoch 675: val_loss did not improve from 16.96770
196/196 - 105s - loss: 16.4384 - MinusLogProbMetric: 16.4384 - val_loss: 16.9780 - val_MinusLogProbMetric: 16.9780 - lr: 1.0417e-05 - 105s/epoch - 536ms/step
Epoch 676/1000
2023-10-11 08:01:52.231 
Epoch 676/1000 
	 loss: 16.4353, MinusLogProbMetric: 16.4353, val_loss: 16.9757, val_MinusLogProbMetric: 16.9757

Epoch 676: val_loss did not improve from 16.96770
196/196 - 110s - loss: 16.4353 - MinusLogProbMetric: 16.4353 - val_loss: 16.9757 - val_MinusLogProbMetric: 16.9757 - lr: 1.0417e-05 - 110s/epoch - 562ms/step
Epoch 677/1000
2023-10-11 08:03:43.008 
Epoch 677/1000 
	 loss: 16.4371, MinusLogProbMetric: 16.4371, val_loss: 16.9665, val_MinusLogProbMetric: 16.9665

Epoch 677: val_loss improved from 16.96770 to 16.96646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 112s - loss: 16.4371 - MinusLogProbMetric: 16.4371 - val_loss: 16.9665 - val_MinusLogProbMetric: 16.9665 - lr: 1.0417e-05 - 112s/epoch - 573ms/step
Epoch 678/1000
2023-10-11 08:05:28.672 
Epoch 678/1000 
	 loss: 16.4343, MinusLogProbMetric: 16.4343, val_loss: 16.9829, val_MinusLogProbMetric: 16.9829

Epoch 678: val_loss did not improve from 16.96646
196/196 - 104s - loss: 16.4343 - MinusLogProbMetric: 16.4343 - val_loss: 16.9829 - val_MinusLogProbMetric: 16.9829 - lr: 1.0417e-05 - 104s/epoch - 531ms/step
Epoch 679/1000
2023-10-11 08:07:13.144 
Epoch 679/1000 
	 loss: 16.4336, MinusLogProbMetric: 16.4336, val_loss: 16.9745, val_MinusLogProbMetric: 16.9745

Epoch 679: val_loss did not improve from 16.96646
196/196 - 104s - loss: 16.4336 - MinusLogProbMetric: 16.4336 - val_loss: 16.9745 - val_MinusLogProbMetric: 16.9745 - lr: 1.0417e-05 - 104s/epoch - 533ms/step
Epoch 680/1000
2023-10-11 08:08:56.033 
Epoch 680/1000 
	 loss: 16.4357, MinusLogProbMetric: 16.4357, val_loss: 16.9688, val_MinusLogProbMetric: 16.9688

Epoch 680: val_loss did not improve from 16.96646
196/196 - 103s - loss: 16.4357 - MinusLogProbMetric: 16.4357 - val_loss: 16.9688 - val_MinusLogProbMetric: 16.9688 - lr: 1.0417e-05 - 103s/epoch - 525ms/step
Epoch 681/1000
2023-10-11 08:10:43.513 
Epoch 681/1000 
	 loss: 16.4368, MinusLogProbMetric: 16.4368, val_loss: 16.9686, val_MinusLogProbMetric: 16.9686

Epoch 681: val_loss did not improve from 16.96646
196/196 - 107s - loss: 16.4368 - MinusLogProbMetric: 16.4368 - val_loss: 16.9686 - val_MinusLogProbMetric: 16.9686 - lr: 1.0417e-05 - 107s/epoch - 548ms/step
Epoch 682/1000
2023-10-11 08:12:30.847 
Epoch 682/1000 
	 loss: 16.4370, MinusLogProbMetric: 16.4370, val_loss: 16.9748, val_MinusLogProbMetric: 16.9748

Epoch 682: val_loss did not improve from 16.96646
196/196 - 107s - loss: 16.4370 - MinusLogProbMetric: 16.4370 - val_loss: 16.9748 - val_MinusLogProbMetric: 16.9748 - lr: 1.0417e-05 - 107s/epoch - 548ms/step
Epoch 683/1000
2023-10-11 08:14:21.914 
Epoch 683/1000 
	 loss: 16.4333, MinusLogProbMetric: 16.4333, val_loss: 16.9709, val_MinusLogProbMetric: 16.9709

Epoch 683: val_loss did not improve from 16.96646
196/196 - 111s - loss: 16.4333 - MinusLogProbMetric: 16.4333 - val_loss: 16.9709 - val_MinusLogProbMetric: 16.9709 - lr: 1.0417e-05 - 111s/epoch - 567ms/step
Epoch 684/1000
2023-10-11 08:16:07.392 
Epoch 684/1000 
	 loss: 16.4376, MinusLogProbMetric: 16.4376, val_loss: 16.9802, val_MinusLogProbMetric: 16.9802

Epoch 684: val_loss did not improve from 16.96646
196/196 - 105s - loss: 16.4376 - MinusLogProbMetric: 16.4376 - val_loss: 16.9802 - val_MinusLogProbMetric: 16.9802 - lr: 1.0417e-05 - 105s/epoch - 538ms/step
Epoch 685/1000
2023-10-11 08:17:55.706 
Epoch 685/1000 
	 loss: 16.4369, MinusLogProbMetric: 16.4369, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 685: val_loss did not improve from 16.96646
196/196 - 108s - loss: 16.4369 - MinusLogProbMetric: 16.4369 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 1.0417e-05 - 108s/epoch - 552ms/step
Epoch 686/1000
2023-10-11 08:19:44.026 
Epoch 686/1000 
	 loss: 16.4363, MinusLogProbMetric: 16.4363, val_loss: 16.9910, val_MinusLogProbMetric: 16.9910

Epoch 686: val_loss did not improve from 16.96646
196/196 - 108s - loss: 16.4363 - MinusLogProbMetric: 16.4363 - val_loss: 16.9910 - val_MinusLogProbMetric: 16.9910 - lr: 1.0417e-05 - 108s/epoch - 552ms/step
Epoch 687/1000
2023-10-11 08:21:31.846 
Epoch 687/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 16.9726, val_MinusLogProbMetric: 16.9726

Epoch 687: val_loss did not improve from 16.96646
196/196 - 108s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 16.9726 - val_MinusLogProbMetric: 16.9726 - lr: 1.0417e-05 - 108s/epoch - 550ms/step
Epoch 688/1000
2023-10-11 08:23:24.553 
Epoch 688/1000 
	 loss: 16.4355, MinusLogProbMetric: 16.4355, val_loss: 16.9698, val_MinusLogProbMetric: 16.9698

Epoch 688: val_loss did not improve from 16.96646
196/196 - 113s - loss: 16.4355 - MinusLogProbMetric: 16.4355 - val_loss: 16.9698 - val_MinusLogProbMetric: 16.9698 - lr: 1.0417e-05 - 113s/epoch - 575ms/step
Epoch 689/1000
2023-10-11 08:25:15.996 
Epoch 689/1000 
	 loss: 16.4349, MinusLogProbMetric: 16.4349, val_loss: 17.0001, val_MinusLogProbMetric: 17.0001

Epoch 689: val_loss did not improve from 16.96646
196/196 - 111s - loss: 16.4349 - MinusLogProbMetric: 16.4349 - val_loss: 17.0001 - val_MinusLogProbMetric: 17.0001 - lr: 1.0417e-05 - 111s/epoch - 569ms/step
Epoch 690/1000
2023-10-11 08:27:02.277 
Epoch 690/1000 
	 loss: 16.4392, MinusLogProbMetric: 16.4392, val_loss: 16.9661, val_MinusLogProbMetric: 16.9661

Epoch 690: val_loss improved from 16.96646 to 16.96611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 108s - loss: 16.4392 - MinusLogProbMetric: 16.4392 - val_loss: 16.9661 - val_MinusLogProbMetric: 16.9661 - lr: 1.0417e-05 - 108s/epoch - 549ms/step
Epoch 691/1000
2023-10-11 08:28:55.878 
Epoch 691/1000 
	 loss: 16.4339, MinusLogProbMetric: 16.4339, val_loss: 16.9830, val_MinusLogProbMetric: 16.9830

Epoch 691: val_loss did not improve from 16.96611
196/196 - 112s - loss: 16.4339 - MinusLogProbMetric: 16.4339 - val_loss: 16.9830 - val_MinusLogProbMetric: 16.9830 - lr: 1.0417e-05 - 112s/epoch - 573ms/step
Epoch 692/1000
2023-10-11 08:30:48.780 
Epoch 692/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 16.9707, val_MinusLogProbMetric: 16.9707

Epoch 692: val_loss did not improve from 16.96611
196/196 - 113s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 16.9707 - val_MinusLogProbMetric: 16.9707 - lr: 1.0417e-05 - 113s/epoch - 576ms/step
Epoch 693/1000
2023-10-11 08:32:36.344 
Epoch 693/1000 
	 loss: 16.4350, MinusLogProbMetric: 16.4350, val_loss: 16.9703, val_MinusLogProbMetric: 16.9703

Epoch 693: val_loss did not improve from 16.96611
196/196 - 108s - loss: 16.4350 - MinusLogProbMetric: 16.4350 - val_loss: 16.9703 - val_MinusLogProbMetric: 16.9703 - lr: 1.0417e-05 - 108s/epoch - 549ms/step
Epoch 694/1000
2023-10-11 08:34:24.745 
Epoch 694/1000 
	 loss: 16.4352, MinusLogProbMetric: 16.4352, val_loss: 16.9804, val_MinusLogProbMetric: 16.9804

Epoch 694: val_loss did not improve from 16.96611
196/196 - 108s - loss: 16.4352 - MinusLogProbMetric: 16.4352 - val_loss: 16.9804 - val_MinusLogProbMetric: 16.9804 - lr: 1.0417e-05 - 108s/epoch - 553ms/step
Epoch 695/1000
2023-10-11 08:36:09.943 
Epoch 695/1000 
	 loss: 16.4328, MinusLogProbMetric: 16.4328, val_loss: 16.9758, val_MinusLogProbMetric: 16.9758

Epoch 695: val_loss did not improve from 16.96611
196/196 - 105s - loss: 16.4328 - MinusLogProbMetric: 16.4328 - val_loss: 16.9758 - val_MinusLogProbMetric: 16.9758 - lr: 1.0417e-05 - 105s/epoch - 537ms/step
Epoch 696/1000
2023-10-11 08:38:00.493 
Epoch 696/1000 
	 loss: 16.4372, MinusLogProbMetric: 16.4372, val_loss: 16.9720, val_MinusLogProbMetric: 16.9720

Epoch 696: val_loss did not improve from 16.96611
196/196 - 111s - loss: 16.4372 - MinusLogProbMetric: 16.4372 - val_loss: 16.9720 - val_MinusLogProbMetric: 16.9720 - lr: 1.0417e-05 - 111s/epoch - 564ms/step
Epoch 697/1000
2023-10-11 08:39:51.433 
Epoch 697/1000 
	 loss: 16.4369, MinusLogProbMetric: 16.4369, val_loss: 16.9682, val_MinusLogProbMetric: 16.9682

Epoch 697: val_loss did not improve from 16.96611
196/196 - 111s - loss: 16.4369 - MinusLogProbMetric: 16.4369 - val_loss: 16.9682 - val_MinusLogProbMetric: 16.9682 - lr: 1.0417e-05 - 111s/epoch - 566ms/step
Epoch 698/1000
2023-10-11 08:41:36.510 
Epoch 698/1000 
	 loss: 16.4332, MinusLogProbMetric: 16.4332, val_loss: 16.9689, val_MinusLogProbMetric: 16.9689

Epoch 698: val_loss did not improve from 16.96611
196/196 - 105s - loss: 16.4332 - MinusLogProbMetric: 16.4332 - val_loss: 16.9689 - val_MinusLogProbMetric: 16.9689 - lr: 1.0417e-05 - 105s/epoch - 536ms/step
Epoch 699/1000
2023-10-11 08:43:24.126 
Epoch 699/1000 
	 loss: 16.4351, MinusLogProbMetric: 16.4351, val_loss: 16.9698, val_MinusLogProbMetric: 16.9698

Epoch 699: val_loss did not improve from 16.96611
196/196 - 108s - loss: 16.4351 - MinusLogProbMetric: 16.4351 - val_loss: 16.9698 - val_MinusLogProbMetric: 16.9698 - lr: 1.0417e-05 - 108s/epoch - 549ms/step
Epoch 700/1000
2023-10-11 08:45:15.717 
Epoch 700/1000 
	 loss: 16.4333, MinusLogProbMetric: 16.4333, val_loss: 16.9671, val_MinusLogProbMetric: 16.9671

Epoch 700: val_loss did not improve from 16.96611
196/196 - 112s - loss: 16.4333 - MinusLogProbMetric: 16.4333 - val_loss: 16.9671 - val_MinusLogProbMetric: 16.9671 - lr: 1.0417e-05 - 112s/epoch - 569ms/step
Epoch 701/1000
2023-10-11 08:47:04.845 
Epoch 701/1000 
	 loss: 16.4334, MinusLogProbMetric: 16.4334, val_loss: 16.9897, val_MinusLogProbMetric: 16.9897

Epoch 701: val_loss did not improve from 16.96611
196/196 - 109s - loss: 16.4334 - MinusLogProbMetric: 16.4334 - val_loss: 16.9897 - val_MinusLogProbMetric: 16.9897 - lr: 1.0417e-05 - 109s/epoch - 557ms/step
Epoch 702/1000
2023-10-11 08:48:53.391 
Epoch 702/1000 
	 loss: 16.4373, MinusLogProbMetric: 16.4373, val_loss: 16.9833, val_MinusLogProbMetric: 16.9833

Epoch 702: val_loss did not improve from 16.96611
196/196 - 109s - loss: 16.4373 - MinusLogProbMetric: 16.4373 - val_loss: 16.9833 - val_MinusLogProbMetric: 16.9833 - lr: 1.0417e-05 - 109s/epoch - 554ms/step
Epoch 703/1000
2023-10-11 08:50:38.409 
Epoch 703/1000 
	 loss: 16.4343, MinusLogProbMetric: 16.4343, val_loss: 16.9848, val_MinusLogProbMetric: 16.9848

Epoch 703: val_loss did not improve from 16.96611
196/196 - 105s - loss: 16.4343 - MinusLogProbMetric: 16.4343 - val_loss: 16.9848 - val_MinusLogProbMetric: 16.9848 - lr: 1.0417e-05 - 105s/epoch - 536ms/step
Epoch 704/1000
2023-10-11 08:52:28.387 
Epoch 704/1000 
	 loss: 16.4346, MinusLogProbMetric: 16.4346, val_loss: 16.9931, val_MinusLogProbMetric: 16.9931

Epoch 704: val_loss did not improve from 16.96611
196/196 - 110s - loss: 16.4346 - MinusLogProbMetric: 16.4346 - val_loss: 16.9931 - val_MinusLogProbMetric: 16.9931 - lr: 1.0417e-05 - 110s/epoch - 561ms/step
Epoch 705/1000
2023-10-11 08:54:13.664 
Epoch 705/1000 
	 loss: 16.4342, MinusLogProbMetric: 16.4342, val_loss: 16.9714, val_MinusLogProbMetric: 16.9714

Epoch 705: val_loss did not improve from 16.96611
196/196 - 105s - loss: 16.4342 - MinusLogProbMetric: 16.4342 - val_loss: 16.9714 - val_MinusLogProbMetric: 16.9714 - lr: 1.0417e-05 - 105s/epoch - 537ms/step
Epoch 706/1000
2023-10-11 08:56:00.341 
Epoch 706/1000 
	 loss: 16.4344, MinusLogProbMetric: 16.4344, val_loss: 16.9691, val_MinusLogProbMetric: 16.9691

Epoch 706: val_loss did not improve from 16.96611
196/196 - 107s - loss: 16.4344 - MinusLogProbMetric: 16.4344 - val_loss: 16.9691 - val_MinusLogProbMetric: 16.9691 - lr: 1.0417e-05 - 107s/epoch - 544ms/step
Epoch 707/1000
2023-10-11 08:57:43.231 
Epoch 707/1000 
	 loss: 16.4314, MinusLogProbMetric: 16.4314, val_loss: 16.9830, val_MinusLogProbMetric: 16.9830

Epoch 707: val_loss did not improve from 16.96611
196/196 - 103s - loss: 16.4314 - MinusLogProbMetric: 16.4314 - val_loss: 16.9830 - val_MinusLogProbMetric: 16.9830 - lr: 1.0417e-05 - 103s/epoch - 525ms/step
Epoch 708/1000
2023-10-11 08:59:30.467 
Epoch 708/1000 
	 loss: 16.4351, MinusLogProbMetric: 16.4351, val_loss: 16.9699, val_MinusLogProbMetric: 16.9699

Epoch 708: val_loss did not improve from 16.96611
196/196 - 107s - loss: 16.4351 - MinusLogProbMetric: 16.4351 - val_loss: 16.9699 - val_MinusLogProbMetric: 16.9699 - lr: 1.0417e-05 - 107s/epoch - 547ms/step
Epoch 709/1000
2023-10-11 09:01:15.479 
Epoch 709/1000 
	 loss: 16.4367, MinusLogProbMetric: 16.4367, val_loss: 16.9712, val_MinusLogProbMetric: 16.9712

Epoch 709: val_loss did not improve from 16.96611
196/196 - 105s - loss: 16.4367 - MinusLogProbMetric: 16.4367 - val_loss: 16.9712 - val_MinusLogProbMetric: 16.9712 - lr: 1.0417e-05 - 105s/epoch - 536ms/step
Epoch 710/1000
2023-10-11 09:02:57.637 
Epoch 710/1000 
	 loss: 16.4376, MinusLogProbMetric: 16.4376, val_loss: 16.9819, val_MinusLogProbMetric: 16.9819

Epoch 710: val_loss did not improve from 16.96611
196/196 - 102s - loss: 16.4376 - MinusLogProbMetric: 16.4376 - val_loss: 16.9819 - val_MinusLogProbMetric: 16.9819 - lr: 1.0417e-05 - 102s/epoch - 521ms/step
Epoch 711/1000
2023-10-11 09:04:42.233 
Epoch 711/1000 
	 loss: 16.4371, MinusLogProbMetric: 16.4371, val_loss: 16.9789, val_MinusLogProbMetric: 16.9789

Epoch 711: val_loss did not improve from 16.96611
196/196 - 105s - loss: 16.4371 - MinusLogProbMetric: 16.4371 - val_loss: 16.9789 - val_MinusLogProbMetric: 16.9789 - lr: 1.0417e-05 - 105s/epoch - 534ms/step
Epoch 712/1000
2023-10-11 09:06:28.793 
Epoch 712/1000 
	 loss: 16.4346, MinusLogProbMetric: 16.4346, val_loss: 16.9766, val_MinusLogProbMetric: 16.9766

Epoch 712: val_loss did not improve from 16.96611
196/196 - 107s - loss: 16.4346 - MinusLogProbMetric: 16.4346 - val_loss: 16.9766 - val_MinusLogProbMetric: 16.9766 - lr: 1.0417e-05 - 107s/epoch - 543ms/step
Epoch 713/1000
2023-10-11 09:08:10.623 
Epoch 713/1000 
	 loss: 16.4317, MinusLogProbMetric: 16.4317, val_loss: 16.9657, val_MinusLogProbMetric: 16.9657

Epoch 713: val_loss improved from 16.96611 to 16.96574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 105s - loss: 16.4317 - MinusLogProbMetric: 16.4317 - val_loss: 16.9657 - val_MinusLogProbMetric: 16.9657 - lr: 1.0417e-05 - 105s/epoch - 534ms/step
Epoch 714/1000
2023-10-11 09:09:57.086 
Epoch 714/1000 
	 loss: 16.4340, MinusLogProbMetric: 16.4340, val_loss: 16.9883, val_MinusLogProbMetric: 16.9883

Epoch 714: val_loss did not improve from 16.96574
196/196 - 104s - loss: 16.4340 - MinusLogProbMetric: 16.4340 - val_loss: 16.9883 - val_MinusLogProbMetric: 16.9883 - lr: 1.0417e-05 - 104s/epoch - 529ms/step
Epoch 715/1000
2023-10-11 09:11:48.542 
Epoch 715/1000 
	 loss: 16.4336, MinusLogProbMetric: 16.4336, val_loss: 16.9861, val_MinusLogProbMetric: 16.9861

Epoch 715: val_loss did not improve from 16.96574
196/196 - 111s - loss: 16.4336 - MinusLogProbMetric: 16.4336 - val_loss: 16.9861 - val_MinusLogProbMetric: 16.9861 - lr: 1.0417e-05 - 111s/epoch - 569ms/step
Epoch 716/1000
2023-10-11 09:13:31.792 
Epoch 716/1000 
	 loss: 16.4323, MinusLogProbMetric: 16.4323, val_loss: 16.9704, val_MinusLogProbMetric: 16.9704

Epoch 716: val_loss did not improve from 16.96574
196/196 - 103s - loss: 16.4323 - MinusLogProbMetric: 16.4323 - val_loss: 16.9704 - val_MinusLogProbMetric: 16.9704 - lr: 1.0417e-05 - 103s/epoch - 527ms/step
Epoch 717/1000
2023-10-11 09:15:16.341 
Epoch 717/1000 
	 loss: 16.4347, MinusLogProbMetric: 16.4347, val_loss: 16.9782, val_MinusLogProbMetric: 16.9782

Epoch 717: val_loss did not improve from 16.96574
196/196 - 105s - loss: 16.4347 - MinusLogProbMetric: 16.4347 - val_loss: 16.9782 - val_MinusLogProbMetric: 16.9782 - lr: 1.0417e-05 - 105s/epoch - 533ms/step
Epoch 718/1000
2023-10-11 09:17:01.375 
Epoch 718/1000 
	 loss: 16.4326, MinusLogProbMetric: 16.4326, val_loss: 16.9787, val_MinusLogProbMetric: 16.9787

Epoch 718: val_loss did not improve from 16.96574
196/196 - 105s - loss: 16.4326 - MinusLogProbMetric: 16.4326 - val_loss: 16.9787 - val_MinusLogProbMetric: 16.9787 - lr: 1.0417e-05 - 105s/epoch - 536ms/step
Epoch 719/1000
2023-10-11 09:18:48.011 
Epoch 719/1000 
	 loss: 16.4357, MinusLogProbMetric: 16.4357, val_loss: 16.9726, val_MinusLogProbMetric: 16.9726

Epoch 719: val_loss did not improve from 16.96574
196/196 - 107s - loss: 16.4357 - MinusLogProbMetric: 16.4357 - val_loss: 16.9726 - val_MinusLogProbMetric: 16.9726 - lr: 1.0417e-05 - 107s/epoch - 544ms/step
Epoch 720/1000
2023-10-11 09:20:33.454 
Epoch 720/1000 
	 loss: 16.4340, MinusLogProbMetric: 16.4340, val_loss: 16.9688, val_MinusLogProbMetric: 16.9688

Epoch 720: val_loss did not improve from 16.96574
196/196 - 105s - loss: 16.4340 - MinusLogProbMetric: 16.4340 - val_loss: 16.9688 - val_MinusLogProbMetric: 16.9688 - lr: 1.0417e-05 - 105s/epoch - 538ms/step
Epoch 721/1000
2023-10-11 09:22:14.404 
Epoch 721/1000 
	 loss: 16.4298, MinusLogProbMetric: 16.4298, val_loss: 16.9697, val_MinusLogProbMetric: 16.9697

Epoch 721: val_loss did not improve from 16.96574
196/196 - 101s - loss: 16.4298 - MinusLogProbMetric: 16.4298 - val_loss: 16.9697 - val_MinusLogProbMetric: 16.9697 - lr: 1.0417e-05 - 101s/epoch - 515ms/step
Epoch 722/1000
2023-10-11 09:23:58.500 
Epoch 722/1000 
	 loss: 16.4321, MinusLogProbMetric: 16.4321, val_loss: 16.9706, val_MinusLogProbMetric: 16.9706

Epoch 722: val_loss did not improve from 16.96574
196/196 - 104s - loss: 16.4321 - MinusLogProbMetric: 16.4321 - val_loss: 16.9706 - val_MinusLogProbMetric: 16.9706 - lr: 1.0417e-05 - 104s/epoch - 531ms/step
Epoch 723/1000
2023-10-11 09:25:48.907 
Epoch 723/1000 
	 loss: 16.4342, MinusLogProbMetric: 16.4342, val_loss: 16.9703, val_MinusLogProbMetric: 16.9703

Epoch 723: val_loss did not improve from 16.96574
196/196 - 110s - loss: 16.4342 - MinusLogProbMetric: 16.4342 - val_loss: 16.9703 - val_MinusLogProbMetric: 16.9703 - lr: 1.0417e-05 - 110s/epoch - 563ms/step
Epoch 724/1000
2023-10-11 09:27:33.612 
Epoch 724/1000 
	 loss: 16.4321, MinusLogProbMetric: 16.4321, val_loss: 16.9688, val_MinusLogProbMetric: 16.9688

Epoch 724: val_loss did not improve from 16.96574
196/196 - 105s - loss: 16.4321 - MinusLogProbMetric: 16.4321 - val_loss: 16.9688 - val_MinusLogProbMetric: 16.9688 - lr: 1.0417e-05 - 105s/epoch - 534ms/step
Epoch 725/1000
2023-10-11 09:29:15.558 
Epoch 725/1000 
	 loss: 16.4349, MinusLogProbMetric: 16.4349, val_loss: 16.9927, val_MinusLogProbMetric: 16.9927

Epoch 725: val_loss did not improve from 16.96574
196/196 - 102s - loss: 16.4349 - MinusLogProbMetric: 16.4349 - val_loss: 16.9927 - val_MinusLogProbMetric: 16.9927 - lr: 1.0417e-05 - 102s/epoch - 520ms/step
Epoch 726/1000
2023-10-11 09:31:05.032 
Epoch 726/1000 
	 loss: 16.4322, MinusLogProbMetric: 16.4322, val_loss: 16.9735, val_MinusLogProbMetric: 16.9735

Epoch 726: val_loss did not improve from 16.96574
196/196 - 109s - loss: 16.4322 - MinusLogProbMetric: 16.4322 - val_loss: 16.9735 - val_MinusLogProbMetric: 16.9735 - lr: 1.0417e-05 - 109s/epoch - 559ms/step
Epoch 727/1000
2023-10-11 09:32:47.637 
Epoch 727/1000 
	 loss: 16.4345, MinusLogProbMetric: 16.4345, val_loss: 16.9727, val_MinusLogProbMetric: 16.9727

Epoch 727: val_loss did not improve from 16.96574
196/196 - 103s - loss: 16.4345 - MinusLogProbMetric: 16.4345 - val_loss: 16.9727 - val_MinusLogProbMetric: 16.9727 - lr: 1.0417e-05 - 103s/epoch - 523ms/step
Epoch 728/1000
2023-10-11 09:34:31.797 
Epoch 728/1000 
	 loss: 16.4361, MinusLogProbMetric: 16.4361, val_loss: 16.9848, val_MinusLogProbMetric: 16.9848

Epoch 728: val_loss did not improve from 16.96574
196/196 - 104s - loss: 16.4361 - MinusLogProbMetric: 16.4361 - val_loss: 16.9848 - val_MinusLogProbMetric: 16.9848 - lr: 1.0417e-05 - 104s/epoch - 532ms/step
Epoch 729/1000
2023-10-11 09:36:20.572 
Epoch 729/1000 
	 loss: 16.4323, MinusLogProbMetric: 16.4323, val_loss: 16.9697, val_MinusLogProbMetric: 16.9697

Epoch 729: val_loss did not improve from 16.96574
196/196 - 109s - loss: 16.4323 - MinusLogProbMetric: 16.4323 - val_loss: 16.9697 - val_MinusLogProbMetric: 16.9697 - lr: 1.0417e-05 - 109s/epoch - 555ms/step
Epoch 730/1000
2023-10-11 09:38:03.826 
Epoch 730/1000 
	 loss: 16.4361, MinusLogProbMetric: 16.4361, val_loss: 16.9704, val_MinusLogProbMetric: 16.9704

Epoch 730: val_loss did not improve from 16.96574
196/196 - 103s - loss: 16.4361 - MinusLogProbMetric: 16.4361 - val_loss: 16.9704 - val_MinusLogProbMetric: 16.9704 - lr: 1.0417e-05 - 103s/epoch - 527ms/step
Epoch 731/1000
2023-10-11 09:39:47.541 
Epoch 731/1000 
	 loss: 16.4329, MinusLogProbMetric: 16.4329, val_loss: 16.9791, val_MinusLogProbMetric: 16.9791

Epoch 731: val_loss did not improve from 16.96574
196/196 - 104s - loss: 16.4329 - MinusLogProbMetric: 16.4329 - val_loss: 16.9791 - val_MinusLogProbMetric: 16.9791 - lr: 1.0417e-05 - 104s/epoch - 529ms/step
Epoch 732/1000
2023-10-11 09:41:32.033 
Epoch 732/1000 
	 loss: 16.4309, MinusLogProbMetric: 16.4309, val_loss: 16.9876, val_MinusLogProbMetric: 16.9876

Epoch 732: val_loss did not improve from 16.96574
196/196 - 105s - loss: 16.4309 - MinusLogProbMetric: 16.4309 - val_loss: 16.9876 - val_MinusLogProbMetric: 16.9876 - lr: 1.0417e-05 - 105s/epoch - 533ms/step
Epoch 733/1000
2023-10-11 09:43:23.335 
Epoch 733/1000 
	 loss: 16.4334, MinusLogProbMetric: 16.4334, val_loss: 17.0035, val_MinusLogProbMetric: 17.0035

Epoch 733: val_loss did not improve from 16.96574
196/196 - 111s - loss: 16.4334 - MinusLogProbMetric: 16.4334 - val_loss: 17.0035 - val_MinusLogProbMetric: 17.0035 - lr: 1.0417e-05 - 111s/epoch - 568ms/step
Epoch 734/1000
2023-10-11 09:45:08.845 
Epoch 734/1000 
	 loss: 16.4333, MinusLogProbMetric: 16.4333, val_loss: 16.9739, val_MinusLogProbMetric: 16.9739

Epoch 734: val_loss did not improve from 16.96574
196/196 - 106s - loss: 16.4333 - MinusLogProbMetric: 16.4333 - val_loss: 16.9739 - val_MinusLogProbMetric: 16.9739 - lr: 1.0417e-05 - 106s/epoch - 538ms/step
Epoch 735/1000
2023-10-11 09:46:59.420 
Epoch 735/1000 
	 loss: 16.4317, MinusLogProbMetric: 16.4317, val_loss: 16.9834, val_MinusLogProbMetric: 16.9834

Epoch 735: val_loss did not improve from 16.96574
196/196 - 111s - loss: 16.4317 - MinusLogProbMetric: 16.4317 - val_loss: 16.9834 - val_MinusLogProbMetric: 16.9834 - lr: 1.0417e-05 - 111s/epoch - 564ms/step
Epoch 736/1000
2023-10-11 09:48:47.246 
Epoch 736/1000 
	 loss: 16.4311, MinusLogProbMetric: 16.4311, val_loss: 16.9672, val_MinusLogProbMetric: 16.9672

Epoch 736: val_loss did not improve from 16.96574
196/196 - 108s - loss: 16.4311 - MinusLogProbMetric: 16.4311 - val_loss: 16.9672 - val_MinusLogProbMetric: 16.9672 - lr: 1.0417e-05 - 108s/epoch - 550ms/step
Epoch 737/1000
2023-10-11 09:50:33.355 
Epoch 737/1000 
	 loss: 16.4329, MinusLogProbMetric: 16.4329, val_loss: 16.9732, val_MinusLogProbMetric: 16.9732

Epoch 737: val_loss did not improve from 16.96574
196/196 - 106s - loss: 16.4329 - MinusLogProbMetric: 16.4329 - val_loss: 16.9732 - val_MinusLogProbMetric: 16.9732 - lr: 1.0417e-05 - 106s/epoch - 541ms/step
Epoch 738/1000
2023-10-11 09:52:22.457 
Epoch 738/1000 
	 loss: 16.4309, MinusLogProbMetric: 16.4309, val_loss: 16.9715, val_MinusLogProbMetric: 16.9715

Epoch 738: val_loss did not improve from 16.96574
196/196 - 109s - loss: 16.4309 - MinusLogProbMetric: 16.4309 - val_loss: 16.9715 - val_MinusLogProbMetric: 16.9715 - lr: 1.0417e-05 - 109s/epoch - 557ms/step
Epoch 739/1000
2023-10-11 09:54:12.161 
Epoch 739/1000 
	 loss: 16.4311, MinusLogProbMetric: 16.4311, val_loss: 16.9864, val_MinusLogProbMetric: 16.9864

Epoch 739: val_loss did not improve from 16.96574
196/196 - 110s - loss: 16.4311 - MinusLogProbMetric: 16.4311 - val_loss: 16.9864 - val_MinusLogProbMetric: 16.9864 - lr: 1.0417e-05 - 110s/epoch - 560ms/step
Epoch 740/1000
2023-10-11 09:56:06.084 
Epoch 740/1000 
	 loss: 16.4317, MinusLogProbMetric: 16.4317, val_loss: 16.9692, val_MinusLogProbMetric: 16.9692

Epoch 740: val_loss did not improve from 16.96574
196/196 - 114s - loss: 16.4317 - MinusLogProbMetric: 16.4317 - val_loss: 16.9692 - val_MinusLogProbMetric: 16.9692 - lr: 1.0417e-05 - 114s/epoch - 581ms/step
Epoch 741/1000
2023-10-11 09:57:59.240 
Epoch 741/1000 
	 loss: 16.4324, MinusLogProbMetric: 16.4324, val_loss: 16.9764, val_MinusLogProbMetric: 16.9764

Epoch 741: val_loss did not improve from 16.96574
196/196 - 113s - loss: 16.4324 - MinusLogProbMetric: 16.4324 - val_loss: 16.9764 - val_MinusLogProbMetric: 16.9764 - lr: 1.0417e-05 - 113s/epoch - 577ms/step
Epoch 742/1000
2023-10-11 09:59:50.121 
Epoch 742/1000 
	 loss: 16.4347, MinusLogProbMetric: 16.4347, val_loss: 16.9678, val_MinusLogProbMetric: 16.9678

Epoch 742: val_loss did not improve from 16.96574
196/196 - 111s - loss: 16.4347 - MinusLogProbMetric: 16.4347 - val_loss: 16.9678 - val_MinusLogProbMetric: 16.9678 - lr: 1.0417e-05 - 111s/epoch - 566ms/step
Epoch 743/1000
2023-10-11 10:01:42.271 
Epoch 743/1000 
	 loss: 16.4344, MinusLogProbMetric: 16.4344, val_loss: 16.9765, val_MinusLogProbMetric: 16.9765

Epoch 743: val_loss did not improve from 16.96574
196/196 - 112s - loss: 16.4344 - MinusLogProbMetric: 16.4344 - val_loss: 16.9765 - val_MinusLogProbMetric: 16.9765 - lr: 1.0417e-05 - 112s/epoch - 572ms/step
Epoch 744/1000
2023-10-11 10:03:37.105 
Epoch 744/1000 
	 loss: 16.4323, MinusLogProbMetric: 16.4323, val_loss: 16.9697, val_MinusLogProbMetric: 16.9697

Epoch 744: val_loss did not improve from 16.96574
196/196 - 115s - loss: 16.4323 - MinusLogProbMetric: 16.4323 - val_loss: 16.9697 - val_MinusLogProbMetric: 16.9697 - lr: 1.0417e-05 - 115s/epoch - 586ms/step
Epoch 745/1000
2023-10-11 10:05:24.893 
Epoch 745/1000 
	 loss: 16.4301, MinusLogProbMetric: 16.4301, val_loss: 16.9705, val_MinusLogProbMetric: 16.9705

Epoch 745: val_loss did not improve from 16.96574
196/196 - 108s - loss: 16.4301 - MinusLogProbMetric: 16.4301 - val_loss: 16.9705 - val_MinusLogProbMetric: 16.9705 - lr: 1.0417e-05 - 108s/epoch - 550ms/step
Epoch 746/1000
2023-10-11 10:07:16.686 
Epoch 746/1000 
	 loss: 16.4318, MinusLogProbMetric: 16.4318, val_loss: 16.9745, val_MinusLogProbMetric: 16.9745

Epoch 746: val_loss did not improve from 16.96574
196/196 - 112s - loss: 16.4318 - MinusLogProbMetric: 16.4318 - val_loss: 16.9745 - val_MinusLogProbMetric: 16.9745 - lr: 1.0417e-05 - 112s/epoch - 570ms/step
Epoch 747/1000
2023-10-11 10:09:15.636 
Epoch 747/1000 
	 loss: 16.4298, MinusLogProbMetric: 16.4298, val_loss: 16.9761, val_MinusLogProbMetric: 16.9761

Epoch 747: val_loss did not improve from 16.96574
196/196 - 119s - loss: 16.4298 - MinusLogProbMetric: 16.4298 - val_loss: 16.9761 - val_MinusLogProbMetric: 16.9761 - lr: 1.0417e-05 - 119s/epoch - 607ms/step
Epoch 748/1000
2023-10-11 10:11:02.494 
Epoch 748/1000 
	 loss: 16.4305, MinusLogProbMetric: 16.4305, val_loss: 16.9857, val_MinusLogProbMetric: 16.9857

Epoch 748: val_loss did not improve from 16.96574
196/196 - 107s - loss: 16.4305 - MinusLogProbMetric: 16.4305 - val_loss: 16.9857 - val_MinusLogProbMetric: 16.9857 - lr: 1.0417e-05 - 107s/epoch - 545ms/step
Epoch 749/1000
2023-10-11 10:12:55.145 
Epoch 749/1000 
	 loss: 16.4326, MinusLogProbMetric: 16.4326, val_loss: 16.9725, val_MinusLogProbMetric: 16.9725

Epoch 749: val_loss did not improve from 16.96574
196/196 - 113s - loss: 16.4326 - MinusLogProbMetric: 16.4326 - val_loss: 16.9725 - val_MinusLogProbMetric: 16.9725 - lr: 1.0417e-05 - 113s/epoch - 575ms/step
Epoch 750/1000
2023-10-11 10:14:43.424 
Epoch 750/1000 
	 loss: 16.4337, MinusLogProbMetric: 16.4337, val_loss: 16.9800, val_MinusLogProbMetric: 16.9800

Epoch 750: val_loss did not improve from 16.96574
196/196 - 108s - loss: 16.4337 - MinusLogProbMetric: 16.4337 - val_loss: 16.9800 - val_MinusLogProbMetric: 16.9800 - lr: 1.0417e-05 - 108s/epoch - 552ms/step
Epoch 751/1000
2023-10-11 10:16:36.944 
Epoch 751/1000 
	 loss: 16.4350, MinusLogProbMetric: 16.4350, val_loss: 16.9759, val_MinusLogProbMetric: 16.9759

Epoch 751: val_loss did not improve from 16.96574
196/196 - 114s - loss: 16.4350 - MinusLogProbMetric: 16.4350 - val_loss: 16.9759 - val_MinusLogProbMetric: 16.9759 - lr: 1.0417e-05 - 114s/epoch - 579ms/step
Epoch 752/1000
2023-10-11 10:18:23.913 
Epoch 752/1000 
	 loss: 16.4320, MinusLogProbMetric: 16.4320, val_loss: 16.9727, val_MinusLogProbMetric: 16.9727

Epoch 752: val_loss did not improve from 16.96574
196/196 - 107s - loss: 16.4320 - MinusLogProbMetric: 16.4320 - val_loss: 16.9727 - val_MinusLogProbMetric: 16.9727 - lr: 1.0417e-05 - 107s/epoch - 546ms/step
Epoch 753/1000
2023-10-11 10:20:15.892 
Epoch 753/1000 
	 loss: 16.4309, MinusLogProbMetric: 16.4309, val_loss: 17.0108, val_MinusLogProbMetric: 17.0108

Epoch 753: val_loss did not improve from 16.96574
196/196 - 112s - loss: 16.4309 - MinusLogProbMetric: 16.4309 - val_loss: 17.0108 - val_MinusLogProbMetric: 17.0108 - lr: 1.0417e-05 - 112s/epoch - 571ms/step
Epoch 754/1000
2023-10-11 10:22:02.168 
Epoch 754/1000 
	 loss: 16.4322, MinusLogProbMetric: 16.4322, val_loss: 17.0147, val_MinusLogProbMetric: 17.0147

Epoch 754: val_loss did not improve from 16.96574
196/196 - 106s - loss: 16.4322 - MinusLogProbMetric: 16.4322 - val_loss: 17.0147 - val_MinusLogProbMetric: 17.0147 - lr: 1.0417e-05 - 106s/epoch - 543ms/step
Epoch 755/1000
2023-10-11 10:23:54.673 
Epoch 755/1000 
	 loss: 16.4315, MinusLogProbMetric: 16.4315, val_loss: 16.9676, val_MinusLogProbMetric: 16.9676

Epoch 755: val_loss did not improve from 16.96574
196/196 - 112s - loss: 16.4315 - MinusLogProbMetric: 16.4315 - val_loss: 16.9676 - val_MinusLogProbMetric: 16.9676 - lr: 1.0417e-05 - 112s/epoch - 574ms/step
Epoch 756/1000
2023-10-11 10:25:43.363 
Epoch 756/1000 
	 loss: 16.4304, MinusLogProbMetric: 16.4304, val_loss: 16.9768, val_MinusLogProbMetric: 16.9768

Epoch 756: val_loss did not improve from 16.96574
196/196 - 109s - loss: 16.4304 - MinusLogProbMetric: 16.4304 - val_loss: 16.9768 - val_MinusLogProbMetric: 16.9768 - lr: 1.0417e-05 - 109s/epoch - 555ms/step
Epoch 757/1000
2023-10-11 10:27:28.192 
Epoch 757/1000 
	 loss: 16.4296, MinusLogProbMetric: 16.4296, val_loss: 16.9762, val_MinusLogProbMetric: 16.9762

Epoch 757: val_loss did not improve from 16.96574
196/196 - 105s - loss: 16.4296 - MinusLogProbMetric: 16.4296 - val_loss: 16.9762 - val_MinusLogProbMetric: 16.9762 - lr: 1.0417e-05 - 105s/epoch - 535ms/step
Epoch 758/1000
2023-10-11 10:29:17.852 
Epoch 758/1000 
	 loss: 16.4311, MinusLogProbMetric: 16.4311, val_loss: 16.9720, val_MinusLogProbMetric: 16.9720

Epoch 758: val_loss did not improve from 16.96574
196/196 - 110s - loss: 16.4311 - MinusLogProbMetric: 16.4311 - val_loss: 16.9720 - val_MinusLogProbMetric: 16.9720 - lr: 1.0417e-05 - 110s/epoch - 559ms/step
Epoch 759/1000
2023-10-11 10:31:07.283 
Epoch 759/1000 
	 loss: 16.4308, MinusLogProbMetric: 16.4308, val_loss: 16.9752, val_MinusLogProbMetric: 16.9752

Epoch 759: val_loss did not improve from 16.96574
196/196 - 109s - loss: 16.4308 - MinusLogProbMetric: 16.4308 - val_loss: 16.9752 - val_MinusLogProbMetric: 16.9752 - lr: 1.0417e-05 - 109s/epoch - 558ms/step
Epoch 760/1000
2023-10-11 10:32:54.345 
Epoch 760/1000 
	 loss: 16.4304, MinusLogProbMetric: 16.4304, val_loss: 16.9682, val_MinusLogProbMetric: 16.9682

Epoch 760: val_loss did not improve from 16.96574
196/196 - 107s - loss: 16.4304 - MinusLogProbMetric: 16.4304 - val_loss: 16.9682 - val_MinusLogProbMetric: 16.9682 - lr: 1.0417e-05 - 107s/epoch - 546ms/step
Epoch 761/1000
2023-10-11 10:34:48.361 
Epoch 761/1000 
	 loss: 16.4329, MinusLogProbMetric: 16.4329, val_loss: 16.9750, val_MinusLogProbMetric: 16.9750

Epoch 761: val_loss did not improve from 16.96574
196/196 - 114s - loss: 16.4329 - MinusLogProbMetric: 16.4329 - val_loss: 16.9750 - val_MinusLogProbMetric: 16.9750 - lr: 1.0417e-05 - 114s/epoch - 582ms/step
Epoch 762/1000
2023-10-11 10:36:39.503 
Epoch 762/1000 
	 loss: 16.4327, MinusLogProbMetric: 16.4327, val_loss: 16.9759, val_MinusLogProbMetric: 16.9759

Epoch 762: val_loss did not improve from 16.96574
196/196 - 111s - loss: 16.4327 - MinusLogProbMetric: 16.4327 - val_loss: 16.9759 - val_MinusLogProbMetric: 16.9759 - lr: 1.0417e-05 - 111s/epoch - 567ms/step
Epoch 763/1000
2023-10-11 10:38:29.635 
Epoch 763/1000 
	 loss: 16.4307, MinusLogProbMetric: 16.4307, val_loss: 16.9747, val_MinusLogProbMetric: 16.9747

Epoch 763: val_loss did not improve from 16.96574
196/196 - 110s - loss: 16.4307 - MinusLogProbMetric: 16.4307 - val_loss: 16.9747 - val_MinusLogProbMetric: 16.9747 - lr: 1.0417e-05 - 110s/epoch - 562ms/step
Epoch 764/1000
2023-10-11 10:40:18.408 
Epoch 764/1000 
	 loss: 16.4215, MinusLogProbMetric: 16.4215, val_loss: 16.9689, val_MinusLogProbMetric: 16.9689

Epoch 764: val_loss did not improve from 16.96574
196/196 - 109s - loss: 16.4215 - MinusLogProbMetric: 16.4215 - val_loss: 16.9689 - val_MinusLogProbMetric: 16.9689 - lr: 5.2083e-06 - 109s/epoch - 555ms/step
Epoch 765/1000
2023-10-11 10:42:10.351 
Epoch 765/1000 
	 loss: 16.4188, MinusLogProbMetric: 16.4188, val_loss: 16.9692, val_MinusLogProbMetric: 16.9692

Epoch 765: val_loss did not improve from 16.96574
196/196 - 112s - loss: 16.4188 - MinusLogProbMetric: 16.4188 - val_loss: 16.9692 - val_MinusLogProbMetric: 16.9692 - lr: 5.2083e-06 - 112s/epoch - 571ms/step
Epoch 766/1000
2023-10-11 10:43:58.190 
Epoch 766/1000 
	 loss: 16.4209, MinusLogProbMetric: 16.4209, val_loss: 16.9630, val_MinusLogProbMetric: 16.9630

Epoch 766: val_loss improved from 16.96574 to 16.96301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 110s - loss: 16.4209 - MinusLogProbMetric: 16.4209 - val_loss: 16.9630 - val_MinusLogProbMetric: 16.9630 - lr: 5.2083e-06 - 110s/epoch - 564ms/step
Epoch 767/1000
2023-10-11 10:45:50.597 
Epoch 767/1000 
	 loss: 16.4197, MinusLogProbMetric: 16.4197, val_loss: 16.9622, val_MinusLogProbMetric: 16.9622

Epoch 767: val_loss improved from 16.96301 to 16.96219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 111s - loss: 16.4197 - MinusLogProbMetric: 16.4197 - val_loss: 16.9622 - val_MinusLogProbMetric: 16.9622 - lr: 5.2083e-06 - 111s/epoch - 569ms/step
Epoch 768/1000
2023-10-11 10:47:38.700 
Epoch 768/1000 
	 loss: 16.4197, MinusLogProbMetric: 16.4197, val_loss: 16.9690, val_MinusLogProbMetric: 16.9690

Epoch 768: val_loss did not improve from 16.96219
196/196 - 106s - loss: 16.4197 - MinusLogProbMetric: 16.4197 - val_loss: 16.9690 - val_MinusLogProbMetric: 16.9690 - lr: 5.2083e-06 - 106s/epoch - 543ms/step
Epoch 769/1000
2023-10-11 10:49:31.641 
Epoch 769/1000 
	 loss: 16.4207, MinusLogProbMetric: 16.4207, val_loss: 16.9636, val_MinusLogProbMetric: 16.9636

Epoch 769: val_loss did not improve from 16.96219
196/196 - 113s - loss: 16.4207 - MinusLogProbMetric: 16.4207 - val_loss: 16.9636 - val_MinusLogProbMetric: 16.9636 - lr: 5.2083e-06 - 113s/epoch - 576ms/step
Epoch 770/1000
2023-10-11 10:51:15.552 
Epoch 770/1000 
	 loss: 16.4198, MinusLogProbMetric: 16.4198, val_loss: 16.9654, val_MinusLogProbMetric: 16.9654

Epoch 770: val_loss did not improve from 16.96219
196/196 - 104s - loss: 16.4198 - MinusLogProbMetric: 16.4198 - val_loss: 16.9654 - val_MinusLogProbMetric: 16.9654 - lr: 5.2083e-06 - 104s/epoch - 530ms/step
Epoch 771/1000
2023-10-11 10:53:04.313 
Epoch 771/1000 
	 loss: 16.4194, MinusLogProbMetric: 16.4194, val_loss: 16.9725, val_MinusLogProbMetric: 16.9725

Epoch 771: val_loss did not improve from 16.96219
196/196 - 109s - loss: 16.4194 - MinusLogProbMetric: 16.4194 - val_loss: 16.9725 - val_MinusLogProbMetric: 16.9725 - lr: 5.2083e-06 - 109s/epoch - 555ms/step
Epoch 772/1000
2023-10-11 10:54:52.315 
Epoch 772/1000 
	 loss: 16.4204, MinusLogProbMetric: 16.4204, val_loss: 16.9725, val_MinusLogProbMetric: 16.9725

Epoch 772: val_loss did not improve from 16.96219
196/196 - 108s - loss: 16.4204 - MinusLogProbMetric: 16.4204 - val_loss: 16.9725 - val_MinusLogProbMetric: 16.9725 - lr: 5.2083e-06 - 108s/epoch - 551ms/step
Epoch 773/1000
2023-10-11 10:56:16.318 
Epoch 773/1000 
	 loss: 16.4211, MinusLogProbMetric: 16.4211, val_loss: 16.9673, val_MinusLogProbMetric: 16.9673

Epoch 773: val_loss did not improve from 16.96219
196/196 - 84s - loss: 16.4211 - MinusLogProbMetric: 16.4211 - val_loss: 16.9673 - val_MinusLogProbMetric: 16.9673 - lr: 5.2083e-06 - 84s/epoch - 429ms/step
Epoch 774/1000
2023-10-11 10:57:03.326 
Epoch 774/1000 
	 loss: 16.4203, MinusLogProbMetric: 16.4203, val_loss: 16.9632, val_MinusLogProbMetric: 16.9632

Epoch 774: val_loss did not improve from 16.96219
196/196 - 47s - loss: 16.4203 - MinusLogProbMetric: 16.4203 - val_loss: 16.9632 - val_MinusLogProbMetric: 16.9632 - lr: 5.2083e-06 - 47s/epoch - 240ms/step
Epoch 775/1000
2023-10-11 10:57:51.220 
Epoch 775/1000 
	 loss: 16.4215, MinusLogProbMetric: 16.4215, val_loss: 16.9754, val_MinusLogProbMetric: 16.9754

Epoch 775: val_loss did not improve from 16.96219
196/196 - 48s - loss: 16.4215 - MinusLogProbMetric: 16.4215 - val_loss: 16.9754 - val_MinusLogProbMetric: 16.9754 - lr: 5.2083e-06 - 48s/epoch - 244ms/step
Epoch 776/1000
2023-10-11 10:58:40.406 
Epoch 776/1000 
	 loss: 16.4220, MinusLogProbMetric: 16.4220, val_loss: 16.9637, val_MinusLogProbMetric: 16.9637

Epoch 776: val_loss did not improve from 16.96219
196/196 - 49s - loss: 16.4220 - MinusLogProbMetric: 16.4220 - val_loss: 16.9637 - val_MinusLogProbMetric: 16.9637 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 777/1000
2023-10-11 10:59:28.619 
Epoch 777/1000 
	 loss: 16.4194, MinusLogProbMetric: 16.4194, val_loss: 16.9631, val_MinusLogProbMetric: 16.9631

Epoch 777: val_loss did not improve from 16.96219
196/196 - 48s - loss: 16.4194 - MinusLogProbMetric: 16.4194 - val_loss: 16.9631 - val_MinusLogProbMetric: 16.9631 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 778/1000
2023-10-11 11:00:16.249 
Epoch 778/1000 
	 loss: 16.4205, MinusLogProbMetric: 16.4205, val_loss: 16.9645, val_MinusLogProbMetric: 16.9645

Epoch 778: val_loss did not improve from 16.96219
196/196 - 48s - loss: 16.4205 - MinusLogProbMetric: 16.4205 - val_loss: 16.9645 - val_MinusLogProbMetric: 16.9645 - lr: 5.2083e-06 - 48s/epoch - 243ms/step
Epoch 779/1000
2023-10-11 11:01:05.012 
Epoch 779/1000 
	 loss: 16.4209, MinusLogProbMetric: 16.4209, val_loss: 16.9662, val_MinusLogProbMetric: 16.9662

Epoch 779: val_loss did not improve from 16.96219
196/196 - 49s - loss: 16.4209 - MinusLogProbMetric: 16.4209 - val_loss: 16.9662 - val_MinusLogProbMetric: 16.9662 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 780/1000
2023-10-11 11:01:53.540 
Epoch 780/1000 
	 loss: 16.4209, MinusLogProbMetric: 16.4209, val_loss: 16.9629, val_MinusLogProbMetric: 16.9629

Epoch 780: val_loss did not improve from 16.96219
196/196 - 49s - loss: 16.4209 - MinusLogProbMetric: 16.4209 - val_loss: 16.9629 - val_MinusLogProbMetric: 16.9629 - lr: 5.2083e-06 - 49s/epoch - 248ms/step
Epoch 781/1000
2023-10-11 11:02:42.448 
Epoch 781/1000 
	 loss: 16.4190, MinusLogProbMetric: 16.4190, val_loss: 16.9666, val_MinusLogProbMetric: 16.9666

Epoch 781: val_loss did not improve from 16.96219
196/196 - 49s - loss: 16.4190 - MinusLogProbMetric: 16.4190 - val_loss: 16.9666 - val_MinusLogProbMetric: 16.9666 - lr: 5.2083e-06 - 49s/epoch - 250ms/step
Epoch 782/1000
2023-10-11 11:03:32.567 
Epoch 782/1000 
	 loss: 16.4193, MinusLogProbMetric: 16.4193, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 782: val_loss improved from 16.96219 to 16.96140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 51s - loss: 16.4193 - MinusLogProbMetric: 16.4193 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 5.2083e-06 - 51s/epoch - 262ms/step
Epoch 783/1000
2023-10-11 11:04:24.701 
Epoch 783/1000 
	 loss: 16.4187, MinusLogProbMetric: 16.4187, val_loss: 16.9648, val_MinusLogProbMetric: 16.9648

Epoch 783: val_loss did not improve from 16.96140
196/196 - 51s - loss: 16.4187 - MinusLogProbMetric: 16.4187 - val_loss: 16.9648 - val_MinusLogProbMetric: 16.9648 - lr: 5.2083e-06 - 51s/epoch - 259ms/step
Epoch 784/1000
2023-10-11 11:05:12.047 
Epoch 784/1000 
	 loss: 16.4196, MinusLogProbMetric: 16.4196, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 784: val_loss did not improve from 16.96140
196/196 - 47s - loss: 16.4196 - MinusLogProbMetric: 16.4196 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 5.2083e-06 - 47s/epoch - 242ms/step
Epoch 785/1000
2023-10-11 11:06:00.094 
Epoch 785/1000 
	 loss: 16.4193, MinusLogProbMetric: 16.4193, val_loss: 16.9619, val_MinusLogProbMetric: 16.9619

Epoch 785: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4193 - MinusLogProbMetric: 16.4193 - val_loss: 16.9619 - val_MinusLogProbMetric: 16.9619 - lr: 5.2083e-06 - 48s/epoch - 245ms/step
Epoch 786/1000
2023-10-11 11:06:47.996 
Epoch 786/1000 
	 loss: 16.4193, MinusLogProbMetric: 16.4193, val_loss: 16.9643, val_MinusLogProbMetric: 16.9643

Epoch 786: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4193 - MinusLogProbMetric: 16.4193 - val_loss: 16.9643 - val_MinusLogProbMetric: 16.9643 - lr: 5.2083e-06 - 48s/epoch - 244ms/step
Epoch 787/1000
2023-10-11 11:07:35.583 
Epoch 787/1000 
	 loss: 16.4199, MinusLogProbMetric: 16.4199, val_loss: 16.9636, val_MinusLogProbMetric: 16.9636

Epoch 787: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4199 - MinusLogProbMetric: 16.4199 - val_loss: 16.9636 - val_MinusLogProbMetric: 16.9636 - lr: 5.2083e-06 - 48s/epoch - 243ms/step
Epoch 788/1000
2023-10-11 11:08:23.532 
Epoch 788/1000 
	 loss: 16.4200, MinusLogProbMetric: 16.4200, val_loss: 16.9654, val_MinusLogProbMetric: 16.9654

Epoch 788: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4200 - MinusLogProbMetric: 16.4200 - val_loss: 16.9654 - val_MinusLogProbMetric: 16.9654 - lr: 5.2083e-06 - 48s/epoch - 245ms/step
Epoch 789/1000
2023-10-11 11:09:12.057 
Epoch 789/1000 
	 loss: 16.4179, MinusLogProbMetric: 16.4179, val_loss: 16.9677, val_MinusLogProbMetric: 16.9677

Epoch 789: val_loss did not improve from 16.96140
196/196 - 49s - loss: 16.4179 - MinusLogProbMetric: 16.4179 - val_loss: 16.9677 - val_MinusLogProbMetric: 16.9677 - lr: 5.2083e-06 - 49s/epoch - 248ms/step
Epoch 790/1000
2023-10-11 11:10:00.947 
Epoch 790/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9670, val_MinusLogProbMetric: 16.9670

Epoch 790: val_loss did not improve from 16.96140
196/196 - 49s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9670 - val_MinusLogProbMetric: 16.9670 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 791/1000
2023-10-11 11:10:49.413 
Epoch 791/1000 
	 loss: 16.4188, MinusLogProbMetric: 16.4188, val_loss: 16.9643, val_MinusLogProbMetric: 16.9643

Epoch 791: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4188 - MinusLogProbMetric: 16.4188 - val_loss: 16.9643 - val_MinusLogProbMetric: 16.9643 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 792/1000
2023-10-11 11:11:38.210 
Epoch 792/1000 
	 loss: 16.4188, MinusLogProbMetric: 16.4188, val_loss: 16.9722, val_MinusLogProbMetric: 16.9722

Epoch 792: val_loss did not improve from 16.96140
196/196 - 49s - loss: 16.4188 - MinusLogProbMetric: 16.4188 - val_loss: 16.9722 - val_MinusLogProbMetric: 16.9722 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 793/1000
2023-10-11 11:12:27.746 
Epoch 793/1000 
	 loss: 16.4204, MinusLogProbMetric: 16.4204, val_loss: 16.9739, val_MinusLogProbMetric: 16.9739

Epoch 793: val_loss did not improve from 16.96140
196/196 - 50s - loss: 16.4204 - MinusLogProbMetric: 16.4204 - val_loss: 16.9739 - val_MinusLogProbMetric: 16.9739 - lr: 5.2083e-06 - 50s/epoch - 253ms/step
Epoch 794/1000
2023-10-11 11:13:15.682 
Epoch 794/1000 
	 loss: 16.4211, MinusLogProbMetric: 16.4211, val_loss: 16.9630, val_MinusLogProbMetric: 16.9630

Epoch 794: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4211 - MinusLogProbMetric: 16.4211 - val_loss: 16.9630 - val_MinusLogProbMetric: 16.9630 - lr: 5.2083e-06 - 48s/epoch - 245ms/step
Epoch 795/1000
2023-10-11 11:14:03.972 
Epoch 795/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9815, val_MinusLogProbMetric: 16.9815

Epoch 795: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9815 - val_MinusLogProbMetric: 16.9815 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 796/1000
2023-10-11 11:14:53.485 
Epoch 796/1000 
	 loss: 16.4191, MinusLogProbMetric: 16.4191, val_loss: 16.9689, val_MinusLogProbMetric: 16.9689

Epoch 796: val_loss did not improve from 16.96140
196/196 - 50s - loss: 16.4191 - MinusLogProbMetric: 16.4191 - val_loss: 16.9689 - val_MinusLogProbMetric: 16.9689 - lr: 5.2083e-06 - 50s/epoch - 253ms/step
Epoch 797/1000
2023-10-11 11:15:42.962 
Epoch 797/1000 
	 loss: 16.4191, MinusLogProbMetric: 16.4191, val_loss: 16.9666, val_MinusLogProbMetric: 16.9666

Epoch 797: val_loss did not improve from 16.96140
196/196 - 49s - loss: 16.4191 - MinusLogProbMetric: 16.4191 - val_loss: 16.9666 - val_MinusLogProbMetric: 16.9666 - lr: 5.2083e-06 - 49s/epoch - 252ms/step
Epoch 798/1000
2023-10-11 11:16:31.201 
Epoch 798/1000 
	 loss: 16.4195, MinusLogProbMetric: 16.4195, val_loss: 16.9816, val_MinusLogProbMetric: 16.9816

Epoch 798: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4195 - MinusLogProbMetric: 16.4195 - val_loss: 16.9816 - val_MinusLogProbMetric: 16.9816 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 799/1000
2023-10-11 11:17:18.858 
Epoch 799/1000 
	 loss: 16.4189, MinusLogProbMetric: 16.4189, val_loss: 16.9679, val_MinusLogProbMetric: 16.9679

Epoch 799: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4189 - MinusLogProbMetric: 16.4189 - val_loss: 16.9679 - val_MinusLogProbMetric: 16.9679 - lr: 5.2083e-06 - 48s/epoch - 243ms/step
Epoch 800/1000
2023-10-11 11:18:06.753 
Epoch 800/1000 
	 loss: 16.4212, MinusLogProbMetric: 16.4212, val_loss: 16.9654, val_MinusLogProbMetric: 16.9654

Epoch 800: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4212 - MinusLogProbMetric: 16.4212 - val_loss: 16.9654 - val_MinusLogProbMetric: 16.9654 - lr: 5.2083e-06 - 48s/epoch - 244ms/step
Epoch 801/1000
2023-10-11 11:18:55.146 
Epoch 801/1000 
	 loss: 16.4194, MinusLogProbMetric: 16.4194, val_loss: 16.9689, val_MinusLogProbMetric: 16.9689

Epoch 801: val_loss did not improve from 16.96140
196/196 - 48s - loss: 16.4194 - MinusLogProbMetric: 16.4194 - val_loss: 16.9689 - val_MinusLogProbMetric: 16.9689 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 802/1000
2023-10-11 11:19:45.124 
Epoch 802/1000 
	 loss: 16.4196, MinusLogProbMetric: 16.4196, val_loss: 16.9612, val_MinusLogProbMetric: 16.9612

Epoch 802: val_loss improved from 16.96140 to 16.96125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 51s - loss: 16.4196 - MinusLogProbMetric: 16.4196 - val_loss: 16.9612 - val_MinusLogProbMetric: 16.9612 - lr: 5.2083e-06 - 51s/epoch - 259ms/step
Epoch 803/1000
2023-10-11 11:20:34.098 
Epoch 803/1000 
	 loss: 16.4201, MinusLogProbMetric: 16.4201, val_loss: 16.9650, val_MinusLogProbMetric: 16.9650

Epoch 803: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4201 - MinusLogProbMetric: 16.4201 - val_loss: 16.9650 - val_MinusLogProbMetric: 16.9650 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 804/1000
2023-10-11 11:21:23.027 
Epoch 804/1000 
	 loss: 16.4198, MinusLogProbMetric: 16.4198, val_loss: 16.9662, val_MinusLogProbMetric: 16.9662

Epoch 804: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4198 - MinusLogProbMetric: 16.4198 - val_loss: 16.9662 - val_MinusLogProbMetric: 16.9662 - lr: 5.2083e-06 - 49s/epoch - 250ms/step
Epoch 805/1000
2023-10-11 11:22:11.329 
Epoch 805/1000 
	 loss: 16.4209, MinusLogProbMetric: 16.4209, val_loss: 16.9634, val_MinusLogProbMetric: 16.9634

Epoch 805: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4209 - MinusLogProbMetric: 16.4209 - val_loss: 16.9634 - val_MinusLogProbMetric: 16.9634 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 806/1000
2023-10-11 11:23:00.036 
Epoch 806/1000 
	 loss: 16.4193, MinusLogProbMetric: 16.4193, val_loss: 16.9679, val_MinusLogProbMetric: 16.9679

Epoch 806: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4193 - MinusLogProbMetric: 16.4193 - val_loss: 16.9679 - val_MinusLogProbMetric: 16.9679 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 807/1000
2023-10-11 11:23:48.872 
Epoch 807/1000 
	 loss: 16.4214, MinusLogProbMetric: 16.4214, val_loss: 16.9649, val_MinusLogProbMetric: 16.9649

Epoch 807: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4214 - MinusLogProbMetric: 16.4214 - val_loss: 16.9649 - val_MinusLogProbMetric: 16.9649 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 808/1000
2023-10-11 11:24:38.386 
Epoch 808/1000 
	 loss: 16.4187, MinusLogProbMetric: 16.4187, val_loss: 16.9650, val_MinusLogProbMetric: 16.9650

Epoch 808: val_loss did not improve from 16.96125
196/196 - 50s - loss: 16.4187 - MinusLogProbMetric: 16.4187 - val_loss: 16.9650 - val_MinusLogProbMetric: 16.9650 - lr: 5.2083e-06 - 50s/epoch - 253ms/step
Epoch 809/1000
2023-10-11 11:25:27.050 
Epoch 809/1000 
	 loss: 16.4202, MinusLogProbMetric: 16.4202, val_loss: 16.9630, val_MinusLogProbMetric: 16.9630

Epoch 809: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4202 - MinusLogProbMetric: 16.4202 - val_loss: 16.9630 - val_MinusLogProbMetric: 16.9630 - lr: 5.2083e-06 - 49s/epoch - 248ms/step
Epoch 810/1000
2023-10-11 11:26:17.220 
Epoch 810/1000 
	 loss: 16.4182, MinusLogProbMetric: 16.4182, val_loss: 16.9680, val_MinusLogProbMetric: 16.9680

Epoch 810: val_loss did not improve from 16.96125
196/196 - 50s - loss: 16.4182 - MinusLogProbMetric: 16.4182 - val_loss: 16.9680 - val_MinusLogProbMetric: 16.9680 - lr: 5.2083e-06 - 50s/epoch - 256ms/step
Epoch 811/1000
2023-10-11 11:27:05.658 
Epoch 811/1000 
	 loss: 16.4182, MinusLogProbMetric: 16.4182, val_loss: 16.9656, val_MinusLogProbMetric: 16.9656

Epoch 811: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4182 - MinusLogProbMetric: 16.4182 - val_loss: 16.9656 - val_MinusLogProbMetric: 16.9656 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 812/1000
2023-10-11 11:27:54.765 
Epoch 812/1000 
	 loss: 16.4191, MinusLogProbMetric: 16.4191, val_loss: 16.9738, val_MinusLogProbMetric: 16.9738

Epoch 812: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4191 - MinusLogProbMetric: 16.4191 - val_loss: 16.9738 - val_MinusLogProbMetric: 16.9738 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 813/1000
2023-10-11 11:28:44.138 
Epoch 813/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9646, val_MinusLogProbMetric: 16.9646

Epoch 813: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9646 - val_MinusLogProbMetric: 16.9646 - lr: 5.2083e-06 - 49s/epoch - 252ms/step
Epoch 814/1000
2023-10-11 11:29:34.193 
Epoch 814/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 814: val_loss did not improve from 16.96125
196/196 - 50s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 5.2083e-06 - 50s/epoch - 255ms/step
Epoch 815/1000
2023-10-11 11:30:23.831 
Epoch 815/1000 
	 loss: 16.4188, MinusLogProbMetric: 16.4188, val_loss: 16.9687, val_MinusLogProbMetric: 16.9687

Epoch 815: val_loss did not improve from 16.96125
196/196 - 50s - loss: 16.4188 - MinusLogProbMetric: 16.4188 - val_loss: 16.9687 - val_MinusLogProbMetric: 16.9687 - lr: 5.2083e-06 - 50s/epoch - 253ms/step
Epoch 816/1000
2023-10-11 11:31:13.215 
Epoch 816/1000 
	 loss: 16.4201, MinusLogProbMetric: 16.4201, val_loss: 16.9699, val_MinusLogProbMetric: 16.9699

Epoch 816: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4201 - MinusLogProbMetric: 16.4201 - val_loss: 16.9699 - val_MinusLogProbMetric: 16.9699 - lr: 5.2083e-06 - 49s/epoch - 252ms/step
Epoch 817/1000
2023-10-11 11:32:01.707 
Epoch 817/1000 
	 loss: 16.4192, MinusLogProbMetric: 16.4192, val_loss: 16.9653, val_MinusLogProbMetric: 16.9653

Epoch 817: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4192 - MinusLogProbMetric: 16.4192 - val_loss: 16.9653 - val_MinusLogProbMetric: 16.9653 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 818/1000
2023-10-11 11:32:49.457 
Epoch 818/1000 
	 loss: 16.4188, MinusLogProbMetric: 16.4188, val_loss: 16.9669, val_MinusLogProbMetric: 16.9669

Epoch 818: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4188 - MinusLogProbMetric: 16.4188 - val_loss: 16.9669 - val_MinusLogProbMetric: 16.9669 - lr: 5.2083e-06 - 48s/epoch - 244ms/step
Epoch 819/1000
2023-10-11 11:33:37.397 
Epoch 819/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9641, val_MinusLogProbMetric: 16.9641

Epoch 819: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9641 - val_MinusLogProbMetric: 16.9641 - lr: 5.2083e-06 - 48s/epoch - 245ms/step
Epoch 820/1000
2023-10-11 11:34:24.953 
Epoch 820/1000 
	 loss: 16.4185, MinusLogProbMetric: 16.4185, val_loss: 16.9662, val_MinusLogProbMetric: 16.9662

Epoch 820: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4185 - MinusLogProbMetric: 16.4185 - val_loss: 16.9662 - val_MinusLogProbMetric: 16.9662 - lr: 5.2083e-06 - 48s/epoch - 243ms/step
Epoch 821/1000
2023-10-11 11:35:13.106 
Epoch 821/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9639, val_MinusLogProbMetric: 16.9639

Epoch 821: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9639 - val_MinusLogProbMetric: 16.9639 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 822/1000
2023-10-11 11:36:01.544 
Epoch 822/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 822: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 823/1000
2023-10-11 11:36:49.620 
Epoch 823/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9662, val_MinusLogProbMetric: 16.9662

Epoch 823: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9662 - val_MinusLogProbMetric: 16.9662 - lr: 5.2083e-06 - 48s/epoch - 245ms/step
Epoch 824/1000
2023-10-11 11:37:38.419 
Epoch 824/1000 
	 loss: 16.4191, MinusLogProbMetric: 16.4191, val_loss: 16.9768, val_MinusLogProbMetric: 16.9768

Epoch 824: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4191 - MinusLogProbMetric: 16.4191 - val_loss: 16.9768 - val_MinusLogProbMetric: 16.9768 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 825/1000
2023-10-11 11:38:26.904 
Epoch 825/1000 
	 loss: 16.4200, MinusLogProbMetric: 16.4200, val_loss: 16.9738, val_MinusLogProbMetric: 16.9738

Epoch 825: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4200 - MinusLogProbMetric: 16.4200 - val_loss: 16.9738 - val_MinusLogProbMetric: 16.9738 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 826/1000
2023-10-11 11:39:15.686 
Epoch 826/1000 
	 loss: 16.4189, MinusLogProbMetric: 16.4189, val_loss: 16.9683, val_MinusLogProbMetric: 16.9683

Epoch 826: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4189 - MinusLogProbMetric: 16.4189 - val_loss: 16.9683 - val_MinusLogProbMetric: 16.9683 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 827/1000
2023-10-11 11:40:04.076 
Epoch 827/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9660, val_MinusLogProbMetric: 16.9660

Epoch 827: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9660 - val_MinusLogProbMetric: 16.9660 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 828/1000
2023-10-11 11:40:51.642 
Epoch 828/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 828: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 5.2083e-06 - 48s/epoch - 243ms/step
Epoch 829/1000
2023-10-11 11:41:39.589 
Epoch 829/1000 
	 loss: 16.4191, MinusLogProbMetric: 16.4191, val_loss: 16.9653, val_MinusLogProbMetric: 16.9653

Epoch 829: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4191 - MinusLogProbMetric: 16.4191 - val_loss: 16.9653 - val_MinusLogProbMetric: 16.9653 - lr: 5.2083e-06 - 48s/epoch - 245ms/step
Epoch 830/1000
2023-10-11 11:42:28.266 
Epoch 830/1000 
	 loss: 16.4175, MinusLogProbMetric: 16.4175, val_loss: 16.9729, val_MinusLogProbMetric: 16.9729

Epoch 830: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4175 - MinusLogProbMetric: 16.4175 - val_loss: 16.9729 - val_MinusLogProbMetric: 16.9729 - lr: 5.2083e-06 - 49s/epoch - 248ms/step
Epoch 831/1000
2023-10-11 11:43:16.875 
Epoch 831/1000 
	 loss: 16.4195, MinusLogProbMetric: 16.4195, val_loss: 16.9859, val_MinusLogProbMetric: 16.9859

Epoch 831: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4195 - MinusLogProbMetric: 16.4195 - val_loss: 16.9859 - val_MinusLogProbMetric: 16.9859 - lr: 5.2083e-06 - 49s/epoch - 248ms/step
Epoch 832/1000
2023-10-11 11:44:05.055 
Epoch 832/1000 
	 loss: 16.4197, MinusLogProbMetric: 16.4197, val_loss: 16.9623, val_MinusLogProbMetric: 16.9623

Epoch 832: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4197 - MinusLogProbMetric: 16.4197 - val_loss: 16.9623 - val_MinusLogProbMetric: 16.9623 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 833/1000
2023-10-11 11:44:54.569 
Epoch 833/1000 
	 loss: 16.4182, MinusLogProbMetric: 16.4182, val_loss: 16.9625, val_MinusLogProbMetric: 16.9625

Epoch 833: val_loss did not improve from 16.96125
196/196 - 50s - loss: 16.4182 - MinusLogProbMetric: 16.4182 - val_loss: 16.9625 - val_MinusLogProbMetric: 16.9625 - lr: 5.2083e-06 - 50s/epoch - 253ms/step
Epoch 834/1000
2023-10-11 11:45:43.899 
Epoch 834/1000 
	 loss: 16.4182, MinusLogProbMetric: 16.4182, val_loss: 16.9646, val_MinusLogProbMetric: 16.9646

Epoch 834: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4182 - MinusLogProbMetric: 16.4182 - val_loss: 16.9646 - val_MinusLogProbMetric: 16.9646 - lr: 5.2083e-06 - 49s/epoch - 252ms/step
Epoch 835/1000
2023-10-11 11:46:31.741 
Epoch 835/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9618, val_MinusLogProbMetric: 16.9618

Epoch 835: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9618 - val_MinusLogProbMetric: 16.9618 - lr: 5.2083e-06 - 48s/epoch - 244ms/step
Epoch 836/1000
2023-10-11 11:47:19.177 
Epoch 836/1000 
	 loss: 16.4172, MinusLogProbMetric: 16.4172, val_loss: 16.9631, val_MinusLogProbMetric: 16.9631

Epoch 836: val_loss did not improve from 16.96125
196/196 - 47s - loss: 16.4172 - MinusLogProbMetric: 16.4172 - val_loss: 16.9631 - val_MinusLogProbMetric: 16.9631 - lr: 5.2083e-06 - 47s/epoch - 242ms/step
Epoch 837/1000
2023-10-11 11:48:07.824 
Epoch 837/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9621, val_MinusLogProbMetric: 16.9621

Epoch 837: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9621 - val_MinusLogProbMetric: 16.9621 - lr: 5.2083e-06 - 49s/epoch - 248ms/step
Epoch 838/1000
2023-10-11 11:48:56.931 
Epoch 838/1000 
	 loss: 16.4182, MinusLogProbMetric: 16.4182, val_loss: 16.9672, val_MinusLogProbMetric: 16.9672

Epoch 838: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4182 - MinusLogProbMetric: 16.4182 - val_loss: 16.9672 - val_MinusLogProbMetric: 16.9672 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 839/1000
2023-10-11 11:49:44.332 
Epoch 839/1000 
	 loss: 16.4177, MinusLogProbMetric: 16.4177, val_loss: 16.9667, val_MinusLogProbMetric: 16.9667

Epoch 839: val_loss did not improve from 16.96125
196/196 - 47s - loss: 16.4177 - MinusLogProbMetric: 16.4177 - val_loss: 16.9667 - val_MinusLogProbMetric: 16.9667 - lr: 5.2083e-06 - 47s/epoch - 242ms/step
Epoch 840/1000
2023-10-11 11:50:32.547 
Epoch 840/1000 
	 loss: 16.4175, MinusLogProbMetric: 16.4175, val_loss: 16.9633, val_MinusLogProbMetric: 16.9633

Epoch 840: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4175 - MinusLogProbMetric: 16.4175 - val_loss: 16.9633 - val_MinusLogProbMetric: 16.9633 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 841/1000
2023-10-11 11:51:21.398 
Epoch 841/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9668, val_MinusLogProbMetric: 16.9668

Epoch 841: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9668 - val_MinusLogProbMetric: 16.9668 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 842/1000
2023-10-11 11:52:10.408 
Epoch 842/1000 
	 loss: 16.4173, MinusLogProbMetric: 16.4173, val_loss: 16.9649, val_MinusLogProbMetric: 16.9649

Epoch 842: val_loss did not improve from 16.96125
196/196 - 49s - loss: 16.4173 - MinusLogProbMetric: 16.4173 - val_loss: 16.9649 - val_MinusLogProbMetric: 16.9649 - lr: 5.2083e-06 - 49s/epoch - 250ms/step
Epoch 843/1000
2023-10-11 11:52:58.826 
Epoch 843/1000 
	 loss: 16.4192, MinusLogProbMetric: 16.4192, val_loss: 16.9649, val_MinusLogProbMetric: 16.9649

Epoch 843: val_loss did not improve from 16.96125
196/196 - 48s - loss: 16.4192 - MinusLogProbMetric: 16.4192 - val_loss: 16.9649 - val_MinusLogProbMetric: 16.9649 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 844/1000
2023-10-11 11:53:47.323 
Epoch 844/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9612, val_MinusLogProbMetric: 16.9612

Epoch 844: val_loss improved from 16.96125 to 16.96117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9612 - val_MinusLogProbMetric: 16.9612 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 845/1000
2023-10-11 11:54:37.361 
Epoch 845/1000 
	 loss: 16.4168, MinusLogProbMetric: 16.4168, val_loss: 16.9676, val_MinusLogProbMetric: 16.9676

Epoch 845: val_loss did not improve from 16.96117
196/196 - 49s - loss: 16.4168 - MinusLogProbMetric: 16.4168 - val_loss: 16.9676 - val_MinusLogProbMetric: 16.9676 - lr: 5.2083e-06 - 49s/epoch - 252ms/step
Epoch 846/1000
2023-10-11 11:55:26.262 
Epoch 846/1000 
	 loss: 16.4164, MinusLogProbMetric: 16.4164, val_loss: 16.9619, val_MinusLogProbMetric: 16.9619

Epoch 846: val_loss did not improve from 16.96117
196/196 - 49s - loss: 16.4164 - MinusLogProbMetric: 16.4164 - val_loss: 16.9619 - val_MinusLogProbMetric: 16.9619 - lr: 5.2083e-06 - 49s/epoch - 249ms/step
Epoch 847/1000
2023-10-11 11:56:15.488 
Epoch 847/1000 
	 loss: 16.4182, MinusLogProbMetric: 16.4182, val_loss: 16.9639, val_MinusLogProbMetric: 16.9639

Epoch 847: val_loss did not improve from 16.96117
196/196 - 49s - loss: 16.4182 - MinusLogProbMetric: 16.4182 - val_loss: 16.9639 - val_MinusLogProbMetric: 16.9639 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 848/1000
2023-10-11 11:57:04.608 
Epoch 848/1000 
	 loss: 16.4175, MinusLogProbMetric: 16.4175, val_loss: 16.9667, val_MinusLogProbMetric: 16.9667

Epoch 848: val_loss did not improve from 16.96117
196/196 - 49s - loss: 16.4175 - MinusLogProbMetric: 16.4175 - val_loss: 16.9667 - val_MinusLogProbMetric: 16.9667 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 849/1000
2023-10-11 11:57:53.713 
Epoch 849/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9668, val_MinusLogProbMetric: 16.9668

Epoch 849: val_loss did not improve from 16.96117
196/196 - 49s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9668 - val_MinusLogProbMetric: 16.9668 - lr: 5.2083e-06 - 49s/epoch - 251ms/step
Epoch 850/1000
2023-10-11 11:58:43.096 
Epoch 850/1000 
	 loss: 16.4169, MinusLogProbMetric: 16.4169, val_loss: 16.9681, val_MinusLogProbMetric: 16.9681

Epoch 850: val_loss did not improve from 16.96117
196/196 - 49s - loss: 16.4169 - MinusLogProbMetric: 16.4169 - val_loss: 16.9681 - val_MinusLogProbMetric: 16.9681 - lr: 5.2083e-06 - 49s/epoch - 252ms/step
Epoch 851/1000
2023-10-11 11:59:31.499 
Epoch 851/1000 
	 loss: 16.4168, MinusLogProbMetric: 16.4168, val_loss: 16.9642, val_MinusLogProbMetric: 16.9642

Epoch 851: val_loss did not improve from 16.96117
196/196 - 48s - loss: 16.4168 - MinusLogProbMetric: 16.4168 - val_loss: 16.9642 - val_MinusLogProbMetric: 16.9642 - lr: 5.2083e-06 - 48s/epoch - 247ms/step
Epoch 852/1000
2023-10-11 12:00:19.689 
Epoch 852/1000 
	 loss: 16.4162, MinusLogProbMetric: 16.4162, val_loss: 16.9728, val_MinusLogProbMetric: 16.9728

Epoch 852: val_loss did not improve from 16.96117
196/196 - 48s - loss: 16.4162 - MinusLogProbMetric: 16.4162 - val_loss: 16.9728 - val_MinusLogProbMetric: 16.9728 - lr: 5.2083e-06 - 48s/epoch - 246ms/step
Epoch 853/1000
2023-10-11 12:01:07.452 
Epoch 853/1000 
	 loss: 16.4135, MinusLogProbMetric: 16.4135, val_loss: 16.9587, val_MinusLogProbMetric: 16.9587

Epoch 853: val_loss improved from 16.96117 to 16.95866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4135 - MinusLogProbMetric: 16.4135 - val_loss: 16.9587 - val_MinusLogProbMetric: 16.9587 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 854/1000
2023-10-11 12:01:57.130 
Epoch 854/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 854: val_loss did not improve from 16.95866
196/196 - 49s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 855/1000
2023-10-11 12:02:45.295 
Epoch 855/1000 
	 loss: 16.4122, MinusLogProbMetric: 16.4122, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 855: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4122 - MinusLogProbMetric: 16.4122 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 856/1000
2023-10-11 12:03:33.305 
Epoch 856/1000 
	 loss: 16.4120, MinusLogProbMetric: 16.4120, val_loss: 16.9611, val_MinusLogProbMetric: 16.9611

Epoch 856: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4120 - MinusLogProbMetric: 16.4120 - val_loss: 16.9611 - val_MinusLogProbMetric: 16.9611 - lr: 2.6042e-06 - 48s/epoch - 245ms/step
Epoch 857/1000
2023-10-11 12:04:20.731 
Epoch 857/1000 
	 loss: 16.4126, MinusLogProbMetric: 16.4126, val_loss: 16.9615, val_MinusLogProbMetric: 16.9615

Epoch 857: val_loss did not improve from 16.95866
196/196 - 47s - loss: 16.4126 - MinusLogProbMetric: 16.4126 - val_loss: 16.9615 - val_MinusLogProbMetric: 16.9615 - lr: 2.6042e-06 - 47s/epoch - 242ms/step
Epoch 858/1000
2023-10-11 12:05:08.549 
Epoch 858/1000 
	 loss: 16.4127, MinusLogProbMetric: 16.4127, val_loss: 16.9603, val_MinusLogProbMetric: 16.9603

Epoch 858: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4127 - MinusLogProbMetric: 16.4127 - val_loss: 16.9603 - val_MinusLogProbMetric: 16.9603 - lr: 2.6042e-06 - 48s/epoch - 244ms/step
Epoch 859/1000
2023-10-11 12:05:58.854 
Epoch 859/1000 
	 loss: 16.4127, MinusLogProbMetric: 16.4127, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 859: val_loss did not improve from 16.95866
196/196 - 50s - loss: 16.4127 - MinusLogProbMetric: 16.4127 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 2.6042e-06 - 50s/epoch - 257ms/step
Epoch 860/1000
2023-10-11 12:06:47.233 
Epoch 860/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 860: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 2.6042e-06 - 48s/epoch - 247ms/step
Epoch 861/1000
2023-10-11 12:07:35.977 
Epoch 861/1000 
	 loss: 16.4130, MinusLogProbMetric: 16.4130, val_loss: 16.9636, val_MinusLogProbMetric: 16.9636

Epoch 861: val_loss did not improve from 16.95866
196/196 - 49s - loss: 16.4130 - MinusLogProbMetric: 16.4130 - val_loss: 16.9636 - val_MinusLogProbMetric: 16.9636 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 862/1000
2023-10-11 12:08:24.376 
Epoch 862/1000 
	 loss: 16.4129, MinusLogProbMetric: 16.4129, val_loss: 16.9656, val_MinusLogProbMetric: 16.9656

Epoch 862: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4129 - MinusLogProbMetric: 16.4129 - val_loss: 16.9656 - val_MinusLogProbMetric: 16.9656 - lr: 2.6042e-06 - 48s/epoch - 247ms/step
Epoch 863/1000
2023-10-11 12:09:12.255 
Epoch 863/1000 
	 loss: 16.4120, MinusLogProbMetric: 16.4120, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 863: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4120 - MinusLogProbMetric: 16.4120 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 2.6042e-06 - 48s/epoch - 244ms/step
Epoch 864/1000
2023-10-11 12:10:00.010 
Epoch 864/1000 
	 loss: 16.4115, MinusLogProbMetric: 16.4115, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 864: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4115 - MinusLogProbMetric: 16.4115 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 2.6042e-06 - 48s/epoch - 244ms/step
Epoch 865/1000
2023-10-11 12:10:48.054 
Epoch 865/1000 
	 loss: 16.4133, MinusLogProbMetric: 16.4133, val_loss: 16.9610, val_MinusLogProbMetric: 16.9610

Epoch 865: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4133 - MinusLogProbMetric: 16.4133 - val_loss: 16.9610 - val_MinusLogProbMetric: 16.9610 - lr: 2.6042e-06 - 48s/epoch - 245ms/step
Epoch 866/1000
2023-10-11 12:11:36.251 
Epoch 866/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9630, val_MinusLogProbMetric: 16.9630

Epoch 866: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9630 - val_MinusLogProbMetric: 16.9630 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 867/1000
2023-10-11 12:12:24.287 
Epoch 867/1000 
	 loss: 16.4129, MinusLogProbMetric: 16.4129, val_loss: 16.9600, val_MinusLogProbMetric: 16.9600

Epoch 867: val_loss did not improve from 16.95866
196/196 - 48s - loss: 16.4129 - MinusLogProbMetric: 16.4129 - val_loss: 16.9600 - val_MinusLogProbMetric: 16.9600 - lr: 2.6042e-06 - 48s/epoch - 245ms/step
Epoch 868/1000
2023-10-11 12:13:12.026 
Epoch 868/1000 
	 loss: 16.4121, MinusLogProbMetric: 16.4121, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 868: val_loss improved from 16.95866 to 16.95835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4121 - MinusLogProbMetric: 16.4121 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 869/1000
2023-10-11 12:14:01.097 
Epoch 869/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9613, val_MinusLogProbMetric: 16.9613

Epoch 869: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9613 - val_MinusLogProbMetric: 16.9613 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 870/1000
2023-10-11 12:14:49.652 
Epoch 870/1000 
	 loss: 16.4118, MinusLogProbMetric: 16.4118, val_loss: 16.9590, val_MinusLogProbMetric: 16.9590

Epoch 870: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4118 - MinusLogProbMetric: 16.4118 - val_loss: 16.9590 - val_MinusLogProbMetric: 16.9590 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 871/1000
2023-10-11 12:15:38.395 
Epoch 871/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9626, val_MinusLogProbMetric: 16.9626

Epoch 871: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9626 - val_MinusLogProbMetric: 16.9626 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 872/1000
2023-10-11 12:16:26.719 
Epoch 872/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9645, val_MinusLogProbMetric: 16.9645

Epoch 872: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9645 - val_MinusLogProbMetric: 16.9645 - lr: 2.6042e-06 - 48s/epoch - 247ms/step
Epoch 873/1000
2023-10-11 12:17:15.463 
Epoch 873/1000 
	 loss: 16.4119, MinusLogProbMetric: 16.4119, val_loss: 16.9586, val_MinusLogProbMetric: 16.9586

Epoch 873: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4119 - MinusLogProbMetric: 16.4119 - val_loss: 16.9586 - val_MinusLogProbMetric: 16.9586 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 874/1000
2023-10-11 12:18:03.585 
Epoch 874/1000 
	 loss: 16.4114, MinusLogProbMetric: 16.4114, val_loss: 16.9600, val_MinusLogProbMetric: 16.9600

Epoch 874: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4114 - MinusLogProbMetric: 16.4114 - val_loss: 16.9600 - val_MinusLogProbMetric: 16.9600 - lr: 2.6042e-06 - 48s/epoch - 245ms/step
Epoch 875/1000
2023-10-11 12:18:52.324 
Epoch 875/1000 
	 loss: 16.4121, MinusLogProbMetric: 16.4121, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 875: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4121 - MinusLogProbMetric: 16.4121 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 876/1000
2023-10-11 12:19:41.278 
Epoch 876/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9616, val_MinusLogProbMetric: 16.9616

Epoch 876: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9616 - val_MinusLogProbMetric: 16.9616 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 877/1000
2023-10-11 12:20:30.194 
Epoch 877/1000 
	 loss: 16.4117, MinusLogProbMetric: 16.4117, val_loss: 16.9637, val_MinusLogProbMetric: 16.9637

Epoch 877: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4117 - MinusLogProbMetric: 16.4117 - val_loss: 16.9637 - val_MinusLogProbMetric: 16.9637 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 878/1000
2023-10-11 12:21:17.716 
Epoch 878/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9593, val_MinusLogProbMetric: 16.9593

Epoch 878: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9593 - val_MinusLogProbMetric: 16.9593 - lr: 2.6042e-06 - 48s/epoch - 242ms/step
Epoch 879/1000
2023-10-11 12:22:06.221 
Epoch 879/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9596, val_MinusLogProbMetric: 16.9596

Epoch 879: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9596 - val_MinusLogProbMetric: 16.9596 - lr: 2.6042e-06 - 49s/epoch - 247ms/step
Epoch 880/1000
2023-10-11 12:22:54.836 
Epoch 880/1000 
	 loss: 16.4132, MinusLogProbMetric: 16.4132, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 880: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4132 - MinusLogProbMetric: 16.4132 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 881/1000
2023-10-11 12:23:43.832 
Epoch 881/1000 
	 loss: 16.4120, MinusLogProbMetric: 16.4120, val_loss: 16.9586, val_MinusLogProbMetric: 16.9586

Epoch 881: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4120 - MinusLogProbMetric: 16.4120 - val_loss: 16.9586 - val_MinusLogProbMetric: 16.9586 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 882/1000
2023-10-11 12:24:32.538 
Epoch 882/1000 
	 loss: 16.4128, MinusLogProbMetric: 16.4128, val_loss: 16.9631, val_MinusLogProbMetric: 16.9631

Epoch 882: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4128 - MinusLogProbMetric: 16.4128 - val_loss: 16.9631 - val_MinusLogProbMetric: 16.9631 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 883/1000
2023-10-11 12:25:20.802 
Epoch 883/1000 
	 loss: 16.4120, MinusLogProbMetric: 16.4120, val_loss: 16.9638, val_MinusLogProbMetric: 16.9638

Epoch 883: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4120 - MinusLogProbMetric: 16.4120 - val_loss: 16.9638 - val_MinusLogProbMetric: 16.9638 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 884/1000
2023-10-11 12:26:09.022 
Epoch 884/1000 
	 loss: 16.4118, MinusLogProbMetric: 16.4118, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 884: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4118 - MinusLogProbMetric: 16.4118 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 885/1000
2023-10-11 12:26:57.882 
Epoch 885/1000 
	 loss: 16.4118, MinusLogProbMetric: 16.4118, val_loss: 16.9681, val_MinusLogProbMetric: 16.9681

Epoch 885: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4118 - MinusLogProbMetric: 16.4118 - val_loss: 16.9681 - val_MinusLogProbMetric: 16.9681 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 886/1000
2023-10-11 12:27:47.287 
Epoch 886/1000 
	 loss: 16.4130, MinusLogProbMetric: 16.4130, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 886: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4130 - MinusLogProbMetric: 16.4130 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 2.6042e-06 - 49s/epoch - 252ms/step
Epoch 887/1000
2023-10-11 12:28:37.533 
Epoch 887/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9617, val_MinusLogProbMetric: 16.9617

Epoch 887: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9617 - val_MinusLogProbMetric: 16.9617 - lr: 2.6042e-06 - 50s/epoch - 256ms/step
Epoch 888/1000
2023-10-11 12:29:26.892 
Epoch 888/1000 
	 loss: 16.4125, MinusLogProbMetric: 16.4125, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 888: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4125 - MinusLogProbMetric: 16.4125 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 2.6042e-06 - 49s/epoch - 252ms/step
Epoch 889/1000
2023-10-11 12:30:16.771 
Epoch 889/1000 
	 loss: 16.4137, MinusLogProbMetric: 16.4137, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 889: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4137 - MinusLogProbMetric: 16.4137 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 2.6042e-06 - 50s/epoch - 254ms/step
Epoch 890/1000
2023-10-11 12:31:05.912 
Epoch 890/1000 
	 loss: 16.4126, MinusLogProbMetric: 16.4126, val_loss: 16.9693, val_MinusLogProbMetric: 16.9693

Epoch 890: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4126 - MinusLogProbMetric: 16.4126 - val_loss: 16.9693 - val_MinusLogProbMetric: 16.9693 - lr: 2.6042e-06 - 49s/epoch - 251ms/step
Epoch 891/1000
2023-10-11 12:31:53.910 
Epoch 891/1000 
	 loss: 16.4127, MinusLogProbMetric: 16.4127, val_loss: 16.9657, val_MinusLogProbMetric: 16.9657

Epoch 891: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4127 - MinusLogProbMetric: 16.4127 - val_loss: 16.9657 - val_MinusLogProbMetric: 16.9657 - lr: 2.6042e-06 - 48s/epoch - 245ms/step
Epoch 892/1000
2023-10-11 12:32:42.079 
Epoch 892/1000 
	 loss: 16.4128, MinusLogProbMetric: 16.4128, val_loss: 16.9626, val_MinusLogProbMetric: 16.9626

Epoch 892: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4128 - MinusLogProbMetric: 16.4128 - val_loss: 16.9626 - val_MinusLogProbMetric: 16.9626 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 893/1000
2023-10-11 12:33:33.053 
Epoch 893/1000 
	 loss: 16.4132, MinusLogProbMetric: 16.4132, val_loss: 16.9602, val_MinusLogProbMetric: 16.9602

Epoch 893: val_loss did not improve from 16.95835
196/196 - 51s - loss: 16.4132 - MinusLogProbMetric: 16.4132 - val_loss: 16.9602 - val_MinusLogProbMetric: 16.9602 - lr: 2.6042e-06 - 51s/epoch - 260ms/step
Epoch 894/1000
2023-10-11 12:34:22.725 
Epoch 894/1000 
	 loss: 16.4124, MinusLogProbMetric: 16.4124, val_loss: 16.9587, val_MinusLogProbMetric: 16.9587

Epoch 894: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4124 - MinusLogProbMetric: 16.4124 - val_loss: 16.9587 - val_MinusLogProbMetric: 16.9587 - lr: 2.6042e-06 - 50s/epoch - 253ms/step
Epoch 895/1000
2023-10-11 12:35:11.842 
Epoch 895/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9623, val_MinusLogProbMetric: 16.9623

Epoch 895: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9623 - val_MinusLogProbMetric: 16.9623 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 896/1000
2023-10-11 12:35:59.099 
Epoch 896/1000 
	 loss: 16.4127, MinusLogProbMetric: 16.4127, val_loss: 16.9602, val_MinusLogProbMetric: 16.9602

Epoch 896: val_loss did not improve from 16.95835
196/196 - 47s - loss: 16.4127 - MinusLogProbMetric: 16.4127 - val_loss: 16.9602 - val_MinusLogProbMetric: 16.9602 - lr: 2.6042e-06 - 47s/epoch - 241ms/step
Epoch 897/1000
2023-10-11 12:36:47.704 
Epoch 897/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9603, val_MinusLogProbMetric: 16.9603

Epoch 897: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9603 - val_MinusLogProbMetric: 16.9603 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 898/1000
2023-10-11 12:37:35.640 
Epoch 898/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 16.9595, val_MinusLogProbMetric: 16.9595

Epoch 898: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 16.9595 - val_MinusLogProbMetric: 16.9595 - lr: 2.6042e-06 - 48s/epoch - 245ms/step
Epoch 899/1000
2023-10-11 12:38:24.431 
Epoch 899/1000 
	 loss: 16.4122, MinusLogProbMetric: 16.4122, val_loss: 16.9611, val_MinusLogProbMetric: 16.9611

Epoch 899: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4122 - MinusLogProbMetric: 16.4122 - val_loss: 16.9611 - val_MinusLogProbMetric: 16.9611 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 900/1000
2023-10-11 12:39:14.511 
Epoch 900/1000 
	 loss: 16.4115, MinusLogProbMetric: 16.4115, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 900: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4115 - MinusLogProbMetric: 16.4115 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 2.6042e-06 - 50s/epoch - 256ms/step
Epoch 901/1000
2023-10-11 12:40:04.303 
Epoch 901/1000 
	 loss: 16.4119, MinusLogProbMetric: 16.4119, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 901: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4119 - MinusLogProbMetric: 16.4119 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 2.6042e-06 - 50s/epoch - 254ms/step
Epoch 902/1000
2023-10-11 12:40:53.956 
Epoch 902/1000 
	 loss: 16.4129, MinusLogProbMetric: 16.4129, val_loss: 16.9600, val_MinusLogProbMetric: 16.9600

Epoch 902: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4129 - MinusLogProbMetric: 16.4129 - val_loss: 16.9600 - val_MinusLogProbMetric: 16.9600 - lr: 2.6042e-06 - 50s/epoch - 253ms/step
Epoch 903/1000
2023-10-11 12:41:43.375 
Epoch 903/1000 
	 loss: 16.4114, MinusLogProbMetric: 16.4114, val_loss: 16.9608, val_MinusLogProbMetric: 16.9608

Epoch 903: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4114 - MinusLogProbMetric: 16.4114 - val_loss: 16.9608 - val_MinusLogProbMetric: 16.9608 - lr: 2.6042e-06 - 49s/epoch - 252ms/step
Epoch 904/1000
2023-10-11 12:42:31.216 
Epoch 904/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9659, val_MinusLogProbMetric: 16.9659

Epoch 904: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9659 - val_MinusLogProbMetric: 16.9659 - lr: 2.6042e-06 - 48s/epoch - 244ms/step
Epoch 905/1000
2023-10-11 12:43:19.076 
Epoch 905/1000 
	 loss: 16.4117, MinusLogProbMetric: 16.4117, val_loss: 16.9608, val_MinusLogProbMetric: 16.9608

Epoch 905: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4117 - MinusLogProbMetric: 16.4117 - val_loss: 16.9608 - val_MinusLogProbMetric: 16.9608 - lr: 2.6042e-06 - 48s/epoch - 244ms/step
Epoch 906/1000
2023-10-11 12:44:07.815 
Epoch 906/1000 
	 loss: 16.4113, MinusLogProbMetric: 16.4113, val_loss: 16.9611, val_MinusLogProbMetric: 16.9611

Epoch 906: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4113 - MinusLogProbMetric: 16.4113 - val_loss: 16.9611 - val_MinusLogProbMetric: 16.9611 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 907/1000
2023-10-11 12:44:56.884 
Epoch 907/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9629, val_MinusLogProbMetric: 16.9629

Epoch 907: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9629 - val_MinusLogProbMetric: 16.9629 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 908/1000
2023-10-11 12:45:46.856 
Epoch 908/1000 
	 loss: 16.4108, MinusLogProbMetric: 16.4108, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 908: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4108 - MinusLogProbMetric: 16.4108 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 2.6042e-06 - 50s/epoch - 255ms/step
Epoch 909/1000
2023-10-11 12:46:36.022 
Epoch 909/1000 
	 loss: 16.4111, MinusLogProbMetric: 16.4111, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 909: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4111 - MinusLogProbMetric: 16.4111 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 2.6042e-06 - 49s/epoch - 251ms/step
Epoch 910/1000
2023-10-11 12:47:24.185 
Epoch 910/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9607, val_MinusLogProbMetric: 16.9607

Epoch 910: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9607 - val_MinusLogProbMetric: 16.9607 - lr: 2.6042e-06 - 48s/epoch - 246ms/step
Epoch 911/1000
2023-10-11 12:48:12.766 
Epoch 911/1000 
	 loss: 16.4122, MinusLogProbMetric: 16.4122, val_loss: 16.9622, val_MinusLogProbMetric: 16.9622

Epoch 911: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4122 - MinusLogProbMetric: 16.4122 - val_loss: 16.9622 - val_MinusLogProbMetric: 16.9622 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 912/1000
2023-10-11 12:49:01.702 
Epoch 912/1000 
	 loss: 16.4117, MinusLogProbMetric: 16.4117, val_loss: 16.9590, val_MinusLogProbMetric: 16.9590

Epoch 912: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4117 - MinusLogProbMetric: 16.4117 - val_loss: 16.9590 - val_MinusLogProbMetric: 16.9590 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 913/1000
2023-10-11 12:49:50.823 
Epoch 913/1000 
	 loss: 16.4103, MinusLogProbMetric: 16.4103, val_loss: 16.9637, val_MinusLogProbMetric: 16.9637

Epoch 913: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4103 - MinusLogProbMetric: 16.4103 - val_loss: 16.9637 - val_MinusLogProbMetric: 16.9637 - lr: 2.6042e-06 - 49s/epoch - 251ms/step
Epoch 914/1000
2023-10-11 12:50:39.730 
Epoch 914/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9600, val_MinusLogProbMetric: 16.9600

Epoch 914: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9600 - val_MinusLogProbMetric: 16.9600 - lr: 2.6042e-06 - 49s/epoch - 249ms/step
Epoch 915/1000
2023-10-11 12:51:28.397 
Epoch 915/1000 
	 loss: 16.4103, MinusLogProbMetric: 16.4103, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 915: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4103 - MinusLogProbMetric: 16.4103 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 2.6042e-06 - 49s/epoch - 248ms/step
Epoch 916/1000
2023-10-11 12:52:16.750 
Epoch 916/1000 
	 loss: 16.4106, MinusLogProbMetric: 16.4106, val_loss: 16.9606, val_MinusLogProbMetric: 16.9606

Epoch 916: val_loss did not improve from 16.95835
196/196 - 48s - loss: 16.4106 - MinusLogProbMetric: 16.4106 - val_loss: 16.9606 - val_MinusLogProbMetric: 16.9606 - lr: 2.6042e-06 - 48s/epoch - 247ms/step
Epoch 917/1000
2023-10-11 12:53:06.151 
Epoch 917/1000 
	 loss: 16.4109, MinusLogProbMetric: 16.4109, val_loss: 16.9624, val_MinusLogProbMetric: 16.9624

Epoch 917: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4109 - MinusLogProbMetric: 16.4109 - val_loss: 16.9624 - val_MinusLogProbMetric: 16.9624 - lr: 2.6042e-06 - 49s/epoch - 252ms/step
Epoch 918/1000
2023-10-11 12:53:55.248 
Epoch 918/1000 
	 loss: 16.4129, MinusLogProbMetric: 16.4129, val_loss: 16.9588, val_MinusLogProbMetric: 16.9588

Epoch 918: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4129 - MinusLogProbMetric: 16.4129 - val_loss: 16.9588 - val_MinusLogProbMetric: 16.9588 - lr: 2.6042e-06 - 49s/epoch - 250ms/step
Epoch 919/1000
2023-10-11 12:54:44.738 
Epoch 919/1000 
	 loss: 16.4090, MinusLogProbMetric: 16.4090, val_loss: 16.9590, val_MinusLogProbMetric: 16.9590

Epoch 919: val_loss did not improve from 16.95835
196/196 - 49s - loss: 16.4090 - MinusLogProbMetric: 16.4090 - val_loss: 16.9590 - val_MinusLogProbMetric: 16.9590 - lr: 1.3021e-06 - 49s/epoch - 252ms/step
Epoch 920/1000
2023-10-11 12:55:34.532 
Epoch 920/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 920: val_loss did not improve from 16.95835
196/196 - 50s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 1.3021e-06 - 50s/epoch - 254ms/step
Epoch 921/1000
2023-10-11 12:56:23.054 
Epoch 921/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9578, val_MinusLogProbMetric: 16.9578

Epoch 921: val_loss improved from 16.95835 to 16.95784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9578 - val_MinusLogProbMetric: 16.9578 - lr: 1.3021e-06 - 49s/epoch - 252ms/step
Epoch 922/1000
2023-10-11 12:57:11.972 
Epoch 922/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9602, val_MinusLogProbMetric: 16.9602

Epoch 922: val_loss did not improve from 16.95784
196/196 - 48s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9602 - val_MinusLogProbMetric: 16.9602 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 923/1000
2023-10-11 12:58:00.011 
Epoch 923/1000 
	 loss: 16.4088, MinusLogProbMetric: 16.4088, val_loss: 16.9576, val_MinusLogProbMetric: 16.9576

Epoch 923: val_loss improved from 16.95784 to 16.95759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4088 - MinusLogProbMetric: 16.4088 - val_loss: 16.9576 - val_MinusLogProbMetric: 16.9576 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 924/1000
2023-10-11 12:58:48.057 
Epoch 924/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9594, val_MinusLogProbMetric: 16.9594

Epoch 924: val_loss did not improve from 16.95759
196/196 - 47s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9594 - val_MinusLogProbMetric: 16.9594 - lr: 1.3021e-06 - 47s/epoch - 241ms/step
Epoch 925/1000
2023-10-11 12:59:36.253 
Epoch 925/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9588, val_MinusLogProbMetric: 16.9588

Epoch 925: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9588 - val_MinusLogProbMetric: 16.9588 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 926/1000
2023-10-11 13:00:24.237 
Epoch 926/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 926: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 927/1000
2023-10-11 13:01:12.435 
Epoch 927/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9588, val_MinusLogProbMetric: 16.9588

Epoch 927: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9588 - val_MinusLogProbMetric: 16.9588 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 928/1000
2023-10-11 13:02:00.845 
Epoch 928/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9595, val_MinusLogProbMetric: 16.9595

Epoch 928: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9595 - val_MinusLogProbMetric: 16.9595 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 929/1000
2023-10-11 13:02:49.235 
Epoch 929/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9594, val_MinusLogProbMetric: 16.9594

Epoch 929: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9594 - val_MinusLogProbMetric: 16.9594 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 930/1000
2023-10-11 13:03:37.224 
Epoch 930/1000 
	 loss: 16.4089, MinusLogProbMetric: 16.4089, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 930: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4089 - MinusLogProbMetric: 16.4089 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 931/1000
2023-10-11 13:04:25.215 
Epoch 931/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 931: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 932/1000
2023-10-11 13:05:13.091 
Epoch 932/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9597, val_MinusLogProbMetric: 16.9597

Epoch 932: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9597 - val_MinusLogProbMetric: 16.9597 - lr: 1.3021e-06 - 48s/epoch - 244ms/step
Epoch 933/1000
2023-10-11 13:06:01.471 
Epoch 933/1000 
	 loss: 16.4089, MinusLogProbMetric: 16.4089, val_loss: 16.9592, val_MinusLogProbMetric: 16.9592

Epoch 933: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4089 - MinusLogProbMetric: 16.4089 - val_loss: 16.9592 - val_MinusLogProbMetric: 16.9592 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 934/1000
2023-10-11 13:06:49.288 
Epoch 934/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9583, val_MinusLogProbMetric: 16.9583

Epoch 934: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9583 - val_MinusLogProbMetric: 16.9583 - lr: 1.3021e-06 - 48s/epoch - 244ms/step
Epoch 935/1000
2023-10-11 13:07:38.567 
Epoch 935/1000 
	 loss: 16.4081, MinusLogProbMetric: 16.4081, val_loss: 16.9583, val_MinusLogProbMetric: 16.9583

Epoch 935: val_loss did not improve from 16.95759
196/196 - 49s - loss: 16.4081 - MinusLogProbMetric: 16.4081 - val_loss: 16.9583 - val_MinusLogProbMetric: 16.9583 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 936/1000
2023-10-11 13:08:26.609 
Epoch 936/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9624, val_MinusLogProbMetric: 16.9624

Epoch 936: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9624 - val_MinusLogProbMetric: 16.9624 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 937/1000
2023-10-11 13:09:14.905 
Epoch 937/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9602, val_MinusLogProbMetric: 16.9602

Epoch 937: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9602 - val_MinusLogProbMetric: 16.9602 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 938/1000
2023-10-11 13:10:02.809 
Epoch 938/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9590, val_MinusLogProbMetric: 16.9590

Epoch 938: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9590 - val_MinusLogProbMetric: 16.9590 - lr: 1.3021e-06 - 48s/epoch - 244ms/step
Epoch 939/1000
2023-10-11 13:10:50.125 
Epoch 939/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9591, val_MinusLogProbMetric: 16.9591

Epoch 939: val_loss did not improve from 16.95759
196/196 - 47s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9591 - val_MinusLogProbMetric: 16.9591 - lr: 1.3021e-06 - 47s/epoch - 241ms/step
Epoch 940/1000
2023-10-11 13:11:37.652 
Epoch 940/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9579, val_MinusLogProbMetric: 16.9579

Epoch 940: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9579 - val_MinusLogProbMetric: 16.9579 - lr: 1.3021e-06 - 48s/epoch - 243ms/step
Epoch 941/1000
2023-10-11 13:12:25.613 
Epoch 941/1000 
	 loss: 16.4081, MinusLogProbMetric: 16.4081, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 941: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4081 - MinusLogProbMetric: 16.4081 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 942/1000
2023-10-11 13:13:14.675 
Epoch 942/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9607, val_MinusLogProbMetric: 16.9607

Epoch 942: val_loss did not improve from 16.95759
196/196 - 49s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9607 - val_MinusLogProbMetric: 16.9607 - lr: 1.3021e-06 - 49s/epoch - 250ms/step
Epoch 943/1000
2023-10-11 13:14:02.584 
Epoch 943/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 943: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.3021e-06 - 48s/epoch - 244ms/step
Epoch 944/1000
2023-10-11 13:14:50.964 
Epoch 944/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9591, val_MinusLogProbMetric: 16.9591

Epoch 944: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9591 - val_MinusLogProbMetric: 16.9591 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 945/1000
2023-10-11 13:15:38.417 
Epoch 945/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9603, val_MinusLogProbMetric: 16.9603

Epoch 945: val_loss did not improve from 16.95759
196/196 - 47s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9603 - val_MinusLogProbMetric: 16.9603 - lr: 1.3021e-06 - 47s/epoch - 242ms/step
Epoch 946/1000
2023-10-11 13:16:26.228 
Epoch 946/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9585, val_MinusLogProbMetric: 16.9585

Epoch 946: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9585 - val_MinusLogProbMetric: 16.9585 - lr: 1.3021e-06 - 48s/epoch - 244ms/step
Epoch 947/1000
2023-10-11 13:17:14.527 
Epoch 947/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9620, val_MinusLogProbMetric: 16.9620

Epoch 947: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9620 - val_MinusLogProbMetric: 16.9620 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 948/1000
2023-10-11 13:18:02.296 
Epoch 948/1000 
	 loss: 16.4077, MinusLogProbMetric: 16.4077, val_loss: 16.9587, val_MinusLogProbMetric: 16.9587

Epoch 948: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4077 - MinusLogProbMetric: 16.4077 - val_loss: 16.9587 - val_MinusLogProbMetric: 16.9587 - lr: 1.3021e-06 - 48s/epoch - 244ms/step
Epoch 949/1000
2023-10-11 13:18:50.695 
Epoch 949/1000 
	 loss: 16.4079, MinusLogProbMetric: 16.4079, val_loss: 16.9596, val_MinusLogProbMetric: 16.9596

Epoch 949: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4079 - MinusLogProbMetric: 16.4079 - val_loss: 16.9596 - val_MinusLogProbMetric: 16.9596 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 950/1000
2023-10-11 13:19:39.268 
Epoch 950/1000 
	 loss: 16.4075, MinusLogProbMetric: 16.4075, val_loss: 16.9585, val_MinusLogProbMetric: 16.9585

Epoch 950: val_loss did not improve from 16.95759
196/196 - 49s - loss: 16.4075 - MinusLogProbMetric: 16.4075 - val_loss: 16.9585 - val_MinusLogProbMetric: 16.9585 - lr: 1.3021e-06 - 49s/epoch - 248ms/step
Epoch 951/1000
2023-10-11 13:20:27.387 
Epoch 951/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9590, val_MinusLogProbMetric: 16.9590

Epoch 951: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9590 - val_MinusLogProbMetric: 16.9590 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 952/1000
2023-10-11 13:21:16.239 
Epoch 952/1000 
	 loss: 16.4086, MinusLogProbMetric: 16.4086, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 952: val_loss did not improve from 16.95759
196/196 - 49s - loss: 16.4086 - MinusLogProbMetric: 16.4086 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 953/1000
2023-10-11 13:22:03.963 
Epoch 953/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9579, val_MinusLogProbMetric: 16.9579

Epoch 953: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9579 - val_MinusLogProbMetric: 16.9579 - lr: 1.3021e-06 - 48s/epoch - 243ms/step
Epoch 954/1000
2023-10-11 13:22:51.964 
Epoch 954/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9589, val_MinusLogProbMetric: 16.9589

Epoch 954: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9589 - val_MinusLogProbMetric: 16.9589 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 955/1000
2023-10-11 13:23:40.101 
Epoch 955/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 955: val_loss did not improve from 16.95759
196/196 - 48s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 956/1000
2023-10-11 13:24:27.925 
Epoch 956/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9575, val_MinusLogProbMetric: 16.9575

Epoch 956: val_loss improved from 16.95759 to 16.95749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 48s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9575 - val_MinusLogProbMetric: 16.9575 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 957/1000
2023-10-11 13:25:16.989 
Epoch 957/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 957: val_loss did not improve from 16.95749
196/196 - 48s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 958/1000
2023-10-11 13:26:05.988 
Epoch 958/1000 
	 loss: 16.4074, MinusLogProbMetric: 16.4074, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 958: val_loss did not improve from 16.95749
196/196 - 49s - loss: 16.4074 - MinusLogProbMetric: 16.4074 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.3021e-06 - 49s/epoch - 250ms/step
Epoch 959/1000
2023-10-11 13:26:55.182 
Epoch 959/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9602, val_MinusLogProbMetric: 16.9602

Epoch 959: val_loss did not improve from 16.95749
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9602 - val_MinusLogProbMetric: 16.9602 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 960/1000
2023-10-11 13:27:43.939 
Epoch 960/1000 
	 loss: 16.4079, MinusLogProbMetric: 16.4079, val_loss: 16.9586, val_MinusLogProbMetric: 16.9586

Epoch 960: val_loss did not improve from 16.95749
196/196 - 49s - loss: 16.4079 - MinusLogProbMetric: 16.4079 - val_loss: 16.9586 - val_MinusLogProbMetric: 16.9586 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 961/1000
2023-10-11 13:28:32.113 
Epoch 961/1000 
	 loss: 16.4079, MinusLogProbMetric: 16.4079, val_loss: 16.9606, val_MinusLogProbMetric: 16.9606

Epoch 961: val_loss did not improve from 16.95749
196/196 - 48s - loss: 16.4079 - MinusLogProbMetric: 16.4079 - val_loss: 16.9606 - val_MinusLogProbMetric: 16.9606 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 962/1000
2023-10-11 13:29:19.467 
Epoch 962/1000 
	 loss: 16.4069, MinusLogProbMetric: 16.4069, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 962: val_loss did not improve from 16.95749
196/196 - 47s - loss: 16.4069 - MinusLogProbMetric: 16.4069 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 1.3021e-06 - 47s/epoch - 242ms/step
Epoch 963/1000
2023-10-11 13:30:07.938 
Epoch 963/1000 
	 loss: 16.4073, MinusLogProbMetric: 16.4073, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 963: val_loss did not improve from 16.95749
196/196 - 48s - loss: 16.4073 - MinusLogProbMetric: 16.4073 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 964/1000
2023-10-11 13:30:55.403 
Epoch 964/1000 
	 loss: 16.4087, MinusLogProbMetric: 16.4087, val_loss: 16.9586, val_MinusLogProbMetric: 16.9586

Epoch 964: val_loss did not improve from 16.95749
196/196 - 47s - loss: 16.4087 - MinusLogProbMetric: 16.4087 - val_loss: 16.9586 - val_MinusLogProbMetric: 16.9586 - lr: 1.3021e-06 - 47s/epoch - 242ms/step
Epoch 965/1000
2023-10-11 13:31:43.275 
Epoch 965/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9575, val_MinusLogProbMetric: 16.9575

Epoch 965: val_loss improved from 16.95749 to 16.95748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9575 - val_MinusLogProbMetric: 16.9575 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 966/1000
2023-10-11 13:32:32.681 
Epoch 966/1000 
	 loss: 16.4075, MinusLogProbMetric: 16.4075, val_loss: 16.9583, val_MinusLogProbMetric: 16.9583

Epoch 966: val_loss did not improve from 16.95748
196/196 - 49s - loss: 16.4075 - MinusLogProbMetric: 16.4075 - val_loss: 16.9583 - val_MinusLogProbMetric: 16.9583 - lr: 1.3021e-06 - 49s/epoch - 248ms/step
Epoch 967/1000
2023-10-11 13:33:21.879 
Epoch 967/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9575, val_MinusLogProbMetric: 16.9575

Epoch 967: val_loss did not improve from 16.95748
196/196 - 49s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9575 - val_MinusLogProbMetric: 16.9575 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 968/1000
2023-10-11 13:34:12.156 
Epoch 968/1000 
	 loss: 16.4079, MinusLogProbMetric: 16.4079, val_loss: 16.9568, val_MinusLogProbMetric: 16.9568

Epoch 968: val_loss improved from 16.95748 to 16.95678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 51s - loss: 16.4079 - MinusLogProbMetric: 16.4079 - val_loss: 16.9568 - val_MinusLogProbMetric: 16.9568 - lr: 1.3021e-06 - 51s/epoch - 261ms/step
Epoch 969/1000
2023-10-11 13:35:01.597 
Epoch 969/1000 
	 loss: 16.4075, MinusLogProbMetric: 16.4075, val_loss: 16.9564, val_MinusLogProbMetric: 16.9564

Epoch 969: val_loss improved from 16.95678 to 16.95645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 49s - loss: 16.4075 - MinusLogProbMetric: 16.4075 - val_loss: 16.9564 - val_MinusLogProbMetric: 16.9564 - lr: 1.3021e-06 - 49s/epoch - 252ms/step
Epoch 970/1000
2023-10-11 13:35:50.355 
Epoch 970/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9599, val_MinusLogProbMetric: 16.9599

Epoch 970: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9599 - val_MinusLogProbMetric: 16.9599 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 971/1000
2023-10-11 13:36:38.776 
Epoch 971/1000 
	 loss: 16.4074, MinusLogProbMetric: 16.4074, val_loss: 16.9592, val_MinusLogProbMetric: 16.9592

Epoch 971: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4074 - MinusLogProbMetric: 16.4074 - val_loss: 16.9592 - val_MinusLogProbMetric: 16.9592 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 972/1000
2023-10-11 13:37:27.673 
Epoch 972/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9577, val_MinusLogProbMetric: 16.9577

Epoch 972: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9577 - val_MinusLogProbMetric: 16.9577 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 973/1000
2023-10-11 13:38:16.076 
Epoch 973/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9610, val_MinusLogProbMetric: 16.9610

Epoch 973: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9610 - val_MinusLogProbMetric: 16.9610 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 974/1000
2023-10-11 13:39:04.414 
Epoch 974/1000 
	 loss: 16.4075, MinusLogProbMetric: 16.4075, val_loss: 16.9576, val_MinusLogProbMetric: 16.9576

Epoch 974: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4075 - MinusLogProbMetric: 16.4075 - val_loss: 16.9576 - val_MinusLogProbMetric: 16.9576 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 975/1000
2023-10-11 13:39:52.867 
Epoch 975/1000 
	 loss: 16.4077, MinusLogProbMetric: 16.4077, val_loss: 16.9583, val_MinusLogProbMetric: 16.9583

Epoch 975: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4077 - MinusLogProbMetric: 16.4077 - val_loss: 16.9583 - val_MinusLogProbMetric: 16.9583 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 976/1000
2023-10-11 13:40:41.925 
Epoch 976/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9599, val_MinusLogProbMetric: 16.9599

Epoch 976: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9599 - val_MinusLogProbMetric: 16.9599 - lr: 1.3021e-06 - 49s/epoch - 250ms/step
Epoch 977/1000
2023-10-11 13:41:31.155 
Epoch 977/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9580, val_MinusLogProbMetric: 16.9580

Epoch 977: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9580 - val_MinusLogProbMetric: 16.9580 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 978/1000
2023-10-11 13:42:19.406 
Epoch 978/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 978: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 979/1000
2023-10-11 13:43:07.385 
Epoch 979/1000 
	 loss: 16.4077, MinusLogProbMetric: 16.4077, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 979: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4077 - MinusLogProbMetric: 16.4077 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 980/1000
2023-10-11 13:43:57.021 
Epoch 980/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9597, val_MinusLogProbMetric: 16.9597

Epoch 980: val_loss did not improve from 16.95645
196/196 - 50s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9597 - val_MinusLogProbMetric: 16.9597 - lr: 1.3021e-06 - 50s/epoch - 253ms/step
Epoch 981/1000
2023-10-11 13:44:45.150 
Epoch 981/1000 
	 loss: 16.4072, MinusLogProbMetric: 16.4072, val_loss: 16.9608, val_MinusLogProbMetric: 16.9608

Epoch 981: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4072 - MinusLogProbMetric: 16.4072 - val_loss: 16.9608 - val_MinusLogProbMetric: 16.9608 - lr: 1.3021e-06 - 48s/epoch - 246ms/step
Epoch 982/1000
2023-10-11 13:45:34.031 
Epoch 982/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 982: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 983/1000
2023-10-11 13:46:23.245 
Epoch 983/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 983: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 984/1000
2023-10-11 13:47:12.742 
Epoch 984/1000 
	 loss: 16.4073, MinusLogProbMetric: 16.4073, val_loss: 16.9597, val_MinusLogProbMetric: 16.9597

Epoch 984: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4073 - MinusLogProbMetric: 16.4073 - val_loss: 16.9597 - val_MinusLogProbMetric: 16.9597 - lr: 1.3021e-06 - 49s/epoch - 253ms/step
Epoch 985/1000
2023-10-11 13:48:01.960 
Epoch 985/1000 
	 loss: 16.4077, MinusLogProbMetric: 16.4077, val_loss: 16.9575, val_MinusLogProbMetric: 16.9575

Epoch 985: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4077 - MinusLogProbMetric: 16.4077 - val_loss: 16.9575 - val_MinusLogProbMetric: 16.9575 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 986/1000
2023-10-11 13:48:50.434 
Epoch 986/1000 
	 loss: 16.4074, MinusLogProbMetric: 16.4074, val_loss: 16.9592, val_MinusLogProbMetric: 16.9592

Epoch 986: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4074 - MinusLogProbMetric: 16.4074 - val_loss: 16.9592 - val_MinusLogProbMetric: 16.9592 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 987/1000
2023-10-11 13:49:38.898 
Epoch 987/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9588, val_MinusLogProbMetric: 16.9588

Epoch 987: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9588 - val_MinusLogProbMetric: 16.9588 - lr: 1.3021e-06 - 48s/epoch - 247ms/step
Epoch 988/1000
2023-10-11 13:50:27.646 
Epoch 988/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9596, val_MinusLogProbMetric: 16.9596

Epoch 988: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9596 - val_MinusLogProbMetric: 16.9596 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 989/1000
2023-10-11 13:51:18.445 
Epoch 989/1000 
	 loss: 16.4077, MinusLogProbMetric: 16.4077, val_loss: 16.9586, val_MinusLogProbMetric: 16.9586

Epoch 989: val_loss did not improve from 16.95645
196/196 - 51s - loss: 16.4077 - MinusLogProbMetric: 16.4077 - val_loss: 16.9586 - val_MinusLogProbMetric: 16.9586 - lr: 1.3021e-06 - 51s/epoch - 259ms/step
Epoch 990/1000
2023-10-11 13:52:07.534 
Epoch 990/1000 
	 loss: 16.4069, MinusLogProbMetric: 16.4069, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 990: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4069 - MinusLogProbMetric: 16.4069 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.3021e-06 - 49s/epoch - 250ms/step
Epoch 991/1000
2023-10-11 13:52:56.078 
Epoch 991/1000 
	 loss: 16.4079, MinusLogProbMetric: 16.4079, val_loss: 16.9572, val_MinusLogProbMetric: 16.9572

Epoch 991: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4079 - MinusLogProbMetric: 16.4079 - val_loss: 16.9572 - val_MinusLogProbMetric: 16.9572 - lr: 1.3021e-06 - 49s/epoch - 248ms/step
Epoch 992/1000
2023-10-11 13:53:44.593 
Epoch 992/1000 
	 loss: 16.4074, MinusLogProbMetric: 16.4074, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 992: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4074 - MinusLogProbMetric: 16.4074 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.3021e-06 - 49s/epoch - 248ms/step
Epoch 993/1000
2023-10-11 13:54:32.634 
Epoch 993/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9580, val_MinusLogProbMetric: 16.9580

Epoch 993: val_loss did not improve from 16.95645
196/196 - 48s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9580 - val_MinusLogProbMetric: 16.9580 - lr: 1.3021e-06 - 48s/epoch - 245ms/step
Epoch 994/1000
2023-10-11 13:55:22.080 
Epoch 994/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9572, val_MinusLogProbMetric: 16.9572

Epoch 994: val_loss did not improve from 16.95645
196/196 - 49s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9572 - val_MinusLogProbMetric: 16.9572 - lr: 1.3021e-06 - 49s/epoch - 252ms/step
Epoch 995/1000
2023-10-11 13:56:11.704 
Epoch 995/1000 
	 loss: 16.4075, MinusLogProbMetric: 16.4075, val_loss: 16.9559, val_MinusLogProbMetric: 16.9559

Epoch 995: val_loss improved from 16.95645 to 16.95594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 51s - loss: 16.4075 - MinusLogProbMetric: 16.4075 - val_loss: 16.9559 - val_MinusLogProbMetric: 16.9559 - lr: 1.3021e-06 - 51s/epoch - 259ms/step
Epoch 996/1000
2023-10-11 13:57:01.786 
Epoch 996/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9593, val_MinusLogProbMetric: 16.9593

Epoch 996: val_loss did not improve from 16.95594
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9593 - val_MinusLogProbMetric: 16.9593 - lr: 1.3021e-06 - 49s/epoch - 249ms/step
Epoch 997/1000
2023-10-11 13:57:51.652 
Epoch 997/1000 
	 loss: 16.4078, MinusLogProbMetric: 16.4078, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 997: val_loss did not improve from 16.95594
196/196 - 50s - loss: 16.4078 - MinusLogProbMetric: 16.4078 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.3021e-06 - 50s/epoch - 254ms/step
Epoch 998/1000
2023-10-11 13:58:40.944 
Epoch 998/1000 
	 loss: 16.4077, MinusLogProbMetric: 16.4077, val_loss: 16.9592, val_MinusLogProbMetric: 16.9592

Epoch 998: val_loss did not improve from 16.95594
196/196 - 49s - loss: 16.4077 - MinusLogProbMetric: 16.4077 - val_loss: 16.9592 - val_MinusLogProbMetric: 16.9592 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Epoch 999/1000
2023-10-11 13:59:31.213 
Epoch 999/1000 
	 loss: 16.4073, MinusLogProbMetric: 16.4073, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 999: val_loss did not improve from 16.95594
196/196 - 50s - loss: 16.4073 - MinusLogProbMetric: 16.4073 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 1.3021e-06 - 50s/epoch - 256ms/step
Epoch 1000/1000
2023-10-11 14:00:20.453 
Epoch 1000/1000 
	 loss: 16.4076, MinusLogProbMetric: 16.4076, val_loss: 16.9571, val_MinusLogProbMetric: 16.9571

Epoch 1000: val_loss did not improve from 16.95594
196/196 - 49s - loss: 16.4076 - MinusLogProbMetric: 16.4076 - val_loss: 16.9571 - val_MinusLogProbMetric: 16.9571 - lr: 1.3021e-06 - 49s/epoch - 251ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 22.43538872594945 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 10.487870847107843 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 8.840101327979937 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Iterating from 0 to 1 out of 10 .
nchunks = 10
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 10.261346717830747 seconds.
Training succeeded with seed 926.
Model trained in 79326.36 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 53.36 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 53.72 s.
===========
Run 310/720 done in 79535.16 s.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

===========
Generating train data for run 319.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f3db4444070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a1dc80b20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a1dc80b20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ad170f220>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a6c4f5060>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a6c4f55d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3a6c4f5690>, <keras.callbacks.EarlyStopping object at 0x7f3a6c4f5900>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a6c4f5930>, <keras.callbacks.TerminateOnNaN object at 0x7f3a6c4f5570>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:01:22.387178
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 45: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:04:11.871 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2328.5117, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 2328.5117 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 169s/epoch - 864ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 319.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f3a14acc100>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a156771f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a156771f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ae4338a90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a1d42b3d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a1d42b940>, <keras.callbacks.ModelCheckpoint object at 0x7f3a1d42ba00>, <keras.callbacks.EarlyStopping object at 0x7f3a1d42bc70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a1d42bca0>, <keras.callbacks.TerminateOnNaN object at 0x7f3a1d42b8e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:04:20.954950
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:07:05.197 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3046.9424, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 3046.9424 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 164s/epoch - 836ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 319.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_252"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_253 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f3b10307c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3b103b34c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3b103b34c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c74217190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a1c7067d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a1c706d40>, <keras.callbacks.ModelCheckpoint object at 0x7f3a1c706e00>, <keras.callbacks.EarlyStopping object at 0x7f3a1c707070>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a1c7070a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3a1c706ce0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:07:14.326426
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:09:40.454 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3307.8606, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 146s - loss: nan - MinusLogProbMetric: 3307.8606 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 146s/epoch - 745ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 3.703703703703703e-05.
===========
Generating train data for run 319.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_263"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_264 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f3c6ea33ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f39f5654730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f39f5654730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c6ebe9b10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c6ea66e30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c6ea673a0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c6ea67460>, <keras.callbacks.EarlyStopping object at 0x7f3c6ea676d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c6ea67700>, <keras.callbacks.TerminateOnNaN object at 0x7f3c6ea67340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:09:48.468821
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 11: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:12:22.588 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3366.5293, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 154s - loss: nan - MinusLogProbMetric: 3366.5293 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 154s/epoch - 785ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.2345679012345677e-05.
===========
Generating train data for run 319.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_274"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_275 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f39dea6f610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c6c89e350>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c6c89e350>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39fee5a230>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a05f026e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a05f02c50>, <keras.callbacks.ModelCheckpoint object at 0x7f3a05f02d10>, <keras.callbacks.EarlyStopping object at 0x7f3a05f02f80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a05f02fb0>, <keras.callbacks.TerminateOnNaN object at 0x7f3a05f02bf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:12:31.545117
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 167: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:15:53.134 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3067.6467, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 201s - loss: nan - MinusLogProbMetric: 3067.6467 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 201s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.115226337448558e-06.
===========
Generating train data for run 319.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_285"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_286 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f3a064d5840>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3eb41c6b60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3eb41c6b60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a059dfb50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a1422f7f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a1422fd60>, <keras.callbacks.ModelCheckpoint object at 0x7f3a1422fe20>, <keras.callbacks.EarlyStopping object at 0x7f3a1422ffd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a1422fd30>, <keras.callbacks.TerminateOnNaN object at 0x7f3a1422ff10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:16:01.819617
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 67: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:19:03.560 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3318.0918, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 181s - loss: nan - MinusLogProbMetric: 3318.0918 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 181s/epoch - 926ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.3717421124828526e-06.
===========
Generating train data for run 319.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_296"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_297 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f3adcfdead0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3f344173d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3f344173d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3f34442350>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a15408790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a15408d00>, <keras.callbacks.ModelCheckpoint object at 0x7f3a15408dc0>, <keras.callbacks.EarlyStopping object at 0x7f3a15409030>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a15409060>, <keras.callbacks.TerminateOnNaN object at 0x7f3a15408ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:19:13.087177
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 27: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:21:50.548 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3377.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 3377.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 157s/epoch - 803ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.572473708276175e-07.
===========
Generating train data for run 319.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f3c6dc2d480>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38f4fd7760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38f4fd7760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f399e8d8f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c6d270d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c6d271270>, <keras.callbacks.ModelCheckpoint object at 0x7f3c6d271330>, <keras.callbacks.EarlyStopping object at 0x7f3c6d2715a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c6d2715d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c6d271210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:21:59.567432
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 68: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:24:55.373 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3383.1499, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 3383.1499 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 176s/epoch - 896ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.524157902758725e-07.
===========
Generating train data for run 319.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_318"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_319 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f39fedc7340>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f399f391cf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f399f391cf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a061b8ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f39fed21a80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f39fed21ff0>, <keras.callbacks.ModelCheckpoint object at 0x7f39fed220b0>, <keras.callbacks.EarlyStopping object at 0x7f39fed22320>, <keras.callbacks.ReduceLROnPlateau object at 0x7f39fed22350>, <keras.callbacks.TerminateOnNaN object at 0x7f39fed21f90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:25:04.019784
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 15: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:27:47.226 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3387.8140, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 3387.8140 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 163s/epoch - 831ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.0805263425290834e-08.
===========
Generating train data for run 319.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_329"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_330 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f3a0e334550>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c6c860be0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c6c860be0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a1d088b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a26416860>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a26416dd0>, <keras.callbacks.ModelCheckpoint object at 0x7f3a26416e90>, <keras.callbacks.EarlyStopping object at 0x7f3a26417100>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a26417130>, <keras.callbacks.TerminateOnNaN object at 0x7f3a26416d70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:27:55.129934
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 40: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:30:31.857 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3389.1753, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 3389.1753 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 157s/epoch - 799ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.6935087808430278e-08.
===========
Generating train data for run 319.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_340"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_341 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f3c4f943cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a264ab820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a264ab820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38b467f160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c4f96ece0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c4f96f250>, <keras.callbacks.ModelCheckpoint object at 0x7f3c4f96f310>, <keras.callbacks.EarlyStopping object at 0x7f3c4f96f580>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c4f96f5b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c4f96f1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-11 14:30:41.037210
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 137: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 14:33:59.657 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3389.1721, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 198s - loss: nan - MinusLogProbMetric: 3389.1721 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 198s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.645029269476759e-09.
===========
Run 319/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

===========
Generating train data for run 321.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_321/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_321/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_321/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_321
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_346"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_347 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f3a15bff580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a0f63da50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a0f63da50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38f4fa1cc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f38f41cca90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f38f41cc280>, <keras.callbacks.ModelCheckpoint object at 0x7f38f41e01c0>, <keras.callbacks.EarlyStopping object at 0x7f38f41e0430>, <keras.callbacks.ReduceLROnPlateau object at 0x7f38f41e0460>, <keras.callbacks.TerminateOnNaN object at 0x7f38f41e00a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_321/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 321/720 with hyperparameters:
timestamp = 2023-10-11 14:34:03.986667
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-10-11 14:35:23.680 
Epoch 1/1000 
	 loss: 1318.7020, MinusLogProbMetric: 1318.7020, val_loss: 393.5629, val_MinusLogProbMetric: 393.5629

Epoch 1: val_loss improved from inf to 393.56290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 80s - loss: 1318.7020 - MinusLogProbMetric: 1318.7020 - val_loss: 393.5629 - val_MinusLogProbMetric: 393.5629 - lr: 0.0010 - 80s/epoch - 408ms/step
Epoch 2/1000
2023-10-11 14:35:50.226 
Epoch 2/1000 
	 loss: 330.0001, MinusLogProbMetric: 330.0001, val_loss: 294.1362, val_MinusLogProbMetric: 294.1362

Epoch 2: val_loss improved from 393.56290 to 294.13620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 26s - loss: 330.0001 - MinusLogProbMetric: 330.0001 - val_loss: 294.1362 - val_MinusLogProbMetric: 294.1362 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 3/1000
2023-10-11 14:36:17.050 
Epoch 3/1000 
	 loss: 312.1311, MinusLogProbMetric: 312.1311, val_loss: 329.9716, val_MinusLogProbMetric: 329.9716

Epoch 3: val_loss did not improve from 294.13620
196/196 - 26s - loss: 312.1311 - MinusLogProbMetric: 312.1311 - val_loss: 329.9716 - val_MinusLogProbMetric: 329.9716 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 4/1000
2023-10-11 14:36:43.037 
Epoch 4/1000 
	 loss: 274.6871, MinusLogProbMetric: 274.6871, val_loss: 254.6767, val_MinusLogProbMetric: 254.6767

Epoch 4: val_loss improved from 294.13620 to 254.67671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 27s - loss: 274.6871 - MinusLogProbMetric: 274.6871 - val_loss: 254.6767 - val_MinusLogProbMetric: 254.6767 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 5/1000
2023-10-11 14:37:09.998 
Epoch 5/1000 
	 loss: 229.5758, MinusLogProbMetric: 229.5758, val_loss: 212.2281, val_MinusLogProbMetric: 212.2281

Epoch 5: val_loss improved from 254.67671 to 212.22807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 27s - loss: 229.5758 - MinusLogProbMetric: 229.5758 - val_loss: 212.2281 - val_MinusLogProbMetric: 212.2281 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 6/1000
2023-10-11 14:37:36.451 
Epoch 6/1000 
	 loss: 249.0419, MinusLogProbMetric: 249.0419, val_loss: 229.6857, val_MinusLogProbMetric: 229.6857

Epoch 6: val_loss did not improve from 212.22807
196/196 - 26s - loss: 249.0419 - MinusLogProbMetric: 249.0419 - val_loss: 229.6857 - val_MinusLogProbMetric: 229.6857 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 7/1000
2023-10-11 14:38:02.811 
Epoch 7/1000 
	 loss: 123.9356, MinusLogProbMetric: 123.9356, val_loss: 102.7338, val_MinusLogProbMetric: 102.7338

Epoch 7: val_loss improved from 212.22807 to 102.73378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 27s - loss: 123.9356 - MinusLogProbMetric: 123.9356 - val_loss: 102.7338 - val_MinusLogProbMetric: 102.7338 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 8/1000
2023-10-11 14:38:30.269 
Epoch 8/1000 
	 loss: 96.5449, MinusLogProbMetric: 96.5449, val_loss: 96.0125, val_MinusLogProbMetric: 96.0125

Epoch 8: val_loss improved from 102.73378 to 96.01247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 27s - loss: 96.5449 - MinusLogProbMetric: 96.5449 - val_loss: 96.0125 - val_MinusLogProbMetric: 96.0125 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 9/1000
2023-10-11 14:38:58.274 
Epoch 9/1000 
	 loss: 87.4120, MinusLogProbMetric: 87.4120, val_loss: 85.5350, val_MinusLogProbMetric: 85.5350

Epoch 9: val_loss improved from 96.01247 to 85.53497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 87.4120 - MinusLogProbMetric: 87.4120 - val_loss: 85.5350 - val_MinusLogProbMetric: 85.5350 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 10/1000
2023-10-11 14:39:25.828 
Epoch 10/1000 
	 loss: 80.0632, MinusLogProbMetric: 80.0632, val_loss: 77.2859, val_MinusLogProbMetric: 77.2859

Epoch 10: val_loss improved from 85.53497 to 77.28591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 80.0632 - MinusLogProbMetric: 80.0632 - val_loss: 77.2859 - val_MinusLogProbMetric: 77.2859 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 11/1000
2023-10-11 14:39:54.140 
Epoch 11/1000 
	 loss: 78.4679, MinusLogProbMetric: 78.4679, val_loss: 73.6975, val_MinusLogProbMetric: 73.6975

Epoch 11: val_loss improved from 77.28591 to 73.69753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 78.4679 - MinusLogProbMetric: 78.4679 - val_loss: 73.6975 - val_MinusLogProbMetric: 73.6975 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 12/1000
2023-10-11 14:40:22.379 
Epoch 12/1000 
	 loss: 71.4082, MinusLogProbMetric: 71.4082, val_loss: 68.9188, val_MinusLogProbMetric: 68.9188

Epoch 12: val_loss improved from 73.69753 to 68.91884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 71.4082 - MinusLogProbMetric: 71.4082 - val_loss: 68.9188 - val_MinusLogProbMetric: 68.9188 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 13/1000
2023-10-11 14:40:50.258 
Epoch 13/1000 
	 loss: 87.3382, MinusLogProbMetric: 87.3382, val_loss: 329.8351, val_MinusLogProbMetric: 329.8351

Epoch 13: val_loss did not improve from 68.91884
196/196 - 27s - loss: 87.3382 - MinusLogProbMetric: 87.3382 - val_loss: 329.8351 - val_MinusLogProbMetric: 329.8351 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 14/1000
2023-10-11 14:41:16.798 
Epoch 14/1000 
	 loss: 130.9758, MinusLogProbMetric: 130.9758, val_loss: 92.9821, val_MinusLogProbMetric: 92.9821

Epoch 14: val_loss did not improve from 68.91884
196/196 - 27s - loss: 130.9758 - MinusLogProbMetric: 130.9758 - val_loss: 92.9821 - val_MinusLogProbMetric: 92.9821 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 15/1000
2023-10-11 14:41:43.513 
Epoch 15/1000 
	 loss: 85.4021, MinusLogProbMetric: 85.4021, val_loss: 80.3552, val_MinusLogProbMetric: 80.3552

Epoch 15: val_loss did not improve from 68.91884
196/196 - 27s - loss: 85.4021 - MinusLogProbMetric: 85.4021 - val_loss: 80.3552 - val_MinusLogProbMetric: 80.3552 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 16/1000
2023-10-11 14:42:10.076 
Epoch 16/1000 
	 loss: 76.0541, MinusLogProbMetric: 76.0541, val_loss: 72.8393, val_MinusLogProbMetric: 72.8393

Epoch 16: val_loss did not improve from 68.91884
196/196 - 27s - loss: 76.0541 - MinusLogProbMetric: 76.0541 - val_loss: 72.8393 - val_MinusLogProbMetric: 72.8393 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 17/1000
2023-10-11 14:42:37.321 
Epoch 17/1000 
	 loss: 71.2711, MinusLogProbMetric: 71.2711, val_loss: 68.4690, val_MinusLogProbMetric: 68.4690

Epoch 17: val_loss improved from 68.91884 to 68.46905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 71.2711 - MinusLogProbMetric: 71.2711 - val_loss: 68.4690 - val_MinusLogProbMetric: 68.4690 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 18/1000
2023-10-11 14:43:05.280 
Epoch 18/1000 
	 loss: 65.9969, MinusLogProbMetric: 65.9969, val_loss: 63.7563, val_MinusLogProbMetric: 63.7563

Epoch 18: val_loss improved from 68.46905 to 63.75630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 65.9969 - MinusLogProbMetric: 65.9969 - val_loss: 63.7563 - val_MinusLogProbMetric: 63.7563 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 19/1000
2023-10-11 14:43:33.327 
Epoch 19/1000 
	 loss: 62.8713, MinusLogProbMetric: 62.8713, val_loss: 61.5290, val_MinusLogProbMetric: 61.5290

Epoch 19: val_loss improved from 63.75630 to 61.52895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 62.8713 - MinusLogProbMetric: 62.8713 - val_loss: 61.5290 - val_MinusLogProbMetric: 61.5290 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 20/1000
2023-10-11 14:44:01.006 
Epoch 20/1000 
	 loss: 60.8811, MinusLogProbMetric: 60.8811, val_loss: 63.1135, val_MinusLogProbMetric: 63.1135

Epoch 20: val_loss did not improve from 61.52895
196/196 - 27s - loss: 60.8811 - MinusLogProbMetric: 60.8811 - val_loss: 63.1135 - val_MinusLogProbMetric: 63.1135 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 21/1000
2023-10-11 14:44:28.663 
Epoch 21/1000 
	 loss: 59.7692, MinusLogProbMetric: 59.7692, val_loss: 58.3527, val_MinusLogProbMetric: 58.3527

Epoch 21: val_loss improved from 61.52895 to 58.35268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 59.7692 - MinusLogProbMetric: 59.7692 - val_loss: 58.3527 - val_MinusLogProbMetric: 58.3527 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 22/1000
2023-10-11 14:44:56.632 
Epoch 22/1000 
	 loss: 57.1750, MinusLogProbMetric: 57.1750, val_loss: 56.4581, val_MinusLogProbMetric: 56.4581

Epoch 22: val_loss improved from 58.35268 to 56.45808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 57.1750 - MinusLogProbMetric: 57.1750 - val_loss: 56.4581 - val_MinusLogProbMetric: 56.4581 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 23/1000
2023-10-11 14:45:24.713 
Epoch 23/1000 
	 loss: 55.6325, MinusLogProbMetric: 55.6325, val_loss: 55.3768, val_MinusLogProbMetric: 55.3768

Epoch 23: val_loss improved from 56.45808 to 55.37684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 55.6325 - MinusLogProbMetric: 55.6325 - val_loss: 55.3768 - val_MinusLogProbMetric: 55.3768 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 24/1000
2023-10-11 14:45:52.486 
Epoch 24/1000 
	 loss: 54.5575, MinusLogProbMetric: 54.5575, val_loss: 55.0199, val_MinusLogProbMetric: 55.0199

Epoch 24: val_loss improved from 55.37684 to 55.01995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 54.5575 - MinusLogProbMetric: 54.5575 - val_loss: 55.0199 - val_MinusLogProbMetric: 55.0199 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 25/1000
2023-10-11 14:46:19.470 
Epoch 25/1000 
	 loss: 53.3763, MinusLogProbMetric: 53.3763, val_loss: 52.8955, val_MinusLogProbMetric: 52.8955

Epoch 25: val_loss improved from 55.01995 to 52.89554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 27s - loss: 53.3763 - MinusLogProbMetric: 53.3763 - val_loss: 52.8955 - val_MinusLogProbMetric: 52.8955 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 26/1000
2023-10-11 14:46:47.143 
Epoch 26/1000 
	 loss: 52.4160, MinusLogProbMetric: 52.4160, val_loss: 52.2352, val_MinusLogProbMetric: 52.2352

Epoch 26: val_loss improved from 52.89554 to 52.23518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 52.4160 - MinusLogProbMetric: 52.4160 - val_loss: 52.2352 - val_MinusLogProbMetric: 52.2352 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 27/1000
2023-10-11 14:47:15.096 
Epoch 27/1000 
	 loss: 51.3797, MinusLogProbMetric: 51.3797, val_loss: 50.7952, val_MinusLogProbMetric: 50.7952

Epoch 27: val_loss improved from 52.23518 to 50.79524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 51.3797 - MinusLogProbMetric: 51.3797 - val_loss: 50.7952 - val_MinusLogProbMetric: 50.7952 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 28/1000
2023-10-11 14:47:43.361 
Epoch 28/1000 
	 loss: 50.5753, MinusLogProbMetric: 50.5753, val_loss: 50.7587, val_MinusLogProbMetric: 50.7587

Epoch 28: val_loss improved from 50.79524 to 50.75873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 50.5753 - MinusLogProbMetric: 50.5753 - val_loss: 50.7587 - val_MinusLogProbMetric: 50.7587 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 29/1000
2023-10-11 14:48:11.851 
Epoch 29/1000 
	 loss: 50.2274, MinusLogProbMetric: 50.2274, val_loss: 50.9825, val_MinusLogProbMetric: 50.9825

Epoch 29: val_loss did not improve from 50.75873
196/196 - 28s - loss: 50.2274 - MinusLogProbMetric: 50.2274 - val_loss: 50.9825 - val_MinusLogProbMetric: 50.9825 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 30/1000
2023-10-11 14:48:38.960 
Epoch 30/1000 
	 loss: 71.1999, MinusLogProbMetric: 71.1999, val_loss: 104.8889, val_MinusLogProbMetric: 104.8889

Epoch 30: val_loss did not improve from 50.75873
196/196 - 27s - loss: 71.1999 - MinusLogProbMetric: 71.1999 - val_loss: 104.8889 - val_MinusLogProbMetric: 104.8889 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 31/1000
2023-10-11 14:49:05.894 
Epoch 31/1000 
	 loss: 95.6779, MinusLogProbMetric: 95.6779, val_loss: 92.3779, val_MinusLogProbMetric: 92.3779

Epoch 31: val_loss did not improve from 50.75873
196/196 - 27s - loss: 95.6779 - MinusLogProbMetric: 95.6779 - val_loss: 92.3779 - val_MinusLogProbMetric: 92.3779 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 32/1000
2023-10-11 14:49:33.352 
Epoch 32/1000 
	 loss: 91.2884, MinusLogProbMetric: 91.2884, val_loss: 90.2538, val_MinusLogProbMetric: 90.2538

Epoch 32: val_loss did not improve from 50.75873
196/196 - 27s - loss: 91.2884 - MinusLogProbMetric: 91.2884 - val_loss: 90.2538 - val_MinusLogProbMetric: 90.2538 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 33/1000
2023-10-11 14:50:01.062 
Epoch 33/1000 
	 loss: 89.5989, MinusLogProbMetric: 89.5989, val_loss: 89.6684, val_MinusLogProbMetric: 89.6684

Epoch 33: val_loss did not improve from 50.75873
196/196 - 28s - loss: 89.5989 - MinusLogProbMetric: 89.5989 - val_loss: 89.6684 - val_MinusLogProbMetric: 89.6684 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 34/1000
2023-10-11 14:50:28.814 
Epoch 34/1000 
	 loss: 88.5190, MinusLogProbMetric: 88.5190, val_loss: 87.6789, val_MinusLogProbMetric: 87.6789

Epoch 34: val_loss did not improve from 50.75873
196/196 - 28s - loss: 88.5190 - MinusLogProbMetric: 88.5190 - val_loss: 87.6789 - val_MinusLogProbMetric: 87.6789 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 35/1000
2023-10-11 14:50:56.878 
Epoch 35/1000 
	 loss: 86.7267, MinusLogProbMetric: 86.7267, val_loss: 85.8376, val_MinusLogProbMetric: 85.8376

Epoch 35: val_loss did not improve from 50.75873
196/196 - 28s - loss: 86.7267 - MinusLogProbMetric: 86.7267 - val_loss: 85.8376 - val_MinusLogProbMetric: 85.8376 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 36/1000
2023-10-11 14:51:24.452 
Epoch 36/1000 
	 loss: 85.8348, MinusLogProbMetric: 85.8348, val_loss: 86.7813, val_MinusLogProbMetric: 86.7813

Epoch 36: val_loss did not improve from 50.75873
196/196 - 28s - loss: 85.8348 - MinusLogProbMetric: 85.8348 - val_loss: 86.7813 - val_MinusLogProbMetric: 86.7813 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 37/1000
2023-10-11 14:51:52.011 
Epoch 37/1000 
	 loss: 85.1361, MinusLogProbMetric: 85.1361, val_loss: 85.7436, val_MinusLogProbMetric: 85.7436

Epoch 37: val_loss did not improve from 50.75873
196/196 - 28s - loss: 85.1361 - MinusLogProbMetric: 85.1361 - val_loss: 85.7436 - val_MinusLogProbMetric: 85.7436 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 38/1000
2023-10-11 14:52:18.913 
Epoch 38/1000 
	 loss: 85.7227, MinusLogProbMetric: 85.7227, val_loss: 104.6282, val_MinusLogProbMetric: 104.6282

Epoch 38: val_loss did not improve from 50.75873
196/196 - 27s - loss: 85.7227 - MinusLogProbMetric: 85.7227 - val_loss: 104.6282 - val_MinusLogProbMetric: 104.6282 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 39/1000
2023-10-11 14:52:46.461 
Epoch 39/1000 
	 loss: 86.6804, MinusLogProbMetric: 86.6804, val_loss: 86.7274, val_MinusLogProbMetric: 86.7274

Epoch 39: val_loss did not improve from 50.75873
196/196 - 28s - loss: 86.6804 - MinusLogProbMetric: 86.6804 - val_loss: 86.7274 - val_MinusLogProbMetric: 86.7274 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 40/1000
2023-10-11 14:53:13.537 
Epoch 40/1000 
	 loss: 84.6174, MinusLogProbMetric: 84.6174, val_loss: 84.6362, val_MinusLogProbMetric: 84.6362

Epoch 40: val_loss did not improve from 50.75873
196/196 - 27s - loss: 84.6174 - MinusLogProbMetric: 84.6174 - val_loss: 84.6362 - val_MinusLogProbMetric: 84.6362 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 41/1000
2023-10-11 14:53:41.322 
Epoch 41/1000 
	 loss: 84.0711, MinusLogProbMetric: 84.0711, val_loss: 84.4122, val_MinusLogProbMetric: 84.4122

Epoch 41: val_loss did not improve from 50.75873
196/196 - 28s - loss: 84.0711 - MinusLogProbMetric: 84.0711 - val_loss: 84.4122 - val_MinusLogProbMetric: 84.4122 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 42/1000
2023-10-11 14:54:08.672 
Epoch 42/1000 
	 loss: 83.7525, MinusLogProbMetric: 83.7525, val_loss: 84.4310, val_MinusLogProbMetric: 84.4310

Epoch 42: val_loss did not improve from 50.75873
196/196 - 27s - loss: 83.7525 - MinusLogProbMetric: 83.7525 - val_loss: 84.4310 - val_MinusLogProbMetric: 84.4310 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 43/1000
2023-10-11 14:54:35.446 
Epoch 43/1000 
	 loss: 83.2363, MinusLogProbMetric: 83.2363, val_loss: 83.0973, val_MinusLogProbMetric: 83.0973

Epoch 43: val_loss did not improve from 50.75873
196/196 - 27s - loss: 83.2363 - MinusLogProbMetric: 83.2363 - val_loss: 83.0973 - val_MinusLogProbMetric: 83.0973 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 44/1000
2023-10-11 14:55:02.662 
Epoch 44/1000 
	 loss: 83.0757, MinusLogProbMetric: 83.0757, val_loss: 82.6516, val_MinusLogProbMetric: 82.6516

Epoch 44: val_loss did not improve from 50.75873
196/196 - 27s - loss: 83.0757 - MinusLogProbMetric: 83.0757 - val_loss: 82.6516 - val_MinusLogProbMetric: 82.6516 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 45/1000
2023-10-11 14:55:29.540 
Epoch 45/1000 
	 loss: 84.9544, MinusLogProbMetric: 84.9544, val_loss: 92.8922, val_MinusLogProbMetric: 92.8922

Epoch 45: val_loss did not improve from 50.75873
196/196 - 27s - loss: 84.9544 - MinusLogProbMetric: 84.9544 - val_loss: 92.8922 - val_MinusLogProbMetric: 92.8922 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 46/1000
2023-10-11 14:55:56.380 
Epoch 46/1000 
	 loss: 84.5093, MinusLogProbMetric: 84.5093, val_loss: 82.3906, val_MinusLogProbMetric: 82.3906

Epoch 46: val_loss did not improve from 50.75873
196/196 - 27s - loss: 84.5093 - MinusLogProbMetric: 84.5093 - val_loss: 82.3906 - val_MinusLogProbMetric: 82.3906 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 47/1000
2023-10-11 14:56:23.586 
Epoch 47/1000 
	 loss: 82.3060, MinusLogProbMetric: 82.3060, val_loss: 82.3976, val_MinusLogProbMetric: 82.3976

Epoch 47: val_loss did not improve from 50.75873
196/196 - 27s - loss: 82.3060 - MinusLogProbMetric: 82.3060 - val_loss: 82.3976 - val_MinusLogProbMetric: 82.3976 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 48/1000
2023-10-11 14:56:50.251 
Epoch 48/1000 
	 loss: 81.8965, MinusLogProbMetric: 81.8965, val_loss: 82.4346, val_MinusLogProbMetric: 82.4346

Epoch 48: val_loss did not improve from 50.75873
196/196 - 27s - loss: 81.8965 - MinusLogProbMetric: 81.8965 - val_loss: 82.4346 - val_MinusLogProbMetric: 82.4346 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 49/1000
2023-10-11 14:57:16.747 
Epoch 49/1000 
	 loss: 81.8177, MinusLogProbMetric: 81.8177, val_loss: 82.1393, val_MinusLogProbMetric: 82.1393

Epoch 49: val_loss did not improve from 50.75873
196/196 - 26s - loss: 81.8177 - MinusLogProbMetric: 81.8177 - val_loss: 82.1393 - val_MinusLogProbMetric: 82.1393 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 50/1000
2023-10-11 14:57:43.749 
Epoch 50/1000 
	 loss: 81.5825, MinusLogProbMetric: 81.5825, val_loss: 80.5143, val_MinusLogProbMetric: 80.5143

Epoch 50: val_loss did not improve from 50.75873
196/196 - 27s - loss: 81.5825 - MinusLogProbMetric: 81.5825 - val_loss: 80.5143 - val_MinusLogProbMetric: 80.5143 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 51/1000
2023-10-11 14:58:10.875 
Epoch 51/1000 
	 loss: 81.0685, MinusLogProbMetric: 81.0685, val_loss: 82.6839, val_MinusLogProbMetric: 82.6839

Epoch 51: val_loss did not improve from 50.75873
196/196 - 27s - loss: 81.0685 - MinusLogProbMetric: 81.0685 - val_loss: 82.6839 - val_MinusLogProbMetric: 82.6839 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 52/1000
2023-10-11 14:58:38.389 
Epoch 52/1000 
	 loss: 81.0743, MinusLogProbMetric: 81.0743, val_loss: 83.0089, val_MinusLogProbMetric: 83.0089

Epoch 52: val_loss did not improve from 50.75873
196/196 - 28s - loss: 81.0743 - MinusLogProbMetric: 81.0743 - val_loss: 83.0089 - val_MinusLogProbMetric: 83.0089 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 53/1000
2023-10-11 14:59:05.242 
Epoch 53/1000 
	 loss: 80.5449, MinusLogProbMetric: 80.5449, val_loss: 82.0840, val_MinusLogProbMetric: 82.0840

Epoch 53: val_loss did not improve from 50.75873
196/196 - 27s - loss: 80.5449 - MinusLogProbMetric: 80.5449 - val_loss: 82.0840 - val_MinusLogProbMetric: 82.0840 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 54/1000
2023-10-11 14:59:32.479 
Epoch 54/1000 
	 loss: 80.5912, MinusLogProbMetric: 80.5912, val_loss: 81.5460, val_MinusLogProbMetric: 81.5460

Epoch 54: val_loss did not improve from 50.75873
196/196 - 27s - loss: 80.5912 - MinusLogProbMetric: 80.5912 - val_loss: 81.5460 - val_MinusLogProbMetric: 81.5460 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 55/1000
2023-10-11 14:59:59.285 
Epoch 55/1000 
	 loss: 80.4561, MinusLogProbMetric: 80.4561, val_loss: 80.6796, val_MinusLogProbMetric: 80.6796

Epoch 55: val_loss did not improve from 50.75873
196/196 - 27s - loss: 80.4561 - MinusLogProbMetric: 80.4561 - val_loss: 80.6796 - val_MinusLogProbMetric: 80.6796 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 56/1000
2023-10-11 15:00:26.588 
Epoch 56/1000 
	 loss: 79.9369, MinusLogProbMetric: 79.9369, val_loss: 79.7733, val_MinusLogProbMetric: 79.7733

Epoch 56: val_loss did not improve from 50.75873
196/196 - 27s - loss: 79.9369 - MinusLogProbMetric: 79.9369 - val_loss: 79.7733 - val_MinusLogProbMetric: 79.7733 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 57/1000
2023-10-11 15:00:53.882 
Epoch 57/1000 
	 loss: 93.0920, MinusLogProbMetric: 93.0920, val_loss: 87.7512, val_MinusLogProbMetric: 87.7512

Epoch 57: val_loss did not improve from 50.75873
196/196 - 27s - loss: 93.0920 - MinusLogProbMetric: 93.0920 - val_loss: 87.7512 - val_MinusLogProbMetric: 87.7512 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 58/1000
2023-10-11 15:01:19.630 
Epoch 58/1000 
	 loss: 84.5571, MinusLogProbMetric: 84.5571, val_loss: 84.3944, val_MinusLogProbMetric: 84.3944

Epoch 58: val_loss did not improve from 50.75873
196/196 - 26s - loss: 84.5571 - MinusLogProbMetric: 84.5571 - val_loss: 84.3944 - val_MinusLogProbMetric: 84.3944 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 59/1000
2023-10-11 15:01:44.994 
Epoch 59/1000 
	 loss: 82.2005, MinusLogProbMetric: 82.2005, val_loss: 81.5400, val_MinusLogProbMetric: 81.5400

Epoch 59: val_loss did not improve from 50.75873
196/196 - 25s - loss: 82.2005 - MinusLogProbMetric: 82.2005 - val_loss: 81.5400 - val_MinusLogProbMetric: 81.5400 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 60/1000
2023-10-11 15:02:10.805 
Epoch 60/1000 
	 loss: 81.2027, MinusLogProbMetric: 81.2027, val_loss: 80.9771, val_MinusLogProbMetric: 80.9771

Epoch 60: val_loss did not improve from 50.75873
196/196 - 26s - loss: 81.2027 - MinusLogProbMetric: 81.2027 - val_loss: 80.9771 - val_MinusLogProbMetric: 80.9771 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 61/1000
2023-10-11 15:02:37.173 
Epoch 61/1000 
	 loss: 80.5271, MinusLogProbMetric: 80.5271, val_loss: 80.9507, val_MinusLogProbMetric: 80.9507

Epoch 61: val_loss did not improve from 50.75873
196/196 - 26s - loss: 80.5271 - MinusLogProbMetric: 80.5271 - val_loss: 80.9507 - val_MinusLogProbMetric: 80.9507 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 62/1000
2023-10-11 15:03:02.508 
Epoch 62/1000 
	 loss: 80.2067, MinusLogProbMetric: 80.2067, val_loss: 80.0386, val_MinusLogProbMetric: 80.0386

Epoch 62: val_loss did not improve from 50.75873
196/196 - 25s - loss: 80.2067 - MinusLogProbMetric: 80.2067 - val_loss: 80.0386 - val_MinusLogProbMetric: 80.0386 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 63/1000
2023-10-11 15:03:27.723 
Epoch 63/1000 
	 loss: 80.0731, MinusLogProbMetric: 80.0731, val_loss: 79.3643, val_MinusLogProbMetric: 79.3643

Epoch 63: val_loss did not improve from 50.75873
196/196 - 25s - loss: 80.0731 - MinusLogProbMetric: 80.0731 - val_loss: 79.3643 - val_MinusLogProbMetric: 79.3643 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 64/1000
2023-10-11 15:03:52.970 
Epoch 64/1000 
	 loss: 79.3349, MinusLogProbMetric: 79.3349, val_loss: 79.9535, val_MinusLogProbMetric: 79.9535

Epoch 64: val_loss did not improve from 50.75873
196/196 - 25s - loss: 79.3349 - MinusLogProbMetric: 79.3349 - val_loss: 79.9535 - val_MinusLogProbMetric: 79.9535 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 65/1000
2023-10-11 15:04:18.779 
Epoch 65/1000 
	 loss: 79.1204, MinusLogProbMetric: 79.1204, val_loss: 78.6651, val_MinusLogProbMetric: 78.6651

Epoch 65: val_loss did not improve from 50.75873
196/196 - 26s - loss: 79.1204 - MinusLogProbMetric: 79.1204 - val_loss: 78.6651 - val_MinusLogProbMetric: 78.6651 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 66/1000
2023-10-11 15:04:43.859 
Epoch 66/1000 
	 loss: 78.7980, MinusLogProbMetric: 78.7980, val_loss: 79.1274, val_MinusLogProbMetric: 79.1274

Epoch 66: val_loss did not improve from 50.75873
196/196 - 25s - loss: 78.7980 - MinusLogProbMetric: 78.7980 - val_loss: 79.1274 - val_MinusLogProbMetric: 79.1274 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 67/1000
2023-10-11 15:05:09.001 
Epoch 67/1000 
	 loss: 78.4288, MinusLogProbMetric: 78.4288, val_loss: 78.8840, val_MinusLogProbMetric: 78.8840

Epoch 67: val_loss did not improve from 50.75873
196/196 - 25s - loss: 78.4288 - MinusLogProbMetric: 78.4288 - val_loss: 78.8840 - val_MinusLogProbMetric: 78.8840 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 68/1000
2023-10-11 15:05:34.025 
Epoch 68/1000 
	 loss: 78.2721, MinusLogProbMetric: 78.2721, val_loss: 77.5411, val_MinusLogProbMetric: 77.5411

Epoch 68: val_loss did not improve from 50.75873
196/196 - 25s - loss: 78.2721 - MinusLogProbMetric: 78.2721 - val_loss: 77.5411 - val_MinusLogProbMetric: 77.5411 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 69/1000
2023-10-11 15:06:00.636 
Epoch 69/1000 
	 loss: 79.0004, MinusLogProbMetric: 79.0004, val_loss: 79.3599, val_MinusLogProbMetric: 79.3599

Epoch 69: val_loss did not improve from 50.75873
196/196 - 27s - loss: 79.0004 - MinusLogProbMetric: 79.0004 - val_loss: 79.3599 - val_MinusLogProbMetric: 79.3599 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 70/1000
2023-10-11 15:06:26.874 
Epoch 70/1000 
	 loss: 77.8620, MinusLogProbMetric: 77.8620, val_loss: 77.4287, val_MinusLogProbMetric: 77.4287

Epoch 70: val_loss did not improve from 50.75873
196/196 - 26s - loss: 77.8620 - MinusLogProbMetric: 77.8620 - val_loss: 77.4287 - val_MinusLogProbMetric: 77.4287 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 71/1000
2023-10-11 15:06:52.474 
Epoch 71/1000 
	 loss: 77.5610, MinusLogProbMetric: 77.5610, val_loss: 76.5706, val_MinusLogProbMetric: 76.5706

Epoch 71: val_loss did not improve from 50.75873
196/196 - 26s - loss: 77.5610 - MinusLogProbMetric: 77.5610 - val_loss: 76.5706 - val_MinusLogProbMetric: 76.5706 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 72/1000
2023-10-11 15:07:17.653 
Epoch 72/1000 
	 loss: 77.6152, MinusLogProbMetric: 77.6152, val_loss: 79.2608, val_MinusLogProbMetric: 79.2608

Epoch 72: val_loss did not improve from 50.75873
196/196 - 25s - loss: 77.6152 - MinusLogProbMetric: 77.6152 - val_loss: 79.2608 - val_MinusLogProbMetric: 79.2608 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 73/1000
2023-10-11 15:07:43.450 
Epoch 73/1000 
	 loss: 76.9223, MinusLogProbMetric: 76.9223, val_loss: 80.5820, val_MinusLogProbMetric: 80.5820

Epoch 73: val_loss did not improve from 50.75873
196/196 - 26s - loss: 76.9223 - MinusLogProbMetric: 76.9223 - val_loss: 80.5820 - val_MinusLogProbMetric: 80.5820 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 74/1000
2023-10-11 15:08:09.088 
Epoch 74/1000 
	 loss: 78.2794, MinusLogProbMetric: 78.2794, val_loss: 78.2403, val_MinusLogProbMetric: 78.2403

Epoch 74: val_loss did not improve from 50.75873
196/196 - 26s - loss: 78.2794 - MinusLogProbMetric: 78.2794 - val_loss: 78.2403 - val_MinusLogProbMetric: 78.2403 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 75/1000
2023-10-11 15:08:34.773 
Epoch 75/1000 
	 loss: 77.0048, MinusLogProbMetric: 77.0048, val_loss: 76.3514, val_MinusLogProbMetric: 76.3514

Epoch 75: val_loss did not improve from 50.75873
196/196 - 26s - loss: 77.0048 - MinusLogProbMetric: 77.0048 - val_loss: 76.3514 - val_MinusLogProbMetric: 76.3514 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 76/1000
2023-10-11 15:08:59.865 
Epoch 76/1000 
	 loss: 76.7732, MinusLogProbMetric: 76.7732, val_loss: 77.4376, val_MinusLogProbMetric: 77.4376

Epoch 76: val_loss did not improve from 50.75873
196/196 - 25s - loss: 76.7732 - MinusLogProbMetric: 76.7732 - val_loss: 77.4376 - val_MinusLogProbMetric: 77.4376 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 77/1000
2023-10-11 15:09:25.258 
Epoch 77/1000 
	 loss: 99.9563, MinusLogProbMetric: 99.9563, val_loss: 92.0209, val_MinusLogProbMetric: 92.0209

Epoch 77: val_loss did not improve from 50.75873
196/196 - 25s - loss: 99.9563 - MinusLogProbMetric: 99.9563 - val_loss: 92.0209 - val_MinusLogProbMetric: 92.0209 - lr: 0.0010 - 25s/epoch - 130ms/step
Epoch 78/1000
2023-10-11 15:09:50.564 
Epoch 78/1000 
	 loss: 85.1078, MinusLogProbMetric: 85.1078, val_loss: 81.3319, val_MinusLogProbMetric: 81.3319

Epoch 78: val_loss did not improve from 50.75873
196/196 - 25s - loss: 85.1078 - MinusLogProbMetric: 85.1078 - val_loss: 81.3319 - val_MinusLogProbMetric: 81.3319 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 79/1000
2023-10-11 15:10:15.687 
Epoch 79/1000 
	 loss: 79.2753, MinusLogProbMetric: 79.2753, val_loss: 78.8493, val_MinusLogProbMetric: 78.8493

Epoch 79: val_loss did not improve from 50.75873
196/196 - 25s - loss: 79.2753 - MinusLogProbMetric: 79.2753 - val_loss: 78.8493 - val_MinusLogProbMetric: 78.8493 - lr: 5.0000e-04 - 25s/epoch - 128ms/step
Epoch 80/1000
2023-10-11 15:10:41.129 
Epoch 80/1000 
	 loss: 78.4497, MinusLogProbMetric: 78.4497, val_loss: 78.8075, val_MinusLogProbMetric: 78.8075

Epoch 80: val_loss did not improve from 50.75873
196/196 - 25s - loss: 78.4497 - MinusLogProbMetric: 78.4497 - val_loss: 78.8075 - val_MinusLogProbMetric: 78.8075 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 81/1000
2023-10-11 15:11:06.648 
Epoch 81/1000 
	 loss: 77.8041, MinusLogProbMetric: 77.8041, val_loss: 77.5882, val_MinusLogProbMetric: 77.5882

Epoch 81: val_loss did not improve from 50.75873
196/196 - 26s - loss: 77.8041 - MinusLogProbMetric: 77.8041 - val_loss: 77.5882 - val_MinusLogProbMetric: 77.5882 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 82/1000
2023-10-11 15:11:32.196 
Epoch 82/1000 
	 loss: 77.4150, MinusLogProbMetric: 77.4150, val_loss: 77.2512, val_MinusLogProbMetric: 77.2512

Epoch 82: val_loss did not improve from 50.75873
196/196 - 26s - loss: 77.4150 - MinusLogProbMetric: 77.4150 - val_loss: 77.2512 - val_MinusLogProbMetric: 77.2512 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 83/1000
2023-10-11 15:11:58.272 
Epoch 83/1000 
	 loss: 77.1148, MinusLogProbMetric: 77.1148, val_loss: 77.0511, val_MinusLogProbMetric: 77.0511

Epoch 83: val_loss did not improve from 50.75873
196/196 - 26s - loss: 77.1148 - MinusLogProbMetric: 77.1148 - val_loss: 77.0511 - val_MinusLogProbMetric: 77.0511 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 84/1000
2023-10-11 15:12:23.742 
Epoch 84/1000 
	 loss: 77.0251, MinusLogProbMetric: 77.0251, val_loss: 77.5884, val_MinusLogProbMetric: 77.5884

Epoch 84: val_loss did not improve from 50.75873
196/196 - 25s - loss: 77.0251 - MinusLogProbMetric: 77.0251 - val_loss: 77.5884 - val_MinusLogProbMetric: 77.5884 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 85/1000
2023-10-11 15:12:49.112 
Epoch 85/1000 
	 loss: 76.7713, MinusLogProbMetric: 76.7713, val_loss: 76.8284, val_MinusLogProbMetric: 76.8284

Epoch 85: val_loss did not improve from 50.75873
196/196 - 25s - loss: 76.7713 - MinusLogProbMetric: 76.7713 - val_loss: 76.8284 - val_MinusLogProbMetric: 76.8284 - lr: 5.0000e-04 - 25s/epoch - 129ms/step
Epoch 86/1000
2023-10-11 15:13:14.459 
Epoch 86/1000 
	 loss: 76.6972, MinusLogProbMetric: 76.6972, val_loss: 76.7729, val_MinusLogProbMetric: 76.7729

Epoch 86: val_loss did not improve from 50.75873
196/196 - 25s - loss: 76.6972 - MinusLogProbMetric: 76.6972 - val_loss: 76.7729 - val_MinusLogProbMetric: 76.7729 - lr: 5.0000e-04 - 25s/epoch - 129ms/step
Epoch 87/1000
2023-10-11 15:13:39.949 
Epoch 87/1000 
	 loss: 76.5981, MinusLogProbMetric: 76.5981, val_loss: 77.0054, val_MinusLogProbMetric: 77.0054

Epoch 87: val_loss did not improve from 50.75873
196/196 - 25s - loss: 76.5981 - MinusLogProbMetric: 76.5981 - val_loss: 77.0054 - val_MinusLogProbMetric: 77.0054 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 88/1000
2023-10-11 15:14:05.930 
Epoch 88/1000 
	 loss: 76.4802, MinusLogProbMetric: 76.4802, val_loss: 76.4328, val_MinusLogProbMetric: 76.4328

Epoch 88: val_loss did not improve from 50.75873
196/196 - 26s - loss: 76.4802 - MinusLogProbMetric: 76.4802 - val_loss: 76.4328 - val_MinusLogProbMetric: 76.4328 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 89/1000
2023-10-11 15:14:31.521 
Epoch 89/1000 
	 loss: 76.1263, MinusLogProbMetric: 76.1263, val_loss: 76.5075, val_MinusLogProbMetric: 76.5075

Epoch 89: val_loss did not improve from 50.75873
196/196 - 26s - loss: 76.1263 - MinusLogProbMetric: 76.1263 - val_loss: 76.5075 - val_MinusLogProbMetric: 76.5075 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 90/1000
2023-10-11 15:14:57.495 
Epoch 90/1000 
	 loss: 75.9959, MinusLogProbMetric: 75.9959, val_loss: 75.7877, val_MinusLogProbMetric: 75.7877

Epoch 90: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.9959 - MinusLogProbMetric: 75.9959 - val_loss: 75.7877 - val_MinusLogProbMetric: 75.7877 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 91/1000
2023-10-11 15:15:22.993 
Epoch 91/1000 
	 loss: 75.9917, MinusLogProbMetric: 75.9917, val_loss: 76.4053, val_MinusLogProbMetric: 76.4053

Epoch 91: val_loss did not improve from 50.75873
196/196 - 25s - loss: 75.9917 - MinusLogProbMetric: 75.9917 - val_loss: 76.4053 - val_MinusLogProbMetric: 76.4053 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 92/1000
2023-10-11 15:15:48.365 
Epoch 92/1000 
	 loss: 75.7125, MinusLogProbMetric: 75.7125, val_loss: 75.7008, val_MinusLogProbMetric: 75.7008

Epoch 92: val_loss did not improve from 50.75873
196/196 - 25s - loss: 75.7125 - MinusLogProbMetric: 75.7125 - val_loss: 75.7008 - val_MinusLogProbMetric: 75.7008 - lr: 5.0000e-04 - 25s/epoch - 129ms/step
Epoch 93/1000
2023-10-11 15:16:13.840 
Epoch 93/1000 
	 loss: 75.7322, MinusLogProbMetric: 75.7322, val_loss: 76.0273, val_MinusLogProbMetric: 76.0273

Epoch 93: val_loss did not improve from 50.75873
196/196 - 25s - loss: 75.7322 - MinusLogProbMetric: 75.7322 - val_loss: 76.0273 - val_MinusLogProbMetric: 76.0273 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 94/1000
2023-10-11 15:16:39.678 
Epoch 94/1000 
	 loss: 75.6790, MinusLogProbMetric: 75.6790, val_loss: 76.7872, val_MinusLogProbMetric: 76.7872

Epoch 94: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.6790 - MinusLogProbMetric: 75.6790 - val_loss: 76.7872 - val_MinusLogProbMetric: 76.7872 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 95/1000
2023-10-11 15:17:05.189 
Epoch 95/1000 
	 loss: 75.7598, MinusLogProbMetric: 75.7598, val_loss: 75.8217, val_MinusLogProbMetric: 75.8217

Epoch 95: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.7598 - MinusLogProbMetric: 75.7598 - val_loss: 75.8217 - val_MinusLogProbMetric: 75.8217 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 96/1000
2023-10-11 15:17:30.993 
Epoch 96/1000 
	 loss: 75.6455, MinusLogProbMetric: 75.6455, val_loss: 75.4196, val_MinusLogProbMetric: 75.4196

Epoch 96: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.6455 - MinusLogProbMetric: 75.6455 - val_loss: 75.4196 - val_MinusLogProbMetric: 75.4196 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 97/1000
2023-10-11 15:17:56.735 
Epoch 97/1000 
	 loss: 75.5471, MinusLogProbMetric: 75.5471, val_loss: 76.2098, val_MinusLogProbMetric: 76.2098

Epoch 97: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.5471 - MinusLogProbMetric: 75.5471 - val_loss: 76.2098 - val_MinusLogProbMetric: 76.2098 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 98/1000
2023-10-11 15:18:22.375 
Epoch 98/1000 
	 loss: 75.3724, MinusLogProbMetric: 75.3724, val_loss: 75.9628, val_MinusLogProbMetric: 75.9628

Epoch 98: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.3724 - MinusLogProbMetric: 75.3724 - val_loss: 75.9628 - val_MinusLogProbMetric: 75.9628 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 99/1000
2023-10-11 15:18:47.805 
Epoch 99/1000 
	 loss: 75.8743, MinusLogProbMetric: 75.8743, val_loss: 77.3405, val_MinusLogProbMetric: 77.3405

Epoch 99: val_loss did not improve from 50.75873
196/196 - 25s - loss: 75.8743 - MinusLogProbMetric: 75.8743 - val_loss: 77.3405 - val_MinusLogProbMetric: 77.3405 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 100/1000
2023-10-11 15:19:13.434 
Epoch 100/1000 
	 loss: 75.3546, MinusLogProbMetric: 75.3546, val_loss: 75.5556, val_MinusLogProbMetric: 75.5556

Epoch 100: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.3546 - MinusLogProbMetric: 75.3546 - val_loss: 75.5556 - val_MinusLogProbMetric: 75.5556 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 101/1000
2023-10-11 15:19:39.020 
Epoch 101/1000 
	 loss: 75.1005, MinusLogProbMetric: 75.1005, val_loss: 74.7008, val_MinusLogProbMetric: 74.7008

Epoch 101: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.1005 - MinusLogProbMetric: 75.1005 - val_loss: 74.7008 - val_MinusLogProbMetric: 74.7008 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 102/1000
2023-10-11 15:20:04.571 
Epoch 102/1000 
	 loss: 75.0184, MinusLogProbMetric: 75.0184, val_loss: 75.1367, val_MinusLogProbMetric: 75.1367

Epoch 102: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.0184 - MinusLogProbMetric: 75.0184 - val_loss: 75.1367 - val_MinusLogProbMetric: 75.1367 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 103/1000
2023-10-11 15:20:30.226 
Epoch 103/1000 
	 loss: 74.9598, MinusLogProbMetric: 74.9598, val_loss: 75.1421, val_MinusLogProbMetric: 75.1421

Epoch 103: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.9598 - MinusLogProbMetric: 74.9598 - val_loss: 75.1421 - val_MinusLogProbMetric: 75.1421 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 104/1000
2023-10-11 15:20:55.760 
Epoch 104/1000 
	 loss: 75.0121, MinusLogProbMetric: 75.0121, val_loss: 74.5890, val_MinusLogProbMetric: 74.5890

Epoch 104: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.0121 - MinusLogProbMetric: 75.0121 - val_loss: 74.5890 - val_MinusLogProbMetric: 74.5890 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 105/1000
2023-10-11 15:21:21.229 
Epoch 105/1000 
	 loss: 74.7827, MinusLogProbMetric: 74.7827, val_loss: 75.3148, val_MinusLogProbMetric: 75.3148

Epoch 105: val_loss did not improve from 50.75873
196/196 - 25s - loss: 74.7827 - MinusLogProbMetric: 74.7827 - val_loss: 75.3148 - val_MinusLogProbMetric: 75.3148 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 106/1000
2023-10-11 15:21:46.665 
Epoch 106/1000 
	 loss: 74.8147, MinusLogProbMetric: 74.8147, val_loss: 74.4830, val_MinusLogProbMetric: 74.4830

Epoch 106: val_loss did not improve from 50.75873
196/196 - 25s - loss: 74.8147 - MinusLogProbMetric: 74.8147 - val_loss: 74.4830 - val_MinusLogProbMetric: 74.4830 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 107/1000
2023-10-11 15:22:12.184 
Epoch 107/1000 
	 loss: 75.1835, MinusLogProbMetric: 75.1835, val_loss: 74.6097, val_MinusLogProbMetric: 74.6097

Epoch 107: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.1835 - MinusLogProbMetric: 75.1835 - val_loss: 74.6097 - val_MinusLogProbMetric: 74.6097 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 108/1000
2023-10-11 15:22:37.785 
Epoch 108/1000 
	 loss: 75.6884, MinusLogProbMetric: 75.6884, val_loss: 74.7303, val_MinusLogProbMetric: 74.7303

Epoch 108: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.6884 - MinusLogProbMetric: 75.6884 - val_loss: 74.7303 - val_MinusLogProbMetric: 74.7303 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 109/1000
2023-10-11 15:23:03.423 
Epoch 109/1000 
	 loss: 74.4355, MinusLogProbMetric: 74.4355, val_loss: 74.7477, val_MinusLogProbMetric: 74.7477

Epoch 109: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.4355 - MinusLogProbMetric: 74.4355 - val_loss: 74.7477 - val_MinusLogProbMetric: 74.7477 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 110/1000
2023-10-11 15:23:29.111 
Epoch 110/1000 
	 loss: 74.2808, MinusLogProbMetric: 74.2808, val_loss: 75.2197, val_MinusLogProbMetric: 75.2197

Epoch 110: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.2808 - MinusLogProbMetric: 74.2808 - val_loss: 75.2197 - val_MinusLogProbMetric: 75.2197 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 111/1000
2023-10-11 15:23:54.953 
Epoch 111/1000 
	 loss: 74.3893, MinusLogProbMetric: 74.3893, val_loss: 74.4900, val_MinusLogProbMetric: 74.4900

Epoch 111: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.3893 - MinusLogProbMetric: 74.3893 - val_loss: 74.4900 - val_MinusLogProbMetric: 74.4900 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 112/1000
2023-10-11 15:24:20.738 
Epoch 112/1000 
	 loss: 74.3182, MinusLogProbMetric: 74.3182, val_loss: 74.5057, val_MinusLogProbMetric: 74.5057

Epoch 112: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.3182 - MinusLogProbMetric: 74.3182 - val_loss: 74.5057 - val_MinusLogProbMetric: 74.5057 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 113/1000
2023-10-11 15:24:46.443 
Epoch 113/1000 
	 loss: 74.2043, MinusLogProbMetric: 74.2043, val_loss: 74.2824, val_MinusLogProbMetric: 74.2824

Epoch 113: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.2043 - MinusLogProbMetric: 74.2043 - val_loss: 74.2824 - val_MinusLogProbMetric: 74.2824 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 114/1000
2023-10-11 15:25:11.862 
Epoch 114/1000 
	 loss: 74.8761, MinusLogProbMetric: 74.8761, val_loss: 74.8832, val_MinusLogProbMetric: 74.8832

Epoch 114: val_loss did not improve from 50.75873
196/196 - 25s - loss: 74.8761 - MinusLogProbMetric: 74.8761 - val_loss: 74.8832 - val_MinusLogProbMetric: 74.8832 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 115/1000
2023-10-11 15:25:37.695 
Epoch 115/1000 
	 loss: 74.6484, MinusLogProbMetric: 74.6484, val_loss: 74.1354, val_MinusLogProbMetric: 74.1354

Epoch 115: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.6484 - MinusLogProbMetric: 74.6484 - val_loss: 74.1354 - val_MinusLogProbMetric: 74.1354 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 116/1000
2023-10-11 15:26:03.315 
Epoch 116/1000 
	 loss: 75.8010, MinusLogProbMetric: 75.8010, val_loss: 75.1979, val_MinusLogProbMetric: 75.1979

Epoch 116: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.8010 - MinusLogProbMetric: 75.8010 - val_loss: 75.1979 - val_MinusLogProbMetric: 75.1979 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 117/1000
2023-10-11 15:26:28.812 
Epoch 117/1000 
	 loss: 74.1166, MinusLogProbMetric: 74.1166, val_loss: 74.2803, val_MinusLogProbMetric: 74.2803

Epoch 117: val_loss did not improve from 50.75873
196/196 - 25s - loss: 74.1166 - MinusLogProbMetric: 74.1166 - val_loss: 74.2803 - val_MinusLogProbMetric: 74.2803 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 118/1000
2023-10-11 15:26:54.621 
Epoch 118/1000 
	 loss: 74.8413, MinusLogProbMetric: 74.8413, val_loss: 74.3996, val_MinusLogProbMetric: 74.3996

Epoch 118: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.8413 - MinusLogProbMetric: 74.8413 - val_loss: 74.3996 - val_MinusLogProbMetric: 74.3996 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 119/1000
2023-10-11 15:27:20.589 
Epoch 119/1000 
	 loss: 74.3974, MinusLogProbMetric: 74.3974, val_loss: 74.2198, val_MinusLogProbMetric: 74.2198

Epoch 119: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.3974 - MinusLogProbMetric: 74.3974 - val_loss: 74.2198 - val_MinusLogProbMetric: 74.2198 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 120/1000
2023-10-11 15:27:46.350 
Epoch 120/1000 
	 loss: 75.7158, MinusLogProbMetric: 75.7158, val_loss: 74.7008, val_MinusLogProbMetric: 74.7008

Epoch 120: val_loss did not improve from 50.75873
196/196 - 26s - loss: 75.7158 - MinusLogProbMetric: 75.7158 - val_loss: 74.7008 - val_MinusLogProbMetric: 74.7008 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 121/1000
2023-10-11 15:28:12.153 
Epoch 121/1000 
	 loss: 74.1588, MinusLogProbMetric: 74.1588, val_loss: 73.7709, val_MinusLogProbMetric: 73.7709

Epoch 121: val_loss did not improve from 50.75873
196/196 - 26s - loss: 74.1588 - MinusLogProbMetric: 74.1588 - val_loss: 73.7709 - val_MinusLogProbMetric: 73.7709 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 122/1000
2023-10-11 15:28:38.022 
Epoch 122/1000 
	 loss: 73.7525, MinusLogProbMetric: 73.7525, val_loss: 74.7641, val_MinusLogProbMetric: 74.7641

Epoch 122: val_loss did not improve from 50.75873
196/196 - 26s - loss: 73.7525 - MinusLogProbMetric: 73.7525 - val_loss: 74.7641 - val_MinusLogProbMetric: 74.7641 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 123/1000
2023-10-11 15:29:03.955 
Epoch 123/1000 
	 loss: 73.7918, MinusLogProbMetric: 73.7918, val_loss: 73.3574, val_MinusLogProbMetric: 73.3574

Epoch 123: val_loss did not improve from 50.75873
196/196 - 26s - loss: 73.7918 - MinusLogProbMetric: 73.7918 - val_loss: 73.3574 - val_MinusLogProbMetric: 73.3574 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 124/1000
2023-10-11 15:29:29.727 
Epoch 124/1000 
	 loss: 73.6991, MinusLogProbMetric: 73.6991, val_loss: 75.0802, val_MinusLogProbMetric: 75.0802

Epoch 124: val_loss did not improve from 50.75873
196/196 - 26s - loss: 73.6991 - MinusLogProbMetric: 73.6991 - val_loss: 75.0802 - val_MinusLogProbMetric: 75.0802 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 125/1000
2023-10-11 15:29:55.242 
Epoch 125/1000 
	 loss: 73.6062, MinusLogProbMetric: 73.6062, val_loss: 73.0897, val_MinusLogProbMetric: 73.0897

Epoch 125: val_loss did not improve from 50.75873
196/196 - 26s - loss: 73.6062 - MinusLogProbMetric: 73.6062 - val_loss: 73.0897 - val_MinusLogProbMetric: 73.0897 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 126/1000
2023-10-11 15:30:20.754 
Epoch 126/1000 
	 loss: 73.3903, MinusLogProbMetric: 73.3903, val_loss: 73.1944, val_MinusLogProbMetric: 73.1944

Epoch 126: val_loss did not improve from 50.75873
196/196 - 26s - loss: 73.3903 - MinusLogProbMetric: 73.3903 - val_loss: 73.1944 - val_MinusLogProbMetric: 73.1944 - lr: 5.0000e-04 - 26s/epoch - 130ms/step
Epoch 127/1000
2023-10-11 15:30:46.364 
Epoch 127/1000 
	 loss: 73.1625, MinusLogProbMetric: 73.1625, val_loss: 73.4244, val_MinusLogProbMetric: 73.4244

Epoch 127: val_loss did not improve from 50.75873
196/196 - 26s - loss: 73.1625 - MinusLogProbMetric: 73.1625 - val_loss: 73.4244 - val_MinusLogProbMetric: 73.4244 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 128/1000
2023-10-11 15:31:11.900 
Epoch 128/1000 
	 loss: 74.0237, MinusLogProbMetric: 74.0237, val_loss: 87.9114, val_MinusLogProbMetric: 87.9114

Epoch 128: val_loss did not improve from 50.75873
Restoring model weights from the end of the best epoch: 28.
196/196 - 26s - loss: 74.0237 - MinusLogProbMetric: 74.0237 - val_loss: 87.9114 - val_MinusLogProbMetric: 87.9114 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 128: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 12.93969217594713 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 7.325424941955134 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 5.70628800499253 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 5.42463855096139 seconds.
Training succeeded with seed 0.
Model trained in 3428.19 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 32.29 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 32.56 s.
===========
Run 321/720 done in 3464.15 s.
===========

===========
Generating train data for run 322.
===========
Train data generated in 0.11 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_322/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_322/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_322/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_322
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_352"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_353 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f38f7567010>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f39d49ed5d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f39d49ed5d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c74de19f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c74d39de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c74d3a350>, <keras.callbacks.ModelCheckpoint object at 0x7f3c74d3a410>, <keras.callbacks.EarlyStopping object at 0x7f3c74d3a680>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c74d3a6b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c74d3a2f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_322/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 322/720 with hyperparameters:
timestamp = 2023-10-11 15:31:47.163742
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-10-11 15:33:13.188 
Epoch 1/1000 
	 loss: 851.9106, MinusLogProbMetric: 851.9106, val_loss: 167.2025, val_MinusLogProbMetric: 167.2025

Epoch 1: val_loss improved from inf to 167.20251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 86s - loss: 851.9106 - MinusLogProbMetric: 851.9106 - val_loss: 167.2025 - val_MinusLogProbMetric: 167.2025 - lr: 0.0010 - 86s/epoch - 441ms/step
Epoch 2/1000
2023-10-11 15:33:39.697 
Epoch 2/1000 
	 loss: 118.6772, MinusLogProbMetric: 118.6772, val_loss: 90.6084, val_MinusLogProbMetric: 90.6084

Epoch 2: val_loss improved from 167.20251 to 90.60842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 26s - loss: 118.6772 - MinusLogProbMetric: 118.6772 - val_loss: 90.6084 - val_MinusLogProbMetric: 90.6084 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 3/1000
2023-10-11 15:34:06.277 
Epoch 3/1000 
	 loss: 78.1951, MinusLogProbMetric: 78.1951, val_loss: 72.4081, val_MinusLogProbMetric: 72.4081

Epoch 3: val_loss improved from 90.60842 to 72.40807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 78.1951 - MinusLogProbMetric: 78.1951 - val_loss: 72.4081 - val_MinusLogProbMetric: 72.4081 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 4/1000
2023-10-11 15:34:32.982 
Epoch 4/1000 
	 loss: 64.9160, MinusLogProbMetric: 64.9160, val_loss: 60.7383, val_MinusLogProbMetric: 60.7383

Epoch 4: val_loss improved from 72.40807 to 60.73835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 64.9160 - MinusLogProbMetric: 64.9160 - val_loss: 60.7383 - val_MinusLogProbMetric: 60.7383 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 5/1000
2023-10-11 15:34:59.340 
Epoch 5/1000 
	 loss: 58.0433, MinusLogProbMetric: 58.0433, val_loss: 56.7999, val_MinusLogProbMetric: 56.7999

Epoch 5: val_loss improved from 60.73835 to 56.79986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 26s - loss: 58.0433 - MinusLogProbMetric: 58.0433 - val_loss: 56.7999 - val_MinusLogProbMetric: 56.7999 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 6/1000
2023-10-11 15:35:25.991 
Epoch 6/1000 
	 loss: 54.0545, MinusLogProbMetric: 54.0545, val_loss: 50.8788, val_MinusLogProbMetric: 50.8788

Epoch 6: val_loss improved from 56.79986 to 50.87884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 54.0545 - MinusLogProbMetric: 54.0545 - val_loss: 50.8788 - val_MinusLogProbMetric: 50.8788 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 7/1000
2023-10-11 15:35:52.782 
Epoch 7/1000 
	 loss: 56.8956, MinusLogProbMetric: 56.8956, val_loss: 51.4986, val_MinusLogProbMetric: 51.4986

Epoch 7: val_loss did not improve from 50.87884
196/196 - 26s - loss: 56.8956 - MinusLogProbMetric: 56.8956 - val_loss: 51.4986 - val_MinusLogProbMetric: 51.4986 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 8/1000
2023-10-11 15:36:19.165 
Epoch 8/1000 
	 loss: 48.5283, MinusLogProbMetric: 48.5283, val_loss: 48.8626, val_MinusLogProbMetric: 48.8626

Epoch 8: val_loss improved from 50.87884 to 48.86258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 48.5283 - MinusLogProbMetric: 48.5283 - val_loss: 48.8626 - val_MinusLogProbMetric: 48.8626 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 9/1000
2023-10-11 15:36:46.352 
Epoch 9/1000 
	 loss: 46.5186, MinusLogProbMetric: 46.5186, val_loss: 45.5223, val_MinusLogProbMetric: 45.5223

Epoch 9: val_loss improved from 48.86258 to 45.52225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 46.5186 - MinusLogProbMetric: 46.5186 - val_loss: 45.5223 - val_MinusLogProbMetric: 45.5223 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 10/1000
2023-10-11 15:37:12.681 
Epoch 10/1000 
	 loss: 45.5078, MinusLogProbMetric: 45.5078, val_loss: 44.8037, val_MinusLogProbMetric: 44.8037

Epoch 10: val_loss improved from 45.52225 to 44.80372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 26s - loss: 45.5078 - MinusLogProbMetric: 45.5078 - val_loss: 44.8037 - val_MinusLogProbMetric: 44.8037 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 11/1000
2023-10-11 15:37:39.298 
Epoch 11/1000 
	 loss: 44.1061, MinusLogProbMetric: 44.1061, val_loss: 44.0041, val_MinusLogProbMetric: 44.0041

Epoch 11: val_loss improved from 44.80372 to 44.00406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 44.1061 - MinusLogProbMetric: 44.1061 - val_loss: 44.0041 - val_MinusLogProbMetric: 44.0041 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 12/1000
2023-10-11 15:38:05.926 
Epoch 12/1000 
	 loss: 43.4853, MinusLogProbMetric: 43.4853, val_loss: 41.7450, val_MinusLogProbMetric: 41.7450

Epoch 12: val_loss improved from 44.00406 to 41.74503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 43.4853 - MinusLogProbMetric: 43.4853 - val_loss: 41.7450 - val_MinusLogProbMetric: 41.7450 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 13/1000
2023-10-11 15:38:32.758 
Epoch 13/1000 
	 loss: 42.1042, MinusLogProbMetric: 42.1042, val_loss: 42.6452, val_MinusLogProbMetric: 42.6452

Epoch 13: val_loss did not improve from 41.74503
196/196 - 26s - loss: 42.1042 - MinusLogProbMetric: 42.1042 - val_loss: 42.6452 - val_MinusLogProbMetric: 42.6452 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 14/1000
2023-10-11 15:38:58.926 
Epoch 14/1000 
	 loss: 42.2744, MinusLogProbMetric: 42.2744, val_loss: 39.9209, val_MinusLogProbMetric: 39.9209

Epoch 14: val_loss improved from 41.74503 to 39.92093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 42.2744 - MinusLogProbMetric: 42.2744 - val_loss: 39.9209 - val_MinusLogProbMetric: 39.9209 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 15/1000
2023-10-11 15:39:25.055 
Epoch 15/1000 
	 loss: 41.0155, MinusLogProbMetric: 41.0155, val_loss: 41.0132, val_MinusLogProbMetric: 41.0132

Epoch 15: val_loss did not improve from 39.92093
196/196 - 26s - loss: 41.0155 - MinusLogProbMetric: 41.0155 - val_loss: 41.0132 - val_MinusLogProbMetric: 41.0132 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 16/1000
2023-10-11 15:39:51.155 
Epoch 16/1000 
	 loss: 40.3348, MinusLogProbMetric: 40.3348, val_loss: 40.3700, val_MinusLogProbMetric: 40.3700

Epoch 16: val_loss did not improve from 39.92093
196/196 - 26s - loss: 40.3348 - MinusLogProbMetric: 40.3348 - val_loss: 40.3700 - val_MinusLogProbMetric: 40.3700 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 17/1000
2023-10-11 15:40:17.339 
Epoch 17/1000 
	 loss: 40.6739, MinusLogProbMetric: 40.6739, val_loss: 39.5626, val_MinusLogProbMetric: 39.5626

Epoch 17: val_loss improved from 39.92093 to 39.56259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 40.6739 - MinusLogProbMetric: 40.6739 - val_loss: 39.5626 - val_MinusLogProbMetric: 39.5626 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 18/1000
2023-10-11 15:40:44.215 
Epoch 18/1000 
	 loss: 39.5639, MinusLogProbMetric: 39.5639, val_loss: 39.5845, val_MinusLogProbMetric: 39.5845

Epoch 18: val_loss did not improve from 39.56259
196/196 - 26s - loss: 39.5639 - MinusLogProbMetric: 39.5639 - val_loss: 39.5845 - val_MinusLogProbMetric: 39.5845 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 19/1000
2023-10-11 15:41:10.211 
Epoch 19/1000 
	 loss: 39.1203, MinusLogProbMetric: 39.1203, val_loss: 38.4963, val_MinusLogProbMetric: 38.4963

Epoch 19: val_loss improved from 39.56259 to 38.49633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 26s - loss: 39.1203 - MinusLogProbMetric: 39.1203 - val_loss: 38.4963 - val_MinusLogProbMetric: 38.4963 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 20/1000
2023-10-11 15:41:37.021 
Epoch 20/1000 
	 loss: 39.3320, MinusLogProbMetric: 39.3320, val_loss: 39.8170, val_MinusLogProbMetric: 39.8170

Epoch 20: val_loss did not improve from 38.49633
196/196 - 26s - loss: 39.3320 - MinusLogProbMetric: 39.3320 - val_loss: 39.8170 - val_MinusLogProbMetric: 39.8170 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 21/1000
2023-10-11 15:42:03.918 
Epoch 21/1000 
	 loss: 38.3364, MinusLogProbMetric: 38.3364, val_loss: 37.1041, val_MinusLogProbMetric: 37.1041

Epoch 21: val_loss improved from 38.49633 to 37.10406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 38.3364 - MinusLogProbMetric: 38.3364 - val_loss: 37.1041 - val_MinusLogProbMetric: 37.1041 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 22/1000
2023-10-11 15:42:30.284 
Epoch 22/1000 
	 loss: 37.9417, MinusLogProbMetric: 37.9417, val_loss: 38.9854, val_MinusLogProbMetric: 38.9854

Epoch 22: val_loss did not improve from 37.10406
196/196 - 26s - loss: 37.9417 - MinusLogProbMetric: 37.9417 - val_loss: 38.9854 - val_MinusLogProbMetric: 38.9854 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 23/1000
2023-10-11 15:42:56.276 
Epoch 23/1000 
	 loss: 38.1849, MinusLogProbMetric: 38.1849, val_loss: 38.4671, val_MinusLogProbMetric: 38.4671

Epoch 23: val_loss did not improve from 37.10406
196/196 - 26s - loss: 38.1849 - MinusLogProbMetric: 38.1849 - val_loss: 38.4671 - val_MinusLogProbMetric: 38.4671 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 24/1000
2023-10-11 15:43:22.174 
Epoch 24/1000 
	 loss: 38.5772, MinusLogProbMetric: 38.5772, val_loss: 36.4471, val_MinusLogProbMetric: 36.4471

Epoch 24: val_loss improved from 37.10406 to 36.44706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 26s - loss: 38.5772 - MinusLogProbMetric: 38.5772 - val_loss: 36.4471 - val_MinusLogProbMetric: 36.4471 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 25/1000
2023-10-11 15:43:48.461 
Epoch 25/1000 
	 loss: 36.8969, MinusLogProbMetric: 36.8969, val_loss: 36.4054, val_MinusLogProbMetric: 36.4054

Epoch 25: val_loss improved from 36.44706 to 36.40540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 26s - loss: 36.8969 - MinusLogProbMetric: 36.8969 - val_loss: 36.4054 - val_MinusLogProbMetric: 36.4054 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 26/1000
2023-10-11 15:44:14.658 
Epoch 26/1000 
	 loss: 37.4787, MinusLogProbMetric: 37.4787, val_loss: 36.7653, val_MinusLogProbMetric: 36.7653

Epoch 26: val_loss did not improve from 36.40540
196/196 - 26s - loss: 37.4787 - MinusLogProbMetric: 37.4787 - val_loss: 36.7653 - val_MinusLogProbMetric: 36.7653 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 27/1000
2023-10-11 15:44:40.723 
Epoch 27/1000 
	 loss: 36.4821, MinusLogProbMetric: 36.4821, val_loss: 36.7714, val_MinusLogProbMetric: 36.7714

Epoch 27: val_loss did not improve from 36.40540
196/196 - 26s - loss: 36.4821 - MinusLogProbMetric: 36.4821 - val_loss: 36.7714 - val_MinusLogProbMetric: 36.7714 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 28/1000
2023-10-11 15:45:06.603 
Epoch 28/1000 
	 loss: 36.6270, MinusLogProbMetric: 36.6270, val_loss: 37.2293, val_MinusLogProbMetric: 37.2293

Epoch 28: val_loss did not improve from 36.40540
196/196 - 26s - loss: 36.6270 - MinusLogProbMetric: 36.6270 - val_loss: 37.2293 - val_MinusLogProbMetric: 37.2293 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 29/1000
2023-10-11 15:45:32.772 
Epoch 29/1000 
	 loss: 36.6237, MinusLogProbMetric: 36.6237, val_loss: 35.4262, val_MinusLogProbMetric: 35.4262

Epoch 29: val_loss improved from 36.40540 to 35.42621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 36.6237 - MinusLogProbMetric: 36.6237 - val_loss: 35.4262 - val_MinusLogProbMetric: 35.4262 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 30/1000
2023-10-11 15:45:59.516 
Epoch 30/1000 
	 loss: 35.9841, MinusLogProbMetric: 35.9841, val_loss: 35.8060, val_MinusLogProbMetric: 35.8060

Epoch 30: val_loss did not improve from 35.42621
196/196 - 26s - loss: 35.9841 - MinusLogProbMetric: 35.9841 - val_loss: 35.8060 - val_MinusLogProbMetric: 35.8060 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 31/1000
2023-10-11 15:46:25.441 
Epoch 31/1000 
	 loss: 36.5240, MinusLogProbMetric: 36.5240, val_loss: 37.4537, val_MinusLogProbMetric: 37.4537

Epoch 31: val_loss did not improve from 35.42621
196/196 - 26s - loss: 36.5240 - MinusLogProbMetric: 36.5240 - val_loss: 37.4537 - val_MinusLogProbMetric: 37.4537 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 32/1000
2023-10-11 15:46:51.865 
Epoch 32/1000 
	 loss: 36.4765, MinusLogProbMetric: 36.4765, val_loss: 34.4941, val_MinusLogProbMetric: 34.4941

Epoch 32: val_loss improved from 35.42621 to 34.49405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 36.4765 - MinusLogProbMetric: 36.4765 - val_loss: 34.4941 - val_MinusLogProbMetric: 34.4941 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 33/1000
2023-10-11 15:47:19.011 
Epoch 33/1000 
	 loss: 35.3092, MinusLogProbMetric: 35.3092, val_loss: 35.1472, val_MinusLogProbMetric: 35.1472

Epoch 33: val_loss did not improve from 34.49405
196/196 - 27s - loss: 35.3092 - MinusLogProbMetric: 35.3092 - val_loss: 35.1472 - val_MinusLogProbMetric: 35.1472 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 34/1000
2023-10-11 15:47:45.785 
Epoch 34/1000 
	 loss: 35.6411, MinusLogProbMetric: 35.6411, val_loss: 35.7339, val_MinusLogProbMetric: 35.7339

Epoch 34: val_loss did not improve from 34.49405
196/196 - 27s - loss: 35.6411 - MinusLogProbMetric: 35.6411 - val_loss: 35.7339 - val_MinusLogProbMetric: 35.7339 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 35/1000
2023-10-11 15:48:11.850 
Epoch 35/1000 
	 loss: 35.3067, MinusLogProbMetric: 35.3067, val_loss: 34.0590, val_MinusLogProbMetric: 34.0590

Epoch 35: val_loss improved from 34.49405 to 34.05899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 35.3067 - MinusLogProbMetric: 35.3067 - val_loss: 34.0590 - val_MinusLogProbMetric: 34.0590 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 36/1000
2023-10-11 15:48:38.217 
Epoch 36/1000 
	 loss: 35.9630, MinusLogProbMetric: 35.9630, val_loss: 35.4733, val_MinusLogProbMetric: 35.4733

Epoch 36: val_loss did not improve from 34.05899
196/196 - 26s - loss: 35.9630 - MinusLogProbMetric: 35.9630 - val_loss: 35.4733 - val_MinusLogProbMetric: 35.4733 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 37/1000
2023-10-11 15:49:04.031 
Epoch 37/1000 
	 loss: 34.8651, MinusLogProbMetric: 34.8651, val_loss: 35.2000, val_MinusLogProbMetric: 35.2000

Epoch 37: val_loss did not improve from 34.05899
196/196 - 26s - loss: 34.8651 - MinusLogProbMetric: 34.8651 - val_loss: 35.2000 - val_MinusLogProbMetric: 35.2000 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 38/1000
2023-10-11 15:49:30.183 
Epoch 38/1000 
	 loss: 35.3355, MinusLogProbMetric: 35.3355, val_loss: 36.3327, val_MinusLogProbMetric: 36.3327

Epoch 38: val_loss did not improve from 34.05899
196/196 - 26s - loss: 35.3355 - MinusLogProbMetric: 35.3355 - val_loss: 36.3327 - val_MinusLogProbMetric: 36.3327 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 39/1000
2023-10-11 15:49:55.968 
Epoch 39/1000 
	 loss: 34.9852, MinusLogProbMetric: 34.9852, val_loss: 34.4171, val_MinusLogProbMetric: 34.4171

Epoch 39: val_loss did not improve from 34.05899
196/196 - 26s - loss: 34.9852 - MinusLogProbMetric: 34.9852 - val_loss: 34.4171 - val_MinusLogProbMetric: 34.4171 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 40/1000
2023-10-11 15:50:21.993 
Epoch 40/1000 
	 loss: 35.6781, MinusLogProbMetric: 35.6781, val_loss: 34.6427, val_MinusLogProbMetric: 34.6427

Epoch 40: val_loss did not improve from 34.05899
196/196 - 26s - loss: 35.6781 - MinusLogProbMetric: 35.6781 - val_loss: 34.6427 - val_MinusLogProbMetric: 34.6427 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 41/1000
2023-10-11 15:50:48.308 
Epoch 41/1000 
	 loss: 34.8375, MinusLogProbMetric: 34.8375, val_loss: 36.3795, val_MinusLogProbMetric: 36.3795

Epoch 41: val_loss did not improve from 34.05899
196/196 - 26s - loss: 34.8375 - MinusLogProbMetric: 34.8375 - val_loss: 36.3795 - val_MinusLogProbMetric: 36.3795 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 42/1000
2023-10-11 15:51:15.702 
Epoch 42/1000 
	 loss: 34.5474, MinusLogProbMetric: 34.5474, val_loss: 34.8857, val_MinusLogProbMetric: 34.8857

Epoch 42: val_loss did not improve from 34.05899
196/196 - 27s - loss: 34.5474 - MinusLogProbMetric: 34.5474 - val_loss: 34.8857 - val_MinusLogProbMetric: 34.8857 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 43/1000
2023-10-11 15:51:42.741 
Epoch 43/1000 
	 loss: 34.5357, MinusLogProbMetric: 34.5357, val_loss: 35.4473, val_MinusLogProbMetric: 35.4473

Epoch 43: val_loss did not improve from 34.05899
196/196 - 27s - loss: 34.5357 - MinusLogProbMetric: 34.5357 - val_loss: 35.4473 - val_MinusLogProbMetric: 35.4473 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 44/1000
2023-10-11 15:52:09.400 
Epoch 44/1000 
	 loss: 34.5362, MinusLogProbMetric: 34.5362, val_loss: 34.2059, val_MinusLogProbMetric: 34.2059

Epoch 44: val_loss did not improve from 34.05899
196/196 - 27s - loss: 34.5362 - MinusLogProbMetric: 34.5362 - val_loss: 34.2059 - val_MinusLogProbMetric: 34.2059 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 45/1000
2023-10-11 15:52:36.780 
Epoch 45/1000 
	 loss: 34.1191, MinusLogProbMetric: 34.1191, val_loss: 37.5150, val_MinusLogProbMetric: 37.5150

Epoch 45: val_loss did not improve from 34.05899
196/196 - 27s - loss: 34.1191 - MinusLogProbMetric: 34.1191 - val_loss: 37.5150 - val_MinusLogProbMetric: 37.5150 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 46/1000
2023-10-11 15:53:03.935 
Epoch 46/1000 
	 loss: 35.8395, MinusLogProbMetric: 35.8395, val_loss: 38.6565, val_MinusLogProbMetric: 38.6565

Epoch 46: val_loss did not improve from 34.05899
196/196 - 27s - loss: 35.8395 - MinusLogProbMetric: 35.8395 - val_loss: 38.6565 - val_MinusLogProbMetric: 38.6565 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 47/1000
2023-10-11 15:53:30.940 
Epoch 47/1000 
	 loss: 34.0268, MinusLogProbMetric: 34.0268, val_loss: 33.2792, val_MinusLogProbMetric: 33.2792

Epoch 47: val_loss improved from 34.05899 to 33.27917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 34.0268 - MinusLogProbMetric: 34.0268 - val_loss: 33.2792 - val_MinusLogProbMetric: 33.2792 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 48/1000
2023-10-11 15:53:58.127 
Epoch 48/1000 
	 loss: 34.0861, MinusLogProbMetric: 34.0861, val_loss: 35.1511, val_MinusLogProbMetric: 35.1511

Epoch 48: val_loss did not improve from 33.27917
196/196 - 27s - loss: 34.0861 - MinusLogProbMetric: 34.0861 - val_loss: 35.1511 - val_MinusLogProbMetric: 35.1511 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 49/1000
2023-10-11 15:54:24.719 
Epoch 49/1000 
	 loss: 34.2290, MinusLogProbMetric: 34.2290, val_loss: 36.9601, val_MinusLogProbMetric: 36.9601

Epoch 49: val_loss did not improve from 33.27917
196/196 - 27s - loss: 34.2290 - MinusLogProbMetric: 34.2290 - val_loss: 36.9601 - val_MinusLogProbMetric: 36.9601 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 50/1000
2023-10-11 15:54:51.593 
Epoch 50/1000 
	 loss: 33.7130, MinusLogProbMetric: 33.7130, val_loss: 36.2123, val_MinusLogProbMetric: 36.2123

Epoch 50: val_loss did not improve from 33.27917
196/196 - 27s - loss: 33.7130 - MinusLogProbMetric: 33.7130 - val_loss: 36.2123 - val_MinusLogProbMetric: 36.2123 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 51/1000
2023-10-11 15:55:18.070 
Epoch 51/1000 
	 loss: 33.5726, MinusLogProbMetric: 33.5726, val_loss: 34.4339, val_MinusLogProbMetric: 34.4339

Epoch 51: val_loss did not improve from 33.27917
196/196 - 26s - loss: 33.5726 - MinusLogProbMetric: 33.5726 - val_loss: 34.4339 - val_MinusLogProbMetric: 34.4339 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 52/1000
2023-10-11 15:55:44.977 
Epoch 52/1000 
	 loss: 34.7444, MinusLogProbMetric: 34.7444, val_loss: 33.7423, val_MinusLogProbMetric: 33.7423

Epoch 52: val_loss did not improve from 33.27917
196/196 - 27s - loss: 34.7444 - MinusLogProbMetric: 34.7444 - val_loss: 33.7423 - val_MinusLogProbMetric: 33.7423 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 53/1000
2023-10-11 15:56:12.034 
Epoch 53/1000 
	 loss: 33.7326, MinusLogProbMetric: 33.7326, val_loss: 34.8155, val_MinusLogProbMetric: 34.8155

Epoch 53: val_loss did not improve from 33.27917
196/196 - 27s - loss: 33.7326 - MinusLogProbMetric: 33.7326 - val_loss: 34.8155 - val_MinusLogProbMetric: 34.8155 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 54/1000
2023-10-11 15:56:38.757 
Epoch 54/1000 
	 loss: 34.0402, MinusLogProbMetric: 34.0402, val_loss: 38.1178, val_MinusLogProbMetric: 38.1178

Epoch 54: val_loss did not improve from 33.27917
196/196 - 27s - loss: 34.0402 - MinusLogProbMetric: 34.0402 - val_loss: 38.1178 - val_MinusLogProbMetric: 38.1178 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 55/1000
2023-10-11 15:57:05.855 
Epoch 55/1000 
	 loss: 33.4209, MinusLogProbMetric: 33.4209, val_loss: 33.5164, val_MinusLogProbMetric: 33.5164

Epoch 55: val_loss did not improve from 33.27917
196/196 - 27s - loss: 33.4209 - MinusLogProbMetric: 33.4209 - val_loss: 33.5164 - val_MinusLogProbMetric: 33.5164 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 56/1000
2023-10-11 15:57:32.411 
Epoch 56/1000 
	 loss: 33.9733, MinusLogProbMetric: 33.9733, val_loss: 33.8765, val_MinusLogProbMetric: 33.8765

Epoch 56: val_loss did not improve from 33.27917
196/196 - 27s - loss: 33.9733 - MinusLogProbMetric: 33.9733 - val_loss: 33.8765 - val_MinusLogProbMetric: 33.8765 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 57/1000
2023-10-11 15:57:59.082 
Epoch 57/1000 
	 loss: 33.9717, MinusLogProbMetric: 33.9717, val_loss: 32.7164, val_MinusLogProbMetric: 32.7164

Epoch 57: val_loss improved from 33.27917 to 32.71635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 33.9717 - MinusLogProbMetric: 33.9717 - val_loss: 32.7164 - val_MinusLogProbMetric: 32.7164 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 58/1000
2023-10-11 15:58:26.452 
Epoch 58/1000 
	 loss: 33.0450, MinusLogProbMetric: 33.0450, val_loss: 36.1793, val_MinusLogProbMetric: 36.1793

Epoch 58: val_loss did not improve from 32.71635
196/196 - 27s - loss: 33.0450 - MinusLogProbMetric: 33.0450 - val_loss: 36.1793 - val_MinusLogProbMetric: 36.1793 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 59/1000
2023-10-11 15:58:53.316 
Epoch 59/1000 
	 loss: 35.4394, MinusLogProbMetric: 35.4394, val_loss: 33.1469, val_MinusLogProbMetric: 33.1469

Epoch 59: val_loss did not improve from 32.71635
196/196 - 27s - loss: 35.4394 - MinusLogProbMetric: 35.4394 - val_loss: 33.1469 - val_MinusLogProbMetric: 33.1469 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 60/1000
2023-10-11 15:59:20.105 
Epoch 60/1000 
	 loss: 33.2294, MinusLogProbMetric: 33.2294, val_loss: 33.9397, val_MinusLogProbMetric: 33.9397

Epoch 60: val_loss did not improve from 32.71635
196/196 - 27s - loss: 33.2294 - MinusLogProbMetric: 33.2294 - val_loss: 33.9397 - val_MinusLogProbMetric: 33.9397 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 61/1000
2023-10-11 15:59:47.217 
Epoch 61/1000 
	 loss: 32.9904, MinusLogProbMetric: 32.9904, val_loss: 32.7177, val_MinusLogProbMetric: 32.7177

Epoch 61: val_loss did not improve from 32.71635
196/196 - 27s - loss: 32.9904 - MinusLogProbMetric: 32.9904 - val_loss: 32.7177 - val_MinusLogProbMetric: 32.7177 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 62/1000
2023-10-11 16:00:13.889 
Epoch 62/1000 
	 loss: 32.9927, MinusLogProbMetric: 32.9927, val_loss: 32.9201, val_MinusLogProbMetric: 32.9201

Epoch 62: val_loss did not improve from 32.71635
196/196 - 27s - loss: 32.9927 - MinusLogProbMetric: 32.9927 - val_loss: 32.9201 - val_MinusLogProbMetric: 32.9201 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 63/1000
2023-10-11 16:00:40.858 
Epoch 63/1000 
	 loss: 33.1541, MinusLogProbMetric: 33.1541, val_loss: 33.2172, val_MinusLogProbMetric: 33.2172

Epoch 63: val_loss did not improve from 32.71635
196/196 - 27s - loss: 33.1541 - MinusLogProbMetric: 33.1541 - val_loss: 33.2172 - val_MinusLogProbMetric: 33.2172 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 64/1000
2023-10-11 16:01:07.694 
Epoch 64/1000 
	 loss: 32.9020, MinusLogProbMetric: 32.9020, val_loss: 33.4973, val_MinusLogProbMetric: 33.4973

Epoch 64: val_loss did not improve from 32.71635
196/196 - 27s - loss: 32.9020 - MinusLogProbMetric: 32.9020 - val_loss: 33.4973 - val_MinusLogProbMetric: 33.4973 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 65/1000
2023-10-11 16:01:34.615 
Epoch 65/1000 
	 loss: 33.9559, MinusLogProbMetric: 33.9559, val_loss: 32.8314, val_MinusLogProbMetric: 32.8314

Epoch 65: val_loss did not improve from 32.71635
196/196 - 27s - loss: 33.9559 - MinusLogProbMetric: 33.9559 - val_loss: 32.8314 - val_MinusLogProbMetric: 32.8314 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 66/1000
2023-10-11 16:02:01.570 
Epoch 66/1000 
	 loss: 32.8736, MinusLogProbMetric: 32.8736, val_loss: 33.6224, val_MinusLogProbMetric: 33.6224

Epoch 66: val_loss did not improve from 32.71635
196/196 - 27s - loss: 32.8736 - MinusLogProbMetric: 32.8736 - val_loss: 33.6224 - val_MinusLogProbMetric: 33.6224 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 67/1000
2023-10-11 16:02:28.203 
Epoch 67/1000 
	 loss: 32.8845, MinusLogProbMetric: 32.8845, val_loss: 32.2264, val_MinusLogProbMetric: 32.2264

Epoch 67: val_loss improved from 32.71635 to 32.22644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 32.8845 - MinusLogProbMetric: 32.8845 - val_loss: 32.2264 - val_MinusLogProbMetric: 32.2264 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 68/1000
2023-10-11 16:02:56.355 
Epoch 68/1000 
	 loss: 33.4571, MinusLogProbMetric: 33.4571, val_loss: 32.4017, val_MinusLogProbMetric: 32.4017

Epoch 68: val_loss did not improve from 32.22644
196/196 - 28s - loss: 33.4571 - MinusLogProbMetric: 33.4571 - val_loss: 32.4017 - val_MinusLogProbMetric: 32.4017 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 69/1000
2023-10-11 16:03:23.829 
Epoch 69/1000 
	 loss: 32.8355, MinusLogProbMetric: 32.8355, val_loss: 33.1445, val_MinusLogProbMetric: 33.1445

Epoch 69: val_loss did not improve from 32.22644
196/196 - 27s - loss: 32.8355 - MinusLogProbMetric: 32.8355 - val_loss: 33.1445 - val_MinusLogProbMetric: 33.1445 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 70/1000
2023-10-11 16:03:50.528 
Epoch 70/1000 
	 loss: 33.0308, MinusLogProbMetric: 33.0308, val_loss: 32.3635, val_MinusLogProbMetric: 32.3635

Epoch 70: val_loss did not improve from 32.22644
196/196 - 27s - loss: 33.0308 - MinusLogProbMetric: 33.0308 - val_loss: 32.3635 - val_MinusLogProbMetric: 32.3635 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 71/1000
2023-10-11 16:04:17.291 
Epoch 71/1000 
	 loss: 32.9818, MinusLogProbMetric: 32.9818, val_loss: 32.4455, val_MinusLogProbMetric: 32.4455

Epoch 71: val_loss did not improve from 32.22644
196/196 - 27s - loss: 32.9818 - MinusLogProbMetric: 32.9818 - val_loss: 32.4455 - val_MinusLogProbMetric: 32.4455 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 72/1000
2023-10-11 16:04:43.965 
Epoch 72/1000 
	 loss: 33.3775, MinusLogProbMetric: 33.3775, val_loss: 33.0236, val_MinusLogProbMetric: 33.0236

Epoch 72: val_loss did not improve from 32.22644
196/196 - 27s - loss: 33.3775 - MinusLogProbMetric: 33.3775 - val_loss: 33.0236 - val_MinusLogProbMetric: 33.0236 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 73/1000
2023-10-11 16:05:10.932 
Epoch 73/1000 
	 loss: 32.3475, MinusLogProbMetric: 32.3475, val_loss: 34.6791, val_MinusLogProbMetric: 34.6791

Epoch 73: val_loss did not improve from 32.22644
196/196 - 27s - loss: 32.3475 - MinusLogProbMetric: 32.3475 - val_loss: 34.6791 - val_MinusLogProbMetric: 34.6791 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 74/1000
2023-10-11 16:05:37.582 
Epoch 74/1000 
	 loss: 33.9256, MinusLogProbMetric: 33.9256, val_loss: 35.0181, val_MinusLogProbMetric: 35.0181

Epoch 74: val_loss did not improve from 32.22644
196/196 - 27s - loss: 33.9256 - MinusLogProbMetric: 33.9256 - val_loss: 35.0181 - val_MinusLogProbMetric: 35.0181 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 75/1000
2023-10-11 16:06:04.029 
Epoch 75/1000 
	 loss: 32.5513, MinusLogProbMetric: 32.5513, val_loss: 31.8410, val_MinusLogProbMetric: 31.8410

Epoch 75: val_loss improved from 32.22644 to 31.84105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 32.5513 - MinusLogProbMetric: 32.5513 - val_loss: 31.8410 - val_MinusLogProbMetric: 31.8410 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 76/1000
2023-10-11 16:06:31.555 
Epoch 76/1000 
	 loss: 32.2757, MinusLogProbMetric: 32.2757, val_loss: 32.2796, val_MinusLogProbMetric: 32.2796

Epoch 76: val_loss did not improve from 31.84105
196/196 - 27s - loss: 32.2757 - MinusLogProbMetric: 32.2757 - val_loss: 32.2796 - val_MinusLogProbMetric: 32.2796 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 77/1000
2023-10-11 16:06:58.008 
Epoch 77/1000 
	 loss: 32.4260, MinusLogProbMetric: 32.4260, val_loss: 32.3669, val_MinusLogProbMetric: 32.3669

Epoch 77: val_loss did not improve from 31.84105
196/196 - 26s - loss: 32.4260 - MinusLogProbMetric: 32.4260 - val_loss: 32.3669 - val_MinusLogProbMetric: 32.3669 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 78/1000
2023-10-11 16:07:24.345 
Epoch 78/1000 
	 loss: 32.5708, MinusLogProbMetric: 32.5708, val_loss: 33.4703, val_MinusLogProbMetric: 33.4703

Epoch 78: val_loss did not improve from 31.84105
196/196 - 26s - loss: 32.5708 - MinusLogProbMetric: 32.5708 - val_loss: 33.4703 - val_MinusLogProbMetric: 33.4703 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 79/1000
2023-10-11 16:07:50.641 
Epoch 79/1000 
	 loss: 33.2290, MinusLogProbMetric: 33.2290, val_loss: 32.5479, val_MinusLogProbMetric: 32.5479

Epoch 79: val_loss did not improve from 31.84105
196/196 - 26s - loss: 33.2290 - MinusLogProbMetric: 33.2290 - val_loss: 32.5479 - val_MinusLogProbMetric: 32.5479 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 80/1000
2023-10-11 16:08:17.378 
Epoch 80/1000 
	 loss: 32.0860, MinusLogProbMetric: 32.0860, val_loss: 31.4205, val_MinusLogProbMetric: 31.4205

Epoch 80: val_loss improved from 31.84105 to 31.42047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 32.0860 - MinusLogProbMetric: 32.0860 - val_loss: 31.4205 - val_MinusLogProbMetric: 31.4205 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 81/1000
2023-10-11 16:08:44.706 
Epoch 81/1000 
	 loss: 32.4698, MinusLogProbMetric: 32.4698, val_loss: 32.5302, val_MinusLogProbMetric: 32.5302

Epoch 81: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.4698 - MinusLogProbMetric: 32.4698 - val_loss: 32.5302 - val_MinusLogProbMetric: 32.5302 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 82/1000
2023-10-11 16:09:11.310 
Epoch 82/1000 
	 loss: 32.7340, MinusLogProbMetric: 32.7340, val_loss: 31.8966, val_MinusLogProbMetric: 31.8966

Epoch 82: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.7340 - MinusLogProbMetric: 32.7340 - val_loss: 31.8966 - val_MinusLogProbMetric: 31.8966 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 83/1000
2023-10-11 16:09:38.133 
Epoch 83/1000 
	 loss: 32.1488, MinusLogProbMetric: 32.1488, val_loss: 32.3535, val_MinusLogProbMetric: 32.3535

Epoch 83: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.1488 - MinusLogProbMetric: 32.1488 - val_loss: 32.3535 - val_MinusLogProbMetric: 32.3535 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 84/1000
2023-10-11 16:10:04.844 
Epoch 84/1000 
	 loss: 32.2397, MinusLogProbMetric: 32.2397, val_loss: 34.0786, val_MinusLogProbMetric: 34.0786

Epoch 84: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.2397 - MinusLogProbMetric: 32.2397 - val_loss: 34.0786 - val_MinusLogProbMetric: 34.0786 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 85/1000
2023-10-11 16:10:32.133 
Epoch 85/1000 
	 loss: 33.3122, MinusLogProbMetric: 33.3122, val_loss: 32.2072, val_MinusLogProbMetric: 32.2072

Epoch 85: val_loss did not improve from 31.42047
196/196 - 27s - loss: 33.3122 - MinusLogProbMetric: 33.3122 - val_loss: 32.2072 - val_MinusLogProbMetric: 32.2072 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 86/1000
2023-10-11 16:10:59.538 
Epoch 86/1000 
	 loss: 32.3465, MinusLogProbMetric: 32.3465, val_loss: 31.8633, val_MinusLogProbMetric: 31.8633

Epoch 86: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.3465 - MinusLogProbMetric: 32.3465 - val_loss: 31.8633 - val_MinusLogProbMetric: 31.8633 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 87/1000
2023-10-11 16:11:27.094 
Epoch 87/1000 
	 loss: 32.1003, MinusLogProbMetric: 32.1003, val_loss: 34.0010, val_MinusLogProbMetric: 34.0010

Epoch 87: val_loss did not improve from 31.42047
196/196 - 28s - loss: 32.1003 - MinusLogProbMetric: 32.1003 - val_loss: 34.0010 - val_MinusLogProbMetric: 34.0010 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 88/1000
2023-10-11 16:11:54.563 
Epoch 88/1000 
	 loss: 32.3944, MinusLogProbMetric: 32.3944, val_loss: 33.2579, val_MinusLogProbMetric: 33.2579

Epoch 88: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.3944 - MinusLogProbMetric: 32.3944 - val_loss: 33.2579 - val_MinusLogProbMetric: 33.2579 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 89/1000
2023-10-11 16:12:22.306 
Epoch 89/1000 
	 loss: 33.0594, MinusLogProbMetric: 33.0594, val_loss: 33.1020, val_MinusLogProbMetric: 33.1020

Epoch 89: val_loss did not improve from 31.42047
196/196 - 28s - loss: 33.0594 - MinusLogProbMetric: 33.0594 - val_loss: 33.1020 - val_MinusLogProbMetric: 33.1020 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 90/1000
2023-10-11 16:12:50.801 
Epoch 90/1000 
	 loss: 31.7590, MinusLogProbMetric: 31.7590, val_loss: 32.7278, val_MinusLogProbMetric: 32.7278

Epoch 90: val_loss did not improve from 31.42047
196/196 - 28s - loss: 31.7590 - MinusLogProbMetric: 31.7590 - val_loss: 32.7278 - val_MinusLogProbMetric: 32.7278 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 91/1000
2023-10-11 16:13:18.870 
Epoch 91/1000 
	 loss: 32.2426, MinusLogProbMetric: 32.2426, val_loss: 32.4456, val_MinusLogProbMetric: 32.4456

Epoch 91: val_loss did not improve from 31.42047
196/196 - 28s - loss: 32.2426 - MinusLogProbMetric: 32.2426 - val_loss: 32.4456 - val_MinusLogProbMetric: 32.4456 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 92/1000
2023-10-11 16:13:45.944 
Epoch 92/1000 
	 loss: 32.0896, MinusLogProbMetric: 32.0896, val_loss: 32.3645, val_MinusLogProbMetric: 32.3645

Epoch 92: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.0896 - MinusLogProbMetric: 32.0896 - val_loss: 32.3645 - val_MinusLogProbMetric: 32.3645 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 93/1000
2023-10-11 16:14:13.430 
Epoch 93/1000 
	 loss: 32.5180, MinusLogProbMetric: 32.5180, val_loss: 33.0930, val_MinusLogProbMetric: 33.0930

Epoch 93: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.5180 - MinusLogProbMetric: 32.5180 - val_loss: 33.0930 - val_MinusLogProbMetric: 33.0930 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 94/1000
2023-10-11 16:14:40.923 
Epoch 94/1000 
	 loss: 31.8938, MinusLogProbMetric: 31.8938, val_loss: 32.6349, val_MinusLogProbMetric: 32.6349

Epoch 94: val_loss did not improve from 31.42047
196/196 - 27s - loss: 31.8938 - MinusLogProbMetric: 31.8938 - val_loss: 32.6349 - val_MinusLogProbMetric: 32.6349 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 95/1000
2023-10-11 16:15:08.332 
Epoch 95/1000 
	 loss: 32.5201, MinusLogProbMetric: 32.5201, val_loss: 31.9417, val_MinusLogProbMetric: 31.9417

Epoch 95: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.5201 - MinusLogProbMetric: 32.5201 - val_loss: 31.9417 - val_MinusLogProbMetric: 31.9417 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 96/1000
2023-10-11 16:15:34.909 
Epoch 96/1000 
	 loss: 31.9662, MinusLogProbMetric: 31.9662, val_loss: 31.8888, val_MinusLogProbMetric: 31.8888

Epoch 96: val_loss did not improve from 31.42047
196/196 - 27s - loss: 31.9662 - MinusLogProbMetric: 31.9662 - val_loss: 31.8888 - val_MinusLogProbMetric: 31.8888 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 97/1000
2023-10-11 16:16:02.061 
Epoch 97/1000 
	 loss: 32.3307, MinusLogProbMetric: 32.3307, val_loss: 31.7754, val_MinusLogProbMetric: 31.7754

Epoch 97: val_loss did not improve from 31.42047
196/196 - 27s - loss: 32.3307 - MinusLogProbMetric: 32.3307 - val_loss: 31.7754 - val_MinusLogProbMetric: 31.7754 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 98/1000
2023-10-11 16:16:29.798 
Epoch 98/1000 
	 loss: 31.7723, MinusLogProbMetric: 31.7723, val_loss: 31.6357, val_MinusLogProbMetric: 31.6357

Epoch 98: val_loss did not improve from 31.42047
196/196 - 28s - loss: 31.7723 - MinusLogProbMetric: 31.7723 - val_loss: 31.6357 - val_MinusLogProbMetric: 31.6357 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 99/1000
2023-10-11 16:16:57.794 
Epoch 99/1000 
	 loss: 32.5627, MinusLogProbMetric: 32.5627, val_loss: 35.0064, val_MinusLogProbMetric: 35.0064

Epoch 99: val_loss did not improve from 31.42047
196/196 - 28s - loss: 32.5627 - MinusLogProbMetric: 32.5627 - val_loss: 35.0064 - val_MinusLogProbMetric: 35.0064 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 100/1000
2023-10-11 16:17:25.378 
Epoch 100/1000 
	 loss: 32.1643, MinusLogProbMetric: 32.1643, val_loss: 34.6127, val_MinusLogProbMetric: 34.6127

Epoch 100: val_loss did not improve from 31.42047
196/196 - 28s - loss: 32.1643 - MinusLogProbMetric: 32.1643 - val_loss: 34.6127 - val_MinusLogProbMetric: 34.6127 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 101/1000
2023-10-11 16:17:53.742 
Epoch 101/1000 
	 loss: 31.6830, MinusLogProbMetric: 31.6830, val_loss: 31.9947, val_MinusLogProbMetric: 31.9947

Epoch 101: val_loss did not improve from 31.42047
196/196 - 28s - loss: 31.6830 - MinusLogProbMetric: 31.6830 - val_loss: 31.9947 - val_MinusLogProbMetric: 31.9947 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 102/1000
2023-10-11 16:18:20.776 
Epoch 102/1000 
	 loss: 32.1147, MinusLogProbMetric: 32.1147, val_loss: 31.4133, val_MinusLogProbMetric: 31.4133

Epoch 102: val_loss improved from 31.42047 to 31.41333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 32.1147 - MinusLogProbMetric: 32.1147 - val_loss: 31.4133 - val_MinusLogProbMetric: 31.4133 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 103/1000
2023-10-11 16:18:48.052 
Epoch 103/1000 
	 loss: 32.1318, MinusLogProbMetric: 32.1318, val_loss: 33.0024, val_MinusLogProbMetric: 33.0024

Epoch 103: val_loss did not improve from 31.41333
196/196 - 27s - loss: 32.1318 - MinusLogProbMetric: 32.1318 - val_loss: 33.0024 - val_MinusLogProbMetric: 33.0024 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 104/1000
2023-10-11 16:19:15.938 
Epoch 104/1000 
	 loss: 31.5775, MinusLogProbMetric: 31.5775, val_loss: 31.9154, val_MinusLogProbMetric: 31.9154

Epoch 104: val_loss did not improve from 31.41333
196/196 - 28s - loss: 31.5775 - MinusLogProbMetric: 31.5775 - val_loss: 31.9154 - val_MinusLogProbMetric: 31.9154 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 105/1000
2023-10-11 16:19:43.575 
Epoch 105/1000 
	 loss: 32.0777, MinusLogProbMetric: 32.0777, val_loss: 32.7058, val_MinusLogProbMetric: 32.7058

Epoch 105: val_loss did not improve from 31.41333
196/196 - 28s - loss: 32.0777 - MinusLogProbMetric: 32.0777 - val_loss: 32.7058 - val_MinusLogProbMetric: 32.7058 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 106/1000
2023-10-11 16:20:10.994 
Epoch 106/1000 
	 loss: 31.9919, MinusLogProbMetric: 31.9919, val_loss: 31.6801, val_MinusLogProbMetric: 31.6801

Epoch 106: val_loss did not improve from 31.41333
196/196 - 27s - loss: 31.9919 - MinusLogProbMetric: 31.9919 - val_loss: 31.6801 - val_MinusLogProbMetric: 31.6801 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 107/1000
2023-10-11 16:20:38.236 
Epoch 107/1000 
	 loss: 31.9693, MinusLogProbMetric: 31.9693, val_loss: 32.8887, val_MinusLogProbMetric: 32.8887

Epoch 107: val_loss did not improve from 31.41333
196/196 - 27s - loss: 31.9693 - MinusLogProbMetric: 31.9693 - val_loss: 32.8887 - val_MinusLogProbMetric: 32.8887 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 108/1000
2023-10-11 16:21:05.964 
Epoch 108/1000 
	 loss: 31.5853, MinusLogProbMetric: 31.5853, val_loss: 31.7522, val_MinusLogProbMetric: 31.7522

Epoch 108: val_loss did not improve from 31.41333
196/196 - 28s - loss: 31.5853 - MinusLogProbMetric: 31.5853 - val_loss: 31.7522 - val_MinusLogProbMetric: 31.7522 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 109/1000
2023-10-11 16:21:33.380 
Epoch 109/1000 
	 loss: 32.2304, MinusLogProbMetric: 32.2304, val_loss: 31.7819, val_MinusLogProbMetric: 31.7819

Epoch 109: val_loss did not improve from 31.41333
196/196 - 27s - loss: 32.2304 - MinusLogProbMetric: 32.2304 - val_loss: 31.7819 - val_MinusLogProbMetric: 31.7819 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 110/1000
2023-10-11 16:22:00.698 
Epoch 110/1000 
	 loss: 31.7168, MinusLogProbMetric: 31.7168, val_loss: 31.2989, val_MinusLogProbMetric: 31.2989

Epoch 110: val_loss improved from 31.41333 to 31.29893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 31.7168 - MinusLogProbMetric: 31.7168 - val_loss: 31.2989 - val_MinusLogProbMetric: 31.2989 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 111/1000
2023-10-11 16:22:28.750 
Epoch 111/1000 
	 loss: 31.4125, MinusLogProbMetric: 31.4125, val_loss: 31.4290, val_MinusLogProbMetric: 31.4290

Epoch 111: val_loss did not improve from 31.29893
196/196 - 28s - loss: 31.4125 - MinusLogProbMetric: 31.4125 - val_loss: 31.4290 - val_MinusLogProbMetric: 31.4290 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 112/1000
2023-10-11 16:22:55.428 
Epoch 112/1000 
	 loss: 31.4060, MinusLogProbMetric: 31.4060, val_loss: 31.4134, val_MinusLogProbMetric: 31.4134

Epoch 112: val_loss did not improve from 31.29893
196/196 - 27s - loss: 31.4060 - MinusLogProbMetric: 31.4060 - val_loss: 31.4134 - val_MinusLogProbMetric: 31.4134 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 113/1000
2023-10-11 16:23:22.628 
Epoch 113/1000 
	 loss: 31.4198, MinusLogProbMetric: 31.4198, val_loss: 31.0691, val_MinusLogProbMetric: 31.0691

Epoch 113: val_loss improved from 31.29893 to 31.06914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 31.4198 - MinusLogProbMetric: 31.4198 - val_loss: 31.0691 - val_MinusLogProbMetric: 31.0691 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 114/1000
2023-10-11 16:23:50.626 
Epoch 114/1000 
	 loss: 31.7291, MinusLogProbMetric: 31.7291, val_loss: 32.0992, val_MinusLogProbMetric: 32.0992

Epoch 114: val_loss did not improve from 31.06914
196/196 - 27s - loss: 31.7291 - MinusLogProbMetric: 31.7291 - val_loss: 32.0992 - val_MinusLogProbMetric: 32.0992 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 115/1000
2023-10-11 16:24:17.656 
Epoch 115/1000 
	 loss: 31.6218, MinusLogProbMetric: 31.6218, val_loss: 31.5289, val_MinusLogProbMetric: 31.5289

Epoch 115: val_loss did not improve from 31.06914
196/196 - 27s - loss: 31.6218 - MinusLogProbMetric: 31.6218 - val_loss: 31.5289 - val_MinusLogProbMetric: 31.5289 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 116/1000
2023-10-11 16:24:44.390 
Epoch 116/1000 
	 loss: 31.6168, MinusLogProbMetric: 31.6168, val_loss: 32.1837, val_MinusLogProbMetric: 32.1837

Epoch 116: val_loss did not improve from 31.06914
196/196 - 27s - loss: 31.6168 - MinusLogProbMetric: 31.6168 - val_loss: 32.1837 - val_MinusLogProbMetric: 32.1837 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 117/1000
2023-10-11 16:25:11.624 
Epoch 117/1000 
	 loss: 31.3985, MinusLogProbMetric: 31.3985, val_loss: 32.2229, val_MinusLogProbMetric: 32.2229

Epoch 117: val_loss did not improve from 31.06914
196/196 - 27s - loss: 31.3985 - MinusLogProbMetric: 31.3985 - val_loss: 32.2229 - val_MinusLogProbMetric: 32.2229 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 118/1000
2023-10-11 16:25:38.600 
Epoch 118/1000 
	 loss: 31.3285, MinusLogProbMetric: 31.3285, val_loss: 30.7307, val_MinusLogProbMetric: 30.7307

Epoch 118: val_loss improved from 31.06914 to 30.73075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 31.3285 - MinusLogProbMetric: 31.3285 - val_loss: 30.7307 - val_MinusLogProbMetric: 30.7307 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 119/1000
2023-10-11 16:26:05.689 
Epoch 119/1000 
	 loss: 31.2870, MinusLogProbMetric: 31.2870, val_loss: 31.5265, val_MinusLogProbMetric: 31.5265

Epoch 119: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.2870 - MinusLogProbMetric: 31.2870 - val_loss: 31.5265 - val_MinusLogProbMetric: 31.5265 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 120/1000
2023-10-11 16:26:32.534 
Epoch 120/1000 
	 loss: 31.4199, MinusLogProbMetric: 31.4199, val_loss: 32.4666, val_MinusLogProbMetric: 32.4666

Epoch 120: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.4199 - MinusLogProbMetric: 31.4199 - val_loss: 32.4666 - val_MinusLogProbMetric: 32.4666 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 121/1000
2023-10-11 16:26:59.465 
Epoch 121/1000 
	 loss: 31.7013, MinusLogProbMetric: 31.7013, val_loss: 31.5370, val_MinusLogProbMetric: 31.5370

Epoch 121: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.7013 - MinusLogProbMetric: 31.7013 - val_loss: 31.5370 - val_MinusLogProbMetric: 31.5370 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 122/1000
2023-10-11 16:27:26.612 
Epoch 122/1000 
	 loss: 31.1159, MinusLogProbMetric: 31.1159, val_loss: 31.8850, val_MinusLogProbMetric: 31.8850

Epoch 122: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.1159 - MinusLogProbMetric: 31.1159 - val_loss: 31.8850 - val_MinusLogProbMetric: 31.8850 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 123/1000
2023-10-11 16:27:52.995 
Epoch 123/1000 
	 loss: 31.2208, MinusLogProbMetric: 31.2208, val_loss: 31.2853, val_MinusLogProbMetric: 31.2853

Epoch 123: val_loss did not improve from 30.73075
196/196 - 26s - loss: 31.2208 - MinusLogProbMetric: 31.2208 - val_loss: 31.2853 - val_MinusLogProbMetric: 31.2853 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 124/1000
2023-10-11 16:28:19.474 
Epoch 124/1000 
	 loss: 31.2913, MinusLogProbMetric: 31.2913, val_loss: 32.4344, val_MinusLogProbMetric: 32.4344

Epoch 124: val_loss did not improve from 30.73075
196/196 - 26s - loss: 31.2913 - MinusLogProbMetric: 31.2913 - val_loss: 32.4344 - val_MinusLogProbMetric: 32.4344 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 125/1000
2023-10-11 16:28:45.761 
Epoch 125/1000 
	 loss: 31.3768, MinusLogProbMetric: 31.3768, val_loss: 31.2883, val_MinusLogProbMetric: 31.2883

Epoch 125: val_loss did not improve from 30.73075
196/196 - 26s - loss: 31.3768 - MinusLogProbMetric: 31.3768 - val_loss: 31.2883 - val_MinusLogProbMetric: 31.2883 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 126/1000
2023-10-11 16:29:12.421 
Epoch 126/1000 
	 loss: 31.5245, MinusLogProbMetric: 31.5245, val_loss: 31.1808, val_MinusLogProbMetric: 31.1808

Epoch 126: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.5245 - MinusLogProbMetric: 31.5245 - val_loss: 31.1808 - val_MinusLogProbMetric: 31.1808 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 127/1000
2023-10-11 16:29:39.237 
Epoch 127/1000 
	 loss: 31.4591, MinusLogProbMetric: 31.4591, val_loss: 31.0725, val_MinusLogProbMetric: 31.0725

Epoch 127: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.4591 - MinusLogProbMetric: 31.4591 - val_loss: 31.0725 - val_MinusLogProbMetric: 31.0725 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 128/1000
2023-10-11 16:30:06.082 
Epoch 128/1000 
	 loss: 31.2410, MinusLogProbMetric: 31.2410, val_loss: 32.7948, val_MinusLogProbMetric: 32.7948

Epoch 128: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.2410 - MinusLogProbMetric: 31.2410 - val_loss: 32.7948 - val_MinusLogProbMetric: 32.7948 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 129/1000
2023-10-11 16:30:33.137 
Epoch 129/1000 
	 loss: 31.1174, MinusLogProbMetric: 31.1174, val_loss: 30.8432, val_MinusLogProbMetric: 30.8432

Epoch 129: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.1174 - MinusLogProbMetric: 31.1174 - val_loss: 30.8432 - val_MinusLogProbMetric: 30.8432 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 130/1000
2023-10-11 16:31:00.182 
Epoch 130/1000 
	 loss: 31.3214, MinusLogProbMetric: 31.3214, val_loss: 31.6817, val_MinusLogProbMetric: 31.6817

Epoch 130: val_loss did not improve from 30.73075
196/196 - 27s - loss: 31.3214 - MinusLogProbMetric: 31.3214 - val_loss: 31.6817 - val_MinusLogProbMetric: 31.6817 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 131/1000
2023-10-11 16:31:26.532 
Epoch 131/1000 
	 loss: 31.7392, MinusLogProbMetric: 31.7392, val_loss: 30.6083, val_MinusLogProbMetric: 30.6083

Epoch 131: val_loss improved from 30.73075 to 30.60834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 31.7392 - MinusLogProbMetric: 31.7392 - val_loss: 30.6083 - val_MinusLogProbMetric: 30.6083 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 132/1000
2023-10-11 16:31:53.982 
Epoch 132/1000 
	 loss: 30.9839, MinusLogProbMetric: 30.9839, val_loss: 31.6314, val_MinusLogProbMetric: 31.6314

Epoch 132: val_loss did not improve from 30.60834
196/196 - 27s - loss: 30.9839 - MinusLogProbMetric: 30.9839 - val_loss: 31.6314 - val_MinusLogProbMetric: 31.6314 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 133/1000
2023-10-11 16:32:20.865 
Epoch 133/1000 
	 loss: 31.2531, MinusLogProbMetric: 31.2531, val_loss: 31.0749, val_MinusLogProbMetric: 31.0749

Epoch 133: val_loss did not improve from 30.60834
196/196 - 27s - loss: 31.2531 - MinusLogProbMetric: 31.2531 - val_loss: 31.0749 - val_MinusLogProbMetric: 31.0749 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 134/1000
2023-10-11 16:32:47.839 
Epoch 134/1000 
	 loss: 31.1303, MinusLogProbMetric: 31.1303, val_loss: 30.5611, val_MinusLogProbMetric: 30.5611

Epoch 134: val_loss improved from 30.60834 to 30.56114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 31.1303 - MinusLogProbMetric: 31.1303 - val_loss: 30.5611 - val_MinusLogProbMetric: 30.5611 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 135/1000
2023-10-11 16:33:14.835 
Epoch 135/1000 
	 loss: 31.3364, MinusLogProbMetric: 31.3364, val_loss: 31.1154, val_MinusLogProbMetric: 31.1154

Epoch 135: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.3364 - MinusLogProbMetric: 31.3364 - val_loss: 31.1154 - val_MinusLogProbMetric: 31.1154 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 136/1000
2023-10-11 16:33:41.277 
Epoch 136/1000 
	 loss: 31.4235, MinusLogProbMetric: 31.4235, val_loss: 30.7881, val_MinusLogProbMetric: 30.7881

Epoch 136: val_loss did not improve from 30.56114
196/196 - 26s - loss: 31.4235 - MinusLogProbMetric: 31.4235 - val_loss: 30.7881 - val_MinusLogProbMetric: 30.7881 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 137/1000
2023-10-11 16:34:08.124 
Epoch 137/1000 
	 loss: 31.1955, MinusLogProbMetric: 31.1955, val_loss: 30.8752, val_MinusLogProbMetric: 30.8752

Epoch 137: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.1955 - MinusLogProbMetric: 31.1955 - val_loss: 30.8752 - val_MinusLogProbMetric: 30.8752 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 138/1000
2023-10-11 16:34:34.815 
Epoch 138/1000 
	 loss: 31.3115, MinusLogProbMetric: 31.3115, val_loss: 31.3360, val_MinusLogProbMetric: 31.3360

Epoch 138: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.3115 - MinusLogProbMetric: 31.3115 - val_loss: 31.3360 - val_MinusLogProbMetric: 31.3360 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 139/1000
2023-10-11 16:35:01.731 
Epoch 139/1000 
	 loss: 31.3184, MinusLogProbMetric: 31.3184, val_loss: 32.3602, val_MinusLogProbMetric: 32.3602

Epoch 139: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.3184 - MinusLogProbMetric: 31.3184 - val_loss: 32.3602 - val_MinusLogProbMetric: 32.3602 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 140/1000
2023-10-11 16:35:28.334 
Epoch 140/1000 
	 loss: 31.5981, MinusLogProbMetric: 31.5981, val_loss: 35.7751, val_MinusLogProbMetric: 35.7751

Epoch 140: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.5981 - MinusLogProbMetric: 31.5981 - val_loss: 35.7751 - val_MinusLogProbMetric: 35.7751 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 141/1000
2023-10-11 16:35:55.130 
Epoch 141/1000 
	 loss: 31.0547, MinusLogProbMetric: 31.0547, val_loss: 31.0636, val_MinusLogProbMetric: 31.0636

Epoch 141: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.0547 - MinusLogProbMetric: 31.0547 - val_loss: 31.0636 - val_MinusLogProbMetric: 31.0636 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 142/1000
2023-10-11 16:36:22.830 
Epoch 142/1000 
	 loss: 32.1494, MinusLogProbMetric: 32.1494, val_loss: 32.3798, val_MinusLogProbMetric: 32.3798

Epoch 142: val_loss did not improve from 30.56114
196/196 - 28s - loss: 32.1494 - MinusLogProbMetric: 32.1494 - val_loss: 32.3798 - val_MinusLogProbMetric: 32.3798 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 143/1000
2023-10-11 16:36:50.050 
Epoch 143/1000 
	 loss: 31.8467, MinusLogProbMetric: 31.8467, val_loss: 31.7445, val_MinusLogProbMetric: 31.7445

Epoch 143: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.8467 - MinusLogProbMetric: 31.8467 - val_loss: 31.7445 - val_MinusLogProbMetric: 31.7445 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 144/1000
2023-10-11 16:37:17.352 
Epoch 144/1000 
	 loss: 31.4657, MinusLogProbMetric: 31.4657, val_loss: 31.3661, val_MinusLogProbMetric: 31.3661

Epoch 144: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.4657 - MinusLogProbMetric: 31.4657 - val_loss: 31.3661 - val_MinusLogProbMetric: 31.3661 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 145/1000
2023-10-11 16:37:44.117 
Epoch 145/1000 
	 loss: 31.1139, MinusLogProbMetric: 31.1139, val_loss: 31.8425, val_MinusLogProbMetric: 31.8425

Epoch 145: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.1139 - MinusLogProbMetric: 31.1139 - val_loss: 31.8425 - val_MinusLogProbMetric: 31.8425 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 146/1000
2023-10-11 16:38:10.885 
Epoch 146/1000 
	 loss: 30.9792, MinusLogProbMetric: 30.9792, val_loss: 31.3010, val_MinusLogProbMetric: 31.3010

Epoch 146: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.9792 - MinusLogProbMetric: 30.9792 - val_loss: 31.3010 - val_MinusLogProbMetric: 31.3010 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 147/1000
2023-10-11 16:38:38.168 
Epoch 147/1000 
	 loss: 31.0525, MinusLogProbMetric: 31.0525, val_loss: 31.3239, val_MinusLogProbMetric: 31.3239

Epoch 147: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.0525 - MinusLogProbMetric: 31.0525 - val_loss: 31.3239 - val_MinusLogProbMetric: 31.3239 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 148/1000
2023-10-11 16:39:04.808 
Epoch 148/1000 
	 loss: 31.2514, MinusLogProbMetric: 31.2514, val_loss: 31.8213, val_MinusLogProbMetric: 31.8213

Epoch 148: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.2514 - MinusLogProbMetric: 31.2514 - val_loss: 31.8213 - val_MinusLogProbMetric: 31.8213 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 149/1000
2023-10-11 16:39:31.657 
Epoch 149/1000 
	 loss: 30.9367, MinusLogProbMetric: 30.9367, val_loss: 31.1553, val_MinusLogProbMetric: 31.1553

Epoch 149: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.9367 - MinusLogProbMetric: 30.9367 - val_loss: 31.1553 - val_MinusLogProbMetric: 31.1553 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 150/1000
2023-10-11 16:39:58.423 
Epoch 150/1000 
	 loss: 30.9075, MinusLogProbMetric: 30.9075, val_loss: 31.2461, val_MinusLogProbMetric: 31.2461

Epoch 150: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.9075 - MinusLogProbMetric: 30.9075 - val_loss: 31.2461 - val_MinusLogProbMetric: 31.2461 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 151/1000
2023-10-11 16:40:25.762 
Epoch 151/1000 
	 loss: 31.0484, MinusLogProbMetric: 31.0484, val_loss: 30.9147, val_MinusLogProbMetric: 30.9147

Epoch 151: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.0484 - MinusLogProbMetric: 31.0484 - val_loss: 30.9147 - val_MinusLogProbMetric: 30.9147 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 152/1000
2023-10-11 16:40:53.091 
Epoch 152/1000 
	 loss: 30.9113, MinusLogProbMetric: 30.9113, val_loss: 31.3247, val_MinusLogProbMetric: 31.3247

Epoch 152: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.9113 - MinusLogProbMetric: 30.9113 - val_loss: 31.3247 - val_MinusLogProbMetric: 31.3247 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 153/1000
2023-10-11 16:41:20.016 
Epoch 153/1000 
	 loss: 30.8151, MinusLogProbMetric: 30.8151, val_loss: 31.6285, val_MinusLogProbMetric: 31.6285

Epoch 153: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.8151 - MinusLogProbMetric: 30.8151 - val_loss: 31.6285 - val_MinusLogProbMetric: 31.6285 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 154/1000
2023-10-11 16:41:46.968 
Epoch 154/1000 
	 loss: 30.7639, MinusLogProbMetric: 30.7639, val_loss: 31.3640, val_MinusLogProbMetric: 31.3640

Epoch 154: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.7639 - MinusLogProbMetric: 30.7639 - val_loss: 31.3640 - val_MinusLogProbMetric: 31.3640 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 155/1000
2023-10-11 16:42:13.800 
Epoch 155/1000 
	 loss: 30.9830, MinusLogProbMetric: 30.9830, val_loss: 30.8466, val_MinusLogProbMetric: 30.8466

Epoch 155: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.9830 - MinusLogProbMetric: 30.9830 - val_loss: 30.8466 - val_MinusLogProbMetric: 30.8466 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 156/1000
2023-10-11 16:42:40.629 
Epoch 156/1000 
	 loss: 30.9347, MinusLogProbMetric: 30.9347, val_loss: 30.6474, val_MinusLogProbMetric: 30.6474

Epoch 156: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.9347 - MinusLogProbMetric: 30.9347 - val_loss: 30.6474 - val_MinusLogProbMetric: 30.6474 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 157/1000
2023-10-11 16:43:07.423 
Epoch 157/1000 
	 loss: 30.5297, MinusLogProbMetric: 30.5297, val_loss: 30.9761, val_MinusLogProbMetric: 30.9761

Epoch 157: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.5297 - MinusLogProbMetric: 30.5297 - val_loss: 30.9761 - val_MinusLogProbMetric: 30.9761 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 158/1000
2023-10-11 16:43:34.070 
Epoch 158/1000 
	 loss: 31.0096, MinusLogProbMetric: 31.0096, val_loss: 31.5579, val_MinusLogProbMetric: 31.5579

Epoch 158: val_loss did not improve from 30.56114
196/196 - 27s - loss: 31.0096 - MinusLogProbMetric: 31.0096 - val_loss: 31.5579 - val_MinusLogProbMetric: 31.5579 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 159/1000
2023-10-11 16:44:00.583 
Epoch 159/1000 
	 loss: 30.6519, MinusLogProbMetric: 30.6519, val_loss: 31.6554, val_MinusLogProbMetric: 31.6554

Epoch 159: val_loss did not improve from 30.56114
196/196 - 27s - loss: 30.6519 - MinusLogProbMetric: 30.6519 - val_loss: 31.6554 - val_MinusLogProbMetric: 31.6554 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 160/1000
2023-10-11 16:44:27.983 
Epoch 160/1000 
	 loss: 30.6699, MinusLogProbMetric: 30.6699, val_loss: 30.3018, val_MinusLogProbMetric: 30.3018

Epoch 160: val_loss improved from 30.56114 to 30.30184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 30.6699 - MinusLogProbMetric: 30.6699 - val_loss: 30.3018 - val_MinusLogProbMetric: 30.3018 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 161/1000
2023-10-11 16:44:55.868 
Epoch 161/1000 
	 loss: 31.0638, MinusLogProbMetric: 31.0638, val_loss: 30.7588, val_MinusLogProbMetric: 30.7588

Epoch 161: val_loss did not improve from 30.30184
196/196 - 27s - loss: 31.0638 - MinusLogProbMetric: 31.0638 - val_loss: 30.7588 - val_MinusLogProbMetric: 30.7588 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 162/1000
2023-10-11 16:45:22.914 
Epoch 162/1000 
	 loss: 30.8773, MinusLogProbMetric: 30.8773, val_loss: 30.3981, val_MinusLogProbMetric: 30.3981

Epoch 162: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.8773 - MinusLogProbMetric: 30.8773 - val_loss: 30.3981 - val_MinusLogProbMetric: 30.3981 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 163/1000
2023-10-11 16:45:49.493 
Epoch 163/1000 
	 loss: 30.7541, MinusLogProbMetric: 30.7541, val_loss: 30.6332, val_MinusLogProbMetric: 30.6332

Epoch 163: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.7541 - MinusLogProbMetric: 30.7541 - val_loss: 30.6332 - val_MinusLogProbMetric: 30.6332 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 164/1000
2023-10-11 16:46:16.703 
Epoch 164/1000 
	 loss: 31.2459, MinusLogProbMetric: 31.2459, val_loss: 31.0917, val_MinusLogProbMetric: 31.0917

Epoch 164: val_loss did not improve from 30.30184
196/196 - 27s - loss: 31.2459 - MinusLogProbMetric: 31.2459 - val_loss: 31.0917 - val_MinusLogProbMetric: 31.0917 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 165/1000
2023-10-11 16:46:44.071 
Epoch 165/1000 
	 loss: 30.5828, MinusLogProbMetric: 30.5828, val_loss: 31.5337, val_MinusLogProbMetric: 31.5337

Epoch 165: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.5828 - MinusLogProbMetric: 30.5828 - val_loss: 31.5337 - val_MinusLogProbMetric: 31.5337 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 166/1000
2023-10-11 16:47:11.200 
Epoch 166/1000 
	 loss: 30.8726, MinusLogProbMetric: 30.8726, val_loss: 30.9805, val_MinusLogProbMetric: 30.9805

Epoch 166: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.8726 - MinusLogProbMetric: 30.8726 - val_loss: 30.9805 - val_MinusLogProbMetric: 30.9805 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 167/1000
2023-10-11 16:47:37.883 
Epoch 167/1000 
	 loss: 30.5461, MinusLogProbMetric: 30.5461, val_loss: 32.0156, val_MinusLogProbMetric: 32.0156

Epoch 167: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.5461 - MinusLogProbMetric: 30.5461 - val_loss: 32.0156 - val_MinusLogProbMetric: 32.0156 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 168/1000
2023-10-11 16:48:04.684 
Epoch 168/1000 
	 loss: 30.6372, MinusLogProbMetric: 30.6372, val_loss: 30.6465, val_MinusLogProbMetric: 30.6465

Epoch 168: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.6372 - MinusLogProbMetric: 30.6372 - val_loss: 30.6465 - val_MinusLogProbMetric: 30.6465 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 169/1000
2023-10-11 16:48:31.290 
Epoch 169/1000 
	 loss: 30.5709, MinusLogProbMetric: 30.5709, val_loss: 32.4721, val_MinusLogProbMetric: 32.4721

Epoch 169: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.5709 - MinusLogProbMetric: 30.5709 - val_loss: 32.4721 - val_MinusLogProbMetric: 32.4721 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 170/1000
2023-10-11 16:48:58.739 
Epoch 170/1000 
	 loss: 30.6163, MinusLogProbMetric: 30.6163, val_loss: 30.9974, val_MinusLogProbMetric: 30.9974

Epoch 170: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.6163 - MinusLogProbMetric: 30.6163 - val_loss: 30.9974 - val_MinusLogProbMetric: 30.9974 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 171/1000
2023-10-11 16:49:24.873 
Epoch 171/1000 
	 loss: 30.7865, MinusLogProbMetric: 30.7865, val_loss: 30.7390, val_MinusLogProbMetric: 30.7390

Epoch 171: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.7865 - MinusLogProbMetric: 30.7865 - val_loss: 30.7390 - val_MinusLogProbMetric: 30.7390 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 172/1000
2023-10-11 16:49:50.762 
Epoch 172/1000 
	 loss: 30.9374, MinusLogProbMetric: 30.9374, val_loss: 33.0593, val_MinusLogProbMetric: 33.0593

Epoch 172: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.9374 - MinusLogProbMetric: 30.9374 - val_loss: 33.0593 - val_MinusLogProbMetric: 33.0593 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 173/1000
2023-10-11 16:50:16.788 
Epoch 173/1000 
	 loss: 31.5494, MinusLogProbMetric: 31.5494, val_loss: 32.3983, val_MinusLogProbMetric: 32.3983

Epoch 173: val_loss did not improve from 30.30184
196/196 - 26s - loss: 31.5494 - MinusLogProbMetric: 31.5494 - val_loss: 32.3983 - val_MinusLogProbMetric: 32.3983 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 174/1000
2023-10-11 16:50:42.821 
Epoch 174/1000 
	 loss: 31.0866, MinusLogProbMetric: 31.0866, val_loss: 31.0327, val_MinusLogProbMetric: 31.0327

Epoch 174: val_loss did not improve from 30.30184
196/196 - 26s - loss: 31.0866 - MinusLogProbMetric: 31.0866 - val_loss: 31.0327 - val_MinusLogProbMetric: 31.0327 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 175/1000
2023-10-11 16:51:08.801 
Epoch 175/1000 
	 loss: 31.6085, MinusLogProbMetric: 31.6085, val_loss: 31.0516, val_MinusLogProbMetric: 31.0516

Epoch 175: val_loss did not improve from 30.30184
196/196 - 26s - loss: 31.6085 - MinusLogProbMetric: 31.6085 - val_loss: 31.0516 - val_MinusLogProbMetric: 31.0516 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 176/1000
2023-10-11 16:51:35.396 
Epoch 176/1000 
	 loss: 30.7879, MinusLogProbMetric: 30.7879, val_loss: 30.7996, val_MinusLogProbMetric: 30.7996

Epoch 176: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.7879 - MinusLogProbMetric: 30.7879 - val_loss: 30.7996 - val_MinusLogProbMetric: 30.7996 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 177/1000
2023-10-11 16:52:02.007 
Epoch 177/1000 
	 loss: 30.5340, MinusLogProbMetric: 30.5340, val_loss: 31.3398, val_MinusLogProbMetric: 31.3398

Epoch 177: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.5340 - MinusLogProbMetric: 30.5340 - val_loss: 31.3398 - val_MinusLogProbMetric: 31.3398 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 178/1000
2023-10-11 16:52:28.061 
Epoch 178/1000 
	 loss: 30.8303, MinusLogProbMetric: 30.8303, val_loss: 30.9038, val_MinusLogProbMetric: 30.9038

Epoch 178: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.8303 - MinusLogProbMetric: 30.8303 - val_loss: 30.9038 - val_MinusLogProbMetric: 30.9038 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 179/1000
2023-10-11 16:52:53.840 
Epoch 179/1000 
	 loss: 30.7553, MinusLogProbMetric: 30.7553, val_loss: 31.1815, val_MinusLogProbMetric: 31.1815

Epoch 179: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.7553 - MinusLogProbMetric: 30.7553 - val_loss: 31.1815 - val_MinusLogProbMetric: 31.1815 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 180/1000
2023-10-11 16:53:19.956 
Epoch 180/1000 
	 loss: 30.2963, MinusLogProbMetric: 30.2963, val_loss: 30.4652, val_MinusLogProbMetric: 30.4652

Epoch 180: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.2963 - MinusLogProbMetric: 30.2963 - val_loss: 30.4652 - val_MinusLogProbMetric: 30.4652 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 181/1000
2023-10-11 16:53:46.141 
Epoch 181/1000 
	 loss: 30.3998, MinusLogProbMetric: 30.3998, val_loss: 30.7038, val_MinusLogProbMetric: 30.7038

Epoch 181: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.3998 - MinusLogProbMetric: 30.3998 - val_loss: 30.7038 - val_MinusLogProbMetric: 30.7038 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 182/1000
2023-10-11 16:54:12.651 
Epoch 182/1000 
	 loss: 30.3645, MinusLogProbMetric: 30.3645, val_loss: 30.8421, val_MinusLogProbMetric: 30.8421

Epoch 182: val_loss did not improve from 30.30184
196/196 - 27s - loss: 30.3645 - MinusLogProbMetric: 30.3645 - val_loss: 30.8421 - val_MinusLogProbMetric: 30.8421 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 183/1000
2023-10-11 16:54:38.957 
Epoch 183/1000 
	 loss: 30.8689, MinusLogProbMetric: 30.8689, val_loss: 31.0093, val_MinusLogProbMetric: 31.0093

Epoch 183: val_loss did not improve from 30.30184
196/196 - 26s - loss: 30.8689 - MinusLogProbMetric: 30.8689 - val_loss: 31.0093 - val_MinusLogProbMetric: 31.0093 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 184/1000
2023-10-11 16:55:05.132 
Epoch 184/1000 
	 loss: 30.2794, MinusLogProbMetric: 30.2794, val_loss: 29.8474, val_MinusLogProbMetric: 29.8474

Epoch 184: val_loss improved from 30.30184 to 29.84743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 30.2794 - MinusLogProbMetric: 30.2794 - val_loss: 29.8474 - val_MinusLogProbMetric: 29.8474 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 185/1000
2023-10-11 16:55:31.751 
Epoch 185/1000 
	 loss: 30.5477, MinusLogProbMetric: 30.5477, val_loss: 30.6584, val_MinusLogProbMetric: 30.6584

Epoch 185: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.5477 - MinusLogProbMetric: 30.5477 - val_loss: 30.6584 - val_MinusLogProbMetric: 30.6584 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 186/1000
2023-10-11 16:55:57.685 
Epoch 186/1000 
	 loss: 30.4106, MinusLogProbMetric: 30.4106, val_loss: 30.2635, val_MinusLogProbMetric: 30.2635

Epoch 186: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.4106 - MinusLogProbMetric: 30.4106 - val_loss: 30.2635 - val_MinusLogProbMetric: 30.2635 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 187/1000
2023-10-11 16:56:23.736 
Epoch 187/1000 
	 loss: 30.3514, MinusLogProbMetric: 30.3514, val_loss: 30.8390, val_MinusLogProbMetric: 30.8390

Epoch 187: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.3514 - MinusLogProbMetric: 30.3514 - val_loss: 30.8390 - val_MinusLogProbMetric: 30.8390 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 188/1000
2023-10-11 16:56:50.115 
Epoch 188/1000 
	 loss: 30.3781, MinusLogProbMetric: 30.3781, val_loss: 31.2523, val_MinusLogProbMetric: 31.2523

Epoch 188: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.3781 - MinusLogProbMetric: 30.3781 - val_loss: 31.2523 - val_MinusLogProbMetric: 31.2523 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 189/1000
2023-10-11 16:57:16.892 
Epoch 189/1000 
	 loss: 32.6313, MinusLogProbMetric: 32.6313, val_loss: 31.3385, val_MinusLogProbMetric: 31.3385

Epoch 189: val_loss did not improve from 29.84743
196/196 - 27s - loss: 32.6313 - MinusLogProbMetric: 32.6313 - val_loss: 31.3385 - val_MinusLogProbMetric: 31.3385 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 190/1000
2023-10-11 16:57:43.155 
Epoch 190/1000 
	 loss: 30.9844, MinusLogProbMetric: 30.9844, val_loss: 32.5423, val_MinusLogProbMetric: 32.5423

Epoch 190: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.9844 - MinusLogProbMetric: 30.9844 - val_loss: 32.5423 - val_MinusLogProbMetric: 32.5423 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 191/1000
2023-10-11 16:58:08.930 
Epoch 191/1000 
	 loss: 30.7133, MinusLogProbMetric: 30.7133, val_loss: 31.4847, val_MinusLogProbMetric: 31.4847

Epoch 191: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.7133 - MinusLogProbMetric: 30.7133 - val_loss: 31.4847 - val_MinusLogProbMetric: 31.4847 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 192/1000
2023-10-11 16:58:35.042 
Epoch 192/1000 
	 loss: 30.7132, MinusLogProbMetric: 30.7132, val_loss: 31.0415, val_MinusLogProbMetric: 31.0415

Epoch 192: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.7132 - MinusLogProbMetric: 30.7132 - val_loss: 31.0415 - val_MinusLogProbMetric: 31.0415 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 193/1000
2023-10-11 16:59:00.974 
Epoch 193/1000 
	 loss: 30.4006, MinusLogProbMetric: 30.4006, val_loss: 31.0272, val_MinusLogProbMetric: 31.0272

Epoch 193: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.4006 - MinusLogProbMetric: 30.4006 - val_loss: 31.0272 - val_MinusLogProbMetric: 31.0272 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 194/1000
2023-10-11 16:59:27.072 
Epoch 194/1000 
	 loss: 30.4356, MinusLogProbMetric: 30.4356, val_loss: 30.7200, val_MinusLogProbMetric: 30.7200

Epoch 194: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.4356 - MinusLogProbMetric: 30.4356 - val_loss: 30.7200 - val_MinusLogProbMetric: 30.7200 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 195/1000
2023-10-11 16:59:53.568 
Epoch 195/1000 
	 loss: 30.3328, MinusLogProbMetric: 30.3328, val_loss: 30.6605, val_MinusLogProbMetric: 30.6605

Epoch 195: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.3328 - MinusLogProbMetric: 30.3328 - val_loss: 30.6605 - val_MinusLogProbMetric: 30.6605 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 196/1000
2023-10-11 17:00:20.337 
Epoch 196/1000 
	 loss: 30.1417, MinusLogProbMetric: 30.1417, val_loss: 30.2380, val_MinusLogProbMetric: 30.2380

Epoch 196: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.1417 - MinusLogProbMetric: 30.1417 - val_loss: 30.2380 - val_MinusLogProbMetric: 30.2380 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 197/1000
2023-10-11 17:00:46.787 
Epoch 197/1000 
	 loss: 30.0769, MinusLogProbMetric: 30.0769, val_loss: 30.3414, val_MinusLogProbMetric: 30.3414

Epoch 197: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0769 - MinusLogProbMetric: 30.0769 - val_loss: 30.3414 - val_MinusLogProbMetric: 30.3414 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 198/1000
2023-10-11 17:01:13.374 
Epoch 198/1000 
	 loss: 30.2572, MinusLogProbMetric: 30.2572, val_loss: 31.1190, val_MinusLogProbMetric: 31.1190

Epoch 198: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.2572 - MinusLogProbMetric: 30.2572 - val_loss: 31.1190 - val_MinusLogProbMetric: 31.1190 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 199/1000
2023-10-11 17:01:39.308 
Epoch 199/1000 
	 loss: 30.2216, MinusLogProbMetric: 30.2216, val_loss: 31.0191, val_MinusLogProbMetric: 31.0191

Epoch 199: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.2216 - MinusLogProbMetric: 30.2216 - val_loss: 31.0191 - val_MinusLogProbMetric: 31.0191 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 200/1000
2023-10-11 17:02:05.407 
Epoch 200/1000 
	 loss: 30.2420, MinusLogProbMetric: 30.2420, val_loss: 29.9095, val_MinusLogProbMetric: 29.9095

Epoch 200: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.2420 - MinusLogProbMetric: 30.2420 - val_loss: 29.9095 - val_MinusLogProbMetric: 29.9095 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 201/1000
2023-10-11 17:02:31.832 
Epoch 201/1000 
	 loss: 30.0866, MinusLogProbMetric: 30.0866, val_loss: 30.6986, val_MinusLogProbMetric: 30.6986

Epoch 201: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0866 - MinusLogProbMetric: 30.0866 - val_loss: 30.6986 - val_MinusLogProbMetric: 30.6986 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 202/1000
2023-10-11 17:02:58.593 
Epoch 202/1000 
	 loss: 30.2431, MinusLogProbMetric: 30.2431, val_loss: 30.4511, val_MinusLogProbMetric: 30.4511

Epoch 202: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.2431 - MinusLogProbMetric: 30.2431 - val_loss: 30.4511 - val_MinusLogProbMetric: 30.4511 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 203/1000
2023-10-11 17:03:25.324 
Epoch 203/1000 
	 loss: 30.2962, MinusLogProbMetric: 30.2962, val_loss: 30.4837, val_MinusLogProbMetric: 30.4837

Epoch 203: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.2962 - MinusLogProbMetric: 30.2962 - val_loss: 30.4837 - val_MinusLogProbMetric: 30.4837 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 204/1000
2023-10-11 17:03:51.851 
Epoch 204/1000 
	 loss: 30.0719, MinusLogProbMetric: 30.0719, val_loss: 30.3893, val_MinusLogProbMetric: 30.3893

Epoch 204: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.0719 - MinusLogProbMetric: 30.0719 - val_loss: 30.3893 - val_MinusLogProbMetric: 30.3893 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 205/1000
2023-10-11 17:04:18.453 
Epoch 205/1000 
	 loss: 30.0305, MinusLogProbMetric: 30.0305, val_loss: 30.2944, val_MinusLogProbMetric: 30.2944

Epoch 205: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.0305 - MinusLogProbMetric: 30.0305 - val_loss: 30.2944 - val_MinusLogProbMetric: 30.2944 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 206/1000
2023-10-11 17:04:45.514 
Epoch 206/1000 
	 loss: 30.0866, MinusLogProbMetric: 30.0866, val_loss: 29.9889, val_MinusLogProbMetric: 29.9889

Epoch 206: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.0866 - MinusLogProbMetric: 30.0866 - val_loss: 29.9889 - val_MinusLogProbMetric: 29.9889 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 207/1000
2023-10-11 17:05:12.215 
Epoch 207/1000 
	 loss: 30.1939, MinusLogProbMetric: 30.1939, val_loss: 30.2475, val_MinusLogProbMetric: 30.2475

Epoch 207: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.1939 - MinusLogProbMetric: 30.1939 - val_loss: 30.2475 - val_MinusLogProbMetric: 30.2475 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 208/1000
2023-10-11 17:05:39.220 
Epoch 208/1000 
	 loss: 30.0495, MinusLogProbMetric: 30.0495, val_loss: 30.2977, val_MinusLogProbMetric: 30.2977

Epoch 208: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.0495 - MinusLogProbMetric: 30.0495 - val_loss: 30.2977 - val_MinusLogProbMetric: 30.2977 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 209/1000
2023-10-11 17:06:05.485 
Epoch 209/1000 
	 loss: 29.9566, MinusLogProbMetric: 29.9566, val_loss: 31.7619, val_MinusLogProbMetric: 31.7619

Epoch 209: val_loss did not improve from 29.84743
196/196 - 26s - loss: 29.9566 - MinusLogProbMetric: 29.9566 - val_loss: 31.7619 - val_MinusLogProbMetric: 31.7619 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 210/1000
2023-10-11 17:06:31.562 
Epoch 210/1000 
	 loss: 30.4146, MinusLogProbMetric: 30.4146, val_loss: 30.9908, val_MinusLogProbMetric: 30.9908

Epoch 210: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.4146 - MinusLogProbMetric: 30.4146 - val_loss: 30.9908 - val_MinusLogProbMetric: 30.9908 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 211/1000
2023-10-11 17:06:57.933 
Epoch 211/1000 
	 loss: 30.1097, MinusLogProbMetric: 30.1097, val_loss: 30.3629, val_MinusLogProbMetric: 30.3629

Epoch 211: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.1097 - MinusLogProbMetric: 30.1097 - val_loss: 30.3629 - val_MinusLogProbMetric: 30.3629 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 212/1000
2023-10-11 17:07:24.173 
Epoch 212/1000 
	 loss: 30.1201, MinusLogProbMetric: 30.1201, val_loss: 32.3472, val_MinusLogProbMetric: 32.3472

Epoch 212: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.1201 - MinusLogProbMetric: 30.1201 - val_loss: 32.3472 - val_MinusLogProbMetric: 32.3472 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 213/1000
2023-10-11 17:07:50.567 
Epoch 213/1000 
	 loss: 30.1967, MinusLogProbMetric: 30.1967, val_loss: 30.1447, val_MinusLogProbMetric: 30.1447

Epoch 213: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.1967 - MinusLogProbMetric: 30.1967 - val_loss: 30.1447 - val_MinusLogProbMetric: 30.1447 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 214/1000
2023-10-11 17:08:17.305 
Epoch 214/1000 
	 loss: 29.8872, MinusLogProbMetric: 29.8872, val_loss: 30.7655, val_MinusLogProbMetric: 30.7655

Epoch 214: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.8872 - MinusLogProbMetric: 29.8872 - val_loss: 30.7655 - val_MinusLogProbMetric: 30.7655 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 215/1000
2023-10-11 17:08:43.863 
Epoch 215/1000 
	 loss: 30.2286, MinusLogProbMetric: 30.2286, val_loss: 30.1238, val_MinusLogProbMetric: 30.1238

Epoch 215: val_loss did not improve from 29.84743
196/196 - 27s - loss: 30.2286 - MinusLogProbMetric: 30.2286 - val_loss: 30.1238 - val_MinusLogProbMetric: 30.1238 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 216/1000
2023-10-11 17:09:10.000 
Epoch 216/1000 
	 loss: 30.0603, MinusLogProbMetric: 30.0603, val_loss: 30.2428, val_MinusLogProbMetric: 30.2428

Epoch 216: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0603 - MinusLogProbMetric: 30.0603 - val_loss: 30.2428 - val_MinusLogProbMetric: 30.2428 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 217/1000
2023-10-11 17:09:36.269 
Epoch 217/1000 
	 loss: 29.9412, MinusLogProbMetric: 29.9412, val_loss: 30.7423, val_MinusLogProbMetric: 30.7423

Epoch 217: val_loss did not improve from 29.84743
196/196 - 26s - loss: 29.9412 - MinusLogProbMetric: 29.9412 - val_loss: 30.7423 - val_MinusLogProbMetric: 30.7423 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 218/1000
2023-10-11 17:10:02.384 
Epoch 218/1000 
	 loss: 29.9278, MinusLogProbMetric: 29.9278, val_loss: 32.7793, val_MinusLogProbMetric: 32.7793

Epoch 218: val_loss did not improve from 29.84743
196/196 - 26s - loss: 29.9278 - MinusLogProbMetric: 29.9278 - val_loss: 32.7793 - val_MinusLogProbMetric: 32.7793 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 219/1000
2023-10-11 17:10:28.829 
Epoch 219/1000 
	 loss: 30.0160, MinusLogProbMetric: 30.0160, val_loss: 30.6197, val_MinusLogProbMetric: 30.6197

Epoch 219: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0160 - MinusLogProbMetric: 30.0160 - val_loss: 30.6197 - val_MinusLogProbMetric: 30.6197 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 220/1000
2023-10-11 17:10:54.975 
Epoch 220/1000 
	 loss: 30.2082, MinusLogProbMetric: 30.2082, val_loss: 30.2897, val_MinusLogProbMetric: 30.2897

Epoch 220: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.2082 - MinusLogProbMetric: 30.2082 - val_loss: 30.2897 - val_MinusLogProbMetric: 30.2897 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 221/1000
2023-10-11 17:11:21.713 
Epoch 221/1000 
	 loss: 29.8016, MinusLogProbMetric: 29.8016, val_loss: 30.3562, val_MinusLogProbMetric: 30.3562

Epoch 221: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.8016 - MinusLogProbMetric: 29.8016 - val_loss: 30.3562 - val_MinusLogProbMetric: 30.3562 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 222/1000
2023-10-11 17:11:48.076 
Epoch 222/1000 
	 loss: 30.0708, MinusLogProbMetric: 30.0708, val_loss: 30.3295, val_MinusLogProbMetric: 30.3295

Epoch 222: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0708 - MinusLogProbMetric: 30.0708 - val_loss: 30.3295 - val_MinusLogProbMetric: 30.3295 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 223/1000
2023-10-11 17:12:14.172 
Epoch 223/1000 
	 loss: 29.8333, MinusLogProbMetric: 29.8333, val_loss: 29.9755, val_MinusLogProbMetric: 29.9755

Epoch 223: val_loss did not improve from 29.84743
196/196 - 26s - loss: 29.8333 - MinusLogProbMetric: 29.8333 - val_loss: 29.9755 - val_MinusLogProbMetric: 29.9755 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 224/1000
2023-10-11 17:12:40.110 
Epoch 224/1000 
	 loss: 29.9688, MinusLogProbMetric: 29.9688, val_loss: 30.4391, val_MinusLogProbMetric: 30.4391

Epoch 224: val_loss did not improve from 29.84743
196/196 - 26s - loss: 29.9688 - MinusLogProbMetric: 29.9688 - val_loss: 30.4391 - val_MinusLogProbMetric: 30.4391 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 225/1000
2023-10-11 17:13:05.693 
Epoch 225/1000 
	 loss: 30.0787, MinusLogProbMetric: 30.0787, val_loss: 30.0908, val_MinusLogProbMetric: 30.0908

Epoch 225: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0787 - MinusLogProbMetric: 30.0787 - val_loss: 30.0908 - val_MinusLogProbMetric: 30.0908 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 226/1000
2023-10-11 17:13:32.227 
Epoch 226/1000 
	 loss: 29.9486, MinusLogProbMetric: 29.9486, val_loss: 31.3233, val_MinusLogProbMetric: 31.3233

Epoch 226: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.9486 - MinusLogProbMetric: 29.9486 - val_loss: 31.3233 - val_MinusLogProbMetric: 31.3233 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 227/1000
2023-10-11 17:13:59.119 
Epoch 227/1000 
	 loss: 29.9807, MinusLogProbMetric: 29.9807, val_loss: 31.4750, val_MinusLogProbMetric: 31.4750

Epoch 227: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.9807 - MinusLogProbMetric: 29.9807 - val_loss: 31.4750 - val_MinusLogProbMetric: 31.4750 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 228/1000
2023-10-11 17:14:25.432 
Epoch 228/1000 
	 loss: 30.0170, MinusLogProbMetric: 30.0170, val_loss: 30.1837, val_MinusLogProbMetric: 30.1837

Epoch 228: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.0170 - MinusLogProbMetric: 30.0170 - val_loss: 30.1837 - val_MinusLogProbMetric: 30.1837 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 229/1000
2023-10-11 17:14:52.005 
Epoch 229/1000 
	 loss: 29.9623, MinusLogProbMetric: 29.9623, val_loss: 30.7016, val_MinusLogProbMetric: 30.7016

Epoch 229: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.9623 - MinusLogProbMetric: 29.9623 - val_loss: 30.7016 - val_MinusLogProbMetric: 30.7016 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 230/1000
2023-10-11 17:15:18.696 
Epoch 230/1000 
	 loss: 29.8809, MinusLogProbMetric: 29.8809, val_loss: 32.0189, val_MinusLogProbMetric: 32.0189

Epoch 230: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.8809 - MinusLogProbMetric: 29.8809 - val_loss: 32.0189 - val_MinusLogProbMetric: 32.0189 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 231/1000
2023-10-11 17:15:44.551 
Epoch 231/1000 
	 loss: 30.3340, MinusLogProbMetric: 30.3340, val_loss: 30.1363, val_MinusLogProbMetric: 30.1363

Epoch 231: val_loss did not improve from 29.84743
196/196 - 26s - loss: 30.3340 - MinusLogProbMetric: 30.3340 - val_loss: 30.1363 - val_MinusLogProbMetric: 30.1363 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 232/1000
2023-10-11 17:16:11.154 
Epoch 232/1000 
	 loss: 29.8039, MinusLogProbMetric: 29.8039, val_loss: 29.8965, val_MinusLogProbMetric: 29.8965

Epoch 232: val_loss did not improve from 29.84743
196/196 - 27s - loss: 29.8039 - MinusLogProbMetric: 29.8039 - val_loss: 29.8965 - val_MinusLogProbMetric: 29.8965 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 233/1000
2023-10-11 17:16:37.507 
Epoch 233/1000 
	 loss: 29.9196, MinusLogProbMetric: 29.9196, val_loss: 29.7179, val_MinusLogProbMetric: 29.7179

Epoch 233: val_loss improved from 29.84743 to 29.71791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 29.9196 - MinusLogProbMetric: 29.9196 - val_loss: 29.7179 - val_MinusLogProbMetric: 29.7179 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 234/1000
2023-10-11 17:17:04.699 
Epoch 234/1000 
	 loss: 29.9846, MinusLogProbMetric: 29.9846, val_loss: 30.1546, val_MinusLogProbMetric: 30.1546

Epoch 234: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.9846 - MinusLogProbMetric: 29.9846 - val_loss: 30.1546 - val_MinusLogProbMetric: 30.1546 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 235/1000
2023-10-11 17:17:30.794 
Epoch 235/1000 
	 loss: 29.6873, MinusLogProbMetric: 29.6873, val_loss: 32.4022, val_MinusLogProbMetric: 32.4022

Epoch 235: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.6873 - MinusLogProbMetric: 29.6873 - val_loss: 32.4022 - val_MinusLogProbMetric: 32.4022 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 236/1000
2023-10-11 17:17:56.794 
Epoch 236/1000 
	 loss: 29.8702, MinusLogProbMetric: 29.8702, val_loss: 30.9425, val_MinusLogProbMetric: 30.9425

Epoch 236: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.8702 - MinusLogProbMetric: 29.8702 - val_loss: 30.9425 - val_MinusLogProbMetric: 30.9425 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 237/1000
2023-10-11 17:18:23.140 
Epoch 237/1000 
	 loss: 29.8986, MinusLogProbMetric: 29.8986, val_loss: 30.0858, val_MinusLogProbMetric: 30.0858

Epoch 237: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.8986 - MinusLogProbMetric: 29.8986 - val_loss: 30.0858 - val_MinusLogProbMetric: 30.0858 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 238/1000
2023-10-11 17:18:49.365 
Epoch 238/1000 
	 loss: 29.8511, MinusLogProbMetric: 29.8511, val_loss: 30.3268, val_MinusLogProbMetric: 30.3268

Epoch 238: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.8511 - MinusLogProbMetric: 29.8511 - val_loss: 30.3268 - val_MinusLogProbMetric: 30.3268 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 239/1000
2023-10-11 17:19:15.457 
Epoch 239/1000 
	 loss: 29.8425, MinusLogProbMetric: 29.8425, val_loss: 30.0916, val_MinusLogProbMetric: 30.0916

Epoch 239: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.8425 - MinusLogProbMetric: 29.8425 - val_loss: 30.0916 - val_MinusLogProbMetric: 30.0916 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 240/1000
2023-10-11 17:19:42.074 
Epoch 240/1000 
	 loss: 29.8496, MinusLogProbMetric: 29.8496, val_loss: 30.1828, val_MinusLogProbMetric: 30.1828

Epoch 240: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.8496 - MinusLogProbMetric: 29.8496 - val_loss: 30.1828 - val_MinusLogProbMetric: 30.1828 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 241/1000
2023-10-11 17:20:08.223 
Epoch 241/1000 
	 loss: 29.8873, MinusLogProbMetric: 29.8873, val_loss: 30.0463, val_MinusLogProbMetric: 30.0463

Epoch 241: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.8873 - MinusLogProbMetric: 29.8873 - val_loss: 30.0463 - val_MinusLogProbMetric: 30.0463 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 242/1000
2023-10-11 17:20:34.688 
Epoch 242/1000 
	 loss: 29.9497, MinusLogProbMetric: 29.9497, val_loss: 33.3380, val_MinusLogProbMetric: 33.3380

Epoch 242: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.9497 - MinusLogProbMetric: 29.9497 - val_loss: 33.3380 - val_MinusLogProbMetric: 33.3380 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 243/1000
2023-10-11 17:21:00.824 
Epoch 243/1000 
	 loss: 29.9943, MinusLogProbMetric: 29.9943, val_loss: 30.5982, val_MinusLogProbMetric: 30.5982

Epoch 243: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.9943 - MinusLogProbMetric: 29.9943 - val_loss: 30.5982 - val_MinusLogProbMetric: 30.5982 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 244/1000
2023-10-11 17:21:27.126 
Epoch 244/1000 
	 loss: 29.7467, MinusLogProbMetric: 29.7467, val_loss: 30.0691, val_MinusLogProbMetric: 30.0691

Epoch 244: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.7467 - MinusLogProbMetric: 29.7467 - val_loss: 30.0691 - val_MinusLogProbMetric: 30.0691 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 245/1000
2023-10-11 17:21:53.149 
Epoch 245/1000 
	 loss: 30.1898, MinusLogProbMetric: 30.1898, val_loss: 30.1984, val_MinusLogProbMetric: 30.1984

Epoch 245: val_loss did not improve from 29.71791
196/196 - 26s - loss: 30.1898 - MinusLogProbMetric: 30.1898 - val_loss: 30.1984 - val_MinusLogProbMetric: 30.1984 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 246/1000
2023-10-11 17:22:19.771 
Epoch 246/1000 
	 loss: 29.8023, MinusLogProbMetric: 29.8023, val_loss: 30.1917, val_MinusLogProbMetric: 30.1917

Epoch 246: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.8023 - MinusLogProbMetric: 29.8023 - val_loss: 30.1917 - val_MinusLogProbMetric: 30.1917 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 247/1000
2023-10-11 17:22:45.835 
Epoch 247/1000 
	 loss: 29.7064, MinusLogProbMetric: 29.7064, val_loss: 29.9637, val_MinusLogProbMetric: 29.9637

Epoch 247: val_loss did not improve from 29.71791
196/196 - 26s - loss: 29.7064 - MinusLogProbMetric: 29.7064 - val_loss: 29.9637 - val_MinusLogProbMetric: 29.9637 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 248/1000
2023-10-11 17:23:13.228 
Epoch 248/1000 
	 loss: 29.6734, MinusLogProbMetric: 29.6734, val_loss: 30.4189, val_MinusLogProbMetric: 30.4189

Epoch 248: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.6734 - MinusLogProbMetric: 29.6734 - val_loss: 30.4189 - val_MinusLogProbMetric: 30.4189 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 249/1000
2023-10-11 17:23:40.926 
Epoch 249/1000 
	 loss: 29.8526, MinusLogProbMetric: 29.8526, val_loss: 29.9065, val_MinusLogProbMetric: 29.9065

Epoch 249: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.8526 - MinusLogProbMetric: 29.8526 - val_loss: 29.9065 - val_MinusLogProbMetric: 29.9065 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 250/1000
2023-10-11 17:24:08.725 
Epoch 250/1000 
	 loss: 29.7568, MinusLogProbMetric: 29.7568, val_loss: 30.3657, val_MinusLogProbMetric: 30.3657

Epoch 250: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7568 - MinusLogProbMetric: 29.7568 - val_loss: 30.3657 - val_MinusLogProbMetric: 30.3657 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 251/1000
2023-10-11 17:24:36.069 
Epoch 251/1000 
	 loss: 29.8470, MinusLogProbMetric: 29.8470, val_loss: 30.0918, val_MinusLogProbMetric: 30.0918

Epoch 251: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.8470 - MinusLogProbMetric: 29.8470 - val_loss: 30.0918 - val_MinusLogProbMetric: 30.0918 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 252/1000
2023-10-11 17:25:03.695 
Epoch 252/1000 
	 loss: 29.7578, MinusLogProbMetric: 29.7578, val_loss: 30.3249, val_MinusLogProbMetric: 30.3249

Epoch 252: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7578 - MinusLogProbMetric: 29.7578 - val_loss: 30.3249 - val_MinusLogProbMetric: 30.3249 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 253/1000
2023-10-11 17:25:31.460 
Epoch 253/1000 
	 loss: 30.2624, MinusLogProbMetric: 30.2624, val_loss: 29.9805, val_MinusLogProbMetric: 29.9805

Epoch 253: val_loss did not improve from 29.71791
196/196 - 28s - loss: 30.2624 - MinusLogProbMetric: 30.2624 - val_loss: 29.9805 - val_MinusLogProbMetric: 29.9805 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 254/1000
2023-10-11 17:25:59.095 
Epoch 254/1000 
	 loss: 29.6344, MinusLogProbMetric: 29.6344, val_loss: 30.6606, val_MinusLogProbMetric: 30.6606

Epoch 254: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.6344 - MinusLogProbMetric: 29.6344 - val_loss: 30.6606 - val_MinusLogProbMetric: 30.6606 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 255/1000
2023-10-11 17:26:26.201 
Epoch 255/1000 
	 loss: 29.6925, MinusLogProbMetric: 29.6925, val_loss: 33.3404, val_MinusLogProbMetric: 33.3404

Epoch 255: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.6925 - MinusLogProbMetric: 29.6925 - val_loss: 33.3404 - val_MinusLogProbMetric: 33.3404 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 256/1000
2023-10-11 17:26:54.141 
Epoch 256/1000 
	 loss: 29.7621, MinusLogProbMetric: 29.7621, val_loss: 30.2093, val_MinusLogProbMetric: 30.2093

Epoch 256: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7621 - MinusLogProbMetric: 29.7621 - val_loss: 30.2093 - val_MinusLogProbMetric: 30.2093 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 257/1000
2023-10-11 17:27:21.679 
Epoch 257/1000 
	 loss: 29.9071, MinusLogProbMetric: 29.9071, val_loss: 30.2761, val_MinusLogProbMetric: 30.2761

Epoch 257: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.9071 - MinusLogProbMetric: 29.9071 - val_loss: 30.2761 - val_MinusLogProbMetric: 30.2761 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 258/1000
2023-10-11 17:27:49.156 
Epoch 258/1000 
	 loss: 29.7810, MinusLogProbMetric: 29.7810, val_loss: 29.9196, val_MinusLogProbMetric: 29.9196

Epoch 258: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.7810 - MinusLogProbMetric: 29.7810 - val_loss: 29.9196 - val_MinusLogProbMetric: 29.9196 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 259/1000
2023-10-11 17:28:16.868 
Epoch 259/1000 
	 loss: 29.7050, MinusLogProbMetric: 29.7050, val_loss: 29.8484, val_MinusLogProbMetric: 29.8484

Epoch 259: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7050 - MinusLogProbMetric: 29.7050 - val_loss: 29.8484 - val_MinusLogProbMetric: 29.8484 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 260/1000
2023-10-11 17:28:44.710 
Epoch 260/1000 
	 loss: 29.7448, MinusLogProbMetric: 29.7448, val_loss: 30.8537, val_MinusLogProbMetric: 30.8537

Epoch 260: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7448 - MinusLogProbMetric: 29.7448 - val_loss: 30.8537 - val_MinusLogProbMetric: 30.8537 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 261/1000
2023-10-11 17:29:12.670 
Epoch 261/1000 
	 loss: 29.7824, MinusLogProbMetric: 29.7824, val_loss: 30.0372, val_MinusLogProbMetric: 30.0372

Epoch 261: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7824 - MinusLogProbMetric: 29.7824 - val_loss: 30.0372 - val_MinusLogProbMetric: 30.0372 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 262/1000
2023-10-11 17:29:40.188 
Epoch 262/1000 
	 loss: 29.8099, MinusLogProbMetric: 29.8099, val_loss: 30.3398, val_MinusLogProbMetric: 30.3398

Epoch 262: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.8099 - MinusLogProbMetric: 29.8099 - val_loss: 30.3398 - val_MinusLogProbMetric: 30.3398 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 263/1000
2023-10-11 17:30:07.725 
Epoch 263/1000 
	 loss: 29.7701, MinusLogProbMetric: 29.7701, val_loss: 30.6944, val_MinusLogProbMetric: 30.6944

Epoch 263: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.7701 - MinusLogProbMetric: 29.7701 - val_loss: 30.6944 - val_MinusLogProbMetric: 30.6944 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 264/1000
2023-10-11 17:30:35.502 
Epoch 264/1000 
	 loss: 29.6068, MinusLogProbMetric: 29.6068, val_loss: 30.3369, val_MinusLogProbMetric: 30.3369

Epoch 264: val_loss did not improve from 29.71791
196/196 - 28s - loss: 29.6068 - MinusLogProbMetric: 29.6068 - val_loss: 30.3369 - val_MinusLogProbMetric: 30.3369 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 265/1000
2023-10-11 17:31:02.990 
Epoch 265/1000 
	 loss: 30.1205, MinusLogProbMetric: 30.1205, val_loss: 30.0624, val_MinusLogProbMetric: 30.0624

Epoch 265: val_loss did not improve from 29.71791
196/196 - 27s - loss: 30.1205 - MinusLogProbMetric: 30.1205 - val_loss: 30.0624 - val_MinusLogProbMetric: 30.0624 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 266/1000
2023-10-11 17:31:30.135 
Epoch 266/1000 
	 loss: 29.5414, MinusLogProbMetric: 29.5414, val_loss: 29.7250, val_MinusLogProbMetric: 29.7250

Epoch 266: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.5414 - MinusLogProbMetric: 29.5414 - val_loss: 29.7250 - val_MinusLogProbMetric: 29.7250 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 267/1000
2023-10-11 17:31:56.988 
Epoch 267/1000 
	 loss: 29.6466, MinusLogProbMetric: 29.6466, val_loss: 30.2104, val_MinusLogProbMetric: 30.2104

Epoch 267: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.6466 - MinusLogProbMetric: 29.6466 - val_loss: 30.2104 - val_MinusLogProbMetric: 30.2104 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 268/1000
2023-10-11 17:32:24.155 
Epoch 268/1000 
	 loss: 29.5686, MinusLogProbMetric: 29.5686, val_loss: 30.0064, val_MinusLogProbMetric: 30.0064

Epoch 268: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.5686 - MinusLogProbMetric: 29.5686 - val_loss: 30.0064 - val_MinusLogProbMetric: 30.0064 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 269/1000
2023-10-11 17:32:51.733 
Epoch 269/1000 
	 loss: 29.5922, MinusLogProbMetric: 29.5922, val_loss: 29.9799, val_MinusLogProbMetric: 29.9799

Epoch 269: val_loss did not improve from 29.71791
196/196 - 27s - loss: 29.5922 - MinusLogProbMetric: 29.5922 - val_loss: 29.9799 - val_MinusLogProbMetric: 29.9799 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 270/1000
2023-10-11 17:33:18.163 
Epoch 270/1000 
	 loss: 29.7220, MinusLogProbMetric: 29.7220, val_loss: 29.6029, val_MinusLogProbMetric: 29.6029

Epoch 270: val_loss improved from 29.71791 to 29.60286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 29.7220 - MinusLogProbMetric: 29.7220 - val_loss: 29.6029 - val_MinusLogProbMetric: 29.6029 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 271/1000
2023-10-11 17:33:45.274 
Epoch 271/1000 
	 loss: 29.5734, MinusLogProbMetric: 29.5734, val_loss: 30.0044, val_MinusLogProbMetric: 30.0044

Epoch 271: val_loss did not improve from 29.60286
196/196 - 27s - loss: 29.5734 - MinusLogProbMetric: 29.5734 - val_loss: 30.0044 - val_MinusLogProbMetric: 30.0044 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 272/1000
2023-10-11 17:34:13.292 
Epoch 272/1000 
	 loss: 29.5069, MinusLogProbMetric: 29.5069, val_loss: 29.7258, val_MinusLogProbMetric: 29.7258

Epoch 272: val_loss did not improve from 29.60286
196/196 - 28s - loss: 29.5069 - MinusLogProbMetric: 29.5069 - val_loss: 29.7258 - val_MinusLogProbMetric: 29.7258 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 273/1000
2023-10-11 17:34:41.429 
Epoch 273/1000 
	 loss: 29.5441, MinusLogProbMetric: 29.5441, val_loss: 29.6127, val_MinusLogProbMetric: 29.6127

Epoch 273: val_loss did not improve from 29.60286
196/196 - 28s - loss: 29.5441 - MinusLogProbMetric: 29.5441 - val_loss: 29.6127 - val_MinusLogProbMetric: 29.6127 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 274/1000
2023-10-11 17:35:09.547 
Epoch 274/1000 
	 loss: 29.8756, MinusLogProbMetric: 29.8756, val_loss: 30.8340, val_MinusLogProbMetric: 30.8340

Epoch 274: val_loss did not improve from 29.60286
196/196 - 28s - loss: 29.8756 - MinusLogProbMetric: 29.8756 - val_loss: 30.8340 - val_MinusLogProbMetric: 30.8340 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 275/1000
2023-10-11 17:35:37.766 
Epoch 275/1000 
	 loss: 29.9005, MinusLogProbMetric: 29.9005, val_loss: 29.4900, val_MinusLogProbMetric: 29.4900

Epoch 275: val_loss improved from 29.60286 to 29.49002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 29s - loss: 29.9005 - MinusLogProbMetric: 29.9005 - val_loss: 29.4900 - val_MinusLogProbMetric: 29.4900 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 276/1000
2023-10-11 17:36:05.995 
Epoch 276/1000 
	 loss: 29.5321, MinusLogProbMetric: 29.5321, val_loss: 30.8547, val_MinusLogProbMetric: 30.8547

Epoch 276: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5321 - MinusLogProbMetric: 29.5321 - val_loss: 30.8547 - val_MinusLogProbMetric: 30.8547 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 277/1000
2023-10-11 17:36:33.810 
Epoch 277/1000 
	 loss: 29.4716, MinusLogProbMetric: 29.4716, val_loss: 30.7258, val_MinusLogProbMetric: 30.7258

Epoch 277: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.4716 - MinusLogProbMetric: 29.4716 - val_loss: 30.7258 - val_MinusLogProbMetric: 30.7258 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 278/1000
2023-10-11 17:37:01.756 
Epoch 278/1000 
	 loss: 29.7047, MinusLogProbMetric: 29.7047, val_loss: 30.7959, val_MinusLogProbMetric: 30.7959

Epoch 278: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.7047 - MinusLogProbMetric: 29.7047 - val_loss: 30.7959 - val_MinusLogProbMetric: 30.7959 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 279/1000
2023-10-11 17:37:29.336 
Epoch 279/1000 
	 loss: 29.7885, MinusLogProbMetric: 29.7885, val_loss: 30.0579, val_MinusLogProbMetric: 30.0579

Epoch 279: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.7885 - MinusLogProbMetric: 29.7885 - val_loss: 30.0579 - val_MinusLogProbMetric: 30.0579 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 280/1000
2023-10-11 17:37:57.404 
Epoch 280/1000 
	 loss: 29.5319, MinusLogProbMetric: 29.5319, val_loss: 29.6732, val_MinusLogProbMetric: 29.6732

Epoch 280: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5319 - MinusLogProbMetric: 29.5319 - val_loss: 29.6732 - val_MinusLogProbMetric: 29.6732 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 281/1000
2023-10-11 17:38:25.221 
Epoch 281/1000 
	 loss: 29.5527, MinusLogProbMetric: 29.5527, val_loss: 29.9629, val_MinusLogProbMetric: 29.9629

Epoch 281: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5527 - MinusLogProbMetric: 29.5527 - val_loss: 29.9629 - val_MinusLogProbMetric: 29.9629 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 282/1000
2023-10-11 17:38:53.104 
Epoch 282/1000 
	 loss: 29.4701, MinusLogProbMetric: 29.4701, val_loss: 30.1474, val_MinusLogProbMetric: 30.1474

Epoch 282: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.4701 - MinusLogProbMetric: 29.4701 - val_loss: 30.1474 - val_MinusLogProbMetric: 30.1474 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 283/1000
2023-10-11 17:39:20.906 
Epoch 283/1000 
	 loss: 29.5256, MinusLogProbMetric: 29.5256, val_loss: 31.0967, val_MinusLogProbMetric: 31.0967

Epoch 283: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5256 - MinusLogProbMetric: 29.5256 - val_loss: 31.0967 - val_MinusLogProbMetric: 31.0967 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 284/1000
2023-10-11 17:39:48.582 
Epoch 284/1000 
	 loss: 29.5041, MinusLogProbMetric: 29.5041, val_loss: 30.4776, val_MinusLogProbMetric: 30.4776

Epoch 284: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5041 - MinusLogProbMetric: 29.5041 - val_loss: 30.4776 - val_MinusLogProbMetric: 30.4776 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 285/1000
2023-10-11 17:40:16.253 
Epoch 285/1000 
	 loss: 29.5350, MinusLogProbMetric: 29.5350, val_loss: 30.1709, val_MinusLogProbMetric: 30.1709

Epoch 285: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5350 - MinusLogProbMetric: 29.5350 - val_loss: 30.1709 - val_MinusLogProbMetric: 30.1709 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 286/1000
2023-10-11 17:40:44.146 
Epoch 286/1000 
	 loss: 29.5497, MinusLogProbMetric: 29.5497, val_loss: 30.3795, val_MinusLogProbMetric: 30.3795

Epoch 286: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5497 - MinusLogProbMetric: 29.5497 - val_loss: 30.3795 - val_MinusLogProbMetric: 30.3795 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 287/1000
2023-10-11 17:41:12.386 
Epoch 287/1000 
	 loss: 29.5123, MinusLogProbMetric: 29.5123, val_loss: 30.0289, val_MinusLogProbMetric: 30.0289

Epoch 287: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5123 - MinusLogProbMetric: 29.5123 - val_loss: 30.0289 - val_MinusLogProbMetric: 30.0289 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 288/1000
2023-10-11 17:41:40.486 
Epoch 288/1000 
	 loss: 29.4822, MinusLogProbMetric: 29.4822, val_loss: 29.5931, val_MinusLogProbMetric: 29.5931

Epoch 288: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.4822 - MinusLogProbMetric: 29.4822 - val_loss: 29.5931 - val_MinusLogProbMetric: 29.5931 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 289/1000
2023-10-11 17:42:07.614 
Epoch 289/1000 
	 loss: 29.3884, MinusLogProbMetric: 29.3884, val_loss: 30.7725, val_MinusLogProbMetric: 30.7725

Epoch 289: val_loss did not improve from 29.49002
196/196 - 27s - loss: 29.3884 - MinusLogProbMetric: 29.3884 - val_loss: 30.7725 - val_MinusLogProbMetric: 30.7725 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 290/1000
2023-10-11 17:42:35.020 
Epoch 290/1000 
	 loss: 29.9270, MinusLogProbMetric: 29.9270, val_loss: 30.1351, val_MinusLogProbMetric: 30.1351

Epoch 290: val_loss did not improve from 29.49002
196/196 - 27s - loss: 29.9270 - MinusLogProbMetric: 29.9270 - val_loss: 30.1351 - val_MinusLogProbMetric: 30.1351 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 291/1000
2023-10-11 17:43:02.972 
Epoch 291/1000 
	 loss: 29.5270, MinusLogProbMetric: 29.5270, val_loss: 30.2125, val_MinusLogProbMetric: 30.2125

Epoch 291: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5270 - MinusLogProbMetric: 29.5270 - val_loss: 30.2125 - val_MinusLogProbMetric: 30.2125 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 292/1000
2023-10-11 17:43:30.602 
Epoch 292/1000 
	 loss: 29.3830, MinusLogProbMetric: 29.3830, val_loss: 29.5599, val_MinusLogProbMetric: 29.5599

Epoch 292: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.3830 - MinusLogProbMetric: 29.3830 - val_loss: 29.5599 - val_MinusLogProbMetric: 29.5599 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 293/1000
2023-10-11 17:43:58.206 
Epoch 293/1000 
	 loss: 29.5926, MinusLogProbMetric: 29.5926, val_loss: 29.8324, val_MinusLogProbMetric: 29.8324

Epoch 293: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.5926 - MinusLogProbMetric: 29.5926 - val_loss: 29.8324 - val_MinusLogProbMetric: 29.8324 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 294/1000
2023-10-11 17:44:25.948 
Epoch 294/1000 
	 loss: 29.3596, MinusLogProbMetric: 29.3596, val_loss: 30.3084, val_MinusLogProbMetric: 30.3084

Epoch 294: val_loss did not improve from 29.49002
196/196 - 28s - loss: 29.3596 - MinusLogProbMetric: 29.3596 - val_loss: 30.3084 - val_MinusLogProbMetric: 30.3084 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 295/1000
2023-10-11 17:44:53.634 
Epoch 295/1000 
	 loss: 29.3892, MinusLogProbMetric: 29.3892, val_loss: 29.4566, val_MinusLogProbMetric: 29.4566

Epoch 295: val_loss improved from 29.49002 to 29.45659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 29.3892 - MinusLogProbMetric: 29.3892 - val_loss: 29.4566 - val_MinusLogProbMetric: 29.4566 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 296/1000
2023-10-11 17:45:21.873 
Epoch 296/1000 
	 loss: 29.4531, MinusLogProbMetric: 29.4531, val_loss: 30.2669, val_MinusLogProbMetric: 30.2669

Epoch 296: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4531 - MinusLogProbMetric: 29.4531 - val_loss: 30.2669 - val_MinusLogProbMetric: 30.2669 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 297/1000
2023-10-11 17:45:49.482 
Epoch 297/1000 
	 loss: 29.4161, MinusLogProbMetric: 29.4161, val_loss: 29.7013, val_MinusLogProbMetric: 29.7013

Epoch 297: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4161 - MinusLogProbMetric: 29.4161 - val_loss: 29.7013 - val_MinusLogProbMetric: 29.7013 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 298/1000
2023-10-11 17:46:16.955 
Epoch 298/1000 
	 loss: 29.7165, MinusLogProbMetric: 29.7165, val_loss: 30.7581, val_MinusLogProbMetric: 30.7581

Epoch 298: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.7165 - MinusLogProbMetric: 29.7165 - val_loss: 30.7581 - val_MinusLogProbMetric: 30.7581 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 299/1000
2023-10-11 17:46:44.786 
Epoch 299/1000 
	 loss: 29.4751, MinusLogProbMetric: 29.4751, val_loss: 29.6528, val_MinusLogProbMetric: 29.6528

Epoch 299: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4751 - MinusLogProbMetric: 29.4751 - val_loss: 29.6528 - val_MinusLogProbMetric: 29.6528 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 300/1000
2023-10-11 17:47:12.900 
Epoch 300/1000 
	 loss: 29.3907, MinusLogProbMetric: 29.3907, val_loss: 31.5523, val_MinusLogProbMetric: 31.5523

Epoch 300: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.3907 - MinusLogProbMetric: 29.3907 - val_loss: 31.5523 - val_MinusLogProbMetric: 31.5523 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 301/1000
2023-10-11 17:47:40.480 
Epoch 301/1000 
	 loss: 29.4091, MinusLogProbMetric: 29.4091, val_loss: 29.9340, val_MinusLogProbMetric: 29.9340

Epoch 301: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4091 - MinusLogProbMetric: 29.4091 - val_loss: 29.9340 - val_MinusLogProbMetric: 29.9340 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 302/1000
2023-10-11 17:48:07.884 
Epoch 302/1000 
	 loss: 29.4773, MinusLogProbMetric: 29.4773, val_loss: 29.9646, val_MinusLogProbMetric: 29.9646

Epoch 302: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.4773 - MinusLogProbMetric: 29.4773 - val_loss: 29.9646 - val_MinusLogProbMetric: 29.9646 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 303/1000
2023-10-11 17:48:35.511 
Epoch 303/1000 
	 loss: 29.4105, MinusLogProbMetric: 29.4105, val_loss: 30.0562, val_MinusLogProbMetric: 30.0562

Epoch 303: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4105 - MinusLogProbMetric: 29.4105 - val_loss: 30.0562 - val_MinusLogProbMetric: 30.0562 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 304/1000
2023-10-11 17:49:03.653 
Epoch 304/1000 
	 loss: 29.3952, MinusLogProbMetric: 29.3952, val_loss: 29.7531, val_MinusLogProbMetric: 29.7531

Epoch 304: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.3952 - MinusLogProbMetric: 29.3952 - val_loss: 29.7531 - val_MinusLogProbMetric: 29.7531 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 305/1000
2023-10-11 17:49:31.454 
Epoch 305/1000 
	 loss: 29.4042, MinusLogProbMetric: 29.4042, val_loss: 30.2368, val_MinusLogProbMetric: 30.2368

Epoch 305: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4042 - MinusLogProbMetric: 29.4042 - val_loss: 30.2368 - val_MinusLogProbMetric: 30.2368 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 306/1000
2023-10-11 17:49:59.356 
Epoch 306/1000 
	 loss: 29.3096, MinusLogProbMetric: 29.3096, val_loss: 29.6795, val_MinusLogProbMetric: 29.6795

Epoch 306: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.3096 - MinusLogProbMetric: 29.3096 - val_loss: 29.6795 - val_MinusLogProbMetric: 29.6795 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 307/1000
2023-10-11 17:50:26.880 
Epoch 307/1000 
	 loss: 29.5935, MinusLogProbMetric: 29.5935, val_loss: 29.5901, val_MinusLogProbMetric: 29.5901

Epoch 307: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.5935 - MinusLogProbMetric: 29.5935 - val_loss: 29.5901 - val_MinusLogProbMetric: 29.5901 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 308/1000
2023-10-11 17:50:54.829 
Epoch 308/1000 
	 loss: 29.2481, MinusLogProbMetric: 29.2481, val_loss: 29.9557, val_MinusLogProbMetric: 29.9557

Epoch 308: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.2481 - MinusLogProbMetric: 29.2481 - val_loss: 29.9557 - val_MinusLogProbMetric: 29.9557 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 309/1000
2023-10-11 17:51:22.273 
Epoch 309/1000 
	 loss: 29.3856, MinusLogProbMetric: 29.3856, val_loss: 30.4261, val_MinusLogProbMetric: 30.4261

Epoch 309: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.3856 - MinusLogProbMetric: 29.3856 - val_loss: 30.4261 - val_MinusLogProbMetric: 30.4261 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 310/1000
2023-10-11 17:51:49.399 
Epoch 310/1000 
	 loss: 29.3776, MinusLogProbMetric: 29.3776, val_loss: 30.1194, val_MinusLogProbMetric: 30.1194

Epoch 310: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.3776 - MinusLogProbMetric: 29.3776 - val_loss: 30.1194 - val_MinusLogProbMetric: 30.1194 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 311/1000
2023-10-11 17:52:16.613 
Epoch 311/1000 
	 loss: 29.3531, MinusLogProbMetric: 29.3531, val_loss: 29.6347, val_MinusLogProbMetric: 29.6347

Epoch 311: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.3531 - MinusLogProbMetric: 29.3531 - val_loss: 29.6347 - val_MinusLogProbMetric: 29.6347 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 312/1000
2023-10-11 17:52:44.263 
Epoch 312/1000 
	 loss: 29.3214, MinusLogProbMetric: 29.3214, val_loss: 30.5123, val_MinusLogProbMetric: 30.5123

Epoch 312: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.3214 - MinusLogProbMetric: 29.3214 - val_loss: 30.5123 - val_MinusLogProbMetric: 30.5123 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 313/1000
2023-10-11 17:53:11.951 
Epoch 313/1000 
	 loss: 29.2757, MinusLogProbMetric: 29.2757, val_loss: 30.1428, val_MinusLogProbMetric: 30.1428

Epoch 313: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.2757 - MinusLogProbMetric: 29.2757 - val_loss: 30.1428 - val_MinusLogProbMetric: 30.1428 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 314/1000
2023-10-11 17:53:39.235 
Epoch 314/1000 
	 loss: 29.3697, MinusLogProbMetric: 29.3697, val_loss: 29.5929, val_MinusLogProbMetric: 29.5929

Epoch 314: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.3697 - MinusLogProbMetric: 29.3697 - val_loss: 29.5929 - val_MinusLogProbMetric: 29.5929 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 315/1000
2023-10-11 17:54:06.443 
Epoch 315/1000 
	 loss: 29.3721, MinusLogProbMetric: 29.3721, val_loss: 29.6771, val_MinusLogProbMetric: 29.6771

Epoch 315: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.3721 - MinusLogProbMetric: 29.3721 - val_loss: 29.6771 - val_MinusLogProbMetric: 29.6771 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 316/1000
2023-10-11 17:54:34.053 
Epoch 316/1000 
	 loss: 29.4417, MinusLogProbMetric: 29.4417, val_loss: 29.8991, val_MinusLogProbMetric: 29.8991

Epoch 316: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.4417 - MinusLogProbMetric: 29.4417 - val_loss: 29.8991 - val_MinusLogProbMetric: 29.8991 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 317/1000
2023-10-11 17:55:01.611 
Epoch 317/1000 
	 loss: 29.2599, MinusLogProbMetric: 29.2599, val_loss: 29.7208, val_MinusLogProbMetric: 29.7208

Epoch 317: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.2599 - MinusLogProbMetric: 29.2599 - val_loss: 29.7208 - val_MinusLogProbMetric: 29.7208 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 318/1000
2023-10-11 17:55:29.551 
Epoch 318/1000 
	 loss: 29.3835, MinusLogProbMetric: 29.3835, val_loss: 30.4504, val_MinusLogProbMetric: 30.4504

Epoch 318: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.3835 - MinusLogProbMetric: 29.3835 - val_loss: 30.4504 - val_MinusLogProbMetric: 30.4504 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 319/1000
2023-10-11 17:55:56.962 
Epoch 319/1000 
	 loss: 29.2506, MinusLogProbMetric: 29.2506, val_loss: 29.7049, val_MinusLogProbMetric: 29.7049

Epoch 319: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.2506 - MinusLogProbMetric: 29.2506 - val_loss: 29.7049 - val_MinusLogProbMetric: 29.7049 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 320/1000
2023-10-11 17:56:24.562 
Epoch 320/1000 
	 loss: 29.2352, MinusLogProbMetric: 29.2352, val_loss: 29.7588, val_MinusLogProbMetric: 29.7588

Epoch 320: val_loss did not improve from 29.45659
196/196 - 28s - loss: 29.2352 - MinusLogProbMetric: 29.2352 - val_loss: 29.7588 - val_MinusLogProbMetric: 29.7588 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 321/1000
2023-10-11 17:56:51.860 
Epoch 321/1000 
	 loss: 29.3600, MinusLogProbMetric: 29.3600, val_loss: 29.9567, val_MinusLogProbMetric: 29.9567

Epoch 321: val_loss did not improve from 29.45659
196/196 - 27s - loss: 29.3600 - MinusLogProbMetric: 29.3600 - val_loss: 29.9567 - val_MinusLogProbMetric: 29.9567 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 322/1000
2023-10-11 17:57:19.801 
Epoch 322/1000 
	 loss: 29.3299, MinusLogProbMetric: 29.3299, val_loss: 29.4357, val_MinusLogProbMetric: 29.4357

Epoch 322: val_loss improved from 29.45659 to 29.43567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 29s - loss: 29.3299 - MinusLogProbMetric: 29.3299 - val_loss: 29.4357 - val_MinusLogProbMetric: 29.4357 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 323/1000
2023-10-11 17:57:47.956 
Epoch 323/1000 
	 loss: 29.4521, MinusLogProbMetric: 29.4521, val_loss: 30.3920, val_MinusLogProbMetric: 30.3920

Epoch 323: val_loss did not improve from 29.43567
196/196 - 28s - loss: 29.4521 - MinusLogProbMetric: 29.4521 - val_loss: 30.3920 - val_MinusLogProbMetric: 30.3920 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 324/1000
2023-10-11 17:58:15.192 
Epoch 324/1000 
	 loss: 29.2817, MinusLogProbMetric: 29.2817, val_loss: 30.9836, val_MinusLogProbMetric: 30.9836

Epoch 324: val_loss did not improve from 29.43567
196/196 - 27s - loss: 29.2817 - MinusLogProbMetric: 29.2817 - val_loss: 30.9836 - val_MinusLogProbMetric: 30.9836 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 325/1000
2023-10-11 17:58:42.029 
Epoch 325/1000 
	 loss: 29.4769, MinusLogProbMetric: 29.4769, val_loss: 30.0043, val_MinusLogProbMetric: 30.0043

Epoch 325: val_loss did not improve from 29.43567
196/196 - 27s - loss: 29.4769 - MinusLogProbMetric: 29.4769 - val_loss: 30.0043 - val_MinusLogProbMetric: 30.0043 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 326/1000
2023-10-11 17:59:08.919 
Epoch 326/1000 
	 loss: 29.4521, MinusLogProbMetric: 29.4521, val_loss: 29.9232, val_MinusLogProbMetric: 29.9232

Epoch 326: val_loss did not improve from 29.43567
196/196 - 27s - loss: 29.4521 - MinusLogProbMetric: 29.4521 - val_loss: 29.9232 - val_MinusLogProbMetric: 29.9232 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 327/1000
2023-10-11 17:59:36.309 
Epoch 327/1000 
	 loss: 29.2976, MinusLogProbMetric: 29.2976, val_loss: 29.8141, val_MinusLogProbMetric: 29.8141

Epoch 327: val_loss did not improve from 29.43567
196/196 - 27s - loss: 29.2976 - MinusLogProbMetric: 29.2976 - val_loss: 29.8141 - val_MinusLogProbMetric: 29.8141 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 328/1000
2023-10-11 18:00:03.137 
Epoch 328/1000 
	 loss: 29.3417, MinusLogProbMetric: 29.3417, val_loss: 29.6036, val_MinusLogProbMetric: 29.6036

Epoch 328: val_loss did not improve from 29.43567
196/196 - 27s - loss: 29.3417 - MinusLogProbMetric: 29.3417 - val_loss: 29.6036 - val_MinusLogProbMetric: 29.6036 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 329/1000
2023-10-11 18:00:30.115 
Epoch 329/1000 
	 loss: 29.2381, MinusLogProbMetric: 29.2381, val_loss: 29.3813, val_MinusLogProbMetric: 29.3813

Epoch 329: val_loss improved from 29.43567 to 29.38125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 29.2381 - MinusLogProbMetric: 29.2381 - val_loss: 29.3813 - val_MinusLogProbMetric: 29.3813 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 330/1000
2023-10-11 18:00:57.594 
Epoch 330/1000 
	 loss: 29.2784, MinusLogProbMetric: 29.2784, val_loss: 29.6857, val_MinusLogProbMetric: 29.6857

Epoch 330: val_loss did not improve from 29.38125
196/196 - 27s - loss: 29.2784 - MinusLogProbMetric: 29.2784 - val_loss: 29.6857 - val_MinusLogProbMetric: 29.6857 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 331/1000
2023-10-11 18:01:24.581 
Epoch 331/1000 
	 loss: 29.2013, MinusLogProbMetric: 29.2013, val_loss: 29.5168, val_MinusLogProbMetric: 29.5168

Epoch 331: val_loss did not improve from 29.38125
196/196 - 27s - loss: 29.2013 - MinusLogProbMetric: 29.2013 - val_loss: 29.5168 - val_MinusLogProbMetric: 29.5168 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 332/1000
2023-10-11 18:01:51.557 
Epoch 332/1000 
	 loss: 29.2930, MinusLogProbMetric: 29.2930, val_loss: 29.8043, val_MinusLogProbMetric: 29.8043

Epoch 332: val_loss did not improve from 29.38125
196/196 - 27s - loss: 29.2930 - MinusLogProbMetric: 29.2930 - val_loss: 29.8043 - val_MinusLogProbMetric: 29.8043 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 333/1000
2023-10-11 18:02:18.236 
Epoch 333/1000 
	 loss: 29.4159, MinusLogProbMetric: 29.4159, val_loss: 29.7172, val_MinusLogProbMetric: 29.7172

Epoch 333: val_loss did not improve from 29.38125
196/196 - 27s - loss: 29.4159 - MinusLogProbMetric: 29.4159 - val_loss: 29.7172 - val_MinusLogProbMetric: 29.7172 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 334/1000
2023-10-11 18:02:45.049 
Epoch 334/1000 
	 loss: 29.0802, MinusLogProbMetric: 29.0802, val_loss: 29.3739, val_MinusLogProbMetric: 29.3739

Epoch 334: val_loss improved from 29.38125 to 29.37387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 29.0802 - MinusLogProbMetric: 29.0802 - val_loss: 29.3739 - val_MinusLogProbMetric: 29.3739 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 335/1000
2023-10-11 18:03:12.710 
Epoch 335/1000 
	 loss: 29.3780, MinusLogProbMetric: 29.3780, val_loss: 30.6050, val_MinusLogProbMetric: 30.6050

Epoch 335: val_loss did not improve from 29.37387
196/196 - 27s - loss: 29.3780 - MinusLogProbMetric: 29.3780 - val_loss: 30.6050 - val_MinusLogProbMetric: 30.6050 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 336/1000
2023-10-11 18:03:39.503 
Epoch 336/1000 
	 loss: 29.1742, MinusLogProbMetric: 29.1742, val_loss: 29.6142, val_MinusLogProbMetric: 29.6142

Epoch 336: val_loss did not improve from 29.37387
196/196 - 27s - loss: 29.1742 - MinusLogProbMetric: 29.1742 - val_loss: 29.6142 - val_MinusLogProbMetric: 29.6142 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 337/1000
2023-10-11 18:04:06.666 
Epoch 337/1000 
	 loss: 29.2551, MinusLogProbMetric: 29.2551, val_loss: 29.4795, val_MinusLogProbMetric: 29.4795

Epoch 337: val_loss did not improve from 29.37387
196/196 - 27s - loss: 29.2551 - MinusLogProbMetric: 29.2551 - val_loss: 29.4795 - val_MinusLogProbMetric: 29.4795 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 338/1000
2023-10-11 18:04:33.966 
Epoch 338/1000 
	 loss: 29.0919, MinusLogProbMetric: 29.0919, val_loss: 29.3407, val_MinusLogProbMetric: 29.3407

Epoch 338: val_loss improved from 29.37387 to 29.34070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 29.0919 - MinusLogProbMetric: 29.0919 - val_loss: 29.3407 - val_MinusLogProbMetric: 29.3407 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 339/1000
2023-10-11 18:05:01.805 
Epoch 339/1000 
	 loss: 29.3495, MinusLogProbMetric: 29.3495, val_loss: 29.5541, val_MinusLogProbMetric: 29.5541

Epoch 339: val_loss did not improve from 29.34070
196/196 - 27s - loss: 29.3495 - MinusLogProbMetric: 29.3495 - val_loss: 29.5541 - val_MinusLogProbMetric: 29.5541 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 340/1000
2023-10-11 18:05:28.874 
Epoch 340/1000 
	 loss: 29.1767, MinusLogProbMetric: 29.1767, val_loss: 30.1257, val_MinusLogProbMetric: 30.1257

Epoch 340: val_loss did not improve from 29.34070
196/196 - 27s - loss: 29.1767 - MinusLogProbMetric: 29.1767 - val_loss: 30.1257 - val_MinusLogProbMetric: 30.1257 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 341/1000
2023-10-11 18:05:55.953 
Epoch 341/1000 
	 loss: 29.1874, MinusLogProbMetric: 29.1874, val_loss: 31.3865, val_MinusLogProbMetric: 31.3865

Epoch 341: val_loss did not improve from 29.34070
196/196 - 27s - loss: 29.1874 - MinusLogProbMetric: 29.1874 - val_loss: 31.3865 - val_MinusLogProbMetric: 31.3865 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 342/1000
2023-10-11 18:06:23.423 
Epoch 342/1000 
	 loss: 29.2496, MinusLogProbMetric: 29.2496, val_loss: 29.3824, val_MinusLogProbMetric: 29.3824

Epoch 342: val_loss did not improve from 29.34070
196/196 - 27s - loss: 29.2496 - MinusLogProbMetric: 29.2496 - val_loss: 29.3824 - val_MinusLogProbMetric: 29.3824 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 343/1000
2023-10-11 18:06:50.544 
Epoch 343/1000 
	 loss: 29.1530, MinusLogProbMetric: 29.1530, val_loss: 31.5853, val_MinusLogProbMetric: 31.5853

Epoch 343: val_loss did not improve from 29.34070
196/196 - 27s - loss: 29.1530 - MinusLogProbMetric: 29.1530 - val_loss: 31.5853 - val_MinusLogProbMetric: 31.5853 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 344/1000
2023-10-11 18:07:17.545 
Epoch 344/1000 
	 loss: 29.2625, MinusLogProbMetric: 29.2625, val_loss: 29.2778, val_MinusLogProbMetric: 29.2778

Epoch 344: val_loss improved from 29.34070 to 29.27781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 29.2625 - MinusLogProbMetric: 29.2625 - val_loss: 29.2778 - val_MinusLogProbMetric: 29.2778 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 345/1000
2023-10-11 18:07:44.910 
Epoch 345/1000 
	 loss: 29.3368, MinusLogProbMetric: 29.3368, val_loss: 29.6874, val_MinusLogProbMetric: 29.6874

Epoch 345: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.3368 - MinusLogProbMetric: 29.3368 - val_loss: 29.6874 - val_MinusLogProbMetric: 29.6874 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 346/1000
2023-10-11 18:08:11.779 
Epoch 346/1000 
	 loss: 29.1346, MinusLogProbMetric: 29.1346, val_loss: 29.4831, val_MinusLogProbMetric: 29.4831

Epoch 346: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1346 - MinusLogProbMetric: 29.1346 - val_loss: 29.4831 - val_MinusLogProbMetric: 29.4831 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 347/1000
2023-10-11 18:08:38.554 
Epoch 347/1000 
	 loss: 29.0998, MinusLogProbMetric: 29.0998, val_loss: 30.3782, val_MinusLogProbMetric: 30.3782

Epoch 347: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.0998 - MinusLogProbMetric: 29.0998 - val_loss: 30.3782 - val_MinusLogProbMetric: 30.3782 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 348/1000
2023-10-11 18:09:05.255 
Epoch 348/1000 
	 loss: 29.3815, MinusLogProbMetric: 29.3815, val_loss: 30.6386, val_MinusLogProbMetric: 30.6386

Epoch 348: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.3815 - MinusLogProbMetric: 29.3815 - val_loss: 30.6386 - val_MinusLogProbMetric: 30.6386 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 349/1000
2023-10-11 18:09:32.413 
Epoch 349/1000 
	 loss: 29.1341, MinusLogProbMetric: 29.1341, val_loss: 29.5652, val_MinusLogProbMetric: 29.5652

Epoch 349: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1341 - MinusLogProbMetric: 29.1341 - val_loss: 29.5652 - val_MinusLogProbMetric: 29.5652 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 350/1000
2023-10-11 18:09:59.509 
Epoch 350/1000 
	 loss: 29.2314, MinusLogProbMetric: 29.2314, val_loss: 29.6826, val_MinusLogProbMetric: 29.6826

Epoch 350: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.2314 - MinusLogProbMetric: 29.2314 - val_loss: 29.6826 - val_MinusLogProbMetric: 29.6826 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 351/1000
2023-10-11 18:10:26.728 
Epoch 351/1000 
	 loss: 29.4143, MinusLogProbMetric: 29.4143, val_loss: 29.3383, val_MinusLogProbMetric: 29.3383

Epoch 351: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.4143 - MinusLogProbMetric: 29.4143 - val_loss: 29.3383 - val_MinusLogProbMetric: 29.3383 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 352/1000
2023-10-11 18:10:53.878 
Epoch 352/1000 
	 loss: 29.1411, MinusLogProbMetric: 29.1411, val_loss: 29.9604, val_MinusLogProbMetric: 29.9604

Epoch 352: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1411 - MinusLogProbMetric: 29.1411 - val_loss: 29.9604 - val_MinusLogProbMetric: 29.9604 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 353/1000
2023-10-11 18:11:20.629 
Epoch 353/1000 
	 loss: 29.1452, MinusLogProbMetric: 29.1452, val_loss: 29.8307, val_MinusLogProbMetric: 29.8307

Epoch 353: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1452 - MinusLogProbMetric: 29.1452 - val_loss: 29.8307 - val_MinusLogProbMetric: 29.8307 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 354/1000
2023-10-11 18:11:47.853 
Epoch 354/1000 
	 loss: 29.1452, MinusLogProbMetric: 29.1452, val_loss: 29.3685, val_MinusLogProbMetric: 29.3685

Epoch 354: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1452 - MinusLogProbMetric: 29.1452 - val_loss: 29.3685 - val_MinusLogProbMetric: 29.3685 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 355/1000
2023-10-11 18:12:14.911 
Epoch 355/1000 
	 loss: 29.3972, MinusLogProbMetric: 29.3972, val_loss: 29.6339, val_MinusLogProbMetric: 29.6339

Epoch 355: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.3972 - MinusLogProbMetric: 29.3972 - val_loss: 29.6339 - val_MinusLogProbMetric: 29.6339 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 356/1000
2023-10-11 18:12:41.882 
Epoch 356/1000 
	 loss: 29.0729, MinusLogProbMetric: 29.0729, val_loss: 30.4647, val_MinusLogProbMetric: 30.4647

Epoch 356: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.0729 - MinusLogProbMetric: 29.0729 - val_loss: 30.4647 - val_MinusLogProbMetric: 30.4647 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 357/1000
2023-10-11 18:13:09.168 
Epoch 357/1000 
	 loss: 29.3273, MinusLogProbMetric: 29.3273, val_loss: 29.7793, val_MinusLogProbMetric: 29.7793

Epoch 357: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.3273 - MinusLogProbMetric: 29.3273 - val_loss: 29.7793 - val_MinusLogProbMetric: 29.7793 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 358/1000
2023-10-11 18:13:36.795 
Epoch 358/1000 
	 loss: 29.0882, MinusLogProbMetric: 29.0882, val_loss: 29.5363, val_MinusLogProbMetric: 29.5363

Epoch 358: val_loss did not improve from 29.27781
196/196 - 28s - loss: 29.0882 - MinusLogProbMetric: 29.0882 - val_loss: 29.5363 - val_MinusLogProbMetric: 29.5363 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 359/1000
2023-10-11 18:14:04.133 
Epoch 359/1000 
	 loss: 28.9994, MinusLogProbMetric: 28.9994, val_loss: 30.2700, val_MinusLogProbMetric: 30.2700

Epoch 359: val_loss did not improve from 29.27781
196/196 - 27s - loss: 28.9994 - MinusLogProbMetric: 28.9994 - val_loss: 30.2700 - val_MinusLogProbMetric: 30.2700 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 360/1000
2023-10-11 18:14:31.295 
Epoch 360/1000 
	 loss: 29.2361, MinusLogProbMetric: 29.2361, val_loss: 29.9306, val_MinusLogProbMetric: 29.9306

Epoch 360: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.2361 - MinusLogProbMetric: 29.2361 - val_loss: 29.9306 - val_MinusLogProbMetric: 29.9306 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 361/1000
2023-10-11 18:14:58.177 
Epoch 361/1000 
	 loss: 29.3090, MinusLogProbMetric: 29.3090, val_loss: 29.4150, val_MinusLogProbMetric: 29.4150

Epoch 361: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.3090 - MinusLogProbMetric: 29.3090 - val_loss: 29.4150 - val_MinusLogProbMetric: 29.4150 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 362/1000
2023-10-11 18:15:25.022 
Epoch 362/1000 
	 loss: 29.1357, MinusLogProbMetric: 29.1357, val_loss: 29.6820, val_MinusLogProbMetric: 29.6820

Epoch 362: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1357 - MinusLogProbMetric: 29.1357 - val_loss: 29.6820 - val_MinusLogProbMetric: 29.6820 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 363/1000
2023-10-11 18:15:52.117 
Epoch 363/1000 
	 loss: 29.0896, MinusLogProbMetric: 29.0896, val_loss: 29.8112, val_MinusLogProbMetric: 29.8112

Epoch 363: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.0896 - MinusLogProbMetric: 29.0896 - val_loss: 29.8112 - val_MinusLogProbMetric: 29.8112 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 364/1000
2023-10-11 18:16:18.990 
Epoch 364/1000 
	 loss: 29.1138, MinusLogProbMetric: 29.1138, val_loss: 29.8402, val_MinusLogProbMetric: 29.8402

Epoch 364: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1138 - MinusLogProbMetric: 29.1138 - val_loss: 29.8402 - val_MinusLogProbMetric: 29.8402 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 365/1000
2023-10-11 18:16:46.079 
Epoch 365/1000 
	 loss: 29.3100, MinusLogProbMetric: 29.3100, val_loss: 29.5362, val_MinusLogProbMetric: 29.5362

Epoch 365: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.3100 - MinusLogProbMetric: 29.3100 - val_loss: 29.5362 - val_MinusLogProbMetric: 29.5362 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 366/1000
2023-10-11 18:17:13.256 
Epoch 366/1000 
	 loss: 29.1008, MinusLogProbMetric: 29.1008, val_loss: 29.5321, val_MinusLogProbMetric: 29.5321

Epoch 366: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1008 - MinusLogProbMetric: 29.1008 - val_loss: 29.5321 - val_MinusLogProbMetric: 29.5321 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 367/1000
2023-10-11 18:17:40.610 
Epoch 367/1000 
	 loss: 29.1498, MinusLogProbMetric: 29.1498, val_loss: 29.4581, val_MinusLogProbMetric: 29.4581

Epoch 367: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1498 - MinusLogProbMetric: 29.1498 - val_loss: 29.4581 - val_MinusLogProbMetric: 29.4581 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 368/1000
2023-10-11 18:18:07.681 
Epoch 368/1000 
	 loss: 29.0535, MinusLogProbMetric: 29.0535, val_loss: 29.5233, val_MinusLogProbMetric: 29.5233

Epoch 368: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.0535 - MinusLogProbMetric: 29.0535 - val_loss: 29.5233 - val_MinusLogProbMetric: 29.5233 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 369/1000
2023-10-11 18:18:34.660 
Epoch 369/1000 
	 loss: 29.0498, MinusLogProbMetric: 29.0498, val_loss: 30.1111, val_MinusLogProbMetric: 30.1111

Epoch 369: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.0498 - MinusLogProbMetric: 29.0498 - val_loss: 30.1111 - val_MinusLogProbMetric: 30.1111 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 370/1000
2023-10-11 18:19:01.604 
Epoch 370/1000 
	 loss: 29.1126, MinusLogProbMetric: 29.1126, val_loss: 29.7928, val_MinusLogProbMetric: 29.7928

Epoch 370: val_loss did not improve from 29.27781
196/196 - 27s - loss: 29.1126 - MinusLogProbMetric: 29.1126 - val_loss: 29.7928 - val_MinusLogProbMetric: 29.7928 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 371/1000
2023-10-11 18:19:28.643 
Epoch 371/1000 
	 loss: 29.0172, MinusLogProbMetric: 29.0172, val_loss: 29.1126, val_MinusLogProbMetric: 29.1126

Epoch 371: val_loss improved from 29.27781 to 29.11258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 29.0172 - MinusLogProbMetric: 29.0172 - val_loss: 29.1126 - val_MinusLogProbMetric: 29.1126 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 372/1000
2023-10-11 18:19:56.334 
Epoch 372/1000 
	 loss: 29.1080, MinusLogProbMetric: 29.1080, val_loss: 29.6423, val_MinusLogProbMetric: 29.6423

Epoch 372: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.1080 - MinusLogProbMetric: 29.1080 - val_loss: 29.6423 - val_MinusLogProbMetric: 29.6423 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 373/1000
2023-10-11 18:20:24.060 
Epoch 373/1000 
	 loss: 28.9680, MinusLogProbMetric: 28.9680, val_loss: 29.2210, val_MinusLogProbMetric: 29.2210

Epoch 373: val_loss did not improve from 29.11258
196/196 - 28s - loss: 28.9680 - MinusLogProbMetric: 28.9680 - val_loss: 29.2210 - val_MinusLogProbMetric: 29.2210 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 374/1000
2023-10-11 18:20:52.188 
Epoch 374/1000 
	 loss: 29.1347, MinusLogProbMetric: 29.1347, val_loss: 29.4201, val_MinusLogProbMetric: 29.4201

Epoch 374: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.1347 - MinusLogProbMetric: 29.1347 - val_loss: 29.4201 - val_MinusLogProbMetric: 29.4201 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 375/1000
2023-10-11 18:21:20.367 
Epoch 375/1000 
	 loss: 29.0995, MinusLogProbMetric: 29.0995, val_loss: 30.7120, val_MinusLogProbMetric: 30.7120

Epoch 375: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0995 - MinusLogProbMetric: 29.0995 - val_loss: 30.7120 - val_MinusLogProbMetric: 30.7120 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 376/1000
2023-10-11 18:21:47.741 
Epoch 376/1000 
	 loss: 28.9170, MinusLogProbMetric: 28.9170, val_loss: 29.4704, val_MinusLogProbMetric: 29.4704

Epoch 376: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.9170 - MinusLogProbMetric: 28.9170 - val_loss: 29.4704 - val_MinusLogProbMetric: 29.4704 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 377/1000
2023-10-11 18:22:15.425 
Epoch 377/1000 
	 loss: 28.9457, MinusLogProbMetric: 28.9457, val_loss: 29.6135, val_MinusLogProbMetric: 29.6135

Epoch 377: val_loss did not improve from 29.11258
196/196 - 28s - loss: 28.9457 - MinusLogProbMetric: 28.9457 - val_loss: 29.6135 - val_MinusLogProbMetric: 29.6135 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 378/1000
2023-10-11 18:22:43.743 
Epoch 378/1000 
	 loss: 29.0535, MinusLogProbMetric: 29.0535, val_loss: 29.4613, val_MinusLogProbMetric: 29.4613

Epoch 378: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0535 - MinusLogProbMetric: 29.0535 - val_loss: 29.4613 - val_MinusLogProbMetric: 29.4613 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 379/1000
2023-10-11 18:23:11.778 
Epoch 379/1000 
	 loss: 29.0285, MinusLogProbMetric: 29.0285, val_loss: 29.5819, val_MinusLogProbMetric: 29.5819

Epoch 379: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0285 - MinusLogProbMetric: 29.0285 - val_loss: 29.5819 - val_MinusLogProbMetric: 29.5819 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 380/1000
2023-10-11 18:23:39.874 
Epoch 380/1000 
	 loss: 29.3769, MinusLogProbMetric: 29.3769, val_loss: 29.3617, val_MinusLogProbMetric: 29.3617

Epoch 380: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.3769 - MinusLogProbMetric: 29.3769 - val_loss: 29.3617 - val_MinusLogProbMetric: 29.3617 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 381/1000
2023-10-11 18:24:07.556 
Epoch 381/1000 
	 loss: 29.0956, MinusLogProbMetric: 29.0956, val_loss: 29.6494, val_MinusLogProbMetric: 29.6494

Epoch 381: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0956 - MinusLogProbMetric: 29.0956 - val_loss: 29.6494 - val_MinusLogProbMetric: 29.6494 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 382/1000
2023-10-11 18:24:35.970 
Epoch 382/1000 
	 loss: 29.2409, MinusLogProbMetric: 29.2409, val_loss: 29.9226, val_MinusLogProbMetric: 29.9226

Epoch 382: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.2409 - MinusLogProbMetric: 29.2409 - val_loss: 29.9226 - val_MinusLogProbMetric: 29.9226 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 383/1000
2023-10-11 18:25:04.138 
Epoch 383/1000 
	 loss: 29.0194, MinusLogProbMetric: 29.0194, val_loss: 29.8447, val_MinusLogProbMetric: 29.8447

Epoch 383: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0194 - MinusLogProbMetric: 29.0194 - val_loss: 29.8447 - val_MinusLogProbMetric: 29.8447 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 384/1000
2023-10-11 18:25:31.682 
Epoch 384/1000 
	 loss: 29.0123, MinusLogProbMetric: 29.0123, val_loss: 29.2737, val_MinusLogProbMetric: 29.2737

Epoch 384: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0123 - MinusLogProbMetric: 29.0123 - val_loss: 29.2737 - val_MinusLogProbMetric: 29.2737 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 385/1000
2023-10-11 18:25:59.012 
Epoch 385/1000 
	 loss: 29.2251, MinusLogProbMetric: 29.2251, val_loss: 29.5669, val_MinusLogProbMetric: 29.5669

Epoch 385: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.2251 - MinusLogProbMetric: 29.2251 - val_loss: 29.5669 - val_MinusLogProbMetric: 29.5669 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 386/1000
2023-10-11 18:26:26.147 
Epoch 386/1000 
	 loss: 29.0069, MinusLogProbMetric: 29.0069, val_loss: 30.5470, val_MinusLogProbMetric: 30.5470

Epoch 386: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.0069 - MinusLogProbMetric: 29.0069 - val_loss: 30.5470 - val_MinusLogProbMetric: 30.5470 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 387/1000
2023-10-11 18:26:53.210 
Epoch 387/1000 
	 loss: 28.9649, MinusLogProbMetric: 28.9649, val_loss: 29.8783, val_MinusLogProbMetric: 29.8783

Epoch 387: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.9649 - MinusLogProbMetric: 28.9649 - val_loss: 29.8783 - val_MinusLogProbMetric: 29.8783 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 388/1000
2023-10-11 18:27:20.416 
Epoch 388/1000 
	 loss: 28.9580, MinusLogProbMetric: 28.9580, val_loss: 29.1375, val_MinusLogProbMetric: 29.1375

Epoch 388: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.9580 - MinusLogProbMetric: 28.9580 - val_loss: 29.1375 - val_MinusLogProbMetric: 29.1375 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 389/1000
2023-10-11 18:27:47.775 
Epoch 389/1000 
	 loss: 29.0429, MinusLogProbMetric: 29.0429, val_loss: 29.5197, val_MinusLogProbMetric: 29.5197

Epoch 389: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.0429 - MinusLogProbMetric: 29.0429 - val_loss: 29.5197 - val_MinusLogProbMetric: 29.5197 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 390/1000
2023-10-11 18:28:14.655 
Epoch 390/1000 
	 loss: 28.9529, MinusLogProbMetric: 28.9529, val_loss: 29.2071, val_MinusLogProbMetric: 29.2071

Epoch 390: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.9529 - MinusLogProbMetric: 28.9529 - val_loss: 29.2071 - val_MinusLogProbMetric: 29.2071 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 391/1000
2023-10-11 18:28:41.516 
Epoch 391/1000 
	 loss: 28.9353, MinusLogProbMetric: 28.9353, val_loss: 29.8223, val_MinusLogProbMetric: 29.8223

Epoch 391: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.9353 - MinusLogProbMetric: 28.9353 - val_loss: 29.8223 - val_MinusLogProbMetric: 29.8223 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 392/1000
2023-10-11 18:29:09.177 
Epoch 392/1000 
	 loss: 28.9779, MinusLogProbMetric: 28.9779, val_loss: 30.5899, val_MinusLogProbMetric: 30.5899

Epoch 392: val_loss did not improve from 29.11258
196/196 - 28s - loss: 28.9779 - MinusLogProbMetric: 28.9779 - val_loss: 30.5899 - val_MinusLogProbMetric: 30.5899 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 393/1000
2023-10-11 18:29:36.729 
Epoch 393/1000 
	 loss: 29.0572, MinusLogProbMetric: 29.0572, val_loss: 29.1191, val_MinusLogProbMetric: 29.1191

Epoch 393: val_loss did not improve from 29.11258
196/196 - 28s - loss: 29.0572 - MinusLogProbMetric: 29.0572 - val_loss: 29.1191 - val_MinusLogProbMetric: 29.1191 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 394/1000
2023-10-11 18:30:03.993 
Epoch 394/1000 
	 loss: 28.8786, MinusLogProbMetric: 28.8786, val_loss: 29.6883, val_MinusLogProbMetric: 29.6883

Epoch 394: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.8786 - MinusLogProbMetric: 28.8786 - val_loss: 29.6883 - val_MinusLogProbMetric: 29.6883 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 395/1000
2023-10-11 18:30:31.336 
Epoch 395/1000 
	 loss: 28.9300, MinusLogProbMetric: 28.9300, val_loss: 29.6563, val_MinusLogProbMetric: 29.6563

Epoch 395: val_loss did not improve from 29.11258
196/196 - 27s - loss: 28.9300 - MinusLogProbMetric: 28.9300 - val_loss: 29.6563 - val_MinusLogProbMetric: 29.6563 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 396/1000
2023-10-11 18:30:58.570 
Epoch 396/1000 
	 loss: 29.0460, MinusLogProbMetric: 29.0460, val_loss: 29.8062, val_MinusLogProbMetric: 29.8062

Epoch 396: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.0460 - MinusLogProbMetric: 29.0460 - val_loss: 29.8062 - val_MinusLogProbMetric: 29.8062 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 397/1000
2023-10-11 18:31:25.348 
Epoch 397/1000 
	 loss: 29.0579, MinusLogProbMetric: 29.0579, val_loss: 29.7857, val_MinusLogProbMetric: 29.7857

Epoch 397: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.0579 - MinusLogProbMetric: 29.0579 - val_loss: 29.7857 - val_MinusLogProbMetric: 29.7857 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 398/1000
2023-10-11 18:31:52.155 
Epoch 398/1000 
	 loss: 29.0062, MinusLogProbMetric: 29.0062, val_loss: 29.4840, val_MinusLogProbMetric: 29.4840

Epoch 398: val_loss did not improve from 29.11258
196/196 - 27s - loss: 29.0062 - MinusLogProbMetric: 29.0062 - val_loss: 29.4840 - val_MinusLogProbMetric: 29.4840 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 399/1000
2023-10-11 18:32:19.048 
Epoch 399/1000 
	 loss: 28.9454, MinusLogProbMetric: 28.9454, val_loss: 29.0656, val_MinusLogProbMetric: 29.0656

Epoch 399: val_loss improved from 29.11258 to 29.06561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.9454 - MinusLogProbMetric: 28.9454 - val_loss: 29.0656 - val_MinusLogProbMetric: 29.0656 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 400/1000
2023-10-11 18:32:46.351 
Epoch 400/1000 
	 loss: 29.1603, MinusLogProbMetric: 29.1603, val_loss: 29.4497, val_MinusLogProbMetric: 29.4497

Epoch 400: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.1603 - MinusLogProbMetric: 29.1603 - val_loss: 29.4497 - val_MinusLogProbMetric: 29.4497 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 401/1000
2023-10-11 18:33:14.051 
Epoch 401/1000 
	 loss: 28.9810, MinusLogProbMetric: 28.9810, val_loss: 29.4754, val_MinusLogProbMetric: 29.4754

Epoch 401: val_loss did not improve from 29.06561
196/196 - 28s - loss: 28.9810 - MinusLogProbMetric: 28.9810 - val_loss: 29.4754 - val_MinusLogProbMetric: 29.4754 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 402/1000
2023-10-11 18:33:41.593 
Epoch 402/1000 
	 loss: 29.0602, MinusLogProbMetric: 29.0602, val_loss: 29.6298, val_MinusLogProbMetric: 29.6298

Epoch 402: val_loss did not improve from 29.06561
196/196 - 28s - loss: 29.0602 - MinusLogProbMetric: 29.0602 - val_loss: 29.6298 - val_MinusLogProbMetric: 29.6298 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 403/1000
2023-10-11 18:34:09.253 
Epoch 403/1000 
	 loss: 28.8807, MinusLogProbMetric: 28.8807, val_loss: 29.3353, val_MinusLogProbMetric: 29.3353

Epoch 403: val_loss did not improve from 29.06561
196/196 - 28s - loss: 28.8807 - MinusLogProbMetric: 28.8807 - val_loss: 29.3353 - val_MinusLogProbMetric: 29.3353 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 404/1000
2023-10-11 18:34:36.348 
Epoch 404/1000 
	 loss: 28.8926, MinusLogProbMetric: 28.8926, val_loss: 29.3783, val_MinusLogProbMetric: 29.3783

Epoch 404: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8926 - MinusLogProbMetric: 28.8926 - val_loss: 29.3783 - val_MinusLogProbMetric: 29.3783 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 405/1000
2023-10-11 18:35:03.905 
Epoch 405/1000 
	 loss: 28.9318, MinusLogProbMetric: 28.9318, val_loss: 29.7064, val_MinusLogProbMetric: 29.7064

Epoch 405: val_loss did not improve from 29.06561
196/196 - 28s - loss: 28.9318 - MinusLogProbMetric: 28.9318 - val_loss: 29.7064 - val_MinusLogProbMetric: 29.7064 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 406/1000
2023-10-11 18:35:30.969 
Epoch 406/1000 
	 loss: 28.9932, MinusLogProbMetric: 28.9932, val_loss: 30.4194, val_MinusLogProbMetric: 30.4194

Epoch 406: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9932 - MinusLogProbMetric: 28.9932 - val_loss: 30.4194 - val_MinusLogProbMetric: 30.4194 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 407/1000
2023-10-11 18:35:58.104 
Epoch 407/1000 
	 loss: 28.9508, MinusLogProbMetric: 28.9508, val_loss: 31.0807, val_MinusLogProbMetric: 31.0807

Epoch 407: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9508 - MinusLogProbMetric: 28.9508 - val_loss: 31.0807 - val_MinusLogProbMetric: 31.0807 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 408/1000
2023-10-11 18:36:24.968 
Epoch 408/1000 
	 loss: 28.9540, MinusLogProbMetric: 28.9540, val_loss: 31.1917, val_MinusLogProbMetric: 31.1917

Epoch 408: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9540 - MinusLogProbMetric: 28.9540 - val_loss: 31.1917 - val_MinusLogProbMetric: 31.1917 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 409/1000
2023-10-11 18:36:52.047 
Epoch 409/1000 
	 loss: 29.0109, MinusLogProbMetric: 29.0109, val_loss: 29.3990, val_MinusLogProbMetric: 29.3990

Epoch 409: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.0109 - MinusLogProbMetric: 29.0109 - val_loss: 29.3990 - val_MinusLogProbMetric: 29.3990 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 410/1000
2023-10-11 18:37:19.403 
Epoch 410/1000 
	 loss: 29.1125, MinusLogProbMetric: 29.1125, val_loss: 29.5830, val_MinusLogProbMetric: 29.5830

Epoch 410: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.1125 - MinusLogProbMetric: 29.1125 - val_loss: 29.5830 - val_MinusLogProbMetric: 29.5830 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 411/1000
2023-10-11 18:37:46.455 
Epoch 411/1000 
	 loss: 28.9346, MinusLogProbMetric: 28.9346, val_loss: 29.5466, val_MinusLogProbMetric: 29.5466

Epoch 411: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9346 - MinusLogProbMetric: 28.9346 - val_loss: 29.5466 - val_MinusLogProbMetric: 29.5466 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 412/1000
2023-10-11 18:38:13.130 
Epoch 412/1000 
	 loss: 28.8841, MinusLogProbMetric: 28.8841, val_loss: 31.7551, val_MinusLogProbMetric: 31.7551

Epoch 412: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8841 - MinusLogProbMetric: 28.8841 - val_loss: 31.7551 - val_MinusLogProbMetric: 31.7551 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 413/1000
2023-10-11 18:38:40.012 
Epoch 413/1000 
	 loss: 29.1917, MinusLogProbMetric: 29.1917, val_loss: 30.2841, val_MinusLogProbMetric: 30.2841

Epoch 413: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.1917 - MinusLogProbMetric: 29.1917 - val_loss: 30.2841 - val_MinusLogProbMetric: 30.2841 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 414/1000
2023-10-11 18:39:06.913 
Epoch 414/1000 
	 loss: 28.8548, MinusLogProbMetric: 28.8548, val_loss: 30.4104, val_MinusLogProbMetric: 30.4104

Epoch 414: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8548 - MinusLogProbMetric: 28.8548 - val_loss: 30.4104 - val_MinusLogProbMetric: 30.4104 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 415/1000
2023-10-11 18:39:33.801 
Epoch 415/1000 
	 loss: 28.9508, MinusLogProbMetric: 28.9508, val_loss: 29.7850, val_MinusLogProbMetric: 29.7850

Epoch 415: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9508 - MinusLogProbMetric: 28.9508 - val_loss: 29.7850 - val_MinusLogProbMetric: 29.7850 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 416/1000
2023-10-11 18:40:00.743 
Epoch 416/1000 
	 loss: 28.9772, MinusLogProbMetric: 28.9772, val_loss: 29.4284, val_MinusLogProbMetric: 29.4284

Epoch 416: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9772 - MinusLogProbMetric: 28.9772 - val_loss: 29.4284 - val_MinusLogProbMetric: 29.4284 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 417/1000
2023-10-11 18:40:27.715 
Epoch 417/1000 
	 loss: 29.0518, MinusLogProbMetric: 29.0518, val_loss: 29.8862, val_MinusLogProbMetric: 29.8862

Epoch 417: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.0518 - MinusLogProbMetric: 29.0518 - val_loss: 29.8862 - val_MinusLogProbMetric: 29.8862 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 418/1000
2023-10-11 18:40:54.544 
Epoch 418/1000 
	 loss: 29.0431, MinusLogProbMetric: 29.0431, val_loss: 29.3118, val_MinusLogProbMetric: 29.3118

Epoch 418: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.0431 - MinusLogProbMetric: 29.0431 - val_loss: 29.3118 - val_MinusLogProbMetric: 29.3118 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 419/1000
2023-10-11 18:41:21.348 
Epoch 419/1000 
	 loss: 28.8404, MinusLogProbMetric: 28.8404, val_loss: 29.9958, val_MinusLogProbMetric: 29.9958

Epoch 419: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8404 - MinusLogProbMetric: 28.8404 - val_loss: 29.9958 - val_MinusLogProbMetric: 29.9958 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 420/1000
2023-10-11 18:41:48.244 
Epoch 420/1000 
	 loss: 28.8349, MinusLogProbMetric: 28.8349, val_loss: 29.3932, val_MinusLogProbMetric: 29.3932

Epoch 420: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8349 - MinusLogProbMetric: 28.8349 - val_loss: 29.3932 - val_MinusLogProbMetric: 29.3932 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 421/1000
2023-10-11 18:42:15.326 
Epoch 421/1000 
	 loss: 28.8578, MinusLogProbMetric: 28.8578, val_loss: 30.1568, val_MinusLogProbMetric: 30.1568

Epoch 421: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8578 - MinusLogProbMetric: 28.8578 - val_loss: 30.1568 - val_MinusLogProbMetric: 30.1568 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 422/1000
2023-10-11 18:42:41.957 
Epoch 422/1000 
	 loss: 28.9535, MinusLogProbMetric: 28.9535, val_loss: 29.6876, val_MinusLogProbMetric: 29.6876

Epoch 422: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9535 - MinusLogProbMetric: 28.9535 - val_loss: 29.6876 - val_MinusLogProbMetric: 29.6876 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 423/1000
2023-10-11 18:43:09.326 
Epoch 423/1000 
	 loss: 28.8406, MinusLogProbMetric: 28.8406, val_loss: 29.8467, val_MinusLogProbMetric: 29.8467

Epoch 423: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8406 - MinusLogProbMetric: 28.8406 - val_loss: 29.8467 - val_MinusLogProbMetric: 29.8467 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 424/1000
2023-10-11 18:43:36.002 
Epoch 424/1000 
	 loss: 28.8802, MinusLogProbMetric: 28.8802, val_loss: 29.5389, val_MinusLogProbMetric: 29.5389

Epoch 424: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8802 - MinusLogProbMetric: 28.8802 - val_loss: 29.5389 - val_MinusLogProbMetric: 29.5389 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 425/1000
2023-10-11 18:44:02.746 
Epoch 425/1000 
	 loss: 29.0486, MinusLogProbMetric: 29.0486, val_loss: 30.2222, val_MinusLogProbMetric: 30.2222

Epoch 425: val_loss did not improve from 29.06561
196/196 - 27s - loss: 29.0486 - MinusLogProbMetric: 29.0486 - val_loss: 30.2222 - val_MinusLogProbMetric: 30.2222 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 426/1000
2023-10-11 18:44:29.866 
Epoch 426/1000 
	 loss: 28.8245, MinusLogProbMetric: 28.8245, val_loss: 29.4494, val_MinusLogProbMetric: 29.4494

Epoch 426: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8245 - MinusLogProbMetric: 28.8245 - val_loss: 29.4494 - val_MinusLogProbMetric: 29.4494 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 427/1000
2023-10-11 18:44:56.876 
Epoch 427/1000 
	 loss: 28.9260, MinusLogProbMetric: 28.9260, val_loss: 29.7243, val_MinusLogProbMetric: 29.7243

Epoch 427: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9260 - MinusLogProbMetric: 28.9260 - val_loss: 29.7243 - val_MinusLogProbMetric: 29.7243 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 428/1000
2023-10-11 18:45:24.088 
Epoch 428/1000 
	 loss: 28.9608, MinusLogProbMetric: 28.9608, val_loss: 29.3189, val_MinusLogProbMetric: 29.3189

Epoch 428: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9608 - MinusLogProbMetric: 28.9608 - val_loss: 29.3189 - val_MinusLogProbMetric: 29.3189 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 429/1000
2023-10-11 18:45:51.215 
Epoch 429/1000 
	 loss: 28.9471, MinusLogProbMetric: 28.9471, val_loss: 29.5290, val_MinusLogProbMetric: 29.5290

Epoch 429: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.9471 - MinusLogProbMetric: 28.9471 - val_loss: 29.5290 - val_MinusLogProbMetric: 29.5290 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 430/1000
2023-10-11 18:46:17.865 
Epoch 430/1000 
	 loss: 28.8299, MinusLogProbMetric: 28.8299, val_loss: 29.5441, val_MinusLogProbMetric: 29.5441

Epoch 430: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8299 - MinusLogProbMetric: 28.8299 - val_loss: 29.5441 - val_MinusLogProbMetric: 29.5441 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 431/1000
2023-10-11 18:46:44.777 
Epoch 431/1000 
	 loss: 28.8481, MinusLogProbMetric: 28.8481, val_loss: 29.3105, val_MinusLogProbMetric: 29.3105

Epoch 431: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8481 - MinusLogProbMetric: 28.8481 - val_loss: 29.3105 - val_MinusLogProbMetric: 29.3105 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 432/1000
2023-10-11 18:47:11.467 
Epoch 432/1000 
	 loss: 28.8661, MinusLogProbMetric: 28.8661, val_loss: 29.5349, val_MinusLogProbMetric: 29.5349

Epoch 432: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.8661 - MinusLogProbMetric: 28.8661 - val_loss: 29.5349 - val_MinusLogProbMetric: 29.5349 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 433/1000
2023-10-11 18:47:38.374 
Epoch 433/1000 
	 loss: 28.7967, MinusLogProbMetric: 28.7967, val_loss: 29.3438, val_MinusLogProbMetric: 29.3438

Epoch 433: val_loss did not improve from 29.06561
196/196 - 27s - loss: 28.7967 - MinusLogProbMetric: 28.7967 - val_loss: 29.3438 - val_MinusLogProbMetric: 29.3438 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 434/1000
2023-10-11 18:48:05.387 
Epoch 434/1000 
	 loss: 28.9443, MinusLogProbMetric: 28.9443, val_loss: 29.0214, val_MinusLogProbMetric: 29.0214

Epoch 434: val_loss improved from 29.06561 to 29.02143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 28.9443 - MinusLogProbMetric: 28.9443 - val_loss: 29.0214 - val_MinusLogProbMetric: 29.0214 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 435/1000
2023-10-11 18:48:32.989 
Epoch 435/1000 
	 loss: 29.0008, MinusLogProbMetric: 29.0008, val_loss: 29.9537, val_MinusLogProbMetric: 29.9537

Epoch 435: val_loss did not improve from 29.02143
196/196 - 27s - loss: 29.0008 - MinusLogProbMetric: 29.0008 - val_loss: 29.9537 - val_MinusLogProbMetric: 29.9537 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 436/1000
2023-10-11 18:48:59.854 
Epoch 436/1000 
	 loss: 28.9212, MinusLogProbMetric: 28.9212, val_loss: 29.6648, val_MinusLogProbMetric: 29.6648

Epoch 436: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.9212 - MinusLogProbMetric: 28.9212 - val_loss: 29.6648 - val_MinusLogProbMetric: 29.6648 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 437/1000
2023-10-11 18:49:26.848 
Epoch 437/1000 
	 loss: 28.9229, MinusLogProbMetric: 28.9229, val_loss: 29.4817, val_MinusLogProbMetric: 29.4817

Epoch 437: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.9229 - MinusLogProbMetric: 28.9229 - val_loss: 29.4817 - val_MinusLogProbMetric: 29.4817 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 438/1000
2023-10-11 18:49:53.740 
Epoch 438/1000 
	 loss: 28.8918, MinusLogProbMetric: 28.8918, val_loss: 29.5397, val_MinusLogProbMetric: 29.5397

Epoch 438: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8918 - MinusLogProbMetric: 28.8918 - val_loss: 29.5397 - val_MinusLogProbMetric: 29.5397 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 439/1000
2023-10-11 18:50:20.245 
Epoch 439/1000 
	 loss: 28.8316, MinusLogProbMetric: 28.8316, val_loss: 29.1473, val_MinusLogProbMetric: 29.1473

Epoch 439: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8316 - MinusLogProbMetric: 28.8316 - val_loss: 29.1473 - val_MinusLogProbMetric: 29.1473 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 440/1000
2023-10-11 18:50:47.460 
Epoch 440/1000 
	 loss: 28.7487, MinusLogProbMetric: 28.7487, val_loss: 29.1759, val_MinusLogProbMetric: 29.1759

Epoch 440: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7487 - MinusLogProbMetric: 28.7487 - val_loss: 29.1759 - val_MinusLogProbMetric: 29.1759 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 441/1000
2023-10-11 18:51:13.977 
Epoch 441/1000 
	 loss: 28.8890, MinusLogProbMetric: 28.8890, val_loss: 29.4469, val_MinusLogProbMetric: 29.4469

Epoch 441: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8890 - MinusLogProbMetric: 28.8890 - val_loss: 29.4469 - val_MinusLogProbMetric: 29.4469 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 442/1000
2023-10-11 18:51:40.633 
Epoch 442/1000 
	 loss: 28.8685, MinusLogProbMetric: 28.8685, val_loss: 32.8293, val_MinusLogProbMetric: 32.8293

Epoch 442: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8685 - MinusLogProbMetric: 28.8685 - val_loss: 32.8293 - val_MinusLogProbMetric: 32.8293 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 443/1000
2023-10-11 18:52:07.563 
Epoch 443/1000 
	 loss: 28.9675, MinusLogProbMetric: 28.9675, val_loss: 29.1709, val_MinusLogProbMetric: 29.1709

Epoch 443: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.9675 - MinusLogProbMetric: 28.9675 - val_loss: 29.1709 - val_MinusLogProbMetric: 29.1709 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 444/1000
2023-10-11 18:52:34.136 
Epoch 444/1000 
	 loss: 28.9712, MinusLogProbMetric: 28.9712, val_loss: 29.1435, val_MinusLogProbMetric: 29.1435

Epoch 444: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.9712 - MinusLogProbMetric: 28.9712 - val_loss: 29.1435 - val_MinusLogProbMetric: 29.1435 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 445/1000
2023-10-11 18:53:01.332 
Epoch 445/1000 
	 loss: 28.7571, MinusLogProbMetric: 28.7571, val_loss: 29.3563, val_MinusLogProbMetric: 29.3563

Epoch 445: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7571 - MinusLogProbMetric: 28.7571 - val_loss: 29.3563 - val_MinusLogProbMetric: 29.3563 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 446/1000
2023-10-11 18:53:28.289 
Epoch 446/1000 
	 loss: 28.9118, MinusLogProbMetric: 28.9118, val_loss: 29.9689, val_MinusLogProbMetric: 29.9689

Epoch 446: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.9118 - MinusLogProbMetric: 28.9118 - val_loss: 29.9689 - val_MinusLogProbMetric: 29.9689 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 447/1000
2023-10-11 18:53:55.714 
Epoch 447/1000 
	 loss: 28.8030, MinusLogProbMetric: 28.8030, val_loss: 29.1354, val_MinusLogProbMetric: 29.1354

Epoch 447: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8030 - MinusLogProbMetric: 28.8030 - val_loss: 29.1354 - val_MinusLogProbMetric: 29.1354 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 448/1000
2023-10-11 18:54:22.488 
Epoch 448/1000 
	 loss: 28.7771, MinusLogProbMetric: 28.7771, val_loss: 30.3842, val_MinusLogProbMetric: 30.3842

Epoch 448: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7771 - MinusLogProbMetric: 28.7771 - val_loss: 30.3842 - val_MinusLogProbMetric: 30.3842 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 449/1000
2023-10-11 18:54:49.301 
Epoch 449/1000 
	 loss: 28.9907, MinusLogProbMetric: 28.9907, val_loss: 29.6124, val_MinusLogProbMetric: 29.6124

Epoch 449: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.9907 - MinusLogProbMetric: 28.9907 - val_loss: 29.6124 - val_MinusLogProbMetric: 29.6124 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 450/1000
2023-10-11 18:55:16.639 
Epoch 450/1000 
	 loss: 28.7707, MinusLogProbMetric: 28.7707, val_loss: 29.1563, val_MinusLogProbMetric: 29.1563

Epoch 450: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7707 - MinusLogProbMetric: 28.7707 - val_loss: 29.1563 - val_MinusLogProbMetric: 29.1563 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 451/1000
2023-10-11 18:55:43.863 
Epoch 451/1000 
	 loss: 28.7919, MinusLogProbMetric: 28.7919, val_loss: 29.3405, val_MinusLogProbMetric: 29.3405

Epoch 451: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7919 - MinusLogProbMetric: 28.7919 - val_loss: 29.3405 - val_MinusLogProbMetric: 29.3405 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 452/1000
2023-10-11 18:56:11.014 
Epoch 452/1000 
	 loss: 28.8230, MinusLogProbMetric: 28.8230, val_loss: 29.2646, val_MinusLogProbMetric: 29.2646

Epoch 452: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8230 - MinusLogProbMetric: 28.8230 - val_loss: 29.2646 - val_MinusLogProbMetric: 29.2646 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 453/1000
2023-10-11 18:56:38.029 
Epoch 453/1000 
	 loss: 28.8835, MinusLogProbMetric: 28.8835, val_loss: 29.3916, val_MinusLogProbMetric: 29.3916

Epoch 453: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.8835 - MinusLogProbMetric: 28.8835 - val_loss: 29.3916 - val_MinusLogProbMetric: 29.3916 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 454/1000
2023-10-11 18:57:04.829 
Epoch 454/1000 
	 loss: 28.7701, MinusLogProbMetric: 28.7701, val_loss: 29.5994, val_MinusLogProbMetric: 29.5994

Epoch 454: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7701 - MinusLogProbMetric: 28.7701 - val_loss: 29.5994 - val_MinusLogProbMetric: 29.5994 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 455/1000
2023-10-11 18:57:31.667 
Epoch 455/1000 
	 loss: 29.0839, MinusLogProbMetric: 29.0839, val_loss: 29.5959, val_MinusLogProbMetric: 29.5959

Epoch 455: val_loss did not improve from 29.02143
196/196 - 27s - loss: 29.0839 - MinusLogProbMetric: 29.0839 - val_loss: 29.5959 - val_MinusLogProbMetric: 29.5959 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 456/1000
2023-10-11 18:57:58.429 
Epoch 456/1000 
	 loss: 28.7936, MinusLogProbMetric: 28.7936, val_loss: 29.4041, val_MinusLogProbMetric: 29.4041

Epoch 456: val_loss did not improve from 29.02143
196/196 - 27s - loss: 28.7936 - MinusLogProbMetric: 28.7936 - val_loss: 29.4041 - val_MinusLogProbMetric: 29.4041 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 457/1000
2023-10-11 18:58:26.095 
Epoch 457/1000 
	 loss: 28.7327, MinusLogProbMetric: 28.7327, val_loss: 29.5297, val_MinusLogProbMetric: 29.5297

Epoch 457: val_loss did not improve from 29.02143
196/196 - 28s - loss: 28.7327 - MinusLogProbMetric: 28.7327 - val_loss: 29.5297 - val_MinusLogProbMetric: 29.5297 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 458/1000
2023-10-11 18:58:53.100 
Epoch 458/1000 
	 loss: 28.8111, MinusLogProbMetric: 28.8111, val_loss: 28.9829, val_MinusLogProbMetric: 28.9829

Epoch 458: val_loss improved from 29.02143 to 28.98290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.8111 - MinusLogProbMetric: 28.8111 - val_loss: 28.9829 - val_MinusLogProbMetric: 28.9829 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 459/1000
2023-10-11 18:59:20.588 
Epoch 459/1000 
	 loss: 28.8230, MinusLogProbMetric: 28.8230, val_loss: 29.1100, val_MinusLogProbMetric: 29.1100

Epoch 459: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8230 - MinusLogProbMetric: 28.8230 - val_loss: 29.1100 - val_MinusLogProbMetric: 29.1100 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 460/1000
2023-10-11 18:59:47.893 
Epoch 460/1000 
	 loss: 28.7816, MinusLogProbMetric: 28.7816, val_loss: 29.8033, val_MinusLogProbMetric: 29.8033

Epoch 460: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7816 - MinusLogProbMetric: 28.7816 - val_loss: 29.8033 - val_MinusLogProbMetric: 29.8033 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 461/1000
2023-10-11 19:00:14.871 
Epoch 461/1000 
	 loss: 28.8300, MinusLogProbMetric: 28.8300, val_loss: 31.7186, val_MinusLogProbMetric: 31.7186

Epoch 461: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8300 - MinusLogProbMetric: 28.8300 - val_loss: 31.7186 - val_MinusLogProbMetric: 31.7186 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 462/1000
2023-10-11 19:00:42.200 
Epoch 462/1000 
	 loss: 28.8140, MinusLogProbMetric: 28.8140, val_loss: 29.4535, val_MinusLogProbMetric: 29.4535

Epoch 462: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8140 - MinusLogProbMetric: 28.8140 - val_loss: 29.4535 - val_MinusLogProbMetric: 29.4535 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 463/1000
2023-10-11 19:01:08.978 
Epoch 463/1000 
	 loss: 28.6635, MinusLogProbMetric: 28.6635, val_loss: 29.3442, val_MinusLogProbMetric: 29.3442

Epoch 463: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6635 - MinusLogProbMetric: 28.6635 - val_loss: 29.3442 - val_MinusLogProbMetric: 29.3442 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 464/1000
2023-10-11 19:01:36.100 
Epoch 464/1000 
	 loss: 28.7558, MinusLogProbMetric: 28.7558, val_loss: 29.5260, val_MinusLogProbMetric: 29.5260

Epoch 464: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7558 - MinusLogProbMetric: 28.7558 - val_loss: 29.5260 - val_MinusLogProbMetric: 29.5260 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 465/1000
2023-10-11 19:02:03.605 
Epoch 465/1000 
	 loss: 28.8250, MinusLogProbMetric: 28.8250, val_loss: 29.0249, val_MinusLogProbMetric: 29.0249

Epoch 465: val_loss did not improve from 28.98290
196/196 - 28s - loss: 28.8250 - MinusLogProbMetric: 28.8250 - val_loss: 29.0249 - val_MinusLogProbMetric: 29.0249 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 466/1000
2023-10-11 19:02:30.506 
Epoch 466/1000 
	 loss: 28.7130, MinusLogProbMetric: 28.7130, val_loss: 29.3346, val_MinusLogProbMetric: 29.3346

Epoch 466: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7130 - MinusLogProbMetric: 28.7130 - val_loss: 29.3346 - val_MinusLogProbMetric: 29.3346 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 467/1000
2023-10-11 19:02:57.362 
Epoch 467/1000 
	 loss: 28.6749, MinusLogProbMetric: 28.6749, val_loss: 29.3046, val_MinusLogProbMetric: 29.3046

Epoch 467: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6749 - MinusLogProbMetric: 28.6749 - val_loss: 29.3046 - val_MinusLogProbMetric: 29.3046 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 468/1000
2023-10-11 19:03:24.262 
Epoch 468/1000 
	 loss: 28.8860, MinusLogProbMetric: 28.8860, val_loss: 29.7043, val_MinusLogProbMetric: 29.7043

Epoch 468: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8860 - MinusLogProbMetric: 28.8860 - val_loss: 29.7043 - val_MinusLogProbMetric: 29.7043 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 469/1000
2023-10-11 19:03:51.033 
Epoch 469/1000 
	 loss: 28.6223, MinusLogProbMetric: 28.6223, val_loss: 29.5561, val_MinusLogProbMetric: 29.5561

Epoch 469: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6223 - MinusLogProbMetric: 28.6223 - val_loss: 29.5561 - val_MinusLogProbMetric: 29.5561 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 470/1000
2023-10-11 19:04:18.192 
Epoch 470/1000 
	 loss: 28.8614, MinusLogProbMetric: 28.8614, val_loss: 29.8648, val_MinusLogProbMetric: 29.8648

Epoch 470: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8614 - MinusLogProbMetric: 28.8614 - val_loss: 29.8648 - val_MinusLogProbMetric: 29.8648 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 471/1000
2023-10-11 19:04:44.954 
Epoch 471/1000 
	 loss: 28.7944, MinusLogProbMetric: 28.7944, val_loss: 29.7783, val_MinusLogProbMetric: 29.7783

Epoch 471: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7944 - MinusLogProbMetric: 28.7944 - val_loss: 29.7783 - val_MinusLogProbMetric: 29.7783 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 472/1000
2023-10-11 19:05:12.165 
Epoch 472/1000 
	 loss: 28.6841, MinusLogProbMetric: 28.6841, val_loss: 29.2653, val_MinusLogProbMetric: 29.2653

Epoch 472: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6841 - MinusLogProbMetric: 28.6841 - val_loss: 29.2653 - val_MinusLogProbMetric: 29.2653 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 473/1000
2023-10-11 19:05:38.957 
Epoch 473/1000 
	 loss: 28.6897, MinusLogProbMetric: 28.6897, val_loss: 29.9019, val_MinusLogProbMetric: 29.9019

Epoch 473: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6897 - MinusLogProbMetric: 28.6897 - val_loss: 29.9019 - val_MinusLogProbMetric: 29.9019 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 474/1000
2023-10-11 19:06:05.969 
Epoch 474/1000 
	 loss: 28.7311, MinusLogProbMetric: 28.7311, val_loss: 30.6734, val_MinusLogProbMetric: 30.6734

Epoch 474: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7311 - MinusLogProbMetric: 28.7311 - val_loss: 30.6734 - val_MinusLogProbMetric: 30.6734 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 475/1000
2023-10-11 19:06:33.084 
Epoch 475/1000 
	 loss: 28.6855, MinusLogProbMetric: 28.6855, val_loss: 29.2168, val_MinusLogProbMetric: 29.2168

Epoch 475: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6855 - MinusLogProbMetric: 28.6855 - val_loss: 29.2168 - val_MinusLogProbMetric: 29.2168 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 476/1000
2023-10-11 19:07:00.121 
Epoch 476/1000 
	 loss: 28.7065, MinusLogProbMetric: 28.7065, val_loss: 29.3314, val_MinusLogProbMetric: 29.3314

Epoch 476: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7065 - MinusLogProbMetric: 28.7065 - val_loss: 29.3314 - val_MinusLogProbMetric: 29.3314 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 477/1000
2023-10-11 19:07:26.810 
Epoch 477/1000 
	 loss: 28.6070, MinusLogProbMetric: 28.6070, val_loss: 29.4143, val_MinusLogProbMetric: 29.4143

Epoch 477: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6070 - MinusLogProbMetric: 28.6070 - val_loss: 29.4143 - val_MinusLogProbMetric: 29.4143 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 478/1000
2023-10-11 19:07:53.671 
Epoch 478/1000 
	 loss: 28.7438, MinusLogProbMetric: 28.7438, val_loss: 29.2639, val_MinusLogProbMetric: 29.2639

Epoch 478: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7438 - MinusLogProbMetric: 28.7438 - val_loss: 29.2639 - val_MinusLogProbMetric: 29.2639 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 479/1000
2023-10-11 19:08:20.695 
Epoch 479/1000 
	 loss: 28.7388, MinusLogProbMetric: 28.7388, val_loss: 29.6944, val_MinusLogProbMetric: 29.6944

Epoch 479: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7388 - MinusLogProbMetric: 28.7388 - val_loss: 29.6944 - val_MinusLogProbMetric: 29.6944 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 480/1000
2023-10-11 19:08:47.524 
Epoch 480/1000 
	 loss: 28.7571, MinusLogProbMetric: 28.7571, val_loss: 30.1203, val_MinusLogProbMetric: 30.1203

Epoch 480: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7571 - MinusLogProbMetric: 28.7571 - val_loss: 30.1203 - val_MinusLogProbMetric: 30.1203 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 481/1000
2023-10-11 19:09:14.503 
Epoch 481/1000 
	 loss: 28.6221, MinusLogProbMetric: 28.6221, val_loss: 29.0848, val_MinusLogProbMetric: 29.0848

Epoch 481: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6221 - MinusLogProbMetric: 28.6221 - val_loss: 29.0848 - val_MinusLogProbMetric: 29.0848 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 482/1000
2023-10-11 19:09:41.215 
Epoch 482/1000 
	 loss: 28.6318, MinusLogProbMetric: 28.6318, val_loss: 29.9943, val_MinusLogProbMetric: 29.9943

Epoch 482: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6318 - MinusLogProbMetric: 28.6318 - val_loss: 29.9943 - val_MinusLogProbMetric: 29.9943 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 483/1000
2023-10-11 19:10:08.144 
Epoch 483/1000 
	 loss: 28.8971, MinusLogProbMetric: 28.8971, val_loss: 29.0413, val_MinusLogProbMetric: 29.0413

Epoch 483: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8971 - MinusLogProbMetric: 28.8971 - val_loss: 29.0413 - val_MinusLogProbMetric: 29.0413 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 484/1000
2023-10-11 19:10:34.953 
Epoch 484/1000 
	 loss: 28.6353, MinusLogProbMetric: 28.6353, val_loss: 29.2259, val_MinusLogProbMetric: 29.2259

Epoch 484: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6353 - MinusLogProbMetric: 28.6353 - val_loss: 29.2259 - val_MinusLogProbMetric: 29.2259 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 485/1000
2023-10-11 19:11:01.780 
Epoch 485/1000 
	 loss: 28.6026, MinusLogProbMetric: 28.6026, val_loss: 29.1967, val_MinusLogProbMetric: 29.1967

Epoch 485: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6026 - MinusLogProbMetric: 28.6026 - val_loss: 29.1967 - val_MinusLogProbMetric: 29.1967 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 486/1000
2023-10-11 19:11:28.830 
Epoch 486/1000 
	 loss: 28.8005, MinusLogProbMetric: 28.8005, val_loss: 29.8052, val_MinusLogProbMetric: 29.8052

Epoch 486: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.8005 - MinusLogProbMetric: 28.8005 - val_loss: 29.8052 - val_MinusLogProbMetric: 29.8052 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 487/1000
2023-10-11 19:11:55.408 
Epoch 487/1000 
	 loss: 28.7478, MinusLogProbMetric: 28.7478, val_loss: 29.4369, val_MinusLogProbMetric: 29.4369

Epoch 487: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7478 - MinusLogProbMetric: 28.7478 - val_loss: 29.4369 - val_MinusLogProbMetric: 29.4369 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 488/1000
2023-10-11 19:12:22.644 
Epoch 488/1000 
	 loss: 28.7514, MinusLogProbMetric: 28.7514, val_loss: 29.7426, val_MinusLogProbMetric: 29.7426

Epoch 488: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7514 - MinusLogProbMetric: 28.7514 - val_loss: 29.7426 - val_MinusLogProbMetric: 29.7426 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 489/1000
2023-10-11 19:12:49.690 
Epoch 489/1000 
	 loss: 28.6605, MinusLogProbMetric: 28.6605, val_loss: 29.4844, val_MinusLogProbMetric: 29.4844

Epoch 489: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6605 - MinusLogProbMetric: 28.6605 - val_loss: 29.4844 - val_MinusLogProbMetric: 29.4844 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 490/1000
2023-10-11 19:13:16.652 
Epoch 490/1000 
	 loss: 28.7513, MinusLogProbMetric: 28.7513, val_loss: 30.0583, val_MinusLogProbMetric: 30.0583

Epoch 490: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7513 - MinusLogProbMetric: 28.7513 - val_loss: 30.0583 - val_MinusLogProbMetric: 30.0583 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 491/1000
2023-10-11 19:13:43.460 
Epoch 491/1000 
	 loss: 28.6944, MinusLogProbMetric: 28.6944, val_loss: 29.3893, val_MinusLogProbMetric: 29.3893

Epoch 491: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6944 - MinusLogProbMetric: 28.6944 - val_loss: 29.3893 - val_MinusLogProbMetric: 29.3893 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 492/1000
2023-10-11 19:14:10.844 
Epoch 492/1000 
	 loss: 28.7148, MinusLogProbMetric: 28.7148, val_loss: 29.3624, val_MinusLogProbMetric: 29.3624

Epoch 492: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7148 - MinusLogProbMetric: 28.7148 - val_loss: 29.3624 - val_MinusLogProbMetric: 29.3624 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 493/1000
2023-10-11 19:14:37.684 
Epoch 493/1000 
	 loss: 28.6669, MinusLogProbMetric: 28.6669, val_loss: 29.3111, val_MinusLogProbMetric: 29.3111

Epoch 493: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6669 - MinusLogProbMetric: 28.6669 - val_loss: 29.3111 - val_MinusLogProbMetric: 29.3111 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 494/1000
2023-10-11 19:15:04.434 
Epoch 494/1000 
	 loss: 28.5769, MinusLogProbMetric: 28.5769, val_loss: 29.4513, val_MinusLogProbMetric: 29.4513

Epoch 494: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.5769 - MinusLogProbMetric: 28.5769 - val_loss: 29.4513 - val_MinusLogProbMetric: 29.4513 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 495/1000
2023-10-11 19:15:31.467 
Epoch 495/1000 
	 loss: 28.7041, MinusLogProbMetric: 28.7041, val_loss: 29.0404, val_MinusLogProbMetric: 29.0404

Epoch 495: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7041 - MinusLogProbMetric: 28.7041 - val_loss: 29.0404 - val_MinusLogProbMetric: 29.0404 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 496/1000
2023-10-11 19:15:58.200 
Epoch 496/1000 
	 loss: 28.7037, MinusLogProbMetric: 28.7037, val_loss: 29.3052, val_MinusLogProbMetric: 29.3052

Epoch 496: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7037 - MinusLogProbMetric: 28.7037 - val_loss: 29.3052 - val_MinusLogProbMetric: 29.3052 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 497/1000
2023-10-11 19:16:24.882 
Epoch 497/1000 
	 loss: 28.6480, MinusLogProbMetric: 28.6480, val_loss: 29.5867, val_MinusLogProbMetric: 29.5867

Epoch 497: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.6480 - MinusLogProbMetric: 28.6480 - val_loss: 29.5867 - val_MinusLogProbMetric: 29.5867 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 498/1000
2023-10-11 19:16:51.986 
Epoch 498/1000 
	 loss: 28.7052, MinusLogProbMetric: 28.7052, val_loss: 29.1002, val_MinusLogProbMetric: 29.1002

Epoch 498: val_loss did not improve from 28.98290
196/196 - 27s - loss: 28.7052 - MinusLogProbMetric: 28.7052 - val_loss: 29.1002 - val_MinusLogProbMetric: 29.1002 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 499/1000
2023-10-11 19:17:19.280 
Epoch 499/1000 
	 loss: 28.6539, MinusLogProbMetric: 28.6539, val_loss: 28.9738, val_MinusLogProbMetric: 28.9738

Epoch 499: val_loss improved from 28.98290 to 28.97382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 28s - loss: 28.6539 - MinusLogProbMetric: 28.6539 - val_loss: 28.9738 - val_MinusLogProbMetric: 28.9738 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 500/1000
2023-10-11 19:17:46.677 
Epoch 500/1000 
	 loss: 28.7930, MinusLogProbMetric: 28.7930, val_loss: 29.0406, val_MinusLogProbMetric: 29.0406

Epoch 500: val_loss did not improve from 28.97382
196/196 - 27s - loss: 28.7930 - MinusLogProbMetric: 28.7930 - val_loss: 29.0406 - val_MinusLogProbMetric: 29.0406 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 501/1000
2023-10-11 19:18:13.877 
Epoch 501/1000 
	 loss: 28.7327, MinusLogProbMetric: 28.7327, val_loss: 29.4181, val_MinusLogProbMetric: 29.4181

Epoch 501: val_loss did not improve from 28.97382
196/196 - 27s - loss: 28.7327 - MinusLogProbMetric: 28.7327 - val_loss: 29.4181 - val_MinusLogProbMetric: 29.4181 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 502/1000
2023-10-11 19:18:41.125 
Epoch 502/1000 
	 loss: 28.6152, MinusLogProbMetric: 28.6152, val_loss: 29.2877, val_MinusLogProbMetric: 29.2877

Epoch 502: val_loss did not improve from 28.97382
196/196 - 27s - loss: 28.6152 - MinusLogProbMetric: 28.6152 - val_loss: 29.2877 - val_MinusLogProbMetric: 29.2877 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 503/1000
2023-10-11 19:19:07.879 
Epoch 503/1000 
	 loss: 28.6529, MinusLogProbMetric: 28.6529, val_loss: 28.9328, val_MinusLogProbMetric: 28.9328

Epoch 503: val_loss improved from 28.97382 to 28.93280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.6529 - MinusLogProbMetric: 28.6529 - val_loss: 28.9328 - val_MinusLogProbMetric: 28.9328 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 504/1000
2023-10-11 19:19:35.265 
Epoch 504/1000 
	 loss: 28.6423, MinusLogProbMetric: 28.6423, val_loss: 29.3929, val_MinusLogProbMetric: 29.3929

Epoch 504: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6423 - MinusLogProbMetric: 28.6423 - val_loss: 29.3929 - val_MinusLogProbMetric: 29.3929 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 505/1000
2023-10-11 19:20:01.854 
Epoch 505/1000 
	 loss: 28.6050, MinusLogProbMetric: 28.6050, val_loss: 29.3981, val_MinusLogProbMetric: 29.3981

Epoch 505: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6050 - MinusLogProbMetric: 28.6050 - val_loss: 29.3981 - val_MinusLogProbMetric: 29.3981 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 506/1000
2023-10-11 19:20:28.864 
Epoch 506/1000 
	 loss: 28.7465, MinusLogProbMetric: 28.7465, val_loss: 29.5536, val_MinusLogProbMetric: 29.5536

Epoch 506: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.7465 - MinusLogProbMetric: 28.7465 - val_loss: 29.5536 - val_MinusLogProbMetric: 29.5536 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 507/1000
2023-10-11 19:20:55.834 
Epoch 507/1000 
	 loss: 28.7047, MinusLogProbMetric: 28.7047, val_loss: 29.1474, val_MinusLogProbMetric: 29.1474

Epoch 507: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.7047 - MinusLogProbMetric: 28.7047 - val_loss: 29.1474 - val_MinusLogProbMetric: 29.1474 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 508/1000
2023-10-11 19:21:22.849 
Epoch 508/1000 
	 loss: 28.6868, MinusLogProbMetric: 28.6868, val_loss: 29.7280, val_MinusLogProbMetric: 29.7280

Epoch 508: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6868 - MinusLogProbMetric: 28.6868 - val_loss: 29.7280 - val_MinusLogProbMetric: 29.7280 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 509/1000
2023-10-11 19:21:49.703 
Epoch 509/1000 
	 loss: 28.6397, MinusLogProbMetric: 28.6397, val_loss: 30.5243, val_MinusLogProbMetric: 30.5243

Epoch 509: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6397 - MinusLogProbMetric: 28.6397 - val_loss: 30.5243 - val_MinusLogProbMetric: 30.5243 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 510/1000
2023-10-11 19:22:16.372 
Epoch 510/1000 
	 loss: 28.5721, MinusLogProbMetric: 28.5721, val_loss: 29.5719, val_MinusLogProbMetric: 29.5719

Epoch 510: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.5721 - MinusLogProbMetric: 28.5721 - val_loss: 29.5719 - val_MinusLogProbMetric: 29.5719 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 511/1000
2023-10-11 19:22:43.460 
Epoch 511/1000 
	 loss: 28.6065, MinusLogProbMetric: 28.6065, val_loss: 29.0468, val_MinusLogProbMetric: 29.0468

Epoch 511: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6065 - MinusLogProbMetric: 28.6065 - val_loss: 29.0468 - val_MinusLogProbMetric: 29.0468 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 512/1000
2023-10-11 19:23:10.422 
Epoch 512/1000 
	 loss: 28.8256, MinusLogProbMetric: 28.8256, val_loss: 28.9438, val_MinusLogProbMetric: 28.9438

Epoch 512: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.8256 - MinusLogProbMetric: 28.8256 - val_loss: 28.9438 - val_MinusLogProbMetric: 28.9438 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 513/1000
2023-10-11 19:23:37.542 
Epoch 513/1000 
	 loss: 28.6565, MinusLogProbMetric: 28.6565, val_loss: 29.3803, val_MinusLogProbMetric: 29.3803

Epoch 513: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6565 - MinusLogProbMetric: 28.6565 - val_loss: 29.3803 - val_MinusLogProbMetric: 29.3803 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 514/1000
2023-10-11 19:24:04.421 
Epoch 514/1000 
	 loss: 28.7853, MinusLogProbMetric: 28.7853, val_loss: 29.7542, val_MinusLogProbMetric: 29.7542

Epoch 514: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.7853 - MinusLogProbMetric: 28.7853 - val_loss: 29.7542 - val_MinusLogProbMetric: 29.7542 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 515/1000
2023-10-11 19:24:31.647 
Epoch 515/1000 
	 loss: 28.6455, MinusLogProbMetric: 28.6455, val_loss: 29.0592, val_MinusLogProbMetric: 29.0592

Epoch 515: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.6455 - MinusLogProbMetric: 28.6455 - val_loss: 29.0592 - val_MinusLogProbMetric: 29.0592 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 516/1000
2023-10-11 19:24:58.629 
Epoch 516/1000 
	 loss: 28.8966, MinusLogProbMetric: 28.8966, val_loss: 29.5207, val_MinusLogProbMetric: 29.5207

Epoch 516: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.8966 - MinusLogProbMetric: 28.8966 - val_loss: 29.5207 - val_MinusLogProbMetric: 29.5207 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 517/1000
2023-10-11 19:25:25.636 
Epoch 517/1000 
	 loss: 28.5462, MinusLogProbMetric: 28.5462, val_loss: 29.2348, val_MinusLogProbMetric: 29.2348

Epoch 517: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.5462 - MinusLogProbMetric: 28.5462 - val_loss: 29.2348 - val_MinusLogProbMetric: 29.2348 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 518/1000
2023-10-11 19:25:52.444 
Epoch 518/1000 
	 loss: 28.5813, MinusLogProbMetric: 28.5813, val_loss: 29.3267, val_MinusLogProbMetric: 29.3267

Epoch 518: val_loss did not improve from 28.93280
196/196 - 27s - loss: 28.5813 - MinusLogProbMetric: 28.5813 - val_loss: 29.3267 - val_MinusLogProbMetric: 29.3267 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 519/1000
2023-10-11 19:26:19.240 
Epoch 519/1000 
	 loss: 28.6784, MinusLogProbMetric: 28.6784, val_loss: 28.9295, val_MinusLogProbMetric: 28.9295

Epoch 519: val_loss improved from 28.93280 to 28.92946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.6784 - MinusLogProbMetric: 28.6784 - val_loss: 28.9295 - val_MinusLogProbMetric: 28.9295 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 520/1000
2023-10-11 19:26:46.866 
Epoch 520/1000 
	 loss: 28.6072, MinusLogProbMetric: 28.6072, val_loss: 29.1934, val_MinusLogProbMetric: 29.1934

Epoch 520: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6072 - MinusLogProbMetric: 28.6072 - val_loss: 29.1934 - val_MinusLogProbMetric: 29.1934 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 521/1000
2023-10-11 19:27:13.552 
Epoch 521/1000 
	 loss: 28.6046, MinusLogProbMetric: 28.6046, val_loss: 29.1714, val_MinusLogProbMetric: 29.1714

Epoch 521: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6046 - MinusLogProbMetric: 28.6046 - val_loss: 29.1714 - val_MinusLogProbMetric: 29.1714 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 522/1000
2023-10-11 19:27:41.050 
Epoch 522/1000 
	 loss: 28.5383, MinusLogProbMetric: 28.5383, val_loss: 30.0626, val_MinusLogProbMetric: 30.0626

Epoch 522: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5383 - MinusLogProbMetric: 28.5383 - val_loss: 30.0626 - val_MinusLogProbMetric: 30.0626 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 523/1000
2023-10-11 19:28:08.175 
Epoch 523/1000 
	 loss: 28.5628, MinusLogProbMetric: 28.5628, val_loss: 29.3151, val_MinusLogProbMetric: 29.3151

Epoch 523: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5628 - MinusLogProbMetric: 28.5628 - val_loss: 29.3151 - val_MinusLogProbMetric: 29.3151 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 524/1000
2023-10-11 19:28:35.299 
Epoch 524/1000 
	 loss: 28.6016, MinusLogProbMetric: 28.6016, val_loss: 30.4065, val_MinusLogProbMetric: 30.4065

Epoch 524: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6016 - MinusLogProbMetric: 28.6016 - val_loss: 30.4065 - val_MinusLogProbMetric: 30.4065 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 525/1000
2023-10-11 19:29:02.316 
Epoch 525/1000 
	 loss: 28.6010, MinusLogProbMetric: 28.6010, val_loss: 30.0169, val_MinusLogProbMetric: 30.0169

Epoch 525: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6010 - MinusLogProbMetric: 28.6010 - val_loss: 30.0169 - val_MinusLogProbMetric: 30.0169 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 526/1000
2023-10-11 19:29:29.271 
Epoch 526/1000 
	 loss: 28.6801, MinusLogProbMetric: 28.6801, val_loss: 29.0561, val_MinusLogProbMetric: 29.0561

Epoch 526: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6801 - MinusLogProbMetric: 28.6801 - val_loss: 29.0561 - val_MinusLogProbMetric: 29.0561 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 527/1000
2023-10-11 19:29:56.339 
Epoch 527/1000 
	 loss: 28.5667, MinusLogProbMetric: 28.5667, val_loss: 29.4167, val_MinusLogProbMetric: 29.4167

Epoch 527: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5667 - MinusLogProbMetric: 28.5667 - val_loss: 29.4167 - val_MinusLogProbMetric: 29.4167 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 528/1000
2023-10-11 19:30:23.390 
Epoch 528/1000 
	 loss: 28.6042, MinusLogProbMetric: 28.6042, val_loss: 29.5293, val_MinusLogProbMetric: 29.5293

Epoch 528: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6042 - MinusLogProbMetric: 28.6042 - val_loss: 29.5293 - val_MinusLogProbMetric: 29.5293 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 529/1000
2023-10-11 19:30:50.142 
Epoch 529/1000 
	 loss: 28.7732, MinusLogProbMetric: 28.7732, val_loss: 29.8173, val_MinusLogProbMetric: 29.8173

Epoch 529: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.7732 - MinusLogProbMetric: 28.7732 - val_loss: 29.8173 - val_MinusLogProbMetric: 29.8173 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 530/1000
2023-10-11 19:31:17.204 
Epoch 530/1000 
	 loss: 28.6069, MinusLogProbMetric: 28.6069, val_loss: 29.2460, val_MinusLogProbMetric: 29.2460

Epoch 530: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6069 - MinusLogProbMetric: 28.6069 - val_loss: 29.2460 - val_MinusLogProbMetric: 29.2460 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 531/1000
2023-10-11 19:31:44.198 
Epoch 531/1000 
	 loss: 28.6845, MinusLogProbMetric: 28.6845, val_loss: 29.3016, val_MinusLogProbMetric: 29.3016

Epoch 531: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6845 - MinusLogProbMetric: 28.6845 - val_loss: 29.3016 - val_MinusLogProbMetric: 29.3016 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 532/1000
2023-10-11 19:32:10.835 
Epoch 532/1000 
	 loss: 28.7153, MinusLogProbMetric: 28.7153, val_loss: 29.2330, val_MinusLogProbMetric: 29.2330

Epoch 532: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.7153 - MinusLogProbMetric: 28.7153 - val_loss: 29.2330 - val_MinusLogProbMetric: 29.2330 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 533/1000
2023-10-11 19:32:37.904 
Epoch 533/1000 
	 loss: 28.4922, MinusLogProbMetric: 28.4922, val_loss: 29.3188, val_MinusLogProbMetric: 29.3188

Epoch 533: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.4922 - MinusLogProbMetric: 28.4922 - val_loss: 29.3188 - val_MinusLogProbMetric: 29.3188 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 534/1000
2023-10-11 19:33:04.695 
Epoch 534/1000 
	 loss: 28.6315, MinusLogProbMetric: 28.6315, val_loss: 29.8575, val_MinusLogProbMetric: 29.8575

Epoch 534: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6315 - MinusLogProbMetric: 28.6315 - val_loss: 29.8575 - val_MinusLogProbMetric: 29.8575 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 535/1000
2023-10-11 19:33:32.032 
Epoch 535/1000 
	 loss: 28.5720, MinusLogProbMetric: 28.5720, val_loss: 29.0799, val_MinusLogProbMetric: 29.0799

Epoch 535: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5720 - MinusLogProbMetric: 28.5720 - val_loss: 29.0799 - val_MinusLogProbMetric: 29.0799 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 536/1000
2023-10-11 19:33:59.011 
Epoch 536/1000 
	 loss: 28.5390, MinusLogProbMetric: 28.5390, val_loss: 29.9478, val_MinusLogProbMetric: 29.9478

Epoch 536: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5390 - MinusLogProbMetric: 28.5390 - val_loss: 29.9478 - val_MinusLogProbMetric: 29.9478 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 537/1000
2023-10-11 19:34:25.870 
Epoch 537/1000 
	 loss: 28.5945, MinusLogProbMetric: 28.5945, val_loss: 29.8843, val_MinusLogProbMetric: 29.8843

Epoch 537: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5945 - MinusLogProbMetric: 28.5945 - val_loss: 29.8843 - val_MinusLogProbMetric: 29.8843 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 538/1000
2023-10-11 19:34:52.684 
Epoch 538/1000 
	 loss: 28.5357, MinusLogProbMetric: 28.5357, val_loss: 29.2104, val_MinusLogProbMetric: 29.2104

Epoch 538: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5357 - MinusLogProbMetric: 28.5357 - val_loss: 29.2104 - val_MinusLogProbMetric: 29.2104 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 539/1000
2023-10-11 19:35:19.575 
Epoch 539/1000 
	 loss: 28.5766, MinusLogProbMetric: 28.5766, val_loss: 29.4557, val_MinusLogProbMetric: 29.4557

Epoch 539: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5766 - MinusLogProbMetric: 28.5766 - val_loss: 29.4557 - val_MinusLogProbMetric: 29.4557 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 540/1000
2023-10-11 19:35:46.368 
Epoch 540/1000 
	 loss: 28.4709, MinusLogProbMetric: 28.4709, val_loss: 28.9718, val_MinusLogProbMetric: 28.9718

Epoch 540: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.4709 - MinusLogProbMetric: 28.4709 - val_loss: 28.9718 - val_MinusLogProbMetric: 28.9718 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 541/1000
2023-10-11 19:36:13.420 
Epoch 541/1000 
	 loss: 28.4581, MinusLogProbMetric: 28.4581, val_loss: 29.3851, val_MinusLogProbMetric: 29.3851

Epoch 541: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.4581 - MinusLogProbMetric: 28.4581 - val_loss: 29.3851 - val_MinusLogProbMetric: 29.3851 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 542/1000
2023-10-11 19:36:40.152 
Epoch 542/1000 
	 loss: 28.5139, MinusLogProbMetric: 28.5139, val_loss: 29.8252, val_MinusLogProbMetric: 29.8252

Epoch 542: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5139 - MinusLogProbMetric: 28.5139 - val_loss: 29.8252 - val_MinusLogProbMetric: 29.8252 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 543/1000
2023-10-11 19:37:07.228 
Epoch 543/1000 
	 loss: 28.6311, MinusLogProbMetric: 28.6311, val_loss: 29.6098, val_MinusLogProbMetric: 29.6098

Epoch 543: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6311 - MinusLogProbMetric: 28.6311 - val_loss: 29.6098 - val_MinusLogProbMetric: 29.6098 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 544/1000
2023-10-11 19:37:33.806 
Epoch 544/1000 
	 loss: 28.5975, MinusLogProbMetric: 28.5975, val_loss: 29.4695, val_MinusLogProbMetric: 29.4695

Epoch 544: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5975 - MinusLogProbMetric: 28.5975 - val_loss: 29.4695 - val_MinusLogProbMetric: 29.4695 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 545/1000
2023-10-11 19:38:00.755 
Epoch 545/1000 
	 loss: 28.6440, MinusLogProbMetric: 28.6440, val_loss: 29.4472, val_MinusLogProbMetric: 29.4472

Epoch 545: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6440 - MinusLogProbMetric: 28.6440 - val_loss: 29.4472 - val_MinusLogProbMetric: 29.4472 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 546/1000
2023-10-11 19:38:27.429 
Epoch 546/1000 
	 loss: 28.5555, MinusLogProbMetric: 28.5555, val_loss: 29.0940, val_MinusLogProbMetric: 29.0940

Epoch 546: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5555 - MinusLogProbMetric: 28.5555 - val_loss: 29.0940 - val_MinusLogProbMetric: 29.0940 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 547/1000
2023-10-11 19:38:54.355 
Epoch 547/1000 
	 loss: 28.6348, MinusLogProbMetric: 28.6348, val_loss: 29.8870, val_MinusLogProbMetric: 29.8870

Epoch 547: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6348 - MinusLogProbMetric: 28.6348 - val_loss: 29.8870 - val_MinusLogProbMetric: 29.8870 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 548/1000
2023-10-11 19:39:21.221 
Epoch 548/1000 
	 loss: 28.6728, MinusLogProbMetric: 28.6728, val_loss: 29.3334, val_MinusLogProbMetric: 29.3334

Epoch 548: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6728 - MinusLogProbMetric: 28.6728 - val_loss: 29.3334 - val_MinusLogProbMetric: 29.3334 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 549/1000
2023-10-11 19:39:47.659 
Epoch 549/1000 
	 loss: 28.4257, MinusLogProbMetric: 28.4257, val_loss: 29.6658, val_MinusLogProbMetric: 29.6658

Epoch 549: val_loss did not improve from 28.92946
196/196 - 26s - loss: 28.4257 - MinusLogProbMetric: 28.4257 - val_loss: 29.6658 - val_MinusLogProbMetric: 29.6658 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 550/1000
2023-10-11 19:40:14.619 
Epoch 550/1000 
	 loss: 28.5742, MinusLogProbMetric: 28.5742, val_loss: 29.2442, val_MinusLogProbMetric: 29.2442

Epoch 550: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5742 - MinusLogProbMetric: 28.5742 - val_loss: 29.2442 - val_MinusLogProbMetric: 29.2442 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 551/1000
2023-10-11 19:40:41.142 
Epoch 551/1000 
	 loss: 28.7348, MinusLogProbMetric: 28.7348, val_loss: 29.1905, val_MinusLogProbMetric: 29.1905

Epoch 551: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.7348 - MinusLogProbMetric: 28.7348 - val_loss: 29.1905 - val_MinusLogProbMetric: 29.1905 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 552/1000
2023-10-11 19:41:08.008 
Epoch 552/1000 
	 loss: 28.5129, MinusLogProbMetric: 28.5129, val_loss: 29.9471, val_MinusLogProbMetric: 29.9471

Epoch 552: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5129 - MinusLogProbMetric: 28.5129 - val_loss: 29.9471 - val_MinusLogProbMetric: 29.9471 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 553/1000
2023-10-11 19:41:34.745 
Epoch 553/1000 
	 loss: 28.6279, MinusLogProbMetric: 28.6279, val_loss: 29.2185, val_MinusLogProbMetric: 29.2185

Epoch 553: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6279 - MinusLogProbMetric: 28.6279 - val_loss: 29.2185 - val_MinusLogProbMetric: 29.2185 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 554/1000
2023-10-11 19:42:01.590 
Epoch 554/1000 
	 loss: 28.4464, MinusLogProbMetric: 28.4464, val_loss: 29.3439, val_MinusLogProbMetric: 29.3439

Epoch 554: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.4464 - MinusLogProbMetric: 28.4464 - val_loss: 29.3439 - val_MinusLogProbMetric: 29.3439 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 555/1000
2023-10-11 19:42:28.737 
Epoch 555/1000 
	 loss: 28.5114, MinusLogProbMetric: 28.5114, val_loss: 29.2998, val_MinusLogProbMetric: 29.2998

Epoch 555: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.5114 - MinusLogProbMetric: 28.5114 - val_loss: 29.2998 - val_MinusLogProbMetric: 29.2998 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 556/1000
2023-10-11 19:42:55.770 
Epoch 556/1000 
	 loss: 28.6037, MinusLogProbMetric: 28.6037, val_loss: 29.0579, val_MinusLogProbMetric: 29.0579

Epoch 556: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.6037 - MinusLogProbMetric: 28.6037 - val_loss: 29.0579 - val_MinusLogProbMetric: 29.0579 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 557/1000
2023-10-11 19:43:22.621 
Epoch 557/1000 
	 loss: 28.4650, MinusLogProbMetric: 28.4650, val_loss: 29.0447, val_MinusLogProbMetric: 29.0447

Epoch 557: val_loss did not improve from 28.92946
196/196 - 27s - loss: 28.4650 - MinusLogProbMetric: 28.4650 - val_loss: 29.0447 - val_MinusLogProbMetric: 29.0447 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 558/1000
2023-10-11 19:43:49.556 
Epoch 558/1000 
	 loss: 28.5200, MinusLogProbMetric: 28.5200, val_loss: 28.9241, val_MinusLogProbMetric: 28.9241

Epoch 558: val_loss improved from 28.92946 to 28.92408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.5200 - MinusLogProbMetric: 28.5200 - val_loss: 28.9241 - val_MinusLogProbMetric: 28.9241 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 559/1000
2023-10-11 19:44:17.246 
Epoch 559/1000 
	 loss: 28.4608, MinusLogProbMetric: 28.4608, val_loss: 29.0658, val_MinusLogProbMetric: 29.0658

Epoch 559: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.4608 - MinusLogProbMetric: 28.4608 - val_loss: 29.0658 - val_MinusLogProbMetric: 29.0658 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 560/1000
2023-10-11 19:44:44.070 
Epoch 560/1000 
	 loss: 28.5555, MinusLogProbMetric: 28.5555, val_loss: 29.6634, val_MinusLogProbMetric: 29.6634

Epoch 560: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.5555 - MinusLogProbMetric: 28.5555 - val_loss: 29.6634 - val_MinusLogProbMetric: 29.6634 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 561/1000
2023-10-11 19:45:10.824 
Epoch 561/1000 
	 loss: 28.5681, MinusLogProbMetric: 28.5681, val_loss: 29.2223, val_MinusLogProbMetric: 29.2223

Epoch 561: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.5681 - MinusLogProbMetric: 28.5681 - val_loss: 29.2223 - val_MinusLogProbMetric: 29.2223 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 562/1000
2023-10-11 19:45:37.721 
Epoch 562/1000 
	 loss: 28.4807, MinusLogProbMetric: 28.4807, val_loss: 29.3140, val_MinusLogProbMetric: 29.3140

Epoch 562: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.4807 - MinusLogProbMetric: 28.4807 - val_loss: 29.3140 - val_MinusLogProbMetric: 29.3140 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 563/1000
2023-10-11 19:46:04.707 
Epoch 563/1000 
	 loss: 28.5156, MinusLogProbMetric: 28.5156, val_loss: 29.9324, val_MinusLogProbMetric: 29.9324

Epoch 563: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.5156 - MinusLogProbMetric: 28.5156 - val_loss: 29.9324 - val_MinusLogProbMetric: 29.9324 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 564/1000
2023-10-11 19:46:31.588 
Epoch 564/1000 
	 loss: 28.5674, MinusLogProbMetric: 28.5674, val_loss: 29.7698, val_MinusLogProbMetric: 29.7698

Epoch 564: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.5674 - MinusLogProbMetric: 28.5674 - val_loss: 29.7698 - val_MinusLogProbMetric: 29.7698 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 565/1000
2023-10-11 19:46:58.782 
Epoch 565/1000 
	 loss: 28.4893, MinusLogProbMetric: 28.4893, val_loss: 29.5036, val_MinusLogProbMetric: 29.5036

Epoch 565: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.4893 - MinusLogProbMetric: 28.4893 - val_loss: 29.5036 - val_MinusLogProbMetric: 29.5036 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 566/1000
2023-10-11 19:47:25.748 
Epoch 566/1000 
	 loss: 28.5863, MinusLogProbMetric: 28.5863, val_loss: 29.3829, val_MinusLogProbMetric: 29.3829

Epoch 566: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.5863 - MinusLogProbMetric: 28.5863 - val_loss: 29.3829 - val_MinusLogProbMetric: 29.3829 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 567/1000
2023-10-11 19:47:52.746 
Epoch 567/1000 
	 loss: 28.5150, MinusLogProbMetric: 28.5150, val_loss: 29.1200, val_MinusLogProbMetric: 29.1200

Epoch 567: val_loss did not improve from 28.92408
196/196 - 27s - loss: 28.5150 - MinusLogProbMetric: 28.5150 - val_loss: 29.1200 - val_MinusLogProbMetric: 29.1200 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 568/1000
2023-10-11 19:48:19.663 
Epoch 568/1000 
	 loss: 28.5091, MinusLogProbMetric: 28.5091, val_loss: 28.8988, val_MinusLogProbMetric: 28.8988

Epoch 568: val_loss improved from 28.92408 to 28.89882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.5091 - MinusLogProbMetric: 28.5091 - val_loss: 28.8988 - val_MinusLogProbMetric: 28.8988 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 569/1000
2023-10-11 19:48:47.092 
Epoch 569/1000 
	 loss: 28.3668, MinusLogProbMetric: 28.3668, val_loss: 29.2261, val_MinusLogProbMetric: 29.2261

Epoch 569: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.3668 - MinusLogProbMetric: 28.3668 - val_loss: 29.2261 - val_MinusLogProbMetric: 29.2261 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 570/1000
2023-10-11 19:49:13.815 
Epoch 570/1000 
	 loss: 28.4690, MinusLogProbMetric: 28.4690, val_loss: 29.5300, val_MinusLogProbMetric: 29.5300

Epoch 570: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4690 - MinusLogProbMetric: 28.4690 - val_loss: 29.5300 - val_MinusLogProbMetric: 29.5300 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 571/1000
2023-10-11 19:49:40.915 
Epoch 571/1000 
	 loss: 28.5968, MinusLogProbMetric: 28.5968, val_loss: 29.6224, val_MinusLogProbMetric: 29.6224

Epoch 571: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5968 - MinusLogProbMetric: 28.5968 - val_loss: 29.6224 - val_MinusLogProbMetric: 29.6224 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 572/1000
2023-10-11 19:50:07.733 
Epoch 572/1000 
	 loss: 28.4506, MinusLogProbMetric: 28.4506, val_loss: 29.9354, val_MinusLogProbMetric: 29.9354

Epoch 572: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4506 - MinusLogProbMetric: 28.4506 - val_loss: 29.9354 - val_MinusLogProbMetric: 29.9354 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 573/1000
2023-10-11 19:50:34.677 
Epoch 573/1000 
	 loss: 28.7738, MinusLogProbMetric: 28.7738, val_loss: 29.3778, val_MinusLogProbMetric: 29.3778

Epoch 573: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.7738 - MinusLogProbMetric: 28.7738 - val_loss: 29.3778 - val_MinusLogProbMetric: 29.3778 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 574/1000
2023-10-11 19:51:01.695 
Epoch 574/1000 
	 loss: 28.4325, MinusLogProbMetric: 28.4325, val_loss: 29.1940, val_MinusLogProbMetric: 29.1940

Epoch 574: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4325 - MinusLogProbMetric: 28.4325 - val_loss: 29.1940 - val_MinusLogProbMetric: 29.1940 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 575/1000
2023-10-11 19:51:28.522 
Epoch 575/1000 
	 loss: 28.4367, MinusLogProbMetric: 28.4367, val_loss: 30.0354, val_MinusLogProbMetric: 30.0354

Epoch 575: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4367 - MinusLogProbMetric: 28.4367 - val_loss: 30.0354 - val_MinusLogProbMetric: 30.0354 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 576/1000
2023-10-11 19:51:55.399 
Epoch 576/1000 
	 loss: 28.4261, MinusLogProbMetric: 28.4261, val_loss: 29.0581, val_MinusLogProbMetric: 29.0581

Epoch 576: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4261 - MinusLogProbMetric: 28.4261 - val_loss: 29.0581 - val_MinusLogProbMetric: 29.0581 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 577/1000
2023-10-11 19:52:22.554 
Epoch 577/1000 
	 loss: 28.4957, MinusLogProbMetric: 28.4957, val_loss: 29.8451, val_MinusLogProbMetric: 29.8451

Epoch 577: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4957 - MinusLogProbMetric: 28.4957 - val_loss: 29.8451 - val_MinusLogProbMetric: 29.8451 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 578/1000
2023-10-11 19:52:49.566 
Epoch 578/1000 
	 loss: 28.4837, MinusLogProbMetric: 28.4837, val_loss: 29.6963, val_MinusLogProbMetric: 29.6963

Epoch 578: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4837 - MinusLogProbMetric: 28.4837 - val_loss: 29.6963 - val_MinusLogProbMetric: 29.6963 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 579/1000
2023-10-11 19:53:16.191 
Epoch 579/1000 
	 loss: 28.6011, MinusLogProbMetric: 28.6011, val_loss: 29.0365, val_MinusLogProbMetric: 29.0365

Epoch 579: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.6011 - MinusLogProbMetric: 28.6011 - val_loss: 29.0365 - val_MinusLogProbMetric: 29.0365 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 580/1000
2023-10-11 19:53:43.112 
Epoch 580/1000 
	 loss: 28.5646, MinusLogProbMetric: 28.5646, val_loss: 28.9194, val_MinusLogProbMetric: 28.9194

Epoch 580: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5646 - MinusLogProbMetric: 28.5646 - val_loss: 28.9194 - val_MinusLogProbMetric: 28.9194 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 581/1000
2023-10-11 19:54:10.132 
Epoch 581/1000 
	 loss: 28.4599, MinusLogProbMetric: 28.4599, val_loss: 28.9693, val_MinusLogProbMetric: 28.9693

Epoch 581: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4599 - MinusLogProbMetric: 28.4599 - val_loss: 28.9693 - val_MinusLogProbMetric: 28.9693 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 582/1000
2023-10-11 19:54:36.546 
Epoch 582/1000 
	 loss: 28.3710, MinusLogProbMetric: 28.3710, val_loss: 29.3407, val_MinusLogProbMetric: 29.3407

Epoch 582: val_loss did not improve from 28.89882
196/196 - 26s - loss: 28.3710 - MinusLogProbMetric: 28.3710 - val_loss: 29.3407 - val_MinusLogProbMetric: 29.3407 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 583/1000
2023-10-11 19:55:03.866 
Epoch 583/1000 
	 loss: 28.4800, MinusLogProbMetric: 28.4800, val_loss: 29.3391, val_MinusLogProbMetric: 29.3391

Epoch 583: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4800 - MinusLogProbMetric: 28.4800 - val_loss: 29.3391 - val_MinusLogProbMetric: 29.3391 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 584/1000
2023-10-11 19:55:30.418 
Epoch 584/1000 
	 loss: 28.5892, MinusLogProbMetric: 28.5892, val_loss: 29.7095, val_MinusLogProbMetric: 29.7095

Epoch 584: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5892 - MinusLogProbMetric: 28.5892 - val_loss: 29.7095 - val_MinusLogProbMetric: 29.7095 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 585/1000
2023-10-11 19:55:57.181 
Epoch 585/1000 
	 loss: 28.5523, MinusLogProbMetric: 28.5523, val_loss: 29.1522, val_MinusLogProbMetric: 29.1522

Epoch 585: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5523 - MinusLogProbMetric: 28.5523 - val_loss: 29.1522 - val_MinusLogProbMetric: 29.1522 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 586/1000
2023-10-11 19:56:24.428 
Epoch 586/1000 
	 loss: 28.4730, MinusLogProbMetric: 28.4730, val_loss: 29.8922, val_MinusLogProbMetric: 29.8922

Epoch 586: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4730 - MinusLogProbMetric: 28.4730 - val_loss: 29.8922 - val_MinusLogProbMetric: 29.8922 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 587/1000
2023-10-11 19:56:51.455 
Epoch 587/1000 
	 loss: 28.5792, MinusLogProbMetric: 28.5792, val_loss: 29.4967, val_MinusLogProbMetric: 29.4967

Epoch 587: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5792 - MinusLogProbMetric: 28.5792 - val_loss: 29.4967 - val_MinusLogProbMetric: 29.4967 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 588/1000
2023-10-11 19:57:18.232 
Epoch 588/1000 
	 loss: 28.4780, MinusLogProbMetric: 28.4780, val_loss: 29.2024, val_MinusLogProbMetric: 29.2024

Epoch 588: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4780 - MinusLogProbMetric: 28.4780 - val_loss: 29.2024 - val_MinusLogProbMetric: 29.2024 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 589/1000
2023-10-11 19:57:45.297 
Epoch 589/1000 
	 loss: 28.4714, MinusLogProbMetric: 28.4714, val_loss: 29.1320, val_MinusLogProbMetric: 29.1320

Epoch 589: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4714 - MinusLogProbMetric: 28.4714 - val_loss: 29.1320 - val_MinusLogProbMetric: 29.1320 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 590/1000
2023-10-11 19:58:12.254 
Epoch 590/1000 
	 loss: 28.4083, MinusLogProbMetric: 28.4083, val_loss: 29.1189, val_MinusLogProbMetric: 29.1189

Epoch 590: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4083 - MinusLogProbMetric: 28.4083 - val_loss: 29.1189 - val_MinusLogProbMetric: 29.1189 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 591/1000
2023-10-11 19:58:38.852 
Epoch 591/1000 
	 loss: 28.4702, MinusLogProbMetric: 28.4702, val_loss: 29.2335, val_MinusLogProbMetric: 29.2335

Epoch 591: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4702 - MinusLogProbMetric: 28.4702 - val_loss: 29.2335 - val_MinusLogProbMetric: 29.2335 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 592/1000
2023-10-11 19:59:05.799 
Epoch 592/1000 
	 loss: 28.3879, MinusLogProbMetric: 28.3879, val_loss: 29.4106, val_MinusLogProbMetric: 29.4106

Epoch 592: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.3879 - MinusLogProbMetric: 28.3879 - val_loss: 29.4106 - val_MinusLogProbMetric: 29.4106 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 593/1000
2023-10-11 19:59:32.438 
Epoch 593/1000 
	 loss: 28.5396, MinusLogProbMetric: 28.5396, val_loss: 29.9283, val_MinusLogProbMetric: 29.9283

Epoch 593: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5396 - MinusLogProbMetric: 28.5396 - val_loss: 29.9283 - val_MinusLogProbMetric: 29.9283 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 594/1000
2023-10-11 19:59:59.029 
Epoch 594/1000 
	 loss: 28.5061, MinusLogProbMetric: 28.5061, val_loss: 29.0566, val_MinusLogProbMetric: 29.0566

Epoch 594: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.5061 - MinusLogProbMetric: 28.5061 - val_loss: 29.0566 - val_MinusLogProbMetric: 29.0566 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 595/1000
2023-10-11 20:00:25.712 
Epoch 595/1000 
	 loss: 28.4411, MinusLogProbMetric: 28.4411, val_loss: 29.2317, val_MinusLogProbMetric: 29.2317

Epoch 595: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4411 - MinusLogProbMetric: 28.4411 - val_loss: 29.2317 - val_MinusLogProbMetric: 29.2317 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 596/1000
2023-10-11 20:00:52.744 
Epoch 596/1000 
	 loss: 28.4292, MinusLogProbMetric: 28.4292, val_loss: 29.3264, val_MinusLogProbMetric: 29.3264

Epoch 596: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.4292 - MinusLogProbMetric: 28.4292 - val_loss: 29.3264 - val_MinusLogProbMetric: 29.3264 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 597/1000
2023-10-11 20:01:19.603 
Epoch 597/1000 
	 loss: 28.3872, MinusLogProbMetric: 28.3872, val_loss: 29.1822, val_MinusLogProbMetric: 29.1822

Epoch 597: val_loss did not improve from 28.89882
196/196 - 27s - loss: 28.3872 - MinusLogProbMetric: 28.3872 - val_loss: 29.1822 - val_MinusLogProbMetric: 29.1822 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 598/1000
2023-10-11 20:01:46.402 
Epoch 598/1000 
	 loss: 28.5739, MinusLogProbMetric: 28.5739, val_loss: 28.8816, val_MinusLogProbMetric: 28.8816

Epoch 598: val_loss improved from 28.89882 to 28.88165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.5739 - MinusLogProbMetric: 28.5739 - val_loss: 28.8816 - val_MinusLogProbMetric: 28.8816 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 599/1000
2023-10-11 20:02:13.681 
Epoch 599/1000 
	 loss: 28.4865, MinusLogProbMetric: 28.4865, val_loss: 29.4533, val_MinusLogProbMetric: 29.4533

Epoch 599: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4865 - MinusLogProbMetric: 28.4865 - val_loss: 29.4533 - val_MinusLogProbMetric: 29.4533 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 600/1000
2023-10-11 20:02:42.020 
Epoch 600/1000 
	 loss: 28.6050, MinusLogProbMetric: 28.6050, val_loss: 29.1890, val_MinusLogProbMetric: 29.1890

Epoch 600: val_loss did not improve from 28.88165
196/196 - 28s - loss: 28.6050 - MinusLogProbMetric: 28.6050 - val_loss: 29.1890 - val_MinusLogProbMetric: 29.1890 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 601/1000
2023-10-11 20:03:10.037 
Epoch 601/1000 
	 loss: 28.3478, MinusLogProbMetric: 28.3478, val_loss: 31.1401, val_MinusLogProbMetric: 31.1401

Epoch 601: val_loss did not improve from 28.88165
196/196 - 28s - loss: 28.3478 - MinusLogProbMetric: 28.3478 - val_loss: 31.1401 - val_MinusLogProbMetric: 31.1401 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 602/1000
2023-10-11 20:03:36.754 
Epoch 602/1000 
	 loss: 28.5717, MinusLogProbMetric: 28.5717, val_loss: 29.1737, val_MinusLogProbMetric: 29.1737

Epoch 602: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.5717 - MinusLogProbMetric: 28.5717 - val_loss: 29.1737 - val_MinusLogProbMetric: 29.1737 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 603/1000
2023-10-11 20:04:03.198 
Epoch 603/1000 
	 loss: 28.4161, MinusLogProbMetric: 28.4161, val_loss: 29.1483, val_MinusLogProbMetric: 29.1483

Epoch 603: val_loss did not improve from 28.88165
196/196 - 26s - loss: 28.4161 - MinusLogProbMetric: 28.4161 - val_loss: 29.1483 - val_MinusLogProbMetric: 29.1483 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 604/1000
2023-10-11 20:04:29.803 
Epoch 604/1000 
	 loss: 28.4683, MinusLogProbMetric: 28.4683, val_loss: 29.3338, val_MinusLogProbMetric: 29.3338

Epoch 604: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4683 - MinusLogProbMetric: 28.4683 - val_loss: 29.3338 - val_MinusLogProbMetric: 29.3338 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 605/1000
2023-10-11 20:04:56.940 
Epoch 605/1000 
	 loss: 28.4316, MinusLogProbMetric: 28.4316, val_loss: 29.0034, val_MinusLogProbMetric: 29.0034

Epoch 605: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4316 - MinusLogProbMetric: 28.4316 - val_loss: 29.0034 - val_MinusLogProbMetric: 29.0034 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 606/1000
2023-10-11 20:05:24.238 
Epoch 606/1000 
	 loss: 28.5876, MinusLogProbMetric: 28.5876, val_loss: 29.9239, val_MinusLogProbMetric: 29.9239

Epoch 606: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.5876 - MinusLogProbMetric: 28.5876 - val_loss: 29.9239 - val_MinusLogProbMetric: 29.9239 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 607/1000
2023-10-11 20:05:50.997 
Epoch 607/1000 
	 loss: 28.4196, MinusLogProbMetric: 28.4196, val_loss: 29.4766, val_MinusLogProbMetric: 29.4766

Epoch 607: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4196 - MinusLogProbMetric: 28.4196 - val_loss: 29.4766 - val_MinusLogProbMetric: 29.4766 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 608/1000
2023-10-11 20:06:17.726 
Epoch 608/1000 
	 loss: 28.4746, MinusLogProbMetric: 28.4746, val_loss: 29.4297, val_MinusLogProbMetric: 29.4297

Epoch 608: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4746 - MinusLogProbMetric: 28.4746 - val_loss: 29.4297 - val_MinusLogProbMetric: 29.4297 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 609/1000
2023-10-11 20:06:44.715 
Epoch 609/1000 
	 loss: 28.5204, MinusLogProbMetric: 28.5204, val_loss: 29.7583, val_MinusLogProbMetric: 29.7583

Epoch 609: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.5204 - MinusLogProbMetric: 28.5204 - val_loss: 29.7583 - val_MinusLogProbMetric: 29.7583 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 610/1000
2023-10-11 20:07:11.368 
Epoch 610/1000 
	 loss: 28.3726, MinusLogProbMetric: 28.3726, val_loss: 29.0409, val_MinusLogProbMetric: 29.0409

Epoch 610: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.3726 - MinusLogProbMetric: 28.3726 - val_loss: 29.0409 - val_MinusLogProbMetric: 29.0409 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 611/1000
2023-10-11 20:07:38.332 
Epoch 611/1000 
	 loss: 28.3724, MinusLogProbMetric: 28.3724, val_loss: 29.3590, val_MinusLogProbMetric: 29.3590

Epoch 611: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.3724 - MinusLogProbMetric: 28.3724 - val_loss: 29.3590 - val_MinusLogProbMetric: 29.3590 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 612/1000
2023-10-11 20:08:04.694 
Epoch 612/1000 
	 loss: 28.4501, MinusLogProbMetric: 28.4501, val_loss: 28.9785, val_MinusLogProbMetric: 28.9785

Epoch 612: val_loss did not improve from 28.88165
196/196 - 26s - loss: 28.4501 - MinusLogProbMetric: 28.4501 - val_loss: 28.9785 - val_MinusLogProbMetric: 28.9785 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 613/1000
2023-10-11 20:08:31.339 
Epoch 613/1000 
	 loss: 28.4851, MinusLogProbMetric: 28.4851, val_loss: 29.0706, val_MinusLogProbMetric: 29.0706

Epoch 613: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4851 - MinusLogProbMetric: 28.4851 - val_loss: 29.0706 - val_MinusLogProbMetric: 29.0706 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 614/1000
2023-10-11 20:08:58.279 
Epoch 614/1000 
	 loss: 28.4434, MinusLogProbMetric: 28.4434, val_loss: 29.1708, val_MinusLogProbMetric: 29.1708

Epoch 614: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4434 - MinusLogProbMetric: 28.4434 - val_loss: 29.1708 - val_MinusLogProbMetric: 29.1708 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 615/1000
2023-10-11 20:09:25.087 
Epoch 615/1000 
	 loss: 28.3743, MinusLogProbMetric: 28.3743, val_loss: 29.5119, val_MinusLogProbMetric: 29.5119

Epoch 615: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.3743 - MinusLogProbMetric: 28.3743 - val_loss: 29.5119 - val_MinusLogProbMetric: 29.5119 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 616/1000
2023-10-11 20:09:51.828 
Epoch 616/1000 
	 loss: 28.4223, MinusLogProbMetric: 28.4223, val_loss: 29.1279, val_MinusLogProbMetric: 29.1279

Epoch 616: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4223 - MinusLogProbMetric: 28.4223 - val_loss: 29.1279 - val_MinusLogProbMetric: 29.1279 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 617/1000
2023-10-11 20:10:18.103 
Epoch 617/1000 
	 loss: 28.4012, MinusLogProbMetric: 28.4012, val_loss: 29.4899, val_MinusLogProbMetric: 29.4899

Epoch 617: val_loss did not improve from 28.88165
196/196 - 26s - loss: 28.4012 - MinusLogProbMetric: 28.4012 - val_loss: 29.4899 - val_MinusLogProbMetric: 29.4899 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 618/1000
2023-10-11 20:10:44.900 
Epoch 618/1000 
	 loss: 28.4135, MinusLogProbMetric: 28.4135, val_loss: 28.9833, val_MinusLogProbMetric: 28.9833

Epoch 618: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4135 - MinusLogProbMetric: 28.4135 - val_loss: 28.9833 - val_MinusLogProbMetric: 28.9833 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 619/1000
2023-10-11 20:11:11.815 
Epoch 619/1000 
	 loss: 28.4143, MinusLogProbMetric: 28.4143, val_loss: 29.9076, val_MinusLogProbMetric: 29.9076

Epoch 619: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4143 - MinusLogProbMetric: 28.4143 - val_loss: 29.9076 - val_MinusLogProbMetric: 29.9076 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 620/1000
2023-10-11 20:11:38.357 
Epoch 620/1000 
	 loss: 28.4393, MinusLogProbMetric: 28.4393, val_loss: 29.2957, val_MinusLogProbMetric: 29.2957

Epoch 620: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4393 - MinusLogProbMetric: 28.4393 - val_loss: 29.2957 - val_MinusLogProbMetric: 29.2957 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 621/1000
2023-10-11 20:12:04.733 
Epoch 621/1000 
	 loss: 28.4328, MinusLogProbMetric: 28.4328, val_loss: 29.3729, val_MinusLogProbMetric: 29.3729

Epoch 621: val_loss did not improve from 28.88165
196/196 - 26s - loss: 28.4328 - MinusLogProbMetric: 28.4328 - val_loss: 29.3729 - val_MinusLogProbMetric: 29.3729 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 622/1000
2023-10-11 20:12:31.555 
Epoch 622/1000 
	 loss: 28.4519, MinusLogProbMetric: 28.4519, val_loss: 29.0493, val_MinusLogProbMetric: 29.0493

Epoch 622: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4519 - MinusLogProbMetric: 28.4519 - val_loss: 29.0493 - val_MinusLogProbMetric: 29.0493 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 623/1000
2023-10-11 20:12:58.185 
Epoch 623/1000 
	 loss: 28.4160, MinusLogProbMetric: 28.4160, val_loss: 29.1864, val_MinusLogProbMetric: 29.1864

Epoch 623: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4160 - MinusLogProbMetric: 28.4160 - val_loss: 29.1864 - val_MinusLogProbMetric: 29.1864 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 624/1000
2023-10-11 20:13:24.700 
Epoch 624/1000 
	 loss: 28.4419, MinusLogProbMetric: 28.4419, val_loss: 29.1138, val_MinusLogProbMetric: 29.1138

Epoch 624: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4419 - MinusLogProbMetric: 28.4419 - val_loss: 29.1138 - val_MinusLogProbMetric: 29.1138 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 625/1000
2023-10-11 20:13:51.389 
Epoch 625/1000 
	 loss: 28.4070, MinusLogProbMetric: 28.4070, val_loss: 29.0486, val_MinusLogProbMetric: 29.0486

Epoch 625: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4070 - MinusLogProbMetric: 28.4070 - val_loss: 29.0486 - val_MinusLogProbMetric: 29.0486 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 626/1000
2023-10-11 20:14:17.671 
Epoch 626/1000 
	 loss: 28.3607, MinusLogProbMetric: 28.3607, val_loss: 29.3917, val_MinusLogProbMetric: 29.3917

Epoch 626: val_loss did not improve from 28.88165
196/196 - 26s - loss: 28.3607 - MinusLogProbMetric: 28.3607 - val_loss: 29.3917 - val_MinusLogProbMetric: 29.3917 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 627/1000
2023-10-11 20:14:44.541 
Epoch 627/1000 
	 loss: 28.4335, MinusLogProbMetric: 28.4335, val_loss: 29.3927, val_MinusLogProbMetric: 29.3927

Epoch 627: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.4335 - MinusLogProbMetric: 28.4335 - val_loss: 29.3927 - val_MinusLogProbMetric: 29.3927 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 628/1000
2023-10-11 20:15:11.074 
Epoch 628/1000 
	 loss: 28.3792, MinusLogProbMetric: 28.3792, val_loss: 29.3239, val_MinusLogProbMetric: 29.3239

Epoch 628: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.3792 - MinusLogProbMetric: 28.3792 - val_loss: 29.3239 - val_MinusLogProbMetric: 29.3239 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 629/1000
2023-10-11 20:15:37.664 
Epoch 629/1000 
	 loss: 28.3546, MinusLogProbMetric: 28.3546, val_loss: 29.3875, val_MinusLogProbMetric: 29.3875

Epoch 629: val_loss did not improve from 28.88165
196/196 - 27s - loss: 28.3546 - MinusLogProbMetric: 28.3546 - val_loss: 29.3875 - val_MinusLogProbMetric: 29.3875 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 630/1000
2023-10-11 20:16:04.091 
Epoch 630/1000 
	 loss: 28.4115, MinusLogProbMetric: 28.4115, val_loss: 29.4582, val_MinusLogProbMetric: 29.4582

Epoch 630: val_loss did not improve from 28.88165
196/196 - 26s - loss: 28.4115 - MinusLogProbMetric: 28.4115 - val_loss: 29.4582 - val_MinusLogProbMetric: 29.4582 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 631/1000
2023-10-11 20:16:30.615 
Epoch 631/1000 
	 loss: 28.3571, MinusLogProbMetric: 28.3571, val_loss: 28.7978, val_MinusLogProbMetric: 28.7978

Epoch 631: val_loss improved from 28.88165 to 28.79780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 28.3571 - MinusLogProbMetric: 28.3571 - val_loss: 28.7978 - val_MinusLogProbMetric: 28.7978 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 632/1000
2023-10-11 20:16:57.445 
Epoch 632/1000 
	 loss: 28.3691, MinusLogProbMetric: 28.3691, val_loss: 29.1054, val_MinusLogProbMetric: 29.1054

Epoch 632: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3691 - MinusLogProbMetric: 28.3691 - val_loss: 29.1054 - val_MinusLogProbMetric: 29.1054 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 633/1000
2023-10-11 20:17:23.935 
Epoch 633/1000 
	 loss: 28.3150, MinusLogProbMetric: 28.3150, val_loss: 29.5225, val_MinusLogProbMetric: 29.5225

Epoch 633: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3150 - MinusLogProbMetric: 28.3150 - val_loss: 29.5225 - val_MinusLogProbMetric: 29.5225 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 634/1000
2023-10-11 20:17:50.553 
Epoch 634/1000 
	 loss: 28.5299, MinusLogProbMetric: 28.5299, val_loss: 29.5980, val_MinusLogProbMetric: 29.5980

Epoch 634: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.5299 - MinusLogProbMetric: 28.5299 - val_loss: 29.5980 - val_MinusLogProbMetric: 29.5980 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 635/1000
2023-10-11 20:18:16.761 
Epoch 635/1000 
	 loss: 28.3445, MinusLogProbMetric: 28.3445, val_loss: 29.0853, val_MinusLogProbMetric: 29.0853

Epoch 635: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3445 - MinusLogProbMetric: 28.3445 - val_loss: 29.0853 - val_MinusLogProbMetric: 29.0853 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 636/1000
2023-10-11 20:18:43.855 
Epoch 636/1000 
	 loss: 28.4797, MinusLogProbMetric: 28.4797, val_loss: 28.9758, val_MinusLogProbMetric: 28.9758

Epoch 636: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.4797 - MinusLogProbMetric: 28.4797 - val_loss: 28.9758 - val_MinusLogProbMetric: 28.9758 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 637/1000
2023-10-11 20:19:10.094 
Epoch 637/1000 
	 loss: 28.2559, MinusLogProbMetric: 28.2559, val_loss: 29.8091, val_MinusLogProbMetric: 29.8091

Epoch 637: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.2559 - MinusLogProbMetric: 28.2559 - val_loss: 29.8091 - val_MinusLogProbMetric: 29.8091 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 638/1000
2023-10-11 20:19:37.031 
Epoch 638/1000 
	 loss: 28.4449, MinusLogProbMetric: 28.4449, val_loss: 28.8617, val_MinusLogProbMetric: 28.8617

Epoch 638: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.4449 - MinusLogProbMetric: 28.4449 - val_loss: 28.8617 - val_MinusLogProbMetric: 28.8617 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 639/1000
2023-10-11 20:20:03.269 
Epoch 639/1000 
	 loss: 28.4161, MinusLogProbMetric: 28.4161, val_loss: 29.1065, val_MinusLogProbMetric: 29.1065

Epoch 639: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.4161 - MinusLogProbMetric: 28.4161 - val_loss: 29.1065 - val_MinusLogProbMetric: 29.1065 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 640/1000
2023-10-11 20:20:30.218 
Epoch 640/1000 
	 loss: 28.3669, MinusLogProbMetric: 28.3669, val_loss: 29.1107, val_MinusLogProbMetric: 29.1107

Epoch 640: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3669 - MinusLogProbMetric: 28.3669 - val_loss: 29.1107 - val_MinusLogProbMetric: 29.1107 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 641/1000
2023-10-11 20:20:57.042 
Epoch 641/1000 
	 loss: 28.3910, MinusLogProbMetric: 28.3910, val_loss: 29.2405, val_MinusLogProbMetric: 29.2405

Epoch 641: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3910 - MinusLogProbMetric: 28.3910 - val_loss: 29.2405 - val_MinusLogProbMetric: 29.2405 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 642/1000
2023-10-11 20:21:23.706 
Epoch 642/1000 
	 loss: 28.2825, MinusLogProbMetric: 28.2825, val_loss: 29.3784, val_MinusLogProbMetric: 29.3784

Epoch 642: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.2825 - MinusLogProbMetric: 28.2825 - val_loss: 29.3784 - val_MinusLogProbMetric: 29.3784 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 643/1000
2023-10-11 20:21:50.885 
Epoch 643/1000 
	 loss: 28.3352, MinusLogProbMetric: 28.3352, val_loss: 29.1474, val_MinusLogProbMetric: 29.1474

Epoch 643: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3352 - MinusLogProbMetric: 28.3352 - val_loss: 29.1474 - val_MinusLogProbMetric: 29.1474 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 644/1000
2023-10-11 20:22:17.264 
Epoch 644/1000 
	 loss: 28.3790, MinusLogProbMetric: 28.3790, val_loss: 29.3632, val_MinusLogProbMetric: 29.3632

Epoch 644: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3790 - MinusLogProbMetric: 28.3790 - val_loss: 29.3632 - val_MinusLogProbMetric: 29.3632 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 645/1000
2023-10-11 20:22:44.962 
Epoch 645/1000 
	 loss: 28.4631, MinusLogProbMetric: 28.4631, val_loss: 29.0012, val_MinusLogProbMetric: 29.0012

Epoch 645: val_loss did not improve from 28.79780
196/196 - 28s - loss: 28.4631 - MinusLogProbMetric: 28.4631 - val_loss: 29.0012 - val_MinusLogProbMetric: 29.0012 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 646/1000
2023-10-11 20:23:11.598 
Epoch 646/1000 
	 loss: 28.2682, MinusLogProbMetric: 28.2682, val_loss: 29.1933, val_MinusLogProbMetric: 29.1933

Epoch 646: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.2682 - MinusLogProbMetric: 28.2682 - val_loss: 29.1933 - val_MinusLogProbMetric: 29.1933 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 647/1000
2023-10-11 20:23:38.007 
Epoch 647/1000 
	 loss: 28.3869, MinusLogProbMetric: 28.3869, val_loss: 29.4596, val_MinusLogProbMetric: 29.4596

Epoch 647: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3869 - MinusLogProbMetric: 28.3869 - val_loss: 29.4596 - val_MinusLogProbMetric: 29.4596 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 648/1000
2023-10-11 20:24:05.004 
Epoch 648/1000 
	 loss: 28.2601, MinusLogProbMetric: 28.2601, val_loss: 29.1485, val_MinusLogProbMetric: 29.1485

Epoch 648: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.2601 - MinusLogProbMetric: 28.2601 - val_loss: 29.1485 - val_MinusLogProbMetric: 29.1485 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 649/1000
2023-10-11 20:24:31.768 
Epoch 649/1000 
	 loss: 28.4098, MinusLogProbMetric: 28.4098, val_loss: 30.0618, val_MinusLogProbMetric: 30.0618

Epoch 649: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.4098 - MinusLogProbMetric: 28.4098 - val_loss: 30.0618 - val_MinusLogProbMetric: 30.0618 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 650/1000
2023-10-11 20:24:58.505 
Epoch 650/1000 
	 loss: 28.4304, MinusLogProbMetric: 28.4304, val_loss: 29.7719, val_MinusLogProbMetric: 29.7719

Epoch 650: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.4304 - MinusLogProbMetric: 28.4304 - val_loss: 29.7719 - val_MinusLogProbMetric: 29.7719 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 651/1000
2023-10-11 20:25:24.829 
Epoch 651/1000 
	 loss: 28.3068, MinusLogProbMetric: 28.3068, val_loss: 29.2401, val_MinusLogProbMetric: 29.2401

Epoch 651: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3068 - MinusLogProbMetric: 28.3068 - val_loss: 29.2401 - val_MinusLogProbMetric: 29.2401 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 652/1000
2023-10-11 20:25:51.229 
Epoch 652/1000 
	 loss: 28.3326, MinusLogProbMetric: 28.3326, val_loss: 29.5075, val_MinusLogProbMetric: 29.5075

Epoch 652: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3326 - MinusLogProbMetric: 28.3326 - val_loss: 29.5075 - val_MinusLogProbMetric: 29.5075 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 653/1000
2023-10-11 20:26:17.619 
Epoch 653/1000 
	 loss: 28.3540, MinusLogProbMetric: 28.3540, val_loss: 29.1021, val_MinusLogProbMetric: 29.1021

Epoch 653: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3540 - MinusLogProbMetric: 28.3540 - val_loss: 29.1021 - val_MinusLogProbMetric: 29.1021 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 654/1000
2023-10-11 20:26:43.854 
Epoch 654/1000 
	 loss: 28.4055, MinusLogProbMetric: 28.4055, val_loss: 29.2710, val_MinusLogProbMetric: 29.2710

Epoch 654: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.4055 - MinusLogProbMetric: 28.4055 - val_loss: 29.2710 - val_MinusLogProbMetric: 29.2710 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 655/1000
2023-10-11 20:27:10.754 
Epoch 655/1000 
	 loss: 28.3344, MinusLogProbMetric: 28.3344, val_loss: 28.9992, val_MinusLogProbMetric: 28.9992

Epoch 655: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3344 - MinusLogProbMetric: 28.3344 - val_loss: 28.9992 - val_MinusLogProbMetric: 28.9992 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 656/1000
2023-10-11 20:27:37.179 
Epoch 656/1000 
	 loss: 28.3443, MinusLogProbMetric: 28.3443, val_loss: 29.1418, val_MinusLogProbMetric: 29.1418

Epoch 656: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3443 - MinusLogProbMetric: 28.3443 - val_loss: 29.1418 - val_MinusLogProbMetric: 29.1418 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 657/1000
2023-10-11 20:28:03.427 
Epoch 657/1000 
	 loss: 28.3212, MinusLogProbMetric: 28.3212, val_loss: 28.9828, val_MinusLogProbMetric: 28.9828

Epoch 657: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3212 - MinusLogProbMetric: 28.3212 - val_loss: 28.9828 - val_MinusLogProbMetric: 28.9828 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 658/1000
2023-10-11 20:28:29.885 
Epoch 658/1000 
	 loss: 28.3149, MinusLogProbMetric: 28.3149, val_loss: 28.8436, val_MinusLogProbMetric: 28.8436

Epoch 658: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3149 - MinusLogProbMetric: 28.3149 - val_loss: 28.8436 - val_MinusLogProbMetric: 28.8436 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 659/1000
2023-10-11 20:28:56.246 
Epoch 659/1000 
	 loss: 28.3427, MinusLogProbMetric: 28.3427, val_loss: 29.9919, val_MinusLogProbMetric: 29.9919

Epoch 659: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3427 - MinusLogProbMetric: 28.3427 - val_loss: 29.9919 - val_MinusLogProbMetric: 29.9919 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 660/1000
2023-10-11 20:29:22.554 
Epoch 660/1000 
	 loss: 28.3577, MinusLogProbMetric: 28.3577, val_loss: 29.9221, val_MinusLogProbMetric: 29.9221

Epoch 660: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3577 - MinusLogProbMetric: 28.3577 - val_loss: 29.9221 - val_MinusLogProbMetric: 29.9221 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 661/1000
2023-10-11 20:29:49.373 
Epoch 661/1000 
	 loss: 28.3762, MinusLogProbMetric: 28.3762, val_loss: 29.0498, val_MinusLogProbMetric: 29.0498

Epoch 661: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3762 - MinusLogProbMetric: 28.3762 - val_loss: 29.0498 - val_MinusLogProbMetric: 29.0498 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 662/1000
2023-10-11 20:30:16.281 
Epoch 662/1000 
	 loss: 28.3617, MinusLogProbMetric: 28.3617, val_loss: 29.3714, val_MinusLogProbMetric: 29.3714

Epoch 662: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3617 - MinusLogProbMetric: 28.3617 - val_loss: 29.3714 - val_MinusLogProbMetric: 29.3714 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 663/1000
2023-10-11 20:30:42.815 
Epoch 663/1000 
	 loss: 28.2757, MinusLogProbMetric: 28.2757, val_loss: 31.1670, val_MinusLogProbMetric: 31.1670

Epoch 663: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.2757 - MinusLogProbMetric: 28.2757 - val_loss: 31.1670 - val_MinusLogProbMetric: 31.1670 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 664/1000
2023-10-11 20:31:09.315 
Epoch 664/1000 
	 loss: 28.4798, MinusLogProbMetric: 28.4798, val_loss: 29.3813, val_MinusLogProbMetric: 29.3813

Epoch 664: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.4798 - MinusLogProbMetric: 28.4798 - val_loss: 29.3813 - val_MinusLogProbMetric: 29.3813 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 665/1000
2023-10-11 20:31:35.569 
Epoch 665/1000 
	 loss: 28.4300, MinusLogProbMetric: 28.4300, val_loss: 29.3900, val_MinusLogProbMetric: 29.3900

Epoch 665: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.4300 - MinusLogProbMetric: 28.4300 - val_loss: 29.3900 - val_MinusLogProbMetric: 29.3900 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 666/1000
2023-10-11 20:32:02.140 
Epoch 666/1000 
	 loss: 28.4765, MinusLogProbMetric: 28.4765, val_loss: 29.5807, val_MinusLogProbMetric: 29.5807

Epoch 666: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.4765 - MinusLogProbMetric: 28.4765 - val_loss: 29.5807 - val_MinusLogProbMetric: 29.5807 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 667/1000
2023-10-11 20:32:29.039 
Epoch 667/1000 
	 loss: 28.3133, MinusLogProbMetric: 28.3133, val_loss: 29.0150, val_MinusLogProbMetric: 29.0150

Epoch 667: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3133 - MinusLogProbMetric: 28.3133 - val_loss: 29.0150 - val_MinusLogProbMetric: 29.0150 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 668/1000
2023-10-11 20:32:55.790 
Epoch 668/1000 
	 loss: 28.3065, MinusLogProbMetric: 28.3065, val_loss: 29.1913, val_MinusLogProbMetric: 29.1913

Epoch 668: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3065 - MinusLogProbMetric: 28.3065 - val_loss: 29.1913 - val_MinusLogProbMetric: 29.1913 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 669/1000
2023-10-11 20:33:22.242 
Epoch 669/1000 
	 loss: 28.2544, MinusLogProbMetric: 28.2544, val_loss: 29.2317, val_MinusLogProbMetric: 29.2317

Epoch 669: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.2544 - MinusLogProbMetric: 28.2544 - val_loss: 29.2317 - val_MinusLogProbMetric: 29.2317 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 670/1000
2023-10-11 20:33:48.986 
Epoch 670/1000 
	 loss: 28.3712, MinusLogProbMetric: 28.3712, val_loss: 29.3059, val_MinusLogProbMetric: 29.3059

Epoch 670: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3712 - MinusLogProbMetric: 28.3712 - val_loss: 29.3059 - val_MinusLogProbMetric: 29.3059 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 671/1000
2023-10-11 20:34:15.633 
Epoch 671/1000 
	 loss: 28.2582, MinusLogProbMetric: 28.2582, val_loss: 30.6106, val_MinusLogProbMetric: 30.6106

Epoch 671: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.2582 - MinusLogProbMetric: 28.2582 - val_loss: 30.6106 - val_MinusLogProbMetric: 30.6106 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 672/1000
2023-10-11 20:34:42.441 
Epoch 672/1000 
	 loss: 28.3478, MinusLogProbMetric: 28.3478, val_loss: 30.4543, val_MinusLogProbMetric: 30.4543

Epoch 672: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3478 - MinusLogProbMetric: 28.3478 - val_loss: 30.4543 - val_MinusLogProbMetric: 30.4543 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 673/1000
2023-10-11 20:35:09.230 
Epoch 673/1000 
	 loss: 28.3746, MinusLogProbMetric: 28.3746, val_loss: 29.2607, val_MinusLogProbMetric: 29.2607

Epoch 673: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3746 - MinusLogProbMetric: 28.3746 - val_loss: 29.2607 - val_MinusLogProbMetric: 29.2607 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 674/1000
2023-10-11 20:35:35.289 
Epoch 674/1000 
	 loss: 28.3306, MinusLogProbMetric: 28.3306, val_loss: 29.3355, val_MinusLogProbMetric: 29.3355

Epoch 674: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3306 - MinusLogProbMetric: 28.3306 - val_loss: 29.3355 - val_MinusLogProbMetric: 29.3355 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 675/1000
2023-10-11 20:36:01.931 
Epoch 675/1000 
	 loss: 28.5479, MinusLogProbMetric: 28.5479, val_loss: 29.4133, val_MinusLogProbMetric: 29.4133

Epoch 675: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.5479 - MinusLogProbMetric: 28.5479 - val_loss: 29.4133 - val_MinusLogProbMetric: 29.4133 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 676/1000
2023-10-11 20:36:28.214 
Epoch 676/1000 
	 loss: 28.3375, MinusLogProbMetric: 28.3375, val_loss: 28.8850, val_MinusLogProbMetric: 28.8850

Epoch 676: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3375 - MinusLogProbMetric: 28.3375 - val_loss: 28.8850 - val_MinusLogProbMetric: 28.8850 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 677/1000
2023-10-11 20:36:55.248 
Epoch 677/1000 
	 loss: 28.3551, MinusLogProbMetric: 28.3551, val_loss: 29.2230, val_MinusLogProbMetric: 29.2230

Epoch 677: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3551 - MinusLogProbMetric: 28.3551 - val_loss: 29.2230 - val_MinusLogProbMetric: 29.2230 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 678/1000
2023-10-11 20:37:21.880 
Epoch 678/1000 
	 loss: 28.3121, MinusLogProbMetric: 28.3121, val_loss: 30.3503, val_MinusLogProbMetric: 30.3503

Epoch 678: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.3121 - MinusLogProbMetric: 28.3121 - val_loss: 30.3503 - val_MinusLogProbMetric: 30.3503 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 679/1000
2023-10-11 20:37:48.673 
Epoch 679/1000 
	 loss: 28.4268, MinusLogProbMetric: 28.4268, val_loss: 30.0116, val_MinusLogProbMetric: 30.0116

Epoch 679: val_loss did not improve from 28.79780
196/196 - 27s - loss: 28.4268 - MinusLogProbMetric: 28.4268 - val_loss: 30.0116 - val_MinusLogProbMetric: 30.0116 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 680/1000
2023-10-11 20:38:14.950 
Epoch 680/1000 
	 loss: 28.3581, MinusLogProbMetric: 28.3581, val_loss: 29.8678, val_MinusLogProbMetric: 29.8678

Epoch 680: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3581 - MinusLogProbMetric: 28.3581 - val_loss: 29.8678 - val_MinusLogProbMetric: 29.8678 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 681/1000
2023-10-11 20:38:41.299 
Epoch 681/1000 
	 loss: 28.3772, MinusLogProbMetric: 28.3772, val_loss: 29.1782, val_MinusLogProbMetric: 29.1782

Epoch 681: val_loss did not improve from 28.79780
196/196 - 26s - loss: 28.3772 - MinusLogProbMetric: 28.3772 - val_loss: 29.1782 - val_MinusLogProbMetric: 29.1782 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 682/1000
2023-10-11 20:39:07.782 
Epoch 682/1000 
	 loss: 27.7531, MinusLogProbMetric: 27.7531, val_loss: 28.8129, val_MinusLogProbMetric: 28.8129

Epoch 682: val_loss did not improve from 28.79780
196/196 - 26s - loss: 27.7531 - MinusLogProbMetric: 27.7531 - val_loss: 28.8129 - val_MinusLogProbMetric: 28.8129 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 683/1000
2023-10-11 20:39:34.243 
Epoch 683/1000 
	 loss: 27.7381, MinusLogProbMetric: 27.7381, val_loss: 28.7328, val_MinusLogProbMetric: 28.7328

Epoch 683: val_loss improved from 28.79780 to 28.73276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 27.7381 - MinusLogProbMetric: 27.7381 - val_loss: 28.7328 - val_MinusLogProbMetric: 28.7328 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 684/1000
2023-10-11 20:40:01.351 
Epoch 684/1000 
	 loss: 27.7213, MinusLogProbMetric: 27.7213, val_loss: 28.8470, val_MinusLogProbMetric: 28.8470

Epoch 684: val_loss did not improve from 28.73276
196/196 - 27s - loss: 27.7213 - MinusLogProbMetric: 27.7213 - val_loss: 28.8470 - val_MinusLogProbMetric: 28.8470 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 685/1000
2023-10-11 20:40:28.201 
Epoch 685/1000 
	 loss: 27.7376, MinusLogProbMetric: 27.7376, val_loss: 28.6475, val_MinusLogProbMetric: 28.6475

Epoch 685: val_loss improved from 28.73276 to 28.64745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 27.7376 - MinusLogProbMetric: 27.7376 - val_loss: 28.6475 - val_MinusLogProbMetric: 28.6475 - lr: 5.0000e-04 - 27s/epoch - 139ms/step
Epoch 686/1000
2023-10-11 20:40:54.721 
Epoch 686/1000 
	 loss: 27.7752, MinusLogProbMetric: 27.7752, val_loss: 29.0028, val_MinusLogProbMetric: 29.0028

Epoch 686: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7752 - MinusLogProbMetric: 27.7752 - val_loss: 29.0028 - val_MinusLogProbMetric: 29.0028 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 687/1000
2023-10-11 20:41:21.082 
Epoch 687/1000 
	 loss: 27.7349, MinusLogProbMetric: 27.7349, val_loss: 28.7613, val_MinusLogProbMetric: 28.7613

Epoch 687: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7349 - MinusLogProbMetric: 27.7349 - val_loss: 28.7613 - val_MinusLogProbMetric: 28.7613 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 688/1000
2023-10-11 20:41:47.549 
Epoch 688/1000 
	 loss: 27.7196, MinusLogProbMetric: 27.7196, val_loss: 28.6973, val_MinusLogProbMetric: 28.6973

Epoch 688: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7196 - MinusLogProbMetric: 27.7196 - val_loss: 28.6973 - val_MinusLogProbMetric: 28.6973 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 689/1000
2023-10-11 20:42:14.311 
Epoch 689/1000 
	 loss: 27.7705, MinusLogProbMetric: 27.7705, val_loss: 28.7714, val_MinusLogProbMetric: 28.7714

Epoch 689: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7705 - MinusLogProbMetric: 27.7705 - val_loss: 28.7714 - val_MinusLogProbMetric: 28.7714 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 690/1000
2023-10-11 20:42:40.492 
Epoch 690/1000 
	 loss: 27.7469, MinusLogProbMetric: 27.7469, val_loss: 28.7844, val_MinusLogProbMetric: 28.7844

Epoch 690: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7469 - MinusLogProbMetric: 27.7469 - val_loss: 28.7844 - val_MinusLogProbMetric: 28.7844 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 691/1000
2023-10-11 20:43:07.267 
Epoch 691/1000 
	 loss: 27.7603, MinusLogProbMetric: 27.7603, val_loss: 28.8083, val_MinusLogProbMetric: 28.8083

Epoch 691: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7603 - MinusLogProbMetric: 27.7603 - val_loss: 28.8083 - val_MinusLogProbMetric: 28.8083 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 692/1000
2023-10-11 20:43:33.723 
Epoch 692/1000 
	 loss: 27.7151, MinusLogProbMetric: 27.7151, val_loss: 29.1748, val_MinusLogProbMetric: 29.1748

Epoch 692: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7151 - MinusLogProbMetric: 27.7151 - val_loss: 29.1748 - val_MinusLogProbMetric: 29.1748 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 693/1000
2023-10-11 20:43:59.703 
Epoch 693/1000 
	 loss: 27.7553, MinusLogProbMetric: 27.7553, val_loss: 28.8460, val_MinusLogProbMetric: 28.8460

Epoch 693: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7553 - MinusLogProbMetric: 27.7553 - val_loss: 28.8460 - val_MinusLogProbMetric: 28.8460 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 694/1000
2023-10-11 20:44:26.244 
Epoch 694/1000 
	 loss: 27.7245, MinusLogProbMetric: 27.7245, val_loss: 28.8689, val_MinusLogProbMetric: 28.8689

Epoch 694: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7245 - MinusLogProbMetric: 27.7245 - val_loss: 28.8689 - val_MinusLogProbMetric: 28.8689 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 695/1000
2023-10-11 20:44:52.567 
Epoch 695/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 28.8385, val_MinusLogProbMetric: 28.8385

Epoch 695: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 28.8385 - val_MinusLogProbMetric: 28.8385 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 696/1000
2023-10-11 20:45:19.351 
Epoch 696/1000 
	 loss: 27.7196, MinusLogProbMetric: 27.7196, val_loss: 28.9253, val_MinusLogProbMetric: 28.9253

Epoch 696: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7196 - MinusLogProbMetric: 27.7196 - val_loss: 28.9253 - val_MinusLogProbMetric: 28.9253 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 697/1000
2023-10-11 20:45:45.655 
Epoch 697/1000 
	 loss: 27.7256, MinusLogProbMetric: 27.7256, val_loss: 28.8445, val_MinusLogProbMetric: 28.8445

Epoch 697: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7256 - MinusLogProbMetric: 27.7256 - val_loss: 28.8445 - val_MinusLogProbMetric: 28.8445 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 698/1000
2023-10-11 20:46:12.051 
Epoch 698/1000 
	 loss: 27.7811, MinusLogProbMetric: 27.7811, val_loss: 28.7998, val_MinusLogProbMetric: 28.7998

Epoch 698: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7811 - MinusLogProbMetric: 27.7811 - val_loss: 28.7998 - val_MinusLogProbMetric: 28.7998 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 699/1000
2023-10-11 20:46:38.407 
Epoch 699/1000 
	 loss: 27.7421, MinusLogProbMetric: 27.7421, val_loss: 28.8719, val_MinusLogProbMetric: 28.8719

Epoch 699: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7421 - MinusLogProbMetric: 27.7421 - val_loss: 28.8719 - val_MinusLogProbMetric: 28.8719 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 700/1000
2023-10-11 20:47:05.214 
Epoch 700/1000 
	 loss: 27.7341, MinusLogProbMetric: 27.7341, val_loss: 28.7024, val_MinusLogProbMetric: 28.7024

Epoch 700: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7341 - MinusLogProbMetric: 27.7341 - val_loss: 28.7024 - val_MinusLogProbMetric: 28.7024 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 701/1000
2023-10-11 20:47:31.944 
Epoch 701/1000 
	 loss: 27.7438, MinusLogProbMetric: 27.7438, val_loss: 28.9239, val_MinusLogProbMetric: 28.9239

Epoch 701: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7438 - MinusLogProbMetric: 27.7438 - val_loss: 28.9239 - val_MinusLogProbMetric: 28.9239 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 702/1000
2023-10-11 20:47:58.225 
Epoch 702/1000 
	 loss: 27.7533, MinusLogProbMetric: 27.7533, val_loss: 28.6576, val_MinusLogProbMetric: 28.6576

Epoch 702: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7533 - MinusLogProbMetric: 27.7533 - val_loss: 28.6576 - val_MinusLogProbMetric: 28.6576 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 703/1000
2023-10-11 20:48:25.188 
Epoch 703/1000 
	 loss: 27.7597, MinusLogProbMetric: 27.7597, val_loss: 28.7615, val_MinusLogProbMetric: 28.7615

Epoch 703: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7597 - MinusLogProbMetric: 27.7597 - val_loss: 28.7615 - val_MinusLogProbMetric: 28.7615 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 704/1000
2023-10-11 20:48:51.962 
Epoch 704/1000 
	 loss: 27.7656, MinusLogProbMetric: 27.7656, val_loss: 29.3314, val_MinusLogProbMetric: 29.3314

Epoch 704: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7656 - MinusLogProbMetric: 27.7656 - val_loss: 29.3314 - val_MinusLogProbMetric: 29.3314 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 705/1000
2023-10-11 20:49:18.794 
Epoch 705/1000 
	 loss: 27.7332, MinusLogProbMetric: 27.7332, val_loss: 28.8734, val_MinusLogProbMetric: 28.8734

Epoch 705: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7332 - MinusLogProbMetric: 27.7332 - val_loss: 28.8734 - val_MinusLogProbMetric: 28.8734 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 706/1000
2023-10-11 20:49:46.397 
Epoch 706/1000 
	 loss: 27.7121, MinusLogProbMetric: 27.7121, val_loss: 28.6533, val_MinusLogProbMetric: 28.6533

Epoch 706: val_loss did not improve from 28.64745
196/196 - 28s - loss: 27.7121 - MinusLogProbMetric: 27.7121 - val_loss: 28.6533 - val_MinusLogProbMetric: 28.6533 - lr: 5.0000e-04 - 28s/epoch - 141ms/step
Epoch 707/1000
2023-10-11 20:50:13.075 
Epoch 707/1000 
	 loss: 27.7463, MinusLogProbMetric: 27.7463, val_loss: 28.7968, val_MinusLogProbMetric: 28.7968

Epoch 707: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7463 - MinusLogProbMetric: 27.7463 - val_loss: 28.7968 - val_MinusLogProbMetric: 28.7968 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 708/1000
2023-10-11 20:50:39.671 
Epoch 708/1000 
	 loss: 27.7133, MinusLogProbMetric: 27.7133, val_loss: 28.9580, val_MinusLogProbMetric: 28.9580

Epoch 708: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7133 - MinusLogProbMetric: 27.7133 - val_loss: 28.9580 - val_MinusLogProbMetric: 28.9580 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 709/1000
2023-10-11 20:51:05.977 
Epoch 709/1000 
	 loss: 27.8351, MinusLogProbMetric: 27.8351, val_loss: 28.7912, val_MinusLogProbMetric: 28.7912

Epoch 709: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.8351 - MinusLogProbMetric: 27.8351 - val_loss: 28.7912 - val_MinusLogProbMetric: 28.7912 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 710/1000
2023-10-11 20:51:32.536 
Epoch 710/1000 
	 loss: 27.7191, MinusLogProbMetric: 27.7191, val_loss: 28.8058, val_MinusLogProbMetric: 28.8058

Epoch 710: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7191 - MinusLogProbMetric: 27.7191 - val_loss: 28.8058 - val_MinusLogProbMetric: 28.8058 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 711/1000
2023-10-11 20:51:59.142 
Epoch 711/1000 
	 loss: 27.7343, MinusLogProbMetric: 27.7343, val_loss: 28.8335, val_MinusLogProbMetric: 28.8335

Epoch 711: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7343 - MinusLogProbMetric: 27.7343 - val_loss: 28.8335 - val_MinusLogProbMetric: 28.8335 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 712/1000
2023-10-11 20:52:26.066 
Epoch 712/1000 
	 loss: 27.7525, MinusLogProbMetric: 27.7525, val_loss: 28.9771, val_MinusLogProbMetric: 28.9771

Epoch 712: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7525 - MinusLogProbMetric: 27.7525 - val_loss: 28.9771 - val_MinusLogProbMetric: 28.9771 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 713/1000
2023-10-11 20:52:52.990 
Epoch 713/1000 
	 loss: 27.7540, MinusLogProbMetric: 27.7540, val_loss: 28.8826, val_MinusLogProbMetric: 28.8826

Epoch 713: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7540 - MinusLogProbMetric: 27.7540 - val_loss: 28.8826 - val_MinusLogProbMetric: 28.8826 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 714/1000
2023-10-11 20:53:19.268 
Epoch 714/1000 
	 loss: 27.7408, MinusLogProbMetric: 27.7408, val_loss: 28.8223, val_MinusLogProbMetric: 28.8223

Epoch 714: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7408 - MinusLogProbMetric: 27.7408 - val_loss: 28.8223 - val_MinusLogProbMetric: 28.8223 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 715/1000
2023-10-11 20:53:45.777 
Epoch 715/1000 
	 loss: 27.7400, MinusLogProbMetric: 27.7400, val_loss: 28.7856, val_MinusLogProbMetric: 28.7856

Epoch 715: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7400 - MinusLogProbMetric: 27.7400 - val_loss: 28.7856 - val_MinusLogProbMetric: 28.7856 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 716/1000
2023-10-11 20:54:12.823 
Epoch 716/1000 
	 loss: 27.7080, MinusLogProbMetric: 27.7080, val_loss: 28.8293, val_MinusLogProbMetric: 28.8293

Epoch 716: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7080 - MinusLogProbMetric: 27.7080 - val_loss: 28.8293 - val_MinusLogProbMetric: 28.8293 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 717/1000
2023-10-11 20:54:39.554 
Epoch 717/1000 
	 loss: 27.7249, MinusLogProbMetric: 27.7249, val_loss: 29.2780, val_MinusLogProbMetric: 29.2780

Epoch 717: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7249 - MinusLogProbMetric: 27.7249 - val_loss: 29.2780 - val_MinusLogProbMetric: 29.2780 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 718/1000
2023-10-11 20:55:05.787 
Epoch 718/1000 
	 loss: 27.7746, MinusLogProbMetric: 27.7746, val_loss: 28.8373, val_MinusLogProbMetric: 28.8373

Epoch 718: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7746 - MinusLogProbMetric: 27.7746 - val_loss: 28.8373 - val_MinusLogProbMetric: 28.8373 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 719/1000
2023-10-11 20:55:32.136 
Epoch 719/1000 
	 loss: 27.7198, MinusLogProbMetric: 27.7198, val_loss: 28.8681, val_MinusLogProbMetric: 28.8681

Epoch 719: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7198 - MinusLogProbMetric: 27.7198 - val_loss: 28.8681 - val_MinusLogProbMetric: 28.8681 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 720/1000
2023-10-11 20:55:58.556 
Epoch 720/1000 
	 loss: 27.7430, MinusLogProbMetric: 27.7430, val_loss: 29.0931, val_MinusLogProbMetric: 29.0931

Epoch 720: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7430 - MinusLogProbMetric: 27.7430 - val_loss: 29.0931 - val_MinusLogProbMetric: 29.0931 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 721/1000
2023-10-11 20:56:25.042 
Epoch 721/1000 
	 loss: 27.7577, MinusLogProbMetric: 27.7577, val_loss: 28.8820, val_MinusLogProbMetric: 28.8820

Epoch 721: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7577 - MinusLogProbMetric: 27.7577 - val_loss: 28.8820 - val_MinusLogProbMetric: 28.8820 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 722/1000
2023-10-11 20:56:51.629 
Epoch 722/1000 
	 loss: 27.7106, MinusLogProbMetric: 27.7106, val_loss: 28.9239, val_MinusLogProbMetric: 28.9239

Epoch 722: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7106 - MinusLogProbMetric: 27.7106 - val_loss: 28.9239 - val_MinusLogProbMetric: 28.9239 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 723/1000
2023-10-11 20:57:17.681 
Epoch 723/1000 
	 loss: 27.7452, MinusLogProbMetric: 27.7452, val_loss: 28.9053, val_MinusLogProbMetric: 28.9053

Epoch 723: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7452 - MinusLogProbMetric: 27.7452 - val_loss: 28.9053 - val_MinusLogProbMetric: 28.9053 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 724/1000
2023-10-11 20:57:44.021 
Epoch 724/1000 
	 loss: 27.7347, MinusLogProbMetric: 27.7347, val_loss: 28.7399, val_MinusLogProbMetric: 28.7399

Epoch 724: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7347 - MinusLogProbMetric: 27.7347 - val_loss: 28.7399 - val_MinusLogProbMetric: 28.7399 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 725/1000
2023-10-11 20:58:10.484 
Epoch 725/1000 
	 loss: 27.7560, MinusLogProbMetric: 27.7560, val_loss: 28.9309, val_MinusLogProbMetric: 28.9309

Epoch 725: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7560 - MinusLogProbMetric: 27.7560 - val_loss: 28.9309 - val_MinusLogProbMetric: 28.9309 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 726/1000
2023-10-11 20:58:37.314 
Epoch 726/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.7661, val_MinusLogProbMetric: 28.7661

Epoch 726: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.7661 - val_MinusLogProbMetric: 28.7661 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 727/1000
2023-10-11 20:59:03.517 
Epoch 727/1000 
	 loss: 27.7254, MinusLogProbMetric: 27.7254, val_loss: 28.6521, val_MinusLogProbMetric: 28.6521

Epoch 727: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7254 - MinusLogProbMetric: 27.7254 - val_loss: 28.6521 - val_MinusLogProbMetric: 28.6521 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 728/1000
2023-10-11 20:59:29.674 
Epoch 728/1000 
	 loss: 27.6805, MinusLogProbMetric: 27.6805, val_loss: 28.8265, val_MinusLogProbMetric: 28.8265

Epoch 728: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.6805 - MinusLogProbMetric: 27.6805 - val_loss: 28.8265 - val_MinusLogProbMetric: 28.8265 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 729/1000
2023-10-11 20:59:56.064 
Epoch 729/1000 
	 loss: 27.7502, MinusLogProbMetric: 27.7502, val_loss: 28.7710, val_MinusLogProbMetric: 28.7710

Epoch 729: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7502 - MinusLogProbMetric: 27.7502 - val_loss: 28.7710 - val_MinusLogProbMetric: 28.7710 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 730/1000
2023-10-11 21:00:23.019 
Epoch 730/1000 
	 loss: 27.7319, MinusLogProbMetric: 27.7319, val_loss: 29.0210, val_MinusLogProbMetric: 29.0210

Epoch 730: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7319 - MinusLogProbMetric: 27.7319 - val_loss: 29.0210 - val_MinusLogProbMetric: 29.0210 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 731/1000
2023-10-11 21:00:49.725 
Epoch 731/1000 
	 loss: 27.7085, MinusLogProbMetric: 27.7085, val_loss: 28.7212, val_MinusLogProbMetric: 28.7212

Epoch 731: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7085 - MinusLogProbMetric: 27.7085 - val_loss: 28.7212 - val_MinusLogProbMetric: 28.7212 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 732/1000
2023-10-11 21:01:15.929 
Epoch 732/1000 
	 loss: 27.6752, MinusLogProbMetric: 27.6752, val_loss: 28.7681, val_MinusLogProbMetric: 28.7681

Epoch 732: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.6752 - MinusLogProbMetric: 27.6752 - val_loss: 28.7681 - val_MinusLogProbMetric: 28.7681 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 733/1000
2023-10-11 21:01:42.328 
Epoch 733/1000 
	 loss: 27.7198, MinusLogProbMetric: 27.7198, val_loss: 28.8967, val_MinusLogProbMetric: 28.8967

Epoch 733: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7198 - MinusLogProbMetric: 27.7198 - val_loss: 28.8967 - val_MinusLogProbMetric: 28.8967 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 734/1000
2023-10-11 21:02:08.634 
Epoch 734/1000 
	 loss: 27.7193, MinusLogProbMetric: 27.7193, val_loss: 28.7067, val_MinusLogProbMetric: 28.7067

Epoch 734: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.7193 - MinusLogProbMetric: 27.7193 - val_loss: 28.7067 - val_MinusLogProbMetric: 28.7067 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 735/1000
2023-10-11 21:02:35.483 
Epoch 735/1000 
	 loss: 27.7679, MinusLogProbMetric: 27.7679, val_loss: 28.8154, val_MinusLogProbMetric: 28.8154

Epoch 735: val_loss did not improve from 28.64745
196/196 - 27s - loss: 27.7679 - MinusLogProbMetric: 27.7679 - val_loss: 28.8154 - val_MinusLogProbMetric: 28.8154 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 736/1000
2023-10-11 21:03:01.977 
Epoch 736/1000 
	 loss: 27.5313, MinusLogProbMetric: 27.5313, val_loss: 28.7285, val_MinusLogProbMetric: 28.7285

Epoch 736: val_loss did not improve from 28.64745
196/196 - 26s - loss: 27.5313 - MinusLogProbMetric: 27.5313 - val_loss: 28.7285 - val_MinusLogProbMetric: 28.7285 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 737/1000
2023-10-11 21:03:28.207 
Epoch 737/1000 
	 loss: 27.5046, MinusLogProbMetric: 27.5046, val_loss: 28.5150, val_MinusLogProbMetric: 28.5150

Epoch 737: val_loss improved from 28.64745 to 28.51497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_322/weights/best_weights.h5
196/196 - 27s - loss: 27.5046 - MinusLogProbMetric: 27.5046 - val_loss: 28.5150 - val_MinusLogProbMetric: 28.5150 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 738/1000
2023-10-11 21:03:54.969 
Epoch 738/1000 
	 loss: 27.5011, MinusLogProbMetric: 27.5011, val_loss: 28.8111, val_MinusLogProbMetric: 28.8111

Epoch 738: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5011 - MinusLogProbMetric: 27.5011 - val_loss: 28.8111 - val_MinusLogProbMetric: 28.8111 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 739/1000
2023-10-11 21:04:21.364 
Epoch 739/1000 
	 loss: 27.5109, MinusLogProbMetric: 27.5109, val_loss: 28.8587, val_MinusLogProbMetric: 28.8587

Epoch 739: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5109 - MinusLogProbMetric: 27.5109 - val_loss: 28.8587 - val_MinusLogProbMetric: 28.8587 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 740/1000
2023-10-11 21:04:47.773 
Epoch 740/1000 
	 loss: 27.5092, MinusLogProbMetric: 27.5092, val_loss: 28.7992, val_MinusLogProbMetric: 28.7992

Epoch 740: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5092 - MinusLogProbMetric: 27.5092 - val_loss: 28.7992 - val_MinusLogProbMetric: 28.7992 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 741/1000
2023-10-11 21:05:14.760 
Epoch 741/1000 
	 loss: 27.5137, MinusLogProbMetric: 27.5137, val_loss: 28.8116, val_MinusLogProbMetric: 28.8116

Epoch 741: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.5137 - MinusLogProbMetric: 27.5137 - val_loss: 28.8116 - val_MinusLogProbMetric: 28.8116 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 742/1000
2023-10-11 21:05:41.127 
Epoch 742/1000 
	 loss: 27.5118, MinusLogProbMetric: 27.5118, val_loss: 28.8124, val_MinusLogProbMetric: 28.8124

Epoch 742: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5118 - MinusLogProbMetric: 27.5118 - val_loss: 28.8124 - val_MinusLogProbMetric: 28.8124 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 743/1000
2023-10-11 21:06:07.408 
Epoch 743/1000 
	 loss: 27.4974, MinusLogProbMetric: 27.4974, val_loss: 28.7523, val_MinusLogProbMetric: 28.7523

Epoch 743: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4974 - MinusLogProbMetric: 27.4974 - val_loss: 28.7523 - val_MinusLogProbMetric: 28.7523 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 744/1000
2023-10-11 21:06:34.036 
Epoch 744/1000 
	 loss: 27.5169, MinusLogProbMetric: 27.5169, val_loss: 28.7688, val_MinusLogProbMetric: 28.7688

Epoch 744: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.5169 - MinusLogProbMetric: 27.5169 - val_loss: 28.7688 - val_MinusLogProbMetric: 28.7688 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 745/1000
2023-10-11 21:07:00.281 
Epoch 745/1000 
	 loss: 27.4987, MinusLogProbMetric: 27.4987, val_loss: 28.7354, val_MinusLogProbMetric: 28.7354

Epoch 745: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4987 - MinusLogProbMetric: 27.4987 - val_loss: 28.7354 - val_MinusLogProbMetric: 28.7354 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 746/1000
2023-10-11 21:07:26.227 
Epoch 746/1000 
	 loss: 27.4986, MinusLogProbMetric: 27.4986, val_loss: 28.7388, val_MinusLogProbMetric: 28.7388

Epoch 746: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4986 - MinusLogProbMetric: 27.4986 - val_loss: 28.7388 - val_MinusLogProbMetric: 28.7388 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 747/1000
2023-10-11 21:07:52.312 
Epoch 747/1000 
	 loss: 27.4834, MinusLogProbMetric: 27.4834, val_loss: 28.7791, val_MinusLogProbMetric: 28.7791

Epoch 747: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4834 - MinusLogProbMetric: 27.4834 - val_loss: 28.7791 - val_MinusLogProbMetric: 28.7791 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 748/1000
2023-10-11 21:08:18.442 
Epoch 748/1000 
	 loss: 27.5048, MinusLogProbMetric: 27.5048, val_loss: 28.7579, val_MinusLogProbMetric: 28.7579

Epoch 748: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5048 - MinusLogProbMetric: 27.5048 - val_loss: 28.7579 - val_MinusLogProbMetric: 28.7579 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 749/1000
2023-10-11 21:08:44.868 
Epoch 749/1000 
	 loss: 27.5057, MinusLogProbMetric: 27.5057, val_loss: 28.7432, val_MinusLogProbMetric: 28.7432

Epoch 749: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5057 - MinusLogProbMetric: 27.5057 - val_loss: 28.7432 - val_MinusLogProbMetric: 28.7432 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 750/1000
2023-10-11 21:09:11.271 
Epoch 750/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 28.7849, val_MinusLogProbMetric: 28.7849

Epoch 750: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 28.7849 - val_MinusLogProbMetric: 28.7849 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 751/1000
2023-10-11 21:09:37.252 
Epoch 751/1000 
	 loss: 27.5113, MinusLogProbMetric: 27.5113, val_loss: 28.6624, val_MinusLogProbMetric: 28.6624

Epoch 751: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5113 - MinusLogProbMetric: 27.5113 - val_loss: 28.6624 - val_MinusLogProbMetric: 28.6624 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 752/1000
2023-10-11 21:10:03.702 
Epoch 752/1000 
	 loss: 27.4939, MinusLogProbMetric: 27.4939, val_loss: 28.7561, val_MinusLogProbMetric: 28.7561

Epoch 752: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4939 - MinusLogProbMetric: 27.4939 - val_loss: 28.7561 - val_MinusLogProbMetric: 28.7561 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 753/1000
2023-10-11 21:10:30.172 
Epoch 753/1000 
	 loss: 27.5081, MinusLogProbMetric: 27.5081, val_loss: 28.8603, val_MinusLogProbMetric: 28.8603

Epoch 753: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5081 - MinusLogProbMetric: 27.5081 - val_loss: 28.8603 - val_MinusLogProbMetric: 28.8603 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 754/1000
2023-10-11 21:10:56.337 
Epoch 754/1000 
	 loss: 27.5343, MinusLogProbMetric: 27.5343, val_loss: 28.8059, val_MinusLogProbMetric: 28.8059

Epoch 754: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5343 - MinusLogProbMetric: 27.5343 - val_loss: 28.8059 - val_MinusLogProbMetric: 28.8059 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 755/1000
2023-10-11 21:11:22.688 
Epoch 755/1000 
	 loss: 27.4934, MinusLogProbMetric: 27.4934, val_loss: 28.7462, val_MinusLogProbMetric: 28.7462

Epoch 755: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4934 - MinusLogProbMetric: 27.4934 - val_loss: 28.7462 - val_MinusLogProbMetric: 28.7462 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 756/1000
2023-10-11 21:11:48.877 
Epoch 756/1000 
	 loss: 27.4947, MinusLogProbMetric: 27.4947, val_loss: 28.7864, val_MinusLogProbMetric: 28.7864

Epoch 756: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4947 - MinusLogProbMetric: 27.4947 - val_loss: 28.7864 - val_MinusLogProbMetric: 28.7864 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 757/1000
2023-10-11 21:12:14.876 
Epoch 757/1000 
	 loss: 27.4909, MinusLogProbMetric: 27.4909, val_loss: 28.7673, val_MinusLogProbMetric: 28.7673

Epoch 757: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4909 - MinusLogProbMetric: 27.4909 - val_loss: 28.7673 - val_MinusLogProbMetric: 28.7673 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 758/1000
2023-10-11 21:12:41.144 
Epoch 758/1000 
	 loss: 27.4959, MinusLogProbMetric: 27.4959, val_loss: 28.7548, val_MinusLogProbMetric: 28.7548

Epoch 758: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4959 - MinusLogProbMetric: 27.4959 - val_loss: 28.7548 - val_MinusLogProbMetric: 28.7548 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 759/1000
2023-10-11 21:13:07.098 
Epoch 759/1000 
	 loss: 27.4919, MinusLogProbMetric: 27.4919, val_loss: 28.7912, val_MinusLogProbMetric: 28.7912

Epoch 759: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4919 - MinusLogProbMetric: 27.4919 - val_loss: 28.7912 - val_MinusLogProbMetric: 28.7912 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 760/1000
2023-10-11 21:13:33.275 
Epoch 760/1000 
	 loss: 27.5165, MinusLogProbMetric: 27.5165, val_loss: 28.7871, val_MinusLogProbMetric: 28.7871

Epoch 760: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5165 - MinusLogProbMetric: 27.5165 - val_loss: 28.7871 - val_MinusLogProbMetric: 28.7871 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 761/1000
2023-10-11 21:13:59.225 
Epoch 761/1000 
	 loss: 27.4878, MinusLogProbMetric: 27.4878, val_loss: 28.7694, val_MinusLogProbMetric: 28.7694

Epoch 761: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4878 - MinusLogProbMetric: 27.4878 - val_loss: 28.7694 - val_MinusLogProbMetric: 28.7694 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 762/1000
2023-10-11 21:14:25.625 
Epoch 762/1000 
	 loss: 27.4974, MinusLogProbMetric: 27.4974, val_loss: 28.6861, val_MinusLogProbMetric: 28.6861

Epoch 762: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4974 - MinusLogProbMetric: 27.4974 - val_loss: 28.6861 - val_MinusLogProbMetric: 28.6861 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 763/1000
2023-10-11 21:14:52.326 
Epoch 763/1000 
	 loss: 27.4924, MinusLogProbMetric: 27.4924, val_loss: 28.6407, val_MinusLogProbMetric: 28.6407

Epoch 763: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4924 - MinusLogProbMetric: 27.4924 - val_loss: 28.6407 - val_MinusLogProbMetric: 28.6407 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 764/1000
2023-10-11 21:15:18.906 
Epoch 764/1000 
	 loss: 27.5002, MinusLogProbMetric: 27.5002, val_loss: 28.8330, val_MinusLogProbMetric: 28.8330

Epoch 764: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.5002 - MinusLogProbMetric: 27.5002 - val_loss: 28.8330 - val_MinusLogProbMetric: 28.8330 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 765/1000
2023-10-11 21:15:45.394 
Epoch 765/1000 
	 loss: 27.5096, MinusLogProbMetric: 27.5096, val_loss: 28.7187, val_MinusLogProbMetric: 28.7187

Epoch 765: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5096 - MinusLogProbMetric: 27.5096 - val_loss: 28.7187 - val_MinusLogProbMetric: 28.7187 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 766/1000
2023-10-11 21:16:11.653 
Epoch 766/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 28.6742, val_MinusLogProbMetric: 28.6742

Epoch 766: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 28.6742 - val_MinusLogProbMetric: 28.6742 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 767/1000
2023-10-11 21:16:37.793 
Epoch 767/1000 
	 loss: 27.4829, MinusLogProbMetric: 27.4829, val_loss: 28.7567, val_MinusLogProbMetric: 28.7567

Epoch 767: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4829 - MinusLogProbMetric: 27.4829 - val_loss: 28.7567 - val_MinusLogProbMetric: 28.7567 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 768/1000
2023-10-11 21:17:04.286 
Epoch 768/1000 
	 loss: 27.5279, MinusLogProbMetric: 27.5279, val_loss: 28.7805, val_MinusLogProbMetric: 28.7805

Epoch 768: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5279 - MinusLogProbMetric: 27.5279 - val_loss: 28.7805 - val_MinusLogProbMetric: 28.7805 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 769/1000
2023-10-11 21:17:30.652 
Epoch 769/1000 
	 loss: 27.4904, MinusLogProbMetric: 27.4904, val_loss: 28.9079, val_MinusLogProbMetric: 28.9079

Epoch 769: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4904 - MinusLogProbMetric: 27.4904 - val_loss: 28.9079 - val_MinusLogProbMetric: 28.9079 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 770/1000
2023-10-11 21:17:57.004 
Epoch 770/1000 
	 loss: 27.5047, MinusLogProbMetric: 27.5047, val_loss: 28.8345, val_MinusLogProbMetric: 28.8345

Epoch 770: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5047 - MinusLogProbMetric: 27.5047 - val_loss: 28.8345 - val_MinusLogProbMetric: 28.8345 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 771/1000
2023-10-11 21:18:23.317 
Epoch 771/1000 
	 loss: 27.4880, MinusLogProbMetric: 27.4880, val_loss: 28.8078, val_MinusLogProbMetric: 28.8078

Epoch 771: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4880 - MinusLogProbMetric: 27.4880 - val_loss: 28.8078 - val_MinusLogProbMetric: 28.8078 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 772/1000
2023-10-11 21:18:49.518 
Epoch 772/1000 
	 loss: 27.4932, MinusLogProbMetric: 27.4932, val_loss: 28.7857, val_MinusLogProbMetric: 28.7857

Epoch 772: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4932 - MinusLogProbMetric: 27.4932 - val_loss: 28.7857 - val_MinusLogProbMetric: 28.7857 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 773/1000
2023-10-11 21:19:15.740 
Epoch 773/1000 
	 loss: 27.4899, MinusLogProbMetric: 27.4899, val_loss: 28.7385, val_MinusLogProbMetric: 28.7385

Epoch 773: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4899 - MinusLogProbMetric: 27.4899 - val_loss: 28.7385 - val_MinusLogProbMetric: 28.7385 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 774/1000
2023-10-11 21:19:41.983 
Epoch 774/1000 
	 loss: 27.4995, MinusLogProbMetric: 27.4995, val_loss: 28.7658, val_MinusLogProbMetric: 28.7658

Epoch 774: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4995 - MinusLogProbMetric: 27.4995 - val_loss: 28.7658 - val_MinusLogProbMetric: 28.7658 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 775/1000
2023-10-11 21:20:08.343 
Epoch 775/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.7398, val_MinusLogProbMetric: 28.7398

Epoch 775: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.7398 - val_MinusLogProbMetric: 28.7398 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 776/1000
2023-10-11 21:20:34.907 
Epoch 776/1000 
	 loss: 27.5252, MinusLogProbMetric: 27.5252, val_loss: 28.7295, val_MinusLogProbMetric: 28.7295

Epoch 776: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.5252 - MinusLogProbMetric: 27.5252 - val_loss: 28.7295 - val_MinusLogProbMetric: 28.7295 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 777/1000
2023-10-11 21:21:01.409 
Epoch 777/1000 
	 loss: 27.4928, MinusLogProbMetric: 27.4928, val_loss: 28.7821, val_MinusLogProbMetric: 28.7821

Epoch 777: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4928 - MinusLogProbMetric: 27.4928 - val_loss: 28.7821 - val_MinusLogProbMetric: 28.7821 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 778/1000
2023-10-11 21:21:27.532 
Epoch 778/1000 
	 loss: 27.4872, MinusLogProbMetric: 27.4872, val_loss: 28.8503, val_MinusLogProbMetric: 28.8503

Epoch 778: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4872 - MinusLogProbMetric: 27.4872 - val_loss: 28.8503 - val_MinusLogProbMetric: 28.8503 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 779/1000
2023-10-11 21:21:53.769 
Epoch 779/1000 
	 loss: 27.4904, MinusLogProbMetric: 27.4904, val_loss: 28.8559, val_MinusLogProbMetric: 28.8559

Epoch 779: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4904 - MinusLogProbMetric: 27.4904 - val_loss: 28.8559 - val_MinusLogProbMetric: 28.8559 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 780/1000
2023-10-11 21:22:20.655 
Epoch 780/1000 
	 loss: 27.4931, MinusLogProbMetric: 27.4931, val_loss: 28.7467, val_MinusLogProbMetric: 28.7467

Epoch 780: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4931 - MinusLogProbMetric: 27.4931 - val_loss: 28.7467 - val_MinusLogProbMetric: 28.7467 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 781/1000
2023-10-11 21:22:47.095 
Epoch 781/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.7306, val_MinusLogProbMetric: 28.7306

Epoch 781: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.7306 - val_MinusLogProbMetric: 28.7306 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 782/1000
2023-10-11 21:23:13.313 
Epoch 782/1000 
	 loss: 27.4912, MinusLogProbMetric: 27.4912, val_loss: 28.7415, val_MinusLogProbMetric: 28.7415

Epoch 782: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4912 - MinusLogProbMetric: 27.4912 - val_loss: 28.7415 - val_MinusLogProbMetric: 28.7415 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 783/1000
2023-10-11 21:23:39.412 
Epoch 783/1000 
	 loss: 27.5013, MinusLogProbMetric: 27.5013, val_loss: 28.7917, val_MinusLogProbMetric: 28.7917

Epoch 783: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.5013 - MinusLogProbMetric: 27.5013 - val_loss: 28.7917 - val_MinusLogProbMetric: 28.7917 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 784/1000
2023-10-11 21:24:05.342 
Epoch 784/1000 
	 loss: 27.4872, MinusLogProbMetric: 27.4872, val_loss: 28.7889, val_MinusLogProbMetric: 28.7889

Epoch 784: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4872 - MinusLogProbMetric: 27.4872 - val_loss: 28.7889 - val_MinusLogProbMetric: 28.7889 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 785/1000
2023-10-11 21:24:31.607 
Epoch 785/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 28.7524, val_MinusLogProbMetric: 28.7524

Epoch 785: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 28.7524 - val_MinusLogProbMetric: 28.7524 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 786/1000
2023-10-11 21:24:58.137 
Epoch 786/1000 
	 loss: 27.4985, MinusLogProbMetric: 27.4985, val_loss: 28.7640, val_MinusLogProbMetric: 28.7640

Epoch 786: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4985 - MinusLogProbMetric: 27.4985 - val_loss: 28.7640 - val_MinusLogProbMetric: 28.7640 - lr: 2.5000e-04 - 27s/epoch - 135ms/step
Epoch 787/1000
2023-10-11 21:25:24.876 
Epoch 787/1000 
	 loss: 27.4839, MinusLogProbMetric: 27.4839, val_loss: 28.7927, val_MinusLogProbMetric: 28.7927

Epoch 787: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4839 - MinusLogProbMetric: 27.4839 - val_loss: 28.7927 - val_MinusLogProbMetric: 28.7927 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 788/1000
2023-10-11 21:25:50.659 
Epoch 788/1000 
	 loss: 27.4183, MinusLogProbMetric: 27.4183, val_loss: 28.8039, val_MinusLogProbMetric: 28.8039

Epoch 788: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4183 - MinusLogProbMetric: 27.4183 - val_loss: 28.8039 - val_MinusLogProbMetric: 28.8039 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 789/1000
2023-10-11 21:26:16.606 
Epoch 789/1000 
	 loss: 27.4116, MinusLogProbMetric: 27.4116, val_loss: 28.7665, val_MinusLogProbMetric: 28.7665

Epoch 789: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4116 - MinusLogProbMetric: 27.4116 - val_loss: 28.7665 - val_MinusLogProbMetric: 28.7665 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 790/1000
2023-10-11 21:26:42.869 
Epoch 790/1000 
	 loss: 27.4046, MinusLogProbMetric: 27.4046, val_loss: 28.7708, val_MinusLogProbMetric: 28.7708

Epoch 790: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4046 - MinusLogProbMetric: 27.4046 - val_loss: 28.7708 - val_MinusLogProbMetric: 28.7708 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 791/1000
2023-10-11 21:27:09.323 
Epoch 791/1000 
	 loss: 27.4135, MinusLogProbMetric: 27.4135, val_loss: 28.7518, val_MinusLogProbMetric: 28.7518

Epoch 791: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4135 - MinusLogProbMetric: 27.4135 - val_loss: 28.7518 - val_MinusLogProbMetric: 28.7518 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 792/1000
2023-10-11 21:27:35.757 
Epoch 792/1000 
	 loss: 27.4049, MinusLogProbMetric: 27.4049, val_loss: 28.7159, val_MinusLogProbMetric: 28.7159

Epoch 792: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4049 - MinusLogProbMetric: 27.4049 - val_loss: 28.7159 - val_MinusLogProbMetric: 28.7159 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 793/1000
2023-10-11 21:28:02.086 
Epoch 793/1000 
	 loss: 27.4079, MinusLogProbMetric: 27.4079, val_loss: 28.8129, val_MinusLogProbMetric: 28.8129

Epoch 793: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4079 - MinusLogProbMetric: 27.4079 - val_loss: 28.8129 - val_MinusLogProbMetric: 28.8129 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 794/1000
2023-10-11 21:28:28.510 
Epoch 794/1000 
	 loss: 27.3981, MinusLogProbMetric: 27.3981, val_loss: 28.7464, val_MinusLogProbMetric: 28.7464

Epoch 794: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3981 - MinusLogProbMetric: 27.3981 - val_loss: 28.7464 - val_MinusLogProbMetric: 28.7464 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 795/1000
2023-10-11 21:28:54.691 
Epoch 795/1000 
	 loss: 27.4072, MinusLogProbMetric: 27.4072, val_loss: 28.7964, val_MinusLogProbMetric: 28.7964

Epoch 795: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4072 - MinusLogProbMetric: 27.4072 - val_loss: 28.7964 - val_MinusLogProbMetric: 28.7964 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 796/1000
2023-10-11 21:29:20.904 
Epoch 796/1000 
	 loss: 27.4052, MinusLogProbMetric: 27.4052, val_loss: 28.7573, val_MinusLogProbMetric: 28.7573

Epoch 796: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4052 - MinusLogProbMetric: 27.4052 - val_loss: 28.7573 - val_MinusLogProbMetric: 28.7573 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 797/1000
2023-10-11 21:29:47.477 
Epoch 797/1000 
	 loss: 27.4072, MinusLogProbMetric: 27.4072, val_loss: 28.8458, val_MinusLogProbMetric: 28.8458

Epoch 797: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4072 - MinusLogProbMetric: 27.4072 - val_loss: 28.8458 - val_MinusLogProbMetric: 28.8458 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 798/1000
2023-10-11 21:30:13.930 
Epoch 798/1000 
	 loss: 27.4065, MinusLogProbMetric: 27.4065, val_loss: 28.7875, val_MinusLogProbMetric: 28.7875

Epoch 798: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4065 - MinusLogProbMetric: 27.4065 - val_loss: 28.7875 - val_MinusLogProbMetric: 28.7875 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 799/1000
2023-10-11 21:30:41.192 
Epoch 799/1000 
	 loss: 27.4034, MinusLogProbMetric: 27.4034, val_loss: 28.7714, val_MinusLogProbMetric: 28.7714

Epoch 799: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4034 - MinusLogProbMetric: 27.4034 - val_loss: 28.7714 - val_MinusLogProbMetric: 28.7714 - lr: 1.2500e-04 - 27s/epoch - 139ms/step
Epoch 800/1000
2023-10-11 21:31:08.244 
Epoch 800/1000 
	 loss: 27.3995, MinusLogProbMetric: 27.3995, val_loss: 28.7707, val_MinusLogProbMetric: 28.7707

Epoch 800: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.3995 - MinusLogProbMetric: 27.3995 - val_loss: 28.7707 - val_MinusLogProbMetric: 28.7707 - lr: 1.2500e-04 - 27s/epoch - 138ms/step
Epoch 801/1000
2023-10-11 21:31:35.105 
Epoch 801/1000 
	 loss: 27.4071, MinusLogProbMetric: 27.4071, val_loss: 28.7812, val_MinusLogProbMetric: 28.7812

Epoch 801: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4071 - MinusLogProbMetric: 27.4071 - val_loss: 28.7812 - val_MinusLogProbMetric: 28.7812 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 802/1000
2023-10-11 21:32:01.397 
Epoch 802/1000 
	 loss: 27.3964, MinusLogProbMetric: 27.3964, val_loss: 28.7249, val_MinusLogProbMetric: 28.7249

Epoch 802: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3964 - MinusLogProbMetric: 27.3964 - val_loss: 28.7249 - val_MinusLogProbMetric: 28.7249 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 803/1000
2023-10-11 21:32:27.809 
Epoch 803/1000 
	 loss: 27.4036, MinusLogProbMetric: 27.4036, val_loss: 28.8098, val_MinusLogProbMetric: 28.8098

Epoch 803: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4036 - MinusLogProbMetric: 27.4036 - val_loss: 28.8098 - val_MinusLogProbMetric: 28.8098 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 804/1000
2023-10-11 21:32:53.608 
Epoch 804/1000 
	 loss: 27.4034, MinusLogProbMetric: 27.4034, val_loss: 28.7886, val_MinusLogProbMetric: 28.7886

Epoch 804: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4034 - MinusLogProbMetric: 27.4034 - val_loss: 28.7886 - val_MinusLogProbMetric: 28.7886 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 805/1000
2023-10-11 21:33:19.745 
Epoch 805/1000 
	 loss: 27.4013, MinusLogProbMetric: 27.4013, val_loss: 28.8533, val_MinusLogProbMetric: 28.8533

Epoch 805: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4013 - MinusLogProbMetric: 27.4013 - val_loss: 28.8533 - val_MinusLogProbMetric: 28.8533 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 806/1000
2023-10-11 21:33:45.643 
Epoch 806/1000 
	 loss: 27.3978, MinusLogProbMetric: 27.3978, val_loss: 28.7737, val_MinusLogProbMetric: 28.7737

Epoch 806: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3978 - MinusLogProbMetric: 27.3978 - val_loss: 28.7737 - val_MinusLogProbMetric: 28.7737 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 807/1000
2023-10-11 21:34:11.611 
Epoch 807/1000 
	 loss: 27.3994, MinusLogProbMetric: 27.3994, val_loss: 28.8558, val_MinusLogProbMetric: 28.8558

Epoch 807: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3994 - MinusLogProbMetric: 27.3994 - val_loss: 28.8558 - val_MinusLogProbMetric: 28.8558 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 808/1000
2023-10-11 21:34:37.691 
Epoch 808/1000 
	 loss: 27.4045, MinusLogProbMetric: 27.4045, val_loss: 28.8282, val_MinusLogProbMetric: 28.8282

Epoch 808: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4045 - MinusLogProbMetric: 27.4045 - val_loss: 28.8282 - val_MinusLogProbMetric: 28.8282 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 809/1000
2023-10-11 21:35:03.982 
Epoch 809/1000 
	 loss: 27.4141, MinusLogProbMetric: 27.4141, val_loss: 28.7640, val_MinusLogProbMetric: 28.7640

Epoch 809: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4141 - MinusLogProbMetric: 27.4141 - val_loss: 28.7640 - val_MinusLogProbMetric: 28.7640 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 810/1000
2023-10-11 21:35:30.807 
Epoch 810/1000 
	 loss: 27.4168, MinusLogProbMetric: 27.4168, val_loss: 28.7960, val_MinusLogProbMetric: 28.7960

Epoch 810: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4168 - MinusLogProbMetric: 27.4168 - val_loss: 28.7960 - val_MinusLogProbMetric: 28.7960 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 811/1000
2023-10-11 21:35:57.482 
Epoch 811/1000 
	 loss: 27.4040, MinusLogProbMetric: 27.4040, val_loss: 28.7994, val_MinusLogProbMetric: 28.7994

Epoch 811: val_loss did not improve from 28.51497
196/196 - 27s - loss: 27.4040 - MinusLogProbMetric: 27.4040 - val_loss: 28.7994 - val_MinusLogProbMetric: 28.7994 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 812/1000
2023-10-11 21:36:23.824 
Epoch 812/1000 
	 loss: 27.4022, MinusLogProbMetric: 27.4022, val_loss: 28.8134, val_MinusLogProbMetric: 28.8134

Epoch 812: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4022 - MinusLogProbMetric: 27.4022 - val_loss: 28.8134 - val_MinusLogProbMetric: 28.8134 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 813/1000
2023-10-11 21:36:49.853 
Epoch 813/1000 
	 loss: 27.3994, MinusLogProbMetric: 27.3994, val_loss: 28.7958, val_MinusLogProbMetric: 28.7958

Epoch 813: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3994 - MinusLogProbMetric: 27.3994 - val_loss: 28.7958 - val_MinusLogProbMetric: 28.7958 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 814/1000
2023-10-11 21:37:15.856 
Epoch 814/1000 
	 loss: 27.4078, MinusLogProbMetric: 27.4078, val_loss: 28.7160, val_MinusLogProbMetric: 28.7160

Epoch 814: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4078 - MinusLogProbMetric: 27.4078 - val_loss: 28.7160 - val_MinusLogProbMetric: 28.7160 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 815/1000
2023-10-11 21:37:41.928 
Epoch 815/1000 
	 loss: 27.4023, MinusLogProbMetric: 27.4023, val_loss: 28.8495, val_MinusLogProbMetric: 28.8495

Epoch 815: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4023 - MinusLogProbMetric: 27.4023 - val_loss: 28.8495 - val_MinusLogProbMetric: 28.8495 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 816/1000
2023-10-11 21:38:07.528 
Epoch 816/1000 
	 loss: 27.3975, MinusLogProbMetric: 27.3975, val_loss: 28.8047, val_MinusLogProbMetric: 28.8047

Epoch 816: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3975 - MinusLogProbMetric: 27.3975 - val_loss: 28.8047 - val_MinusLogProbMetric: 28.8047 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 817/1000
2023-10-11 21:38:33.930 
Epoch 817/1000 
	 loss: 27.4026, MinusLogProbMetric: 27.4026, val_loss: 28.7296, val_MinusLogProbMetric: 28.7296

Epoch 817: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4026 - MinusLogProbMetric: 27.4026 - val_loss: 28.7296 - val_MinusLogProbMetric: 28.7296 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 818/1000
2023-10-11 21:39:00.266 
Epoch 818/1000 
	 loss: 27.4015, MinusLogProbMetric: 27.4015, val_loss: 28.8129, val_MinusLogProbMetric: 28.8129

Epoch 818: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4015 - MinusLogProbMetric: 27.4015 - val_loss: 28.8129 - val_MinusLogProbMetric: 28.8129 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 819/1000
2023-10-11 21:39:25.925 
Epoch 819/1000 
	 loss: 27.3973, MinusLogProbMetric: 27.3973, val_loss: 28.7528, val_MinusLogProbMetric: 28.7528

Epoch 819: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3973 - MinusLogProbMetric: 27.3973 - val_loss: 28.7528 - val_MinusLogProbMetric: 28.7528 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 820/1000
2023-10-11 21:39:51.982 
Epoch 820/1000 
	 loss: 27.4159, MinusLogProbMetric: 27.4159, val_loss: 28.7960, val_MinusLogProbMetric: 28.7960

Epoch 820: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4159 - MinusLogProbMetric: 27.4159 - val_loss: 28.7960 - val_MinusLogProbMetric: 28.7960 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 821/1000
2023-10-11 21:40:18.461 
Epoch 821/1000 
	 loss: 27.3997, MinusLogProbMetric: 27.3997, val_loss: 28.7215, val_MinusLogProbMetric: 28.7215

Epoch 821: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3997 - MinusLogProbMetric: 27.3997 - val_loss: 28.7215 - val_MinusLogProbMetric: 28.7215 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 822/1000
2023-10-11 21:40:44.185 
Epoch 822/1000 
	 loss: 27.4047, MinusLogProbMetric: 27.4047, val_loss: 28.8094, val_MinusLogProbMetric: 28.8094

Epoch 822: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4047 - MinusLogProbMetric: 27.4047 - val_loss: 28.8094 - val_MinusLogProbMetric: 28.8094 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 823/1000
2023-10-11 21:41:09.919 
Epoch 823/1000 
	 loss: 27.3996, MinusLogProbMetric: 27.3996, val_loss: 28.8072, val_MinusLogProbMetric: 28.8072

Epoch 823: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3996 - MinusLogProbMetric: 27.3996 - val_loss: 28.8072 - val_MinusLogProbMetric: 28.8072 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 824/1000
2023-10-11 21:41:35.775 
Epoch 824/1000 
	 loss: 27.4004, MinusLogProbMetric: 27.4004, val_loss: 28.7540, val_MinusLogProbMetric: 28.7540

Epoch 824: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4004 - MinusLogProbMetric: 27.4004 - val_loss: 28.7540 - val_MinusLogProbMetric: 28.7540 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 825/1000
2023-10-11 21:42:01.526 
Epoch 825/1000 
	 loss: 27.3986, MinusLogProbMetric: 27.3986, val_loss: 28.7970, val_MinusLogProbMetric: 28.7970

Epoch 825: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3986 - MinusLogProbMetric: 27.3986 - val_loss: 28.7970 - val_MinusLogProbMetric: 28.7970 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 826/1000
2023-10-11 21:42:27.344 
Epoch 826/1000 
	 loss: 27.3980, MinusLogProbMetric: 27.3980, val_loss: 28.7852, val_MinusLogProbMetric: 28.7852

Epoch 826: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3980 - MinusLogProbMetric: 27.3980 - val_loss: 28.7852 - val_MinusLogProbMetric: 28.7852 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 827/1000
2023-10-11 21:42:53.071 
Epoch 827/1000 
	 loss: 27.3980, MinusLogProbMetric: 27.3980, val_loss: 28.7972, val_MinusLogProbMetric: 28.7972

Epoch 827: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3980 - MinusLogProbMetric: 27.3980 - val_loss: 28.7972 - val_MinusLogProbMetric: 28.7972 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 828/1000
2023-10-11 21:43:18.782 
Epoch 828/1000 
	 loss: 27.4002, MinusLogProbMetric: 27.4002, val_loss: 28.8038, val_MinusLogProbMetric: 28.8038

Epoch 828: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4002 - MinusLogProbMetric: 27.4002 - val_loss: 28.8038 - val_MinusLogProbMetric: 28.8038 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 829/1000
2023-10-11 21:43:44.524 
Epoch 829/1000 
	 loss: 27.3973, MinusLogProbMetric: 27.3973, val_loss: 28.8238, val_MinusLogProbMetric: 28.8238

Epoch 829: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3973 - MinusLogProbMetric: 27.3973 - val_loss: 28.8238 - val_MinusLogProbMetric: 28.8238 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 830/1000
2023-10-11 21:44:10.255 
Epoch 830/1000 
	 loss: 27.3947, MinusLogProbMetric: 27.3947, val_loss: 28.8101, val_MinusLogProbMetric: 28.8101

Epoch 830: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3947 - MinusLogProbMetric: 27.3947 - val_loss: 28.8101 - val_MinusLogProbMetric: 28.8101 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 831/1000
2023-10-11 21:44:36.031 
Epoch 831/1000 
	 loss: 27.4002, MinusLogProbMetric: 27.4002, val_loss: 28.7996, val_MinusLogProbMetric: 28.7996

Epoch 831: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4002 - MinusLogProbMetric: 27.4002 - val_loss: 28.7996 - val_MinusLogProbMetric: 28.7996 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 832/1000
2023-10-11 21:45:02.127 
Epoch 832/1000 
	 loss: 27.3945, MinusLogProbMetric: 27.3945, val_loss: 28.7116, val_MinusLogProbMetric: 28.7116

Epoch 832: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3945 - MinusLogProbMetric: 27.3945 - val_loss: 28.7116 - val_MinusLogProbMetric: 28.7116 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 833/1000
2023-10-11 21:45:27.786 
Epoch 833/1000 
	 loss: 27.3991, MinusLogProbMetric: 27.3991, val_loss: 28.8153, val_MinusLogProbMetric: 28.8153

Epoch 833: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3991 - MinusLogProbMetric: 27.3991 - val_loss: 28.8153 - val_MinusLogProbMetric: 28.8153 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 834/1000
2023-10-11 21:45:53.574 
Epoch 834/1000 
	 loss: 27.4037, MinusLogProbMetric: 27.4037, val_loss: 28.7465, val_MinusLogProbMetric: 28.7465

Epoch 834: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.4037 - MinusLogProbMetric: 27.4037 - val_loss: 28.7465 - val_MinusLogProbMetric: 28.7465 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 835/1000
2023-10-11 21:46:19.556 
Epoch 835/1000 
	 loss: 27.3935, MinusLogProbMetric: 27.3935, val_loss: 28.8185, val_MinusLogProbMetric: 28.8185

Epoch 835: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3935 - MinusLogProbMetric: 27.3935 - val_loss: 28.8185 - val_MinusLogProbMetric: 28.8185 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 836/1000
2023-10-11 21:46:45.212 
Epoch 836/1000 
	 loss: 27.3938, MinusLogProbMetric: 27.3938, val_loss: 28.7670, val_MinusLogProbMetric: 28.7670

Epoch 836: val_loss did not improve from 28.51497
196/196 - 26s - loss: 27.3938 - MinusLogProbMetric: 27.3938 - val_loss: 28.7670 - val_MinusLogProbMetric: 28.7670 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 837/1000
2023-10-11 21:47:10.935 
Epoch 837/1000 
	 loss: 27.3996, MinusLogProbMetric: 27.3996, val_loss: 28.8314, val_MinusLogProbMetric: 28.8314

Epoch 837: val_loss did not improve from 28.51497
Restoring model weights from the end of the best epoch: 737.
196/196 - 26s - loss: 27.3996 - MinusLogProbMetric: 27.3996 - val_loss: 28.8314 - val_MinusLogProbMetric: 28.8314 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 837: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7f3b9072e9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 12.565178887918591 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:5 out of the last 5 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7f3b9072ea70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 6.671372026205063 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7f3b9072c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 5.441628308966756 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:5 out of the last 5 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7f3b9072c310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 6.451738790841773 seconds.
Training succeeded with seed 0.
Model trained in 22524.02 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 32.06 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 32.34 s.
===========
Run 322/720 done in 22558.80 s.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

===========
Generating train data for run 336.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_363"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_364 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f3ad0cea7d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a07fd12a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a07fd12a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3b206cbdc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a06828fd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a06829540>, <keras.callbacks.ModelCheckpoint object at 0x7f3a06829600>, <keras.callbacks.EarlyStopping object at 0x7f3a06829870>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a068298a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3a068294e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 21:47:51.873803
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:50:19.471 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6337.8062, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 6337.8062 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 147s/epoch - 752ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 336.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_374"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_375 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f3abd918400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3abdc19750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3abdc19750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3cd0505c30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f39f68402e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f39f6840850>, <keras.callbacks.ModelCheckpoint object at 0x7f39f6840910>, <keras.callbacks.EarlyStopping object at 0x7f39f6840b80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f39f6840bb0>, <keras.callbacks.TerminateOnNaN object at 0x7f39f68407f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 21:50:29.095611
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 81: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:53:08.610 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3943.6860, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 3943.6860 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 159s/epoch - 812ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 336.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_385"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_386 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f3c4f652ad0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f389c566f50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f389c566f50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38aca23ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c4f2ec730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c4f2ecca0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c4f2ecd60>, <keras.callbacks.EarlyStopping object at 0x7f3c4f2ecfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c4f2ed000>, <keras.callbacks.TerminateOnNaN object at 0x7f3c4f2ecc40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 21:53:16.003362
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-11 21:56:35.268 
Epoch 1/1000 
	 loss: 4134.3491, MinusLogProbMetric: 4134.3491, val_loss: 2154.9189, val_MinusLogProbMetric: 2154.9189

Epoch 1: val_loss improved from inf to 2154.91895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 200s - loss: 4134.3491 - MinusLogProbMetric: 4134.3491 - val_loss: 2154.9189 - val_MinusLogProbMetric: 2154.9189 - lr: 1.1111e-04 - 200s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 38: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:56:50.972 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 1842.9352, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 2154.91895
196/196 - 14s - loss: nan - MinusLogProbMetric: 1842.9352 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 14s/epoch - 73ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 336.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_396"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_397 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f3e4efd0a00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ac9c6d300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ac9c6d300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38ad622830>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f38ad68c2b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f38ad68c820>, <keras.callbacks.ModelCheckpoint object at 0x7f38ad68c8e0>, <keras.callbacks.EarlyStopping object at 0x7f38ad68cb50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f38ad68cb80>, <keras.callbacks.TerminateOnNaN object at 0x7f38ad68c7c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 21:56:59.013765
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-11 22:00:05.192 
Epoch 1/1000 
	 loss: 1889.2312, MinusLogProbMetric: 1889.2312, val_loss: 1413.6123, val_MinusLogProbMetric: 1413.6123

Epoch 1: val_loss improved from inf to 1413.61230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 187s - loss: 1889.2312 - MinusLogProbMetric: 1889.2312 - val_loss: 1413.6123 - val_MinusLogProbMetric: 1413.6123 - lr: 3.7037e-05 - 187s/epoch - 955ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 176: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 22:00:58.494 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 2308.4307, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 1413.61230
196/196 - 52s - loss: nan - MinusLogProbMetric: 2308.4307 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 52s/epoch - 265ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 336.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_407"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_408 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f3ac940a0e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38f7359060>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38f7359060>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38ae8c2230>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f43039d3c10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4303a641c0>, <keras.callbacks.ModelCheckpoint object at 0x7f4303a64280>, <keras.callbacks.EarlyStopping object at 0x7f4303a644f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4303a64520>, <keras.callbacks.TerminateOnNaN object at 0x7f4303a64160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 22:01:06.749221
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-11 22:04:25.474 
Epoch 1/1000 
	 loss: 1597.3687, MinusLogProbMetric: 1597.3687, val_loss: 1709.7390, val_MinusLogProbMetric: 1709.7390

Epoch 1: val_loss improved from inf to 1709.73901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 200s - loss: 1597.3687 - MinusLogProbMetric: 1597.3687 - val_loss: 1709.7390 - val_MinusLogProbMetric: 1709.7390 - lr: 1.2346e-05 - 200s/epoch - 1s/step
Epoch 2/1000
2023-10-11 22:05:23.243 
Epoch 2/1000 
	 loss: 1426.1339, MinusLogProbMetric: 1426.1339, val_loss: 1237.8339, val_MinusLogProbMetric: 1237.8339

Epoch 2: val_loss improved from 1709.73901 to 1237.83386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 1426.1339 - MinusLogProbMetric: 1426.1339 - val_loss: 1237.8339 - val_MinusLogProbMetric: 1237.8339 - lr: 1.2346e-05 - 57s/epoch - 293ms/step
Epoch 3/1000
2023-10-11 22:06:21.701 
Epoch 3/1000 
	 loss: 1088.5702, MinusLogProbMetric: 1088.5702, val_loss: 1003.1995, val_MinusLogProbMetric: 1003.1995

Epoch 3: val_loss improved from 1237.83386 to 1003.19952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 1088.5702 - MinusLogProbMetric: 1088.5702 - val_loss: 1003.1995 - val_MinusLogProbMetric: 1003.1995 - lr: 1.2346e-05 - 58s/epoch - 298ms/step
Epoch 4/1000
2023-10-11 22:07:19.448 
Epoch 4/1000 
	 loss: 930.7000, MinusLogProbMetric: 930.7000, val_loss: 863.5618, val_MinusLogProbMetric: 863.5618

Epoch 4: val_loss improved from 1003.19952 to 863.56183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 930.7000 - MinusLogProbMetric: 930.7000 - val_loss: 863.5618 - val_MinusLogProbMetric: 863.5618 - lr: 1.2346e-05 - 57s/epoch - 293ms/step
Epoch 5/1000
2023-10-11 22:08:17.214 
Epoch 5/1000 
	 loss: 834.9426, MinusLogProbMetric: 834.9426, val_loss: 868.1873, val_MinusLogProbMetric: 868.1873

Epoch 5: val_loss did not improve from 863.56183
196/196 - 57s - loss: 834.9426 - MinusLogProbMetric: 834.9426 - val_loss: 868.1873 - val_MinusLogProbMetric: 868.1873 - lr: 1.2346e-05 - 57s/epoch - 291ms/step
Epoch 6/1000
2023-10-11 22:09:14.254 
Epoch 6/1000 
	 loss: 822.6832, MinusLogProbMetric: 822.6832, val_loss: 800.6213, val_MinusLogProbMetric: 800.6213

Epoch 6: val_loss improved from 863.56183 to 800.62134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 822.6832 - MinusLogProbMetric: 822.6832 - val_loss: 800.6213 - val_MinusLogProbMetric: 800.6213 - lr: 1.2346e-05 - 58s/epoch - 297ms/step
Epoch 7/1000
2023-10-11 22:10:12.693 
Epoch 7/1000 
	 loss: 777.9575, MinusLogProbMetric: 777.9575, val_loss: 734.6119, val_MinusLogProbMetric: 734.6119

Epoch 7: val_loss improved from 800.62134 to 734.61194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 777.9575 - MinusLogProbMetric: 777.9575 - val_loss: 734.6119 - val_MinusLogProbMetric: 734.6119 - lr: 1.2346e-05 - 58s/epoch - 298ms/step
Epoch 8/1000
2023-10-11 22:11:10.467 
Epoch 8/1000 
	 loss: 813.7887, MinusLogProbMetric: 813.7887, val_loss: 793.7206, val_MinusLogProbMetric: 793.7206

Epoch 8: val_loss did not improve from 734.61194
196/196 - 57s - loss: 813.7887 - MinusLogProbMetric: 813.7887 - val_loss: 793.7206 - val_MinusLogProbMetric: 793.7206 - lr: 1.2346e-05 - 57s/epoch - 289ms/step
Epoch 9/1000
2023-10-11 22:12:08.409 
Epoch 9/1000 
	 loss: 743.1184, MinusLogProbMetric: 743.1184, val_loss: 712.3307, val_MinusLogProbMetric: 712.3307

Epoch 9: val_loss improved from 734.61194 to 712.33075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 743.1184 - MinusLogProbMetric: 743.1184 - val_loss: 712.3307 - val_MinusLogProbMetric: 712.3307 - lr: 1.2346e-05 - 59s/epoch - 301ms/step
Epoch 10/1000
2023-10-11 22:13:07.346 
Epoch 10/1000 
	 loss: 697.2689, MinusLogProbMetric: 697.2689, val_loss: 697.0474, val_MinusLogProbMetric: 697.0474

Epoch 10: val_loss improved from 712.33075 to 697.04742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 697.2689 - MinusLogProbMetric: 697.2689 - val_loss: 697.0474 - val_MinusLogProbMetric: 697.0474 - lr: 1.2346e-05 - 59s/epoch - 300ms/step
Epoch 11/1000
2023-10-11 22:14:05.804 
Epoch 11/1000 
	 loss: 663.2751, MinusLogProbMetric: 663.2751, val_loss: 643.5219, val_MinusLogProbMetric: 643.5219

Epoch 11: val_loss improved from 697.04742 to 643.52185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 663.2751 - MinusLogProbMetric: 663.2751 - val_loss: 643.5219 - val_MinusLogProbMetric: 643.5219 - lr: 1.2346e-05 - 58s/epoch - 298ms/step
Epoch 12/1000
2023-10-11 22:15:03.991 
Epoch 12/1000 
	 loss: 638.6135, MinusLogProbMetric: 638.6135, val_loss: 631.7059, val_MinusLogProbMetric: 631.7059

Epoch 12: val_loss improved from 643.52185 to 631.70593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 638.6135 - MinusLogProbMetric: 638.6135 - val_loss: 631.7059 - val_MinusLogProbMetric: 631.7059 - lr: 1.2346e-05 - 58s/epoch - 298ms/step
Epoch 13/1000
2023-10-11 22:16:02.074 
Epoch 13/1000 
	 loss: 619.7224, MinusLogProbMetric: 619.7224, val_loss: 613.8716, val_MinusLogProbMetric: 613.8716

Epoch 13: val_loss improved from 631.70593 to 613.87164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 619.7224 - MinusLogProbMetric: 619.7224 - val_loss: 613.8716 - val_MinusLogProbMetric: 613.8716 - lr: 1.2346e-05 - 58s/epoch - 298ms/step
Epoch 14/1000
2023-10-11 22:16:59.796 
Epoch 14/1000 
	 loss: 600.9073, MinusLogProbMetric: 600.9073, val_loss: 586.5120, val_MinusLogProbMetric: 586.5120

Epoch 14: val_loss improved from 613.87164 to 586.51196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 600.9073 - MinusLogProbMetric: 600.9073 - val_loss: 586.5120 - val_MinusLogProbMetric: 586.5120 - lr: 1.2346e-05 - 58s/epoch - 294ms/step
Epoch 15/1000
2023-10-11 22:17:57.247 
Epoch 15/1000 
	 loss: 584.1909, MinusLogProbMetric: 584.1909, val_loss: 579.3058, val_MinusLogProbMetric: 579.3058

Epoch 15: val_loss improved from 586.51196 to 579.30585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 584.1909 - MinusLogProbMetric: 584.1909 - val_loss: 579.3058 - val_MinusLogProbMetric: 579.3058 - lr: 1.2346e-05 - 57s/epoch - 293ms/step
Epoch 16/1000
2023-10-11 22:18:55.254 
Epoch 16/1000 
	 loss: 573.7391, MinusLogProbMetric: 573.7391, val_loss: 631.9902, val_MinusLogProbMetric: 631.9902

Epoch 16: val_loss did not improve from 579.30585
196/196 - 57s - loss: 573.7391 - MinusLogProbMetric: 573.7391 - val_loss: 631.9902 - val_MinusLogProbMetric: 631.9902 - lr: 1.2346e-05 - 57s/epoch - 291ms/step
Epoch 17/1000
2023-10-11 22:19:52.866 
Epoch 17/1000 
	 loss: 562.0621, MinusLogProbMetric: 562.0621, val_loss: 608.0474, val_MinusLogProbMetric: 608.0474

Epoch 17: val_loss did not improve from 579.30585
196/196 - 58s - loss: 562.0621 - MinusLogProbMetric: 562.0621 - val_loss: 608.0474 - val_MinusLogProbMetric: 608.0474 - lr: 1.2346e-05 - 58s/epoch - 294ms/step
Epoch 18/1000
2023-10-11 22:20:50.531 
Epoch 18/1000 
	 loss: 562.4792, MinusLogProbMetric: 562.4792, val_loss: 540.6852, val_MinusLogProbMetric: 540.6852

Epoch 18: val_loss improved from 579.30585 to 540.68524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 562.4792 - MinusLogProbMetric: 562.4792 - val_loss: 540.6852 - val_MinusLogProbMetric: 540.6852 - lr: 1.2346e-05 - 59s/epoch - 299ms/step
Epoch 19/1000
2023-10-11 22:21:47.970 
Epoch 19/1000 
	 loss: 545.1849, MinusLogProbMetric: 545.1849, val_loss: 531.5020, val_MinusLogProbMetric: 531.5020

Epoch 19: val_loss improved from 540.68524 to 531.50195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 545.1849 - MinusLogProbMetric: 545.1849 - val_loss: 531.5020 - val_MinusLogProbMetric: 531.5020 - lr: 1.2346e-05 - 57s/epoch - 293ms/step
Epoch 20/1000
2023-10-11 22:22:45.956 
Epoch 20/1000 
	 loss: 535.0568, MinusLogProbMetric: 535.0568, val_loss: 550.0103, val_MinusLogProbMetric: 550.0103

Epoch 20: val_loss did not improve from 531.50195
196/196 - 57s - loss: 535.0568 - MinusLogProbMetric: 535.0568 - val_loss: 550.0103 - val_MinusLogProbMetric: 550.0103 - lr: 1.2346e-05 - 57s/epoch - 291ms/step
Epoch 21/1000
2023-10-11 22:23:42.627 
Epoch 21/1000 
	 loss: 524.1030, MinusLogProbMetric: 524.1030, val_loss: 543.9288, val_MinusLogProbMetric: 543.9288

Epoch 21: val_loss did not improve from 531.50195
196/196 - 57s - loss: 524.1030 - MinusLogProbMetric: 524.1030 - val_loss: 543.9288 - val_MinusLogProbMetric: 543.9288 - lr: 1.2346e-05 - 57s/epoch - 289ms/step
Epoch 22/1000
2023-10-11 22:24:39.324 
Epoch 22/1000 
	 loss: 522.2465, MinusLogProbMetric: 522.2465, val_loss: 514.8029, val_MinusLogProbMetric: 514.8029

Epoch 22: val_loss improved from 531.50195 to 514.80292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 522.2465 - MinusLogProbMetric: 522.2465 - val_loss: 514.8029 - val_MinusLogProbMetric: 514.8029 - lr: 1.2346e-05 - 58s/epoch - 294ms/step
Epoch 23/1000
2023-10-11 22:25:38.145 
Epoch 23/1000 
	 loss: 504.1819, MinusLogProbMetric: 504.1819, val_loss: 498.1316, val_MinusLogProbMetric: 498.1316

Epoch 23: val_loss improved from 514.80292 to 498.13156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 504.1819 - MinusLogProbMetric: 504.1819 - val_loss: 498.1316 - val_MinusLogProbMetric: 498.1316 - lr: 1.2346e-05 - 59s/epoch - 299ms/step
Epoch 24/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 170: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 22:26:30.301 
Epoch 24/1000 
	 loss: nan, MinusLogProbMetric: 503.7320, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 24: val_loss did not improve from 498.13156
196/196 - 51s - loss: nan - MinusLogProbMetric: 503.7320 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 51s/epoch - 262ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 336.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_418"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_419 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f39f6cd64a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a0c606ad0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a0c606ad0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39f6cd7700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e0c5e7070>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e0c5e75e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e0c5e76a0>, <keras.callbacks.EarlyStopping object at 0x7f3e0c5e7910>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e0c5e7940>, <keras.callbacks.TerminateOnNaN object at 0x7f3e0c5e7580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 22:26:39.142511
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-11 22:29:54.604 
Epoch 1/1000 
	 loss: 577.1130, MinusLogProbMetric: 577.1130, val_loss: 619.7230, val_MinusLogProbMetric: 619.7230

Epoch 1: val_loss improved from inf to 619.72302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 196s - loss: 577.1130 - MinusLogProbMetric: 577.1130 - val_loss: 619.7230 - val_MinusLogProbMetric: 619.7230 - lr: 4.1152e-06 - 196s/epoch - 1s/step
Epoch 2/1000
2023-10-11 22:30:53.275 
Epoch 2/1000 
	 loss: 743.2280, MinusLogProbMetric: 743.2280, val_loss: 652.8716, val_MinusLogProbMetric: 652.8716

Epoch 2: val_loss did not improve from 619.72302
196/196 - 57s - loss: 743.2280 - MinusLogProbMetric: 743.2280 - val_loss: 652.8716 - val_MinusLogProbMetric: 652.8716 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 3/1000
2023-10-11 22:31:50.519 
Epoch 3/1000 
	 loss: 674.6359, MinusLogProbMetric: 674.6359, val_loss: 676.9271, val_MinusLogProbMetric: 676.9271

Epoch 3: val_loss did not improve from 619.72302
196/196 - 57s - loss: 674.6359 - MinusLogProbMetric: 674.6359 - val_loss: 676.9271 - val_MinusLogProbMetric: 676.9271 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 37: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 22:32:04.123 
Epoch 4/1000 
	 loss: nan, MinusLogProbMetric: 677.7615, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 4: val_loss did not improve from 619.72302
196/196 - 14s - loss: nan - MinusLogProbMetric: 677.7615 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 14s/epoch - 69ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 336.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_429"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_430 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f3ad080a020>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f39d48798d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f39d48798d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3f504f1690>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f38b409bca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f38b4034250>, <keras.callbacks.ModelCheckpoint object at 0x7f38b4034310>, <keras.callbacks.EarlyStopping object at 0x7f38b4034580>, <keras.callbacks.ReduceLROnPlateau object at 0x7f38b40345b0>, <keras.callbacks.TerminateOnNaN object at 0x7f38b40341f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-11 22:32:12.936800
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-11 22:35:20.336 
Epoch 1/1000 
	 loss: 568.1933, MinusLogProbMetric: 568.1933, val_loss: 559.5527, val_MinusLogProbMetric: 559.5527

Epoch 1: val_loss improved from inf to 559.55267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 188s - loss: 568.1933 - MinusLogProbMetric: 568.1933 - val_loss: 559.5527 - val_MinusLogProbMetric: 559.5527 - lr: 1.3717e-06 - 188s/epoch - 961ms/step
Epoch 2/1000
2023-10-11 22:36:17.373 
Epoch 2/1000 
	 loss: 553.4305, MinusLogProbMetric: 553.4305, val_loss: 524.9434, val_MinusLogProbMetric: 524.9434

Epoch 2: val_loss improved from 559.55267 to 524.94342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 553.4305 - MinusLogProbMetric: 553.4305 - val_loss: 524.9434 - val_MinusLogProbMetric: 524.9434 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 3/1000
2023-10-11 22:37:14.842 
Epoch 3/1000 
	 loss: 520.0891, MinusLogProbMetric: 520.0891, val_loss: 514.6108, val_MinusLogProbMetric: 514.6108

Epoch 3: val_loss improved from 524.94342 to 514.61084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 520.0891 - MinusLogProbMetric: 520.0891 - val_loss: 514.6108 - val_MinusLogProbMetric: 514.6108 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 4/1000
2023-10-11 22:38:11.549 
Epoch 4/1000 
	 loss: 501.5722, MinusLogProbMetric: 501.5722, val_loss: 491.7971, val_MinusLogProbMetric: 491.7971

Epoch 4: val_loss improved from 514.61084 to 491.79709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 501.5722 - MinusLogProbMetric: 501.5722 - val_loss: 491.7971 - val_MinusLogProbMetric: 491.7971 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 5/1000
2023-10-11 22:39:09.554 
Epoch 5/1000 
	 loss: 488.7878, MinusLogProbMetric: 488.7878, val_loss: 484.1197, val_MinusLogProbMetric: 484.1197

Epoch 5: val_loss improved from 491.79709 to 484.11969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 488.7878 - MinusLogProbMetric: 488.7878 - val_loss: 484.1197 - val_MinusLogProbMetric: 484.1197 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 6/1000
2023-10-11 22:40:07.701 
Epoch 6/1000 
	 loss: 484.7527, MinusLogProbMetric: 484.7527, val_loss: 527.3173, val_MinusLogProbMetric: 527.3173

Epoch 6: val_loss did not improve from 484.11969
196/196 - 57s - loss: 484.7527 - MinusLogProbMetric: 484.7527 - val_loss: 527.3173 - val_MinusLogProbMetric: 527.3173 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 7/1000
2023-10-11 22:41:04.864 
Epoch 7/1000 
	 loss: 532.0808, MinusLogProbMetric: 532.0808, val_loss: 509.7618, val_MinusLogProbMetric: 509.7618

Epoch 7: val_loss did not improve from 484.11969
196/196 - 57s - loss: 532.0808 - MinusLogProbMetric: 532.0808 - val_loss: 509.7618 - val_MinusLogProbMetric: 509.7618 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 8/1000
2023-10-11 22:42:02.027 
Epoch 8/1000 
	 loss: 550.3317, MinusLogProbMetric: 550.3317, val_loss: 578.3055, val_MinusLogProbMetric: 578.3055

Epoch 8: val_loss did not improve from 484.11969
196/196 - 57s - loss: 550.3317 - MinusLogProbMetric: 550.3317 - val_loss: 578.3055 - val_MinusLogProbMetric: 578.3055 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 9/1000
2023-10-11 22:42:59.771 
Epoch 9/1000 
	 loss: 562.9990, MinusLogProbMetric: 562.9990, val_loss: 552.7669, val_MinusLogProbMetric: 552.7669

Epoch 9: val_loss did not improve from 484.11969
196/196 - 58s - loss: 562.9990 - MinusLogProbMetric: 562.9990 - val_loss: 552.7669 - val_MinusLogProbMetric: 552.7669 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 10/1000
2023-10-11 22:43:58.574 
Epoch 10/1000 
	 loss: 553.0952, MinusLogProbMetric: 553.0952, val_loss: 544.4238, val_MinusLogProbMetric: 544.4238

Epoch 10: val_loss did not improve from 484.11969
196/196 - 59s - loss: 553.0952 - MinusLogProbMetric: 553.0952 - val_loss: 544.4238 - val_MinusLogProbMetric: 544.4238 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 11/1000
2023-10-11 22:44:57.287 
Epoch 11/1000 
	 loss: 536.9897, MinusLogProbMetric: 536.9897, val_loss: 533.1379, val_MinusLogProbMetric: 533.1379

Epoch 11: val_loss did not improve from 484.11969
196/196 - 59s - loss: 536.9897 - MinusLogProbMetric: 536.9897 - val_loss: 533.1379 - val_MinusLogProbMetric: 533.1379 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 12/1000
2023-10-11 22:45:58.516 
Epoch 12/1000 
	 loss: 526.8441, MinusLogProbMetric: 526.8441, val_loss: 511.1449, val_MinusLogProbMetric: 511.1449

Epoch 12: val_loss did not improve from 484.11969
196/196 - 61s - loss: 526.8441 - MinusLogProbMetric: 526.8441 - val_loss: 511.1449 - val_MinusLogProbMetric: 511.1449 - lr: 1.3717e-06 - 61s/epoch - 312ms/step
Epoch 13/1000
2023-10-11 22:46:56.864 
Epoch 13/1000 
	 loss: 507.4192, MinusLogProbMetric: 507.4192, val_loss: 503.4827, val_MinusLogProbMetric: 503.4827

Epoch 13: val_loss did not improve from 484.11969
196/196 - 58s - loss: 507.4192 - MinusLogProbMetric: 507.4192 - val_loss: 503.4827 - val_MinusLogProbMetric: 503.4827 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 14/1000
2023-10-11 22:47:53.048 
Epoch 14/1000 
	 loss: 501.1860, MinusLogProbMetric: 501.1860, val_loss: 497.9103, val_MinusLogProbMetric: 497.9103

Epoch 14: val_loss did not improve from 484.11969
196/196 - 56s - loss: 501.1860 - MinusLogProbMetric: 501.1860 - val_loss: 497.9103 - val_MinusLogProbMetric: 497.9103 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 15/1000
2023-10-11 22:48:48.384 
Epoch 15/1000 
	 loss: 497.3676, MinusLogProbMetric: 497.3676, val_loss: 493.8746, val_MinusLogProbMetric: 493.8746

Epoch 15: val_loss did not improve from 484.11969
196/196 - 55s - loss: 497.3676 - MinusLogProbMetric: 497.3676 - val_loss: 493.8746 - val_MinusLogProbMetric: 493.8746 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 16/1000
2023-10-11 22:49:43.973 
Epoch 16/1000 
	 loss: 492.4465, MinusLogProbMetric: 492.4465, val_loss: 488.9849, val_MinusLogProbMetric: 488.9849

Epoch 16: val_loss did not improve from 484.11969
196/196 - 56s - loss: 492.4465 - MinusLogProbMetric: 492.4465 - val_loss: 488.9849 - val_MinusLogProbMetric: 488.9849 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 17/1000
2023-10-11 22:50:39.188 
Epoch 17/1000 
	 loss: 486.9594, MinusLogProbMetric: 486.9594, val_loss: 484.0092, val_MinusLogProbMetric: 484.0092

Epoch 17: val_loss improved from 484.11969 to 484.00916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 486.9594 - MinusLogProbMetric: 486.9594 - val_loss: 484.0092 - val_MinusLogProbMetric: 484.0092 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 18/1000
2023-10-11 22:51:36.854 
Epoch 18/1000 
	 loss: 482.1778, MinusLogProbMetric: 482.1778, val_loss: 479.8759, val_MinusLogProbMetric: 479.8759

Epoch 18: val_loss improved from 484.00916 to 479.87589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 482.1778 - MinusLogProbMetric: 482.1778 - val_loss: 479.8759 - val_MinusLogProbMetric: 479.8759 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 19/1000
2023-10-11 22:52:34.671 
Epoch 19/1000 
	 loss: 478.8234, MinusLogProbMetric: 478.8234, val_loss: 476.5674, val_MinusLogProbMetric: 476.5674

Epoch 19: val_loss improved from 479.87589 to 476.56744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 478.8234 - MinusLogProbMetric: 478.8234 - val_loss: 476.5674 - val_MinusLogProbMetric: 476.5674 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 20/1000
2023-10-11 22:53:31.983 
Epoch 20/1000 
	 loss: 475.3866, MinusLogProbMetric: 475.3866, val_loss: 473.2284, val_MinusLogProbMetric: 473.2284

Epoch 20: val_loss improved from 476.56744 to 473.22836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 475.3866 - MinusLogProbMetric: 475.3866 - val_loss: 473.2284 - val_MinusLogProbMetric: 473.2284 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 21/1000
2023-10-11 22:54:28.682 
Epoch 21/1000 
	 loss: 472.1758, MinusLogProbMetric: 472.1758, val_loss: 470.1712, val_MinusLogProbMetric: 470.1712

Epoch 21: val_loss improved from 473.22836 to 470.17117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 472.1758 - MinusLogProbMetric: 472.1758 - val_loss: 470.1712 - val_MinusLogProbMetric: 470.1712 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 22/1000
2023-10-11 22:55:25.447 
Epoch 22/1000 
	 loss: 469.2388, MinusLogProbMetric: 469.2388, val_loss: 467.1609, val_MinusLogProbMetric: 467.1609

Epoch 22: val_loss improved from 470.17117 to 467.16092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 469.2388 - MinusLogProbMetric: 469.2388 - val_loss: 467.1609 - val_MinusLogProbMetric: 467.1609 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 23/1000
2023-10-11 22:56:22.139 
Epoch 23/1000 
	 loss: 466.5022, MinusLogProbMetric: 466.5022, val_loss: 464.8736, val_MinusLogProbMetric: 464.8736

Epoch 23: val_loss improved from 467.16092 to 464.87360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 466.5022 - MinusLogProbMetric: 466.5022 - val_loss: 464.8736 - val_MinusLogProbMetric: 464.8736 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 24/1000
2023-10-11 22:57:20.231 
Epoch 24/1000 
	 loss: 465.2753, MinusLogProbMetric: 465.2753, val_loss: 462.8680, val_MinusLogProbMetric: 462.8680

Epoch 24: val_loss improved from 464.87360 to 462.86801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 465.2753 - MinusLogProbMetric: 465.2753 - val_loss: 462.8680 - val_MinusLogProbMetric: 462.8680 - lr: 1.3717e-06 - 58s/epoch - 296ms/step
Epoch 25/1000
2023-10-11 22:58:18.578 
Epoch 25/1000 
	 loss: 461.9951, MinusLogProbMetric: 461.9951, val_loss: 460.2702, val_MinusLogProbMetric: 460.2702

Epoch 25: val_loss improved from 462.86801 to 460.27017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 461.9951 - MinusLogProbMetric: 461.9951 - val_loss: 460.2702 - val_MinusLogProbMetric: 460.2702 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 26/1000
2023-10-11 22:59:15.684 
Epoch 26/1000 
	 loss: 458.2779, MinusLogProbMetric: 458.2779, val_loss: 456.0930, val_MinusLogProbMetric: 456.0930

Epoch 26: val_loss improved from 460.27017 to 456.09299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 458.2779 - MinusLogProbMetric: 458.2779 - val_loss: 456.0930 - val_MinusLogProbMetric: 456.0930 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 27/1000
2023-10-11 23:00:12.943 
Epoch 27/1000 
	 loss: 456.0339, MinusLogProbMetric: 456.0339, val_loss: 457.7129, val_MinusLogProbMetric: 457.7129

Epoch 27: val_loss did not improve from 456.09299
196/196 - 56s - loss: 456.0339 - MinusLogProbMetric: 456.0339 - val_loss: 457.7129 - val_MinusLogProbMetric: 457.7129 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 28/1000
2023-10-11 23:01:10.267 
Epoch 28/1000 
	 loss: 455.6633, MinusLogProbMetric: 455.6633, val_loss: 455.2823, val_MinusLogProbMetric: 455.2823

Epoch 28: val_loss improved from 456.09299 to 455.28229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 455.6633 - MinusLogProbMetric: 455.6633 - val_loss: 455.2823 - val_MinusLogProbMetric: 455.2823 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 29/1000
2023-10-11 23:02:08.062 
Epoch 29/1000 
	 loss: 452.4858, MinusLogProbMetric: 452.4858, val_loss: 451.3334, val_MinusLogProbMetric: 451.3334

Epoch 29: val_loss improved from 455.28229 to 451.33340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 452.4858 - MinusLogProbMetric: 452.4858 - val_loss: 451.3334 - val_MinusLogProbMetric: 451.3334 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 30/1000
2023-10-11 23:03:05.611 
Epoch 30/1000 
	 loss: 449.8285, MinusLogProbMetric: 449.8285, val_loss: 449.1989, val_MinusLogProbMetric: 449.1989

Epoch 30: val_loss improved from 451.33340 to 449.19885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 449.8285 - MinusLogProbMetric: 449.8285 - val_loss: 449.1989 - val_MinusLogProbMetric: 449.1989 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 31/1000
2023-10-11 23:04:02.588 
Epoch 31/1000 
	 loss: 450.2185, MinusLogProbMetric: 450.2185, val_loss: 452.0390, val_MinusLogProbMetric: 452.0390

Epoch 31: val_loss did not improve from 449.19885
196/196 - 56s - loss: 450.2185 - MinusLogProbMetric: 450.2185 - val_loss: 452.0390 - val_MinusLogProbMetric: 452.0390 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 32/1000
2023-10-11 23:04:58.402 
Epoch 32/1000 
	 loss: 447.8615, MinusLogProbMetric: 447.8615, val_loss: 446.2978, val_MinusLogProbMetric: 446.2978

Epoch 32: val_loss improved from 449.19885 to 446.29779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 447.8615 - MinusLogProbMetric: 447.8615 - val_loss: 446.2978 - val_MinusLogProbMetric: 446.2978 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 33/1000
2023-10-11 23:05:54.477 
Epoch 33/1000 
	 loss: 444.9493, MinusLogProbMetric: 444.9493, val_loss: 444.3198, val_MinusLogProbMetric: 444.3198

Epoch 33: val_loss improved from 446.29779 to 444.31982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 444.9493 - MinusLogProbMetric: 444.9493 - val_loss: 444.3198 - val_MinusLogProbMetric: 444.3198 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 34/1000
2023-10-11 23:06:51.194 
Epoch 34/1000 
	 loss: 443.7599, MinusLogProbMetric: 443.7599, val_loss: 443.4669, val_MinusLogProbMetric: 443.4669

Epoch 34: val_loss improved from 444.31982 to 443.46686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 443.7599 - MinusLogProbMetric: 443.7599 - val_loss: 443.4669 - val_MinusLogProbMetric: 443.4669 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 35/1000
2023-10-11 23:07:46.990 
Epoch 35/1000 
	 loss: 442.8011, MinusLogProbMetric: 442.8011, val_loss: 442.3604, val_MinusLogProbMetric: 442.3604

Epoch 35: val_loss improved from 443.46686 to 442.36044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 442.8011 - MinusLogProbMetric: 442.8011 - val_loss: 442.3604 - val_MinusLogProbMetric: 442.3604 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 36/1000
2023-10-11 23:08:42.903 
Epoch 36/1000 
	 loss: 441.9190, MinusLogProbMetric: 441.9190, val_loss: 439.6937, val_MinusLogProbMetric: 439.6937

Epoch 36: val_loss improved from 442.36044 to 439.69373, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 441.9190 - MinusLogProbMetric: 441.9190 - val_loss: 439.6937 - val_MinusLogProbMetric: 439.6937 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 37/1000
2023-10-11 23:09:39.778 
Epoch 37/1000 
	 loss: 438.1949, MinusLogProbMetric: 438.1949, val_loss: 438.4302, val_MinusLogProbMetric: 438.4302

Epoch 37: val_loss improved from 439.69373 to 438.43024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 438.1949 - MinusLogProbMetric: 438.1949 - val_loss: 438.4302 - val_MinusLogProbMetric: 438.4302 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 38/1000
2023-10-11 23:10:35.817 
Epoch 38/1000 
	 loss: 436.8345, MinusLogProbMetric: 436.8345, val_loss: 436.0101, val_MinusLogProbMetric: 436.0101

Epoch 38: val_loss improved from 438.43024 to 436.01007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 436.8345 - MinusLogProbMetric: 436.8345 - val_loss: 436.0101 - val_MinusLogProbMetric: 436.0101 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 39/1000
2023-10-11 23:11:31.991 
Epoch 39/1000 
	 loss: 435.2279, MinusLogProbMetric: 435.2279, val_loss: 434.9454, val_MinusLogProbMetric: 434.9454

Epoch 39: val_loss improved from 436.01007 to 434.94537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 435.2279 - MinusLogProbMetric: 435.2279 - val_loss: 434.9454 - val_MinusLogProbMetric: 434.9454 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 40/1000
2023-10-11 23:12:28.512 
Epoch 40/1000 
	 loss: 433.5236, MinusLogProbMetric: 433.5236, val_loss: 432.9243, val_MinusLogProbMetric: 432.9243

Epoch 40: val_loss improved from 434.94537 to 432.92426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 433.5236 - MinusLogProbMetric: 433.5236 - val_loss: 432.9243 - val_MinusLogProbMetric: 432.9243 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 41/1000
2023-10-11 23:13:24.421 
Epoch 41/1000 
	 loss: 431.1882, MinusLogProbMetric: 431.1882, val_loss: 430.3135, val_MinusLogProbMetric: 430.3135

Epoch 41: val_loss improved from 432.92426 to 430.31354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 431.1882 - MinusLogProbMetric: 431.1882 - val_loss: 430.3135 - val_MinusLogProbMetric: 430.3135 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 42/1000
2023-10-11 23:14:21.067 
Epoch 42/1000 
	 loss: 429.5439, MinusLogProbMetric: 429.5439, val_loss: 430.3745, val_MinusLogProbMetric: 430.3745

Epoch 42: val_loss did not improve from 430.31354
196/196 - 56s - loss: 429.5439 - MinusLogProbMetric: 429.5439 - val_loss: 430.3745 - val_MinusLogProbMetric: 430.3745 - lr: 1.3717e-06 - 56s/epoch - 284ms/step
Epoch 43/1000
2023-10-11 23:15:17.813 
Epoch 43/1000 
	 loss: 432.9279, MinusLogProbMetric: 432.9279, val_loss: 431.9537, val_MinusLogProbMetric: 431.9537

Epoch 43: val_loss did not improve from 430.31354
196/196 - 57s - loss: 432.9279 - MinusLogProbMetric: 432.9279 - val_loss: 431.9537 - val_MinusLogProbMetric: 431.9537 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 44/1000
2023-10-11 23:16:13.249 
Epoch 44/1000 
	 loss: 429.2190, MinusLogProbMetric: 429.2190, val_loss: 428.0003, val_MinusLogProbMetric: 428.0003

Epoch 44: val_loss improved from 430.31354 to 428.00027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 429.2190 - MinusLogProbMetric: 429.2190 - val_loss: 428.0003 - val_MinusLogProbMetric: 428.0003 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 45/1000
2023-10-11 23:17:10.817 
Epoch 45/1000 
	 loss: 427.7575, MinusLogProbMetric: 427.7575, val_loss: 426.9842, val_MinusLogProbMetric: 426.9842

Epoch 45: val_loss improved from 428.00027 to 426.98419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 427.7575 - MinusLogProbMetric: 427.7575 - val_loss: 426.9842 - val_MinusLogProbMetric: 426.9842 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 46/1000
2023-10-11 23:18:09.682 
Epoch 46/1000 
	 loss: 425.5517, MinusLogProbMetric: 425.5517, val_loss: 425.4343, val_MinusLogProbMetric: 425.4343

Epoch 46: val_loss improved from 426.98419 to 425.43433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 425.5517 - MinusLogProbMetric: 425.5517 - val_loss: 425.4343 - val_MinusLogProbMetric: 425.4343 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 47/1000
2023-10-11 23:19:08.403 
Epoch 47/1000 
	 loss: 424.2456, MinusLogProbMetric: 424.2456, val_loss: 423.6119, val_MinusLogProbMetric: 423.6119

Epoch 47: val_loss improved from 425.43433 to 423.61191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 424.2456 - MinusLogProbMetric: 424.2456 - val_loss: 423.6119 - val_MinusLogProbMetric: 423.6119 - lr: 1.3717e-06 - 59s/epoch - 300ms/step
Epoch 48/1000
2023-10-11 23:20:05.880 
Epoch 48/1000 
	 loss: 422.9381, MinusLogProbMetric: 422.9381, val_loss: 422.2949, val_MinusLogProbMetric: 422.2949

Epoch 48: val_loss improved from 423.61191 to 422.29486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 422.9381 - MinusLogProbMetric: 422.9381 - val_loss: 422.2949 - val_MinusLogProbMetric: 422.2949 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 49/1000
2023-10-11 23:21:03.438 
Epoch 49/1000 
	 loss: 421.6213, MinusLogProbMetric: 421.6213, val_loss: 421.4741, val_MinusLogProbMetric: 421.4741

Epoch 49: val_loss improved from 422.29486 to 421.47406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 421.6213 - MinusLogProbMetric: 421.6213 - val_loss: 421.4741 - val_MinusLogProbMetric: 421.4741 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 50/1000
2023-10-11 23:22:01.802 
Epoch 50/1000 
	 loss: 420.2096, MinusLogProbMetric: 420.2096, val_loss: 419.2876, val_MinusLogProbMetric: 419.2876

Epoch 50: val_loss improved from 421.47406 to 419.28760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 420.2096 - MinusLogProbMetric: 420.2096 - val_loss: 419.2876 - val_MinusLogProbMetric: 419.2876 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 51/1000
2023-10-11 23:23:00.355 
Epoch 51/1000 
	 loss: 418.7735, MinusLogProbMetric: 418.7735, val_loss: 417.8960, val_MinusLogProbMetric: 417.8960

Epoch 51: val_loss improved from 419.28760 to 417.89603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 418.7735 - MinusLogProbMetric: 418.7735 - val_loss: 417.8960 - val_MinusLogProbMetric: 417.8960 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 52/1000
2023-10-11 23:23:58.772 
Epoch 52/1000 
	 loss: 417.4732, MinusLogProbMetric: 417.4732, val_loss: 417.1899, val_MinusLogProbMetric: 417.1899

Epoch 52: val_loss improved from 417.89603 to 417.18994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 417.4732 - MinusLogProbMetric: 417.4732 - val_loss: 417.1899 - val_MinusLogProbMetric: 417.1899 - lr: 1.3717e-06 - 59s/epoch - 299ms/step
Epoch 53/1000
2023-10-11 23:24:56.741 
Epoch 53/1000 
	 loss: 416.3446, MinusLogProbMetric: 416.3446, val_loss: 415.6837, val_MinusLogProbMetric: 415.6837

Epoch 53: val_loss improved from 417.18994 to 415.68375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 416.3446 - MinusLogProbMetric: 416.3446 - val_loss: 415.6837 - val_MinusLogProbMetric: 415.6837 - lr: 1.3717e-06 - 58s/epoch - 295ms/step
Epoch 54/1000
2023-10-11 23:25:53.795 
Epoch 54/1000 
	 loss: 415.2014, MinusLogProbMetric: 415.2014, val_loss: 415.3965, val_MinusLogProbMetric: 415.3965

Epoch 54: val_loss improved from 415.68375 to 415.39655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 415.2014 - MinusLogProbMetric: 415.2014 - val_loss: 415.3965 - val_MinusLogProbMetric: 415.3965 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 55/1000
2023-10-11 23:26:50.717 
Epoch 55/1000 
	 loss: 414.0938, MinusLogProbMetric: 414.0938, val_loss: 413.0968, val_MinusLogProbMetric: 413.0968

Epoch 55: val_loss improved from 415.39655 to 413.09683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 414.0938 - MinusLogProbMetric: 414.0938 - val_loss: 413.0968 - val_MinusLogProbMetric: 413.0968 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 56/1000
2023-10-11 23:27:46.920 
Epoch 56/1000 
	 loss: 412.8198, MinusLogProbMetric: 412.8198, val_loss: 412.5767, val_MinusLogProbMetric: 412.5767

Epoch 56: val_loss improved from 413.09683 to 412.57666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 412.8198 - MinusLogProbMetric: 412.8198 - val_loss: 412.5767 - val_MinusLogProbMetric: 412.5767 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 57/1000
2023-10-11 23:28:43.417 
Epoch 57/1000 
	 loss: 411.6318, MinusLogProbMetric: 411.6318, val_loss: 411.4843, val_MinusLogProbMetric: 411.4843

Epoch 57: val_loss improved from 412.57666 to 411.48425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 411.6318 - MinusLogProbMetric: 411.6318 - val_loss: 411.4843 - val_MinusLogProbMetric: 411.4843 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 58/1000
2023-10-11 23:29:40.305 
Epoch 58/1000 
	 loss: 410.6353, MinusLogProbMetric: 410.6353, val_loss: 409.9800, val_MinusLogProbMetric: 409.9800

Epoch 58: val_loss improved from 411.48425 to 409.98001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 410.6353 - MinusLogProbMetric: 410.6353 - val_loss: 409.9800 - val_MinusLogProbMetric: 409.9800 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 59/1000
2023-10-11 23:30:36.431 
Epoch 59/1000 
	 loss: 409.4492, MinusLogProbMetric: 409.4492, val_loss: 408.8952, val_MinusLogProbMetric: 408.8952

Epoch 59: val_loss improved from 409.98001 to 408.89523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 409.4492 - MinusLogProbMetric: 409.4492 - val_loss: 408.8952 - val_MinusLogProbMetric: 408.8952 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 60/1000
2023-10-11 23:31:32.956 
Epoch 60/1000 
	 loss: 408.4244, MinusLogProbMetric: 408.4244, val_loss: 407.7114, val_MinusLogProbMetric: 407.7114

Epoch 60: val_loss improved from 408.89523 to 407.71140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 408.4244 - MinusLogProbMetric: 408.4244 - val_loss: 407.7114 - val_MinusLogProbMetric: 407.7114 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 61/1000
2023-10-11 23:32:29.805 
Epoch 61/1000 
	 loss: 407.3965, MinusLogProbMetric: 407.3965, val_loss: 406.8182, val_MinusLogProbMetric: 406.8182

Epoch 61: val_loss improved from 407.71140 to 406.81818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 407.3965 - MinusLogProbMetric: 407.3965 - val_loss: 406.8182 - val_MinusLogProbMetric: 406.8182 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 62/1000
2023-10-11 23:33:26.560 
Epoch 62/1000 
	 loss: 406.6191, MinusLogProbMetric: 406.6191, val_loss: 405.9486, val_MinusLogProbMetric: 405.9486

Epoch 62: val_loss improved from 406.81818 to 405.94861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 406.6191 - MinusLogProbMetric: 406.6191 - val_loss: 405.9486 - val_MinusLogProbMetric: 405.9486 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 63/1000
2023-10-11 23:34:22.701 
Epoch 63/1000 
	 loss: 405.6490, MinusLogProbMetric: 405.6490, val_loss: 405.1240, val_MinusLogProbMetric: 405.1240

Epoch 63: val_loss improved from 405.94861 to 405.12399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 405.6490 - MinusLogProbMetric: 405.6490 - val_loss: 405.1240 - val_MinusLogProbMetric: 405.1240 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 64/1000
2023-10-11 23:35:19.129 
Epoch 64/1000 
	 loss: 404.9202, MinusLogProbMetric: 404.9202, val_loss: 404.8329, val_MinusLogProbMetric: 404.8329

Epoch 64: val_loss improved from 405.12399 to 404.83295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 404.9202 - MinusLogProbMetric: 404.9202 - val_loss: 404.8329 - val_MinusLogProbMetric: 404.8329 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 65/1000
2023-10-11 23:36:15.321 
Epoch 65/1000 
	 loss: 404.0231, MinusLogProbMetric: 404.0231, val_loss: 403.8784, val_MinusLogProbMetric: 403.8784

Epoch 65: val_loss improved from 404.83295 to 403.87845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 404.0231 - MinusLogProbMetric: 404.0231 - val_loss: 403.8784 - val_MinusLogProbMetric: 403.8784 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 66/1000
2023-10-11 23:37:11.610 
Epoch 66/1000 
	 loss: 402.9010, MinusLogProbMetric: 402.9010, val_loss: 405.0336, val_MinusLogProbMetric: 405.0336

Epoch 66: val_loss did not improve from 403.87845
196/196 - 55s - loss: 402.9010 - MinusLogProbMetric: 402.9010 - val_loss: 405.0336 - val_MinusLogProbMetric: 405.0336 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 67/1000
2023-10-11 23:38:07.480 
Epoch 67/1000 
	 loss: 403.0580, MinusLogProbMetric: 403.0580, val_loss: 402.0158, val_MinusLogProbMetric: 402.0158

Epoch 67: val_loss improved from 403.87845 to 402.01581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 403.0580 - MinusLogProbMetric: 403.0580 - val_loss: 402.0158 - val_MinusLogProbMetric: 402.0158 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 68/1000
2023-10-11 23:39:03.844 
Epoch 68/1000 
	 loss: 401.2668, MinusLogProbMetric: 401.2668, val_loss: 400.4673, val_MinusLogProbMetric: 400.4673

Epoch 68: val_loss improved from 402.01581 to 400.46735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 401.2668 - MinusLogProbMetric: 401.2668 - val_loss: 400.4673 - val_MinusLogProbMetric: 400.4673 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 69/1000
2023-10-11 23:40:00.636 
Epoch 69/1000 
	 loss: 401.0334, MinusLogProbMetric: 401.0334, val_loss: 400.4071, val_MinusLogProbMetric: 400.4071

Epoch 69: val_loss improved from 400.46735 to 400.40714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 401.0334 - MinusLogProbMetric: 401.0334 - val_loss: 400.4071 - val_MinusLogProbMetric: 400.4071 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 70/1000
2023-10-11 23:40:57.636 
Epoch 70/1000 
	 loss: 399.9106, MinusLogProbMetric: 399.9106, val_loss: 400.9352, val_MinusLogProbMetric: 400.9352

Epoch 70: val_loss did not improve from 400.40714
196/196 - 56s - loss: 399.9106 - MinusLogProbMetric: 399.9106 - val_loss: 400.9352 - val_MinusLogProbMetric: 400.9352 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 71/1000
2023-10-11 23:41:53.573 
Epoch 71/1000 
	 loss: 399.5090, MinusLogProbMetric: 399.5090, val_loss: 398.7939, val_MinusLogProbMetric: 398.7939

Epoch 71: val_loss improved from 400.40714 to 398.79395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 399.5090 - MinusLogProbMetric: 399.5090 - val_loss: 398.7939 - val_MinusLogProbMetric: 398.7939 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 72/1000
2023-10-11 23:42:50.120 
Epoch 72/1000 
	 loss: 399.0119, MinusLogProbMetric: 399.0119, val_loss: 398.9804, val_MinusLogProbMetric: 398.9804

Epoch 72: val_loss did not improve from 398.79395
196/196 - 56s - loss: 399.0119 - MinusLogProbMetric: 399.0119 - val_loss: 398.9804 - val_MinusLogProbMetric: 398.9804 - lr: 1.3717e-06 - 56s/epoch - 283ms/step
Epoch 73/1000
2023-10-11 23:43:46.466 
Epoch 73/1000 
	 loss: 398.3222, MinusLogProbMetric: 398.3222, val_loss: 397.8577, val_MinusLogProbMetric: 397.8577

Epoch 73: val_loss improved from 398.79395 to 397.85773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 398.3222 - MinusLogProbMetric: 398.3222 - val_loss: 397.8577 - val_MinusLogProbMetric: 397.8577 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 74/1000
2023-10-11 23:44:43.223 
Epoch 74/1000 
	 loss: 398.7555, MinusLogProbMetric: 398.7555, val_loss: 396.8407, val_MinusLogProbMetric: 396.8407

Epoch 74: val_loss improved from 397.85773 to 396.84070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 398.7555 - MinusLogProbMetric: 398.7555 - val_loss: 396.8407 - val_MinusLogProbMetric: 396.8407 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 75/1000
2023-10-11 23:45:39.541 
Epoch 75/1000 
	 loss: 396.9298, MinusLogProbMetric: 396.9298, val_loss: 397.8203, val_MinusLogProbMetric: 397.8203

Epoch 75: val_loss did not improve from 396.84070
196/196 - 55s - loss: 396.9298 - MinusLogProbMetric: 396.9298 - val_loss: 397.8203 - val_MinusLogProbMetric: 397.8203 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 76/1000
2023-10-11 23:46:35.828 
Epoch 76/1000 
	 loss: 398.1219, MinusLogProbMetric: 398.1219, val_loss: 400.4383, val_MinusLogProbMetric: 400.4383

Epoch 76: val_loss did not improve from 396.84070
196/196 - 56s - loss: 398.1219 - MinusLogProbMetric: 398.1219 - val_loss: 400.4383 - val_MinusLogProbMetric: 400.4383 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 77/1000
2023-10-11 23:47:31.662 
Epoch 77/1000 
	 loss: 398.0405, MinusLogProbMetric: 398.0405, val_loss: 396.9477, val_MinusLogProbMetric: 396.9477

Epoch 77: val_loss did not improve from 396.84070
196/196 - 56s - loss: 398.0405 - MinusLogProbMetric: 398.0405 - val_loss: 396.9477 - val_MinusLogProbMetric: 396.9477 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 78/1000
2023-10-11 23:48:27.189 
Epoch 78/1000 
	 loss: 395.7636, MinusLogProbMetric: 395.7636, val_loss: 395.0317, val_MinusLogProbMetric: 395.0317

Epoch 78: val_loss improved from 396.84070 to 395.03174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 395.7636 - MinusLogProbMetric: 395.7636 - val_loss: 395.0317 - val_MinusLogProbMetric: 395.0317 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 79/1000
2023-10-11 23:49:23.147 
Epoch 79/1000 
	 loss: 394.2916, MinusLogProbMetric: 394.2916, val_loss: 393.8365, val_MinusLogProbMetric: 393.8365

Epoch 79: val_loss improved from 395.03174 to 393.83655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 394.2916 - MinusLogProbMetric: 394.2916 - val_loss: 393.8365 - val_MinusLogProbMetric: 393.8365 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 80/1000
2023-10-11 23:50:19.689 
Epoch 80/1000 
	 loss: 393.2590, MinusLogProbMetric: 393.2590, val_loss: 392.8337, val_MinusLogProbMetric: 392.8337

Epoch 80: val_loss improved from 393.83655 to 392.83368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 393.2590 - MinusLogProbMetric: 393.2590 - val_loss: 392.8337 - val_MinusLogProbMetric: 392.8337 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 81/1000
2023-10-11 23:51:16.822 
Epoch 81/1000 
	 loss: 392.1216, MinusLogProbMetric: 392.1216, val_loss: 391.6272, val_MinusLogProbMetric: 391.6272

Epoch 81: val_loss improved from 392.83368 to 391.62723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 392.1216 - MinusLogProbMetric: 392.1216 - val_loss: 391.6272 - val_MinusLogProbMetric: 391.6272 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 82/1000
2023-10-11 23:52:13.008 
Epoch 82/1000 
	 loss: 391.2432, MinusLogProbMetric: 391.2432, val_loss: 390.7226, val_MinusLogProbMetric: 390.7226

Epoch 82: val_loss improved from 391.62723 to 390.72256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 391.2432 - MinusLogProbMetric: 391.2432 - val_loss: 390.7226 - val_MinusLogProbMetric: 390.7226 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 83/1000
2023-10-11 23:53:09.288 
Epoch 83/1000 
	 loss: 390.4835, MinusLogProbMetric: 390.4835, val_loss: 390.9024, val_MinusLogProbMetric: 390.9024

Epoch 83: val_loss did not improve from 390.72256
196/196 - 55s - loss: 390.4835 - MinusLogProbMetric: 390.4835 - val_loss: 390.9024 - val_MinusLogProbMetric: 390.9024 - lr: 1.3717e-06 - 55s/epoch - 282ms/step
Epoch 84/1000
2023-10-11 23:54:04.715 
Epoch 84/1000 
	 loss: 389.5372, MinusLogProbMetric: 389.5372, val_loss: 389.4907, val_MinusLogProbMetric: 389.4907

Epoch 84: val_loss improved from 390.72256 to 389.49066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 389.5372 - MinusLogProbMetric: 389.5372 - val_loss: 389.4907 - val_MinusLogProbMetric: 389.4907 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 85/1000
2023-10-11 23:55:01.660 
Epoch 85/1000 
	 loss: 388.5134, MinusLogProbMetric: 388.5134, val_loss: 387.8183, val_MinusLogProbMetric: 387.8183

Epoch 85: val_loss improved from 389.49066 to 387.81827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 388.5134 - MinusLogProbMetric: 388.5134 - val_loss: 387.8183 - val_MinusLogProbMetric: 387.8183 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 86/1000
2023-10-11 23:55:57.857 
Epoch 86/1000 
	 loss: 387.2910, MinusLogProbMetric: 387.2910, val_loss: 390.1625, val_MinusLogProbMetric: 390.1625

Epoch 86: val_loss did not improve from 387.81827
196/196 - 55s - loss: 387.2910 - MinusLogProbMetric: 387.2910 - val_loss: 390.1625 - val_MinusLogProbMetric: 390.1625 - lr: 1.3717e-06 - 55s/epoch - 281ms/step
Epoch 87/1000
2023-10-11 23:56:54.250 
Epoch 87/1000 
	 loss: 387.3568, MinusLogProbMetric: 387.3568, val_loss: 387.3839, val_MinusLogProbMetric: 387.3839

Epoch 87: val_loss improved from 387.81827 to 387.38394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 387.3568 - MinusLogProbMetric: 387.3568 - val_loss: 387.3839 - val_MinusLogProbMetric: 387.3839 - lr: 1.3717e-06 - 57s/epoch - 293ms/step
Epoch 88/1000
2023-10-11 23:57:54.450 
Epoch 88/1000 
	 loss: 386.4890, MinusLogProbMetric: 386.4890, val_loss: 385.7191, val_MinusLogProbMetric: 385.7191

Epoch 88: val_loss improved from 387.38394 to 385.71909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 60s - loss: 386.4890 - MinusLogProbMetric: 386.4890 - val_loss: 385.7191 - val_MinusLogProbMetric: 385.7191 - lr: 1.3717e-06 - 60s/epoch - 307ms/step
Epoch 89/1000
2023-10-11 23:58:52.732 
Epoch 89/1000 
	 loss: 385.2314, MinusLogProbMetric: 385.2314, val_loss: 386.1138, val_MinusLogProbMetric: 386.1138

Epoch 89: val_loss did not improve from 385.71909
196/196 - 57s - loss: 385.2314 - MinusLogProbMetric: 385.2314 - val_loss: 386.1138 - val_MinusLogProbMetric: 386.1138 - lr: 1.3717e-06 - 57s/epoch - 292ms/step
Epoch 90/1000
2023-10-11 23:59:48.742 
Epoch 90/1000 
	 loss: 384.1853, MinusLogProbMetric: 384.1853, val_loss: 383.5680, val_MinusLogProbMetric: 383.5680

Epoch 90: val_loss improved from 385.71909 to 383.56802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 384.1853 - MinusLogProbMetric: 384.1853 - val_loss: 383.5680 - val_MinusLogProbMetric: 383.5680 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 91/1000
2023-10-12 00:00:46.812 
Epoch 91/1000 
	 loss: 383.5340, MinusLogProbMetric: 383.5340, val_loss: 383.1240, val_MinusLogProbMetric: 383.1240

Epoch 91: val_loss improved from 383.56802 to 383.12396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 383.5340 - MinusLogProbMetric: 383.5340 - val_loss: 383.1240 - val_MinusLogProbMetric: 383.1240 - lr: 1.3717e-06 - 58s/epoch - 297ms/step
Epoch 92/1000
2023-10-12 00:01:45.328 
Epoch 92/1000 
	 loss: 382.6833, MinusLogProbMetric: 382.6833, val_loss: 382.0485, val_MinusLogProbMetric: 382.0485

Epoch 92: val_loss improved from 383.12396 to 382.04846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 382.6833 - MinusLogProbMetric: 382.6833 - val_loss: 382.0485 - val_MinusLogProbMetric: 382.0485 - lr: 1.3717e-06 - 58s/epoch - 298ms/step
Epoch 93/1000
2023-10-12 00:02:41.305 
Epoch 93/1000 
	 loss: 381.8333, MinusLogProbMetric: 381.8333, val_loss: 381.8341, val_MinusLogProbMetric: 381.8341

Epoch 93: val_loss improved from 382.04846 to 381.83408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 381.8333 - MinusLogProbMetric: 381.8333 - val_loss: 381.8341 - val_MinusLogProbMetric: 381.8341 - lr: 1.3717e-06 - 56s/epoch - 286ms/step
Epoch 94/1000
2023-10-12 00:03:37.082 
Epoch 94/1000 
	 loss: 380.9581, MinusLogProbMetric: 380.9581, val_loss: 380.5511, val_MinusLogProbMetric: 380.5511

Epoch 94: val_loss improved from 381.83408 to 380.55106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 380.9581 - MinusLogProbMetric: 380.9581 - val_loss: 380.5511 - val_MinusLogProbMetric: 380.5511 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 95/1000
2023-10-12 00:04:33.359 
Epoch 95/1000 
	 loss: 380.1128, MinusLogProbMetric: 380.1128, val_loss: 380.5272, val_MinusLogProbMetric: 380.5272

Epoch 95: val_loss improved from 380.55106 to 380.52719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 380.1128 - MinusLogProbMetric: 380.1128 - val_loss: 380.5272 - val_MinusLogProbMetric: 380.5272 - lr: 1.3717e-06 - 56s/epoch - 287ms/step
Epoch 96/1000
2023-10-12 00:05:29.940 
Epoch 96/1000 
	 loss: 379.4250, MinusLogProbMetric: 379.4250, val_loss: 378.6142, val_MinusLogProbMetric: 378.6142

Epoch 96: val_loss improved from 380.52719 to 378.61417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 379.4250 - MinusLogProbMetric: 379.4250 - val_loss: 378.6142 - val_MinusLogProbMetric: 378.6142 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 97/1000
2023-10-12 00:06:26.721 
Epoch 97/1000 
	 loss: 378.3730, MinusLogProbMetric: 378.3730, val_loss: 377.7996, val_MinusLogProbMetric: 377.7996

Epoch 97: val_loss improved from 378.61417 to 377.79956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 378.3730 - MinusLogProbMetric: 378.3730 - val_loss: 377.7996 - val_MinusLogProbMetric: 377.7996 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 98/1000
2023-10-12 00:07:23.155 
Epoch 98/1000 
	 loss: 376.8840, MinusLogProbMetric: 376.8840, val_loss: 376.3423, val_MinusLogProbMetric: 376.3423

Epoch 98: val_loss improved from 377.79956 to 376.34235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 376.8840 - MinusLogProbMetric: 376.8840 - val_loss: 376.3423 - val_MinusLogProbMetric: 376.3423 - lr: 1.3717e-06 - 56s/epoch - 288ms/step
Epoch 99/1000
2023-10-12 00:08:20.105 
Epoch 99/1000 
	 loss: 376.1242, MinusLogProbMetric: 376.1242, val_loss: 376.2007, val_MinusLogProbMetric: 376.2007

Epoch 99: val_loss improved from 376.34235 to 376.20074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 376.1242 - MinusLogProbMetric: 376.1242 - val_loss: 376.2007 - val_MinusLogProbMetric: 376.2007 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 100/1000
2023-10-12 00:09:16.954 
Epoch 100/1000 
	 loss: 375.4206, MinusLogProbMetric: 375.4206, val_loss: 375.2432, val_MinusLogProbMetric: 375.2432

Epoch 100: val_loss improved from 376.20074 to 375.24323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 375.4206 - MinusLogProbMetric: 375.4206 - val_loss: 375.2432 - val_MinusLogProbMetric: 375.2432 - lr: 1.3717e-06 - 57s/epoch - 290ms/step
Epoch 101/1000
2023-10-12 00:10:13.808 
Epoch 101/1000 
	 loss: 374.4869, MinusLogProbMetric: 374.4869, val_loss: 375.4174, val_MinusLogProbMetric: 375.4174

Epoch 101: val_loss did not improve from 375.24323
196/196 - 56s - loss: 374.4869 - MinusLogProbMetric: 374.4869 - val_loss: 375.4174 - val_MinusLogProbMetric: 375.4174 - lr: 1.3717e-06 - 56s/epoch - 285ms/step
Epoch 102/1000
2023-10-12 00:11:09.455 
Epoch 102/1000 
	 loss: 373.9042, MinusLogProbMetric: 373.9042, val_loss: 374.3102, val_MinusLogProbMetric: 374.3102

Epoch 102: val_loss improved from 375.24323 to 374.31018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 373.9042 - MinusLogProbMetric: 373.9042 - val_loss: 374.3102 - val_MinusLogProbMetric: 374.3102 - lr: 1.3717e-06 - 57s/epoch - 289ms/step
Epoch 103/1000
2023-10-12 00:12:06.416 
Epoch 103/1000 
	 loss: 373.0995, MinusLogProbMetric: 373.0995, val_loss: 373.0589, val_MinusLogProbMetric: 373.0589

Epoch 103: val_loss improved from 374.31018 to 373.05893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 373.0995 - MinusLogProbMetric: 373.0995 - val_loss: 373.0589 - val_MinusLogProbMetric: 373.0589 - lr: 1.3717e-06 - 57s/epoch - 291ms/step
Epoch 104/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 34: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-12 00:12:19.773 
Epoch 104/1000 
	 loss: nan, MinusLogProbMetric: 371.8136, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 104: val_loss did not improve from 373.05893
196/196 - 12s - loss: nan - MinusLogProbMetric: 371.8136 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 12s/epoch - 63ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 336.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_440"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_441 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f3c4e886980>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c4e615150>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c4e615150>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a0438e1d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c4e28ad70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c4e28b2e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c4e28b3a0>, <keras.callbacks.EarlyStopping object at 0x7f3c4e28b610>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c4e28b640>, <keras.callbacks.TerminateOnNaN object at 0x7f3c4e28b280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-12 00:12:27.684510
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-12 00:15:45.885 
Epoch 1/1000 
	 loss: 370.4134, MinusLogProbMetric: 370.4134, val_loss: 368.5134, val_MinusLogProbMetric: 368.5134

Epoch 1: val_loss improved from inf to 368.51337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 199s - loss: 370.4134 - MinusLogProbMetric: 370.4134 - val_loss: 368.5134 - val_MinusLogProbMetric: 368.5134 - lr: 4.5725e-07 - 199s/epoch - 1s/step
Epoch 2/1000
2023-10-12 00:16:46.055 
Epoch 2/1000 
	 loss: 367.6190, MinusLogProbMetric: 367.6190, val_loss: 367.2475, val_MinusLogProbMetric: 367.2475

Epoch 2: val_loss improved from 368.51337 to 367.24750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 60s - loss: 367.6190 - MinusLogProbMetric: 367.6190 - val_loss: 367.2475 - val_MinusLogProbMetric: 367.2475 - lr: 4.5725e-07 - 60s/epoch - 305ms/step
Epoch 3/1000
2023-10-12 00:17:44.985 
Epoch 3/1000 
	 loss: 364.9796, MinusLogProbMetric: 364.9796, val_loss: 364.8458, val_MinusLogProbMetric: 364.8458

Epoch 3: val_loss improved from 367.24750 to 364.84579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 364.9796 - MinusLogProbMetric: 364.9796 - val_loss: 364.8458 - val_MinusLogProbMetric: 364.8458 - lr: 4.5725e-07 - 59s/epoch - 301ms/step
Epoch 4/1000
2023-10-12 00:18:42.654 
Epoch 4/1000 
	 loss: 362.3396, MinusLogProbMetric: 362.3396, val_loss: 361.1951, val_MinusLogProbMetric: 361.1951

Epoch 4: val_loss improved from 364.84579 to 361.19513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 362.3396 - MinusLogProbMetric: 362.3396 - val_loss: 361.1951 - val_MinusLogProbMetric: 361.1951 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 5/1000
2023-10-12 00:19:40.302 
Epoch 5/1000 
	 loss: 360.3383, MinusLogProbMetric: 360.3383, val_loss: 360.1538, val_MinusLogProbMetric: 360.1538

Epoch 5: val_loss improved from 361.19513 to 360.15384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 360.3383 - MinusLogProbMetric: 360.3383 - val_loss: 360.1538 - val_MinusLogProbMetric: 360.1538 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 6/1000
2023-10-12 00:20:36.985 
Epoch 6/1000 
	 loss: 358.4513, MinusLogProbMetric: 358.4513, val_loss: 357.6134, val_MinusLogProbMetric: 357.6134

Epoch 6: val_loss improved from 360.15384 to 357.61340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 358.4513 - MinusLogProbMetric: 358.4513 - val_loss: 357.6134 - val_MinusLogProbMetric: 357.6134 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 7/1000
2023-10-12 00:21:34.513 
Epoch 7/1000 
	 loss: 356.6618, MinusLogProbMetric: 356.6618, val_loss: 355.9853, val_MinusLogProbMetric: 355.9853

Epoch 7: val_loss improved from 357.61340 to 355.98526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 356.6618 - MinusLogProbMetric: 356.6618 - val_loss: 355.9853 - val_MinusLogProbMetric: 355.9853 - lr: 4.5725e-07 - 58s/epoch - 293ms/step
Epoch 8/1000
2023-10-12 00:22:32.208 
Epoch 8/1000 
	 loss: 354.8031, MinusLogProbMetric: 354.8031, val_loss: 354.3665, val_MinusLogProbMetric: 354.3665

Epoch 8: val_loss improved from 355.98526 to 354.36649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 354.8031 - MinusLogProbMetric: 354.8031 - val_loss: 354.3665 - val_MinusLogProbMetric: 354.3665 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 9/1000
2023-10-12 00:23:29.421 
Epoch 9/1000 
	 loss: 354.4523, MinusLogProbMetric: 354.4523, val_loss: 359.9672, val_MinusLogProbMetric: 359.9672

Epoch 9: val_loss did not improve from 354.36649
196/196 - 56s - loss: 354.4523 - MinusLogProbMetric: 354.4523 - val_loss: 359.9672 - val_MinusLogProbMetric: 359.9672 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 10/1000
2023-10-12 00:24:26.565 
Epoch 10/1000 
	 loss: 361.9490, MinusLogProbMetric: 361.9490, val_loss: 361.8514, val_MinusLogProbMetric: 361.8514

Epoch 10: val_loss did not improve from 354.36649
196/196 - 57s - loss: 361.9490 - MinusLogProbMetric: 361.9490 - val_loss: 361.8514 - val_MinusLogProbMetric: 361.8514 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 11/1000
2023-10-12 00:25:22.249 
Epoch 11/1000 
	 loss: 357.4789, MinusLogProbMetric: 357.4789, val_loss: 354.4566, val_MinusLogProbMetric: 354.4566

Epoch 11: val_loss did not improve from 354.36649
196/196 - 56s - loss: 357.4789 - MinusLogProbMetric: 357.4789 - val_loss: 354.4566 - val_MinusLogProbMetric: 354.4566 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 12/1000
2023-10-12 00:26:18.205 
Epoch 12/1000 
	 loss: 352.9507, MinusLogProbMetric: 352.9507, val_loss: 352.5227, val_MinusLogProbMetric: 352.5227

Epoch 12: val_loss improved from 354.36649 to 352.52271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 352.9507 - MinusLogProbMetric: 352.9507 - val_loss: 352.5227 - val_MinusLogProbMetric: 352.5227 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 13/1000
2023-10-12 00:27:14.330 
Epoch 13/1000 
	 loss: 353.0462, MinusLogProbMetric: 353.0462, val_loss: 353.1180, val_MinusLogProbMetric: 353.1180

Epoch 13: val_loss did not improve from 352.52271
196/196 - 55s - loss: 353.0462 - MinusLogProbMetric: 353.0462 - val_loss: 353.1180 - val_MinusLogProbMetric: 353.1180 - lr: 4.5725e-07 - 55s/epoch - 281ms/step
Epoch 14/1000
2023-10-12 00:28:10.416 
Epoch 14/1000 
	 loss: 351.1742, MinusLogProbMetric: 351.1742, val_loss: 350.2435, val_MinusLogProbMetric: 350.2435

Epoch 14: val_loss improved from 352.52271 to 350.24353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 351.1742 - MinusLogProbMetric: 351.1742 - val_loss: 350.2435 - val_MinusLogProbMetric: 350.2435 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 15/1000
2023-10-12 00:29:07.002 
Epoch 15/1000 
	 loss: 349.5070, MinusLogProbMetric: 349.5070, val_loss: 348.8183, val_MinusLogProbMetric: 348.8183

Epoch 15: val_loss improved from 350.24353 to 348.81833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 349.5070 - MinusLogProbMetric: 349.5070 - val_loss: 348.8183 - val_MinusLogProbMetric: 348.8183 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 16/1000
2023-10-12 00:30:03.916 
Epoch 16/1000 
	 loss: 352.5373, MinusLogProbMetric: 352.5373, val_loss: 364.5297, val_MinusLogProbMetric: 364.5297

Epoch 16: val_loss did not improve from 348.81833
196/196 - 56s - loss: 352.5373 - MinusLogProbMetric: 352.5373 - val_loss: 364.5297 - val_MinusLogProbMetric: 364.5297 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 17/1000
2023-10-12 00:30:59.705 
Epoch 17/1000 
	 loss: 361.6750, MinusLogProbMetric: 361.6750, val_loss: 360.4557, val_MinusLogProbMetric: 360.4557

Epoch 17: val_loss did not improve from 348.81833
196/196 - 56s - loss: 361.6750 - MinusLogProbMetric: 361.6750 - val_loss: 360.4557 - val_MinusLogProbMetric: 360.4557 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 18/1000
2023-10-12 00:31:57.690 
Epoch 18/1000 
	 loss: 359.0189, MinusLogProbMetric: 359.0189, val_loss: 358.0675, val_MinusLogProbMetric: 358.0675

Epoch 18: val_loss did not improve from 348.81833
196/196 - 58s - loss: 359.0189 - MinusLogProbMetric: 359.0189 - val_loss: 358.0675 - val_MinusLogProbMetric: 358.0675 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 19/1000
2023-10-12 00:32:53.957 
Epoch 19/1000 
	 loss: 357.2788, MinusLogProbMetric: 357.2788, val_loss: 356.1754, val_MinusLogProbMetric: 356.1754

Epoch 19: val_loss did not improve from 348.81833
196/196 - 56s - loss: 357.2788 - MinusLogProbMetric: 357.2788 - val_loss: 356.1754 - val_MinusLogProbMetric: 356.1754 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 20/1000
2023-10-12 00:33:50.253 
Epoch 20/1000 
	 loss: 354.9575, MinusLogProbMetric: 354.9575, val_loss: 354.4828, val_MinusLogProbMetric: 354.4828

Epoch 20: val_loss did not improve from 348.81833
196/196 - 56s - loss: 354.9575 - MinusLogProbMetric: 354.9575 - val_loss: 354.4828 - val_MinusLogProbMetric: 354.4828 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 21/1000
2023-10-12 00:34:47.650 
Epoch 21/1000 
	 loss: 353.4668, MinusLogProbMetric: 353.4668, val_loss: 352.7604, val_MinusLogProbMetric: 352.7604

Epoch 21: val_loss did not improve from 348.81833
196/196 - 57s - loss: 353.4668 - MinusLogProbMetric: 353.4668 - val_loss: 352.7604 - val_MinusLogProbMetric: 352.7604 - lr: 4.5725e-07 - 57s/epoch - 293ms/step
Epoch 22/1000
2023-10-12 00:35:44.720 
Epoch 22/1000 
	 loss: 352.7254, MinusLogProbMetric: 352.7254, val_loss: 353.6833, val_MinusLogProbMetric: 353.6833

Epoch 22: val_loss did not improve from 348.81833
196/196 - 57s - loss: 352.7254 - MinusLogProbMetric: 352.7254 - val_loss: 353.6833 - val_MinusLogProbMetric: 353.6833 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 23/1000
2023-10-12 00:36:41.098 
Epoch 23/1000 
	 loss: 352.6715, MinusLogProbMetric: 352.6715, val_loss: 352.3773, val_MinusLogProbMetric: 352.3773

Epoch 23: val_loss did not improve from 348.81833
196/196 - 56s - loss: 352.6715 - MinusLogProbMetric: 352.6715 - val_loss: 352.3773 - val_MinusLogProbMetric: 352.3773 - lr: 4.5725e-07 - 56s/epoch - 288ms/step
Epoch 24/1000
2023-10-12 00:37:37.487 
Epoch 24/1000 
	 loss: 351.2812, MinusLogProbMetric: 351.2812, val_loss: 350.8436, val_MinusLogProbMetric: 350.8436

Epoch 24: val_loss did not improve from 348.81833
196/196 - 56s - loss: 351.2812 - MinusLogProbMetric: 351.2812 - val_loss: 350.8436 - val_MinusLogProbMetric: 350.8436 - lr: 4.5725e-07 - 56s/epoch - 288ms/step
Epoch 25/1000
2023-10-12 00:38:34.403 
Epoch 25/1000 
	 loss: 353.4844, MinusLogProbMetric: 353.4844, val_loss: 351.0665, val_MinusLogProbMetric: 351.0665

Epoch 25: val_loss did not improve from 348.81833
196/196 - 57s - loss: 353.4844 - MinusLogProbMetric: 353.4844 - val_loss: 351.0665 - val_MinusLogProbMetric: 351.0665 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 26/1000
2023-10-12 00:39:30.085 
Epoch 26/1000 
	 loss: 350.0548, MinusLogProbMetric: 350.0548, val_loss: 349.3802, val_MinusLogProbMetric: 349.3802

Epoch 26: val_loss did not improve from 348.81833
196/196 - 56s - loss: 350.0548 - MinusLogProbMetric: 350.0548 - val_loss: 349.3802 - val_MinusLogProbMetric: 349.3802 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 27/1000
2023-10-12 00:40:26.388 
Epoch 27/1000 
	 loss: 350.3853, MinusLogProbMetric: 350.3853, val_loss: 349.2511, val_MinusLogProbMetric: 349.2511

Epoch 27: val_loss did not improve from 348.81833
196/196 - 56s - loss: 350.3853 - MinusLogProbMetric: 350.3853 - val_loss: 349.2511 - val_MinusLogProbMetric: 349.2511 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 28/1000
2023-10-12 00:41:22.472 
Epoch 28/1000 
	 loss: 348.5763, MinusLogProbMetric: 348.5763, val_loss: 347.7529, val_MinusLogProbMetric: 347.7529

Epoch 28: val_loss improved from 348.81833 to 347.75287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 348.5763 - MinusLogProbMetric: 348.5763 - val_loss: 347.7529 - val_MinusLogProbMetric: 347.7529 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 29/1000
2023-10-12 00:42:20.187 
Epoch 29/1000 
	 loss: 347.1398, MinusLogProbMetric: 347.1398, val_loss: 346.7607, val_MinusLogProbMetric: 346.7607

Epoch 29: val_loss improved from 347.75287 to 346.76068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 347.1398 - MinusLogProbMetric: 347.1398 - val_loss: 346.7607 - val_MinusLogProbMetric: 346.7607 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 30/1000
2023-10-12 00:43:16.479 
Epoch 30/1000 
	 loss: 346.4054, MinusLogProbMetric: 346.4054, val_loss: 345.8643, val_MinusLogProbMetric: 345.8643

Epoch 30: val_loss improved from 346.76068 to 345.86429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 346.4054 - MinusLogProbMetric: 346.4054 - val_loss: 345.8643 - val_MinusLogProbMetric: 345.8643 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 31/1000
2023-10-12 00:44:13.353 
Epoch 31/1000 
	 loss: 345.5096, MinusLogProbMetric: 345.5096, val_loss: 345.7502, val_MinusLogProbMetric: 345.7502

Epoch 31: val_loss improved from 345.86429 to 345.75015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 345.5096 - MinusLogProbMetric: 345.5096 - val_loss: 345.7502 - val_MinusLogProbMetric: 345.7502 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 32/1000
2023-10-12 00:45:10.269 
Epoch 32/1000 
	 loss: 345.1967, MinusLogProbMetric: 345.1967, val_loss: 344.5139, val_MinusLogProbMetric: 344.5139

Epoch 32: val_loss improved from 345.75015 to 344.51389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 345.1967 - MinusLogProbMetric: 345.1967 - val_loss: 344.5139 - val_MinusLogProbMetric: 344.5139 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 33/1000
2023-10-12 00:46:07.369 
Epoch 33/1000 
	 loss: 343.9267, MinusLogProbMetric: 343.9267, val_loss: 343.5109, val_MinusLogProbMetric: 343.5109

Epoch 33: val_loss improved from 344.51389 to 343.51089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 343.9267 - MinusLogProbMetric: 343.9267 - val_loss: 343.5109 - val_MinusLogProbMetric: 343.5109 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 34/1000
2023-10-12 00:47:04.582 
Epoch 34/1000 
	 loss: 343.0683, MinusLogProbMetric: 343.0683, val_loss: 343.0589, val_MinusLogProbMetric: 343.0589

Epoch 34: val_loss improved from 343.51089 to 343.05887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 343.0683 - MinusLogProbMetric: 343.0683 - val_loss: 343.0589 - val_MinusLogProbMetric: 343.0589 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 35/1000
2023-10-12 00:48:01.937 
Epoch 35/1000 
	 loss: 342.2631, MinusLogProbMetric: 342.2631, val_loss: 341.7438, val_MinusLogProbMetric: 341.7438

Epoch 35: val_loss improved from 343.05887 to 341.74380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 342.2631 - MinusLogProbMetric: 342.2631 - val_loss: 341.7438 - val_MinusLogProbMetric: 341.7438 - lr: 4.5725e-07 - 57s/epoch - 293ms/step
Epoch 36/1000
2023-10-12 00:49:00.071 
Epoch 36/1000 
	 loss: 341.4115, MinusLogProbMetric: 341.4115, val_loss: 341.4313, val_MinusLogProbMetric: 341.4313

Epoch 36: val_loss improved from 341.74380 to 341.43127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 341.4115 - MinusLogProbMetric: 341.4115 - val_loss: 341.4313 - val_MinusLogProbMetric: 341.4313 - lr: 4.5725e-07 - 58s/epoch - 296ms/step
Epoch 37/1000
2023-10-12 00:49:57.284 
Epoch 37/1000 
	 loss: 340.8678, MinusLogProbMetric: 340.8678, val_loss: 340.9122, val_MinusLogProbMetric: 340.9122

Epoch 37: val_loss improved from 341.43127 to 340.91217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 340.8678 - MinusLogProbMetric: 340.8678 - val_loss: 340.9122 - val_MinusLogProbMetric: 340.9122 - lr: 4.5725e-07 - 57s/epoch - 292ms/step
Epoch 38/1000
2023-10-12 00:50:53.660 
Epoch 38/1000 
	 loss: 340.6183, MinusLogProbMetric: 340.6183, val_loss: 340.5825, val_MinusLogProbMetric: 340.5825

Epoch 38: val_loss improved from 340.91217 to 340.58246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 56s - loss: 340.6183 - MinusLogProbMetric: 340.6183 - val_loss: 340.5825 - val_MinusLogProbMetric: 340.5825 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 39/1000
2023-10-12 00:51:50.961 
Epoch 39/1000 
	 loss: 340.4546, MinusLogProbMetric: 340.4546, val_loss: 340.0583, val_MinusLogProbMetric: 340.0583

Epoch 39: val_loss improved from 340.58246 to 340.05832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 340.4546 - MinusLogProbMetric: 340.4546 - val_loss: 340.0583 - val_MinusLogProbMetric: 340.0583 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 40/1000
2023-10-12 00:52:48.541 
Epoch 40/1000 
	 loss: 339.4405, MinusLogProbMetric: 339.4405, val_loss: 339.5045, val_MinusLogProbMetric: 339.5045

Epoch 40: val_loss improved from 340.05832 to 339.50446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 339.4405 - MinusLogProbMetric: 339.4405 - val_loss: 339.5045 - val_MinusLogProbMetric: 339.5045 - lr: 4.5725e-07 - 58s/epoch - 294ms/step
Epoch 41/1000
2023-10-12 00:53:45.541 
Epoch 41/1000 
	 loss: 339.2025, MinusLogProbMetric: 339.2025, val_loss: 339.5639, val_MinusLogProbMetric: 339.5639

Epoch 41: val_loss did not improve from 339.50446
196/196 - 56s - loss: 339.2025 - MinusLogProbMetric: 339.2025 - val_loss: 339.5639 - val_MinusLogProbMetric: 339.5639 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 42/1000
2023-10-12 00:54:41.234 
Epoch 42/1000 
	 loss: 338.9562, MinusLogProbMetric: 338.9562, val_loss: 339.0622, val_MinusLogProbMetric: 339.0622

Epoch 42: val_loss improved from 339.50446 to 339.06223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 338.9562 - MinusLogProbMetric: 338.9562 - val_loss: 339.0622 - val_MinusLogProbMetric: 339.0622 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 43/1000
2023-10-12 00:55:38.148 
Epoch 43/1000 
	 loss: 338.3708, MinusLogProbMetric: 338.3708, val_loss: 338.7221, val_MinusLogProbMetric: 338.7221

Epoch 43: val_loss improved from 339.06223 to 338.72208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 338.3708 - MinusLogProbMetric: 338.3708 - val_loss: 338.7221 - val_MinusLogProbMetric: 338.7221 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 44/1000
2023-10-12 00:56:35.879 
Epoch 44/1000 
	 loss: 338.0298, MinusLogProbMetric: 338.0298, val_loss: 338.1208, val_MinusLogProbMetric: 338.1208

Epoch 44: val_loss improved from 338.72208 to 338.12076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 338.0298 - MinusLogProbMetric: 338.0298 - val_loss: 338.1208 - val_MinusLogProbMetric: 338.1208 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 45/1000
2023-10-12 00:57:32.638 
Epoch 45/1000 
	 loss: 337.7475, MinusLogProbMetric: 337.7475, val_loss: 339.1755, val_MinusLogProbMetric: 339.1755

Epoch 45: val_loss did not improve from 338.12076
196/196 - 56s - loss: 337.7475 - MinusLogProbMetric: 337.7475 - val_loss: 339.1755 - val_MinusLogProbMetric: 339.1755 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 46/1000
2023-10-12 00:58:28.102 
Epoch 46/1000 
	 loss: 337.5780, MinusLogProbMetric: 337.5780, val_loss: 337.5695, val_MinusLogProbMetric: 337.5695

Epoch 46: val_loss improved from 338.12076 to 337.56949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 337.5780 - MinusLogProbMetric: 337.5780 - val_loss: 337.5695 - val_MinusLogProbMetric: 337.5695 - lr: 4.5725e-07 - 57s/epoch - 288ms/step
Epoch 47/1000
2023-10-12 00:59:24.950 
Epoch 47/1000 
	 loss: 337.3826, MinusLogProbMetric: 337.3826, val_loss: 337.8365, val_MinusLogProbMetric: 337.8365

Epoch 47: val_loss did not improve from 337.56949
196/196 - 56s - loss: 337.3826 - MinusLogProbMetric: 337.3826 - val_loss: 337.8365 - val_MinusLogProbMetric: 337.8365 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 48/1000
2023-10-12 01:00:20.843 
Epoch 48/1000 
	 loss: 337.2719, MinusLogProbMetric: 337.2719, val_loss: 337.0286, val_MinusLogProbMetric: 337.0286

Epoch 48: val_loss improved from 337.56949 to 337.02856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 337.2719 - MinusLogProbMetric: 337.2719 - val_loss: 337.0286 - val_MinusLogProbMetric: 337.0286 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 49/1000
2023-10-12 01:01:17.592 
Epoch 49/1000 
	 loss: 387.0063, MinusLogProbMetric: 387.0063, val_loss: 401.5436, val_MinusLogProbMetric: 401.5436

Epoch 49: val_loss did not improve from 337.02856
196/196 - 56s - loss: 387.0063 - MinusLogProbMetric: 387.0063 - val_loss: 401.5436 - val_MinusLogProbMetric: 401.5436 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 50/1000
2023-10-12 01:02:13.393 
Epoch 50/1000 
	 loss: 401.1620, MinusLogProbMetric: 401.1620, val_loss: 395.5534, val_MinusLogProbMetric: 395.5534

Epoch 50: val_loss did not improve from 337.02856
196/196 - 56s - loss: 401.1620 - MinusLogProbMetric: 401.1620 - val_loss: 395.5534 - val_MinusLogProbMetric: 395.5534 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 51/1000
2023-10-12 01:03:08.768 
Epoch 51/1000 
	 loss: 394.4812, MinusLogProbMetric: 394.4812, val_loss: 391.5771, val_MinusLogProbMetric: 391.5771

Epoch 51: val_loss did not improve from 337.02856
196/196 - 55s - loss: 394.4812 - MinusLogProbMetric: 394.4812 - val_loss: 391.5771 - val_MinusLogProbMetric: 391.5771 - lr: 4.5725e-07 - 55s/epoch - 283ms/step
Epoch 52/1000
2023-10-12 01:04:04.302 
Epoch 52/1000 
	 loss: 393.1795, MinusLogProbMetric: 393.1795, val_loss: 393.7923, val_MinusLogProbMetric: 393.7923

Epoch 52: val_loss did not improve from 337.02856
196/196 - 56s - loss: 393.1795 - MinusLogProbMetric: 393.1795 - val_loss: 393.7923 - val_MinusLogProbMetric: 393.7923 - lr: 4.5725e-07 - 56s/epoch - 283ms/step
Epoch 53/1000
2023-10-12 01:05:00.639 
Epoch 53/1000 
	 loss: 391.4211, MinusLogProbMetric: 391.4211, val_loss: 388.4224, val_MinusLogProbMetric: 388.4224

Epoch 53: val_loss did not improve from 337.02856
196/196 - 56s - loss: 391.4211 - MinusLogProbMetric: 391.4211 - val_loss: 388.4224 - val_MinusLogProbMetric: 388.4224 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 54/1000
2023-10-12 01:05:56.377 
Epoch 54/1000 
	 loss: 392.2663, MinusLogProbMetric: 392.2663, val_loss: 393.1014, val_MinusLogProbMetric: 393.1014

Epoch 54: val_loss did not improve from 337.02856
196/196 - 56s - loss: 392.2663 - MinusLogProbMetric: 392.2663 - val_loss: 393.1014 - val_MinusLogProbMetric: 393.1014 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 55/1000
2023-10-12 01:06:51.681 
Epoch 55/1000 
	 loss: 389.7998, MinusLogProbMetric: 389.7998, val_loss: 383.5892, val_MinusLogProbMetric: 383.5892

Epoch 55: val_loss did not improve from 337.02856
196/196 - 55s - loss: 389.7998 - MinusLogProbMetric: 389.7998 - val_loss: 383.5892 - val_MinusLogProbMetric: 383.5892 - lr: 4.5725e-07 - 55s/epoch - 282ms/step
Epoch 56/1000
2023-10-12 01:07:47.063 
Epoch 56/1000 
	 loss: 381.9348, MinusLogProbMetric: 381.9348, val_loss: 379.4208, val_MinusLogProbMetric: 379.4208

Epoch 56: val_loss did not improve from 337.02856
196/196 - 55s - loss: 381.9348 - MinusLogProbMetric: 381.9348 - val_loss: 379.4208 - val_MinusLogProbMetric: 379.4208 - lr: 4.5725e-07 - 55s/epoch - 283ms/step
Epoch 57/1000
2023-10-12 01:08:42.665 
Epoch 57/1000 
	 loss: 378.4536, MinusLogProbMetric: 378.4536, val_loss: 376.7211, val_MinusLogProbMetric: 376.7211

Epoch 57: val_loss did not improve from 337.02856
196/196 - 56s - loss: 378.4536 - MinusLogProbMetric: 378.4536 - val_loss: 376.7211 - val_MinusLogProbMetric: 376.7211 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 58/1000
2023-10-12 01:09:38.612 
Epoch 58/1000 
	 loss: 376.3131, MinusLogProbMetric: 376.3131, val_loss: 374.9491, val_MinusLogProbMetric: 374.9491

Epoch 58: val_loss did not improve from 337.02856
196/196 - 56s - loss: 376.3131 - MinusLogProbMetric: 376.3131 - val_loss: 374.9491 - val_MinusLogProbMetric: 374.9491 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 59/1000
2023-10-12 01:10:34.327 
Epoch 59/1000 
	 loss: 374.6010, MinusLogProbMetric: 374.6010, val_loss: 373.8607, val_MinusLogProbMetric: 373.8607

Epoch 59: val_loss did not improve from 337.02856
196/196 - 56s - loss: 374.6010 - MinusLogProbMetric: 374.6010 - val_loss: 373.8607 - val_MinusLogProbMetric: 373.8607 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 60/1000
2023-10-12 01:11:30.561 
Epoch 60/1000 
	 loss: 373.5022, MinusLogProbMetric: 373.5022, val_loss: 372.6795, val_MinusLogProbMetric: 372.6795

Epoch 60: val_loss did not improve from 337.02856
196/196 - 56s - loss: 373.5022 - MinusLogProbMetric: 373.5022 - val_loss: 372.6795 - val_MinusLogProbMetric: 372.6795 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 61/1000
2023-10-12 01:12:26.168 
Epoch 61/1000 
	 loss: 371.1335, MinusLogProbMetric: 371.1335, val_loss: 369.3493, val_MinusLogProbMetric: 369.3493

Epoch 61: val_loss did not improve from 337.02856
196/196 - 56s - loss: 371.1335 - MinusLogProbMetric: 371.1335 - val_loss: 369.3493 - val_MinusLogProbMetric: 369.3493 - lr: 4.5725e-07 - 56s/epoch - 284ms/step
Epoch 62/1000
2023-10-12 01:13:22.486 
Epoch 62/1000 
	 loss: 368.9011, MinusLogProbMetric: 368.9011, val_loss: 368.2934, val_MinusLogProbMetric: 368.2934

Epoch 62: val_loss did not improve from 337.02856
196/196 - 56s - loss: 368.9011 - MinusLogProbMetric: 368.9011 - val_loss: 368.2934 - val_MinusLogProbMetric: 368.2934 - lr: 4.5725e-07 - 56s/epoch - 287ms/step
Epoch 63/1000
2023-10-12 01:14:19.175 
Epoch 63/1000 
	 loss: 368.0959, MinusLogProbMetric: 368.0959, val_loss: 367.6233, val_MinusLogProbMetric: 367.6233

Epoch 63: val_loss did not improve from 337.02856
196/196 - 57s - loss: 368.0959 - MinusLogProbMetric: 368.0959 - val_loss: 367.6233 - val_MinusLogProbMetric: 367.6233 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 64/1000
2023-10-12 01:15:17.617 
Epoch 64/1000 
	 loss: 367.3337, MinusLogProbMetric: 367.3337, val_loss: 367.3632, val_MinusLogProbMetric: 367.3632

Epoch 64: val_loss did not improve from 337.02856
196/196 - 58s - loss: 367.3337 - MinusLogProbMetric: 367.3337 - val_loss: 367.3632 - val_MinusLogProbMetric: 367.3632 - lr: 4.5725e-07 - 58s/epoch - 298ms/step
Epoch 65/1000
2023-10-12 01:16:15.439 
Epoch 65/1000 
	 loss: 366.6878, MinusLogProbMetric: 366.6878, val_loss: 366.3740, val_MinusLogProbMetric: 366.3740

Epoch 65: val_loss did not improve from 337.02856
196/196 - 58s - loss: 366.6878 - MinusLogProbMetric: 366.6878 - val_loss: 366.3740 - val_MinusLogProbMetric: 366.3740 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 66/1000
2023-10-12 01:17:11.315 
Epoch 66/1000 
	 loss: 366.1552, MinusLogProbMetric: 366.1552, val_loss: 366.0644, val_MinusLogProbMetric: 366.0644

Epoch 66: val_loss did not improve from 337.02856
196/196 - 56s - loss: 366.1552 - MinusLogProbMetric: 366.1552 - val_loss: 366.0644 - val_MinusLogProbMetric: 366.0644 - lr: 4.5725e-07 - 56s/epoch - 285ms/step
Epoch 67/1000
2023-10-12 01:18:08.166 
Epoch 67/1000 
	 loss: 365.4587, MinusLogProbMetric: 365.4587, val_loss: 365.1043, val_MinusLogProbMetric: 365.1043

Epoch 67: val_loss did not improve from 337.02856
196/196 - 57s - loss: 365.4587 - MinusLogProbMetric: 365.4587 - val_loss: 365.1043 - val_MinusLogProbMetric: 365.1043 - lr: 4.5725e-07 - 57s/epoch - 290ms/step
Epoch 68/1000
2023-10-12 01:19:06.531 
Epoch 68/1000 
	 loss: 364.9265, MinusLogProbMetric: 364.9265, val_loss: 364.6837, val_MinusLogProbMetric: 364.6837

Epoch 68: val_loss did not improve from 337.02856
196/196 - 58s - loss: 364.9265 - MinusLogProbMetric: 364.9265 - val_loss: 364.6837 - val_MinusLogProbMetric: 364.6837 - lr: 4.5725e-07 - 58s/epoch - 298ms/step
Epoch 69/1000
2023-10-12 01:20:02.500 
Epoch 69/1000 
	 loss: 364.3516, MinusLogProbMetric: 364.3516, val_loss: 364.0706, val_MinusLogProbMetric: 364.0706

Epoch 69: val_loss did not improve from 337.02856
196/196 - 56s - loss: 364.3516 - MinusLogProbMetric: 364.3516 - val_loss: 364.0706 - val_MinusLogProbMetric: 364.0706 - lr: 4.5725e-07 - 56s/epoch - 286ms/step
Epoch 70/1000
2023-10-12 01:20:59.083 
Epoch 70/1000 
	 loss: 370.2481, MinusLogProbMetric: 370.2481, val_loss: 370.5009, val_MinusLogProbMetric: 370.5009

Epoch 70: val_loss did not improve from 337.02856
196/196 - 57s - loss: 370.2481 - MinusLogProbMetric: 370.2481 - val_loss: 370.5009 - val_MinusLogProbMetric: 370.5009 - lr: 4.5725e-07 - 57s/epoch - 289ms/step
Epoch 71/1000
2023-10-12 01:21:56.817 
Epoch 71/1000 
	 loss: 370.2788, MinusLogProbMetric: 370.2788, val_loss: 369.5208, val_MinusLogProbMetric: 369.5208

Epoch 71: val_loss did not improve from 337.02856
196/196 - 58s - loss: 370.2788 - MinusLogProbMetric: 370.2788 - val_loss: 369.5208 - val_MinusLogProbMetric: 369.5208 - lr: 4.5725e-07 - 58s/epoch - 295ms/step
Epoch 72/1000
2023-10-12 01:22:54.952 
Epoch 72/1000 
	 loss: 369.2353, MinusLogProbMetric: 369.2353, val_loss: 368.8568, val_MinusLogProbMetric: 368.8568

Epoch 72: val_loss did not improve from 337.02856
196/196 - 58s - loss: 369.2353 - MinusLogProbMetric: 369.2353 - val_loss: 368.8568 - val_MinusLogProbMetric: 368.8568 - lr: 4.5725e-07 - 58s/epoch - 297ms/step
Epoch 73/1000
2023-10-12 01:23:52.073 
Epoch 73/1000 
	 loss: 368.8195, MinusLogProbMetric: 368.8195, val_loss: 368.6585, val_MinusLogProbMetric: 368.6585

Epoch 73: val_loss did not improve from 337.02856
196/196 - 57s - loss: 368.8195 - MinusLogProbMetric: 368.8195 - val_loss: 368.6585 - val_MinusLogProbMetric: 368.6585 - lr: 4.5725e-07 - 57s/epoch - 291ms/step
Epoch 74/1000
2023-10-12 01:24:47.523 
Epoch 74/1000 
	 loss: 368.0161, MinusLogProbMetric: 368.0161, val_loss: 367.5567, val_MinusLogProbMetric: 367.5567

Epoch 74: val_loss did not improve from 337.02856
196/196 - 55s - loss: 368.0161 - MinusLogProbMetric: 368.0161 - val_loss: 367.5567 - val_MinusLogProbMetric: 367.5567 - lr: 4.5725e-07 - 55s/epoch - 283ms/step
Epoch 75/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 129: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-12 01:25:25.675 
Epoch 75/1000 
	 loss: nan, MinusLogProbMetric: 367.3839, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 75: val_loss did not improve from 337.02856
196/196 - 38s - loss: nan - MinusLogProbMetric: 367.3839 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 38s/epoch - 195ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 336.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_451"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_452 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f39dfee6cb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ac9269f00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ac9269f00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a78929870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c7431dae0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c7431cd00>, <keras.callbacks.ModelCheckpoint object at 0x7f3c7431cc70>, <keras.callbacks.EarlyStopping object at 0x7f3c7431c5b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c7431c520>, <keras.callbacks.TerminateOnNaN object at 0x7f3c7431cf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-12 01:25:33.753647
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-12 01:28:37.662 
Epoch 1/1000 
	 loss: 336.6716, MinusLogProbMetric: 336.6716, val_loss: 335.3416, val_MinusLogProbMetric: 335.3416

Epoch 1: val_loss improved from inf to 335.34161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 185s - loss: 336.6716 - MinusLogProbMetric: 336.6716 - val_loss: 335.3416 - val_MinusLogProbMetric: 335.3416 - lr: 1.5242e-07 - 185s/epoch - 944ms/step
Epoch 2/1000
2023-10-12 01:29:37.110 
Epoch 2/1000 
	 loss: 335.7408, MinusLogProbMetric: 335.7408, val_loss: 336.8807, val_MinusLogProbMetric: 336.8807

Epoch 2: val_loss did not improve from 335.34161
196/196 - 58s - loss: 335.7408 - MinusLogProbMetric: 335.7408 - val_loss: 336.8807 - val_MinusLogProbMetric: 336.8807 - lr: 1.5242e-07 - 58s/epoch - 295ms/step
Epoch 3/1000
2023-10-12 01:30:33.717 
Epoch 3/1000 
	 loss: 336.3084, MinusLogProbMetric: 336.3084, val_loss: 336.3945, val_MinusLogProbMetric: 336.3945

Epoch 3: val_loss did not improve from 335.34161
196/196 - 57s - loss: 336.3084 - MinusLogProbMetric: 336.3084 - val_loss: 336.3945 - val_MinusLogProbMetric: 336.3945 - lr: 1.5242e-07 - 57s/epoch - 289ms/step
Epoch 4/1000
2023-10-12 01:31:29.603 
Epoch 4/1000 
	 loss: 334.6808, MinusLogProbMetric: 334.6808, val_loss: 334.7831, val_MinusLogProbMetric: 334.7831

Epoch 4: val_loss improved from 335.34161 to 334.78314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 334.6808 - MinusLogProbMetric: 334.6808 - val_loss: 334.7831 - val_MinusLogProbMetric: 334.7831 - lr: 1.5242e-07 - 57s/epoch - 291ms/step
Epoch 5/1000
2023-10-12 01:32:27.401 
Epoch 5/1000 
	 loss: 333.4788, MinusLogProbMetric: 333.4788, val_loss: 333.3663, val_MinusLogProbMetric: 333.3663

Epoch 5: val_loss improved from 334.78314 to 333.36630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 333.4788 - MinusLogProbMetric: 333.4788 - val_loss: 333.3663 - val_MinusLogProbMetric: 333.3663 - lr: 1.5242e-07 - 58s/epoch - 295ms/step
Epoch 6/1000
2023-10-12 01:33:23.567 
Epoch 6/1000 
	 loss: 333.0945, MinusLogProbMetric: 333.0945, val_loss: 333.5878, val_MinusLogProbMetric: 333.5878

Epoch 6: val_loss did not improve from 333.36630
196/196 - 55s - loss: 333.0945 - MinusLogProbMetric: 333.0945 - val_loss: 333.5878 - val_MinusLogProbMetric: 333.5878 - lr: 1.5242e-07 - 55s/epoch - 281ms/step
Epoch 7/1000
2023-10-12 01:34:18.319 
Epoch 7/1000 
	 loss: 333.3417, MinusLogProbMetric: 333.3417, val_loss: 334.6192, val_MinusLogProbMetric: 334.6192

Epoch 7: val_loss did not improve from 333.36630
196/196 - 55s - loss: 333.3417 - MinusLogProbMetric: 333.3417 - val_loss: 334.6192 - val_MinusLogProbMetric: 334.6192 - lr: 1.5242e-07 - 55s/epoch - 279ms/step
Epoch 8/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 84: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-12 01:34:43.557 
Epoch 8/1000 
	 loss: nan, MinusLogProbMetric: 334.9244, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 8: val_loss did not improve from 333.36630
196/196 - 25s - loss: nan - MinusLogProbMetric: 334.9244 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 25s/epoch - 129ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 336.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_462"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_463 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f389f8e1e10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a0e631690>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a0e631690>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38adc2fee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f389dd99330>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f389dd998a0>, <keras.callbacks.ModelCheckpoint object at 0x7f389dd99960>, <keras.callbacks.EarlyStopping object at 0x7f389dd99bd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f389dd99c00>, <keras.callbacks.TerminateOnNaN object at 0x7f389dd99840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-12 01:34:51.614485
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-12 01:38:09.788 
Epoch 1/1000 
	 loss: 333.0909, MinusLogProbMetric: 333.0909, val_loss: 333.2798, val_MinusLogProbMetric: 333.2798

Epoch 1: val_loss improved from inf to 333.27979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 199s - loss: 333.0909 - MinusLogProbMetric: 333.0909 - val_loss: 333.2798 - val_MinusLogProbMetric: 333.2798 - lr: 5.0805e-08 - 199s/epoch - 1s/step
Epoch 2/1000
2023-10-12 01:39:07.979 
Epoch 2/1000 
	 loss: 333.2529, MinusLogProbMetric: 333.2529, val_loss: 333.7237, val_MinusLogProbMetric: 333.7237

Epoch 2: val_loss did not improve from 333.27979
196/196 - 57s - loss: 333.2529 - MinusLogProbMetric: 333.2529 - val_loss: 333.7237 - val_MinusLogProbMetric: 333.7237 - lr: 5.0805e-08 - 57s/epoch - 290ms/step
Epoch 3/1000
2023-10-12 01:40:04.618 
Epoch 3/1000 
	 loss: 333.7921, MinusLogProbMetric: 333.7921, val_loss: 334.4617, val_MinusLogProbMetric: 334.4617

Epoch 3: val_loss did not improve from 333.27979
196/196 - 57s - loss: 333.7921 - MinusLogProbMetric: 333.7921 - val_loss: 334.4617 - val_MinusLogProbMetric: 334.4617 - lr: 5.0805e-08 - 57s/epoch - 289ms/step
Epoch 4/1000
2023-10-12 01:41:02.204 
Epoch 4/1000 
	 loss: 334.4022, MinusLogProbMetric: 334.4022, val_loss: 334.6011, val_MinusLogProbMetric: 334.6011

Epoch 4: val_loss did not improve from 333.27979
196/196 - 58s - loss: 334.4022 - MinusLogProbMetric: 334.4022 - val_loss: 334.6011 - val_MinusLogProbMetric: 334.6011 - lr: 5.0805e-08 - 58s/epoch - 294ms/step
Epoch 5/1000
2023-10-12 01:41:59.532 
Epoch 5/1000 
	 loss: 334.1791, MinusLogProbMetric: 334.1791, val_loss: 334.4740, val_MinusLogProbMetric: 334.4740

Epoch 5: val_loss did not improve from 333.27979
196/196 - 57s - loss: 334.1791 - MinusLogProbMetric: 334.1791 - val_loss: 334.4740 - val_MinusLogProbMetric: 334.4740 - lr: 5.0805e-08 - 57s/epoch - 292ms/step
Epoch 6/1000
2023-10-12 01:42:58.620 
Epoch 6/1000 
	 loss: 333.9272, MinusLogProbMetric: 333.9272, val_loss: 333.9841, val_MinusLogProbMetric: 333.9841

Epoch 6: val_loss did not improve from 333.27979
196/196 - 59s - loss: 333.9272 - MinusLogProbMetric: 333.9272 - val_loss: 333.9841 - val_MinusLogProbMetric: 333.9841 - lr: 5.0805e-08 - 59s/epoch - 301ms/step
Epoch 7/1000
2023-10-12 01:43:56.479 
Epoch 7/1000 
	 loss: 333.4290, MinusLogProbMetric: 333.4290, val_loss: 333.4299, val_MinusLogProbMetric: 333.4299

Epoch 7: val_loss did not improve from 333.27979
196/196 - 58s - loss: 333.4290 - MinusLogProbMetric: 333.4290 - val_loss: 333.4299 - val_MinusLogProbMetric: 333.4299 - lr: 5.0805e-08 - 58s/epoch - 295ms/step
Epoch 8/1000
2023-10-12 01:44:54.138 
Epoch 8/1000 
	 loss: 333.0239, MinusLogProbMetric: 333.0239, val_loss: 333.1629, val_MinusLogProbMetric: 333.1629

Epoch 8: val_loss improved from 333.27979 to 333.16293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 333.0239 - MinusLogProbMetric: 333.0239 - val_loss: 333.1629 - val_MinusLogProbMetric: 333.1629 - lr: 5.0805e-08 - 59s/epoch - 300ms/step
Epoch 9/1000
2023-10-12 01:45:52.599 
Epoch 9/1000 
	 loss: 332.7116, MinusLogProbMetric: 332.7116, val_loss: 333.2208, val_MinusLogProbMetric: 333.2208

Epoch 9: val_loss did not improve from 333.16293
196/196 - 57s - loss: 332.7116 - MinusLogProbMetric: 332.7116 - val_loss: 333.2208 - val_MinusLogProbMetric: 333.2208 - lr: 5.0805e-08 - 57s/epoch - 293ms/step
Epoch 10/1000
2023-10-12 01:46:50.321 
Epoch 10/1000 
	 loss: 332.6524, MinusLogProbMetric: 332.6524, val_loss: 333.0172, val_MinusLogProbMetric: 333.0172

Epoch 10: val_loss improved from 333.16293 to 333.01721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 332.6524 - MinusLogProbMetric: 332.6524 - val_loss: 333.0172 - val_MinusLogProbMetric: 333.0172 - lr: 5.0805e-08 - 59s/epoch - 300ms/step
Epoch 11/1000
2023-10-12 01:47:49.434 
Epoch 11/1000 
	 loss: 332.6019, MinusLogProbMetric: 332.6019, val_loss: 332.9616, val_MinusLogProbMetric: 332.9616

Epoch 11: val_loss improved from 333.01721 to 332.96164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 332.6019 - MinusLogProbMetric: 332.6019 - val_loss: 332.9616 - val_MinusLogProbMetric: 332.9616 - lr: 5.0805e-08 - 59s/epoch - 302ms/step
Epoch 12/1000
2023-10-12 01:48:47.675 
Epoch 12/1000 
	 loss: 332.8856, MinusLogProbMetric: 332.8856, val_loss: 333.0588, val_MinusLogProbMetric: 333.0588

Epoch 12: val_loss did not improve from 332.96164
196/196 - 57s - loss: 332.8856 - MinusLogProbMetric: 332.8856 - val_loss: 333.0588 - val_MinusLogProbMetric: 333.0588 - lr: 5.0805e-08 - 57s/epoch - 291ms/step
Epoch 13/1000
2023-10-12 01:49:45.296 
Epoch 13/1000 
	 loss: 332.5109, MinusLogProbMetric: 332.5109, val_loss: 332.6499, val_MinusLogProbMetric: 332.6499

Epoch 13: val_loss improved from 332.96164 to 332.64987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 332.5109 - MinusLogProbMetric: 332.5109 - val_loss: 332.6499 - val_MinusLogProbMetric: 332.6499 - lr: 5.0805e-08 - 59s/epoch - 299ms/step
Epoch 14/1000
2023-10-12 01:50:42.560 
Epoch 14/1000 
	 loss: 332.2671, MinusLogProbMetric: 332.2671, val_loss: 332.3511, val_MinusLogProbMetric: 332.3511

Epoch 14: val_loss improved from 332.64987 to 332.35110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 332.2671 - MinusLogProbMetric: 332.2671 - val_loss: 332.3511 - val_MinusLogProbMetric: 332.3511 - lr: 5.0805e-08 - 57s/epoch - 292ms/step
Epoch 15/1000
2023-10-12 01:51:39.554 
Epoch 15/1000 
	 loss: 332.1989, MinusLogProbMetric: 332.1989, val_loss: 332.4850, val_MinusLogProbMetric: 332.4850

Epoch 15: val_loss did not improve from 332.35110
196/196 - 56s - loss: 332.1989 - MinusLogProbMetric: 332.1989 - val_loss: 332.4850 - val_MinusLogProbMetric: 332.4850 - lr: 5.0805e-08 - 56s/epoch - 285ms/step
Epoch 16/1000
2023-10-12 01:52:34.847 
Epoch 16/1000 
	 loss: 332.2336, MinusLogProbMetric: 332.2336, val_loss: 332.5133, val_MinusLogProbMetric: 332.5133

Epoch 16: val_loss did not improve from 332.35110
196/196 - 55s - loss: 332.2336 - MinusLogProbMetric: 332.2336 - val_loss: 332.5133 - val_MinusLogProbMetric: 332.5133 - lr: 5.0805e-08 - 55s/epoch - 282ms/step
Epoch 17/1000
2023-10-12 01:53:31.331 
Epoch 17/1000 
	 loss: 332.1774, MinusLogProbMetric: 332.1774, val_loss: 332.4279, val_MinusLogProbMetric: 332.4279

Epoch 17: val_loss did not improve from 332.35110
196/196 - 56s - loss: 332.1774 - MinusLogProbMetric: 332.1774 - val_loss: 332.4279 - val_MinusLogProbMetric: 332.4279 - lr: 5.0805e-08 - 56s/epoch - 288ms/step
Epoch 18/1000
2023-10-12 01:54:27.130 
Epoch 18/1000 
	 loss: 331.9456, MinusLogProbMetric: 331.9456, val_loss: 332.3587, val_MinusLogProbMetric: 332.3587

Epoch 18: val_loss did not improve from 332.35110
196/196 - 56s - loss: 331.9456 - MinusLogProbMetric: 331.9456 - val_loss: 332.3587 - val_MinusLogProbMetric: 332.3587 - lr: 5.0805e-08 - 56s/epoch - 285ms/step
Epoch 19/1000
2023-10-12 01:55:23.403 
Epoch 19/1000 
	 loss: 332.0743, MinusLogProbMetric: 332.0743, val_loss: 332.5136, val_MinusLogProbMetric: 332.5136

Epoch 19: val_loss did not improve from 332.35110
196/196 - 56s - loss: 332.0743 - MinusLogProbMetric: 332.0743 - val_loss: 332.5136 - val_MinusLogProbMetric: 332.5136 - lr: 5.0805e-08 - 56s/epoch - 287ms/step
Epoch 20/1000
2023-10-12 01:56:19.265 
Epoch 20/1000 
	 loss: 332.2573, MinusLogProbMetric: 332.2573, val_loss: 332.3599, val_MinusLogProbMetric: 332.3599

Epoch 20: val_loss did not improve from 332.35110
196/196 - 56s - loss: 332.2573 - MinusLogProbMetric: 332.2573 - val_loss: 332.3599 - val_MinusLogProbMetric: 332.3599 - lr: 5.0805e-08 - 56s/epoch - 285ms/step
Epoch 21/1000
2023-10-12 01:57:15.096 
Epoch 21/1000 
	 loss: 332.1064, MinusLogProbMetric: 332.1064, val_loss: 332.3658, val_MinusLogProbMetric: 332.3658

Epoch 21: val_loss did not improve from 332.35110
196/196 - 56s - loss: 332.1064 - MinusLogProbMetric: 332.1064 - val_loss: 332.3658 - val_MinusLogProbMetric: 332.3658 - lr: 5.0805e-08 - 56s/epoch - 285ms/step
Epoch 22/1000
2023-10-12 01:58:11.615 
Epoch 22/1000 
	 loss: 331.9449, MinusLogProbMetric: 331.9449, val_loss: 332.2948, val_MinusLogProbMetric: 332.2948

Epoch 22: val_loss improved from 332.35110 to 332.29483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 331.9449 - MinusLogProbMetric: 331.9449 - val_loss: 332.2948 - val_MinusLogProbMetric: 332.2948 - lr: 5.0805e-08 - 58s/epoch - 294ms/step
Epoch 23/1000
2023-10-12 01:59:08.808 
Epoch 23/1000 
	 loss: 331.7280, MinusLogProbMetric: 331.7280, val_loss: 331.9660, val_MinusLogProbMetric: 331.9660

Epoch 23: val_loss improved from 332.29483 to 331.96600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 331.7280 - MinusLogProbMetric: 331.7280 - val_loss: 331.9660 - val_MinusLogProbMetric: 331.9660 - lr: 5.0805e-08 - 57s/epoch - 292ms/step
Epoch 24/1000
2023-10-12 02:00:05.921 
Epoch 24/1000 
	 loss: 330.0432, MinusLogProbMetric: 330.0432, val_loss: 330.2826, val_MinusLogProbMetric: 330.2826

Epoch 24: val_loss improved from 331.96600 to 330.28259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 330.0432 - MinusLogProbMetric: 330.0432 - val_loss: 330.2826 - val_MinusLogProbMetric: 330.2826 - lr: 5.0805e-08 - 57s/epoch - 291ms/step
Epoch 25/1000
2023-10-12 02:01:03.101 
Epoch 25/1000 
	 loss: 329.8873, MinusLogProbMetric: 329.8873, val_loss: 330.3025, val_MinusLogProbMetric: 330.3025

Epoch 25: val_loss did not improve from 330.28259
196/196 - 56s - loss: 329.8873 - MinusLogProbMetric: 329.8873 - val_loss: 330.3025 - val_MinusLogProbMetric: 330.3025 - lr: 5.0805e-08 - 56s/epoch - 286ms/step
Epoch 26/1000
2023-10-12 02:01:59.666 
Epoch 26/1000 
	 loss: 329.7802, MinusLogProbMetric: 329.7802, val_loss: 330.4171, val_MinusLogProbMetric: 330.4171

Epoch 26: val_loss did not improve from 330.28259
196/196 - 57s - loss: 329.7802 - MinusLogProbMetric: 329.7802 - val_loss: 330.4171 - val_MinusLogProbMetric: 330.4171 - lr: 5.0805e-08 - 57s/epoch - 289ms/step
Epoch 27/1000
2023-10-12 02:02:55.944 
Epoch 27/1000 
	 loss: 329.7318, MinusLogProbMetric: 329.7318, val_loss: 330.2144, val_MinusLogProbMetric: 330.2144

Epoch 27: val_loss improved from 330.28259 to 330.21445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 329.7318 - MinusLogProbMetric: 329.7318 - val_loss: 330.2144 - val_MinusLogProbMetric: 330.2144 - lr: 5.0805e-08 - 57s/epoch - 292ms/step
Epoch 28/1000
2023-10-12 02:03:53.481 
Epoch 28/1000 
	 loss: 329.5994, MinusLogProbMetric: 329.5994, val_loss: 330.1851, val_MinusLogProbMetric: 330.1851

Epoch 28: val_loss improved from 330.21445 to 330.18509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 329.5994 - MinusLogProbMetric: 329.5994 - val_loss: 330.1851 - val_MinusLogProbMetric: 330.1851 - lr: 5.0805e-08 - 58s/epoch - 293ms/step
Epoch 29/1000
2023-10-12 02:04:50.387 
Epoch 29/1000 
	 loss: 329.5719, MinusLogProbMetric: 329.5719, val_loss: 330.1164, val_MinusLogProbMetric: 330.1164

Epoch 29: val_loss improved from 330.18509 to 330.11639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 329.5719 - MinusLogProbMetric: 329.5719 - val_loss: 330.1164 - val_MinusLogProbMetric: 330.1164 - lr: 5.0805e-08 - 57s/epoch - 290ms/step
Epoch 30/1000
2023-10-12 02:05:47.815 
Epoch 30/1000 
	 loss: 329.3265, MinusLogProbMetric: 329.3265, val_loss: 329.6647, val_MinusLogProbMetric: 329.6647

Epoch 30: val_loss improved from 330.11639 to 329.66470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 329.3265 - MinusLogProbMetric: 329.3265 - val_loss: 329.6647 - val_MinusLogProbMetric: 329.6647 - lr: 5.0805e-08 - 57s/epoch - 293ms/step
Epoch 31/1000
2023-10-12 02:06:44.082 
Epoch 31/1000 
	 loss: 329.2770, MinusLogProbMetric: 329.2770, val_loss: 329.7481, val_MinusLogProbMetric: 329.7481

Epoch 31: val_loss did not improve from 329.66470
196/196 - 55s - loss: 329.2770 - MinusLogProbMetric: 329.2770 - val_loss: 329.7481 - val_MinusLogProbMetric: 329.7481 - lr: 5.0805e-08 - 55s/epoch - 282ms/step
Epoch 32/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 73: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-12 02:07:07.216 
Epoch 32/1000 
	 loss: nan, MinusLogProbMetric: 329.2716, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 32: val_loss did not improve from 329.66470
196/196 - 23s - loss: nan - MinusLogProbMetric: 329.2716 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 23s/epoch - 118ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 336.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_473"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_474 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f39f6fc9d20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3cc591da20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3cc591da20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f399ff45870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cc59abd00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f39a7ae02b0>, <keras.callbacks.ModelCheckpoint object at 0x7f39a7ae0370>, <keras.callbacks.EarlyStopping object at 0x7f39a7ae05e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f39a7ae0610>, <keras.callbacks.TerminateOnNaN object at 0x7f39a7ae0250>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-10-12 02:07:15.550320
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-10-12 02:10:38.816 
Epoch 1/1000 
	 loss: 329.1351, MinusLogProbMetric: 329.1351, val_loss: 329.7100, val_MinusLogProbMetric: 329.7100

Epoch 1: val_loss improved from inf to 329.71002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 204s - loss: 329.1351 - MinusLogProbMetric: 329.1351 - val_loss: 329.7100 - val_MinusLogProbMetric: 329.7100 - lr: 1.6935e-08 - 204s/epoch - 1s/step
Epoch 2/1000
2023-10-12 02:11:40.190 
Epoch 2/1000 
	 loss: 329.0500, MinusLogProbMetric: 329.0500, val_loss: 329.5797, val_MinusLogProbMetric: 329.5797

Epoch 2: val_loss improved from 329.71002 to 329.57974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 61s - loss: 329.0500 - MinusLogProbMetric: 329.0500 - val_loss: 329.5797 - val_MinusLogProbMetric: 329.5797 - lr: 1.6935e-08 - 61s/epoch - 309ms/step
Epoch 3/1000
2023-10-12 02:12:38.473 
Epoch 3/1000 
	 loss: 329.0259, MinusLogProbMetric: 329.0259, val_loss: 329.2798, val_MinusLogProbMetric: 329.2798

Epoch 3: val_loss improved from 329.57974 to 329.27985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 329.0259 - MinusLogProbMetric: 329.0259 - val_loss: 329.2798 - val_MinusLogProbMetric: 329.2798 - lr: 1.6935e-08 - 59s/epoch - 299ms/step
Epoch 4/1000
2023-10-12 02:13:38.347 
Epoch 4/1000 
	 loss: 328.9459, MinusLogProbMetric: 328.9459, val_loss: 329.4340, val_MinusLogProbMetric: 329.4340

Epoch 4: val_loss did not improve from 329.27985
196/196 - 59s - loss: 328.9459 - MinusLogProbMetric: 328.9459 - val_loss: 329.4340 - val_MinusLogProbMetric: 329.4340 - lr: 1.6935e-08 - 59s/epoch - 300ms/step
Epoch 5/1000
2023-10-12 02:14:36.722 
Epoch 5/1000 
	 loss: 328.9828, MinusLogProbMetric: 328.9828, val_loss: 329.6088, val_MinusLogProbMetric: 329.6088

Epoch 5: val_loss did not improve from 329.27985
196/196 - 58s - loss: 328.9828 - MinusLogProbMetric: 328.9828 - val_loss: 329.6088 - val_MinusLogProbMetric: 329.6088 - lr: 1.6935e-08 - 58s/epoch - 298ms/step
Epoch 6/1000
2023-10-12 02:15:33.487 
Epoch 6/1000 
	 loss: 328.7918, MinusLogProbMetric: 328.7918, val_loss: 329.4727, val_MinusLogProbMetric: 329.4727

Epoch 6: val_loss did not improve from 329.27985
196/196 - 57s - loss: 328.7918 - MinusLogProbMetric: 328.7918 - val_loss: 329.4727 - val_MinusLogProbMetric: 329.4727 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 7/1000
2023-10-12 02:16:30.526 
Epoch 7/1000 
	 loss: 328.9109, MinusLogProbMetric: 328.9109, val_loss: 329.6583, val_MinusLogProbMetric: 329.6583

Epoch 7: val_loss did not improve from 329.27985
196/196 - 57s - loss: 328.9109 - MinusLogProbMetric: 328.9109 - val_loss: 329.6583 - val_MinusLogProbMetric: 329.6583 - lr: 1.6935e-08 - 57s/epoch - 291ms/step
Epoch 8/1000
2023-10-12 02:17:27.978 
Epoch 8/1000 
	 loss: 328.9304, MinusLogProbMetric: 328.9304, val_loss: 329.4230, val_MinusLogProbMetric: 329.4230

Epoch 8: val_loss did not improve from 329.27985
196/196 - 57s - loss: 328.9304 - MinusLogProbMetric: 328.9304 - val_loss: 329.4230 - val_MinusLogProbMetric: 329.4230 - lr: 1.6935e-08 - 57s/epoch - 293ms/step
Epoch 9/1000
2023-10-12 02:18:25.444 
Epoch 9/1000 
	 loss: 328.9694, MinusLogProbMetric: 328.9694, val_loss: 329.5038, val_MinusLogProbMetric: 329.5038

Epoch 9: val_loss did not improve from 329.27985
196/196 - 57s - loss: 328.9694 - MinusLogProbMetric: 328.9694 - val_loss: 329.5038 - val_MinusLogProbMetric: 329.5038 - lr: 1.6935e-08 - 57s/epoch - 293ms/step
Epoch 10/1000
2023-10-12 02:19:22.025 
Epoch 10/1000 
	 loss: 328.8343, MinusLogProbMetric: 328.8343, val_loss: 329.3909, val_MinusLogProbMetric: 329.3909

Epoch 10: val_loss did not improve from 329.27985
196/196 - 57s - loss: 328.8343 - MinusLogProbMetric: 328.8343 - val_loss: 329.3909 - val_MinusLogProbMetric: 329.3909 - lr: 1.6935e-08 - 57s/epoch - 289ms/step
Epoch 11/1000
2023-10-12 02:20:18.383 
Epoch 11/1000 
	 loss: 328.7883, MinusLogProbMetric: 328.7883, val_loss: 329.2344, val_MinusLogProbMetric: 329.2344

Epoch 11: val_loss improved from 329.27985 to 329.23444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 328.7883 - MinusLogProbMetric: 328.7883 - val_loss: 329.2344 - val_MinusLogProbMetric: 329.2344 - lr: 1.6935e-08 - 58s/epoch - 294ms/step
Epoch 12/1000
2023-10-12 02:21:16.147 
Epoch 12/1000 
	 loss: 328.7094, MinusLogProbMetric: 328.7094, val_loss: 329.2878, val_MinusLogProbMetric: 329.2878

Epoch 12: val_loss did not improve from 329.23444
196/196 - 57s - loss: 328.7094 - MinusLogProbMetric: 328.7094 - val_loss: 329.2878 - val_MinusLogProbMetric: 329.2878 - lr: 1.6935e-08 - 57s/epoch - 289ms/step
Epoch 13/1000
2023-10-12 02:22:14.596 
Epoch 13/1000 
	 loss: 328.6350, MinusLogProbMetric: 328.6350, val_loss: 329.3376, val_MinusLogProbMetric: 329.3376

Epoch 13: val_loss did not improve from 329.23444
196/196 - 58s - loss: 328.6350 - MinusLogProbMetric: 328.6350 - val_loss: 329.3376 - val_MinusLogProbMetric: 329.3376 - lr: 1.6935e-08 - 58s/epoch - 298ms/step
Epoch 14/1000
2023-10-12 02:23:13.257 
Epoch 14/1000 
	 loss: 328.6209, MinusLogProbMetric: 328.6209, val_loss: 329.2857, val_MinusLogProbMetric: 329.2857

Epoch 14: val_loss did not improve from 329.23444
196/196 - 59s - loss: 328.6209 - MinusLogProbMetric: 328.6209 - val_loss: 329.2857 - val_MinusLogProbMetric: 329.2857 - lr: 1.6935e-08 - 59s/epoch - 299ms/step
Epoch 15/1000
2023-10-12 02:24:13.134 
Epoch 15/1000 
	 loss: 328.6586, MinusLogProbMetric: 328.6586, val_loss: 329.1970, val_MinusLogProbMetric: 329.1970

Epoch 15: val_loss improved from 329.23444 to 329.19699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 61s - loss: 328.6586 - MinusLogProbMetric: 328.6586 - val_loss: 329.1970 - val_MinusLogProbMetric: 329.1970 - lr: 1.6935e-08 - 61s/epoch - 311ms/step
Epoch 16/1000
2023-10-12 02:25:10.654 
Epoch 16/1000 
	 loss: 328.6602, MinusLogProbMetric: 328.6602, val_loss: 329.2093, val_MinusLogProbMetric: 329.2093

Epoch 16: val_loss did not improve from 329.19699
196/196 - 57s - loss: 328.6602 - MinusLogProbMetric: 328.6602 - val_loss: 329.2093 - val_MinusLogProbMetric: 329.2093 - lr: 1.6935e-08 - 57s/epoch - 288ms/step
Epoch 17/1000
2023-10-12 02:26:07.557 
Epoch 17/1000 
	 loss: 328.6921, MinusLogProbMetric: 328.6921, val_loss: 329.3268, val_MinusLogProbMetric: 329.3268

Epoch 17: val_loss did not improve from 329.19699
196/196 - 57s - loss: 328.6921 - MinusLogProbMetric: 328.6921 - val_loss: 329.3268 - val_MinusLogProbMetric: 329.3268 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 18/1000
2023-10-12 02:27:04.208 
Epoch 18/1000 
	 loss: 328.6807, MinusLogProbMetric: 328.6807, val_loss: 329.3251, val_MinusLogProbMetric: 329.3251

Epoch 18: val_loss did not improve from 329.19699
196/196 - 57s - loss: 328.6807 - MinusLogProbMetric: 328.6807 - val_loss: 329.3251 - val_MinusLogProbMetric: 329.3251 - lr: 1.6935e-08 - 57s/epoch - 289ms/step
Epoch 19/1000
2023-10-12 02:28:01.076 
Epoch 19/1000 
	 loss: 328.6802, MinusLogProbMetric: 328.6802, val_loss: 329.1786, val_MinusLogProbMetric: 329.1786

Epoch 19: val_loss improved from 329.19699 to 329.17859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 328.6802 - MinusLogProbMetric: 328.6802 - val_loss: 329.1786 - val_MinusLogProbMetric: 329.1786 - lr: 1.6935e-08 - 58s/epoch - 296ms/step
Epoch 20/1000
2023-10-12 02:28:58.561 
Epoch 20/1000 
	 loss: 328.6286, MinusLogProbMetric: 328.6286, val_loss: 329.1691, val_MinusLogProbMetric: 329.1691

Epoch 20: val_loss improved from 329.17859 to 329.16907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 328.6286 - MinusLogProbMetric: 328.6286 - val_loss: 329.1691 - val_MinusLogProbMetric: 329.1691 - lr: 1.6935e-08 - 57s/epoch - 293ms/step
Epoch 21/1000
2023-10-12 02:29:56.159 
Epoch 21/1000 
	 loss: 328.6206, MinusLogProbMetric: 328.6206, val_loss: 329.1795, val_MinusLogProbMetric: 329.1795

Epoch 21: val_loss did not improve from 329.16907
196/196 - 57s - loss: 328.6206 - MinusLogProbMetric: 328.6206 - val_loss: 329.1795 - val_MinusLogProbMetric: 329.1795 - lr: 1.6935e-08 - 57s/epoch - 288ms/step
Epoch 22/1000
2023-10-12 02:30:53.236 
Epoch 22/1000 
	 loss: 328.5558, MinusLogProbMetric: 328.5558, val_loss: 329.1802, val_MinusLogProbMetric: 329.1802

Epoch 22: val_loss did not improve from 329.16907
196/196 - 57s - loss: 328.5558 - MinusLogProbMetric: 328.5558 - val_loss: 329.1802 - val_MinusLogProbMetric: 329.1802 - lr: 1.6935e-08 - 57s/epoch - 291ms/step
Epoch 23/1000
2023-10-12 02:31:50.736 
Epoch 23/1000 
	 loss: 328.5082, MinusLogProbMetric: 328.5082, val_loss: 329.0496, val_MinusLogProbMetric: 329.0496

Epoch 23: val_loss improved from 329.16907 to 329.04962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 58s - loss: 328.5082 - MinusLogProbMetric: 328.5082 - val_loss: 329.0496 - val_MinusLogProbMetric: 329.0496 - lr: 1.6935e-08 - 58s/epoch - 298ms/step
Epoch 24/1000
2023-10-12 02:32:49.697 
Epoch 24/1000 
	 loss: 328.4406, MinusLogProbMetric: 328.4406, val_loss: 328.9567, val_MinusLogProbMetric: 328.9567

Epoch 24: val_loss improved from 329.04962 to 328.95670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 328.4406 - MinusLogProbMetric: 328.4406 - val_loss: 328.9567 - val_MinusLogProbMetric: 328.9567 - lr: 1.6935e-08 - 59s/epoch - 301ms/step
Epoch 25/1000
2023-10-12 02:33:48.010 
Epoch 25/1000 
	 loss: 328.3676, MinusLogProbMetric: 328.3676, val_loss: 329.0204, val_MinusLogProbMetric: 329.0204

Epoch 25: val_loss did not improve from 328.95670
196/196 - 57s - loss: 328.3676 - MinusLogProbMetric: 328.3676 - val_loss: 329.0204 - val_MinusLogProbMetric: 329.0204 - lr: 1.6935e-08 - 57s/epoch - 292ms/step
Epoch 26/1000
2023-10-12 02:34:43.848 
Epoch 26/1000 
	 loss: 328.3386, MinusLogProbMetric: 328.3386, val_loss: 328.9317, val_MinusLogProbMetric: 328.9317

Epoch 26: val_loss improved from 328.95670 to 328.93167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 57s - loss: 328.3386 - MinusLogProbMetric: 328.3386 - val_loss: 328.9317 - val_MinusLogProbMetric: 328.9317 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 27/1000
2023-10-12 02:35:41.698 
Epoch 27/1000 
	 loss: 328.3852, MinusLogProbMetric: 328.3852, val_loss: 329.0648, val_MinusLogProbMetric: 329.0648

Epoch 27: val_loss did not improve from 328.93167
196/196 - 57s - loss: 328.3852 - MinusLogProbMetric: 328.3852 - val_loss: 329.0648 - val_MinusLogProbMetric: 329.0648 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 28/1000
2023-10-12 02:36:38.228 
Epoch 28/1000 
	 loss: 328.3376, MinusLogProbMetric: 328.3376, val_loss: 329.0248, val_MinusLogProbMetric: 329.0248

Epoch 28: val_loss did not improve from 328.93167
196/196 - 57s - loss: 328.3376 - MinusLogProbMetric: 328.3376 - val_loss: 329.0248 - val_MinusLogProbMetric: 329.0248 - lr: 1.6935e-08 - 57s/epoch - 288ms/step
Epoch 29/1000
2023-10-12 02:37:35.355 
Epoch 29/1000 
	 loss: 328.4588, MinusLogProbMetric: 328.4588, val_loss: 329.0931, val_MinusLogProbMetric: 329.0931

Epoch 29: val_loss did not improve from 328.93167
196/196 - 57s - loss: 328.4588 - MinusLogProbMetric: 328.4588 - val_loss: 329.0931 - val_MinusLogProbMetric: 329.0931 - lr: 1.6935e-08 - 57s/epoch - 291ms/step
Epoch 30/1000
2023-10-12 02:38:31.934 
Epoch 30/1000 
	 loss: 328.4603, MinusLogProbMetric: 328.4603, val_loss: 329.0029, val_MinusLogProbMetric: 329.0029

Epoch 30: val_loss did not improve from 328.93167
196/196 - 57s - loss: 328.4603 - MinusLogProbMetric: 328.4603 - val_loss: 329.0029 - val_MinusLogProbMetric: 329.0029 - lr: 1.6935e-08 - 57s/epoch - 289ms/step
Epoch 31/1000
2023-10-12 02:39:29.940 
Epoch 31/1000 
	 loss: 328.4568, MinusLogProbMetric: 328.4568, val_loss: 328.9577, val_MinusLogProbMetric: 328.9577

Epoch 31: val_loss did not improve from 328.93167
196/196 - 58s - loss: 328.4568 - MinusLogProbMetric: 328.4568 - val_loss: 328.9577 - val_MinusLogProbMetric: 328.9577 - lr: 1.6935e-08 - 58s/epoch - 296ms/step
Epoch 32/1000
2023-10-12 02:40:27.668 
Epoch 32/1000 
	 loss: 328.4007, MinusLogProbMetric: 328.4007, val_loss: 328.8926, val_MinusLogProbMetric: 328.8926

Epoch 32: val_loss improved from 328.93167 to 328.89261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 328.4007 - MinusLogProbMetric: 328.4007 - val_loss: 328.8926 - val_MinusLogProbMetric: 328.8926 - lr: 1.6935e-08 - 59s/epoch - 300ms/step
Epoch 33/1000
2023-10-12 02:41:27.802 
Epoch 33/1000 
	 loss: 328.3824, MinusLogProbMetric: 328.3824, val_loss: 328.8394, val_MinusLogProbMetric: 328.8394

Epoch 33: val_loss improved from 328.89261 to 328.83936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 60s - loss: 328.3824 - MinusLogProbMetric: 328.3824 - val_loss: 328.8394 - val_MinusLogProbMetric: 328.8394 - lr: 1.6935e-08 - 60s/epoch - 307ms/step
Epoch 34/1000
2023-10-12 02:42:25.695 
Epoch 34/1000 
	 loss: 328.3192, MinusLogProbMetric: 328.3192, val_loss: 328.9498, val_MinusLogProbMetric: 328.9498

Epoch 34: val_loss did not improve from 328.83936
196/196 - 57s - loss: 328.3192 - MinusLogProbMetric: 328.3192 - val_loss: 328.9498 - val_MinusLogProbMetric: 328.9498 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 35/1000
2023-10-12 02:43:22.107 
Epoch 35/1000 
	 loss: 328.2809, MinusLogProbMetric: 328.2809, val_loss: 328.8685, val_MinusLogProbMetric: 328.8685

Epoch 35: val_loss did not improve from 328.83936
196/196 - 56s - loss: 328.2809 - MinusLogProbMetric: 328.2809 - val_loss: 328.8685 - val_MinusLogProbMetric: 328.8685 - lr: 1.6935e-08 - 56s/epoch - 288ms/step
Epoch 36/1000
2023-10-12 02:44:19.621 
Epoch 36/1000 
	 loss: 328.1793, MinusLogProbMetric: 328.1793, val_loss: 328.7362, val_MinusLogProbMetric: 328.7362

Epoch 36: val_loss improved from 328.83936 to 328.73621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 59s - loss: 328.1793 - MinusLogProbMetric: 328.1793 - val_loss: 328.7362 - val_MinusLogProbMetric: 328.7362 - lr: 1.6935e-08 - 59s/epoch - 299ms/step
Epoch 37/1000
2023-10-12 02:45:17.530 
Epoch 37/1000 
	 loss: 328.1897, MinusLogProbMetric: 328.1897, val_loss: 328.8580, val_MinusLogProbMetric: 328.8580

Epoch 37: val_loss did not improve from 328.73621
196/196 - 57s - loss: 328.1897 - MinusLogProbMetric: 328.1897 - val_loss: 328.8580 - val_MinusLogProbMetric: 328.8580 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 38/1000
2023-10-12 02:46:14.715 
Epoch 38/1000 
	 loss: 328.2144, MinusLogProbMetric: 328.2144, val_loss: 328.8020, val_MinusLogProbMetric: 328.8020

Epoch 38: val_loss did not improve from 328.73621
196/196 - 57s - loss: 328.2144 - MinusLogProbMetric: 328.2144 - val_loss: 328.8020 - val_MinusLogProbMetric: 328.8020 - lr: 1.6935e-08 - 57s/epoch - 292ms/step
Epoch 39/1000
2023-10-12 02:47:11.649 
Epoch 39/1000 
	 loss: 328.1968, MinusLogProbMetric: 328.1968, val_loss: 328.8438, val_MinusLogProbMetric: 328.8438

Epoch 39: val_loss did not improve from 328.73621
196/196 - 57s - loss: 328.1968 - MinusLogProbMetric: 328.1968 - val_loss: 328.8438 - val_MinusLogProbMetric: 328.8438 - lr: 1.6935e-08 - 57s/epoch - 290ms/step
Epoch 40/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 129: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-12 02:47:50.947 
Epoch 40/1000 
	 loss: nan, MinusLogProbMetric: 328.1146, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 40: val_loss did not improve from 328.73621
196/196 - 39s - loss: nan - MinusLogProbMetric: 328.1146 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 39s/epoch - 200ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 336/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

===========
Generating train data for run 338.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_338/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_338/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_338/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_338
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_479"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_480 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f388ea76a40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8c49e6e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8c49e6e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39d451b7c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c743ebdc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c743e9000>, <keras.callbacks.ModelCheckpoint object at 0x7f3c743e8280>, <keras.callbacks.EarlyStopping object at 0x7f3c743e8370>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c743ea200>, <keras.callbacks.TerminateOnNaN object at 0x7f3a0d0d8550>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_338/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 338/720 with hyperparameters:
timestamp = 2023-10-12 02:47:55.234278
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-12 02:49:10.514 
Epoch 1/1000 
	 loss: 400.5111, MinusLogProbMetric: 400.5111, val_loss: 111.3148, val_MinusLogProbMetric: 111.3148

Epoch 1: val_loss improved from inf to 111.31483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 76s - loss: 400.5111 - MinusLogProbMetric: 400.5111 - val_loss: 111.3148 - val_MinusLogProbMetric: 111.3148 - lr: 0.0010 - 76s/epoch - 385ms/step
Epoch 2/1000
2023-10-12 02:49:38.116 
Epoch 2/1000 
	 loss: 85.8762, MinusLogProbMetric: 85.8762, val_loss: 71.4326, val_MinusLogProbMetric: 71.4326

Epoch 2: val_loss improved from 111.31483 to 71.43256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 85.8762 - MinusLogProbMetric: 85.8762 - val_loss: 71.4326 - val_MinusLogProbMetric: 71.4326 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 3/1000
2023-10-12 02:50:05.764 
Epoch 3/1000 
	 loss: 61.6545, MinusLogProbMetric: 61.6545, val_loss: 56.7333, val_MinusLogProbMetric: 56.7333

Epoch 3: val_loss improved from 71.43256 to 56.73328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 61.6545 - MinusLogProbMetric: 61.6545 - val_loss: 56.7333 - val_MinusLogProbMetric: 56.7333 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 4/1000
2023-10-12 02:50:33.149 
Epoch 4/1000 
	 loss: 52.1841, MinusLogProbMetric: 52.1841, val_loss: 49.9403, val_MinusLogProbMetric: 49.9403

Epoch 4: val_loss improved from 56.73328 to 49.94028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 52.1841 - MinusLogProbMetric: 52.1841 - val_loss: 49.9403 - val_MinusLogProbMetric: 49.9403 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 5/1000
2023-10-12 02:51:00.991 
Epoch 5/1000 
	 loss: 48.1078, MinusLogProbMetric: 48.1078, val_loss: 44.5074, val_MinusLogProbMetric: 44.5074

Epoch 5: val_loss improved from 49.94028 to 44.50741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 48.1078 - MinusLogProbMetric: 48.1078 - val_loss: 44.5074 - val_MinusLogProbMetric: 44.5074 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 6/1000
2023-10-12 02:51:27.813 
Epoch 6/1000 
	 loss: 44.5598, MinusLogProbMetric: 44.5598, val_loss: 45.5624, val_MinusLogProbMetric: 45.5624

Epoch 6: val_loss did not improve from 44.50741
196/196 - 26s - loss: 44.5598 - MinusLogProbMetric: 44.5598 - val_loss: 45.5624 - val_MinusLogProbMetric: 45.5624 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 7/1000
2023-10-12 02:51:54.053 
Epoch 7/1000 
	 loss: 43.1006, MinusLogProbMetric: 43.1006, val_loss: 47.8565, val_MinusLogProbMetric: 47.8565

Epoch 7: val_loss did not improve from 44.50741
196/196 - 26s - loss: 43.1006 - MinusLogProbMetric: 43.1006 - val_loss: 47.8565 - val_MinusLogProbMetric: 47.8565 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 8/1000
2023-10-12 02:52:21.041 
Epoch 8/1000 
	 loss: 41.3001, MinusLogProbMetric: 41.3001, val_loss: 39.7685, val_MinusLogProbMetric: 39.7685

Epoch 8: val_loss improved from 44.50741 to 39.76848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 41.3001 - MinusLogProbMetric: 41.3001 - val_loss: 39.7685 - val_MinusLogProbMetric: 39.7685 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 9/1000
2023-10-12 02:52:47.715 
Epoch 9/1000 
	 loss: 40.6305, MinusLogProbMetric: 40.6305, val_loss: 40.4361, val_MinusLogProbMetric: 40.4361

Epoch 9: val_loss did not improve from 39.76848
196/196 - 26s - loss: 40.6305 - MinusLogProbMetric: 40.6305 - val_loss: 40.4361 - val_MinusLogProbMetric: 40.4361 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 10/1000
2023-10-12 02:53:13.792 
Epoch 10/1000 
	 loss: 39.2992, MinusLogProbMetric: 39.2992, val_loss: 42.7120, val_MinusLogProbMetric: 42.7120

Epoch 10: val_loss did not improve from 39.76848
196/196 - 26s - loss: 39.2992 - MinusLogProbMetric: 39.2992 - val_loss: 42.7120 - val_MinusLogProbMetric: 42.7120 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 11/1000
2023-10-12 02:53:39.696 
Epoch 11/1000 
	 loss: 38.9256, MinusLogProbMetric: 38.9256, val_loss: 38.3036, val_MinusLogProbMetric: 38.3036

Epoch 11: val_loss improved from 39.76848 to 38.30359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 38.9256 - MinusLogProbMetric: 38.9256 - val_loss: 38.3036 - val_MinusLogProbMetric: 38.3036 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 12/1000
2023-10-12 02:54:05.764 
Epoch 12/1000 
	 loss: 38.5049, MinusLogProbMetric: 38.5049, val_loss: 44.5565, val_MinusLogProbMetric: 44.5565

Epoch 12: val_loss did not improve from 38.30359
196/196 - 26s - loss: 38.5049 - MinusLogProbMetric: 38.5049 - val_loss: 44.5565 - val_MinusLogProbMetric: 44.5565 - lr: 0.0010 - 26s/epoch - 130ms/step
Epoch 13/1000
2023-10-12 02:54:31.399 
Epoch 13/1000 
	 loss: 37.9788, MinusLogProbMetric: 37.9788, val_loss: 36.9210, val_MinusLogProbMetric: 36.9210

Epoch 13: val_loss improved from 38.30359 to 36.92099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 37.9788 - MinusLogProbMetric: 37.9788 - val_loss: 36.9210 - val_MinusLogProbMetric: 36.9210 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 14/1000
2023-10-12 02:54:57.595 
Epoch 14/1000 
	 loss: 37.5800, MinusLogProbMetric: 37.5800, val_loss: 38.1285, val_MinusLogProbMetric: 38.1285

Epoch 14: val_loss did not improve from 36.92099
196/196 - 26s - loss: 37.5800 - MinusLogProbMetric: 37.5800 - val_loss: 38.1285 - val_MinusLogProbMetric: 38.1285 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 15/1000
2023-10-12 02:55:23.477 
Epoch 15/1000 
	 loss: 37.1561, MinusLogProbMetric: 37.1561, val_loss: 38.3620, val_MinusLogProbMetric: 38.3620

Epoch 15: val_loss did not improve from 36.92099
196/196 - 26s - loss: 37.1561 - MinusLogProbMetric: 37.1561 - val_loss: 38.3620 - val_MinusLogProbMetric: 38.3620 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 16/1000
2023-10-12 02:55:50.095 
Epoch 16/1000 
	 loss: 37.0415, MinusLogProbMetric: 37.0415, val_loss: 37.9744, val_MinusLogProbMetric: 37.9744

Epoch 16: val_loss did not improve from 36.92099
196/196 - 27s - loss: 37.0415 - MinusLogProbMetric: 37.0415 - val_loss: 37.9744 - val_MinusLogProbMetric: 37.9744 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 17/1000
2023-10-12 02:56:17.078 
Epoch 17/1000 
	 loss: 36.3254, MinusLogProbMetric: 36.3254, val_loss: 36.2526, val_MinusLogProbMetric: 36.2526

Epoch 17: val_loss improved from 36.92099 to 36.25259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 36.3254 - MinusLogProbMetric: 36.3254 - val_loss: 36.2526 - val_MinusLogProbMetric: 36.2526 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 18/1000
2023-10-12 02:56:45.510 
Epoch 18/1000 
	 loss: 36.1183, MinusLogProbMetric: 36.1183, val_loss: 35.6843, val_MinusLogProbMetric: 35.6843

Epoch 18: val_loss improved from 36.25259 to 35.68431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 36.1183 - MinusLogProbMetric: 36.1183 - val_loss: 35.6843 - val_MinusLogProbMetric: 35.6843 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 19/1000
2023-10-12 02:57:13.168 
Epoch 19/1000 
	 loss: 35.8525, MinusLogProbMetric: 35.8525, val_loss: 34.9933, val_MinusLogProbMetric: 34.9933

Epoch 19: val_loss improved from 35.68431 to 34.99332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 35.8525 - MinusLogProbMetric: 35.8525 - val_loss: 34.9933 - val_MinusLogProbMetric: 34.9933 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 20/1000
2023-10-12 02:57:41.396 
Epoch 20/1000 
	 loss: 35.8827, MinusLogProbMetric: 35.8827, val_loss: 35.2183, val_MinusLogProbMetric: 35.2183

Epoch 20: val_loss did not improve from 34.99332
196/196 - 28s - loss: 35.8827 - MinusLogProbMetric: 35.8827 - val_loss: 35.2183 - val_MinusLogProbMetric: 35.2183 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 21/1000
2023-10-12 02:58:08.106 
Epoch 21/1000 
	 loss: 35.7869, MinusLogProbMetric: 35.7869, val_loss: 35.4014, val_MinusLogProbMetric: 35.4014

Epoch 21: val_loss did not improve from 34.99332
196/196 - 27s - loss: 35.7869 - MinusLogProbMetric: 35.7869 - val_loss: 35.4014 - val_MinusLogProbMetric: 35.4014 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 22/1000
2023-10-12 02:58:34.882 
Epoch 22/1000 
	 loss: 35.3733, MinusLogProbMetric: 35.3733, val_loss: 35.9979, val_MinusLogProbMetric: 35.9979

Epoch 22: val_loss did not improve from 34.99332
196/196 - 27s - loss: 35.3733 - MinusLogProbMetric: 35.3733 - val_loss: 35.9979 - val_MinusLogProbMetric: 35.9979 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 23/1000
2023-10-12 02:59:01.116 
Epoch 23/1000 
	 loss: 35.0973, MinusLogProbMetric: 35.0973, val_loss: 34.0705, val_MinusLogProbMetric: 34.0705

Epoch 23: val_loss improved from 34.99332 to 34.07050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 35.0973 - MinusLogProbMetric: 35.0973 - val_loss: 34.0705 - val_MinusLogProbMetric: 34.0705 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 24/1000
2023-10-12 02:59:27.370 
Epoch 24/1000 
	 loss: 35.0781, MinusLogProbMetric: 35.0781, val_loss: 35.0693, val_MinusLogProbMetric: 35.0693

Epoch 24: val_loss did not improve from 34.07050
196/196 - 26s - loss: 35.0781 - MinusLogProbMetric: 35.0781 - val_loss: 35.0693 - val_MinusLogProbMetric: 35.0693 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 25/1000
2023-10-12 02:59:52.948 
Epoch 25/1000 
	 loss: 34.8374, MinusLogProbMetric: 34.8374, val_loss: 34.6490, val_MinusLogProbMetric: 34.6490

Epoch 25: val_loss did not improve from 34.07050
196/196 - 26s - loss: 34.8374 - MinusLogProbMetric: 34.8374 - val_loss: 34.6490 - val_MinusLogProbMetric: 34.6490 - lr: 0.0010 - 26s/epoch - 130ms/step
Epoch 26/1000
2023-10-12 03:00:18.766 
Epoch 26/1000 
	 loss: 34.8476, MinusLogProbMetric: 34.8476, val_loss: 35.7622, val_MinusLogProbMetric: 35.7622

Epoch 26: val_loss did not improve from 34.07050
196/196 - 26s - loss: 34.8476 - MinusLogProbMetric: 34.8476 - val_loss: 35.7622 - val_MinusLogProbMetric: 35.7622 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 27/1000
2023-10-12 03:00:44.312 
Epoch 27/1000 
	 loss: 34.3231, MinusLogProbMetric: 34.3231, val_loss: 35.2094, val_MinusLogProbMetric: 35.2094

Epoch 27: val_loss did not improve from 34.07050
196/196 - 26s - loss: 34.3231 - MinusLogProbMetric: 34.3231 - val_loss: 35.2094 - val_MinusLogProbMetric: 35.2094 - lr: 0.0010 - 26s/epoch - 130ms/step
Epoch 28/1000
2023-10-12 03:01:10.074 
Epoch 28/1000 
	 loss: 34.8387, MinusLogProbMetric: 34.8387, val_loss: 33.2096, val_MinusLogProbMetric: 33.2096

Epoch 28: val_loss improved from 34.07050 to 33.20960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 34.8387 - MinusLogProbMetric: 34.8387 - val_loss: 33.2096 - val_MinusLogProbMetric: 33.2096 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 29/1000
2023-10-12 03:01:36.793 
Epoch 29/1000 
	 loss: 34.1377, MinusLogProbMetric: 34.1377, val_loss: 36.0927, val_MinusLogProbMetric: 36.0927

Epoch 29: val_loss did not improve from 33.20960
196/196 - 26s - loss: 34.1377 - MinusLogProbMetric: 34.1377 - val_loss: 36.0927 - val_MinusLogProbMetric: 36.0927 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 30/1000
2023-10-12 03:02:03.094 
Epoch 30/1000 
	 loss: 34.2896, MinusLogProbMetric: 34.2896, val_loss: 36.0992, val_MinusLogProbMetric: 36.0992

Epoch 30: val_loss did not improve from 33.20960
196/196 - 26s - loss: 34.2896 - MinusLogProbMetric: 34.2896 - val_loss: 36.0992 - val_MinusLogProbMetric: 36.0992 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 31/1000
2023-10-12 03:02:29.402 
Epoch 31/1000 
	 loss: 34.0042, MinusLogProbMetric: 34.0042, val_loss: 36.7556, val_MinusLogProbMetric: 36.7556

Epoch 31: val_loss did not improve from 33.20960
196/196 - 26s - loss: 34.0042 - MinusLogProbMetric: 34.0042 - val_loss: 36.7556 - val_MinusLogProbMetric: 36.7556 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 32/1000
2023-10-12 03:02:55.624 
Epoch 32/1000 
	 loss: 34.3854, MinusLogProbMetric: 34.3854, val_loss: 35.5609, val_MinusLogProbMetric: 35.5609

Epoch 32: val_loss did not improve from 33.20960
196/196 - 26s - loss: 34.3854 - MinusLogProbMetric: 34.3854 - val_loss: 35.5609 - val_MinusLogProbMetric: 35.5609 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 33/1000
2023-10-12 03:03:21.768 
Epoch 33/1000 
	 loss: 34.0702, MinusLogProbMetric: 34.0702, val_loss: 33.6586, val_MinusLogProbMetric: 33.6586

Epoch 33: val_loss did not improve from 33.20960
196/196 - 26s - loss: 34.0702 - MinusLogProbMetric: 34.0702 - val_loss: 33.6586 - val_MinusLogProbMetric: 33.6586 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 34/1000
2023-10-12 03:03:47.937 
Epoch 34/1000 
	 loss: 33.9358, MinusLogProbMetric: 33.9358, val_loss: 35.7635, val_MinusLogProbMetric: 35.7635

Epoch 34: val_loss did not improve from 33.20960
196/196 - 26s - loss: 33.9358 - MinusLogProbMetric: 33.9358 - val_loss: 35.7635 - val_MinusLogProbMetric: 35.7635 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 35/1000
2023-10-12 03:04:14.816 
Epoch 35/1000 
	 loss: 33.8614, MinusLogProbMetric: 33.8614, val_loss: 32.9413, val_MinusLogProbMetric: 32.9413

Epoch 35: val_loss improved from 33.20960 to 32.94134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 33.8614 - MinusLogProbMetric: 33.8614 - val_loss: 32.9413 - val_MinusLogProbMetric: 32.9413 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 36/1000
2023-10-12 03:04:43.710 
Epoch 36/1000 
	 loss: 33.7344, MinusLogProbMetric: 33.7344, val_loss: 35.8250, val_MinusLogProbMetric: 35.8250

Epoch 36: val_loss did not improve from 32.94134
196/196 - 28s - loss: 33.7344 - MinusLogProbMetric: 33.7344 - val_loss: 35.8250 - val_MinusLogProbMetric: 35.8250 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 37/1000
2023-10-12 03:05:10.706 
Epoch 37/1000 
	 loss: 33.4816, MinusLogProbMetric: 33.4816, val_loss: 33.1283, val_MinusLogProbMetric: 33.1283

Epoch 37: val_loss did not improve from 32.94134
196/196 - 27s - loss: 33.4816 - MinusLogProbMetric: 33.4816 - val_loss: 33.1283 - val_MinusLogProbMetric: 33.1283 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 38/1000
2023-10-12 03:05:37.845 
Epoch 38/1000 
	 loss: 33.8223, MinusLogProbMetric: 33.8223, val_loss: 33.6103, val_MinusLogProbMetric: 33.6103

Epoch 38: val_loss did not improve from 32.94134
196/196 - 27s - loss: 33.8223 - MinusLogProbMetric: 33.8223 - val_loss: 33.6103 - val_MinusLogProbMetric: 33.6103 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 39/1000
2023-10-12 03:06:05.580 
Epoch 39/1000 
	 loss: 33.5204, MinusLogProbMetric: 33.5204, val_loss: 33.8518, val_MinusLogProbMetric: 33.8518

Epoch 39: val_loss did not improve from 32.94134
196/196 - 28s - loss: 33.5204 - MinusLogProbMetric: 33.5204 - val_loss: 33.8518 - val_MinusLogProbMetric: 33.8518 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 40/1000
2023-10-12 03:06:32.700 
Epoch 40/1000 
	 loss: 33.0287, MinusLogProbMetric: 33.0287, val_loss: 34.6759, val_MinusLogProbMetric: 34.6759

Epoch 40: val_loss did not improve from 32.94134
196/196 - 27s - loss: 33.0287 - MinusLogProbMetric: 33.0287 - val_loss: 34.6759 - val_MinusLogProbMetric: 34.6759 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 41/1000
2023-10-12 03:06:59.527 
Epoch 41/1000 
	 loss: 33.2685, MinusLogProbMetric: 33.2685, val_loss: 33.3595, val_MinusLogProbMetric: 33.3595

Epoch 41: val_loss did not improve from 32.94134
196/196 - 27s - loss: 33.2685 - MinusLogProbMetric: 33.2685 - val_loss: 33.3595 - val_MinusLogProbMetric: 33.3595 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 42/1000
2023-10-12 03:07:26.665 
Epoch 42/1000 
	 loss: 33.0992, MinusLogProbMetric: 33.0992, val_loss: 33.0028, val_MinusLogProbMetric: 33.0028

Epoch 42: val_loss did not improve from 32.94134
196/196 - 27s - loss: 33.0992 - MinusLogProbMetric: 33.0992 - val_loss: 33.0028 - val_MinusLogProbMetric: 33.0028 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 43/1000
2023-10-12 03:07:54.695 
Epoch 43/1000 
	 loss: 33.1088, MinusLogProbMetric: 33.1088, val_loss: 32.5520, val_MinusLogProbMetric: 32.5520

Epoch 43: val_loss improved from 32.94134 to 32.55204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 33.1088 - MinusLogProbMetric: 33.1088 - val_loss: 32.5520 - val_MinusLogProbMetric: 32.5520 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 44/1000
2023-10-12 03:08:22.221 
Epoch 44/1000 
	 loss: 33.1330, MinusLogProbMetric: 33.1330, val_loss: 31.9138, val_MinusLogProbMetric: 31.9138

Epoch 44: val_loss improved from 32.55204 to 31.91380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 33.1330 - MinusLogProbMetric: 33.1330 - val_loss: 31.9138 - val_MinusLogProbMetric: 31.9138 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 45/1000
2023-10-12 03:08:49.243 
Epoch 45/1000 
	 loss: 33.3004, MinusLogProbMetric: 33.3004, val_loss: 33.1800, val_MinusLogProbMetric: 33.1800

Epoch 45: val_loss did not improve from 31.91380
196/196 - 27s - loss: 33.3004 - MinusLogProbMetric: 33.3004 - val_loss: 33.1800 - val_MinusLogProbMetric: 33.1800 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 46/1000
2023-10-12 03:09:15.291 
Epoch 46/1000 
	 loss: 32.9706, MinusLogProbMetric: 32.9706, val_loss: 33.6516, val_MinusLogProbMetric: 33.6516

Epoch 46: val_loss did not improve from 31.91380
196/196 - 26s - loss: 32.9706 - MinusLogProbMetric: 32.9706 - val_loss: 33.6516 - val_MinusLogProbMetric: 33.6516 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 47/1000
2023-10-12 03:09:42.041 
Epoch 47/1000 
	 loss: 32.6684, MinusLogProbMetric: 32.6684, val_loss: 32.6818, val_MinusLogProbMetric: 32.6818

Epoch 47: val_loss did not improve from 31.91380
196/196 - 27s - loss: 32.6684 - MinusLogProbMetric: 32.6684 - val_loss: 32.6818 - val_MinusLogProbMetric: 32.6818 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 48/1000
2023-10-12 03:10:08.881 
Epoch 48/1000 
	 loss: 32.6626, MinusLogProbMetric: 32.6626, val_loss: 32.9310, val_MinusLogProbMetric: 32.9310

Epoch 48: val_loss did not improve from 31.91380
196/196 - 27s - loss: 32.6626 - MinusLogProbMetric: 32.6626 - val_loss: 32.9310 - val_MinusLogProbMetric: 32.9310 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 49/1000
2023-10-12 03:10:35.487 
Epoch 49/1000 
	 loss: 32.6681, MinusLogProbMetric: 32.6681, val_loss: 32.4101, val_MinusLogProbMetric: 32.4101

Epoch 49: val_loss did not improve from 31.91380
196/196 - 27s - loss: 32.6681 - MinusLogProbMetric: 32.6681 - val_loss: 32.4101 - val_MinusLogProbMetric: 32.4101 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 50/1000
2023-10-12 03:11:02.448 
Epoch 50/1000 
	 loss: 32.6002, MinusLogProbMetric: 32.6002, val_loss: 34.0649, val_MinusLogProbMetric: 34.0649

Epoch 50: val_loss did not improve from 31.91380
196/196 - 27s - loss: 32.6002 - MinusLogProbMetric: 32.6002 - val_loss: 34.0649 - val_MinusLogProbMetric: 34.0649 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 51/1000
2023-10-12 03:11:29.022 
Epoch 51/1000 
	 loss: 32.5922, MinusLogProbMetric: 32.5922, val_loss: 32.7499, val_MinusLogProbMetric: 32.7499

Epoch 51: val_loss did not improve from 31.91380
196/196 - 27s - loss: 32.5922 - MinusLogProbMetric: 32.5922 - val_loss: 32.7499 - val_MinusLogProbMetric: 32.7499 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 52/1000
2023-10-12 03:11:55.288 
Epoch 52/1000 
	 loss: 31.9693, MinusLogProbMetric: 31.9693, val_loss: 34.1426, val_MinusLogProbMetric: 34.1426

Epoch 52: val_loss did not improve from 31.91380
196/196 - 26s - loss: 31.9693 - MinusLogProbMetric: 31.9693 - val_loss: 34.1426 - val_MinusLogProbMetric: 34.1426 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 53/1000
2023-10-12 03:12:21.258 
Epoch 53/1000 
	 loss: 32.0529, MinusLogProbMetric: 32.0529, val_loss: 32.5239, val_MinusLogProbMetric: 32.5239

Epoch 53: val_loss did not improve from 31.91380
196/196 - 26s - loss: 32.0529 - MinusLogProbMetric: 32.0529 - val_loss: 32.5239 - val_MinusLogProbMetric: 32.5239 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 54/1000
2023-10-12 03:12:48.055 
Epoch 54/1000 
	 loss: 31.9487, MinusLogProbMetric: 31.9487, val_loss: 32.9915, val_MinusLogProbMetric: 32.9915

Epoch 54: val_loss did not improve from 31.91380
196/196 - 27s - loss: 31.9487 - MinusLogProbMetric: 31.9487 - val_loss: 32.9915 - val_MinusLogProbMetric: 32.9915 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 55/1000
2023-10-12 03:13:15.147 
Epoch 55/1000 
	 loss: 31.9373, MinusLogProbMetric: 31.9373, val_loss: 31.3885, val_MinusLogProbMetric: 31.3885

Epoch 55: val_loss improved from 31.91380 to 31.38848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 31.9373 - MinusLogProbMetric: 31.9373 - val_loss: 31.3885 - val_MinusLogProbMetric: 31.3885 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 56/1000
2023-10-12 03:13:43.953 
Epoch 56/1000 
	 loss: 32.1145, MinusLogProbMetric: 32.1145, val_loss: 32.0777, val_MinusLogProbMetric: 32.0777

Epoch 56: val_loss did not improve from 31.38848
196/196 - 28s - loss: 32.1145 - MinusLogProbMetric: 32.1145 - val_loss: 32.0777 - val_MinusLogProbMetric: 32.0777 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 57/1000
2023-10-12 03:14:11.145 
Epoch 57/1000 
	 loss: 32.0578, MinusLogProbMetric: 32.0578, val_loss: 31.5673, val_MinusLogProbMetric: 31.5673

Epoch 57: val_loss did not improve from 31.38848
196/196 - 27s - loss: 32.0578 - MinusLogProbMetric: 32.0578 - val_loss: 31.5673 - val_MinusLogProbMetric: 31.5673 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 58/1000
2023-10-12 03:14:37.672 
Epoch 58/1000 
	 loss: 31.9880, MinusLogProbMetric: 31.9880, val_loss: 33.0614, val_MinusLogProbMetric: 33.0614

Epoch 58: val_loss did not improve from 31.38848
196/196 - 27s - loss: 31.9880 - MinusLogProbMetric: 31.9880 - val_loss: 33.0614 - val_MinusLogProbMetric: 33.0614 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 59/1000
2023-10-12 03:15:03.880 
Epoch 59/1000 
	 loss: 31.7656, MinusLogProbMetric: 31.7656, val_loss: 32.2935, val_MinusLogProbMetric: 32.2935

Epoch 59: val_loss did not improve from 31.38848
196/196 - 26s - loss: 31.7656 - MinusLogProbMetric: 31.7656 - val_loss: 32.2935 - val_MinusLogProbMetric: 32.2935 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 60/1000
2023-10-12 03:15:29.687 
Epoch 60/1000 
	 loss: 31.8049, MinusLogProbMetric: 31.8049, val_loss: 32.1133, val_MinusLogProbMetric: 32.1133

Epoch 60: val_loss did not improve from 31.38848
196/196 - 26s - loss: 31.8049 - MinusLogProbMetric: 31.8049 - val_loss: 32.1133 - val_MinusLogProbMetric: 32.1133 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 61/1000
2023-10-12 03:15:55.140 
Epoch 61/1000 
	 loss: 31.7585, MinusLogProbMetric: 31.7585, val_loss: 32.4904, val_MinusLogProbMetric: 32.4904

Epoch 61: val_loss did not improve from 31.38848
196/196 - 25s - loss: 31.7585 - MinusLogProbMetric: 31.7585 - val_loss: 32.4904 - val_MinusLogProbMetric: 32.4904 - lr: 0.0010 - 25s/epoch - 130ms/step
Epoch 62/1000
2023-10-12 03:16:20.800 
Epoch 62/1000 
	 loss: 31.7245, MinusLogProbMetric: 31.7245, val_loss: 32.1337, val_MinusLogProbMetric: 32.1337

Epoch 62: val_loss did not improve from 31.38848
196/196 - 26s - loss: 31.7245 - MinusLogProbMetric: 31.7245 - val_loss: 32.1337 - val_MinusLogProbMetric: 32.1337 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 63/1000
2023-10-12 03:16:46.102 
Epoch 63/1000 
	 loss: 31.6728, MinusLogProbMetric: 31.6728, val_loss: 32.8745, val_MinusLogProbMetric: 32.8745

Epoch 63: val_loss did not improve from 31.38848
196/196 - 25s - loss: 31.6728 - MinusLogProbMetric: 31.6728 - val_loss: 32.8745 - val_MinusLogProbMetric: 32.8745 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 64/1000
2023-10-12 03:17:12.835 
Epoch 64/1000 
	 loss: 31.7319, MinusLogProbMetric: 31.7319, val_loss: 31.2025, val_MinusLogProbMetric: 31.2025

Epoch 64: val_loss improved from 31.38848 to 31.20250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 31.7319 - MinusLogProbMetric: 31.7319 - val_loss: 31.2025 - val_MinusLogProbMetric: 31.2025 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 65/1000
2023-10-12 03:17:39.838 
Epoch 65/1000 
	 loss: 31.6992, MinusLogProbMetric: 31.6992, val_loss: 34.8815, val_MinusLogProbMetric: 34.8815

Epoch 65: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.6992 - MinusLogProbMetric: 31.6992 - val_loss: 34.8815 - val_MinusLogProbMetric: 34.8815 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 66/1000
2023-10-12 03:18:07.960 
Epoch 66/1000 
	 loss: 31.6617, MinusLogProbMetric: 31.6617, val_loss: 31.6047, val_MinusLogProbMetric: 31.6047

Epoch 66: val_loss did not improve from 31.20250
196/196 - 28s - loss: 31.6617 - MinusLogProbMetric: 31.6617 - val_loss: 31.6047 - val_MinusLogProbMetric: 31.6047 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 67/1000
2023-10-12 03:18:34.849 
Epoch 67/1000 
	 loss: 31.6844, MinusLogProbMetric: 31.6844, val_loss: 31.2139, val_MinusLogProbMetric: 31.2139

Epoch 67: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.6844 - MinusLogProbMetric: 31.6844 - val_loss: 31.2139 - val_MinusLogProbMetric: 31.2139 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 68/1000
2023-10-12 03:19:00.986 
Epoch 68/1000 
	 loss: 31.6779, MinusLogProbMetric: 31.6779, val_loss: 31.7121, val_MinusLogProbMetric: 31.7121

Epoch 68: val_loss did not improve from 31.20250
196/196 - 26s - loss: 31.6779 - MinusLogProbMetric: 31.6779 - val_loss: 31.7121 - val_MinusLogProbMetric: 31.7121 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 69/1000
2023-10-12 03:19:27.238 
Epoch 69/1000 
	 loss: 31.5715, MinusLogProbMetric: 31.5715, val_loss: 35.2350, val_MinusLogProbMetric: 35.2350

Epoch 69: val_loss did not improve from 31.20250
196/196 - 26s - loss: 31.5715 - MinusLogProbMetric: 31.5715 - val_loss: 35.2350 - val_MinusLogProbMetric: 35.2350 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 70/1000
2023-10-12 03:19:54.021 
Epoch 70/1000 
	 loss: 31.6573, MinusLogProbMetric: 31.6573, val_loss: 31.4800, val_MinusLogProbMetric: 31.4800

Epoch 70: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.6573 - MinusLogProbMetric: 31.6573 - val_loss: 31.4800 - val_MinusLogProbMetric: 31.4800 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 71/1000
2023-10-12 03:20:20.594 
Epoch 71/1000 
	 loss: 31.4661, MinusLogProbMetric: 31.4661, val_loss: 31.4283, val_MinusLogProbMetric: 31.4283

Epoch 71: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.4661 - MinusLogProbMetric: 31.4661 - val_loss: 31.4283 - val_MinusLogProbMetric: 31.4283 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 72/1000
2023-10-12 03:20:46.825 
Epoch 72/1000 
	 loss: 31.6120, MinusLogProbMetric: 31.6120, val_loss: 32.6725, val_MinusLogProbMetric: 32.6725

Epoch 72: val_loss did not improve from 31.20250
196/196 - 26s - loss: 31.6120 - MinusLogProbMetric: 31.6120 - val_loss: 32.6725 - val_MinusLogProbMetric: 32.6725 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 73/1000
2023-10-12 03:21:14.100 
Epoch 73/1000 
	 loss: 31.4404, MinusLogProbMetric: 31.4404, val_loss: 31.7877, val_MinusLogProbMetric: 31.7877

Epoch 73: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.4404 - MinusLogProbMetric: 31.4404 - val_loss: 31.7877 - val_MinusLogProbMetric: 31.7877 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 74/1000
2023-10-12 03:21:42.050 
Epoch 74/1000 
	 loss: 31.3941, MinusLogProbMetric: 31.3941, val_loss: 31.7659, val_MinusLogProbMetric: 31.7659

Epoch 74: val_loss did not improve from 31.20250
196/196 - 28s - loss: 31.3941 - MinusLogProbMetric: 31.3941 - val_loss: 31.7659 - val_MinusLogProbMetric: 31.7659 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 75/1000
2023-10-12 03:22:08.669 
Epoch 75/1000 
	 loss: 31.3637, MinusLogProbMetric: 31.3637, val_loss: 31.8225, val_MinusLogProbMetric: 31.8225

Epoch 75: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.3637 - MinusLogProbMetric: 31.3637 - val_loss: 31.8225 - val_MinusLogProbMetric: 31.8225 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 76/1000
2023-10-12 03:22:35.217 
Epoch 76/1000 
	 loss: 31.1125, MinusLogProbMetric: 31.1125, val_loss: 33.2068, val_MinusLogProbMetric: 33.2068

Epoch 76: val_loss did not improve from 31.20250
196/196 - 27s - loss: 31.1125 - MinusLogProbMetric: 31.1125 - val_loss: 33.2068 - val_MinusLogProbMetric: 33.2068 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 77/1000
2023-10-12 03:23:02.313 
Epoch 77/1000 
	 loss: 31.1311, MinusLogProbMetric: 31.1311, val_loss: 30.6292, val_MinusLogProbMetric: 30.6292

Epoch 77: val_loss improved from 31.20250 to 30.62917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 31.1311 - MinusLogProbMetric: 31.1311 - val_loss: 30.6292 - val_MinusLogProbMetric: 30.6292 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 78/1000
2023-10-12 03:23:29.260 
Epoch 78/1000 
	 loss: 31.3443, MinusLogProbMetric: 31.3443, val_loss: 32.1316, val_MinusLogProbMetric: 32.1316

Epoch 78: val_loss did not improve from 30.62917
196/196 - 26s - loss: 31.3443 - MinusLogProbMetric: 31.3443 - val_loss: 32.1316 - val_MinusLogProbMetric: 32.1316 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 79/1000
2023-10-12 03:23:56.304 
Epoch 79/1000 
	 loss: 31.2810, MinusLogProbMetric: 31.2810, val_loss: 31.6038, val_MinusLogProbMetric: 31.6038

Epoch 79: val_loss did not improve from 30.62917
196/196 - 27s - loss: 31.2810 - MinusLogProbMetric: 31.2810 - val_loss: 31.6038 - val_MinusLogProbMetric: 31.6038 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 80/1000
2023-10-12 03:24:23.699 
Epoch 80/1000 
	 loss: 31.1480, MinusLogProbMetric: 31.1480, val_loss: 32.3538, val_MinusLogProbMetric: 32.3538

Epoch 80: val_loss did not improve from 30.62917
196/196 - 27s - loss: 31.1480 - MinusLogProbMetric: 31.1480 - val_loss: 32.3538 - val_MinusLogProbMetric: 32.3538 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 81/1000
2023-10-12 03:24:50.062 
Epoch 81/1000 
	 loss: 31.4351, MinusLogProbMetric: 31.4351, val_loss: 30.9489, val_MinusLogProbMetric: 30.9489

Epoch 81: val_loss did not improve from 30.62917
196/196 - 26s - loss: 31.4351 - MinusLogProbMetric: 31.4351 - val_loss: 30.9489 - val_MinusLogProbMetric: 30.9489 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 82/1000
2023-10-12 03:25:16.998 
Epoch 82/1000 
	 loss: 31.0937, MinusLogProbMetric: 31.0937, val_loss: 31.1662, val_MinusLogProbMetric: 31.1662

Epoch 82: val_loss did not improve from 30.62917
196/196 - 27s - loss: 31.0937 - MinusLogProbMetric: 31.0937 - val_loss: 31.1662 - val_MinusLogProbMetric: 31.1662 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 83/1000
2023-10-12 03:25:44.258 
Epoch 83/1000 
	 loss: 31.0228, MinusLogProbMetric: 31.0228, val_loss: 31.1944, val_MinusLogProbMetric: 31.1944

Epoch 83: val_loss did not improve from 30.62917
196/196 - 27s - loss: 31.0228 - MinusLogProbMetric: 31.0228 - val_loss: 31.1944 - val_MinusLogProbMetric: 31.1944 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 84/1000
2023-10-12 03:26:11.756 
Epoch 84/1000 
	 loss: 30.8769, MinusLogProbMetric: 30.8769, val_loss: 30.5762, val_MinusLogProbMetric: 30.5762

Epoch 84: val_loss improved from 30.62917 to 30.57625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 30.8769 - MinusLogProbMetric: 30.8769 - val_loss: 30.5762 - val_MinusLogProbMetric: 30.5762 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 85/1000
2023-10-12 03:26:38.655 
Epoch 85/1000 
	 loss: 31.2697, MinusLogProbMetric: 31.2697, val_loss: 30.8986, val_MinusLogProbMetric: 30.8986

Epoch 85: val_loss did not improve from 30.57625
196/196 - 26s - loss: 31.2697 - MinusLogProbMetric: 31.2697 - val_loss: 30.8986 - val_MinusLogProbMetric: 30.8986 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 86/1000
2023-10-12 03:27:06.237 
Epoch 86/1000 
	 loss: 31.0941, MinusLogProbMetric: 31.0941, val_loss: 31.6850, val_MinusLogProbMetric: 31.6850

Epoch 86: val_loss did not improve from 30.57625
196/196 - 28s - loss: 31.0941 - MinusLogProbMetric: 31.0941 - val_loss: 31.6850 - val_MinusLogProbMetric: 31.6850 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 87/1000
2023-10-12 03:27:32.988 
Epoch 87/1000 
	 loss: 31.1674, MinusLogProbMetric: 31.1674, val_loss: 31.7941, val_MinusLogProbMetric: 31.7941

Epoch 87: val_loss did not improve from 30.57625
196/196 - 27s - loss: 31.1674 - MinusLogProbMetric: 31.1674 - val_loss: 31.7941 - val_MinusLogProbMetric: 31.7941 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 88/1000
2023-10-12 03:27:59.743 
Epoch 88/1000 
	 loss: 30.8406, MinusLogProbMetric: 30.8406, val_loss: 32.6862, val_MinusLogProbMetric: 32.6862

Epoch 88: val_loss did not improve from 30.57625
196/196 - 27s - loss: 30.8406 - MinusLogProbMetric: 30.8406 - val_loss: 32.6862 - val_MinusLogProbMetric: 32.6862 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 89/1000
2023-10-12 03:28:25.774 
Epoch 89/1000 
	 loss: 30.9566, MinusLogProbMetric: 30.9566, val_loss: 31.4909, val_MinusLogProbMetric: 31.4909

Epoch 89: val_loss did not improve from 30.57625
196/196 - 26s - loss: 30.9566 - MinusLogProbMetric: 30.9566 - val_loss: 31.4909 - val_MinusLogProbMetric: 31.4909 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 90/1000
2023-10-12 03:28:51.435 
Epoch 90/1000 
	 loss: 31.0332, MinusLogProbMetric: 31.0332, val_loss: 31.4151, val_MinusLogProbMetric: 31.4151

Epoch 90: val_loss did not improve from 30.57625
196/196 - 26s - loss: 31.0332 - MinusLogProbMetric: 31.0332 - val_loss: 31.4151 - val_MinusLogProbMetric: 31.4151 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 91/1000
2023-10-12 03:29:17.293 
Epoch 91/1000 
	 loss: 31.0677, MinusLogProbMetric: 31.0677, val_loss: 31.3105, val_MinusLogProbMetric: 31.3105

Epoch 91: val_loss did not improve from 30.57625
196/196 - 26s - loss: 31.0677 - MinusLogProbMetric: 31.0677 - val_loss: 31.3105 - val_MinusLogProbMetric: 31.3105 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 92/1000
2023-10-12 03:29:43.238 
Epoch 92/1000 
	 loss: 30.9627, MinusLogProbMetric: 30.9627, val_loss: 31.7762, val_MinusLogProbMetric: 31.7762

Epoch 92: val_loss did not improve from 30.57625
196/196 - 26s - loss: 30.9627 - MinusLogProbMetric: 30.9627 - val_loss: 31.7762 - val_MinusLogProbMetric: 31.7762 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 93/1000
2023-10-12 03:30:08.951 
Epoch 93/1000 
	 loss: 30.7064, MinusLogProbMetric: 30.7064, val_loss: 32.1260, val_MinusLogProbMetric: 32.1260

Epoch 93: val_loss did not improve from 30.57625
196/196 - 26s - loss: 30.7064 - MinusLogProbMetric: 30.7064 - val_loss: 32.1260 - val_MinusLogProbMetric: 32.1260 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 94/1000
2023-10-12 03:30:35.102 
Epoch 94/1000 
	 loss: 30.8979, MinusLogProbMetric: 30.8979, val_loss: 32.1380, val_MinusLogProbMetric: 32.1380

Epoch 94: val_loss did not improve from 30.57625
196/196 - 26s - loss: 30.8979 - MinusLogProbMetric: 30.8979 - val_loss: 32.1380 - val_MinusLogProbMetric: 32.1380 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 95/1000
2023-10-12 03:31:02.365 
Epoch 95/1000 
	 loss: 31.1841, MinusLogProbMetric: 31.1841, val_loss: 30.3503, val_MinusLogProbMetric: 30.3503

Epoch 95: val_loss improved from 30.57625 to 30.35028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 31.1841 - MinusLogProbMetric: 31.1841 - val_loss: 30.3503 - val_MinusLogProbMetric: 30.3503 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 96/1000
2023-10-12 03:31:29.424 
Epoch 96/1000 
	 loss: 30.7346, MinusLogProbMetric: 30.7346, val_loss: 30.9173, val_MinusLogProbMetric: 30.9173

Epoch 96: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.7346 - MinusLogProbMetric: 30.7346 - val_loss: 30.9173 - val_MinusLogProbMetric: 30.9173 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 97/1000
2023-10-12 03:31:56.925 
Epoch 97/1000 
	 loss: 30.8826, MinusLogProbMetric: 30.8826, val_loss: 30.9469, val_MinusLogProbMetric: 30.9469

Epoch 97: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.8826 - MinusLogProbMetric: 30.8826 - val_loss: 30.9469 - val_MinusLogProbMetric: 30.9469 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 98/1000
2023-10-12 03:32:25.027 
Epoch 98/1000 
	 loss: 30.7732, MinusLogProbMetric: 30.7732, val_loss: 33.2108, val_MinusLogProbMetric: 33.2108

Epoch 98: val_loss did not improve from 30.35028
196/196 - 28s - loss: 30.7732 - MinusLogProbMetric: 30.7732 - val_loss: 33.2108 - val_MinusLogProbMetric: 33.2108 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 99/1000
2023-10-12 03:32:52.305 
Epoch 99/1000 
	 loss: 30.7873, MinusLogProbMetric: 30.7873, val_loss: 31.8109, val_MinusLogProbMetric: 31.8109

Epoch 99: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.7873 - MinusLogProbMetric: 30.7873 - val_loss: 31.8109 - val_MinusLogProbMetric: 31.8109 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 100/1000
2023-10-12 03:33:19.929 
Epoch 100/1000 
	 loss: 30.6937, MinusLogProbMetric: 30.6937, val_loss: 30.8205, val_MinusLogProbMetric: 30.8205

Epoch 100: val_loss did not improve from 30.35028
196/196 - 28s - loss: 30.6937 - MinusLogProbMetric: 30.6937 - val_loss: 30.8205 - val_MinusLogProbMetric: 30.8205 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 101/1000
2023-10-12 03:33:47.137 
Epoch 101/1000 
	 loss: 30.6637, MinusLogProbMetric: 30.6637, val_loss: 31.5250, val_MinusLogProbMetric: 31.5250

Epoch 101: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.6637 - MinusLogProbMetric: 30.6637 - val_loss: 31.5250 - val_MinusLogProbMetric: 31.5250 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 102/1000
2023-10-12 03:34:14.506 
Epoch 102/1000 
	 loss: 30.6207, MinusLogProbMetric: 30.6207, val_loss: 31.7906, val_MinusLogProbMetric: 31.7906

Epoch 102: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.6207 - MinusLogProbMetric: 30.6207 - val_loss: 31.7906 - val_MinusLogProbMetric: 31.7906 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 103/1000
2023-10-12 03:34:41.628 
Epoch 103/1000 
	 loss: 30.5143, MinusLogProbMetric: 30.5143, val_loss: 31.5402, val_MinusLogProbMetric: 31.5402

Epoch 103: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.5143 - MinusLogProbMetric: 30.5143 - val_loss: 31.5402 - val_MinusLogProbMetric: 31.5402 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 104/1000
2023-10-12 03:35:08.842 
Epoch 104/1000 
	 loss: 30.7096, MinusLogProbMetric: 30.7096, val_loss: 32.2561, val_MinusLogProbMetric: 32.2561

Epoch 104: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.7096 - MinusLogProbMetric: 30.7096 - val_loss: 32.2561 - val_MinusLogProbMetric: 32.2561 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 105/1000
2023-10-12 03:35:35.925 
Epoch 105/1000 
	 loss: 30.5948, MinusLogProbMetric: 30.5948, val_loss: 30.8113, val_MinusLogProbMetric: 30.8113

Epoch 105: val_loss did not improve from 30.35028
196/196 - 27s - loss: 30.5948 - MinusLogProbMetric: 30.5948 - val_loss: 30.8113 - val_MinusLogProbMetric: 30.8113 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 106/1000
2023-10-12 03:36:03.256 
Epoch 106/1000 
	 loss: 30.5366, MinusLogProbMetric: 30.5366, val_loss: 30.2012, val_MinusLogProbMetric: 30.2012

Epoch 106: val_loss improved from 30.35028 to 30.20119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 30.5366 - MinusLogProbMetric: 30.5366 - val_loss: 30.2012 - val_MinusLogProbMetric: 30.2012 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 107/1000
2023-10-12 03:36:30.544 
Epoch 107/1000 
	 loss: 30.6046, MinusLogProbMetric: 30.6046, val_loss: 30.2735, val_MinusLogProbMetric: 30.2735

Epoch 107: val_loss did not improve from 30.20119
196/196 - 27s - loss: 30.6046 - MinusLogProbMetric: 30.6046 - val_loss: 30.2735 - val_MinusLogProbMetric: 30.2735 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 108/1000
2023-10-12 03:36:57.238 
Epoch 108/1000 
	 loss: 30.5928, MinusLogProbMetric: 30.5928, val_loss: 30.1176, val_MinusLogProbMetric: 30.1176

Epoch 108: val_loss improved from 30.20119 to 30.11764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 30.5928 - MinusLogProbMetric: 30.5928 - val_loss: 30.1176 - val_MinusLogProbMetric: 30.1176 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 109/1000
2023-10-12 03:37:25.624 
Epoch 109/1000 
	 loss: 30.4844, MinusLogProbMetric: 30.4844, val_loss: 31.7661, val_MinusLogProbMetric: 31.7661

Epoch 109: val_loss did not improve from 30.11764
196/196 - 28s - loss: 30.4844 - MinusLogProbMetric: 30.4844 - val_loss: 31.7661 - val_MinusLogProbMetric: 31.7661 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 110/1000
2023-10-12 03:37:52.464 
Epoch 110/1000 
	 loss: 30.6811, MinusLogProbMetric: 30.6811, val_loss: 30.3337, val_MinusLogProbMetric: 30.3337

Epoch 110: val_loss did not improve from 30.11764
196/196 - 27s - loss: 30.6811 - MinusLogProbMetric: 30.6811 - val_loss: 30.3337 - val_MinusLogProbMetric: 30.3337 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 111/1000
2023-10-12 03:38:19.314 
Epoch 111/1000 
	 loss: 30.6063, MinusLogProbMetric: 30.6063, val_loss: 33.0820, val_MinusLogProbMetric: 33.0820

Epoch 111: val_loss did not improve from 30.11764
196/196 - 27s - loss: 30.6063 - MinusLogProbMetric: 30.6063 - val_loss: 33.0820 - val_MinusLogProbMetric: 33.0820 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 112/1000
2023-10-12 03:38:46.104 
Epoch 112/1000 
	 loss: 30.5214, MinusLogProbMetric: 30.5214, val_loss: 31.4671, val_MinusLogProbMetric: 31.4671

Epoch 112: val_loss did not improve from 30.11764
196/196 - 27s - loss: 30.5214 - MinusLogProbMetric: 30.5214 - val_loss: 31.4671 - val_MinusLogProbMetric: 31.4671 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 113/1000
2023-10-12 03:39:12.954 
Epoch 113/1000 
	 loss: 30.6169, MinusLogProbMetric: 30.6169, val_loss: 32.7118, val_MinusLogProbMetric: 32.7118

Epoch 113: val_loss did not improve from 30.11764
196/196 - 27s - loss: 30.6169 - MinusLogProbMetric: 30.6169 - val_loss: 32.7118 - val_MinusLogProbMetric: 32.7118 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 114/1000
2023-10-12 03:39:39.831 
Epoch 114/1000 
	 loss: 30.4485, MinusLogProbMetric: 30.4485, val_loss: 30.4217, val_MinusLogProbMetric: 30.4217

Epoch 114: val_loss did not improve from 30.11764
196/196 - 27s - loss: 30.4485 - MinusLogProbMetric: 30.4485 - val_loss: 30.4217 - val_MinusLogProbMetric: 30.4217 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 115/1000
2023-10-12 03:40:07.041 
Epoch 115/1000 
	 loss: 30.3878, MinusLogProbMetric: 30.3878, val_loss: 30.8076, val_MinusLogProbMetric: 30.8076

Epoch 115: val_loss did not improve from 30.11764
196/196 - 27s - loss: 30.3878 - MinusLogProbMetric: 30.3878 - val_loss: 30.8076 - val_MinusLogProbMetric: 30.8076 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 116/1000
2023-10-12 03:40:34.162 
Epoch 116/1000 
	 loss: 30.3423, MinusLogProbMetric: 30.3423, val_loss: 30.0802, val_MinusLogProbMetric: 30.0802

Epoch 116: val_loss improved from 30.11764 to 30.08015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 30.3423 - MinusLogProbMetric: 30.3423 - val_loss: 30.0802 - val_MinusLogProbMetric: 30.0802 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 117/1000
2023-10-12 03:41:01.355 
Epoch 117/1000 
	 loss: 30.4508, MinusLogProbMetric: 30.4508, val_loss: 30.8578, val_MinusLogProbMetric: 30.8578

Epoch 117: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.4508 - MinusLogProbMetric: 30.4508 - val_loss: 30.8578 - val_MinusLogProbMetric: 30.8578 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 118/1000
2023-10-12 03:41:27.729 
Epoch 118/1000 
	 loss: 30.5318, MinusLogProbMetric: 30.5318, val_loss: 31.9461, val_MinusLogProbMetric: 31.9461

Epoch 118: val_loss did not improve from 30.08015
196/196 - 26s - loss: 30.5318 - MinusLogProbMetric: 30.5318 - val_loss: 31.9461 - val_MinusLogProbMetric: 31.9461 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 119/1000
2023-10-12 03:41:55.004 
Epoch 119/1000 
	 loss: 30.4503, MinusLogProbMetric: 30.4503, val_loss: 30.3963, val_MinusLogProbMetric: 30.3963

Epoch 119: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.4503 - MinusLogProbMetric: 30.4503 - val_loss: 30.3963 - val_MinusLogProbMetric: 30.3963 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 120/1000
2023-10-12 03:42:22.964 
Epoch 120/1000 
	 loss: 30.3490, MinusLogProbMetric: 30.3490, val_loss: 30.2925, val_MinusLogProbMetric: 30.2925

Epoch 120: val_loss did not improve from 30.08015
196/196 - 28s - loss: 30.3490 - MinusLogProbMetric: 30.3490 - val_loss: 30.2925 - val_MinusLogProbMetric: 30.2925 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 121/1000
2023-10-12 03:42:49.545 
Epoch 121/1000 
	 loss: 30.3566, MinusLogProbMetric: 30.3566, val_loss: 30.4162, val_MinusLogProbMetric: 30.4162

Epoch 121: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.3566 - MinusLogProbMetric: 30.3566 - val_loss: 30.4162 - val_MinusLogProbMetric: 30.4162 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 122/1000
2023-10-12 03:43:16.496 
Epoch 122/1000 
	 loss: 30.2909, MinusLogProbMetric: 30.2909, val_loss: 30.5552, val_MinusLogProbMetric: 30.5552

Epoch 122: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.2909 - MinusLogProbMetric: 30.2909 - val_loss: 30.5552 - val_MinusLogProbMetric: 30.5552 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 123/1000
2023-10-12 03:43:43.305 
Epoch 123/1000 
	 loss: 30.3610, MinusLogProbMetric: 30.3610, val_loss: 30.6651, val_MinusLogProbMetric: 30.6651

Epoch 123: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.3610 - MinusLogProbMetric: 30.3610 - val_loss: 30.6651 - val_MinusLogProbMetric: 30.6651 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 124/1000
2023-10-12 03:44:09.979 
Epoch 124/1000 
	 loss: 30.4758, MinusLogProbMetric: 30.4758, val_loss: 31.9832, val_MinusLogProbMetric: 31.9832

Epoch 124: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.4758 - MinusLogProbMetric: 30.4758 - val_loss: 31.9832 - val_MinusLogProbMetric: 31.9832 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 125/1000
2023-10-12 03:44:38.201 
Epoch 125/1000 
	 loss: 30.1477, MinusLogProbMetric: 30.1477, val_loss: 30.4523, val_MinusLogProbMetric: 30.4523

Epoch 125: val_loss did not improve from 30.08015
196/196 - 28s - loss: 30.1477 - MinusLogProbMetric: 30.1477 - val_loss: 30.4523 - val_MinusLogProbMetric: 30.4523 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 126/1000
2023-10-12 03:45:06.413 
Epoch 126/1000 
	 loss: 30.2698, MinusLogProbMetric: 30.2698, val_loss: 30.1835, val_MinusLogProbMetric: 30.1835

Epoch 126: val_loss did not improve from 30.08015
196/196 - 28s - loss: 30.2698 - MinusLogProbMetric: 30.2698 - val_loss: 30.1835 - val_MinusLogProbMetric: 30.1835 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 127/1000
2023-10-12 03:45:34.519 
Epoch 127/1000 
	 loss: 30.2579, MinusLogProbMetric: 30.2579, val_loss: 30.8487, val_MinusLogProbMetric: 30.8487

Epoch 127: val_loss did not improve from 30.08015
196/196 - 28s - loss: 30.2579 - MinusLogProbMetric: 30.2579 - val_loss: 30.8487 - val_MinusLogProbMetric: 30.8487 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 128/1000
2023-10-12 03:46:01.279 
Epoch 128/1000 
	 loss: 30.3116, MinusLogProbMetric: 30.3116, val_loss: 30.6971, val_MinusLogProbMetric: 30.6971

Epoch 128: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.3116 - MinusLogProbMetric: 30.3116 - val_loss: 30.6971 - val_MinusLogProbMetric: 30.6971 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 129/1000
2023-10-12 03:46:29.791 
Epoch 129/1000 
	 loss: 30.2276, MinusLogProbMetric: 30.2276, val_loss: 31.0060, val_MinusLogProbMetric: 31.0060

Epoch 129: val_loss did not improve from 30.08015
196/196 - 29s - loss: 30.2276 - MinusLogProbMetric: 30.2276 - val_loss: 31.0060 - val_MinusLogProbMetric: 31.0060 - lr: 0.0010 - 29s/epoch - 145ms/step
Epoch 130/1000
2023-10-12 03:46:57.148 
Epoch 130/1000 
	 loss: 30.0915, MinusLogProbMetric: 30.0915, val_loss: 30.1039, val_MinusLogProbMetric: 30.1039

Epoch 130: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.0915 - MinusLogProbMetric: 30.0915 - val_loss: 30.1039 - val_MinusLogProbMetric: 30.1039 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 131/1000
2023-10-12 03:47:24.365 
Epoch 131/1000 
	 loss: 30.3491, MinusLogProbMetric: 30.3491, val_loss: 30.3391, val_MinusLogProbMetric: 30.3391

Epoch 131: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.3491 - MinusLogProbMetric: 30.3491 - val_loss: 30.3391 - val_MinusLogProbMetric: 30.3391 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 132/1000
2023-10-12 03:47:50.814 
Epoch 132/1000 
	 loss: 30.1144, MinusLogProbMetric: 30.1144, val_loss: 30.9591, val_MinusLogProbMetric: 30.9591

Epoch 132: val_loss did not improve from 30.08015
196/196 - 26s - loss: 30.1144 - MinusLogProbMetric: 30.1144 - val_loss: 30.9591 - val_MinusLogProbMetric: 30.9591 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 133/1000
2023-10-12 03:48:18.845 
Epoch 133/1000 
	 loss: 30.1636, MinusLogProbMetric: 30.1636, val_loss: 30.6893, val_MinusLogProbMetric: 30.6893

Epoch 133: val_loss did not improve from 30.08015
196/196 - 28s - loss: 30.1636 - MinusLogProbMetric: 30.1636 - val_loss: 30.6893 - val_MinusLogProbMetric: 30.6893 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 134/1000
2023-10-12 03:48:45.057 
Epoch 134/1000 
	 loss: 30.3236, MinusLogProbMetric: 30.3236, val_loss: 32.1179, val_MinusLogProbMetric: 32.1179

Epoch 134: val_loss did not improve from 30.08015
196/196 - 26s - loss: 30.3236 - MinusLogProbMetric: 30.3236 - val_loss: 32.1179 - val_MinusLogProbMetric: 32.1179 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 135/1000
2023-10-12 03:49:12.453 
Epoch 135/1000 
	 loss: 30.0328, MinusLogProbMetric: 30.0328, val_loss: 30.8684, val_MinusLogProbMetric: 30.8684

Epoch 135: val_loss did not improve from 30.08015
196/196 - 27s - loss: 30.0328 - MinusLogProbMetric: 30.0328 - val_loss: 30.8684 - val_MinusLogProbMetric: 30.8684 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 136/1000
2023-10-12 03:49:39.838 
Epoch 136/1000 
	 loss: 29.9013, MinusLogProbMetric: 29.9013, val_loss: 30.8910, val_MinusLogProbMetric: 30.8910

Epoch 136: val_loss did not improve from 30.08015
196/196 - 27s - loss: 29.9013 - MinusLogProbMetric: 29.9013 - val_loss: 30.8910 - val_MinusLogProbMetric: 30.8910 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 137/1000
2023-10-12 03:50:07.263 
Epoch 137/1000 
	 loss: 30.2300, MinusLogProbMetric: 30.2300, val_loss: 29.8821, val_MinusLogProbMetric: 29.8821

Epoch 137: val_loss improved from 30.08015 to 29.88211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 30.2300 - MinusLogProbMetric: 30.2300 - val_loss: 29.8821 - val_MinusLogProbMetric: 29.8821 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 138/1000
2023-10-12 03:50:34.822 
Epoch 138/1000 
	 loss: 30.1168, MinusLogProbMetric: 30.1168, val_loss: 30.4523, val_MinusLogProbMetric: 30.4523

Epoch 138: val_loss did not improve from 29.88211
196/196 - 27s - loss: 30.1168 - MinusLogProbMetric: 30.1168 - val_loss: 30.4523 - val_MinusLogProbMetric: 30.4523 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 139/1000
2023-10-12 03:51:02.480 
Epoch 139/1000 
	 loss: 30.0423, MinusLogProbMetric: 30.0423, val_loss: 31.6692, val_MinusLogProbMetric: 31.6692

Epoch 139: val_loss did not improve from 29.88211
196/196 - 28s - loss: 30.0423 - MinusLogProbMetric: 30.0423 - val_loss: 31.6692 - val_MinusLogProbMetric: 31.6692 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 140/1000
2023-10-12 03:51:29.244 
Epoch 140/1000 
	 loss: 29.7684, MinusLogProbMetric: 29.7684, val_loss: 30.5657, val_MinusLogProbMetric: 30.5657

Epoch 140: val_loss did not improve from 29.88211
196/196 - 27s - loss: 29.7684 - MinusLogProbMetric: 29.7684 - val_loss: 30.5657 - val_MinusLogProbMetric: 30.5657 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 141/1000
2023-10-12 03:51:56.094 
Epoch 141/1000 
	 loss: 30.0349, MinusLogProbMetric: 30.0349, val_loss: 30.9999, val_MinusLogProbMetric: 30.9999

Epoch 141: val_loss did not improve from 29.88211
196/196 - 27s - loss: 30.0349 - MinusLogProbMetric: 30.0349 - val_loss: 30.9999 - val_MinusLogProbMetric: 30.9999 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 142/1000
2023-10-12 03:52:23.900 
Epoch 142/1000 
	 loss: 29.9541, MinusLogProbMetric: 29.9541, val_loss: 30.3924, val_MinusLogProbMetric: 30.3924

Epoch 142: val_loss did not improve from 29.88211
196/196 - 28s - loss: 29.9541 - MinusLogProbMetric: 29.9541 - val_loss: 30.3924 - val_MinusLogProbMetric: 30.3924 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 143/1000
2023-10-12 03:52:50.917 
Epoch 143/1000 
	 loss: 29.8261, MinusLogProbMetric: 29.8261, val_loss: 30.8071, val_MinusLogProbMetric: 30.8071

Epoch 143: val_loss did not improve from 29.88211
196/196 - 27s - loss: 29.8261 - MinusLogProbMetric: 29.8261 - val_loss: 30.8071 - val_MinusLogProbMetric: 30.8071 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 144/1000
2023-10-12 03:53:17.588 
Epoch 144/1000 
	 loss: 29.9280, MinusLogProbMetric: 29.9280, val_loss: 30.8775, val_MinusLogProbMetric: 30.8775

Epoch 144: val_loss did not improve from 29.88211
196/196 - 27s - loss: 29.9280 - MinusLogProbMetric: 29.9280 - val_loss: 30.8775 - val_MinusLogProbMetric: 30.8775 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 145/1000
2023-10-12 03:53:45.577 
Epoch 145/1000 
	 loss: 29.6683, MinusLogProbMetric: 29.6683, val_loss: 29.5706, val_MinusLogProbMetric: 29.5706

Epoch 145: val_loss improved from 29.88211 to 29.57059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 29.6683 - MinusLogProbMetric: 29.6683 - val_loss: 29.5706 - val_MinusLogProbMetric: 29.5706 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 146/1000
2023-10-12 03:54:12.950 
Epoch 146/1000 
	 loss: 29.8253, MinusLogProbMetric: 29.8253, val_loss: 30.4138, val_MinusLogProbMetric: 30.4138

Epoch 146: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.8253 - MinusLogProbMetric: 29.8253 - val_loss: 30.4138 - val_MinusLogProbMetric: 30.4138 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 147/1000
2023-10-12 03:54:39.888 
Epoch 147/1000 
	 loss: 29.7437, MinusLogProbMetric: 29.7437, val_loss: 30.0832, val_MinusLogProbMetric: 30.0832

Epoch 147: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.7437 - MinusLogProbMetric: 29.7437 - val_loss: 30.0832 - val_MinusLogProbMetric: 30.0832 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 148/1000
2023-10-12 03:55:06.689 
Epoch 148/1000 
	 loss: 29.7655, MinusLogProbMetric: 29.7655, val_loss: 30.8128, val_MinusLogProbMetric: 30.8128

Epoch 148: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.7655 - MinusLogProbMetric: 29.7655 - val_loss: 30.8128 - val_MinusLogProbMetric: 30.8128 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 149/1000
2023-10-12 03:55:33.698 
Epoch 149/1000 
	 loss: 29.9453, MinusLogProbMetric: 29.9453, val_loss: 30.7852, val_MinusLogProbMetric: 30.7852

Epoch 149: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.9453 - MinusLogProbMetric: 29.9453 - val_loss: 30.7852 - val_MinusLogProbMetric: 30.7852 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 150/1000
2023-10-12 03:56:00.315 
Epoch 150/1000 
	 loss: 29.7682, MinusLogProbMetric: 29.7682, val_loss: 30.0869, val_MinusLogProbMetric: 30.0869

Epoch 150: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.7682 - MinusLogProbMetric: 29.7682 - val_loss: 30.0869 - val_MinusLogProbMetric: 30.0869 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 151/1000
2023-10-12 03:56:26.880 
Epoch 151/1000 
	 loss: 29.6449, MinusLogProbMetric: 29.6449, val_loss: 29.6094, val_MinusLogProbMetric: 29.6094

Epoch 151: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.6449 - MinusLogProbMetric: 29.6449 - val_loss: 29.6094 - val_MinusLogProbMetric: 29.6094 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 152/1000
2023-10-12 03:56:53.102 
Epoch 152/1000 
	 loss: 29.8713, MinusLogProbMetric: 29.8713, val_loss: 29.8810, val_MinusLogProbMetric: 29.8810

Epoch 152: val_loss did not improve from 29.57059
196/196 - 26s - loss: 29.8713 - MinusLogProbMetric: 29.8713 - val_loss: 29.8810 - val_MinusLogProbMetric: 29.8810 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 153/1000
2023-10-12 03:57:19.220 
Epoch 153/1000 
	 loss: 29.8628, MinusLogProbMetric: 29.8628, val_loss: 31.2603, val_MinusLogProbMetric: 31.2603

Epoch 153: val_loss did not improve from 29.57059
196/196 - 26s - loss: 29.8628 - MinusLogProbMetric: 29.8628 - val_loss: 31.2603 - val_MinusLogProbMetric: 31.2603 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 154/1000
2023-10-12 03:57:46.095 
Epoch 154/1000 
	 loss: 29.7738, MinusLogProbMetric: 29.7738, val_loss: 30.1794, val_MinusLogProbMetric: 30.1794

Epoch 154: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.7738 - MinusLogProbMetric: 29.7738 - val_loss: 30.1794 - val_MinusLogProbMetric: 30.1794 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 155/1000
2023-10-12 03:58:12.958 
Epoch 155/1000 
	 loss: 29.5512, MinusLogProbMetric: 29.5512, val_loss: 30.1128, val_MinusLogProbMetric: 30.1128

Epoch 155: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.5512 - MinusLogProbMetric: 29.5512 - val_loss: 30.1128 - val_MinusLogProbMetric: 30.1128 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 156/1000
2023-10-12 03:58:40.107 
Epoch 156/1000 
	 loss: 29.6927, MinusLogProbMetric: 29.6927, val_loss: 29.8117, val_MinusLogProbMetric: 29.8117

Epoch 156: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.6927 - MinusLogProbMetric: 29.6927 - val_loss: 29.8117 - val_MinusLogProbMetric: 29.8117 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 157/1000
2023-10-12 03:59:07.071 
Epoch 157/1000 
	 loss: 29.9841, MinusLogProbMetric: 29.9841, val_loss: 29.9772, val_MinusLogProbMetric: 29.9772

Epoch 157: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.9841 - MinusLogProbMetric: 29.9841 - val_loss: 29.9772 - val_MinusLogProbMetric: 29.9772 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 158/1000
2023-10-12 03:59:34.005 
Epoch 158/1000 
	 loss: 29.8016, MinusLogProbMetric: 29.8016, val_loss: 30.1046, val_MinusLogProbMetric: 30.1046

Epoch 158: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.8016 - MinusLogProbMetric: 29.8016 - val_loss: 30.1046 - val_MinusLogProbMetric: 30.1046 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 159/1000
2023-10-12 04:00:00.885 
Epoch 159/1000 
	 loss: 29.4485, MinusLogProbMetric: 29.4485, val_loss: 31.0130, val_MinusLogProbMetric: 31.0130

Epoch 159: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.4485 - MinusLogProbMetric: 29.4485 - val_loss: 31.0130 - val_MinusLogProbMetric: 31.0130 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 160/1000
2023-10-12 04:00:27.620 
Epoch 160/1000 
	 loss: 29.5671, MinusLogProbMetric: 29.5671, val_loss: 31.8401, val_MinusLogProbMetric: 31.8401

Epoch 160: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.5671 - MinusLogProbMetric: 29.5671 - val_loss: 31.8401 - val_MinusLogProbMetric: 31.8401 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 161/1000
2023-10-12 04:00:55.361 
Epoch 161/1000 
	 loss: 29.6835, MinusLogProbMetric: 29.6835, val_loss: 30.5106, val_MinusLogProbMetric: 30.5106

Epoch 161: val_loss did not improve from 29.57059
196/196 - 28s - loss: 29.6835 - MinusLogProbMetric: 29.6835 - val_loss: 30.5106 - val_MinusLogProbMetric: 30.5106 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 162/1000
2023-10-12 04:01:22.213 
Epoch 162/1000 
	 loss: 29.5477, MinusLogProbMetric: 29.5477, val_loss: 30.0954, val_MinusLogProbMetric: 30.0954

Epoch 162: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.5477 - MinusLogProbMetric: 29.5477 - val_loss: 30.0954 - val_MinusLogProbMetric: 30.0954 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 163/1000
2023-10-12 04:01:48.972 
Epoch 163/1000 
	 loss: 29.4535, MinusLogProbMetric: 29.4535, val_loss: 29.9380, val_MinusLogProbMetric: 29.9380

Epoch 163: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.4535 - MinusLogProbMetric: 29.4535 - val_loss: 29.9380 - val_MinusLogProbMetric: 29.9380 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 164/1000
2023-10-12 04:02:16.315 
Epoch 164/1000 
	 loss: 29.6803, MinusLogProbMetric: 29.6803, val_loss: 30.5768, val_MinusLogProbMetric: 30.5768

Epoch 164: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.6803 - MinusLogProbMetric: 29.6803 - val_loss: 30.5768 - val_MinusLogProbMetric: 30.5768 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 165/1000
2023-10-12 04:02:43.344 
Epoch 165/1000 
	 loss: 29.4007, MinusLogProbMetric: 29.4007, val_loss: 30.7363, val_MinusLogProbMetric: 30.7363

Epoch 165: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.4007 - MinusLogProbMetric: 29.4007 - val_loss: 30.7363 - val_MinusLogProbMetric: 30.7363 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 166/1000
2023-10-12 04:03:10.446 
Epoch 166/1000 
	 loss: 29.5546, MinusLogProbMetric: 29.5546, val_loss: 31.2300, val_MinusLogProbMetric: 31.2300

Epoch 166: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.5546 - MinusLogProbMetric: 29.5546 - val_loss: 31.2300 - val_MinusLogProbMetric: 31.2300 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 167/1000
2023-10-12 04:03:38.615 
Epoch 167/1000 
	 loss: 29.5502, MinusLogProbMetric: 29.5502, val_loss: 30.7932, val_MinusLogProbMetric: 30.7932

Epoch 167: val_loss did not improve from 29.57059
196/196 - 28s - loss: 29.5502 - MinusLogProbMetric: 29.5502 - val_loss: 30.7932 - val_MinusLogProbMetric: 30.7932 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 168/1000
2023-10-12 04:04:05.328 
Epoch 168/1000 
	 loss: 29.4070, MinusLogProbMetric: 29.4070, val_loss: 29.6814, val_MinusLogProbMetric: 29.6814

Epoch 168: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.4070 - MinusLogProbMetric: 29.4070 - val_loss: 29.6814 - val_MinusLogProbMetric: 29.6814 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 169/1000
2023-10-12 04:04:31.780 
Epoch 169/1000 
	 loss: 29.7300, MinusLogProbMetric: 29.7300, val_loss: 30.1656, val_MinusLogProbMetric: 30.1656

Epoch 169: val_loss did not improve from 29.57059
196/196 - 26s - loss: 29.7300 - MinusLogProbMetric: 29.7300 - val_loss: 30.1656 - val_MinusLogProbMetric: 30.1656 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 170/1000
2023-10-12 04:04:58.073 
Epoch 170/1000 
	 loss: 29.5474, MinusLogProbMetric: 29.5474, val_loss: 29.8852, val_MinusLogProbMetric: 29.8852

Epoch 170: val_loss did not improve from 29.57059
196/196 - 26s - loss: 29.5474 - MinusLogProbMetric: 29.5474 - val_loss: 29.8852 - val_MinusLogProbMetric: 29.8852 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 171/1000
2023-10-12 04:05:24.871 
Epoch 171/1000 
	 loss: 29.6229, MinusLogProbMetric: 29.6229, val_loss: 30.2291, val_MinusLogProbMetric: 30.2291

Epoch 171: val_loss did not improve from 29.57059
196/196 - 27s - loss: 29.6229 - MinusLogProbMetric: 29.6229 - val_loss: 30.2291 - val_MinusLogProbMetric: 30.2291 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 172/1000
2023-10-12 04:05:53.041 
Epoch 172/1000 
	 loss: 29.6587, MinusLogProbMetric: 29.6587, val_loss: 29.8915, val_MinusLogProbMetric: 29.8915

Epoch 172: val_loss did not improve from 29.57059
196/196 - 28s - loss: 29.6587 - MinusLogProbMetric: 29.6587 - val_loss: 29.8915 - val_MinusLogProbMetric: 29.8915 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 173/1000
2023-10-12 04:06:20.195 
Epoch 173/1000 
	 loss: 29.6608, MinusLogProbMetric: 29.6608, val_loss: 29.5689, val_MinusLogProbMetric: 29.5689

Epoch 173: val_loss improved from 29.57059 to 29.56889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 29.6608 - MinusLogProbMetric: 29.6608 - val_loss: 29.5689 - val_MinusLogProbMetric: 29.5689 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 174/1000
2023-10-12 04:06:47.976 
Epoch 174/1000 
	 loss: 29.5493, MinusLogProbMetric: 29.5493, val_loss: 29.8904, val_MinusLogProbMetric: 29.8904

Epoch 174: val_loss did not improve from 29.56889
196/196 - 27s - loss: 29.5493 - MinusLogProbMetric: 29.5493 - val_loss: 29.8904 - val_MinusLogProbMetric: 29.8904 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 175/1000
2023-10-12 04:07:14.225 
Epoch 175/1000 
	 loss: 29.5248, MinusLogProbMetric: 29.5248, val_loss: 30.2344, val_MinusLogProbMetric: 30.2344

Epoch 175: val_loss did not improve from 29.56889
196/196 - 26s - loss: 29.5248 - MinusLogProbMetric: 29.5248 - val_loss: 30.2344 - val_MinusLogProbMetric: 30.2344 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 176/1000
2023-10-12 04:07:41.914 
Epoch 176/1000 
	 loss: 29.3502, MinusLogProbMetric: 29.3502, val_loss: 30.3857, val_MinusLogProbMetric: 30.3857

Epoch 176: val_loss did not improve from 29.56889
196/196 - 28s - loss: 29.3502 - MinusLogProbMetric: 29.3502 - val_loss: 30.3857 - val_MinusLogProbMetric: 30.3857 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 177/1000
2023-10-12 04:08:08.952 
Epoch 177/1000 
	 loss: 29.7288, MinusLogProbMetric: 29.7288, val_loss: 30.2652, val_MinusLogProbMetric: 30.2652

Epoch 177: val_loss did not improve from 29.56889
196/196 - 27s - loss: 29.7288 - MinusLogProbMetric: 29.7288 - val_loss: 30.2652 - val_MinusLogProbMetric: 30.2652 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 178/1000
2023-10-12 04:08:36.224 
Epoch 178/1000 
	 loss: 29.5212, MinusLogProbMetric: 29.5212, val_loss: 30.5238, val_MinusLogProbMetric: 30.5238

Epoch 178: val_loss did not improve from 29.56889
196/196 - 27s - loss: 29.5212 - MinusLogProbMetric: 29.5212 - val_loss: 30.5238 - val_MinusLogProbMetric: 30.5238 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 179/1000
2023-10-12 04:09:04.960 
Epoch 179/1000 
	 loss: 29.4501, MinusLogProbMetric: 29.4501, val_loss: 29.7856, val_MinusLogProbMetric: 29.7856

Epoch 179: val_loss did not improve from 29.56889
196/196 - 29s - loss: 29.4501 - MinusLogProbMetric: 29.4501 - val_loss: 29.7856 - val_MinusLogProbMetric: 29.7856 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 180/1000
2023-10-12 04:09:32.697 
Epoch 180/1000 
	 loss: 29.3602, MinusLogProbMetric: 29.3602, val_loss: 29.6397, val_MinusLogProbMetric: 29.6397

Epoch 180: val_loss did not improve from 29.56889
196/196 - 28s - loss: 29.3602 - MinusLogProbMetric: 29.3602 - val_loss: 29.6397 - val_MinusLogProbMetric: 29.6397 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 181/1000
2023-10-12 04:09:59.739 
Epoch 181/1000 
	 loss: 29.5023, MinusLogProbMetric: 29.5023, val_loss: 29.9075, val_MinusLogProbMetric: 29.9075

Epoch 181: val_loss did not improve from 29.56889
196/196 - 27s - loss: 29.5023 - MinusLogProbMetric: 29.5023 - val_loss: 29.9075 - val_MinusLogProbMetric: 29.9075 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 182/1000
2023-10-12 04:10:26.926 
Epoch 182/1000 
	 loss: 29.5661, MinusLogProbMetric: 29.5661, val_loss: 30.2908, val_MinusLogProbMetric: 30.2908

Epoch 182: val_loss did not improve from 29.56889
196/196 - 27s - loss: 29.5661 - MinusLogProbMetric: 29.5661 - val_loss: 30.2908 - val_MinusLogProbMetric: 30.2908 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 183/1000
2023-10-12 04:10:53.447 
Epoch 183/1000 
	 loss: 29.3664, MinusLogProbMetric: 29.3664, val_loss: 29.8682, val_MinusLogProbMetric: 29.8682

Epoch 183: val_loss did not improve from 29.56889
196/196 - 27s - loss: 29.3664 - MinusLogProbMetric: 29.3664 - val_loss: 29.8682 - val_MinusLogProbMetric: 29.8682 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 184/1000
2023-10-12 04:11:20.998 
Epoch 184/1000 
	 loss: 29.5583, MinusLogProbMetric: 29.5583, val_loss: 29.4891, val_MinusLogProbMetric: 29.4891

Epoch 184: val_loss improved from 29.56889 to 29.48910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 29.5583 - MinusLogProbMetric: 29.5583 - val_loss: 29.4891 - val_MinusLogProbMetric: 29.4891 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 185/1000
2023-10-12 04:11:48.758 
Epoch 185/1000 
	 loss: 29.5422, MinusLogProbMetric: 29.5422, val_loss: 29.6464, val_MinusLogProbMetric: 29.6464

Epoch 185: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.5422 - MinusLogProbMetric: 29.5422 - val_loss: 29.6464 - val_MinusLogProbMetric: 29.6464 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 186/1000
2023-10-12 04:12:16.221 
Epoch 186/1000 
	 loss: 29.4904, MinusLogProbMetric: 29.4904, val_loss: 29.8827, val_MinusLogProbMetric: 29.8827

Epoch 186: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.4904 - MinusLogProbMetric: 29.4904 - val_loss: 29.8827 - val_MinusLogProbMetric: 29.8827 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 187/1000
2023-10-12 04:12:43.725 
Epoch 187/1000 
	 loss: 29.3943, MinusLogProbMetric: 29.3943, val_loss: 30.8347, val_MinusLogProbMetric: 30.8347

Epoch 187: val_loss did not improve from 29.48910
196/196 - 28s - loss: 29.3943 - MinusLogProbMetric: 29.3943 - val_loss: 30.8347 - val_MinusLogProbMetric: 30.8347 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 188/1000
2023-10-12 04:13:10.572 
Epoch 188/1000 
	 loss: 29.3485, MinusLogProbMetric: 29.3485, val_loss: 30.0086, val_MinusLogProbMetric: 30.0086

Epoch 188: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.3485 - MinusLogProbMetric: 29.3485 - val_loss: 30.0086 - val_MinusLogProbMetric: 30.0086 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 189/1000
2023-10-12 04:13:37.548 
Epoch 189/1000 
	 loss: 29.6172, MinusLogProbMetric: 29.6172, val_loss: 30.4608, val_MinusLogProbMetric: 30.4608

Epoch 189: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.6172 - MinusLogProbMetric: 29.6172 - val_loss: 30.4608 - val_MinusLogProbMetric: 30.4608 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 190/1000
2023-10-12 04:14:04.491 
Epoch 190/1000 
	 loss: 29.5094, MinusLogProbMetric: 29.5094, val_loss: 29.8811, val_MinusLogProbMetric: 29.8811

Epoch 190: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.5094 - MinusLogProbMetric: 29.5094 - val_loss: 29.8811 - val_MinusLogProbMetric: 29.8811 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 191/1000
2023-10-12 04:14:31.417 
Epoch 191/1000 
	 loss: 29.3145, MinusLogProbMetric: 29.3145, val_loss: 30.2277, val_MinusLogProbMetric: 30.2277

Epoch 191: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.3145 - MinusLogProbMetric: 29.3145 - val_loss: 30.2277 - val_MinusLogProbMetric: 30.2277 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 192/1000
2023-10-12 04:14:58.294 
Epoch 192/1000 
	 loss: 29.3628, MinusLogProbMetric: 29.3628, val_loss: 30.6496, val_MinusLogProbMetric: 30.6496

Epoch 192: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.3628 - MinusLogProbMetric: 29.3628 - val_loss: 30.6496 - val_MinusLogProbMetric: 30.6496 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 193/1000
2023-10-12 04:15:25.190 
Epoch 193/1000 
	 loss: 29.4831, MinusLogProbMetric: 29.4831, val_loss: 30.1300, val_MinusLogProbMetric: 30.1300

Epoch 193: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.4831 - MinusLogProbMetric: 29.4831 - val_loss: 30.1300 - val_MinusLogProbMetric: 30.1300 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 194/1000
2023-10-12 04:15:51.756 
Epoch 194/1000 
	 loss: 29.5362, MinusLogProbMetric: 29.5362, val_loss: 30.1375, val_MinusLogProbMetric: 30.1375

Epoch 194: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.5362 - MinusLogProbMetric: 29.5362 - val_loss: 30.1375 - val_MinusLogProbMetric: 30.1375 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 195/1000
2023-10-12 04:16:20.058 
Epoch 195/1000 
	 loss: 29.2753, MinusLogProbMetric: 29.2753, val_loss: 30.4492, val_MinusLogProbMetric: 30.4492

Epoch 195: val_loss did not improve from 29.48910
196/196 - 28s - loss: 29.2753 - MinusLogProbMetric: 29.2753 - val_loss: 30.4492 - val_MinusLogProbMetric: 30.4492 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 196/1000
2023-10-12 04:16:46.943 
Epoch 196/1000 
	 loss: 29.1163, MinusLogProbMetric: 29.1163, val_loss: 30.4672, val_MinusLogProbMetric: 30.4672

Epoch 196: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.1163 - MinusLogProbMetric: 29.1163 - val_loss: 30.4672 - val_MinusLogProbMetric: 30.4672 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 197/1000
2023-10-12 04:17:14.183 
Epoch 197/1000 
	 loss: 29.2688, MinusLogProbMetric: 29.2688, val_loss: 29.7258, val_MinusLogProbMetric: 29.7258

Epoch 197: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.2688 - MinusLogProbMetric: 29.2688 - val_loss: 29.7258 - val_MinusLogProbMetric: 29.7258 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 198/1000
2023-10-12 04:17:41.131 
Epoch 198/1000 
	 loss: 29.4096, MinusLogProbMetric: 29.4096, val_loss: 30.5466, val_MinusLogProbMetric: 30.5466

Epoch 198: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.4096 - MinusLogProbMetric: 29.4096 - val_loss: 30.5466 - val_MinusLogProbMetric: 30.5466 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 199/1000
2023-10-12 04:18:08.234 
Epoch 199/1000 
	 loss: 29.2030, MinusLogProbMetric: 29.2030, val_loss: 31.7494, val_MinusLogProbMetric: 31.7494

Epoch 199: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.2030 - MinusLogProbMetric: 29.2030 - val_loss: 31.7494 - val_MinusLogProbMetric: 31.7494 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 200/1000
2023-10-12 04:18:34.803 
Epoch 200/1000 
	 loss: 29.2130, MinusLogProbMetric: 29.2130, val_loss: 30.3836, val_MinusLogProbMetric: 30.3836

Epoch 200: val_loss did not improve from 29.48910
196/196 - 27s - loss: 29.2130 - MinusLogProbMetric: 29.2130 - val_loss: 30.3836 - val_MinusLogProbMetric: 30.3836 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 201/1000
2023-10-12 04:19:00.998 
Epoch 201/1000 
	 loss: 29.4423, MinusLogProbMetric: 29.4423, val_loss: 29.3586, val_MinusLogProbMetric: 29.3586

Epoch 201: val_loss improved from 29.48910 to 29.35865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 29.4423 - MinusLogProbMetric: 29.4423 - val_loss: 29.3586 - val_MinusLogProbMetric: 29.3586 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 202/1000
2023-10-12 04:19:28.134 
Epoch 202/1000 
	 loss: 29.4264, MinusLogProbMetric: 29.4264, val_loss: 29.9039, val_MinusLogProbMetric: 29.9039

Epoch 202: val_loss did not improve from 29.35865
196/196 - 27s - loss: 29.4264 - MinusLogProbMetric: 29.4264 - val_loss: 29.9039 - val_MinusLogProbMetric: 29.9039 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 203/1000
2023-10-12 04:19:54.167 
Epoch 203/1000 
	 loss: 29.2726, MinusLogProbMetric: 29.2726, val_loss: 30.1977, val_MinusLogProbMetric: 30.1977

Epoch 203: val_loss did not improve from 29.35865
196/196 - 26s - loss: 29.2726 - MinusLogProbMetric: 29.2726 - val_loss: 30.1977 - val_MinusLogProbMetric: 30.1977 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 204/1000
2023-10-12 04:20:20.772 
Epoch 204/1000 
	 loss: 29.3329, MinusLogProbMetric: 29.3329, val_loss: 29.6565, val_MinusLogProbMetric: 29.6565

Epoch 204: val_loss did not improve from 29.35865
196/196 - 27s - loss: 29.3329 - MinusLogProbMetric: 29.3329 - val_loss: 29.6565 - val_MinusLogProbMetric: 29.6565 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 205/1000
2023-10-12 04:20:47.857 
Epoch 205/1000 
	 loss: 29.0783, MinusLogProbMetric: 29.0783, val_loss: 29.3418, val_MinusLogProbMetric: 29.3418

Epoch 205: val_loss improved from 29.35865 to 29.34183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 29.0783 - MinusLogProbMetric: 29.0783 - val_loss: 29.3418 - val_MinusLogProbMetric: 29.3418 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 206/1000
2023-10-12 04:21:15.176 
Epoch 206/1000 
	 loss: 29.3054, MinusLogProbMetric: 29.3054, val_loss: 30.5070, val_MinusLogProbMetric: 30.5070

Epoch 206: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.3054 - MinusLogProbMetric: 29.3054 - val_loss: 30.5070 - val_MinusLogProbMetric: 30.5070 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 207/1000
2023-10-12 04:21:41.028 
Epoch 207/1000 
	 loss: 29.1467, MinusLogProbMetric: 29.1467, val_loss: 29.5768, val_MinusLogProbMetric: 29.5768

Epoch 207: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.1467 - MinusLogProbMetric: 29.1467 - val_loss: 29.5768 - val_MinusLogProbMetric: 29.5768 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 208/1000
2023-10-12 04:22:07.393 
Epoch 208/1000 
	 loss: 29.3227, MinusLogProbMetric: 29.3227, val_loss: 29.8228, val_MinusLogProbMetric: 29.8228

Epoch 208: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.3227 - MinusLogProbMetric: 29.3227 - val_loss: 29.8228 - val_MinusLogProbMetric: 29.8228 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 209/1000
2023-10-12 04:22:33.683 
Epoch 209/1000 
	 loss: 29.4257, MinusLogProbMetric: 29.4257, val_loss: 29.6181, val_MinusLogProbMetric: 29.6181

Epoch 209: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.4257 - MinusLogProbMetric: 29.4257 - val_loss: 29.6181 - val_MinusLogProbMetric: 29.6181 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 210/1000
2023-10-12 04:22:59.888 
Epoch 210/1000 
	 loss: 29.3315, MinusLogProbMetric: 29.3315, val_loss: 29.6063, val_MinusLogProbMetric: 29.6063

Epoch 210: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.3315 - MinusLogProbMetric: 29.3315 - val_loss: 29.6063 - val_MinusLogProbMetric: 29.6063 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 211/1000
2023-10-12 04:23:26.516 
Epoch 211/1000 
	 loss: 29.3820, MinusLogProbMetric: 29.3820, val_loss: 30.4719, val_MinusLogProbMetric: 30.4719

Epoch 211: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.3820 - MinusLogProbMetric: 29.3820 - val_loss: 30.4719 - val_MinusLogProbMetric: 30.4719 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 212/1000
2023-10-12 04:23:53.544 
Epoch 212/1000 
	 loss: 29.0671, MinusLogProbMetric: 29.0671, val_loss: 29.5111, val_MinusLogProbMetric: 29.5111

Epoch 212: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.0671 - MinusLogProbMetric: 29.0671 - val_loss: 29.5111 - val_MinusLogProbMetric: 29.5111 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 213/1000
2023-10-12 04:24:20.479 
Epoch 213/1000 
	 loss: 29.3187, MinusLogProbMetric: 29.3187, val_loss: 29.9342, val_MinusLogProbMetric: 29.9342

Epoch 213: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.3187 - MinusLogProbMetric: 29.3187 - val_loss: 29.9342 - val_MinusLogProbMetric: 29.9342 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 214/1000
2023-10-12 04:24:46.860 
Epoch 214/1000 
	 loss: 29.1339, MinusLogProbMetric: 29.1339, val_loss: 29.4580, val_MinusLogProbMetric: 29.4580

Epoch 214: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.1339 - MinusLogProbMetric: 29.1339 - val_loss: 29.4580 - val_MinusLogProbMetric: 29.4580 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 215/1000
2023-10-12 04:25:13.640 
Epoch 215/1000 
	 loss: 29.2025, MinusLogProbMetric: 29.2025, val_loss: 29.9700, val_MinusLogProbMetric: 29.9700

Epoch 215: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.2025 - MinusLogProbMetric: 29.2025 - val_loss: 29.9700 - val_MinusLogProbMetric: 29.9700 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 216/1000
2023-10-12 04:25:41.411 
Epoch 216/1000 
	 loss: 29.2179, MinusLogProbMetric: 29.2179, val_loss: 29.7732, val_MinusLogProbMetric: 29.7732

Epoch 216: val_loss did not improve from 29.34183
196/196 - 28s - loss: 29.2179 - MinusLogProbMetric: 29.2179 - val_loss: 29.7732 - val_MinusLogProbMetric: 29.7732 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 217/1000
2023-10-12 04:26:08.225 
Epoch 217/1000 
	 loss: 29.0994, MinusLogProbMetric: 29.0994, val_loss: 30.2904, val_MinusLogProbMetric: 30.2904

Epoch 217: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.0994 - MinusLogProbMetric: 29.0994 - val_loss: 30.2904 - val_MinusLogProbMetric: 30.2904 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 218/1000
2023-10-12 04:26:34.816 
Epoch 218/1000 
	 loss: 29.2632, MinusLogProbMetric: 29.2632, val_loss: 29.6507, val_MinusLogProbMetric: 29.6507

Epoch 218: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.2632 - MinusLogProbMetric: 29.2632 - val_loss: 29.6507 - val_MinusLogProbMetric: 29.6507 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 219/1000
2023-10-12 04:27:01.266 
Epoch 219/1000 
	 loss: 29.2464, MinusLogProbMetric: 29.2464, val_loss: 30.9533, val_MinusLogProbMetric: 30.9533

Epoch 219: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.2464 - MinusLogProbMetric: 29.2464 - val_loss: 30.9533 - val_MinusLogProbMetric: 30.9533 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 220/1000
2023-10-12 04:27:26.977 
Epoch 220/1000 
	 loss: 29.0723, MinusLogProbMetric: 29.0723, val_loss: 29.6191, val_MinusLogProbMetric: 29.6191

Epoch 220: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.0723 - MinusLogProbMetric: 29.0723 - val_loss: 29.6191 - val_MinusLogProbMetric: 29.6191 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 221/1000
2023-10-12 04:27:52.829 
Epoch 221/1000 
	 loss: 29.1489, MinusLogProbMetric: 29.1489, val_loss: 29.6864, val_MinusLogProbMetric: 29.6864

Epoch 221: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.1489 - MinusLogProbMetric: 29.1489 - val_loss: 29.6864 - val_MinusLogProbMetric: 29.6864 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 222/1000
2023-10-12 04:28:18.866 
Epoch 222/1000 
	 loss: 29.1930, MinusLogProbMetric: 29.1930, val_loss: 30.3511, val_MinusLogProbMetric: 30.3511

Epoch 222: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.1930 - MinusLogProbMetric: 29.1930 - val_loss: 30.3511 - val_MinusLogProbMetric: 30.3511 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 223/1000
2023-10-12 04:28:44.775 
Epoch 223/1000 
	 loss: 29.3685, MinusLogProbMetric: 29.3685, val_loss: 30.1609, val_MinusLogProbMetric: 30.1609

Epoch 223: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.3685 - MinusLogProbMetric: 29.3685 - val_loss: 30.1609 - val_MinusLogProbMetric: 30.1609 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 224/1000
2023-10-12 04:29:11.570 
Epoch 224/1000 
	 loss: 29.1020, MinusLogProbMetric: 29.1020, val_loss: 29.4934, val_MinusLogProbMetric: 29.4934

Epoch 224: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.1020 - MinusLogProbMetric: 29.1020 - val_loss: 29.4934 - val_MinusLogProbMetric: 29.4934 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 225/1000
2023-10-12 04:29:39.905 
Epoch 225/1000 
	 loss: 29.1225, MinusLogProbMetric: 29.1225, val_loss: 29.9715, val_MinusLogProbMetric: 29.9715

Epoch 225: val_loss did not improve from 29.34183
196/196 - 28s - loss: 29.1225 - MinusLogProbMetric: 29.1225 - val_loss: 29.9715 - val_MinusLogProbMetric: 29.9715 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 226/1000
2023-10-12 04:30:07.180 
Epoch 226/1000 
	 loss: 29.0343, MinusLogProbMetric: 29.0343, val_loss: 29.6510, val_MinusLogProbMetric: 29.6510

Epoch 226: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.0343 - MinusLogProbMetric: 29.0343 - val_loss: 29.6510 - val_MinusLogProbMetric: 29.6510 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 227/1000
2023-10-12 04:30:34.007 
Epoch 227/1000 
	 loss: 29.0862, MinusLogProbMetric: 29.0862, val_loss: 29.6466, val_MinusLogProbMetric: 29.6466

Epoch 227: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.0862 - MinusLogProbMetric: 29.0862 - val_loss: 29.6466 - val_MinusLogProbMetric: 29.6466 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 228/1000
2023-10-12 04:31:00.619 
Epoch 228/1000 
	 loss: 29.0125, MinusLogProbMetric: 29.0125, val_loss: 30.1765, val_MinusLogProbMetric: 30.1765

Epoch 228: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.0125 - MinusLogProbMetric: 29.0125 - val_loss: 30.1765 - val_MinusLogProbMetric: 30.1765 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 229/1000
2023-10-12 04:31:27.953 
Epoch 229/1000 
	 loss: 29.1838, MinusLogProbMetric: 29.1838, val_loss: 30.3318, val_MinusLogProbMetric: 30.3318

Epoch 229: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.1838 - MinusLogProbMetric: 29.1838 - val_loss: 30.3318 - val_MinusLogProbMetric: 30.3318 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 230/1000
2023-10-12 04:31:54.549 
Epoch 230/1000 
	 loss: 29.0288, MinusLogProbMetric: 29.0288, val_loss: 30.5919, val_MinusLogProbMetric: 30.5919

Epoch 230: val_loss did not improve from 29.34183
196/196 - 27s - loss: 29.0288 - MinusLogProbMetric: 29.0288 - val_loss: 30.5919 - val_MinusLogProbMetric: 30.5919 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 231/1000
2023-10-12 04:32:20.839 
Epoch 231/1000 
	 loss: 29.1959, MinusLogProbMetric: 29.1959, val_loss: 29.8050, val_MinusLogProbMetric: 29.8050

Epoch 231: val_loss did not improve from 29.34183
196/196 - 26s - loss: 29.1959 - MinusLogProbMetric: 29.1959 - val_loss: 29.8050 - val_MinusLogProbMetric: 29.8050 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 232/1000
2023-10-12 04:32:47.605 
Epoch 232/1000 
	 loss: 28.9910, MinusLogProbMetric: 28.9910, val_loss: 29.1504, val_MinusLogProbMetric: 29.1504

Epoch 232: val_loss improved from 29.34183 to 29.15045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 28.9910 - MinusLogProbMetric: 28.9910 - val_loss: 29.1504 - val_MinusLogProbMetric: 29.1504 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 233/1000
2023-10-12 04:33:15.163 
Epoch 233/1000 
	 loss: 29.0146, MinusLogProbMetric: 29.0146, val_loss: 29.3678, val_MinusLogProbMetric: 29.3678

Epoch 233: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.0146 - MinusLogProbMetric: 29.0146 - val_loss: 29.3678 - val_MinusLogProbMetric: 29.3678 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 234/1000
2023-10-12 04:33:42.516 
Epoch 234/1000 
	 loss: 29.1241, MinusLogProbMetric: 29.1241, val_loss: 29.5340, val_MinusLogProbMetric: 29.5340

Epoch 234: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.1241 - MinusLogProbMetric: 29.1241 - val_loss: 29.5340 - val_MinusLogProbMetric: 29.5340 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 235/1000
2023-10-12 04:34:09.383 
Epoch 235/1000 
	 loss: 29.2233, MinusLogProbMetric: 29.2233, val_loss: 29.8451, val_MinusLogProbMetric: 29.8451

Epoch 235: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.2233 - MinusLogProbMetric: 29.2233 - val_loss: 29.8451 - val_MinusLogProbMetric: 29.8451 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 236/1000
2023-10-12 04:34:35.528 
Epoch 236/1000 
	 loss: 29.1173, MinusLogProbMetric: 29.1173, val_loss: 29.4231, val_MinusLogProbMetric: 29.4231

Epoch 236: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.1173 - MinusLogProbMetric: 29.1173 - val_loss: 29.4231 - val_MinusLogProbMetric: 29.4231 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 237/1000
2023-10-12 04:35:01.539 
Epoch 237/1000 
	 loss: 28.9340, MinusLogProbMetric: 28.9340, val_loss: 30.0424, val_MinusLogProbMetric: 30.0424

Epoch 237: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.9340 - MinusLogProbMetric: 28.9340 - val_loss: 30.0424 - val_MinusLogProbMetric: 30.0424 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 238/1000
2023-10-12 04:35:27.122 
Epoch 238/1000 
	 loss: 29.1932, MinusLogProbMetric: 29.1932, val_loss: 30.6024, val_MinusLogProbMetric: 30.6024

Epoch 238: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.1932 - MinusLogProbMetric: 29.1932 - val_loss: 30.6024 - val_MinusLogProbMetric: 30.6024 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 239/1000
2023-10-12 04:35:52.571 
Epoch 239/1000 
	 loss: 29.1393, MinusLogProbMetric: 29.1393, val_loss: 29.8178, val_MinusLogProbMetric: 29.8178

Epoch 239: val_loss did not improve from 29.15045
196/196 - 25s - loss: 29.1393 - MinusLogProbMetric: 29.1393 - val_loss: 29.8178 - val_MinusLogProbMetric: 29.8178 - lr: 0.0010 - 25s/epoch - 130ms/step
Epoch 240/1000
2023-10-12 04:36:18.945 
Epoch 240/1000 
	 loss: 29.0502, MinusLogProbMetric: 29.0502, val_loss: 29.4429, val_MinusLogProbMetric: 29.4429

Epoch 240: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.0502 - MinusLogProbMetric: 29.0502 - val_loss: 29.4429 - val_MinusLogProbMetric: 29.4429 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 241/1000
2023-10-12 04:36:45.846 
Epoch 241/1000 
	 loss: 28.9560, MinusLogProbMetric: 28.9560, val_loss: 29.3320, val_MinusLogProbMetric: 29.3320

Epoch 241: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.9560 - MinusLogProbMetric: 28.9560 - val_loss: 29.3320 - val_MinusLogProbMetric: 29.3320 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 242/1000
2023-10-12 04:37:12.622 
Epoch 242/1000 
	 loss: 28.9963, MinusLogProbMetric: 28.9963, val_loss: 29.3547, val_MinusLogProbMetric: 29.3547

Epoch 242: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.9963 - MinusLogProbMetric: 28.9963 - val_loss: 29.3547 - val_MinusLogProbMetric: 29.3547 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 243/1000
2023-10-12 04:37:38.161 
Epoch 243/1000 
	 loss: 29.1019, MinusLogProbMetric: 29.1019, val_loss: 30.8136, val_MinusLogProbMetric: 30.8136

Epoch 243: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.1019 - MinusLogProbMetric: 29.1019 - val_loss: 30.8136 - val_MinusLogProbMetric: 30.8136 - lr: 0.0010 - 26s/epoch - 130ms/step
Epoch 244/1000
2023-10-12 04:38:04.166 
Epoch 244/1000 
	 loss: 28.9322, MinusLogProbMetric: 28.9322, val_loss: 29.3591, val_MinusLogProbMetric: 29.3591

Epoch 244: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.9322 - MinusLogProbMetric: 28.9322 - val_loss: 29.3591 - val_MinusLogProbMetric: 29.3591 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 245/1000
2023-10-12 04:38:30.120 
Epoch 245/1000 
	 loss: 29.0240, MinusLogProbMetric: 29.0240, val_loss: 30.4098, val_MinusLogProbMetric: 30.4098

Epoch 245: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.0240 - MinusLogProbMetric: 29.0240 - val_loss: 30.4098 - val_MinusLogProbMetric: 30.4098 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 246/1000
2023-10-12 04:38:56.395 
Epoch 246/1000 
	 loss: 29.0541, MinusLogProbMetric: 29.0541, val_loss: 29.1628, val_MinusLogProbMetric: 29.1628

Epoch 246: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.0541 - MinusLogProbMetric: 29.0541 - val_loss: 29.1628 - val_MinusLogProbMetric: 29.1628 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 247/1000
2023-10-12 04:39:24.344 
Epoch 247/1000 
	 loss: 28.8958, MinusLogProbMetric: 28.8958, val_loss: 32.7879, val_MinusLogProbMetric: 32.7879

Epoch 247: val_loss did not improve from 29.15045
196/196 - 28s - loss: 28.8958 - MinusLogProbMetric: 28.8958 - val_loss: 32.7879 - val_MinusLogProbMetric: 32.7879 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 248/1000
2023-10-12 04:39:51.531 
Epoch 248/1000 
	 loss: 28.8281, MinusLogProbMetric: 28.8281, val_loss: 30.3477, val_MinusLogProbMetric: 30.3477

Epoch 248: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8281 - MinusLogProbMetric: 28.8281 - val_loss: 30.3477 - val_MinusLogProbMetric: 30.3477 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 249/1000
2023-10-12 04:40:18.326 
Epoch 249/1000 
	 loss: 29.0675, MinusLogProbMetric: 29.0675, val_loss: 29.6918, val_MinusLogProbMetric: 29.6918

Epoch 249: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.0675 - MinusLogProbMetric: 29.0675 - val_loss: 29.6918 - val_MinusLogProbMetric: 29.6918 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 250/1000
2023-10-12 04:40:45.090 
Epoch 250/1000 
	 loss: 28.9612, MinusLogProbMetric: 28.9612, val_loss: 30.8114, val_MinusLogProbMetric: 30.8114

Epoch 250: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.9612 - MinusLogProbMetric: 28.9612 - val_loss: 30.8114 - val_MinusLogProbMetric: 30.8114 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 251/1000
2023-10-12 04:41:11.793 
Epoch 251/1000 
	 loss: 28.8576, MinusLogProbMetric: 28.8576, val_loss: 29.7108, val_MinusLogProbMetric: 29.7108

Epoch 251: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8576 - MinusLogProbMetric: 28.8576 - val_loss: 29.7108 - val_MinusLogProbMetric: 29.7108 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 252/1000
2023-10-12 04:41:38.199 
Epoch 252/1000 
	 loss: 29.1109, MinusLogProbMetric: 29.1109, val_loss: 29.3361, val_MinusLogProbMetric: 29.3361

Epoch 252: val_loss did not improve from 29.15045
196/196 - 26s - loss: 29.1109 - MinusLogProbMetric: 29.1109 - val_loss: 29.3361 - val_MinusLogProbMetric: 29.3361 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 253/1000
2023-10-12 04:42:06.292 
Epoch 253/1000 
	 loss: 29.0792, MinusLogProbMetric: 29.0792, val_loss: 29.5563, val_MinusLogProbMetric: 29.5563

Epoch 253: val_loss did not improve from 29.15045
196/196 - 28s - loss: 29.0792 - MinusLogProbMetric: 29.0792 - val_loss: 29.5563 - val_MinusLogProbMetric: 29.5563 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 254/1000
2023-10-12 04:42:33.140 
Epoch 254/1000 
	 loss: 28.7911, MinusLogProbMetric: 28.7911, val_loss: 29.3418, val_MinusLogProbMetric: 29.3418

Epoch 254: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.7911 - MinusLogProbMetric: 28.7911 - val_loss: 29.3418 - val_MinusLogProbMetric: 29.3418 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 255/1000
2023-10-12 04:42:59.822 
Epoch 255/1000 
	 loss: 29.0743, MinusLogProbMetric: 29.0743, val_loss: 29.4056, val_MinusLogProbMetric: 29.4056

Epoch 255: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.0743 - MinusLogProbMetric: 29.0743 - val_loss: 29.4056 - val_MinusLogProbMetric: 29.4056 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 256/1000
2023-10-12 04:43:26.512 
Epoch 256/1000 
	 loss: 28.9272, MinusLogProbMetric: 28.9272, val_loss: 29.3502, val_MinusLogProbMetric: 29.3502

Epoch 256: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.9272 - MinusLogProbMetric: 28.9272 - val_loss: 29.3502 - val_MinusLogProbMetric: 29.3502 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 257/1000
2023-10-12 04:43:53.923 
Epoch 257/1000 
	 loss: 28.8965, MinusLogProbMetric: 28.8965, val_loss: 29.5434, val_MinusLogProbMetric: 29.5434

Epoch 257: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8965 - MinusLogProbMetric: 28.8965 - val_loss: 29.5434 - val_MinusLogProbMetric: 29.5434 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 258/1000
2023-10-12 04:44:21.101 
Epoch 258/1000 
	 loss: 29.0096, MinusLogProbMetric: 29.0096, val_loss: 30.1339, val_MinusLogProbMetric: 30.1339

Epoch 258: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.0096 - MinusLogProbMetric: 29.0096 - val_loss: 30.1339 - val_MinusLogProbMetric: 30.1339 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 259/1000
2023-10-12 04:44:47.821 
Epoch 259/1000 
	 loss: 28.7945, MinusLogProbMetric: 28.7945, val_loss: 30.0320, val_MinusLogProbMetric: 30.0320

Epoch 259: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.7945 - MinusLogProbMetric: 28.7945 - val_loss: 30.0320 - val_MinusLogProbMetric: 30.0320 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 260/1000
2023-10-12 04:45:15.081 
Epoch 260/1000 
	 loss: 29.0988, MinusLogProbMetric: 29.0988, val_loss: 29.3916, val_MinusLogProbMetric: 29.3916

Epoch 260: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.0988 - MinusLogProbMetric: 29.0988 - val_loss: 29.3916 - val_MinusLogProbMetric: 29.3916 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 261/1000
2023-10-12 04:45:43.882 
Epoch 261/1000 
	 loss: 28.8530, MinusLogProbMetric: 28.8530, val_loss: 30.2160, val_MinusLogProbMetric: 30.2160

Epoch 261: val_loss did not improve from 29.15045
196/196 - 29s - loss: 28.8530 - MinusLogProbMetric: 28.8530 - val_loss: 30.2160 - val_MinusLogProbMetric: 30.2160 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 262/1000
2023-10-12 04:46:11.417 
Epoch 262/1000 
	 loss: 28.8488, MinusLogProbMetric: 28.8488, val_loss: 30.5201, val_MinusLogProbMetric: 30.5201

Epoch 262: val_loss did not improve from 29.15045
196/196 - 28s - loss: 28.8488 - MinusLogProbMetric: 28.8488 - val_loss: 30.5201 - val_MinusLogProbMetric: 30.5201 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 263/1000
2023-10-12 04:46:38.274 
Epoch 263/1000 
	 loss: 28.8980, MinusLogProbMetric: 28.8980, val_loss: 29.8361, val_MinusLogProbMetric: 29.8361

Epoch 263: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8980 - MinusLogProbMetric: 28.8980 - val_loss: 29.8361 - val_MinusLogProbMetric: 29.8361 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 264/1000
2023-10-12 04:47:04.543 
Epoch 264/1000 
	 loss: 28.8695, MinusLogProbMetric: 28.8695, val_loss: 30.4093, val_MinusLogProbMetric: 30.4093

Epoch 264: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.8695 - MinusLogProbMetric: 28.8695 - val_loss: 30.4093 - val_MinusLogProbMetric: 30.4093 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 265/1000
2023-10-12 04:47:30.543 
Epoch 265/1000 
	 loss: 28.9904, MinusLogProbMetric: 28.9904, val_loss: 29.6159, val_MinusLogProbMetric: 29.6159

Epoch 265: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.9904 - MinusLogProbMetric: 28.9904 - val_loss: 29.6159 - val_MinusLogProbMetric: 29.6159 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 266/1000
2023-10-12 04:47:57.081 
Epoch 266/1000 
	 loss: 28.8583, MinusLogProbMetric: 28.8583, val_loss: 30.4267, val_MinusLogProbMetric: 30.4267

Epoch 266: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8583 - MinusLogProbMetric: 28.8583 - val_loss: 30.4267 - val_MinusLogProbMetric: 30.4267 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 267/1000
2023-10-12 04:48:24.951 
Epoch 267/1000 
	 loss: 28.8858, MinusLogProbMetric: 28.8858, val_loss: 29.3886, val_MinusLogProbMetric: 29.3886

Epoch 267: val_loss did not improve from 29.15045
196/196 - 28s - loss: 28.8858 - MinusLogProbMetric: 28.8858 - val_loss: 29.3886 - val_MinusLogProbMetric: 29.3886 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 268/1000
2023-10-12 04:48:52.209 
Epoch 268/1000 
	 loss: 28.8684, MinusLogProbMetric: 28.8684, val_loss: 29.9142, val_MinusLogProbMetric: 29.9142

Epoch 268: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8684 - MinusLogProbMetric: 28.8684 - val_loss: 29.9142 - val_MinusLogProbMetric: 29.9142 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 269/1000
2023-10-12 04:49:19.416 
Epoch 269/1000 
	 loss: 29.0257, MinusLogProbMetric: 29.0257, val_loss: 29.4671, val_MinusLogProbMetric: 29.4671

Epoch 269: val_loss did not improve from 29.15045
196/196 - 27s - loss: 29.0257 - MinusLogProbMetric: 29.0257 - val_loss: 29.4671 - val_MinusLogProbMetric: 29.4671 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 270/1000
2023-10-12 04:49:45.579 
Epoch 270/1000 
	 loss: 28.7868, MinusLogProbMetric: 28.7868, val_loss: 30.1945, val_MinusLogProbMetric: 30.1945

Epoch 270: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.7868 - MinusLogProbMetric: 28.7868 - val_loss: 30.1945 - val_MinusLogProbMetric: 30.1945 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 271/1000
2023-10-12 04:50:11.601 
Epoch 271/1000 
	 loss: 28.8451, MinusLogProbMetric: 28.8451, val_loss: 30.2403, val_MinusLogProbMetric: 30.2403

Epoch 271: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.8451 - MinusLogProbMetric: 28.8451 - val_loss: 30.2403 - val_MinusLogProbMetric: 30.2403 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 272/1000
2023-10-12 04:50:37.434 
Epoch 272/1000 
	 loss: 28.9385, MinusLogProbMetric: 28.9385, val_loss: 29.8277, val_MinusLogProbMetric: 29.8277

Epoch 272: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.9385 - MinusLogProbMetric: 28.9385 - val_loss: 29.8277 - val_MinusLogProbMetric: 29.8277 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 273/1000
2023-10-12 04:51:03.458 
Epoch 273/1000 
	 loss: 28.7909, MinusLogProbMetric: 28.7909, val_loss: 29.7878, val_MinusLogProbMetric: 29.7878

Epoch 273: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.7909 - MinusLogProbMetric: 28.7909 - val_loss: 29.7878 - val_MinusLogProbMetric: 29.7878 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 274/1000
2023-10-12 04:51:29.782 
Epoch 274/1000 
	 loss: 28.8506, MinusLogProbMetric: 28.8506, val_loss: 29.5841, val_MinusLogProbMetric: 29.5841

Epoch 274: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.8506 - MinusLogProbMetric: 28.8506 - val_loss: 29.5841 - val_MinusLogProbMetric: 29.5841 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 275/1000
2023-10-12 04:51:55.913 
Epoch 275/1000 
	 loss: 28.8407, MinusLogProbMetric: 28.8407, val_loss: 29.8014, val_MinusLogProbMetric: 29.8014

Epoch 275: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.8407 - MinusLogProbMetric: 28.8407 - val_loss: 29.8014 - val_MinusLogProbMetric: 29.8014 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 276/1000
2023-10-12 04:52:21.791 
Epoch 276/1000 
	 loss: 28.9630, MinusLogProbMetric: 28.9630, val_loss: 29.3623, val_MinusLogProbMetric: 29.3623

Epoch 276: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.9630 - MinusLogProbMetric: 28.9630 - val_loss: 29.3623 - val_MinusLogProbMetric: 29.3623 - lr: 0.0010 - 26s/epoch - 132ms/step
Epoch 277/1000
2023-10-12 04:52:48.435 
Epoch 277/1000 
	 loss: 28.8908, MinusLogProbMetric: 28.8908, val_loss: 29.8391, val_MinusLogProbMetric: 29.8391

Epoch 277: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.8908 - MinusLogProbMetric: 28.8908 - val_loss: 29.8391 - val_MinusLogProbMetric: 29.8391 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 278/1000
2023-10-12 04:53:15.179 
Epoch 278/1000 
	 loss: 28.9123, MinusLogProbMetric: 28.9123, val_loss: 29.4385, val_MinusLogProbMetric: 29.4385

Epoch 278: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.9123 - MinusLogProbMetric: 28.9123 - val_loss: 29.4385 - val_MinusLogProbMetric: 29.4385 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 279/1000
2023-10-12 04:53:42.111 
Epoch 279/1000 
	 loss: 28.9428, MinusLogProbMetric: 28.9428, val_loss: 29.6829, val_MinusLogProbMetric: 29.6829

Epoch 279: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.9428 - MinusLogProbMetric: 28.9428 - val_loss: 29.6829 - val_MinusLogProbMetric: 29.6829 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 280/1000
2023-10-12 04:54:08.874 
Epoch 280/1000 
	 loss: 28.6946, MinusLogProbMetric: 28.6946, val_loss: 29.3546, val_MinusLogProbMetric: 29.3546

Epoch 280: val_loss did not improve from 29.15045
196/196 - 27s - loss: 28.6946 - MinusLogProbMetric: 28.6946 - val_loss: 29.3546 - val_MinusLogProbMetric: 29.3546 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 281/1000
2023-10-12 04:54:35.115 
Epoch 281/1000 
	 loss: 28.7164, MinusLogProbMetric: 28.7164, val_loss: 29.3164, val_MinusLogProbMetric: 29.3164

Epoch 281: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.7164 - MinusLogProbMetric: 28.7164 - val_loss: 29.3164 - val_MinusLogProbMetric: 29.3164 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 282/1000
2023-10-12 04:55:01.101 
Epoch 282/1000 
	 loss: 28.8992, MinusLogProbMetric: 28.8992, val_loss: 30.2767, val_MinusLogProbMetric: 30.2767

Epoch 282: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.8992 - MinusLogProbMetric: 28.8992 - val_loss: 30.2767 - val_MinusLogProbMetric: 30.2767 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 283/1000
2023-10-12 04:55:27.434 
Epoch 283/1000 
	 loss: 28.0617, MinusLogProbMetric: 28.0617, val_loss: 29.2607, val_MinusLogProbMetric: 29.2607

Epoch 283: val_loss did not improve from 29.15045
196/196 - 26s - loss: 28.0617 - MinusLogProbMetric: 28.0617 - val_loss: 29.2607 - val_MinusLogProbMetric: 29.2607 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 284/1000
2023-10-12 04:55:53.490 
Epoch 284/1000 
	 loss: 28.0621, MinusLogProbMetric: 28.0621, val_loss: 28.8962, val_MinusLogProbMetric: 28.8962

Epoch 284: val_loss improved from 29.15045 to 28.89617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 28.0621 - MinusLogProbMetric: 28.0621 - val_loss: 28.8962 - val_MinusLogProbMetric: 28.8962 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 285/1000
2023-10-12 04:56:20.495 
Epoch 285/1000 
	 loss: 28.0591, MinusLogProbMetric: 28.0591, val_loss: 29.2414, val_MinusLogProbMetric: 29.2414

Epoch 285: val_loss did not improve from 28.89617
196/196 - 27s - loss: 28.0591 - MinusLogProbMetric: 28.0591 - val_loss: 29.2414 - val_MinusLogProbMetric: 29.2414 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 286/1000
2023-10-12 04:56:46.302 
Epoch 286/1000 
	 loss: 28.1076, MinusLogProbMetric: 28.1076, val_loss: 29.9799, val_MinusLogProbMetric: 29.9799

Epoch 286: val_loss did not improve from 28.89617
196/196 - 26s - loss: 28.1076 - MinusLogProbMetric: 28.1076 - val_loss: 29.9799 - val_MinusLogProbMetric: 29.9799 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 287/1000
2023-10-12 04:57:12.199 
Epoch 287/1000 
	 loss: 28.0210, MinusLogProbMetric: 28.0210, val_loss: 28.7209, val_MinusLogProbMetric: 28.7209

Epoch 287: val_loss improved from 28.89617 to 28.72094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 28.0210 - MinusLogProbMetric: 28.0210 - val_loss: 28.7209 - val_MinusLogProbMetric: 28.7209 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 288/1000
2023-10-12 04:57:38.486 
Epoch 288/1000 
	 loss: 28.1090, MinusLogProbMetric: 28.1090, val_loss: 29.7260, val_MinusLogProbMetric: 29.7260

Epoch 288: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.1090 - MinusLogProbMetric: 28.1090 - val_loss: 29.7260 - val_MinusLogProbMetric: 29.7260 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 289/1000
2023-10-12 04:58:04.290 
Epoch 289/1000 
	 loss: 28.1053, MinusLogProbMetric: 28.1053, val_loss: 29.2100, val_MinusLogProbMetric: 29.2100

Epoch 289: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.1053 - MinusLogProbMetric: 28.1053 - val_loss: 29.2100 - val_MinusLogProbMetric: 29.2100 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 290/1000
2023-10-12 04:58:30.698 
Epoch 290/1000 
	 loss: 28.0313, MinusLogProbMetric: 28.0313, val_loss: 28.7820, val_MinusLogProbMetric: 28.7820

Epoch 290: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.0313 - MinusLogProbMetric: 28.0313 - val_loss: 28.7820 - val_MinusLogProbMetric: 28.7820 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 291/1000
2023-10-12 04:58:56.979 
Epoch 291/1000 
	 loss: 28.0683, MinusLogProbMetric: 28.0683, val_loss: 28.7960, val_MinusLogProbMetric: 28.7960

Epoch 291: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.0683 - MinusLogProbMetric: 28.0683 - val_loss: 28.7960 - val_MinusLogProbMetric: 28.7960 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 292/1000
2023-10-12 04:59:22.374 
Epoch 292/1000 
	 loss: 28.0275, MinusLogProbMetric: 28.0275, val_loss: 28.8820, val_MinusLogProbMetric: 28.8820

Epoch 292: val_loss did not improve from 28.72094
196/196 - 25s - loss: 28.0275 - MinusLogProbMetric: 28.0275 - val_loss: 28.8820 - val_MinusLogProbMetric: 28.8820 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 293/1000
2023-10-12 04:59:47.814 
Epoch 293/1000 
	 loss: 28.0539, MinusLogProbMetric: 28.0539, val_loss: 29.0056, val_MinusLogProbMetric: 29.0056

Epoch 293: val_loss did not improve from 28.72094
196/196 - 25s - loss: 28.0539 - MinusLogProbMetric: 28.0539 - val_loss: 29.0056 - val_MinusLogProbMetric: 29.0056 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 294/1000
2023-10-12 05:00:13.733 
Epoch 294/1000 
	 loss: 27.9885, MinusLogProbMetric: 27.9885, val_loss: 28.8775, val_MinusLogProbMetric: 28.8775

Epoch 294: val_loss did not improve from 28.72094
196/196 - 26s - loss: 27.9885 - MinusLogProbMetric: 27.9885 - val_loss: 28.8775 - val_MinusLogProbMetric: 28.8775 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 295/1000
2023-10-12 05:00:39.846 
Epoch 295/1000 
	 loss: 28.1518, MinusLogProbMetric: 28.1518, val_loss: 29.1126, val_MinusLogProbMetric: 29.1126

Epoch 295: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.1518 - MinusLogProbMetric: 28.1518 - val_loss: 29.1126 - val_MinusLogProbMetric: 29.1126 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 296/1000
2023-10-12 05:01:05.956 
Epoch 296/1000 
	 loss: 28.1388, MinusLogProbMetric: 28.1388, val_loss: 29.4191, val_MinusLogProbMetric: 29.4191

Epoch 296: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.1388 - MinusLogProbMetric: 28.1388 - val_loss: 29.4191 - val_MinusLogProbMetric: 29.4191 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 297/1000
2023-10-12 05:01:31.945 
Epoch 297/1000 
	 loss: 28.0211, MinusLogProbMetric: 28.0211, val_loss: 29.0966, val_MinusLogProbMetric: 29.0966

Epoch 297: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.0211 - MinusLogProbMetric: 28.0211 - val_loss: 29.0966 - val_MinusLogProbMetric: 29.0966 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 298/1000
2023-10-12 05:01:58.343 
Epoch 298/1000 
	 loss: 28.0777, MinusLogProbMetric: 28.0777, val_loss: 29.6028, val_MinusLogProbMetric: 29.6028

Epoch 298: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.0777 - MinusLogProbMetric: 28.0777 - val_loss: 29.6028 - val_MinusLogProbMetric: 29.6028 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 299/1000
2023-10-12 05:02:25.113 
Epoch 299/1000 
	 loss: 28.1393, MinusLogProbMetric: 28.1393, val_loss: 28.8513, val_MinusLogProbMetric: 28.8513

Epoch 299: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.1393 - MinusLogProbMetric: 28.1393 - val_loss: 28.8513 - val_MinusLogProbMetric: 28.8513 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 300/1000
2023-10-12 05:02:52.422 
Epoch 300/1000 
	 loss: 28.0052, MinusLogProbMetric: 28.0052, val_loss: 28.9024, val_MinusLogProbMetric: 28.9024

Epoch 300: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0052 - MinusLogProbMetric: 28.0052 - val_loss: 28.9024 - val_MinusLogProbMetric: 28.9024 - lr: 5.0000e-04 - 27s/epoch - 139ms/step
Epoch 301/1000
2023-10-12 05:03:20.145 
Epoch 301/1000 
	 loss: 28.0905, MinusLogProbMetric: 28.0905, val_loss: 29.1048, val_MinusLogProbMetric: 29.1048

Epoch 301: val_loss did not improve from 28.72094
196/196 - 28s - loss: 28.0905 - MinusLogProbMetric: 28.0905 - val_loss: 29.1048 - val_MinusLogProbMetric: 29.1048 - lr: 5.0000e-04 - 28s/epoch - 141ms/step
Epoch 302/1000
2023-10-12 05:03:46.867 
Epoch 302/1000 
	 loss: 28.0911, MinusLogProbMetric: 28.0911, val_loss: 28.9362, val_MinusLogProbMetric: 28.9362

Epoch 302: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0911 - MinusLogProbMetric: 28.0911 - val_loss: 28.9362 - val_MinusLogProbMetric: 28.9362 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 303/1000
2023-10-12 05:04:13.811 
Epoch 303/1000 
	 loss: 28.0854, MinusLogProbMetric: 28.0854, val_loss: 29.0498, val_MinusLogProbMetric: 29.0498

Epoch 303: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0854 - MinusLogProbMetric: 28.0854 - val_loss: 29.0498 - val_MinusLogProbMetric: 29.0498 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 304/1000
2023-10-12 05:04:40.829 
Epoch 304/1000 
	 loss: 28.0170, MinusLogProbMetric: 28.0170, val_loss: 28.9503, val_MinusLogProbMetric: 28.9503

Epoch 304: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0170 - MinusLogProbMetric: 28.0170 - val_loss: 28.9503 - val_MinusLogProbMetric: 28.9503 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 305/1000
2023-10-12 05:05:07.893 
Epoch 305/1000 
	 loss: 28.0266, MinusLogProbMetric: 28.0266, val_loss: 29.0977, val_MinusLogProbMetric: 29.0977

Epoch 305: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0266 - MinusLogProbMetric: 28.0266 - val_loss: 29.0977 - val_MinusLogProbMetric: 29.0977 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 306/1000
2023-10-12 05:05:34.825 
Epoch 306/1000 
	 loss: 28.0213, MinusLogProbMetric: 28.0213, val_loss: 29.8313, val_MinusLogProbMetric: 29.8313

Epoch 306: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0213 - MinusLogProbMetric: 28.0213 - val_loss: 29.8313 - val_MinusLogProbMetric: 29.8313 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 307/1000
2023-10-12 05:06:01.532 
Epoch 307/1000 
	 loss: 28.1790, MinusLogProbMetric: 28.1790, val_loss: 28.9526, val_MinusLogProbMetric: 28.9526

Epoch 307: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.1790 - MinusLogProbMetric: 28.1790 - val_loss: 28.9526 - val_MinusLogProbMetric: 28.9526 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 308/1000
2023-10-12 05:06:28.574 
Epoch 308/1000 
	 loss: 28.0137, MinusLogProbMetric: 28.0137, val_loss: 28.8229, val_MinusLogProbMetric: 28.8229

Epoch 308: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0137 - MinusLogProbMetric: 28.0137 - val_loss: 28.8229 - val_MinusLogProbMetric: 28.8229 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 309/1000
2023-10-12 05:06:54.981 
Epoch 309/1000 
	 loss: 28.1040, MinusLogProbMetric: 28.1040, val_loss: 28.9105, val_MinusLogProbMetric: 28.9105

Epoch 309: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.1040 - MinusLogProbMetric: 28.1040 - val_loss: 28.9105 - val_MinusLogProbMetric: 28.9105 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 310/1000
2023-10-12 05:07:23.433 
Epoch 310/1000 
	 loss: 28.0941, MinusLogProbMetric: 28.0941, val_loss: 29.1156, val_MinusLogProbMetric: 29.1156

Epoch 310: val_loss did not improve from 28.72094
196/196 - 28s - loss: 28.0941 - MinusLogProbMetric: 28.0941 - val_loss: 29.1156 - val_MinusLogProbMetric: 29.1156 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 311/1000
2023-10-12 05:07:51.804 
Epoch 311/1000 
	 loss: 28.1976, MinusLogProbMetric: 28.1976, val_loss: 28.8958, val_MinusLogProbMetric: 28.8958

Epoch 311: val_loss did not improve from 28.72094
196/196 - 28s - loss: 28.1976 - MinusLogProbMetric: 28.1976 - val_loss: 28.8958 - val_MinusLogProbMetric: 28.8958 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 312/1000
2023-10-12 05:08:18.151 
Epoch 312/1000 
	 loss: 27.9780, MinusLogProbMetric: 27.9780, val_loss: 28.9457, val_MinusLogProbMetric: 28.9457

Epoch 312: val_loss did not improve from 28.72094
196/196 - 26s - loss: 27.9780 - MinusLogProbMetric: 27.9780 - val_loss: 28.9457 - val_MinusLogProbMetric: 28.9457 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 313/1000
2023-10-12 05:08:44.668 
Epoch 313/1000 
	 loss: 28.0460, MinusLogProbMetric: 28.0460, val_loss: 30.2051, val_MinusLogProbMetric: 30.2051

Epoch 313: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0460 - MinusLogProbMetric: 28.0460 - val_loss: 30.2051 - val_MinusLogProbMetric: 30.2051 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 314/1000
2023-10-12 05:09:10.351 
Epoch 314/1000 
	 loss: 28.0643, MinusLogProbMetric: 28.0643, val_loss: 29.3300, val_MinusLogProbMetric: 29.3300

Epoch 314: val_loss did not improve from 28.72094
196/196 - 26s - loss: 28.0643 - MinusLogProbMetric: 28.0643 - val_loss: 29.3300 - val_MinusLogProbMetric: 29.3300 - lr: 5.0000e-04 - 26s/epoch - 131ms/step
Epoch 315/1000
2023-10-12 05:09:36.455 
Epoch 315/1000 
	 loss: 27.9959, MinusLogProbMetric: 27.9959, val_loss: 28.9082, val_MinusLogProbMetric: 28.9082

Epoch 315: val_loss did not improve from 28.72094
196/196 - 26s - loss: 27.9959 - MinusLogProbMetric: 27.9959 - val_loss: 28.9082 - val_MinusLogProbMetric: 28.9082 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 316/1000
2023-10-12 05:10:04.061 
Epoch 316/1000 
	 loss: 27.9625, MinusLogProbMetric: 27.9625, val_loss: 29.0132, val_MinusLogProbMetric: 29.0132

Epoch 316: val_loss did not improve from 28.72094
196/196 - 28s - loss: 27.9625 - MinusLogProbMetric: 27.9625 - val_loss: 29.0132 - val_MinusLogProbMetric: 29.0132 - lr: 5.0000e-04 - 28s/epoch - 141ms/step
Epoch 317/1000
2023-10-12 05:10:30.915 
Epoch 317/1000 
	 loss: 28.0772, MinusLogProbMetric: 28.0772, val_loss: 28.8531, val_MinusLogProbMetric: 28.8531

Epoch 317: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0772 - MinusLogProbMetric: 28.0772 - val_loss: 28.8531 - val_MinusLogProbMetric: 28.8531 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 318/1000
2023-10-12 05:10:58.290 
Epoch 318/1000 
	 loss: 28.1170, MinusLogProbMetric: 28.1170, val_loss: 28.8094, val_MinusLogProbMetric: 28.8094

Epoch 318: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.1170 - MinusLogProbMetric: 28.1170 - val_loss: 28.8094 - val_MinusLogProbMetric: 28.8094 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 319/1000
2023-10-12 05:11:25.790 
Epoch 319/1000 
	 loss: 28.0688, MinusLogProbMetric: 28.0688, val_loss: 29.0692, val_MinusLogProbMetric: 29.0692

Epoch 319: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0688 - MinusLogProbMetric: 28.0688 - val_loss: 29.0692 - val_MinusLogProbMetric: 29.0692 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 320/1000
2023-10-12 05:11:52.764 
Epoch 320/1000 
	 loss: 27.9323, MinusLogProbMetric: 27.9323, val_loss: 29.6330, val_MinusLogProbMetric: 29.6330

Epoch 320: val_loss did not improve from 28.72094
196/196 - 27s - loss: 27.9323 - MinusLogProbMetric: 27.9323 - val_loss: 29.6330 - val_MinusLogProbMetric: 29.6330 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 321/1000
2023-10-12 05:12:20.984 
Epoch 321/1000 
	 loss: 28.0782, MinusLogProbMetric: 28.0782, val_loss: 28.9558, val_MinusLogProbMetric: 28.9558

Epoch 321: val_loss did not improve from 28.72094
196/196 - 28s - loss: 28.0782 - MinusLogProbMetric: 28.0782 - val_loss: 28.9558 - val_MinusLogProbMetric: 28.9558 - lr: 5.0000e-04 - 28s/epoch - 144ms/step
Epoch 322/1000
2023-10-12 05:12:47.975 
Epoch 322/1000 
	 loss: 28.0220, MinusLogProbMetric: 28.0220, val_loss: 28.9435, val_MinusLogProbMetric: 28.9435

Epoch 322: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0220 - MinusLogProbMetric: 28.0220 - val_loss: 28.9435 - val_MinusLogProbMetric: 28.9435 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 323/1000
2023-10-12 05:13:14.487 
Epoch 323/1000 
	 loss: 28.0158, MinusLogProbMetric: 28.0158, val_loss: 29.0548, val_MinusLogProbMetric: 29.0548

Epoch 323: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0158 - MinusLogProbMetric: 28.0158 - val_loss: 29.0548 - val_MinusLogProbMetric: 29.0548 - lr: 5.0000e-04 - 27s/epoch - 135ms/step
Epoch 324/1000
2023-10-12 05:13:41.082 
Epoch 324/1000 
	 loss: 27.9659, MinusLogProbMetric: 27.9659, val_loss: 30.0549, val_MinusLogProbMetric: 30.0549

Epoch 324: val_loss did not improve from 28.72094
196/196 - 27s - loss: 27.9659 - MinusLogProbMetric: 27.9659 - val_loss: 30.0549 - val_MinusLogProbMetric: 30.0549 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 325/1000
2023-10-12 05:14:08.039 
Epoch 325/1000 
	 loss: 28.1670, MinusLogProbMetric: 28.1670, val_loss: 29.4554, val_MinusLogProbMetric: 29.4554

Epoch 325: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.1670 - MinusLogProbMetric: 28.1670 - val_loss: 29.4554 - val_MinusLogProbMetric: 29.4554 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 326/1000
2023-10-12 05:14:34.747 
Epoch 326/1000 
	 loss: 28.2020, MinusLogProbMetric: 28.2020, val_loss: 29.1305, val_MinusLogProbMetric: 29.1305

Epoch 326: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.2020 - MinusLogProbMetric: 28.2020 - val_loss: 29.1305 - val_MinusLogProbMetric: 29.1305 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 327/1000
2023-10-12 05:15:01.635 
Epoch 327/1000 
	 loss: 27.9875, MinusLogProbMetric: 27.9875, val_loss: 29.0280, val_MinusLogProbMetric: 29.0280

Epoch 327: val_loss did not improve from 28.72094
196/196 - 27s - loss: 27.9875 - MinusLogProbMetric: 27.9875 - val_loss: 29.0280 - val_MinusLogProbMetric: 29.0280 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 328/1000
2023-10-12 05:15:29.996 
Epoch 328/1000 
	 loss: 28.0962, MinusLogProbMetric: 28.0962, val_loss: 29.1239, val_MinusLogProbMetric: 29.1239

Epoch 328: val_loss did not improve from 28.72094
196/196 - 28s - loss: 28.0962 - MinusLogProbMetric: 28.0962 - val_loss: 29.1239 - val_MinusLogProbMetric: 29.1239 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 329/1000
2023-10-12 05:15:57.743 
Epoch 329/1000 
	 loss: 28.1139, MinusLogProbMetric: 28.1139, val_loss: 28.9199, val_MinusLogProbMetric: 28.9199

Epoch 329: val_loss did not improve from 28.72094
196/196 - 28s - loss: 28.1139 - MinusLogProbMetric: 28.1139 - val_loss: 28.9199 - val_MinusLogProbMetric: 28.9199 - lr: 5.0000e-04 - 28s/epoch - 142ms/step
Epoch 330/1000
2023-10-12 05:16:24.675 
Epoch 330/1000 
	 loss: 27.9262, MinusLogProbMetric: 27.9262, val_loss: 29.0540, val_MinusLogProbMetric: 29.0540

Epoch 330: val_loss did not improve from 28.72094
196/196 - 27s - loss: 27.9262 - MinusLogProbMetric: 27.9262 - val_loss: 29.0540 - val_MinusLogProbMetric: 29.0540 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 331/1000
2023-10-12 05:16:51.795 
Epoch 331/1000 
	 loss: 28.1009, MinusLogProbMetric: 28.1009, val_loss: 29.0916, val_MinusLogProbMetric: 29.0916

Epoch 331: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.1009 - MinusLogProbMetric: 28.1009 - val_loss: 29.0916 - val_MinusLogProbMetric: 29.0916 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 332/1000
2023-10-12 05:17:18.866 
Epoch 332/1000 
	 loss: 28.0133, MinusLogProbMetric: 28.0133, val_loss: 28.9014, val_MinusLogProbMetric: 28.9014

Epoch 332: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0133 - MinusLogProbMetric: 28.0133 - val_loss: 28.9014 - val_MinusLogProbMetric: 28.9014 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 333/1000
2023-10-12 05:17:46.170 
Epoch 333/1000 
	 loss: 27.9303, MinusLogProbMetric: 27.9303, val_loss: 28.7768, val_MinusLogProbMetric: 28.7768

Epoch 333: val_loss did not improve from 28.72094
196/196 - 27s - loss: 27.9303 - MinusLogProbMetric: 27.9303 - val_loss: 28.7768 - val_MinusLogProbMetric: 28.7768 - lr: 5.0000e-04 - 27s/epoch - 139ms/step
Epoch 334/1000
2023-10-12 05:18:12.917 
Epoch 334/1000 
	 loss: 28.0633, MinusLogProbMetric: 28.0633, val_loss: 29.1578, val_MinusLogProbMetric: 29.1578

Epoch 334: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0633 - MinusLogProbMetric: 28.0633 - val_loss: 29.1578 - val_MinusLogProbMetric: 29.1578 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 335/1000
2023-10-12 05:18:39.928 
Epoch 335/1000 
	 loss: 28.1918, MinusLogProbMetric: 28.1918, val_loss: 29.5138, val_MinusLogProbMetric: 29.5138

Epoch 335: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.1918 - MinusLogProbMetric: 28.1918 - val_loss: 29.5138 - val_MinusLogProbMetric: 29.5138 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 336/1000
2023-10-12 05:19:06.651 
Epoch 336/1000 
	 loss: 28.0025, MinusLogProbMetric: 28.0025, val_loss: 28.8277, val_MinusLogProbMetric: 28.8277

Epoch 336: val_loss did not improve from 28.72094
196/196 - 27s - loss: 28.0025 - MinusLogProbMetric: 28.0025 - val_loss: 28.8277 - val_MinusLogProbMetric: 28.8277 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 337/1000
2023-10-12 05:19:32.127 
Epoch 337/1000 
	 loss: 28.4616, MinusLogProbMetric: 28.4616, val_loss: 29.2504, val_MinusLogProbMetric: 29.2504

Epoch 337: val_loss did not improve from 28.72094
196/196 - 25s - loss: 28.4616 - MinusLogProbMetric: 28.4616 - val_loss: 29.2504 - val_MinusLogProbMetric: 29.2504 - lr: 5.0000e-04 - 25s/epoch - 130ms/step
Epoch 338/1000
2023-10-12 05:19:58.056 
Epoch 338/1000 
	 loss: 27.6493, MinusLogProbMetric: 27.6493, val_loss: 28.7365, val_MinusLogProbMetric: 28.7365

Epoch 338: val_loss did not improve from 28.72094
196/196 - 26s - loss: 27.6493 - MinusLogProbMetric: 27.6493 - val_loss: 28.7365 - val_MinusLogProbMetric: 28.7365 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 339/1000
2023-10-12 05:20:24.669 
Epoch 339/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 28.6477, val_MinusLogProbMetric: 28.6477

Epoch 339: val_loss improved from 28.72094 to 28.64773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 28.6477 - val_MinusLogProbMetric: 28.6477 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 340/1000
2023-10-12 05:20:51.856 
Epoch 340/1000 
	 loss: 27.5781, MinusLogProbMetric: 27.5781, val_loss: 28.7970, val_MinusLogProbMetric: 28.7970

Epoch 340: val_loss did not improve from 28.64773
196/196 - 27s - loss: 27.5781 - MinusLogProbMetric: 27.5781 - val_loss: 28.7970 - val_MinusLogProbMetric: 28.7970 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 341/1000
2023-10-12 05:21:20.288 
Epoch 341/1000 
	 loss: 27.6204, MinusLogProbMetric: 27.6204, val_loss: 28.6219, val_MinusLogProbMetric: 28.6219

Epoch 341: val_loss improved from 28.64773 to 28.62193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 29s - loss: 27.6204 - MinusLogProbMetric: 27.6204 - val_loss: 28.6219 - val_MinusLogProbMetric: 28.6219 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 342/1000
2023-10-12 05:21:48.535 
Epoch 342/1000 
	 loss: 27.5745, MinusLogProbMetric: 27.5745, val_loss: 28.6204, val_MinusLogProbMetric: 28.6204

Epoch 342: val_loss improved from 28.62193 to 28.62041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 27.5745 - MinusLogProbMetric: 27.5745 - val_loss: 28.6204 - val_MinusLogProbMetric: 28.6204 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 343/1000
2023-10-12 05:22:16.132 
Epoch 343/1000 
	 loss: 27.6245, MinusLogProbMetric: 27.6245, val_loss: 28.7604, val_MinusLogProbMetric: 28.7604

Epoch 343: val_loss did not improve from 28.62041
196/196 - 27s - loss: 27.6245 - MinusLogProbMetric: 27.6245 - val_loss: 28.7604 - val_MinusLogProbMetric: 28.7604 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 344/1000
2023-10-12 05:22:42.820 
Epoch 344/1000 
	 loss: 27.6008, MinusLogProbMetric: 27.6008, val_loss: 28.6772, val_MinusLogProbMetric: 28.6772

Epoch 344: val_loss did not improve from 28.62041
196/196 - 27s - loss: 27.6008 - MinusLogProbMetric: 27.6008 - val_loss: 28.6772 - val_MinusLogProbMetric: 28.6772 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 345/1000
2023-10-12 05:23:08.388 
Epoch 345/1000 
	 loss: 27.5995, MinusLogProbMetric: 27.5995, val_loss: 28.6023, val_MinusLogProbMetric: 28.6023

Epoch 345: val_loss improved from 28.62041 to 28.60233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 27.5995 - MinusLogProbMetric: 27.5995 - val_loss: 28.6023 - val_MinusLogProbMetric: 28.6023 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 346/1000
2023-10-12 05:23:34.984 
Epoch 346/1000 
	 loss: 27.6461, MinusLogProbMetric: 27.6461, val_loss: 28.5982, val_MinusLogProbMetric: 28.5982

Epoch 346: val_loss improved from 28.60233 to 28.59817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.6461 - MinusLogProbMetric: 27.6461 - val_loss: 28.5982 - val_MinusLogProbMetric: 28.5982 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 347/1000
2023-10-12 05:24:01.938 
Epoch 347/1000 
	 loss: 27.5777, MinusLogProbMetric: 27.5777, val_loss: 28.6572, val_MinusLogProbMetric: 28.6572

Epoch 347: val_loss did not improve from 28.59817
196/196 - 26s - loss: 27.5777 - MinusLogProbMetric: 27.5777 - val_loss: 28.6572 - val_MinusLogProbMetric: 28.6572 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 348/1000
2023-10-12 05:24:27.915 
Epoch 348/1000 
	 loss: 27.6884, MinusLogProbMetric: 27.6884, val_loss: 28.7690, val_MinusLogProbMetric: 28.7690

Epoch 348: val_loss did not improve from 28.59817
196/196 - 26s - loss: 27.6884 - MinusLogProbMetric: 27.6884 - val_loss: 28.7690 - val_MinusLogProbMetric: 28.7690 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 349/1000
2023-10-12 05:24:53.861 
Epoch 349/1000 
	 loss: 27.6405, MinusLogProbMetric: 27.6405, val_loss: 28.7434, val_MinusLogProbMetric: 28.7434

Epoch 349: val_loss did not improve from 28.59817
196/196 - 26s - loss: 27.6405 - MinusLogProbMetric: 27.6405 - val_loss: 28.7434 - val_MinusLogProbMetric: 28.7434 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 350/1000
2023-10-12 05:25:19.771 
Epoch 350/1000 
	 loss: 27.6130, MinusLogProbMetric: 27.6130, val_loss: 28.6329, val_MinusLogProbMetric: 28.6329

Epoch 350: val_loss did not improve from 28.59817
196/196 - 26s - loss: 27.6130 - MinusLogProbMetric: 27.6130 - val_loss: 28.6329 - val_MinusLogProbMetric: 28.6329 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 351/1000
2023-10-12 05:25:45.710 
Epoch 351/1000 
	 loss: 27.5884, MinusLogProbMetric: 27.5884, val_loss: 29.4259, val_MinusLogProbMetric: 29.4259

Epoch 351: val_loss did not improve from 28.59817
196/196 - 26s - loss: 27.5884 - MinusLogProbMetric: 27.5884 - val_loss: 29.4259 - val_MinusLogProbMetric: 29.4259 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 352/1000
2023-10-12 05:26:12.295 
Epoch 352/1000 
	 loss: 27.6028, MinusLogProbMetric: 27.6028, val_loss: 28.5917, val_MinusLogProbMetric: 28.5917

Epoch 352: val_loss improved from 28.59817 to 28.59175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.6028 - MinusLogProbMetric: 27.6028 - val_loss: 28.5917 - val_MinusLogProbMetric: 28.5917 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 353/1000
2023-10-12 05:26:38.634 
Epoch 353/1000 
	 loss: 27.6274, MinusLogProbMetric: 27.6274, val_loss: 29.1327, val_MinusLogProbMetric: 29.1327

Epoch 353: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.6274 - MinusLogProbMetric: 27.6274 - val_loss: 29.1327 - val_MinusLogProbMetric: 29.1327 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 354/1000
2023-10-12 05:27:04.340 
Epoch 354/1000 
	 loss: 27.5834, MinusLogProbMetric: 27.5834, val_loss: 28.8137, val_MinusLogProbMetric: 28.8137

Epoch 354: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.5834 - MinusLogProbMetric: 27.5834 - val_loss: 28.8137 - val_MinusLogProbMetric: 28.8137 - lr: 2.5000e-04 - 26s/epoch - 131ms/step
Epoch 355/1000
2023-10-12 05:27:30.486 
Epoch 355/1000 
	 loss: 27.5895, MinusLogProbMetric: 27.5895, val_loss: 28.8629, val_MinusLogProbMetric: 28.8629

Epoch 355: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.5895 - MinusLogProbMetric: 27.5895 - val_loss: 28.8629 - val_MinusLogProbMetric: 28.8629 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 356/1000
2023-10-12 05:27:56.297 
Epoch 356/1000 
	 loss: 27.6023, MinusLogProbMetric: 27.6023, val_loss: 28.7544, val_MinusLogProbMetric: 28.7544

Epoch 356: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.6023 - MinusLogProbMetric: 27.6023 - val_loss: 28.7544 - val_MinusLogProbMetric: 28.7544 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 357/1000
2023-10-12 05:28:22.037 
Epoch 357/1000 
	 loss: 27.5748, MinusLogProbMetric: 27.5748, val_loss: 29.2710, val_MinusLogProbMetric: 29.2710

Epoch 357: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.5748 - MinusLogProbMetric: 27.5748 - val_loss: 29.2710 - val_MinusLogProbMetric: 29.2710 - lr: 2.5000e-04 - 26s/epoch - 131ms/step
Epoch 358/1000
2023-10-12 05:28:47.517 
Epoch 358/1000 
	 loss: 27.8993, MinusLogProbMetric: 27.8993, val_loss: 28.7218, val_MinusLogProbMetric: 28.7218

Epoch 358: val_loss did not improve from 28.59175
196/196 - 25s - loss: 27.8993 - MinusLogProbMetric: 27.8993 - val_loss: 28.7218 - val_MinusLogProbMetric: 28.7218 - lr: 2.5000e-04 - 25s/epoch - 130ms/step
Epoch 359/1000
2023-10-12 05:29:15.415 
Epoch 359/1000 
	 loss: 27.5631, MinusLogProbMetric: 27.5631, val_loss: 28.8700, val_MinusLogProbMetric: 28.8700

Epoch 359: val_loss did not improve from 28.59175
196/196 - 28s - loss: 27.5631 - MinusLogProbMetric: 27.5631 - val_loss: 28.8700 - val_MinusLogProbMetric: 28.8700 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 360/1000
2023-10-12 05:29:42.066 
Epoch 360/1000 
	 loss: 27.5596, MinusLogProbMetric: 27.5596, val_loss: 28.7584, val_MinusLogProbMetric: 28.7584

Epoch 360: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5596 - MinusLogProbMetric: 27.5596 - val_loss: 28.7584 - val_MinusLogProbMetric: 28.7584 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 361/1000
2023-10-12 05:30:08.296 
Epoch 361/1000 
	 loss: 27.6089, MinusLogProbMetric: 27.6089, val_loss: 28.9363, val_MinusLogProbMetric: 28.9363

Epoch 361: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.6089 - MinusLogProbMetric: 27.6089 - val_loss: 28.9363 - val_MinusLogProbMetric: 28.9363 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 362/1000
2023-10-12 05:30:34.906 
Epoch 362/1000 
	 loss: 27.5481, MinusLogProbMetric: 27.5481, val_loss: 28.6074, val_MinusLogProbMetric: 28.6074

Epoch 362: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5481 - MinusLogProbMetric: 27.5481 - val_loss: 28.6074 - val_MinusLogProbMetric: 28.6074 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 363/1000
2023-10-12 05:31:01.913 
Epoch 363/1000 
	 loss: 27.6251, MinusLogProbMetric: 27.6251, val_loss: 28.6704, val_MinusLogProbMetric: 28.6704

Epoch 363: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.6251 - MinusLogProbMetric: 27.6251 - val_loss: 28.6704 - val_MinusLogProbMetric: 28.6704 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 364/1000
2023-10-12 05:31:28.616 
Epoch 364/1000 
	 loss: 27.5838, MinusLogProbMetric: 27.5838, val_loss: 28.6038, val_MinusLogProbMetric: 28.6038

Epoch 364: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5838 - MinusLogProbMetric: 27.5838 - val_loss: 28.6038 - val_MinusLogProbMetric: 28.6038 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 365/1000
2023-10-12 05:31:55.930 
Epoch 365/1000 
	 loss: 27.5762, MinusLogProbMetric: 27.5762, val_loss: 28.9689, val_MinusLogProbMetric: 28.9689

Epoch 365: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5762 - MinusLogProbMetric: 27.5762 - val_loss: 28.9689 - val_MinusLogProbMetric: 28.9689 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 366/1000
2023-10-12 05:32:23.325 
Epoch 366/1000 
	 loss: 27.5877, MinusLogProbMetric: 27.5877, val_loss: 28.9299, val_MinusLogProbMetric: 28.9299

Epoch 366: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5877 - MinusLogProbMetric: 27.5877 - val_loss: 28.9299 - val_MinusLogProbMetric: 28.9299 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 367/1000
2023-10-12 05:32:50.379 
Epoch 367/1000 
	 loss: 27.5918, MinusLogProbMetric: 27.5918, val_loss: 28.7501, val_MinusLogProbMetric: 28.7501

Epoch 367: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5918 - MinusLogProbMetric: 27.5918 - val_loss: 28.7501 - val_MinusLogProbMetric: 28.7501 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 368/1000
2023-10-12 05:33:17.851 
Epoch 368/1000 
	 loss: 27.7782, MinusLogProbMetric: 27.7782, val_loss: 28.6424, val_MinusLogProbMetric: 28.6424

Epoch 368: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.7782 - MinusLogProbMetric: 27.7782 - val_loss: 28.6424 - val_MinusLogProbMetric: 28.6424 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 369/1000
2023-10-12 05:33:45.698 
Epoch 369/1000 
	 loss: 27.5999, MinusLogProbMetric: 27.5999, val_loss: 28.7477, val_MinusLogProbMetric: 28.7477

Epoch 369: val_loss did not improve from 28.59175
196/196 - 28s - loss: 27.5999 - MinusLogProbMetric: 27.5999 - val_loss: 28.7477 - val_MinusLogProbMetric: 28.7477 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 370/1000
2023-10-12 05:34:12.364 
Epoch 370/1000 
	 loss: 27.5725, MinusLogProbMetric: 27.5725, val_loss: 28.9018, val_MinusLogProbMetric: 28.9018

Epoch 370: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5725 - MinusLogProbMetric: 27.5725 - val_loss: 28.9018 - val_MinusLogProbMetric: 28.9018 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 371/1000
2023-10-12 05:34:39.064 
Epoch 371/1000 
	 loss: 27.6081, MinusLogProbMetric: 27.6081, val_loss: 28.6000, val_MinusLogProbMetric: 28.6000

Epoch 371: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.6081 - MinusLogProbMetric: 27.6081 - val_loss: 28.6000 - val_MinusLogProbMetric: 28.6000 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 372/1000
2023-10-12 05:35:05.910 
Epoch 372/1000 
	 loss: 27.5658, MinusLogProbMetric: 27.5658, val_loss: 28.8047, val_MinusLogProbMetric: 28.8047

Epoch 372: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5658 - MinusLogProbMetric: 27.5658 - val_loss: 28.8047 - val_MinusLogProbMetric: 28.8047 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 373/1000
2023-10-12 05:35:31.977 
Epoch 373/1000 
	 loss: 27.5639, MinusLogProbMetric: 27.5639, val_loss: 28.6002, val_MinusLogProbMetric: 28.6002

Epoch 373: val_loss did not improve from 28.59175
196/196 - 26s - loss: 27.5639 - MinusLogProbMetric: 27.5639 - val_loss: 28.6002 - val_MinusLogProbMetric: 28.6002 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 374/1000
2023-10-12 05:35:59.275 
Epoch 374/1000 
	 loss: 27.5720, MinusLogProbMetric: 27.5720, val_loss: 29.5603, val_MinusLogProbMetric: 29.5603

Epoch 374: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5720 - MinusLogProbMetric: 27.5720 - val_loss: 29.5603 - val_MinusLogProbMetric: 29.5603 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 375/1000
2023-10-12 05:36:26.029 
Epoch 375/1000 
	 loss: 27.6282, MinusLogProbMetric: 27.6282, val_loss: 28.7753, val_MinusLogProbMetric: 28.7753

Epoch 375: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.6282 - MinusLogProbMetric: 27.6282 - val_loss: 28.7753 - val_MinusLogProbMetric: 28.7753 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 376/1000
2023-10-12 05:36:53.923 
Epoch 376/1000 
	 loss: 27.5594, MinusLogProbMetric: 27.5594, val_loss: 28.6154, val_MinusLogProbMetric: 28.6154

Epoch 376: val_loss did not improve from 28.59175
196/196 - 28s - loss: 27.5594 - MinusLogProbMetric: 27.5594 - val_loss: 28.6154 - val_MinusLogProbMetric: 28.6154 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 377/1000
2023-10-12 05:37:21.269 
Epoch 377/1000 
	 loss: 27.5606, MinusLogProbMetric: 27.5606, val_loss: 28.6536, val_MinusLogProbMetric: 28.6536

Epoch 377: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.5606 - MinusLogProbMetric: 27.5606 - val_loss: 28.6536 - val_MinusLogProbMetric: 28.6536 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 378/1000
2023-10-12 05:37:48.462 
Epoch 378/1000 
	 loss: 27.6128, MinusLogProbMetric: 27.6128, val_loss: 28.6368, val_MinusLogProbMetric: 28.6368

Epoch 378: val_loss did not improve from 28.59175
196/196 - 27s - loss: 27.6128 - MinusLogProbMetric: 27.6128 - val_loss: 28.6368 - val_MinusLogProbMetric: 28.6368 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 379/1000
2023-10-12 05:38:15.600 
Epoch 379/1000 
	 loss: 27.5530, MinusLogProbMetric: 27.5530, val_loss: 28.5682, val_MinusLogProbMetric: 28.5682

Epoch 379: val_loss improved from 28.59175 to 28.56817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 27.5530 - MinusLogProbMetric: 27.5530 - val_loss: 28.5682 - val_MinusLogProbMetric: 28.5682 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 380/1000
2023-10-12 05:38:43.352 
Epoch 380/1000 
	 loss: 27.6038, MinusLogProbMetric: 27.6038, val_loss: 28.6087, val_MinusLogProbMetric: 28.6087

Epoch 380: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.6038 - MinusLogProbMetric: 27.6038 - val_loss: 28.6087 - val_MinusLogProbMetric: 28.6087 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 381/1000
2023-10-12 05:39:10.349 
Epoch 381/1000 
	 loss: 27.5627, MinusLogProbMetric: 27.5627, val_loss: 28.6211, val_MinusLogProbMetric: 28.6211

Epoch 381: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5627 - MinusLogProbMetric: 27.5627 - val_loss: 28.6211 - val_MinusLogProbMetric: 28.6211 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 382/1000
2023-10-12 05:39:36.487 
Epoch 382/1000 
	 loss: 27.7768, MinusLogProbMetric: 27.7768, val_loss: 29.2459, val_MinusLogProbMetric: 29.2459

Epoch 382: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.7768 - MinusLogProbMetric: 27.7768 - val_loss: 29.2459 - val_MinusLogProbMetric: 29.2459 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 383/1000
2023-10-12 05:40:03.306 
Epoch 383/1000 
	 loss: 27.5791, MinusLogProbMetric: 27.5791, val_loss: 28.7448, val_MinusLogProbMetric: 28.7448

Epoch 383: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5791 - MinusLogProbMetric: 27.5791 - val_loss: 28.7448 - val_MinusLogProbMetric: 28.7448 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 384/1000
2023-10-12 05:40:30.351 
Epoch 384/1000 
	 loss: 27.6146, MinusLogProbMetric: 27.6146, val_loss: 28.8421, val_MinusLogProbMetric: 28.8421

Epoch 384: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.6146 - MinusLogProbMetric: 27.6146 - val_loss: 28.8421 - val_MinusLogProbMetric: 28.8421 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 385/1000
2023-10-12 05:40:58.898 
Epoch 385/1000 
	 loss: 27.7428, MinusLogProbMetric: 27.7428, val_loss: 28.8707, val_MinusLogProbMetric: 28.8707

Epoch 385: val_loss did not improve from 28.56817
196/196 - 29s - loss: 27.7428 - MinusLogProbMetric: 27.7428 - val_loss: 28.8707 - val_MinusLogProbMetric: 28.8707 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 386/1000
2023-10-12 05:41:27.663 
Epoch 386/1000 
	 loss: 27.5677, MinusLogProbMetric: 27.5677, val_loss: 28.6492, val_MinusLogProbMetric: 28.6492

Epoch 386: val_loss did not improve from 28.56817
196/196 - 29s - loss: 27.5677 - MinusLogProbMetric: 27.5677 - val_loss: 28.6492 - val_MinusLogProbMetric: 28.6492 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 387/1000
2023-10-12 05:41:54.924 
Epoch 387/1000 
	 loss: 27.5603, MinusLogProbMetric: 27.5603, val_loss: 28.7556, val_MinusLogProbMetric: 28.7556

Epoch 387: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5603 - MinusLogProbMetric: 27.5603 - val_loss: 28.7556 - val_MinusLogProbMetric: 28.7556 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 388/1000
2023-10-12 05:42:22.170 
Epoch 388/1000 
	 loss: 27.6084, MinusLogProbMetric: 27.6084, val_loss: 28.6787, val_MinusLogProbMetric: 28.6787

Epoch 388: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.6084 - MinusLogProbMetric: 27.6084 - val_loss: 28.6787 - val_MinusLogProbMetric: 28.6787 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 389/1000
2023-10-12 05:42:48.873 
Epoch 389/1000 
	 loss: 27.5355, MinusLogProbMetric: 27.5355, val_loss: 28.9726, val_MinusLogProbMetric: 28.9726

Epoch 389: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5355 - MinusLogProbMetric: 27.5355 - val_loss: 28.9726 - val_MinusLogProbMetric: 28.9726 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 390/1000
2023-10-12 05:43:15.697 
Epoch 390/1000 
	 loss: 27.7120, MinusLogProbMetric: 27.7120, val_loss: 28.7304, val_MinusLogProbMetric: 28.7304

Epoch 390: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.7120 - MinusLogProbMetric: 27.7120 - val_loss: 28.7304 - val_MinusLogProbMetric: 28.7304 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 391/1000
2023-10-12 05:43:42.580 
Epoch 391/1000 
	 loss: 27.5469, MinusLogProbMetric: 27.5469, val_loss: 28.6406, val_MinusLogProbMetric: 28.6406

Epoch 391: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5469 - MinusLogProbMetric: 27.5469 - val_loss: 28.6406 - val_MinusLogProbMetric: 28.6406 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 392/1000
2023-10-12 05:44:09.540 
Epoch 392/1000 
	 loss: 27.5327, MinusLogProbMetric: 27.5327, val_loss: 28.8596, val_MinusLogProbMetric: 28.8596

Epoch 392: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5327 - MinusLogProbMetric: 27.5327 - val_loss: 28.8596 - val_MinusLogProbMetric: 28.8596 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 393/1000
2023-10-12 05:44:35.693 
Epoch 393/1000 
	 loss: 27.5661, MinusLogProbMetric: 27.5661, val_loss: 28.7474, val_MinusLogProbMetric: 28.7474

Epoch 393: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5661 - MinusLogProbMetric: 27.5661 - val_loss: 28.7474 - val_MinusLogProbMetric: 28.7474 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 394/1000
2023-10-12 05:45:02.373 
Epoch 394/1000 
	 loss: 27.5572, MinusLogProbMetric: 27.5572, val_loss: 28.8290, val_MinusLogProbMetric: 28.8290

Epoch 394: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5572 - MinusLogProbMetric: 27.5572 - val_loss: 28.8290 - val_MinusLogProbMetric: 28.8290 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 395/1000
2023-10-12 05:45:28.700 
Epoch 395/1000 
	 loss: 27.7766, MinusLogProbMetric: 27.7766, val_loss: 28.6858, val_MinusLogProbMetric: 28.6858

Epoch 395: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.7766 - MinusLogProbMetric: 27.7766 - val_loss: 28.6858 - val_MinusLogProbMetric: 28.6858 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 396/1000
2023-10-12 05:45:54.933 
Epoch 396/1000 
	 loss: 27.5051, MinusLogProbMetric: 27.5051, val_loss: 28.8499, val_MinusLogProbMetric: 28.8499

Epoch 396: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5051 - MinusLogProbMetric: 27.5051 - val_loss: 28.8499 - val_MinusLogProbMetric: 28.8499 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 397/1000
2023-10-12 05:46:20.723 
Epoch 397/1000 
	 loss: 27.5335, MinusLogProbMetric: 27.5335, val_loss: 28.7361, val_MinusLogProbMetric: 28.7361

Epoch 397: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5335 - MinusLogProbMetric: 27.5335 - val_loss: 28.7361 - val_MinusLogProbMetric: 28.7361 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 398/1000
2023-10-12 05:46:47.030 
Epoch 398/1000 
	 loss: 27.5687, MinusLogProbMetric: 27.5687, val_loss: 28.9844, val_MinusLogProbMetric: 28.9844

Epoch 398: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5687 - MinusLogProbMetric: 27.5687 - val_loss: 28.9844 - val_MinusLogProbMetric: 28.9844 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 399/1000
2023-10-12 05:47:13.566 
Epoch 399/1000 
	 loss: 27.5747, MinusLogProbMetric: 27.5747, val_loss: 28.6051, val_MinusLogProbMetric: 28.6051

Epoch 399: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5747 - MinusLogProbMetric: 27.5747 - val_loss: 28.6051 - val_MinusLogProbMetric: 28.6051 - lr: 2.5000e-04 - 27s/epoch - 135ms/step
Epoch 400/1000
2023-10-12 05:47:40.236 
Epoch 400/1000 
	 loss: 27.5744, MinusLogProbMetric: 27.5744, val_loss: 28.6292, val_MinusLogProbMetric: 28.6292

Epoch 400: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5744 - MinusLogProbMetric: 27.5744 - val_loss: 28.6292 - val_MinusLogProbMetric: 28.6292 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 401/1000
2023-10-12 05:48:07.242 
Epoch 401/1000 
	 loss: 27.6006, MinusLogProbMetric: 27.6006, val_loss: 28.5825, val_MinusLogProbMetric: 28.5825

Epoch 401: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.6006 - MinusLogProbMetric: 27.6006 - val_loss: 28.5825 - val_MinusLogProbMetric: 28.5825 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 402/1000
2023-10-12 05:48:34.291 
Epoch 402/1000 
	 loss: 27.5464, MinusLogProbMetric: 27.5464, val_loss: 28.6716, val_MinusLogProbMetric: 28.6716

Epoch 402: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5464 - MinusLogProbMetric: 27.5464 - val_loss: 28.6716 - val_MinusLogProbMetric: 28.6716 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 403/1000
2023-10-12 05:49:01.120 
Epoch 403/1000 
	 loss: 27.6918, MinusLogProbMetric: 27.6918, val_loss: 28.7065, val_MinusLogProbMetric: 28.7065

Epoch 403: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.6918 - MinusLogProbMetric: 27.6918 - val_loss: 28.7065 - val_MinusLogProbMetric: 28.7065 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 404/1000
2023-10-12 05:49:26.693 
Epoch 404/1000 
	 loss: 27.5632, MinusLogProbMetric: 27.5632, val_loss: 28.8139, val_MinusLogProbMetric: 28.8139

Epoch 404: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5632 - MinusLogProbMetric: 27.5632 - val_loss: 28.8139 - val_MinusLogProbMetric: 28.8139 - lr: 2.5000e-04 - 26s/epoch - 130ms/step
Epoch 405/1000
2023-10-12 05:49:52.569 
Epoch 405/1000 
	 loss: 27.6858, MinusLogProbMetric: 27.6858, val_loss: 28.6781, val_MinusLogProbMetric: 28.6781

Epoch 405: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.6858 - MinusLogProbMetric: 27.6858 - val_loss: 28.6781 - val_MinusLogProbMetric: 28.6781 - lr: 2.5000e-04 - 26s/epoch - 132ms/step
Epoch 406/1000
2023-10-12 05:50:18.640 
Epoch 406/1000 
	 loss: 27.8227, MinusLogProbMetric: 27.8227, val_loss: 28.9474, val_MinusLogProbMetric: 28.9474

Epoch 406: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.8227 - MinusLogProbMetric: 27.8227 - val_loss: 28.9474 - val_MinusLogProbMetric: 28.9474 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 407/1000
2023-10-12 05:50:44.763 
Epoch 407/1000 
	 loss: 27.5144, MinusLogProbMetric: 27.5144, val_loss: 28.9231, val_MinusLogProbMetric: 28.9231

Epoch 407: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5144 - MinusLogProbMetric: 27.5144 - val_loss: 28.9231 - val_MinusLogProbMetric: 28.9231 - lr: 2.5000e-04 - 26s/epoch - 133ms/step
Epoch 408/1000
2023-10-12 05:51:10.484 
Epoch 408/1000 
	 loss: 27.5049, MinusLogProbMetric: 27.5049, val_loss: 28.7939, val_MinusLogProbMetric: 28.7939

Epoch 408: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5049 - MinusLogProbMetric: 27.5049 - val_loss: 28.7939 - val_MinusLogProbMetric: 28.7939 - lr: 2.5000e-04 - 26s/epoch - 131ms/step
Epoch 409/1000
2023-10-12 05:51:36.782 
Epoch 409/1000 
	 loss: 27.5480, MinusLogProbMetric: 27.5480, val_loss: 28.6024, val_MinusLogProbMetric: 28.6024

Epoch 409: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5480 - MinusLogProbMetric: 27.5480 - val_loss: 28.6024 - val_MinusLogProbMetric: 28.6024 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 410/1000
2023-10-12 05:52:04.273 
Epoch 410/1000 
	 loss: 27.5715, MinusLogProbMetric: 27.5715, val_loss: 29.1066, val_MinusLogProbMetric: 29.1066

Epoch 410: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5715 - MinusLogProbMetric: 27.5715 - val_loss: 29.1066 - val_MinusLogProbMetric: 29.1066 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 411/1000
2023-10-12 05:52:31.062 
Epoch 411/1000 
	 loss: 27.5467, MinusLogProbMetric: 27.5467, val_loss: 28.8297, val_MinusLogProbMetric: 28.8297

Epoch 411: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5467 - MinusLogProbMetric: 27.5467 - val_loss: 28.8297 - val_MinusLogProbMetric: 28.8297 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 412/1000
2023-10-12 05:52:57.831 
Epoch 412/1000 
	 loss: 27.5443, MinusLogProbMetric: 27.5443, val_loss: 28.6369, val_MinusLogProbMetric: 28.6369

Epoch 412: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5443 - MinusLogProbMetric: 27.5443 - val_loss: 28.6369 - val_MinusLogProbMetric: 28.6369 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 413/1000
2023-10-12 05:53:24.850 
Epoch 413/1000 
	 loss: 27.5263, MinusLogProbMetric: 27.5263, val_loss: 28.9313, val_MinusLogProbMetric: 28.9313

Epoch 413: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5263 - MinusLogProbMetric: 27.5263 - val_loss: 28.9313 - val_MinusLogProbMetric: 28.9313 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 414/1000
2023-10-12 05:53:51.643 
Epoch 414/1000 
	 loss: 27.5469, MinusLogProbMetric: 27.5469, val_loss: 29.7257, val_MinusLogProbMetric: 29.7257

Epoch 414: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5469 - MinusLogProbMetric: 27.5469 - val_loss: 29.7257 - val_MinusLogProbMetric: 29.7257 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 415/1000
2023-10-12 05:54:18.235 
Epoch 415/1000 
	 loss: 27.6148, MinusLogProbMetric: 27.6148, val_loss: 29.0318, val_MinusLogProbMetric: 29.0318

Epoch 415: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.6148 - MinusLogProbMetric: 27.6148 - val_loss: 29.0318 - val_MinusLogProbMetric: 29.0318 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 416/1000
2023-10-12 05:54:45.251 
Epoch 416/1000 
	 loss: 27.5688, MinusLogProbMetric: 27.5688, val_loss: 28.6385, val_MinusLogProbMetric: 28.6385

Epoch 416: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5688 - MinusLogProbMetric: 27.5688 - val_loss: 28.6385 - val_MinusLogProbMetric: 28.6385 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 417/1000
2023-10-12 05:55:11.868 
Epoch 417/1000 
	 loss: 27.5221, MinusLogProbMetric: 27.5221, val_loss: 28.7166, val_MinusLogProbMetric: 28.7166

Epoch 417: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5221 - MinusLogProbMetric: 27.5221 - val_loss: 28.7166 - val_MinusLogProbMetric: 28.7166 - lr: 2.5000e-04 - 27s/epoch - 136ms/step
Epoch 418/1000
2023-10-12 05:55:37.415 
Epoch 418/1000 
	 loss: 27.5520, MinusLogProbMetric: 27.5520, val_loss: 28.7024, val_MinusLogProbMetric: 28.7024

Epoch 418: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5520 - MinusLogProbMetric: 27.5520 - val_loss: 28.7024 - val_MinusLogProbMetric: 28.7024 - lr: 2.5000e-04 - 26s/epoch - 130ms/step
Epoch 419/1000
2023-10-12 05:56:03.608 
Epoch 419/1000 
	 loss: 27.5643, MinusLogProbMetric: 27.5643, val_loss: 28.7575, val_MinusLogProbMetric: 28.7575

Epoch 419: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5643 - MinusLogProbMetric: 27.5643 - val_loss: 28.7575 - val_MinusLogProbMetric: 28.7575 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 420/1000
2023-10-12 05:56:30.091 
Epoch 420/1000 
	 loss: 27.6269, MinusLogProbMetric: 27.6269, val_loss: 28.6961, val_MinusLogProbMetric: 28.6961

Epoch 420: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.6269 - MinusLogProbMetric: 27.6269 - val_loss: 28.6961 - val_MinusLogProbMetric: 28.6961 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 421/1000
2023-10-12 05:56:56.385 
Epoch 421/1000 
	 loss: 27.5181, MinusLogProbMetric: 27.5181, val_loss: 28.6431, val_MinusLogProbMetric: 28.6431

Epoch 421: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5181 - MinusLogProbMetric: 27.5181 - val_loss: 28.6431 - val_MinusLogProbMetric: 28.6431 - lr: 2.5000e-04 - 26s/epoch - 134ms/step
Epoch 422/1000
2023-10-12 05:57:22.080 
Epoch 422/1000 
	 loss: 27.5169, MinusLogProbMetric: 27.5169, val_loss: 28.6405, val_MinusLogProbMetric: 28.6405

Epoch 422: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5169 - MinusLogProbMetric: 27.5169 - val_loss: 28.6405 - val_MinusLogProbMetric: 28.6405 - lr: 2.5000e-04 - 26s/epoch - 131ms/step
Epoch 423/1000
2023-10-12 05:57:48.580 
Epoch 423/1000 
	 loss: 27.8176, MinusLogProbMetric: 27.8176, val_loss: 28.8844, val_MinusLogProbMetric: 28.8844

Epoch 423: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.8176 - MinusLogProbMetric: 27.8176 - val_loss: 28.8844 - val_MinusLogProbMetric: 28.8844 - lr: 2.5000e-04 - 27s/epoch - 135ms/step
Epoch 424/1000
2023-10-12 05:58:15.559 
Epoch 424/1000 
	 loss: 27.5805, MinusLogProbMetric: 27.5805, val_loss: 28.7453, val_MinusLogProbMetric: 28.7453

Epoch 424: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5805 - MinusLogProbMetric: 27.5805 - val_loss: 28.7453 - val_MinusLogProbMetric: 28.7453 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 425/1000
2023-10-12 05:58:42.009 
Epoch 425/1000 
	 loss: 27.5049, MinusLogProbMetric: 27.5049, val_loss: 29.1311, val_MinusLogProbMetric: 29.1311

Epoch 425: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5049 - MinusLogProbMetric: 27.5049 - val_loss: 29.1311 - val_MinusLogProbMetric: 29.1311 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 426/1000
2023-10-12 05:59:08.565 
Epoch 426/1000 
	 loss: 27.5988, MinusLogProbMetric: 27.5988, val_loss: 28.7843, val_MinusLogProbMetric: 28.7843

Epoch 426: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5988 - MinusLogProbMetric: 27.5988 - val_loss: 28.7843 - val_MinusLogProbMetric: 28.7843 - lr: 2.5000e-04 - 27s/epoch - 135ms/step
Epoch 427/1000
2023-10-12 05:59:35.724 
Epoch 427/1000 
	 loss: 27.5237, MinusLogProbMetric: 27.5237, val_loss: 28.6087, val_MinusLogProbMetric: 28.6087

Epoch 427: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5237 - MinusLogProbMetric: 27.5237 - val_loss: 28.6087 - val_MinusLogProbMetric: 28.6087 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 428/1000
2023-10-12 06:00:02.115 
Epoch 428/1000 
	 loss: 27.5274, MinusLogProbMetric: 27.5274, val_loss: 28.6242, val_MinusLogProbMetric: 28.6242

Epoch 428: val_loss did not improve from 28.56817
196/196 - 26s - loss: 27.5274 - MinusLogProbMetric: 27.5274 - val_loss: 28.6242 - val_MinusLogProbMetric: 28.6242 - lr: 2.5000e-04 - 26s/epoch - 135ms/step
Epoch 429/1000
2023-10-12 06:00:29.201 
Epoch 429/1000 
	 loss: 27.5566, MinusLogProbMetric: 27.5566, val_loss: 28.6558, val_MinusLogProbMetric: 28.6558

Epoch 429: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.5566 - MinusLogProbMetric: 27.5566 - val_loss: 28.6558 - val_MinusLogProbMetric: 28.6558 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 430/1000
2023-10-12 06:00:56.073 
Epoch 430/1000 
	 loss: 27.3741, MinusLogProbMetric: 27.3741, val_loss: 28.5746, val_MinusLogProbMetric: 28.5746

Epoch 430: val_loss did not improve from 28.56817
196/196 - 27s - loss: 27.3741 - MinusLogProbMetric: 27.3741 - val_loss: 28.5746 - val_MinusLogProbMetric: 28.5746 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 431/1000
2023-10-12 06:01:23.053 
Epoch 431/1000 
	 loss: 27.3528, MinusLogProbMetric: 27.3528, val_loss: 28.5473, val_MinusLogProbMetric: 28.5473

Epoch 431: val_loss improved from 28.56817 to 28.54730, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.3528 - MinusLogProbMetric: 27.3528 - val_loss: 28.5473 - val_MinusLogProbMetric: 28.5473 - lr: 1.2500e-04 - 27s/epoch - 140ms/step
Epoch 432/1000
2023-10-12 06:01:50.179 
Epoch 432/1000 
	 loss: 27.3627, MinusLogProbMetric: 27.3627, val_loss: 28.5628, val_MinusLogProbMetric: 28.5628

Epoch 432: val_loss did not improve from 28.54730
196/196 - 27s - loss: 27.3627 - MinusLogProbMetric: 27.3627 - val_loss: 28.5628 - val_MinusLogProbMetric: 28.5628 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 433/1000
2023-10-12 06:02:16.765 
Epoch 433/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 28.5795, val_MinusLogProbMetric: 28.5795

Epoch 433: val_loss did not improve from 28.54730
196/196 - 27s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 28.5795 - val_MinusLogProbMetric: 28.5795 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 434/1000
2023-10-12 06:02:43.777 
Epoch 434/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 28.5854, val_MinusLogProbMetric: 28.5854

Epoch 434: val_loss did not improve from 28.54730
196/196 - 27s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 28.5854 - val_MinusLogProbMetric: 28.5854 - lr: 1.2500e-04 - 27s/epoch - 138ms/step
Epoch 435/1000
2023-10-12 06:03:10.555 
Epoch 435/1000 
	 loss: 27.3424, MinusLogProbMetric: 27.3424, val_loss: 28.5334, val_MinusLogProbMetric: 28.5334

Epoch 435: val_loss improved from 28.54730 to 28.53344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.3424 - MinusLogProbMetric: 27.3424 - val_loss: 28.5334 - val_MinusLogProbMetric: 28.5334 - lr: 1.2500e-04 - 27s/epoch - 140ms/step
Epoch 436/1000
2023-10-12 06:03:38.063 
Epoch 436/1000 
	 loss: 27.3490, MinusLogProbMetric: 27.3490, val_loss: 28.5581, val_MinusLogProbMetric: 28.5581

Epoch 436: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3490 - MinusLogProbMetric: 27.3490 - val_loss: 28.5581 - val_MinusLogProbMetric: 28.5581 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 437/1000
2023-10-12 06:04:04.210 
Epoch 437/1000 
	 loss: 27.3457, MinusLogProbMetric: 27.3457, val_loss: 28.5366, val_MinusLogProbMetric: 28.5366

Epoch 437: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3457 - MinusLogProbMetric: 27.3457 - val_loss: 28.5366 - val_MinusLogProbMetric: 28.5366 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 438/1000
2023-10-12 06:04:30.513 
Epoch 438/1000 
	 loss: 27.3615, MinusLogProbMetric: 27.3615, val_loss: 28.5974, val_MinusLogProbMetric: 28.5974

Epoch 438: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3615 - MinusLogProbMetric: 27.3615 - val_loss: 28.5974 - val_MinusLogProbMetric: 28.5974 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 439/1000
2023-10-12 06:04:57.450 
Epoch 439/1000 
	 loss: 27.3809, MinusLogProbMetric: 27.3809, val_loss: 28.7704, val_MinusLogProbMetric: 28.7704

Epoch 439: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3809 - MinusLogProbMetric: 27.3809 - val_loss: 28.7704 - val_MinusLogProbMetric: 28.7704 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 440/1000
2023-10-12 06:05:24.252 
Epoch 440/1000 
	 loss: 27.3528, MinusLogProbMetric: 27.3528, val_loss: 28.5545, val_MinusLogProbMetric: 28.5545

Epoch 440: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3528 - MinusLogProbMetric: 27.3528 - val_loss: 28.5545 - val_MinusLogProbMetric: 28.5545 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 441/1000
2023-10-12 06:05:51.027 
Epoch 441/1000 
	 loss: 27.3487, MinusLogProbMetric: 27.3487, val_loss: 28.5544, val_MinusLogProbMetric: 28.5544

Epoch 441: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3487 - MinusLogProbMetric: 27.3487 - val_loss: 28.5544 - val_MinusLogProbMetric: 28.5544 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 442/1000
2023-10-12 06:06:17.181 
Epoch 442/1000 
	 loss: 27.3567, MinusLogProbMetric: 27.3567, val_loss: 28.5445, val_MinusLogProbMetric: 28.5445

Epoch 442: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3567 - MinusLogProbMetric: 27.3567 - val_loss: 28.5445 - val_MinusLogProbMetric: 28.5445 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 443/1000
2023-10-12 06:06:43.700 
Epoch 443/1000 
	 loss: 27.3513, MinusLogProbMetric: 27.3513, val_loss: 28.6190, val_MinusLogProbMetric: 28.6190

Epoch 443: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3513 - MinusLogProbMetric: 27.3513 - val_loss: 28.6190 - val_MinusLogProbMetric: 28.6190 - lr: 1.2500e-04 - 27s/epoch - 135ms/step
Epoch 444/1000
2023-10-12 06:07:09.576 
Epoch 444/1000 
	 loss: 27.3504, MinusLogProbMetric: 27.3504, val_loss: 28.7287, val_MinusLogProbMetric: 28.7287

Epoch 444: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3504 - MinusLogProbMetric: 27.3504 - val_loss: 28.7287 - val_MinusLogProbMetric: 28.7287 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 445/1000
2023-10-12 06:07:35.343 
Epoch 445/1000 
	 loss: 27.3472, MinusLogProbMetric: 27.3472, val_loss: 28.6668, val_MinusLogProbMetric: 28.6668

Epoch 445: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3472 - MinusLogProbMetric: 27.3472 - val_loss: 28.6668 - val_MinusLogProbMetric: 28.6668 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 446/1000
2023-10-12 06:08:02.130 
Epoch 446/1000 
	 loss: 27.3832, MinusLogProbMetric: 27.3832, val_loss: 28.6191, val_MinusLogProbMetric: 28.6191

Epoch 446: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3832 - MinusLogProbMetric: 27.3832 - val_loss: 28.6191 - val_MinusLogProbMetric: 28.6191 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 447/1000
2023-10-12 06:08:28.358 
Epoch 447/1000 
	 loss: 27.3459, MinusLogProbMetric: 27.3459, val_loss: 28.6160, val_MinusLogProbMetric: 28.6160

Epoch 447: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3459 - MinusLogProbMetric: 27.3459 - val_loss: 28.6160 - val_MinusLogProbMetric: 28.6160 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 448/1000
2023-10-12 06:08:55.192 
Epoch 448/1000 
	 loss: 27.3536, MinusLogProbMetric: 27.3536, val_loss: 28.5418, val_MinusLogProbMetric: 28.5418

Epoch 448: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3536 - MinusLogProbMetric: 27.3536 - val_loss: 28.5418 - val_MinusLogProbMetric: 28.5418 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 449/1000
2023-10-12 06:09:22.061 
Epoch 449/1000 
	 loss: 27.3409, MinusLogProbMetric: 27.3409, val_loss: 28.6343, val_MinusLogProbMetric: 28.6343

Epoch 449: val_loss did not improve from 28.53344
196/196 - 27s - loss: 27.3409 - MinusLogProbMetric: 27.3409 - val_loss: 28.6343 - val_MinusLogProbMetric: 28.6343 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 450/1000
2023-10-12 06:09:48.173 
Epoch 450/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.5448, val_MinusLogProbMetric: 28.5448

Epoch 450: val_loss did not improve from 28.53344
196/196 - 26s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.5448 - val_MinusLogProbMetric: 28.5448 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 451/1000
2023-10-12 06:10:14.362 
Epoch 451/1000 
	 loss: 27.3529, MinusLogProbMetric: 27.3529, val_loss: 28.5237, val_MinusLogProbMetric: 28.5237

Epoch 451: val_loss improved from 28.53344 to 28.52375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.3529 - MinusLogProbMetric: 27.3529 - val_loss: 28.5237 - val_MinusLogProbMetric: 28.5237 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 452/1000
2023-10-12 06:10:41.325 
Epoch 452/1000 
	 loss: 27.3554, MinusLogProbMetric: 27.3554, val_loss: 28.5449, val_MinusLogProbMetric: 28.5449

Epoch 452: val_loss did not improve from 28.52375
196/196 - 26s - loss: 27.3554 - MinusLogProbMetric: 27.3554 - val_loss: 28.5449 - val_MinusLogProbMetric: 28.5449 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 453/1000
2023-10-12 06:11:06.709 
Epoch 453/1000 
	 loss: 27.3409, MinusLogProbMetric: 27.3409, val_loss: 28.5635, val_MinusLogProbMetric: 28.5635

Epoch 453: val_loss did not improve from 28.52375
196/196 - 25s - loss: 27.3409 - MinusLogProbMetric: 27.3409 - val_loss: 28.5635 - val_MinusLogProbMetric: 28.5635 - lr: 1.2500e-04 - 25s/epoch - 129ms/step
Epoch 454/1000
2023-10-12 06:11:32.454 
Epoch 454/1000 
	 loss: 27.3536, MinusLogProbMetric: 27.3536, val_loss: 28.5311, val_MinusLogProbMetric: 28.5311

Epoch 454: val_loss did not improve from 28.52375
196/196 - 26s - loss: 27.3536 - MinusLogProbMetric: 27.3536 - val_loss: 28.5311 - val_MinusLogProbMetric: 28.5311 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 455/1000
2023-10-12 06:11:58.090 
Epoch 455/1000 
	 loss: 27.3683, MinusLogProbMetric: 27.3683, val_loss: 28.5321, val_MinusLogProbMetric: 28.5321

Epoch 455: val_loss did not improve from 28.52375
196/196 - 26s - loss: 27.3683 - MinusLogProbMetric: 27.3683 - val_loss: 28.5321 - val_MinusLogProbMetric: 28.5321 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 456/1000
2023-10-12 06:12:23.559 
Epoch 456/1000 
	 loss: 27.3317, MinusLogProbMetric: 27.3317, val_loss: 28.5584, val_MinusLogProbMetric: 28.5584

Epoch 456: val_loss did not improve from 28.52375
196/196 - 25s - loss: 27.3317 - MinusLogProbMetric: 27.3317 - val_loss: 28.5584 - val_MinusLogProbMetric: 28.5584 - lr: 1.2500e-04 - 25s/epoch - 130ms/step
Epoch 457/1000
2023-10-12 06:12:49.260 
Epoch 457/1000 
	 loss: 27.3489, MinusLogProbMetric: 27.3489, val_loss: 28.5521, val_MinusLogProbMetric: 28.5521

Epoch 457: val_loss did not improve from 28.52375
196/196 - 26s - loss: 27.3489 - MinusLogProbMetric: 27.3489 - val_loss: 28.5521 - val_MinusLogProbMetric: 28.5521 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 458/1000
2023-10-12 06:13:15.396 
Epoch 458/1000 
	 loss: 27.3428, MinusLogProbMetric: 27.3428, val_loss: 28.5197, val_MinusLogProbMetric: 28.5197

Epoch 458: val_loss improved from 28.52375 to 28.51967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.3428 - MinusLogProbMetric: 27.3428 - val_loss: 28.5197 - val_MinusLogProbMetric: 28.5197 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 459/1000
2023-10-12 06:13:41.711 
Epoch 459/1000 
	 loss: 27.3465, MinusLogProbMetric: 27.3465, val_loss: 28.5499, val_MinusLogProbMetric: 28.5499

Epoch 459: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3465 - MinusLogProbMetric: 27.3465 - val_loss: 28.5499 - val_MinusLogProbMetric: 28.5499 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 460/1000
2023-10-12 06:14:09.891 
Epoch 460/1000 
	 loss: 27.3407, MinusLogProbMetric: 27.3407, val_loss: 28.5964, val_MinusLogProbMetric: 28.5964

Epoch 460: val_loss did not improve from 28.51967
196/196 - 28s - loss: 27.3407 - MinusLogProbMetric: 27.3407 - val_loss: 28.5964 - val_MinusLogProbMetric: 28.5964 - lr: 1.2500e-04 - 28s/epoch - 144ms/step
Epoch 461/1000
2023-10-12 06:14:37.503 
Epoch 461/1000 
	 loss: 27.3915, MinusLogProbMetric: 27.3915, val_loss: 28.6721, val_MinusLogProbMetric: 28.6721

Epoch 461: val_loss did not improve from 28.51967
196/196 - 28s - loss: 27.3915 - MinusLogProbMetric: 27.3915 - val_loss: 28.6721 - val_MinusLogProbMetric: 28.6721 - lr: 1.2500e-04 - 28s/epoch - 141ms/step
Epoch 462/1000
2023-10-12 06:15:04.004 
Epoch 462/1000 
	 loss: 27.3573, MinusLogProbMetric: 27.3573, val_loss: 28.5516, val_MinusLogProbMetric: 28.5516

Epoch 462: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3573 - MinusLogProbMetric: 27.3573 - val_loss: 28.5516 - val_MinusLogProbMetric: 28.5516 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 463/1000
2023-10-12 06:15:30.503 
Epoch 463/1000 
	 loss: 27.3402, MinusLogProbMetric: 27.3402, val_loss: 28.5473, val_MinusLogProbMetric: 28.5473

Epoch 463: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3402 - MinusLogProbMetric: 27.3402 - val_loss: 28.5473 - val_MinusLogProbMetric: 28.5473 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 464/1000
2023-10-12 06:15:56.718 
Epoch 464/1000 
	 loss: 27.3466, MinusLogProbMetric: 27.3466, val_loss: 28.6168, val_MinusLogProbMetric: 28.6168

Epoch 464: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3466 - MinusLogProbMetric: 27.3466 - val_loss: 28.6168 - val_MinusLogProbMetric: 28.6168 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 465/1000
2023-10-12 06:16:23.067 
Epoch 465/1000 
	 loss: 27.3484, MinusLogProbMetric: 27.3484, val_loss: 28.6865, val_MinusLogProbMetric: 28.6865

Epoch 465: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3484 - MinusLogProbMetric: 27.3484 - val_loss: 28.6865 - val_MinusLogProbMetric: 28.6865 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 466/1000
2023-10-12 06:16:51.471 
Epoch 466/1000 
	 loss: 27.3458, MinusLogProbMetric: 27.3458, val_loss: 28.5375, val_MinusLogProbMetric: 28.5375

Epoch 466: val_loss did not improve from 28.51967
196/196 - 28s - loss: 27.3458 - MinusLogProbMetric: 27.3458 - val_loss: 28.5375 - val_MinusLogProbMetric: 28.5375 - lr: 1.2500e-04 - 28s/epoch - 145ms/step
Epoch 467/1000
2023-10-12 06:17:18.243 
Epoch 467/1000 
	 loss: 27.3516, MinusLogProbMetric: 27.3516, val_loss: 28.6260, val_MinusLogProbMetric: 28.6260

Epoch 467: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3516 - MinusLogProbMetric: 27.3516 - val_loss: 28.6260 - val_MinusLogProbMetric: 28.6260 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 468/1000
2023-10-12 06:17:44.796 
Epoch 468/1000 
	 loss: 27.3376, MinusLogProbMetric: 27.3376, val_loss: 28.5620, val_MinusLogProbMetric: 28.5620

Epoch 468: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3376 - MinusLogProbMetric: 27.3376 - val_loss: 28.5620 - val_MinusLogProbMetric: 28.5620 - lr: 1.2500e-04 - 27s/epoch - 135ms/step
Epoch 469/1000
2023-10-12 06:18:12.257 
Epoch 469/1000 
	 loss: 27.3564, MinusLogProbMetric: 27.3564, val_loss: 28.5555, val_MinusLogProbMetric: 28.5555

Epoch 469: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3564 - MinusLogProbMetric: 27.3564 - val_loss: 28.5555 - val_MinusLogProbMetric: 28.5555 - lr: 1.2500e-04 - 27s/epoch - 140ms/step
Epoch 470/1000
2023-10-12 06:18:38.925 
Epoch 470/1000 
	 loss: 27.3287, MinusLogProbMetric: 27.3287, val_loss: 28.5298, val_MinusLogProbMetric: 28.5298

Epoch 470: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3287 - MinusLogProbMetric: 27.3287 - val_loss: 28.5298 - val_MinusLogProbMetric: 28.5298 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 471/1000
2023-10-12 06:19:05.738 
Epoch 471/1000 
	 loss: 27.3520, MinusLogProbMetric: 27.3520, val_loss: 28.9410, val_MinusLogProbMetric: 28.9410

Epoch 471: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3520 - MinusLogProbMetric: 27.3520 - val_loss: 28.9410 - val_MinusLogProbMetric: 28.9410 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 472/1000
2023-10-12 06:19:31.628 
Epoch 472/1000 
	 loss: 27.3695, MinusLogProbMetric: 27.3695, val_loss: 28.5763, val_MinusLogProbMetric: 28.5763

Epoch 472: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3695 - MinusLogProbMetric: 27.3695 - val_loss: 28.5763 - val_MinusLogProbMetric: 28.5763 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 473/1000
2023-10-12 06:19:57.356 
Epoch 473/1000 
	 loss: 27.3644, MinusLogProbMetric: 27.3644, val_loss: 28.5427, val_MinusLogProbMetric: 28.5427

Epoch 473: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3644 - MinusLogProbMetric: 27.3644 - val_loss: 28.5427 - val_MinusLogProbMetric: 28.5427 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 474/1000
2023-10-12 06:20:23.054 
Epoch 474/1000 
	 loss: 27.3213, MinusLogProbMetric: 27.3213, val_loss: 28.5701, val_MinusLogProbMetric: 28.5701

Epoch 474: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3213 - MinusLogProbMetric: 27.3213 - val_loss: 28.5701 - val_MinusLogProbMetric: 28.5701 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 475/1000
2023-10-12 06:20:49.871 
Epoch 475/1000 
	 loss: 27.3688, MinusLogProbMetric: 27.3688, val_loss: 28.5304, val_MinusLogProbMetric: 28.5304

Epoch 475: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3688 - MinusLogProbMetric: 27.3688 - val_loss: 28.5304 - val_MinusLogProbMetric: 28.5304 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 476/1000
2023-10-12 06:21:17.892 
Epoch 476/1000 
	 loss: 27.3502, MinusLogProbMetric: 27.3502, val_loss: 28.5904, val_MinusLogProbMetric: 28.5904

Epoch 476: val_loss did not improve from 28.51967
196/196 - 28s - loss: 27.3502 - MinusLogProbMetric: 27.3502 - val_loss: 28.5904 - val_MinusLogProbMetric: 28.5904 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 477/1000
2023-10-12 06:21:44.683 
Epoch 477/1000 
	 loss: 27.3527, MinusLogProbMetric: 27.3527, val_loss: 28.5910, val_MinusLogProbMetric: 28.5910

Epoch 477: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3527 - MinusLogProbMetric: 27.3527 - val_loss: 28.5910 - val_MinusLogProbMetric: 28.5910 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 478/1000
2023-10-12 06:22:12.197 
Epoch 478/1000 
	 loss: 27.3250, MinusLogProbMetric: 27.3250, val_loss: 28.5549, val_MinusLogProbMetric: 28.5549

Epoch 478: val_loss did not improve from 28.51967
196/196 - 28s - loss: 27.3250 - MinusLogProbMetric: 27.3250 - val_loss: 28.5549 - val_MinusLogProbMetric: 28.5549 - lr: 1.2500e-04 - 28s/epoch - 140ms/step
Epoch 479/1000
2023-10-12 06:22:38.938 
Epoch 479/1000 
	 loss: 27.3332, MinusLogProbMetric: 27.3332, val_loss: 28.7930, val_MinusLogProbMetric: 28.7930

Epoch 479: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3332 - MinusLogProbMetric: 27.3332 - val_loss: 28.7930 - val_MinusLogProbMetric: 28.7930 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 480/1000
2023-10-12 06:23:06.643 
Epoch 480/1000 
	 loss: 27.3429, MinusLogProbMetric: 27.3429, val_loss: 28.5965, val_MinusLogProbMetric: 28.5965

Epoch 480: val_loss did not improve from 28.51967
196/196 - 28s - loss: 27.3429 - MinusLogProbMetric: 27.3429 - val_loss: 28.5965 - val_MinusLogProbMetric: 28.5965 - lr: 1.2500e-04 - 28s/epoch - 141ms/step
Epoch 481/1000
2023-10-12 06:23:33.403 
Epoch 481/1000 
	 loss: 27.3320, MinusLogProbMetric: 27.3320, val_loss: 28.6019, val_MinusLogProbMetric: 28.6019

Epoch 481: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3320 - MinusLogProbMetric: 27.3320 - val_loss: 28.6019 - val_MinusLogProbMetric: 28.6019 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 482/1000
2023-10-12 06:23:59.469 
Epoch 482/1000 
	 loss: 27.3375, MinusLogProbMetric: 27.3375, val_loss: 28.5470, val_MinusLogProbMetric: 28.5470

Epoch 482: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3375 - MinusLogProbMetric: 27.3375 - val_loss: 28.5470 - val_MinusLogProbMetric: 28.5470 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 483/1000
2023-10-12 06:24:25.164 
Epoch 483/1000 
	 loss: 27.3404, MinusLogProbMetric: 27.3404, val_loss: 28.6370, val_MinusLogProbMetric: 28.6370

Epoch 483: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3404 - MinusLogProbMetric: 27.3404 - val_loss: 28.6370 - val_MinusLogProbMetric: 28.6370 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 484/1000
2023-10-12 06:24:51.057 
Epoch 484/1000 
	 loss: 27.3439, MinusLogProbMetric: 27.3439, val_loss: 28.5265, val_MinusLogProbMetric: 28.5265

Epoch 484: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3439 - MinusLogProbMetric: 27.3439 - val_loss: 28.5265 - val_MinusLogProbMetric: 28.5265 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 485/1000
2023-10-12 06:25:16.472 
Epoch 485/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 28.5678, val_MinusLogProbMetric: 28.5678

Epoch 485: val_loss did not improve from 28.51967
196/196 - 25s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 28.5678 - val_MinusLogProbMetric: 28.5678 - lr: 1.2500e-04 - 25s/epoch - 130ms/step
Epoch 486/1000
2023-10-12 06:25:42.618 
Epoch 486/1000 
	 loss: 27.3816, MinusLogProbMetric: 27.3816, val_loss: 28.6522, val_MinusLogProbMetric: 28.6522

Epoch 486: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3816 - MinusLogProbMetric: 27.3816 - val_loss: 28.6522 - val_MinusLogProbMetric: 28.6522 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 487/1000
2023-10-12 06:26:08.742 
Epoch 487/1000 
	 loss: 27.3271, MinusLogProbMetric: 27.3271, val_loss: 28.5448, val_MinusLogProbMetric: 28.5448

Epoch 487: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3271 - MinusLogProbMetric: 27.3271 - val_loss: 28.5448 - val_MinusLogProbMetric: 28.5448 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 488/1000
2023-10-12 06:26:34.705 
Epoch 488/1000 
	 loss: 27.3429, MinusLogProbMetric: 27.3429, val_loss: 28.5293, val_MinusLogProbMetric: 28.5293

Epoch 488: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3429 - MinusLogProbMetric: 27.3429 - val_loss: 28.5293 - val_MinusLogProbMetric: 28.5293 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 489/1000
2023-10-12 06:27:01.505 
Epoch 489/1000 
	 loss: 27.3240, MinusLogProbMetric: 27.3240, val_loss: 28.6161, val_MinusLogProbMetric: 28.6161

Epoch 489: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3240 - MinusLogProbMetric: 27.3240 - val_loss: 28.6161 - val_MinusLogProbMetric: 28.6161 - lr: 1.2500e-04 - 27s/epoch - 137ms/step
Epoch 490/1000
2023-10-12 06:27:27.773 
Epoch 490/1000 
	 loss: 27.3465, MinusLogProbMetric: 27.3465, val_loss: 28.6707, val_MinusLogProbMetric: 28.6707

Epoch 490: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3465 - MinusLogProbMetric: 27.3465 - val_loss: 28.6707 - val_MinusLogProbMetric: 28.6707 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 491/1000
2023-10-12 06:27:54.204 
Epoch 491/1000 
	 loss: 27.3407, MinusLogProbMetric: 27.3407, val_loss: 28.6917, val_MinusLogProbMetric: 28.6917

Epoch 491: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3407 - MinusLogProbMetric: 27.3407 - val_loss: 28.6917 - val_MinusLogProbMetric: 28.6917 - lr: 1.2500e-04 - 26s/epoch - 135ms/step
Epoch 492/1000
2023-10-12 06:28:19.730 
Epoch 492/1000 
	 loss: 27.3286, MinusLogProbMetric: 27.3286, val_loss: 28.7408, val_MinusLogProbMetric: 28.7408

Epoch 492: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3286 - MinusLogProbMetric: 27.3286 - val_loss: 28.7408 - val_MinusLogProbMetric: 28.7408 - lr: 1.2500e-04 - 26s/epoch - 130ms/step
Epoch 493/1000
2023-10-12 06:28:45.738 
Epoch 493/1000 
	 loss: 27.3374, MinusLogProbMetric: 27.3374, val_loss: 28.7672, val_MinusLogProbMetric: 28.7672

Epoch 493: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3374 - MinusLogProbMetric: 27.3374 - val_loss: 28.7672 - val_MinusLogProbMetric: 28.7672 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 494/1000
2023-10-12 06:29:11.179 
Epoch 494/1000 
	 loss: 27.4396, MinusLogProbMetric: 27.4396, val_loss: 28.5662, val_MinusLogProbMetric: 28.5662

Epoch 494: val_loss did not improve from 28.51967
196/196 - 25s - loss: 27.4396 - MinusLogProbMetric: 27.4396 - val_loss: 28.5662 - val_MinusLogProbMetric: 28.5662 - lr: 1.2500e-04 - 25s/epoch - 130ms/step
Epoch 495/1000
2023-10-12 06:29:36.634 
Epoch 495/1000 
	 loss: 27.3438, MinusLogProbMetric: 27.3438, val_loss: 28.6420, val_MinusLogProbMetric: 28.6420

Epoch 495: val_loss did not improve from 28.51967
196/196 - 25s - loss: 27.3438 - MinusLogProbMetric: 27.3438 - val_loss: 28.6420 - val_MinusLogProbMetric: 28.6420 - lr: 1.2500e-04 - 25s/epoch - 130ms/step
Epoch 496/1000
2023-10-12 06:30:02.505 
Epoch 496/1000 
	 loss: 27.3314, MinusLogProbMetric: 27.3314, val_loss: 28.6762, val_MinusLogProbMetric: 28.6762

Epoch 496: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3314 - MinusLogProbMetric: 27.3314 - val_loss: 28.6762 - val_MinusLogProbMetric: 28.6762 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 497/1000
2023-10-12 06:30:28.319 
Epoch 497/1000 
	 loss: 27.3558, MinusLogProbMetric: 27.3558, val_loss: 28.5359, val_MinusLogProbMetric: 28.5359

Epoch 497: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3558 - MinusLogProbMetric: 27.3558 - val_loss: 28.5359 - val_MinusLogProbMetric: 28.5359 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 498/1000
2023-10-12 06:30:54.297 
Epoch 498/1000 
	 loss: 27.3447, MinusLogProbMetric: 27.3447, val_loss: 28.5499, val_MinusLogProbMetric: 28.5499

Epoch 498: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3447 - MinusLogProbMetric: 27.3447 - val_loss: 28.5499 - val_MinusLogProbMetric: 28.5499 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 499/1000
2023-10-12 06:31:20.908 
Epoch 499/1000 
	 loss: 27.3217, MinusLogProbMetric: 27.3217, val_loss: 28.5896, val_MinusLogProbMetric: 28.5896

Epoch 499: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3217 - MinusLogProbMetric: 27.3217 - val_loss: 28.5896 - val_MinusLogProbMetric: 28.5896 - lr: 1.2500e-04 - 27s/epoch - 136ms/step
Epoch 500/1000
2023-10-12 06:31:48.040 
Epoch 500/1000 
	 loss: 27.3429, MinusLogProbMetric: 27.3429, val_loss: 28.5807, val_MinusLogProbMetric: 28.5807

Epoch 500: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3429 - MinusLogProbMetric: 27.3429 - val_loss: 28.5807 - val_MinusLogProbMetric: 28.5807 - lr: 1.2500e-04 - 27s/epoch - 138ms/step
Epoch 501/1000
2023-10-12 06:32:14.281 
Epoch 501/1000 
	 loss: 27.3373, MinusLogProbMetric: 27.3373, val_loss: 28.5722, val_MinusLogProbMetric: 28.5722

Epoch 501: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3373 - MinusLogProbMetric: 27.3373 - val_loss: 28.5722 - val_MinusLogProbMetric: 28.5722 - lr: 1.2500e-04 - 26s/epoch - 134ms/step
Epoch 502/1000
2023-10-12 06:32:39.917 
Epoch 502/1000 
	 loss: 27.3758, MinusLogProbMetric: 27.3758, val_loss: 28.5856, val_MinusLogProbMetric: 28.5856

Epoch 502: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3758 - MinusLogProbMetric: 27.3758 - val_loss: 28.5856 - val_MinusLogProbMetric: 28.5856 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 503/1000
2023-10-12 06:33:06.079 
Epoch 503/1000 
	 loss: 27.3249, MinusLogProbMetric: 27.3249, val_loss: 28.6819, val_MinusLogProbMetric: 28.6819

Epoch 503: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3249 - MinusLogProbMetric: 27.3249 - val_loss: 28.6819 - val_MinusLogProbMetric: 28.6819 - lr: 1.2500e-04 - 26s/epoch - 133ms/step
Epoch 504/1000
2023-10-12 06:33:31.902 
Epoch 504/1000 
	 loss: 27.3202, MinusLogProbMetric: 27.3202, val_loss: 28.5734, val_MinusLogProbMetric: 28.5734

Epoch 504: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3202 - MinusLogProbMetric: 27.3202 - val_loss: 28.5734 - val_MinusLogProbMetric: 28.5734 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 505/1000
2023-10-12 06:33:58.408 
Epoch 505/1000 
	 loss: 27.3508, MinusLogProbMetric: 27.3508, val_loss: 28.5349, val_MinusLogProbMetric: 28.5349

Epoch 505: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.3508 - MinusLogProbMetric: 27.3508 - val_loss: 28.5349 - val_MinusLogProbMetric: 28.5349 - lr: 1.2500e-04 - 27s/epoch - 135ms/step
Epoch 506/1000
2023-10-12 06:34:24.072 
Epoch 506/1000 
	 loss: 27.3378, MinusLogProbMetric: 27.3378, val_loss: 28.5575, val_MinusLogProbMetric: 28.5575

Epoch 506: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3378 - MinusLogProbMetric: 27.3378 - val_loss: 28.5575 - val_MinusLogProbMetric: 28.5575 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 507/1000
2023-10-12 06:34:49.690 
Epoch 507/1000 
	 loss: 27.3265, MinusLogProbMetric: 27.3265, val_loss: 28.5411, val_MinusLogProbMetric: 28.5411

Epoch 507: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3265 - MinusLogProbMetric: 27.3265 - val_loss: 28.5411 - val_MinusLogProbMetric: 28.5411 - lr: 1.2500e-04 - 26s/epoch - 131ms/step
Epoch 508/1000
2023-10-12 06:35:15.562 
Epoch 508/1000 
	 loss: 27.3506, MinusLogProbMetric: 27.3506, val_loss: 28.5755, val_MinusLogProbMetric: 28.5755

Epoch 508: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.3506 - MinusLogProbMetric: 27.3506 - val_loss: 28.5755 - val_MinusLogProbMetric: 28.5755 - lr: 1.2500e-04 - 26s/epoch - 132ms/step
Epoch 509/1000
2023-10-12 06:35:41.452 
Epoch 509/1000 
	 loss: 27.2562, MinusLogProbMetric: 27.2562, val_loss: 28.5296, val_MinusLogProbMetric: 28.5296

Epoch 509: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.2562 - MinusLogProbMetric: 27.2562 - val_loss: 28.5296 - val_MinusLogProbMetric: 28.5296 - lr: 6.2500e-05 - 26s/epoch - 132ms/step
Epoch 510/1000
2023-10-12 06:36:08.351 
Epoch 510/1000 
	 loss: 27.2635, MinusLogProbMetric: 27.2635, val_loss: 28.5225, val_MinusLogProbMetric: 28.5225

Epoch 510: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.2635 - MinusLogProbMetric: 27.2635 - val_loss: 28.5225 - val_MinusLogProbMetric: 28.5225 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 511/1000
2023-10-12 06:36:34.370 
Epoch 511/1000 
	 loss: 27.2567, MinusLogProbMetric: 27.2567, val_loss: 28.7100, val_MinusLogProbMetric: 28.7100

Epoch 511: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.2567 - MinusLogProbMetric: 27.2567 - val_loss: 28.7100 - val_MinusLogProbMetric: 28.7100 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 512/1000
2023-10-12 06:37:00.348 
Epoch 512/1000 
	 loss: 27.2632, MinusLogProbMetric: 27.2632, val_loss: 28.5852, val_MinusLogProbMetric: 28.5852

Epoch 512: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.2632 - MinusLogProbMetric: 27.2632 - val_loss: 28.5852 - val_MinusLogProbMetric: 28.5852 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 513/1000
2023-10-12 06:37:27.059 
Epoch 513/1000 
	 loss: 27.2593, MinusLogProbMetric: 27.2593, val_loss: 28.5449, val_MinusLogProbMetric: 28.5449

Epoch 513: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.2593 - MinusLogProbMetric: 27.2593 - val_loss: 28.5449 - val_MinusLogProbMetric: 28.5449 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 514/1000
2023-10-12 06:37:53.939 
Epoch 514/1000 
	 loss: 27.2574, MinusLogProbMetric: 27.2574, val_loss: 28.5346, val_MinusLogProbMetric: 28.5346

Epoch 514: val_loss did not improve from 28.51967
196/196 - 27s - loss: 27.2574 - MinusLogProbMetric: 27.2574 - val_loss: 28.5346 - val_MinusLogProbMetric: 28.5346 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 515/1000
2023-10-12 06:38:19.959 
Epoch 515/1000 
	 loss: 27.2521, MinusLogProbMetric: 27.2521, val_loss: 28.5344, val_MinusLogProbMetric: 28.5344

Epoch 515: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.2521 - MinusLogProbMetric: 27.2521 - val_loss: 28.5344 - val_MinusLogProbMetric: 28.5344 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 516/1000
2023-10-12 06:38:45.376 
Epoch 516/1000 
	 loss: 27.2485, MinusLogProbMetric: 27.2485, val_loss: 28.5742, val_MinusLogProbMetric: 28.5742

Epoch 516: val_loss did not improve from 28.51967
196/196 - 25s - loss: 27.2485 - MinusLogProbMetric: 27.2485 - val_loss: 28.5742 - val_MinusLogProbMetric: 28.5742 - lr: 6.2500e-05 - 25s/epoch - 130ms/step
Epoch 517/1000
2023-10-12 06:39:10.933 
Epoch 517/1000 
	 loss: 27.2441, MinusLogProbMetric: 27.2441, val_loss: 28.5212, val_MinusLogProbMetric: 28.5212

Epoch 517: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.2441 - MinusLogProbMetric: 27.2441 - val_loss: 28.5212 - val_MinusLogProbMetric: 28.5212 - lr: 6.2500e-05 - 26s/epoch - 130ms/step
Epoch 518/1000
2023-10-12 06:39:36.985 
Epoch 518/1000 
	 loss: 27.2523, MinusLogProbMetric: 27.2523, val_loss: 28.5222, val_MinusLogProbMetric: 28.5222

Epoch 518: val_loss did not improve from 28.51967
196/196 - 26s - loss: 27.2523 - MinusLogProbMetric: 27.2523 - val_loss: 28.5222 - val_MinusLogProbMetric: 28.5222 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 519/1000
2023-10-12 06:40:02.789 
Epoch 519/1000 
	 loss: 27.2535, MinusLogProbMetric: 27.2535, val_loss: 28.5073, val_MinusLogProbMetric: 28.5073

Epoch 519: val_loss improved from 28.51967 to 28.50731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 27.2535 - MinusLogProbMetric: 27.2535 - val_loss: 28.5073 - val_MinusLogProbMetric: 28.5073 - lr: 6.2500e-05 - 26s/epoch - 134ms/step
Epoch 520/1000
2023-10-12 06:40:31.351 
Epoch 520/1000 
	 loss: 27.2501, MinusLogProbMetric: 27.2501, val_loss: 28.5981, val_MinusLogProbMetric: 28.5981

Epoch 520: val_loss did not improve from 28.50731
196/196 - 28s - loss: 27.2501 - MinusLogProbMetric: 27.2501 - val_loss: 28.5981 - val_MinusLogProbMetric: 28.5981 - lr: 6.2500e-05 - 28s/epoch - 143ms/step
Epoch 521/1000
2023-10-12 06:40:58.154 
Epoch 521/1000 
	 loss: 27.2479, MinusLogProbMetric: 27.2479, val_loss: 28.5280, val_MinusLogProbMetric: 28.5280

Epoch 521: val_loss did not improve from 28.50731
196/196 - 27s - loss: 27.2479 - MinusLogProbMetric: 27.2479 - val_loss: 28.5280 - val_MinusLogProbMetric: 28.5280 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 522/1000
2023-10-12 06:41:25.990 
Epoch 522/1000 
	 loss: 27.2531, MinusLogProbMetric: 27.2531, val_loss: 28.5071, val_MinusLogProbMetric: 28.5071

Epoch 522: val_loss improved from 28.50731 to 28.50706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 28s - loss: 27.2531 - MinusLogProbMetric: 27.2531 - val_loss: 28.5071 - val_MinusLogProbMetric: 28.5071 - lr: 6.2500e-05 - 28s/epoch - 144ms/step
Epoch 523/1000
2023-10-12 06:41:53.746 
Epoch 523/1000 
	 loss: 27.2597, MinusLogProbMetric: 27.2597, val_loss: 28.5416, val_MinusLogProbMetric: 28.5416

Epoch 523: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2597 - MinusLogProbMetric: 27.2597 - val_loss: 28.5416 - val_MinusLogProbMetric: 28.5416 - lr: 6.2500e-05 - 27s/epoch - 139ms/step
Epoch 524/1000
2023-10-12 06:42:20.566 
Epoch 524/1000 
	 loss: 27.2601, MinusLogProbMetric: 27.2601, val_loss: 28.5163, val_MinusLogProbMetric: 28.5163

Epoch 524: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2601 - MinusLogProbMetric: 27.2601 - val_loss: 28.5163 - val_MinusLogProbMetric: 28.5163 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 525/1000
2023-10-12 06:42:47.234 
Epoch 525/1000 
	 loss: 27.2504, MinusLogProbMetric: 27.2504, val_loss: 28.5340, val_MinusLogProbMetric: 28.5340

Epoch 525: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2504 - MinusLogProbMetric: 27.2504 - val_loss: 28.5340 - val_MinusLogProbMetric: 28.5340 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 526/1000
2023-10-12 06:43:14.033 
Epoch 526/1000 
	 loss: 27.2463, MinusLogProbMetric: 27.2463, val_loss: 28.5301, val_MinusLogProbMetric: 28.5301

Epoch 526: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2463 - MinusLogProbMetric: 27.2463 - val_loss: 28.5301 - val_MinusLogProbMetric: 28.5301 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 527/1000
2023-10-12 06:43:40.104 
Epoch 527/1000 
	 loss: 27.2498, MinusLogProbMetric: 27.2498, val_loss: 28.6086, val_MinusLogProbMetric: 28.6086

Epoch 527: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2498 - MinusLogProbMetric: 27.2498 - val_loss: 28.6086 - val_MinusLogProbMetric: 28.6086 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 528/1000
2023-10-12 06:44:06.027 
Epoch 528/1000 
	 loss: 27.2453, MinusLogProbMetric: 27.2453, val_loss: 28.5153, val_MinusLogProbMetric: 28.5153

Epoch 528: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2453 - MinusLogProbMetric: 27.2453 - val_loss: 28.5153 - val_MinusLogProbMetric: 28.5153 - lr: 6.2500e-05 - 26s/epoch - 132ms/step
Epoch 529/1000
2023-10-12 06:44:31.644 
Epoch 529/1000 
	 loss: 27.2554, MinusLogProbMetric: 27.2554, val_loss: 28.5252, val_MinusLogProbMetric: 28.5252

Epoch 529: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2554 - MinusLogProbMetric: 27.2554 - val_loss: 28.5252 - val_MinusLogProbMetric: 28.5252 - lr: 6.2500e-05 - 26s/epoch - 131ms/step
Epoch 530/1000
2023-10-12 06:44:58.407 
Epoch 530/1000 
	 loss: 27.2564, MinusLogProbMetric: 27.2564, val_loss: 28.5497, val_MinusLogProbMetric: 28.5497

Epoch 530: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2564 - MinusLogProbMetric: 27.2564 - val_loss: 28.5497 - val_MinusLogProbMetric: 28.5497 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 531/1000
2023-10-12 06:45:25.333 
Epoch 531/1000 
	 loss: 27.2491, MinusLogProbMetric: 27.2491, val_loss: 28.5174, val_MinusLogProbMetric: 28.5174

Epoch 531: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2491 - MinusLogProbMetric: 27.2491 - val_loss: 28.5174 - val_MinusLogProbMetric: 28.5174 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 532/1000
2023-10-12 06:45:52.255 
Epoch 532/1000 
	 loss: 27.2551, MinusLogProbMetric: 27.2551, val_loss: 28.5735, val_MinusLogProbMetric: 28.5735

Epoch 532: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2551 - MinusLogProbMetric: 27.2551 - val_loss: 28.5735 - val_MinusLogProbMetric: 28.5735 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 533/1000
2023-10-12 06:46:20.035 
Epoch 533/1000 
	 loss: 27.2453, MinusLogProbMetric: 27.2453, val_loss: 28.5354, val_MinusLogProbMetric: 28.5354

Epoch 533: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2453 - MinusLogProbMetric: 27.2453 - val_loss: 28.5354 - val_MinusLogProbMetric: 28.5354 - lr: 6.2500e-05 - 28s/epoch - 142ms/step
Epoch 534/1000
2023-10-12 06:46:47.759 
Epoch 534/1000 
	 loss: 27.2386, MinusLogProbMetric: 27.2386, val_loss: 28.5512, val_MinusLogProbMetric: 28.5512

Epoch 534: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2386 - MinusLogProbMetric: 27.2386 - val_loss: 28.5512 - val_MinusLogProbMetric: 28.5512 - lr: 6.2500e-05 - 28s/epoch - 141ms/step
Epoch 535/1000
2023-10-12 06:47:15.902 
Epoch 535/1000 
	 loss: 27.2464, MinusLogProbMetric: 27.2464, val_loss: 28.5314, val_MinusLogProbMetric: 28.5314

Epoch 535: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2464 - MinusLogProbMetric: 27.2464 - val_loss: 28.5314 - val_MinusLogProbMetric: 28.5314 - lr: 6.2500e-05 - 28s/epoch - 144ms/step
Epoch 536/1000
2023-10-12 06:47:41.508 
Epoch 536/1000 
	 loss: 27.2423, MinusLogProbMetric: 27.2423, val_loss: 28.5260, val_MinusLogProbMetric: 28.5260

Epoch 536: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2423 - MinusLogProbMetric: 27.2423 - val_loss: 28.5260 - val_MinusLogProbMetric: 28.5260 - lr: 6.2500e-05 - 26s/epoch - 131ms/step
Epoch 537/1000
2023-10-12 06:48:07.404 
Epoch 537/1000 
	 loss: 27.2539, MinusLogProbMetric: 27.2539, val_loss: 28.5472, val_MinusLogProbMetric: 28.5472

Epoch 537: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2539 - MinusLogProbMetric: 27.2539 - val_loss: 28.5472 - val_MinusLogProbMetric: 28.5472 - lr: 6.2500e-05 - 26s/epoch - 132ms/step
Epoch 538/1000
2023-10-12 06:48:34.061 
Epoch 538/1000 
	 loss: 27.2470, MinusLogProbMetric: 27.2470, val_loss: 28.5132, val_MinusLogProbMetric: 28.5132

Epoch 538: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2470 - MinusLogProbMetric: 27.2470 - val_loss: 28.5132 - val_MinusLogProbMetric: 28.5132 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 539/1000
2023-10-12 06:49:00.980 
Epoch 539/1000 
	 loss: 27.2419, MinusLogProbMetric: 27.2419, val_loss: 28.6235, val_MinusLogProbMetric: 28.6235

Epoch 539: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2419 - MinusLogProbMetric: 27.2419 - val_loss: 28.6235 - val_MinusLogProbMetric: 28.6235 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 540/1000
2023-10-12 06:49:28.133 
Epoch 540/1000 
	 loss: 27.2506, MinusLogProbMetric: 27.2506, val_loss: 28.5228, val_MinusLogProbMetric: 28.5228

Epoch 540: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2506 - MinusLogProbMetric: 27.2506 - val_loss: 28.5228 - val_MinusLogProbMetric: 28.5228 - lr: 6.2500e-05 - 27s/epoch - 139ms/step
Epoch 541/1000
2023-10-12 06:49:55.987 
Epoch 541/1000 
	 loss: 27.2483, MinusLogProbMetric: 27.2483, val_loss: 28.5109, val_MinusLogProbMetric: 28.5109

Epoch 541: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2483 - MinusLogProbMetric: 27.2483 - val_loss: 28.5109 - val_MinusLogProbMetric: 28.5109 - lr: 6.2500e-05 - 28s/epoch - 142ms/step
Epoch 542/1000
2023-10-12 06:50:23.254 
Epoch 542/1000 
	 loss: 27.2497, MinusLogProbMetric: 27.2497, val_loss: 28.5243, val_MinusLogProbMetric: 28.5243

Epoch 542: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2497 - MinusLogProbMetric: 27.2497 - val_loss: 28.5243 - val_MinusLogProbMetric: 28.5243 - lr: 6.2500e-05 - 27s/epoch - 139ms/step
Epoch 543/1000
2023-10-12 06:50:51.527 
Epoch 543/1000 
	 loss: 27.2522, MinusLogProbMetric: 27.2522, val_loss: 28.5319, val_MinusLogProbMetric: 28.5319

Epoch 543: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2522 - MinusLogProbMetric: 27.2522 - val_loss: 28.5319 - val_MinusLogProbMetric: 28.5319 - lr: 6.2500e-05 - 28s/epoch - 144ms/step
Epoch 544/1000
2023-10-12 06:51:19.041 
Epoch 544/1000 
	 loss: 27.2493, MinusLogProbMetric: 27.2493, val_loss: 28.5177, val_MinusLogProbMetric: 28.5177

Epoch 544: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2493 - MinusLogProbMetric: 27.2493 - val_loss: 28.5177 - val_MinusLogProbMetric: 28.5177 - lr: 6.2500e-05 - 28s/epoch - 140ms/step
Epoch 545/1000
2023-10-12 06:51:46.174 
Epoch 545/1000 
	 loss: 27.2523, MinusLogProbMetric: 27.2523, val_loss: 28.5536, val_MinusLogProbMetric: 28.5536

Epoch 545: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2523 - MinusLogProbMetric: 27.2523 - val_loss: 28.5536 - val_MinusLogProbMetric: 28.5536 - lr: 6.2500e-05 - 27s/epoch - 138ms/step
Epoch 546/1000
2023-10-12 06:52:13.287 
Epoch 546/1000 
	 loss: 27.2575, MinusLogProbMetric: 27.2575, val_loss: 28.5444, val_MinusLogProbMetric: 28.5444

Epoch 546: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2575 - MinusLogProbMetric: 27.2575 - val_loss: 28.5444 - val_MinusLogProbMetric: 28.5444 - lr: 6.2500e-05 - 27s/epoch - 138ms/step
Epoch 547/1000
2023-10-12 06:52:40.525 
Epoch 547/1000 
	 loss: 27.2409, MinusLogProbMetric: 27.2409, val_loss: 28.5292, val_MinusLogProbMetric: 28.5292

Epoch 547: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2409 - MinusLogProbMetric: 27.2409 - val_loss: 28.5292 - val_MinusLogProbMetric: 28.5292 - lr: 6.2500e-05 - 27s/epoch - 139ms/step
Epoch 548/1000
2023-10-12 06:53:08.480 
Epoch 548/1000 
	 loss: 27.2425, MinusLogProbMetric: 27.2425, val_loss: 28.5355, val_MinusLogProbMetric: 28.5355

Epoch 548: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2425 - MinusLogProbMetric: 27.2425 - val_loss: 28.5355 - val_MinusLogProbMetric: 28.5355 - lr: 6.2500e-05 - 28s/epoch - 143ms/step
Epoch 549/1000
2023-10-12 06:53:35.189 
Epoch 549/1000 
	 loss: 27.2444, MinusLogProbMetric: 27.2444, val_loss: 28.5157, val_MinusLogProbMetric: 28.5157

Epoch 549: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2444 - MinusLogProbMetric: 27.2444 - val_loss: 28.5157 - val_MinusLogProbMetric: 28.5157 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 550/1000
2023-10-12 06:54:02.114 
Epoch 550/1000 
	 loss: 27.2462, MinusLogProbMetric: 27.2462, val_loss: 28.5201, val_MinusLogProbMetric: 28.5201

Epoch 550: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2462 - MinusLogProbMetric: 27.2462 - val_loss: 28.5201 - val_MinusLogProbMetric: 28.5201 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 551/1000
2023-10-12 06:54:28.716 
Epoch 551/1000 
	 loss: 27.2624, MinusLogProbMetric: 27.2624, val_loss: 28.5694, val_MinusLogProbMetric: 28.5694

Epoch 551: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2624 - MinusLogProbMetric: 27.2624 - val_loss: 28.5694 - val_MinusLogProbMetric: 28.5694 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 552/1000
2023-10-12 06:54:54.713 
Epoch 552/1000 
	 loss: 27.2427, MinusLogProbMetric: 27.2427, val_loss: 28.5526, val_MinusLogProbMetric: 28.5526

Epoch 552: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2427 - MinusLogProbMetric: 27.2427 - val_loss: 28.5526 - val_MinusLogProbMetric: 28.5526 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 553/1000
2023-10-12 06:55:21.690 
Epoch 553/1000 
	 loss: 27.2493, MinusLogProbMetric: 27.2493, val_loss: 28.5417, val_MinusLogProbMetric: 28.5417

Epoch 553: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2493 - MinusLogProbMetric: 27.2493 - val_loss: 28.5417 - val_MinusLogProbMetric: 28.5417 - lr: 6.2500e-05 - 27s/epoch - 138ms/step
Epoch 554/1000
2023-10-12 06:55:48.581 
Epoch 554/1000 
	 loss: 27.2367, MinusLogProbMetric: 27.2367, val_loss: 28.5100, val_MinusLogProbMetric: 28.5100

Epoch 554: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2367 - MinusLogProbMetric: 27.2367 - val_loss: 28.5100 - val_MinusLogProbMetric: 28.5100 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 555/1000
2023-10-12 06:56:14.571 
Epoch 555/1000 
	 loss: 27.2512, MinusLogProbMetric: 27.2512, val_loss: 28.5090, val_MinusLogProbMetric: 28.5090

Epoch 555: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2512 - MinusLogProbMetric: 27.2512 - val_loss: 28.5090 - val_MinusLogProbMetric: 28.5090 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 556/1000
2023-10-12 06:56:41.276 
Epoch 556/1000 
	 loss: 27.2489, MinusLogProbMetric: 27.2489, val_loss: 28.6142, val_MinusLogProbMetric: 28.6142

Epoch 556: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2489 - MinusLogProbMetric: 27.2489 - val_loss: 28.6142 - val_MinusLogProbMetric: 28.6142 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 557/1000
2023-10-12 06:57:08.380 
Epoch 557/1000 
	 loss: 27.2438, MinusLogProbMetric: 27.2438, val_loss: 28.6436, val_MinusLogProbMetric: 28.6436

Epoch 557: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2438 - MinusLogProbMetric: 27.2438 - val_loss: 28.6436 - val_MinusLogProbMetric: 28.6436 - lr: 6.2500e-05 - 27s/epoch - 138ms/step
Epoch 558/1000
2023-10-12 06:57:36.265 
Epoch 558/1000 
	 loss: 27.2462, MinusLogProbMetric: 27.2462, val_loss: 28.5155, val_MinusLogProbMetric: 28.5155

Epoch 558: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2462 - MinusLogProbMetric: 27.2462 - val_loss: 28.5155 - val_MinusLogProbMetric: 28.5155 - lr: 6.2500e-05 - 28s/epoch - 142ms/step
Epoch 559/1000
2023-10-12 06:58:03.258 
Epoch 559/1000 
	 loss: 27.2526, MinusLogProbMetric: 27.2526, val_loss: 28.5172, val_MinusLogProbMetric: 28.5172

Epoch 559: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2526 - MinusLogProbMetric: 27.2526 - val_loss: 28.5172 - val_MinusLogProbMetric: 28.5172 - lr: 6.2500e-05 - 27s/epoch - 138ms/step
Epoch 560/1000
2023-10-12 06:58:31.527 
Epoch 560/1000 
	 loss: 27.2377, MinusLogProbMetric: 27.2377, val_loss: 28.5420, val_MinusLogProbMetric: 28.5420

Epoch 560: val_loss did not improve from 28.50706
196/196 - 28s - loss: 27.2377 - MinusLogProbMetric: 27.2377 - val_loss: 28.5420 - val_MinusLogProbMetric: 28.5420 - lr: 6.2500e-05 - 28s/epoch - 144ms/step
Epoch 561/1000
2023-10-12 06:58:58.437 
Epoch 561/1000 
	 loss: 27.2407, MinusLogProbMetric: 27.2407, val_loss: 28.5237, val_MinusLogProbMetric: 28.5237

Epoch 561: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2407 - MinusLogProbMetric: 27.2407 - val_loss: 28.5237 - val_MinusLogProbMetric: 28.5237 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 562/1000
2023-10-12 06:59:25.346 
Epoch 562/1000 
	 loss: 27.2387, MinusLogProbMetric: 27.2387, val_loss: 28.5485, val_MinusLogProbMetric: 28.5485

Epoch 562: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2387 - MinusLogProbMetric: 27.2387 - val_loss: 28.5485 - val_MinusLogProbMetric: 28.5485 - lr: 6.2500e-05 - 27s/epoch - 137ms/step
Epoch 563/1000
2023-10-12 06:59:52.091 
Epoch 563/1000 
	 loss: 27.2362, MinusLogProbMetric: 27.2362, val_loss: 28.5191, val_MinusLogProbMetric: 28.5191

Epoch 563: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2362 - MinusLogProbMetric: 27.2362 - val_loss: 28.5191 - val_MinusLogProbMetric: 28.5191 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 564/1000
2023-10-12 07:00:18.723 
Epoch 564/1000 
	 loss: 27.2437, MinusLogProbMetric: 27.2437, val_loss: 28.5081, val_MinusLogProbMetric: 28.5081

Epoch 564: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2437 - MinusLogProbMetric: 27.2437 - val_loss: 28.5081 - val_MinusLogProbMetric: 28.5081 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 565/1000
2023-10-12 07:00:44.557 
Epoch 565/1000 
	 loss: 27.2396, MinusLogProbMetric: 27.2396, val_loss: 28.5371, val_MinusLogProbMetric: 28.5371

Epoch 565: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2396 - MinusLogProbMetric: 27.2396 - val_loss: 28.5371 - val_MinusLogProbMetric: 28.5371 - lr: 6.2500e-05 - 26s/epoch - 132ms/step
Epoch 566/1000
2023-10-12 07:01:10.502 
Epoch 566/1000 
	 loss: 27.2354, MinusLogProbMetric: 27.2354, val_loss: 28.5396, val_MinusLogProbMetric: 28.5396

Epoch 566: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2354 - MinusLogProbMetric: 27.2354 - val_loss: 28.5396 - val_MinusLogProbMetric: 28.5396 - lr: 6.2500e-05 - 26s/epoch - 132ms/step
Epoch 567/1000
2023-10-12 07:01:36.243 
Epoch 567/1000 
	 loss: 27.2367, MinusLogProbMetric: 27.2367, val_loss: 28.5941, val_MinusLogProbMetric: 28.5941

Epoch 567: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2367 - MinusLogProbMetric: 27.2367 - val_loss: 28.5941 - val_MinusLogProbMetric: 28.5941 - lr: 6.2500e-05 - 26s/epoch - 131ms/step
Epoch 568/1000
2023-10-12 07:02:02.590 
Epoch 568/1000 
	 loss: 27.2364, MinusLogProbMetric: 27.2364, val_loss: 28.5629, val_MinusLogProbMetric: 28.5629

Epoch 568: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2364 - MinusLogProbMetric: 27.2364 - val_loss: 28.5629 - val_MinusLogProbMetric: 28.5629 - lr: 6.2500e-05 - 26s/epoch - 134ms/step
Epoch 569/1000
2023-10-12 07:02:28.721 
Epoch 569/1000 
	 loss: 27.2411, MinusLogProbMetric: 27.2411, val_loss: 28.5298, val_MinusLogProbMetric: 28.5298

Epoch 569: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2411 - MinusLogProbMetric: 27.2411 - val_loss: 28.5298 - val_MinusLogProbMetric: 28.5298 - lr: 6.2500e-05 - 26s/epoch - 133ms/step
Epoch 570/1000
2023-10-12 07:02:55.342 
Epoch 570/1000 
	 loss: 27.2349, MinusLogProbMetric: 27.2349, val_loss: 28.5274, val_MinusLogProbMetric: 28.5274

Epoch 570: val_loss did not improve from 28.50706
196/196 - 27s - loss: 27.2349 - MinusLogProbMetric: 27.2349 - val_loss: 28.5274 - val_MinusLogProbMetric: 28.5274 - lr: 6.2500e-05 - 27s/epoch - 136ms/step
Epoch 571/1000
2023-10-12 07:03:21.729 
Epoch 571/1000 
	 loss: 27.2434, MinusLogProbMetric: 27.2434, val_loss: 28.5401, val_MinusLogProbMetric: 28.5401

Epoch 571: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2434 - MinusLogProbMetric: 27.2434 - val_loss: 28.5401 - val_MinusLogProbMetric: 28.5401 - lr: 6.2500e-05 - 26s/epoch - 135ms/step
Epoch 572/1000
2023-10-12 07:03:47.181 
Epoch 572/1000 
	 loss: 27.2606, MinusLogProbMetric: 27.2606, val_loss: 28.8295, val_MinusLogProbMetric: 28.8295

Epoch 572: val_loss did not improve from 28.50706
196/196 - 25s - loss: 27.2606 - MinusLogProbMetric: 27.2606 - val_loss: 28.8295 - val_MinusLogProbMetric: 28.8295 - lr: 6.2500e-05 - 25s/epoch - 130ms/step
Epoch 573/1000
2023-10-12 07:04:12.947 
Epoch 573/1000 
	 loss: 27.2189, MinusLogProbMetric: 27.2189, val_loss: 28.5472, val_MinusLogProbMetric: 28.5472

Epoch 573: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2189 - MinusLogProbMetric: 27.2189 - val_loss: 28.5472 - val_MinusLogProbMetric: 28.5472 - lr: 3.1250e-05 - 26s/epoch - 131ms/step
Epoch 574/1000
2023-10-12 07:04:39.059 
Epoch 574/1000 
	 loss: 27.2128, MinusLogProbMetric: 27.2128, val_loss: 28.5155, val_MinusLogProbMetric: 28.5155

Epoch 574: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2128 - MinusLogProbMetric: 27.2128 - val_loss: 28.5155 - val_MinusLogProbMetric: 28.5155 - lr: 3.1250e-05 - 26s/epoch - 133ms/step
Epoch 575/1000
2023-10-12 07:05:04.817 
Epoch 575/1000 
	 loss: 27.2119, MinusLogProbMetric: 27.2119, val_loss: 28.5394, val_MinusLogProbMetric: 28.5394

Epoch 575: val_loss did not improve from 28.50706
196/196 - 26s - loss: 27.2119 - MinusLogProbMetric: 27.2119 - val_loss: 28.5394 - val_MinusLogProbMetric: 28.5394 - lr: 3.1250e-05 - 26s/epoch - 131ms/step
Epoch 576/1000
2023-10-12 07:05:30.697 
Epoch 576/1000 
	 loss: 27.2066, MinusLogProbMetric: 27.2066, val_loss: 28.5031, val_MinusLogProbMetric: 28.5031

Epoch 576: val_loss improved from 28.50706 to 28.50309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 27.2066 - MinusLogProbMetric: 27.2066 - val_loss: 28.5031 - val_MinusLogProbMetric: 28.5031 - lr: 3.1250e-05 - 26s/epoch - 134ms/step
Epoch 577/1000
2023-10-12 07:05:57.095 
Epoch 577/1000 
	 loss: 27.2083, MinusLogProbMetric: 27.2083, val_loss: 28.4991, val_MinusLogProbMetric: 28.4991

Epoch 577: val_loss improved from 28.50309 to 28.49910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 26s - loss: 27.2083 - MinusLogProbMetric: 27.2083 - val_loss: 28.4991 - val_MinusLogProbMetric: 28.4991 - lr: 3.1250e-05 - 26s/epoch - 135ms/step
Epoch 578/1000
2023-10-12 07:06:23.568 
Epoch 578/1000 
	 loss: 27.2042, MinusLogProbMetric: 27.2042, val_loss: 28.4937, val_MinusLogProbMetric: 28.4937

Epoch 578: val_loss improved from 28.49910 to 28.49374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5
196/196 - 27s - loss: 27.2042 - MinusLogProbMetric: 27.2042 - val_loss: 28.4937 - val_MinusLogProbMetric: 28.4937 - lr: 3.1250e-05 - 27s/epoch - 135ms/step
Epoch 579/1000
2023-10-12 07:06:50.054 
Epoch 579/1000 
	 loss: 27.2075, MinusLogProbMetric: 27.2075, val_loss: 28.5021, val_MinusLogProbMetric: 28.5021

Epoch 579: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2075 - MinusLogProbMetric: 27.2075 - val_loss: 28.5021 - val_MinusLogProbMetric: 28.5021 - lr: 3.1250e-05 - 26s/epoch - 132ms/step
Epoch 580/1000
2023-10-12 07:07:17.321 
Epoch 580/1000 
	 loss: 27.2054, MinusLogProbMetric: 27.2054, val_loss: 28.5081, val_MinusLogProbMetric: 28.5081

Epoch 580: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2054 - MinusLogProbMetric: 27.2054 - val_loss: 28.5081 - val_MinusLogProbMetric: 28.5081 - lr: 3.1250e-05 - 27s/epoch - 139ms/step
Epoch 581/1000
2023-10-12 07:07:44.287 
Epoch 581/1000 
	 loss: 27.2072, MinusLogProbMetric: 27.2072, val_loss: 28.5580, val_MinusLogProbMetric: 28.5580

Epoch 581: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2072 - MinusLogProbMetric: 27.2072 - val_loss: 28.5580 - val_MinusLogProbMetric: 28.5580 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 582/1000
2023-10-12 07:08:11.188 
Epoch 582/1000 
	 loss: 27.2115, MinusLogProbMetric: 27.2115, val_loss: 28.4989, val_MinusLogProbMetric: 28.4989

Epoch 582: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2115 - MinusLogProbMetric: 27.2115 - val_loss: 28.4989 - val_MinusLogProbMetric: 28.4989 - lr: 3.1250e-05 - 27s/epoch - 137ms/step
Epoch 583/1000
2023-10-12 07:08:38.406 
Epoch 583/1000 
	 loss: 27.2070, MinusLogProbMetric: 27.2070, val_loss: 28.5131, val_MinusLogProbMetric: 28.5131

Epoch 583: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2070 - MinusLogProbMetric: 27.2070 - val_loss: 28.5131 - val_MinusLogProbMetric: 28.5131 - lr: 3.1250e-05 - 27s/epoch - 139ms/step
Epoch 584/1000
2023-10-12 07:09:05.765 
Epoch 584/1000 
	 loss: 27.2110, MinusLogProbMetric: 27.2110, val_loss: 28.5136, val_MinusLogProbMetric: 28.5136

Epoch 584: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2110 - MinusLogProbMetric: 27.2110 - val_loss: 28.5136 - val_MinusLogProbMetric: 28.5136 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 585/1000
2023-10-12 07:09:32.313 
Epoch 585/1000 
	 loss: 27.2064, MinusLogProbMetric: 27.2064, val_loss: 28.5045, val_MinusLogProbMetric: 28.5045

Epoch 585: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2064 - MinusLogProbMetric: 27.2064 - val_loss: 28.5045 - val_MinusLogProbMetric: 28.5045 - lr: 3.1250e-05 - 27s/epoch - 135ms/step
Epoch 586/1000
2023-10-12 07:09:58.293 
Epoch 586/1000 
	 loss: 27.2084, MinusLogProbMetric: 27.2084, val_loss: 28.5432, val_MinusLogProbMetric: 28.5432

Epoch 586: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2084 - MinusLogProbMetric: 27.2084 - val_loss: 28.5432 - val_MinusLogProbMetric: 28.5432 - lr: 3.1250e-05 - 26s/epoch - 133ms/step
Epoch 587/1000
2023-10-12 07:10:24.091 
Epoch 587/1000 
	 loss: 27.2066, MinusLogProbMetric: 27.2066, val_loss: 28.5656, val_MinusLogProbMetric: 28.5656

Epoch 587: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2066 - MinusLogProbMetric: 27.2066 - val_loss: 28.5656 - val_MinusLogProbMetric: 28.5656 - lr: 3.1250e-05 - 26s/epoch - 132ms/step
Epoch 588/1000
2023-10-12 07:10:51.088 
Epoch 588/1000 
	 loss: 27.2092, MinusLogProbMetric: 27.2092, val_loss: 28.5546, val_MinusLogProbMetric: 28.5546

Epoch 588: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2092 - MinusLogProbMetric: 27.2092 - val_loss: 28.5546 - val_MinusLogProbMetric: 28.5546 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 589/1000
2023-10-12 07:11:17.346 
Epoch 589/1000 
	 loss: 27.2066, MinusLogProbMetric: 27.2066, val_loss: 28.5162, val_MinusLogProbMetric: 28.5162

Epoch 589: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2066 - MinusLogProbMetric: 27.2066 - val_loss: 28.5162 - val_MinusLogProbMetric: 28.5162 - lr: 3.1250e-05 - 26s/epoch - 134ms/step
Epoch 590/1000
2023-10-12 07:11:43.768 
Epoch 590/1000 
	 loss: 27.2048, MinusLogProbMetric: 27.2048, val_loss: 28.5310, val_MinusLogProbMetric: 28.5310

Epoch 590: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2048 - MinusLogProbMetric: 27.2048 - val_loss: 28.5310 - val_MinusLogProbMetric: 28.5310 - lr: 3.1250e-05 - 26s/epoch - 135ms/step
Epoch 591/1000
2023-10-12 07:12:10.396 
Epoch 591/1000 
	 loss: 27.2082, MinusLogProbMetric: 27.2082, val_loss: 28.5090, val_MinusLogProbMetric: 28.5090

Epoch 591: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2082 - MinusLogProbMetric: 27.2082 - val_loss: 28.5090 - val_MinusLogProbMetric: 28.5090 - lr: 3.1250e-05 - 27s/epoch - 136ms/step
Epoch 592/1000
2023-10-12 07:12:36.941 
Epoch 592/1000 
	 loss: 27.2123, MinusLogProbMetric: 27.2123, val_loss: 28.5469, val_MinusLogProbMetric: 28.5469

Epoch 592: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2123 - MinusLogProbMetric: 27.2123 - val_loss: 28.5469 - val_MinusLogProbMetric: 28.5469 - lr: 3.1250e-05 - 27s/epoch - 135ms/step
Epoch 593/1000
2023-10-12 07:13:03.665 
Epoch 593/1000 
	 loss: 27.2098, MinusLogProbMetric: 27.2098, val_loss: 28.5556, val_MinusLogProbMetric: 28.5556

Epoch 593: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2098 - MinusLogProbMetric: 27.2098 - val_loss: 28.5556 - val_MinusLogProbMetric: 28.5556 - lr: 3.1250e-05 - 27s/epoch - 136ms/step
Epoch 594/1000
2023-10-12 07:13:30.013 
Epoch 594/1000 
	 loss: 27.2065, MinusLogProbMetric: 27.2065, val_loss: 28.5296, val_MinusLogProbMetric: 28.5296

Epoch 594: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2065 - MinusLogProbMetric: 27.2065 - val_loss: 28.5296 - val_MinusLogProbMetric: 28.5296 - lr: 3.1250e-05 - 26s/epoch - 134ms/step
Epoch 595/1000
2023-10-12 07:13:56.928 
Epoch 595/1000 
	 loss: 27.2032, MinusLogProbMetric: 27.2032, val_loss: 28.5162, val_MinusLogProbMetric: 28.5162

Epoch 595: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2032 - MinusLogProbMetric: 27.2032 - val_loss: 28.5162 - val_MinusLogProbMetric: 28.5162 - lr: 3.1250e-05 - 27s/epoch - 137ms/step
Epoch 596/1000
2023-10-12 07:14:23.990 
Epoch 596/1000 
	 loss: 27.2046, MinusLogProbMetric: 27.2046, val_loss: 28.5164, val_MinusLogProbMetric: 28.5164

Epoch 596: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2046 - MinusLogProbMetric: 27.2046 - val_loss: 28.5164 - val_MinusLogProbMetric: 28.5164 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 597/1000
2023-10-12 07:14:51.145 
Epoch 597/1000 
	 loss: 27.2105, MinusLogProbMetric: 27.2105, val_loss: 28.5018, val_MinusLogProbMetric: 28.5018

Epoch 597: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2105 - MinusLogProbMetric: 27.2105 - val_loss: 28.5018 - val_MinusLogProbMetric: 28.5018 - lr: 3.1250e-05 - 27s/epoch - 139ms/step
Epoch 598/1000
2023-10-12 07:15:19.123 
Epoch 598/1000 
	 loss: 27.2075, MinusLogProbMetric: 27.2075, val_loss: 28.5066, val_MinusLogProbMetric: 28.5066

Epoch 598: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.2075 - MinusLogProbMetric: 27.2075 - val_loss: 28.5066 - val_MinusLogProbMetric: 28.5066 - lr: 3.1250e-05 - 28s/epoch - 143ms/step
Epoch 599/1000
2023-10-12 07:15:47.435 
Epoch 599/1000 
	 loss: 27.2075, MinusLogProbMetric: 27.2075, val_loss: 28.5119, val_MinusLogProbMetric: 28.5119

Epoch 599: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.2075 - MinusLogProbMetric: 27.2075 - val_loss: 28.5119 - val_MinusLogProbMetric: 28.5119 - lr: 3.1250e-05 - 28s/epoch - 144ms/step
Epoch 600/1000
2023-10-12 07:16:14.106 
Epoch 600/1000 
	 loss: 27.2056, MinusLogProbMetric: 27.2056, val_loss: 28.5070, val_MinusLogProbMetric: 28.5070

Epoch 600: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2056 - MinusLogProbMetric: 27.2056 - val_loss: 28.5070 - val_MinusLogProbMetric: 28.5070 - lr: 3.1250e-05 - 27s/epoch - 136ms/step
Epoch 601/1000
2023-10-12 07:16:40.408 
Epoch 601/1000 
	 loss: 27.2028, MinusLogProbMetric: 27.2028, val_loss: 28.5354, val_MinusLogProbMetric: 28.5354

Epoch 601: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2028 - MinusLogProbMetric: 27.2028 - val_loss: 28.5354 - val_MinusLogProbMetric: 28.5354 - lr: 3.1250e-05 - 26s/epoch - 134ms/step
Epoch 602/1000
2023-10-12 07:17:05.586 
Epoch 602/1000 
	 loss: 27.2061, MinusLogProbMetric: 27.2061, val_loss: 28.5117, val_MinusLogProbMetric: 28.5117

Epoch 602: val_loss did not improve from 28.49374
196/196 - 25s - loss: 27.2061 - MinusLogProbMetric: 27.2061 - val_loss: 28.5117 - val_MinusLogProbMetric: 28.5117 - lr: 3.1250e-05 - 25s/epoch - 128ms/step
Epoch 603/1000
2023-10-12 07:17:31.229 
Epoch 603/1000 
	 loss: 27.2053, MinusLogProbMetric: 27.2053, val_loss: 28.5056, val_MinusLogProbMetric: 28.5056

Epoch 603: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2053 - MinusLogProbMetric: 27.2053 - val_loss: 28.5056 - val_MinusLogProbMetric: 28.5056 - lr: 3.1250e-05 - 26s/epoch - 131ms/step
Epoch 604/1000
2023-10-12 07:17:56.423 
Epoch 604/1000 
	 loss: 27.2131, MinusLogProbMetric: 27.2131, val_loss: 28.5218, val_MinusLogProbMetric: 28.5218

Epoch 604: val_loss did not improve from 28.49374
196/196 - 25s - loss: 27.2131 - MinusLogProbMetric: 27.2131 - val_loss: 28.5218 - val_MinusLogProbMetric: 28.5218 - lr: 3.1250e-05 - 25s/epoch - 129ms/step
Epoch 605/1000
2023-10-12 07:18:22.325 
Epoch 605/1000 
	 loss: 27.2064, MinusLogProbMetric: 27.2064, val_loss: 28.5666, val_MinusLogProbMetric: 28.5666

Epoch 605: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2064 - MinusLogProbMetric: 27.2064 - val_loss: 28.5666 - val_MinusLogProbMetric: 28.5666 - lr: 3.1250e-05 - 26s/epoch - 132ms/step
Epoch 606/1000
2023-10-12 07:18:48.114 
Epoch 606/1000 
	 loss: 27.2071, MinusLogProbMetric: 27.2071, val_loss: 28.5250, val_MinusLogProbMetric: 28.5250

Epoch 606: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2071 - MinusLogProbMetric: 27.2071 - val_loss: 28.5250 - val_MinusLogProbMetric: 28.5250 - lr: 3.1250e-05 - 26s/epoch - 132ms/step
Epoch 607/1000
2023-10-12 07:19:14.936 
Epoch 607/1000 
	 loss: 27.2015, MinusLogProbMetric: 27.2015, val_loss: 28.5158, val_MinusLogProbMetric: 28.5158

Epoch 607: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2015 - MinusLogProbMetric: 27.2015 - val_loss: 28.5158 - val_MinusLogProbMetric: 28.5158 - lr: 3.1250e-05 - 27s/epoch - 137ms/step
Epoch 608/1000
2023-10-12 07:19:41.694 
Epoch 608/1000 
	 loss: 27.2039, MinusLogProbMetric: 27.2039, val_loss: 28.5057, val_MinusLogProbMetric: 28.5057

Epoch 608: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2039 - MinusLogProbMetric: 27.2039 - val_loss: 28.5057 - val_MinusLogProbMetric: 28.5057 - lr: 3.1250e-05 - 27s/epoch - 137ms/step
Epoch 609/1000
2023-10-12 07:20:09.743 
Epoch 609/1000 
	 loss: 27.2048, MinusLogProbMetric: 27.2048, val_loss: 28.5207, val_MinusLogProbMetric: 28.5207

Epoch 609: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.2048 - MinusLogProbMetric: 27.2048 - val_loss: 28.5207 - val_MinusLogProbMetric: 28.5207 - lr: 3.1250e-05 - 28s/epoch - 143ms/step
Epoch 610/1000
2023-10-12 07:20:36.752 
Epoch 610/1000 
	 loss: 27.2045, MinusLogProbMetric: 27.2045, val_loss: 28.5134, val_MinusLogProbMetric: 28.5134

Epoch 610: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2045 - MinusLogProbMetric: 27.2045 - val_loss: 28.5134 - val_MinusLogProbMetric: 28.5134 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 611/1000
2023-10-12 07:21:04.397 
Epoch 611/1000 
	 loss: 27.2029, MinusLogProbMetric: 27.2029, val_loss: 28.5103, val_MinusLogProbMetric: 28.5103

Epoch 611: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.2029 - MinusLogProbMetric: 27.2029 - val_loss: 28.5103 - val_MinusLogProbMetric: 28.5103 - lr: 3.1250e-05 - 28s/epoch - 141ms/step
Epoch 612/1000
2023-10-12 07:21:31.123 
Epoch 612/1000 
	 loss: 27.2072, MinusLogProbMetric: 27.2072, val_loss: 28.5343, val_MinusLogProbMetric: 28.5343

Epoch 612: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2072 - MinusLogProbMetric: 27.2072 - val_loss: 28.5343 - val_MinusLogProbMetric: 28.5343 - lr: 3.1250e-05 - 27s/epoch - 136ms/step
Epoch 613/1000
2023-10-12 07:21:58.544 
Epoch 613/1000 
	 loss: 27.2091, MinusLogProbMetric: 27.2091, val_loss: 28.5274, val_MinusLogProbMetric: 28.5274

Epoch 613: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2091 - MinusLogProbMetric: 27.2091 - val_loss: 28.5274 - val_MinusLogProbMetric: 28.5274 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 614/1000
2023-10-12 07:22:26.422 
Epoch 614/1000 
	 loss: 27.2033, MinusLogProbMetric: 27.2033, val_loss: 28.5319, val_MinusLogProbMetric: 28.5319

Epoch 614: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.2033 - MinusLogProbMetric: 27.2033 - val_loss: 28.5319 - val_MinusLogProbMetric: 28.5319 - lr: 3.1250e-05 - 28s/epoch - 142ms/step
Epoch 615/1000
2023-10-12 07:22:53.719 
Epoch 615/1000 
	 loss: 27.2079, MinusLogProbMetric: 27.2079, val_loss: 28.5096, val_MinusLogProbMetric: 28.5096

Epoch 615: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2079 - MinusLogProbMetric: 27.2079 - val_loss: 28.5096 - val_MinusLogProbMetric: 28.5096 - lr: 3.1250e-05 - 27s/epoch - 139ms/step
Epoch 616/1000
2023-10-12 07:23:20.757 
Epoch 616/1000 
	 loss: 27.2029, MinusLogProbMetric: 27.2029, val_loss: 28.5070, val_MinusLogProbMetric: 28.5070

Epoch 616: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2029 - MinusLogProbMetric: 27.2029 - val_loss: 28.5070 - val_MinusLogProbMetric: 28.5070 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 617/1000
2023-10-12 07:23:48.022 
Epoch 617/1000 
	 loss: 27.2041, MinusLogProbMetric: 27.2041, val_loss: 28.5136, val_MinusLogProbMetric: 28.5136

Epoch 617: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2041 - MinusLogProbMetric: 27.2041 - val_loss: 28.5136 - val_MinusLogProbMetric: 28.5136 - lr: 3.1250e-05 - 27s/epoch - 139ms/step
Epoch 618/1000
2023-10-12 07:24:15.082 
Epoch 618/1000 
	 loss: 27.2064, MinusLogProbMetric: 27.2064, val_loss: 28.5306, val_MinusLogProbMetric: 28.5306

Epoch 618: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2064 - MinusLogProbMetric: 27.2064 - val_loss: 28.5306 - val_MinusLogProbMetric: 28.5306 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 619/1000
2023-10-12 07:24:42.433 
Epoch 619/1000 
	 loss: 27.2146, MinusLogProbMetric: 27.2146, val_loss: 28.5304, val_MinusLogProbMetric: 28.5304

Epoch 619: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2146 - MinusLogProbMetric: 27.2146 - val_loss: 28.5304 - val_MinusLogProbMetric: 28.5304 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 620/1000
2023-10-12 07:25:09.807 
Epoch 620/1000 
	 loss: 27.2058, MinusLogProbMetric: 27.2058, val_loss: 28.5186, val_MinusLogProbMetric: 28.5186

Epoch 620: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2058 - MinusLogProbMetric: 27.2058 - val_loss: 28.5186 - val_MinusLogProbMetric: 28.5186 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 621/1000
2023-10-12 07:25:35.975 
Epoch 621/1000 
	 loss: 27.2092, MinusLogProbMetric: 27.2092, val_loss: 28.5131, val_MinusLogProbMetric: 28.5131

Epoch 621: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2092 - MinusLogProbMetric: 27.2092 - val_loss: 28.5131 - val_MinusLogProbMetric: 28.5131 - lr: 3.1250e-05 - 26s/epoch - 133ms/step
Epoch 622/1000
2023-10-12 07:26:01.973 
Epoch 622/1000 
	 loss: 27.2087, MinusLogProbMetric: 27.2087, val_loss: 28.5065, val_MinusLogProbMetric: 28.5065

Epoch 622: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2087 - MinusLogProbMetric: 27.2087 - val_loss: 28.5065 - val_MinusLogProbMetric: 28.5065 - lr: 3.1250e-05 - 26s/epoch - 133ms/step
Epoch 623/1000
2023-10-12 07:26:28.632 
Epoch 623/1000 
	 loss: 27.2068, MinusLogProbMetric: 27.2068, val_loss: 28.5142, val_MinusLogProbMetric: 28.5142

Epoch 623: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2068 - MinusLogProbMetric: 27.2068 - val_loss: 28.5142 - val_MinusLogProbMetric: 28.5142 - lr: 3.1250e-05 - 27s/epoch - 136ms/step
Epoch 624/1000
2023-10-12 07:26:55.297 
Epoch 624/1000 
	 loss: 27.2027, MinusLogProbMetric: 27.2027, val_loss: 28.5103, val_MinusLogProbMetric: 28.5103

Epoch 624: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.2027 - MinusLogProbMetric: 27.2027 - val_loss: 28.5103 - val_MinusLogProbMetric: 28.5103 - lr: 3.1250e-05 - 27s/epoch - 136ms/step
Epoch 625/1000
2023-10-12 07:27:22.311 
Epoch 625/1000 
	 loss: 27.1990, MinusLogProbMetric: 27.1990, val_loss: 28.5056, val_MinusLogProbMetric: 28.5056

Epoch 625: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1990 - MinusLogProbMetric: 27.1990 - val_loss: 28.5056 - val_MinusLogProbMetric: 28.5056 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 626/1000
2023-10-12 07:27:48.013 
Epoch 626/1000 
	 loss: 27.2076, MinusLogProbMetric: 27.2076, val_loss: 28.5075, val_MinusLogProbMetric: 28.5075

Epoch 626: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2076 - MinusLogProbMetric: 27.2076 - val_loss: 28.5075 - val_MinusLogProbMetric: 28.5075 - lr: 3.1250e-05 - 26s/epoch - 131ms/step
Epoch 627/1000
2023-10-12 07:28:13.731 
Epoch 627/1000 
	 loss: 27.2109, MinusLogProbMetric: 27.2109, val_loss: 28.5379, val_MinusLogProbMetric: 28.5379

Epoch 627: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.2109 - MinusLogProbMetric: 27.2109 - val_loss: 28.5379 - val_MinusLogProbMetric: 28.5379 - lr: 3.1250e-05 - 26s/epoch - 131ms/step
Epoch 628/1000
2023-10-12 07:28:41.429 
Epoch 628/1000 
	 loss: 27.2012, MinusLogProbMetric: 27.2012, val_loss: 28.5027, val_MinusLogProbMetric: 28.5027

Epoch 628: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.2012 - MinusLogProbMetric: 27.2012 - val_loss: 28.5027 - val_MinusLogProbMetric: 28.5027 - lr: 3.1250e-05 - 28s/epoch - 141ms/step
Epoch 629/1000
2023-10-12 07:29:08.148 
Epoch 629/1000 
	 loss: 27.1906, MinusLogProbMetric: 27.1906, val_loss: 28.5010, val_MinusLogProbMetric: 28.5010

Epoch 629: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1906 - MinusLogProbMetric: 27.1906 - val_loss: 28.5010 - val_MinusLogProbMetric: 28.5010 - lr: 1.5625e-05 - 27s/epoch - 136ms/step
Epoch 630/1000
2023-10-12 07:29:34.813 
Epoch 630/1000 
	 loss: 27.1899, MinusLogProbMetric: 27.1899, val_loss: 28.4985, val_MinusLogProbMetric: 28.4985

Epoch 630: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1899 - MinusLogProbMetric: 27.1899 - val_loss: 28.4985 - val_MinusLogProbMetric: 28.4985 - lr: 1.5625e-05 - 27s/epoch - 136ms/step
Epoch 631/1000
2023-10-12 07:30:00.801 
Epoch 631/1000 
	 loss: 27.1886, MinusLogProbMetric: 27.1886, val_loss: 28.5031, val_MinusLogProbMetric: 28.5031

Epoch 631: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1886 - MinusLogProbMetric: 27.1886 - val_loss: 28.5031 - val_MinusLogProbMetric: 28.5031 - lr: 1.5625e-05 - 26s/epoch - 133ms/step
Epoch 632/1000
2023-10-12 07:30:26.580 
Epoch 632/1000 
	 loss: 27.1896, MinusLogProbMetric: 27.1896, val_loss: 28.5093, val_MinusLogProbMetric: 28.5093

Epoch 632: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1896 - MinusLogProbMetric: 27.1896 - val_loss: 28.5093 - val_MinusLogProbMetric: 28.5093 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 633/1000
2023-10-12 07:30:52.194 
Epoch 633/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.5023, val_MinusLogProbMetric: 28.5023

Epoch 633: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.5023 - val_MinusLogProbMetric: 28.5023 - lr: 1.5625e-05 - 26s/epoch - 131ms/step
Epoch 634/1000
2023-10-12 07:31:18.333 
Epoch 634/1000 
	 loss: 27.1887, MinusLogProbMetric: 27.1887, val_loss: 28.5053, val_MinusLogProbMetric: 28.5053

Epoch 634: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1887 - MinusLogProbMetric: 27.1887 - val_loss: 28.5053 - val_MinusLogProbMetric: 28.5053 - lr: 1.5625e-05 - 26s/epoch - 133ms/step
Epoch 635/1000
2023-10-12 07:31:44.776 
Epoch 635/1000 
	 loss: 27.1886, MinusLogProbMetric: 27.1886, val_loss: 28.5061, val_MinusLogProbMetric: 28.5061

Epoch 635: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1886 - MinusLogProbMetric: 27.1886 - val_loss: 28.5061 - val_MinusLogProbMetric: 28.5061 - lr: 1.5625e-05 - 26s/epoch - 135ms/step
Epoch 636/1000
2023-10-12 07:32:11.912 
Epoch 636/1000 
	 loss: 27.1863, MinusLogProbMetric: 27.1863, val_loss: 28.5008, val_MinusLogProbMetric: 28.5008

Epoch 636: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1863 - MinusLogProbMetric: 27.1863 - val_loss: 28.5008 - val_MinusLogProbMetric: 28.5008 - lr: 1.5625e-05 - 27s/epoch - 138ms/step
Epoch 637/1000
2023-10-12 07:32:37.536 
Epoch 637/1000 
	 loss: 27.1876, MinusLogProbMetric: 27.1876, val_loss: 28.5051, val_MinusLogProbMetric: 28.5051

Epoch 637: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1876 - MinusLogProbMetric: 27.1876 - val_loss: 28.5051 - val_MinusLogProbMetric: 28.5051 - lr: 1.5625e-05 - 26s/epoch - 131ms/step
Epoch 638/1000
2023-10-12 07:33:03.562 
Epoch 638/1000 
	 loss: 27.1886, MinusLogProbMetric: 27.1886, val_loss: 28.5028, val_MinusLogProbMetric: 28.5028

Epoch 638: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1886 - MinusLogProbMetric: 27.1886 - val_loss: 28.5028 - val_MinusLogProbMetric: 28.5028 - lr: 1.5625e-05 - 26s/epoch - 133ms/step
Epoch 639/1000
2023-10-12 07:33:29.116 
Epoch 639/1000 
	 loss: 27.1901, MinusLogProbMetric: 27.1901, val_loss: 28.5015, val_MinusLogProbMetric: 28.5015

Epoch 639: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1901 - MinusLogProbMetric: 27.1901 - val_loss: 28.5015 - val_MinusLogProbMetric: 28.5015 - lr: 1.5625e-05 - 26s/epoch - 130ms/step
Epoch 640/1000
2023-10-12 07:33:54.573 
Epoch 640/1000 
	 loss: 27.1881, MinusLogProbMetric: 27.1881, val_loss: 28.5262, val_MinusLogProbMetric: 28.5262

Epoch 640: val_loss did not improve from 28.49374
196/196 - 25s - loss: 27.1881 - MinusLogProbMetric: 27.1881 - val_loss: 28.5262 - val_MinusLogProbMetric: 28.5262 - lr: 1.5625e-05 - 25s/epoch - 130ms/step
Epoch 641/1000
2023-10-12 07:34:20.481 
Epoch 641/1000 
	 loss: 27.1872, MinusLogProbMetric: 27.1872, val_loss: 28.5187, val_MinusLogProbMetric: 28.5187

Epoch 641: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1872 - MinusLogProbMetric: 27.1872 - val_loss: 28.5187 - val_MinusLogProbMetric: 28.5187 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 642/1000
2023-10-12 07:34:46.350 
Epoch 642/1000 
	 loss: 27.1892, MinusLogProbMetric: 27.1892, val_loss: 28.5025, val_MinusLogProbMetric: 28.5025

Epoch 642: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1892 - MinusLogProbMetric: 27.1892 - val_loss: 28.5025 - val_MinusLogProbMetric: 28.5025 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 643/1000
2023-10-12 07:35:12.365 
Epoch 643/1000 
	 loss: 27.1884, MinusLogProbMetric: 27.1884, val_loss: 28.5014, val_MinusLogProbMetric: 28.5014

Epoch 643: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1884 - MinusLogProbMetric: 27.1884 - val_loss: 28.5014 - val_MinusLogProbMetric: 28.5014 - lr: 1.5625e-05 - 26s/epoch - 133ms/step
Epoch 644/1000
2023-10-12 07:35:38.428 
Epoch 644/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.5048, val_MinusLogProbMetric: 28.5048

Epoch 644: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.5048 - val_MinusLogProbMetric: 28.5048 - lr: 1.5625e-05 - 26s/epoch - 133ms/step
Epoch 645/1000
2023-10-12 07:36:04.366 
Epoch 645/1000 
	 loss: 27.1883, MinusLogProbMetric: 27.1883, val_loss: 28.5033, val_MinusLogProbMetric: 28.5033

Epoch 645: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1883 - MinusLogProbMetric: 27.1883 - val_loss: 28.5033 - val_MinusLogProbMetric: 28.5033 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 646/1000
2023-10-12 07:36:30.215 
Epoch 646/1000 
	 loss: 27.1905, MinusLogProbMetric: 27.1905, val_loss: 28.5039, val_MinusLogProbMetric: 28.5039

Epoch 646: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1905 - MinusLogProbMetric: 27.1905 - val_loss: 28.5039 - val_MinusLogProbMetric: 28.5039 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 647/1000
2023-10-12 07:36:56.686 
Epoch 647/1000 
	 loss: 27.1903, MinusLogProbMetric: 27.1903, val_loss: 28.4982, val_MinusLogProbMetric: 28.4982

Epoch 647: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1903 - MinusLogProbMetric: 27.1903 - val_loss: 28.4982 - val_MinusLogProbMetric: 28.4982 - lr: 1.5625e-05 - 26s/epoch - 135ms/step
Epoch 648/1000
2023-10-12 07:37:22.244 
Epoch 648/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.5124, val_MinusLogProbMetric: 28.5124

Epoch 648: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.5124 - val_MinusLogProbMetric: 28.5124 - lr: 1.5625e-05 - 26s/epoch - 130ms/step
Epoch 649/1000
2023-10-12 07:37:48.080 
Epoch 649/1000 
	 loss: 27.1882, MinusLogProbMetric: 27.1882, val_loss: 28.5001, val_MinusLogProbMetric: 28.5001

Epoch 649: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1882 - MinusLogProbMetric: 27.1882 - val_loss: 28.5001 - val_MinusLogProbMetric: 28.5001 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 650/1000
2023-10-12 07:38:13.836 
Epoch 650/1000 
	 loss: 27.1882, MinusLogProbMetric: 27.1882, val_loss: 28.5011, val_MinusLogProbMetric: 28.5011

Epoch 650: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1882 - MinusLogProbMetric: 27.1882 - val_loss: 28.5011 - val_MinusLogProbMetric: 28.5011 - lr: 1.5625e-05 - 26s/epoch - 131ms/step
Epoch 651/1000
2023-10-12 07:38:39.714 
Epoch 651/1000 
	 loss: 27.1872, MinusLogProbMetric: 27.1872, val_loss: 28.5111, val_MinusLogProbMetric: 28.5111

Epoch 651: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1872 - MinusLogProbMetric: 27.1872 - val_loss: 28.5111 - val_MinusLogProbMetric: 28.5111 - lr: 1.5625e-05 - 26s/epoch - 132ms/step
Epoch 652/1000
2023-10-12 07:39:05.358 
Epoch 652/1000 
	 loss: 27.1869, MinusLogProbMetric: 27.1869, val_loss: 28.5044, val_MinusLogProbMetric: 28.5044

Epoch 652: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1869 - MinusLogProbMetric: 27.1869 - val_loss: 28.5044 - val_MinusLogProbMetric: 28.5044 - lr: 1.5625e-05 - 26s/epoch - 131ms/step
Epoch 653/1000
2023-10-12 07:39:31.435 
Epoch 653/1000 
	 loss: 27.1877, MinusLogProbMetric: 27.1877, val_loss: 28.5134, val_MinusLogProbMetric: 28.5134

Epoch 653: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1877 - MinusLogProbMetric: 27.1877 - val_loss: 28.5134 - val_MinusLogProbMetric: 28.5134 - lr: 1.5625e-05 - 26s/epoch - 133ms/step
Epoch 654/1000
2023-10-12 07:39:57.777 
Epoch 654/1000 
	 loss: 27.1871, MinusLogProbMetric: 27.1871, val_loss: 28.5052, val_MinusLogProbMetric: 28.5052

Epoch 654: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1871 - MinusLogProbMetric: 27.1871 - val_loss: 28.5052 - val_MinusLogProbMetric: 28.5052 - lr: 1.5625e-05 - 26s/epoch - 134ms/step
Epoch 655/1000
2023-10-12 07:40:24.244 
Epoch 655/1000 
	 loss: 27.1865, MinusLogProbMetric: 27.1865, val_loss: 28.5033, val_MinusLogProbMetric: 28.5033

Epoch 655: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1865 - MinusLogProbMetric: 27.1865 - val_loss: 28.5033 - val_MinusLogProbMetric: 28.5033 - lr: 1.5625e-05 - 26s/epoch - 135ms/step
Epoch 656/1000
2023-10-12 07:40:51.028 
Epoch 656/1000 
	 loss: 27.1886, MinusLogProbMetric: 27.1886, val_loss: 28.5170, val_MinusLogProbMetric: 28.5170

Epoch 656: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1886 - MinusLogProbMetric: 27.1886 - val_loss: 28.5170 - val_MinusLogProbMetric: 28.5170 - lr: 1.5625e-05 - 27s/epoch - 137ms/step
Epoch 657/1000
2023-10-12 07:41:18.635 
Epoch 657/1000 
	 loss: 27.1887, MinusLogProbMetric: 27.1887, val_loss: 28.5102, val_MinusLogProbMetric: 28.5102

Epoch 657: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1887 - MinusLogProbMetric: 27.1887 - val_loss: 28.5102 - val_MinusLogProbMetric: 28.5102 - lr: 1.5625e-05 - 28s/epoch - 141ms/step
Epoch 658/1000
2023-10-12 07:41:46.062 
Epoch 658/1000 
	 loss: 27.1915, MinusLogProbMetric: 27.1915, val_loss: 28.5021, val_MinusLogProbMetric: 28.5021

Epoch 658: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1915 - MinusLogProbMetric: 27.1915 - val_loss: 28.5021 - val_MinusLogProbMetric: 28.5021 - lr: 1.5625e-05 - 27s/epoch - 140ms/step
Epoch 659/1000
2023-10-12 07:42:13.633 
Epoch 659/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.5006, val_MinusLogProbMetric: 28.5006

Epoch 659: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.5006 - val_MinusLogProbMetric: 28.5006 - lr: 1.5625e-05 - 28s/epoch - 141ms/step
Epoch 660/1000
2023-10-12 07:42:40.662 
Epoch 660/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.4991, val_MinusLogProbMetric: 28.4991

Epoch 660: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.4991 - val_MinusLogProbMetric: 28.4991 - lr: 1.5625e-05 - 27s/epoch - 138ms/step
Epoch 661/1000
2023-10-12 07:43:08.461 
Epoch 661/1000 
	 loss: 27.1873, MinusLogProbMetric: 27.1873, val_loss: 28.5199, val_MinusLogProbMetric: 28.5199

Epoch 661: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1873 - MinusLogProbMetric: 27.1873 - val_loss: 28.5199 - val_MinusLogProbMetric: 28.5199 - lr: 1.5625e-05 - 28s/epoch - 142ms/step
Epoch 662/1000
2023-10-12 07:43:36.134 
Epoch 662/1000 
	 loss: 27.1860, MinusLogProbMetric: 27.1860, val_loss: 28.5058, val_MinusLogProbMetric: 28.5058

Epoch 662: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1860 - MinusLogProbMetric: 27.1860 - val_loss: 28.5058 - val_MinusLogProbMetric: 28.5058 - lr: 1.5625e-05 - 28s/epoch - 141ms/step
Epoch 663/1000
2023-10-12 07:44:03.538 
Epoch 663/1000 
	 loss: 27.1879, MinusLogProbMetric: 27.1879, val_loss: 28.5060, val_MinusLogProbMetric: 28.5060

Epoch 663: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1879 - MinusLogProbMetric: 27.1879 - val_loss: 28.5060 - val_MinusLogProbMetric: 28.5060 - lr: 1.5625e-05 - 27s/epoch - 140ms/step
Epoch 664/1000
2023-10-12 07:44:32.014 
Epoch 664/1000 
	 loss: 27.1872, MinusLogProbMetric: 27.1872, val_loss: 28.5135, val_MinusLogProbMetric: 28.5135

Epoch 664: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1872 - MinusLogProbMetric: 27.1872 - val_loss: 28.5135 - val_MinusLogProbMetric: 28.5135 - lr: 1.5625e-05 - 28s/epoch - 145ms/step
Epoch 665/1000
2023-10-12 07:44:59.756 
Epoch 665/1000 
	 loss: 27.1854, MinusLogProbMetric: 27.1854, val_loss: 28.5008, val_MinusLogProbMetric: 28.5008

Epoch 665: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1854 - MinusLogProbMetric: 27.1854 - val_loss: 28.5008 - val_MinusLogProbMetric: 28.5008 - lr: 1.5625e-05 - 28s/epoch - 142ms/step
Epoch 666/1000
2023-10-12 07:45:27.903 
Epoch 666/1000 
	 loss: 27.1861, MinusLogProbMetric: 27.1861, val_loss: 28.5052, val_MinusLogProbMetric: 28.5052

Epoch 666: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1861 - MinusLogProbMetric: 27.1861 - val_loss: 28.5052 - val_MinusLogProbMetric: 28.5052 - lr: 1.5625e-05 - 28s/epoch - 144ms/step
Epoch 667/1000
2023-10-12 07:45:55.752 
Epoch 667/1000 
	 loss: 27.1859, MinusLogProbMetric: 27.1859, val_loss: 28.5109, val_MinusLogProbMetric: 28.5109

Epoch 667: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1859 - MinusLogProbMetric: 27.1859 - val_loss: 28.5109 - val_MinusLogProbMetric: 28.5109 - lr: 1.5625e-05 - 28s/epoch - 142ms/step
Epoch 668/1000
2023-10-12 07:46:23.582 
Epoch 668/1000 
	 loss: 27.1906, MinusLogProbMetric: 27.1906, val_loss: 28.5141, val_MinusLogProbMetric: 28.5141

Epoch 668: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1906 - MinusLogProbMetric: 27.1906 - val_loss: 28.5141 - val_MinusLogProbMetric: 28.5141 - lr: 1.5625e-05 - 28s/epoch - 142ms/step
Epoch 669/1000
2023-10-12 07:46:50.632 
Epoch 669/1000 
	 loss: 27.1877, MinusLogProbMetric: 27.1877, val_loss: 28.5020, val_MinusLogProbMetric: 28.5020

Epoch 669: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1877 - MinusLogProbMetric: 27.1877 - val_loss: 28.5020 - val_MinusLogProbMetric: 28.5020 - lr: 1.5625e-05 - 27s/epoch - 138ms/step
Epoch 670/1000
2023-10-12 07:47:18.617 
Epoch 670/1000 
	 loss: 27.1856, MinusLogProbMetric: 27.1856, val_loss: 28.5105, val_MinusLogProbMetric: 28.5105

Epoch 670: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1856 - MinusLogProbMetric: 27.1856 - val_loss: 28.5105 - val_MinusLogProbMetric: 28.5105 - lr: 1.5625e-05 - 28s/epoch - 143ms/step
Epoch 671/1000
2023-10-12 07:47:46.106 
Epoch 671/1000 
	 loss: 27.1877, MinusLogProbMetric: 27.1877, val_loss: 28.5029, val_MinusLogProbMetric: 28.5029

Epoch 671: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1877 - MinusLogProbMetric: 27.1877 - val_loss: 28.5029 - val_MinusLogProbMetric: 28.5029 - lr: 1.5625e-05 - 27s/epoch - 140ms/step
Epoch 672/1000
2023-10-12 07:48:13.621 
Epoch 672/1000 
	 loss: 27.1861, MinusLogProbMetric: 27.1861, val_loss: 28.5115, val_MinusLogProbMetric: 28.5115

Epoch 672: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1861 - MinusLogProbMetric: 27.1861 - val_loss: 28.5115 - val_MinusLogProbMetric: 28.5115 - lr: 1.5625e-05 - 28s/epoch - 140ms/step
Epoch 673/1000
2023-10-12 07:48:41.993 
Epoch 673/1000 
	 loss: 27.1860, MinusLogProbMetric: 27.1860, val_loss: 28.5000, val_MinusLogProbMetric: 28.5000

Epoch 673: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1860 - MinusLogProbMetric: 27.1860 - val_loss: 28.5000 - val_MinusLogProbMetric: 28.5000 - lr: 1.5625e-05 - 28s/epoch - 145ms/step
Epoch 674/1000
2023-10-12 07:49:08.547 
Epoch 674/1000 
	 loss: 27.1865, MinusLogProbMetric: 27.1865, val_loss: 28.5024, val_MinusLogProbMetric: 28.5024

Epoch 674: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1865 - MinusLogProbMetric: 27.1865 - val_loss: 28.5024 - val_MinusLogProbMetric: 28.5024 - lr: 1.5625e-05 - 27s/epoch - 135ms/step
Epoch 675/1000
2023-10-12 07:49:34.827 
Epoch 675/1000 
	 loss: 27.1867, MinusLogProbMetric: 27.1867, val_loss: 28.5010, val_MinusLogProbMetric: 28.5010

Epoch 675: val_loss did not improve from 28.49374
196/196 - 26s - loss: 27.1867 - MinusLogProbMetric: 27.1867 - val_loss: 28.5010 - val_MinusLogProbMetric: 28.5010 - lr: 1.5625e-05 - 26s/epoch - 134ms/step
Epoch 676/1000
2023-10-12 07:50:01.644 
Epoch 676/1000 
	 loss: 27.1872, MinusLogProbMetric: 27.1872, val_loss: 28.5047, val_MinusLogProbMetric: 28.5047

Epoch 676: val_loss did not improve from 28.49374
196/196 - 27s - loss: 27.1872 - MinusLogProbMetric: 27.1872 - val_loss: 28.5047 - val_MinusLogProbMetric: 28.5047 - lr: 1.5625e-05 - 27s/epoch - 137ms/step
Epoch 677/1000
2023-10-12 07:50:29.750 
Epoch 677/1000 
	 loss: 27.1878, MinusLogProbMetric: 27.1878, val_loss: 28.5021, val_MinusLogProbMetric: 28.5021

Epoch 677: val_loss did not improve from 28.49374
196/196 - 28s - loss: 27.1878 - MinusLogProbMetric: 27.1878 - val_loss: 28.5021 - val_MinusLogProbMetric: 28.5021 - lr: 1.5625e-05 - 28s/epoch - 143ms/step
Epoch 678/1000
2023-10-12 07:50:56.475 
Epoch 678/1000 
	 loss: 27.1852, MinusLogProbMetric: 27.1852, val_loss: 28.4986, val_MinusLogProbMetric: 28.4986

Epoch 678: val_loss did not improve from 28.49374
Restoring model weights from the end of the best epoch: 578.
196/196 - 27s - loss: 27.1852 - MinusLogProbMetric: 27.1852 - val_loss: 28.4986 - val_MinusLogProbMetric: 28.4986 - lr: 1.5625e-05 - 27s/epoch - 137ms/step
Epoch 678: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7f3ac9a971c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 12.457808353938162 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:6 out of the last 6 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7f3ac9a95360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 7.345670372014865 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7f3ac9a960e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 4.376805672189221 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:6 out of the last 6 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7f3ac9a976d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 4.9578985900152475 seconds.
Training succeeded with seed 377.
Model trained in 18181.49 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 29.99 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 30.44 s.
===========
Run 338/720 done in 18216.01 s.
===========

===========
Generating train data for run 339.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_339
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_485"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_486 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f3c4e040790>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c4dbd2290>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c4dbd2290>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38975e36a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c4da678e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c4da67e50>, <keras.callbacks.ModelCheckpoint object at 0x7f3c4da67f10>, <keras.callbacks.EarlyStopping object at 0x7f3c4da67e20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c4da67fd0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c4da8c0a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_339/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 339/720 with hyperparameters:
timestamp = 2023-10-12 07:51:30.212317
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-12 07:52:32.206 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6308.4302, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 62s - loss: nan - MinusLogProbMetric: 6308.4302 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 62s/epoch - 316ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 339.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_339
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_491"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_492 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f3c4cf86860>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c4d185600>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c4d185600>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f387ce5a7d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c4cfd4fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c4cfd5510>, <keras.callbacks.ModelCheckpoint object at 0x7f3c4cfd55d0>, <keras.callbacks.EarlyStopping object at 0x7f3c4cfd5840>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c4cfd5870>, <keras.callbacks.TerminateOnNaN object at 0x7f3c4cfd54b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_339/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 339/720 with hyperparameters:
timestamp = 2023-10-12 07:52:35.969306
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-12 07:54:06.263 
Epoch 1/1000 
	 loss: 1515.2430, MinusLogProbMetric: 1515.2430, val_loss: 355.8911, val_MinusLogProbMetric: 355.8911

Epoch 1: val_loss improved from inf to 355.89105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 91s - loss: 1515.2430 - MinusLogProbMetric: 1515.2430 - val_loss: 355.8911 - val_MinusLogProbMetric: 355.8911 - lr: 3.3333e-04 - 91s/epoch - 462ms/step
Epoch 2/1000
2023-10-12 07:54:38.695 
Epoch 2/1000 
	 loss: 265.2274, MinusLogProbMetric: 265.2274, val_loss: 196.4781, val_MinusLogProbMetric: 196.4781

Epoch 2: val_loss improved from 355.89105 to 196.47812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 265.2274 - MinusLogProbMetric: 265.2274 - val_loss: 196.4781 - val_MinusLogProbMetric: 196.4781 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 3/1000
2023-10-12 07:55:10.772 
Epoch 3/1000 
	 loss: 172.5705, MinusLogProbMetric: 172.5705, val_loss: 161.0301, val_MinusLogProbMetric: 161.0301

Epoch 3: val_loss improved from 196.47812 to 161.03014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 172.5705 - MinusLogProbMetric: 172.5705 - val_loss: 161.0301 - val_MinusLogProbMetric: 161.0301 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 4/1000
2023-10-12 07:55:42.700 
Epoch 4/1000 
	 loss: 145.3949, MinusLogProbMetric: 145.3949, val_loss: 131.2618, val_MinusLogProbMetric: 131.2618

Epoch 4: val_loss improved from 161.03014 to 131.26184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 145.3949 - MinusLogProbMetric: 145.3949 - val_loss: 131.2618 - val_MinusLogProbMetric: 131.2618 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 5/1000
2023-10-12 07:56:14.600 
Epoch 5/1000 
	 loss: 120.1547, MinusLogProbMetric: 120.1547, val_loss: 113.0749, val_MinusLogProbMetric: 113.0749

Epoch 5: val_loss improved from 131.26184 to 113.07488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 120.1547 - MinusLogProbMetric: 120.1547 - val_loss: 113.0749 - val_MinusLogProbMetric: 113.0749 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 6/1000
2023-10-12 07:56:45.742 
Epoch 6/1000 
	 loss: 104.8229, MinusLogProbMetric: 104.8229, val_loss: 100.5334, val_MinusLogProbMetric: 100.5334

Epoch 6: val_loss improved from 113.07488 to 100.53336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 104.8229 - MinusLogProbMetric: 104.8229 - val_loss: 100.5334 - val_MinusLogProbMetric: 100.5334 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 7/1000
2023-10-12 07:57:17.352 
Epoch 7/1000 
	 loss: 95.2760, MinusLogProbMetric: 95.2760, val_loss: 90.4832, val_MinusLogProbMetric: 90.4832

Epoch 7: val_loss improved from 100.53336 to 90.48316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 95.2760 - MinusLogProbMetric: 95.2760 - val_loss: 90.4832 - val_MinusLogProbMetric: 90.4832 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 8/1000
2023-10-12 07:57:49.488 
Epoch 8/1000 
	 loss: 87.8806, MinusLogProbMetric: 87.8806, val_loss: 84.0737, val_MinusLogProbMetric: 84.0737

Epoch 8: val_loss improved from 90.48316 to 84.07368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 87.8806 - MinusLogProbMetric: 87.8806 - val_loss: 84.0737 - val_MinusLogProbMetric: 84.0737 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 9/1000
2023-10-12 07:58:22.800 
Epoch 9/1000 
	 loss: 81.0971, MinusLogProbMetric: 81.0971, val_loss: 78.2906, val_MinusLogProbMetric: 78.2906

Epoch 9: val_loss improved from 84.07368 to 78.29060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 33s - loss: 81.0971 - MinusLogProbMetric: 81.0971 - val_loss: 78.2906 - val_MinusLogProbMetric: 78.2906 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 10/1000
2023-10-12 07:58:55.485 
Epoch 10/1000 
	 loss: 75.5433, MinusLogProbMetric: 75.5433, val_loss: 72.8153, val_MinusLogProbMetric: 72.8153

Epoch 10: val_loss improved from 78.29060 to 72.81526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 33s - loss: 75.5433 - MinusLogProbMetric: 75.5433 - val_loss: 72.8153 - val_MinusLogProbMetric: 72.8153 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 11/1000
2023-10-12 07:59:27.649 
Epoch 11/1000 
	 loss: 70.6799, MinusLogProbMetric: 70.6799, val_loss: 69.0264, val_MinusLogProbMetric: 69.0264

Epoch 11: val_loss improved from 72.81526 to 69.02641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 70.6799 - MinusLogProbMetric: 70.6799 - val_loss: 69.0264 - val_MinusLogProbMetric: 69.0264 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 12/1000
2023-10-12 07:59:59.122 
Epoch 12/1000 
	 loss: 66.8124, MinusLogProbMetric: 66.8124, val_loss: 66.8671, val_MinusLogProbMetric: 66.8671

Epoch 12: val_loss improved from 69.02641 to 66.86710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 66.8124 - MinusLogProbMetric: 66.8124 - val_loss: 66.8671 - val_MinusLogProbMetric: 66.8671 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 13/1000
2023-10-12 08:00:33.696 
Epoch 13/1000 
	 loss: 63.4472, MinusLogProbMetric: 63.4472, val_loss: 62.6786, val_MinusLogProbMetric: 62.6786

Epoch 13: val_loss improved from 66.86710 to 62.67860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 63.4472 - MinusLogProbMetric: 63.4472 - val_loss: 62.6786 - val_MinusLogProbMetric: 62.6786 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 14/1000
2023-10-12 08:01:08.854 
Epoch 14/1000 
	 loss: 62.7571, MinusLogProbMetric: 62.7571, val_loss: 62.8436, val_MinusLogProbMetric: 62.8436

Epoch 14: val_loss did not improve from 62.67860
196/196 - 35s - loss: 62.7571 - MinusLogProbMetric: 62.7571 - val_loss: 62.8436 - val_MinusLogProbMetric: 62.8436 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 15/1000
2023-10-12 08:01:44.595 
Epoch 15/1000 
	 loss: 59.5861, MinusLogProbMetric: 59.5861, val_loss: 58.0511, val_MinusLogProbMetric: 58.0511

Epoch 15: val_loss improved from 62.67860 to 58.05110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 59.5861 - MinusLogProbMetric: 59.5861 - val_loss: 58.0511 - val_MinusLogProbMetric: 58.0511 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 16/1000
2023-10-12 08:02:17.861 
Epoch 16/1000 
	 loss: 56.8197, MinusLogProbMetric: 56.8197, val_loss: 55.6487, val_MinusLogProbMetric: 55.6487

Epoch 16: val_loss improved from 58.05110 to 55.64868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 33s - loss: 56.8197 - MinusLogProbMetric: 56.8197 - val_loss: 55.6487 - val_MinusLogProbMetric: 55.6487 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 17/1000
2023-10-12 08:02:49.245 
Epoch 17/1000 
	 loss: 54.5967, MinusLogProbMetric: 54.5967, val_loss: 53.6071, val_MinusLogProbMetric: 53.6071

Epoch 17: val_loss improved from 55.64868 to 53.60714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 54.5967 - MinusLogProbMetric: 54.5967 - val_loss: 53.6071 - val_MinusLogProbMetric: 53.6071 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 18/1000
2023-10-12 08:03:20.413 
Epoch 18/1000 
	 loss: 52.7963, MinusLogProbMetric: 52.7963, val_loss: 52.5338, val_MinusLogProbMetric: 52.5338

Epoch 18: val_loss improved from 53.60714 to 52.53378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 52.7963 - MinusLogProbMetric: 52.7963 - val_loss: 52.5338 - val_MinusLogProbMetric: 52.5338 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 19/1000
2023-10-12 08:03:51.372 
Epoch 19/1000 
	 loss: 51.3053, MinusLogProbMetric: 51.3053, val_loss: 50.7683, val_MinusLogProbMetric: 50.7683

Epoch 19: val_loss improved from 52.53378 to 50.76830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 51.3053 - MinusLogProbMetric: 51.3053 - val_loss: 50.7683 - val_MinusLogProbMetric: 50.7683 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 20/1000
2023-10-12 08:04:22.803 
Epoch 20/1000 
	 loss: 50.0006, MinusLogProbMetric: 50.0006, val_loss: 49.2313, val_MinusLogProbMetric: 49.2313

Epoch 20: val_loss improved from 50.76830 to 49.23130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 50.0006 - MinusLogProbMetric: 50.0006 - val_loss: 49.2313 - val_MinusLogProbMetric: 49.2313 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 21/1000
2023-10-12 08:04:55.209 
Epoch 21/1000 
	 loss: 49.0917, MinusLogProbMetric: 49.0917, val_loss: 48.3568, val_MinusLogProbMetric: 48.3568

Epoch 21: val_loss improved from 49.23130 to 48.35676, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 49.0917 - MinusLogProbMetric: 49.0917 - val_loss: 48.3568 - val_MinusLogProbMetric: 48.3568 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 22/1000
2023-10-12 08:05:28.774 
Epoch 22/1000 
	 loss: 47.6008, MinusLogProbMetric: 47.6008, val_loss: 56.9759, val_MinusLogProbMetric: 56.9759

Epoch 22: val_loss did not improve from 48.35676
196/196 - 33s - loss: 47.6008 - MinusLogProbMetric: 47.6008 - val_loss: 56.9759 - val_MinusLogProbMetric: 56.9759 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 23/1000
2023-10-12 08:06:02.563 
Epoch 23/1000 
	 loss: 47.0169, MinusLogProbMetric: 47.0169, val_loss: 46.8064, val_MinusLogProbMetric: 46.8064

Epoch 23: val_loss improved from 48.35676 to 46.80639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 47.0169 - MinusLogProbMetric: 47.0169 - val_loss: 46.8064 - val_MinusLogProbMetric: 46.8064 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 24/1000
2023-10-12 08:06:37.188 
Epoch 24/1000 
	 loss: 45.7045, MinusLogProbMetric: 45.7045, val_loss: 45.4792, val_MinusLogProbMetric: 45.4792

Epoch 24: val_loss improved from 46.80639 to 45.47918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 45.7045 - MinusLogProbMetric: 45.7045 - val_loss: 45.4792 - val_MinusLogProbMetric: 45.4792 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 25/1000
2023-10-12 08:07:11.298 
Epoch 25/1000 
	 loss: 45.0101, MinusLogProbMetric: 45.0101, val_loss: 45.4989, val_MinusLogProbMetric: 45.4989

Epoch 25: val_loss did not improve from 45.47918
196/196 - 33s - loss: 45.0101 - MinusLogProbMetric: 45.0101 - val_loss: 45.4989 - val_MinusLogProbMetric: 45.4989 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 26/1000
2023-10-12 08:07:45.812 
Epoch 26/1000 
	 loss: 44.2889, MinusLogProbMetric: 44.2889, val_loss: 43.7002, val_MinusLogProbMetric: 43.7002

Epoch 26: val_loss improved from 45.47918 to 43.70023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 44.2889 - MinusLogProbMetric: 44.2889 - val_loss: 43.7002 - val_MinusLogProbMetric: 43.7002 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 27/1000
2023-10-12 08:08:16.677 
Epoch 27/1000 
	 loss: 43.5976, MinusLogProbMetric: 43.5976, val_loss: 43.3000, val_MinusLogProbMetric: 43.3000

Epoch 27: val_loss improved from 43.70023 to 43.30000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 31s - loss: 43.5976 - MinusLogProbMetric: 43.5976 - val_loss: 43.3000 - val_MinusLogProbMetric: 43.3000 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 28/1000
2023-10-12 08:08:49.674 
Epoch 28/1000 
	 loss: 42.7213, MinusLogProbMetric: 42.7213, val_loss: 42.7069, val_MinusLogProbMetric: 42.7069

Epoch 28: val_loss improved from 43.30000 to 42.70691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 33s - loss: 42.7213 - MinusLogProbMetric: 42.7213 - val_loss: 42.7069 - val_MinusLogProbMetric: 42.7069 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 29/1000
2023-10-12 08:09:23.151 
Epoch 29/1000 
	 loss: 42.3190, MinusLogProbMetric: 42.3190, val_loss: 43.4983, val_MinusLogProbMetric: 43.4983

Epoch 29: val_loss did not improve from 42.70691
196/196 - 33s - loss: 42.3190 - MinusLogProbMetric: 42.3190 - val_loss: 43.4983 - val_MinusLogProbMetric: 43.4983 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 30/1000
2023-10-12 08:09:59.608 
Epoch 30/1000 
	 loss: 42.0270, MinusLogProbMetric: 42.0270, val_loss: 42.2966, val_MinusLogProbMetric: 42.2966

Epoch 30: val_loss improved from 42.70691 to 42.29661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 37s - loss: 42.0270 - MinusLogProbMetric: 42.0270 - val_loss: 42.2966 - val_MinusLogProbMetric: 42.2966 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 31/1000
2023-10-12 08:10:37.564 
Epoch 31/1000 
	 loss: 41.2403, MinusLogProbMetric: 41.2403, val_loss: 41.1263, val_MinusLogProbMetric: 41.1263

Epoch 31: val_loss improved from 42.29661 to 41.12633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 41.2403 - MinusLogProbMetric: 41.2403 - val_loss: 41.1263 - val_MinusLogProbMetric: 41.1263 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 32/1000
2023-10-12 08:11:13.841 
Epoch 32/1000 
	 loss: 40.9195, MinusLogProbMetric: 40.9195, val_loss: 40.4956, val_MinusLogProbMetric: 40.4956

Epoch 32: val_loss improved from 41.12633 to 40.49559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 40.9195 - MinusLogProbMetric: 40.9195 - val_loss: 40.4956 - val_MinusLogProbMetric: 40.4956 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 33/1000
2023-10-12 08:11:48.815 
Epoch 33/1000 
	 loss: 40.2495, MinusLogProbMetric: 40.2495, val_loss: 40.5358, val_MinusLogProbMetric: 40.5358

Epoch 33: val_loss did not improve from 40.49559
196/196 - 34s - loss: 40.2495 - MinusLogProbMetric: 40.2495 - val_loss: 40.5358 - val_MinusLogProbMetric: 40.5358 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 34/1000
2023-10-12 08:12:22.607 
Epoch 34/1000 
	 loss: 40.4679, MinusLogProbMetric: 40.4679, val_loss: 39.9161, val_MinusLogProbMetric: 39.9161

Epoch 34: val_loss improved from 40.49559 to 39.91608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 40.4679 - MinusLogProbMetric: 40.4679 - val_loss: 39.9161 - val_MinusLogProbMetric: 39.9161 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 35/1000
2023-10-12 08:12:56.645 
Epoch 35/1000 
	 loss: 39.8813, MinusLogProbMetric: 39.8813, val_loss: 39.9831, val_MinusLogProbMetric: 39.9831

Epoch 35: val_loss did not improve from 39.91608
196/196 - 33s - loss: 39.8813 - MinusLogProbMetric: 39.8813 - val_loss: 39.9831 - val_MinusLogProbMetric: 39.9831 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 36/1000
2023-10-12 08:13:30.706 
Epoch 36/1000 
	 loss: 39.3214, MinusLogProbMetric: 39.3214, val_loss: 39.1722, val_MinusLogProbMetric: 39.1722

Epoch 36: val_loss improved from 39.91608 to 39.17224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 39.3214 - MinusLogProbMetric: 39.3214 - val_loss: 39.1722 - val_MinusLogProbMetric: 39.1722 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 37/1000
2023-10-12 08:14:04.948 
Epoch 37/1000 
	 loss: 39.0628, MinusLogProbMetric: 39.0628, val_loss: 39.6114, val_MinusLogProbMetric: 39.6114

Epoch 37: val_loss did not improve from 39.17224
196/196 - 34s - loss: 39.0628 - MinusLogProbMetric: 39.0628 - val_loss: 39.6114 - val_MinusLogProbMetric: 39.6114 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 38/1000
2023-10-12 08:14:38.055 
Epoch 38/1000 
	 loss: 38.8489, MinusLogProbMetric: 38.8489, val_loss: 39.9182, val_MinusLogProbMetric: 39.9182

Epoch 38: val_loss did not improve from 39.17224
196/196 - 33s - loss: 38.8489 - MinusLogProbMetric: 38.8489 - val_loss: 39.9182 - val_MinusLogProbMetric: 39.9182 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 39/1000
2023-10-12 08:15:11.513 
Epoch 39/1000 
	 loss: 38.4021, MinusLogProbMetric: 38.4021, val_loss: 38.4079, val_MinusLogProbMetric: 38.4079

Epoch 39: val_loss improved from 39.17224 to 38.40793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 38.4021 - MinusLogProbMetric: 38.4021 - val_loss: 38.4079 - val_MinusLogProbMetric: 38.4079 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 40/1000
2023-10-12 08:15:45.621 
Epoch 40/1000 
	 loss: 38.4229, MinusLogProbMetric: 38.4229, val_loss: 38.8770, val_MinusLogProbMetric: 38.8770

Epoch 40: val_loss did not improve from 38.40793
196/196 - 33s - loss: 38.4229 - MinusLogProbMetric: 38.4229 - val_loss: 38.8770 - val_MinusLogProbMetric: 38.8770 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 41/1000
2023-10-12 08:16:19.860 
Epoch 41/1000 
	 loss: 38.0757, MinusLogProbMetric: 38.0757, val_loss: 39.2481, val_MinusLogProbMetric: 39.2481

Epoch 41: val_loss did not improve from 38.40793
196/196 - 34s - loss: 38.0757 - MinusLogProbMetric: 38.0757 - val_loss: 39.2481 - val_MinusLogProbMetric: 39.2481 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 42/1000
2023-10-12 08:16:54.072 
Epoch 42/1000 
	 loss: 40.0160, MinusLogProbMetric: 40.0160, val_loss: 37.5212, val_MinusLogProbMetric: 37.5212

Epoch 42: val_loss improved from 38.40793 to 37.52124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 40.0160 - MinusLogProbMetric: 40.0160 - val_loss: 37.5212 - val_MinusLogProbMetric: 37.5212 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 43/1000
2023-10-12 08:17:28.109 
Epoch 43/1000 
	 loss: 37.3231, MinusLogProbMetric: 37.3231, val_loss: 37.5981, val_MinusLogProbMetric: 37.5981

Epoch 43: val_loss did not improve from 37.52124
196/196 - 33s - loss: 37.3231 - MinusLogProbMetric: 37.3231 - val_loss: 37.5981 - val_MinusLogProbMetric: 37.5981 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 44/1000
2023-10-12 08:18:01.753 
Epoch 44/1000 
	 loss: 37.2218, MinusLogProbMetric: 37.2218, val_loss: 37.2439, val_MinusLogProbMetric: 37.2439

Epoch 44: val_loss improved from 37.52124 to 37.24395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 37.2218 - MinusLogProbMetric: 37.2218 - val_loss: 37.2439 - val_MinusLogProbMetric: 37.2439 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 45/1000
2023-10-12 08:18:36.177 
Epoch 45/1000 
	 loss: 36.9828, MinusLogProbMetric: 36.9828, val_loss: 37.2142, val_MinusLogProbMetric: 37.2142

Epoch 45: val_loss improved from 37.24395 to 37.21419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 36.9828 - MinusLogProbMetric: 36.9828 - val_loss: 37.2142 - val_MinusLogProbMetric: 37.2142 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 46/1000
2023-10-12 08:19:10.386 
Epoch 46/1000 
	 loss: 36.8725, MinusLogProbMetric: 36.8725, val_loss: 37.1341, val_MinusLogProbMetric: 37.1341

Epoch 46: val_loss improved from 37.21419 to 37.13408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 36.8725 - MinusLogProbMetric: 36.8725 - val_loss: 37.1341 - val_MinusLogProbMetric: 37.1341 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 47/1000
2023-10-12 08:19:44.541 
Epoch 47/1000 
	 loss: 36.6036, MinusLogProbMetric: 36.6036, val_loss: 37.0092, val_MinusLogProbMetric: 37.0092

Epoch 47: val_loss improved from 37.13408 to 37.00921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 36.6036 - MinusLogProbMetric: 36.6036 - val_loss: 37.0092 - val_MinusLogProbMetric: 37.0092 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 48/1000
2023-10-12 08:20:19.091 
Epoch 48/1000 
	 loss: 36.5326, MinusLogProbMetric: 36.5326, val_loss: 36.9184, val_MinusLogProbMetric: 36.9184

Epoch 48: val_loss improved from 37.00921 to 36.91838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 36.5326 - MinusLogProbMetric: 36.5326 - val_loss: 36.9184 - val_MinusLogProbMetric: 36.9184 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 49/1000
2023-10-12 08:20:53.729 
Epoch 49/1000 
	 loss: 36.5497, MinusLogProbMetric: 36.5497, val_loss: 36.5656, val_MinusLogProbMetric: 36.5656

Epoch 49: val_loss improved from 36.91838 to 36.56555, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 36.5497 - MinusLogProbMetric: 36.5497 - val_loss: 36.5656 - val_MinusLogProbMetric: 36.5656 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 50/1000
2023-10-12 08:21:30.126 
Epoch 50/1000 
	 loss: 36.5895, MinusLogProbMetric: 36.5895, val_loss: 36.8457, val_MinusLogProbMetric: 36.8457

Epoch 50: val_loss did not improve from 36.56555
196/196 - 36s - loss: 36.5895 - MinusLogProbMetric: 36.5895 - val_loss: 36.8457 - val_MinusLogProbMetric: 36.8457 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 51/1000
2023-10-12 08:22:05.126 
Epoch 51/1000 
	 loss: 36.0971, MinusLogProbMetric: 36.0971, val_loss: 36.7834, val_MinusLogProbMetric: 36.7834

Epoch 51: val_loss did not improve from 36.56555
196/196 - 35s - loss: 36.0971 - MinusLogProbMetric: 36.0971 - val_loss: 36.7834 - val_MinusLogProbMetric: 36.7834 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 52/1000
2023-10-12 08:22:40.698 
Epoch 52/1000 
	 loss: 35.8060, MinusLogProbMetric: 35.8060, val_loss: 35.8865, val_MinusLogProbMetric: 35.8865

Epoch 52: val_loss improved from 36.56555 to 35.88651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 35.8060 - MinusLogProbMetric: 35.8060 - val_loss: 35.8865 - val_MinusLogProbMetric: 35.8865 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 53/1000
2023-10-12 08:23:15.262 
Epoch 53/1000 
	 loss: 38.0504, MinusLogProbMetric: 38.0504, val_loss: 36.7897, val_MinusLogProbMetric: 36.7897

Epoch 53: val_loss did not improve from 35.88651
196/196 - 34s - loss: 38.0504 - MinusLogProbMetric: 38.0504 - val_loss: 36.7897 - val_MinusLogProbMetric: 36.7897 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 54/1000
2023-10-12 08:23:49.157 
Epoch 54/1000 
	 loss: 35.7051, MinusLogProbMetric: 35.7051, val_loss: 37.2377, val_MinusLogProbMetric: 37.2377

Epoch 54: val_loss did not improve from 35.88651
196/196 - 34s - loss: 35.7051 - MinusLogProbMetric: 35.7051 - val_loss: 37.2377 - val_MinusLogProbMetric: 37.2377 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 55/1000
2023-10-12 08:24:23.848 
Epoch 55/1000 
	 loss: 35.7361, MinusLogProbMetric: 35.7361, val_loss: 36.2736, val_MinusLogProbMetric: 36.2736

Epoch 55: val_loss did not improve from 35.88651
196/196 - 35s - loss: 35.7361 - MinusLogProbMetric: 35.7361 - val_loss: 36.2736 - val_MinusLogProbMetric: 36.2736 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 56/1000
2023-10-12 08:24:58.018 
Epoch 56/1000 
	 loss: 35.4040, MinusLogProbMetric: 35.4040, val_loss: 36.6683, val_MinusLogProbMetric: 36.6683

Epoch 56: val_loss did not improve from 35.88651
196/196 - 34s - loss: 35.4040 - MinusLogProbMetric: 35.4040 - val_loss: 36.6683 - val_MinusLogProbMetric: 36.6683 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 57/1000
2023-10-12 08:25:31.907 
Epoch 57/1000 
	 loss: 35.1045, MinusLogProbMetric: 35.1045, val_loss: 37.0410, val_MinusLogProbMetric: 37.0410

Epoch 57: val_loss did not improve from 35.88651
196/196 - 34s - loss: 35.1045 - MinusLogProbMetric: 35.1045 - val_loss: 37.0410 - val_MinusLogProbMetric: 37.0410 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 58/1000
2023-10-12 08:26:05.492 
Epoch 58/1000 
	 loss: 34.9967, MinusLogProbMetric: 34.9967, val_loss: 35.3325, val_MinusLogProbMetric: 35.3325

Epoch 58: val_loss improved from 35.88651 to 35.33251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 34.9967 - MinusLogProbMetric: 34.9967 - val_loss: 35.3325 - val_MinusLogProbMetric: 35.3325 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 59/1000
2023-10-12 08:26:39.671 
Epoch 59/1000 
	 loss: 34.9239, MinusLogProbMetric: 34.9239, val_loss: 34.9347, val_MinusLogProbMetric: 34.9347

Epoch 59: val_loss improved from 35.33251 to 34.93467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 34.9239 - MinusLogProbMetric: 34.9239 - val_loss: 34.9347 - val_MinusLogProbMetric: 34.9347 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 60/1000
2023-10-12 08:27:11.983 
Epoch 60/1000 
	 loss: 34.9527, MinusLogProbMetric: 34.9527, val_loss: 34.6329, val_MinusLogProbMetric: 34.6329

Epoch 60: val_loss improved from 34.93467 to 34.63286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 34.9527 - MinusLogProbMetric: 34.9527 - val_loss: 34.6329 - val_MinusLogProbMetric: 34.6329 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 61/1000
2023-10-12 08:27:43.171 
Epoch 61/1000 
	 loss: 34.8918, MinusLogProbMetric: 34.8918, val_loss: 37.1123, val_MinusLogProbMetric: 37.1123

Epoch 61: val_loss did not improve from 34.63286
196/196 - 31s - loss: 34.8918 - MinusLogProbMetric: 34.8918 - val_loss: 37.1123 - val_MinusLogProbMetric: 37.1123 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 62/1000
2023-10-12 08:28:15.363 
Epoch 62/1000 
	 loss: 34.8678, MinusLogProbMetric: 34.8678, val_loss: 34.6221, val_MinusLogProbMetric: 34.6221

Epoch 62: val_loss improved from 34.63286 to 34.62213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 33s - loss: 34.8678 - MinusLogProbMetric: 34.8678 - val_loss: 34.6221 - val_MinusLogProbMetric: 34.6221 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 63/1000
2023-10-12 08:28:50.298 
Epoch 63/1000 
	 loss: 34.3908, MinusLogProbMetric: 34.3908, val_loss: 34.3263, val_MinusLogProbMetric: 34.3263

Epoch 63: val_loss improved from 34.62213 to 34.32628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 34.3908 - MinusLogProbMetric: 34.3908 - val_loss: 34.3263 - val_MinusLogProbMetric: 34.3263 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 64/1000
2023-10-12 08:29:25.253 
Epoch 64/1000 
	 loss: 34.6061, MinusLogProbMetric: 34.6061, val_loss: 34.5654, val_MinusLogProbMetric: 34.5654

Epoch 64: val_loss did not improve from 34.32628
196/196 - 34s - loss: 34.6061 - MinusLogProbMetric: 34.6061 - val_loss: 34.5654 - val_MinusLogProbMetric: 34.5654 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 65/1000
2023-10-12 08:29:58.774 
Epoch 65/1000 
	 loss: 34.4845, MinusLogProbMetric: 34.4845, val_loss: 34.7975, val_MinusLogProbMetric: 34.7975

Epoch 65: val_loss did not improve from 34.32628
196/196 - 34s - loss: 34.4845 - MinusLogProbMetric: 34.4845 - val_loss: 34.7975 - val_MinusLogProbMetric: 34.7975 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 66/1000
2023-10-12 08:30:32.511 
Epoch 66/1000 
	 loss: 34.3841, MinusLogProbMetric: 34.3841, val_loss: 37.7193, val_MinusLogProbMetric: 37.7193

Epoch 66: val_loss did not improve from 34.32628
196/196 - 34s - loss: 34.3841 - MinusLogProbMetric: 34.3841 - val_loss: 37.7193 - val_MinusLogProbMetric: 37.7193 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 67/1000
2023-10-12 08:31:06.229 
Epoch 67/1000 
	 loss: 34.4778, MinusLogProbMetric: 34.4778, val_loss: 34.4173, val_MinusLogProbMetric: 34.4173

Epoch 67: val_loss did not improve from 34.32628
196/196 - 34s - loss: 34.4778 - MinusLogProbMetric: 34.4778 - val_loss: 34.4173 - val_MinusLogProbMetric: 34.4173 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 68/1000
2023-10-12 08:31:40.075 
Epoch 68/1000 
	 loss: 34.5671, MinusLogProbMetric: 34.5671, val_loss: 35.1685, val_MinusLogProbMetric: 35.1685

Epoch 68: val_loss did not improve from 34.32628
196/196 - 34s - loss: 34.5671 - MinusLogProbMetric: 34.5671 - val_loss: 35.1685 - val_MinusLogProbMetric: 35.1685 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 69/1000
2023-10-12 08:32:14.119 
Epoch 69/1000 
	 loss: 34.8576, MinusLogProbMetric: 34.8576, val_loss: 34.9928, val_MinusLogProbMetric: 34.9928

Epoch 69: val_loss did not improve from 34.32628
196/196 - 34s - loss: 34.8576 - MinusLogProbMetric: 34.8576 - val_loss: 34.9928 - val_MinusLogProbMetric: 34.9928 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 70/1000
2023-10-12 08:32:48.242 
Epoch 70/1000 
	 loss: 34.0343, MinusLogProbMetric: 34.0343, val_loss: 34.2424, val_MinusLogProbMetric: 34.2424

Epoch 70: val_loss improved from 34.32628 to 34.24239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 34.0343 - MinusLogProbMetric: 34.0343 - val_loss: 34.2424 - val_MinusLogProbMetric: 34.2424 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 71/1000
2023-10-12 08:33:23.252 
Epoch 71/1000 
	 loss: 33.9445, MinusLogProbMetric: 33.9445, val_loss: 34.2621, val_MinusLogProbMetric: 34.2621

Epoch 71: val_loss did not improve from 34.24239
196/196 - 34s - loss: 33.9445 - MinusLogProbMetric: 33.9445 - val_loss: 34.2621 - val_MinusLogProbMetric: 34.2621 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 72/1000
2023-10-12 08:33:57.543 
Epoch 72/1000 
	 loss: 33.7700, MinusLogProbMetric: 33.7700, val_loss: 35.3253, val_MinusLogProbMetric: 35.3253

Epoch 72: val_loss did not improve from 34.24239
196/196 - 34s - loss: 33.7700 - MinusLogProbMetric: 33.7700 - val_loss: 35.3253 - val_MinusLogProbMetric: 35.3253 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 73/1000
2023-10-12 08:34:31.315 
Epoch 73/1000 
	 loss: 34.5836, MinusLogProbMetric: 34.5836, val_loss: 34.0744, val_MinusLogProbMetric: 34.0744

Epoch 73: val_loss improved from 34.24239 to 34.07439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 34.5836 - MinusLogProbMetric: 34.5836 - val_loss: 34.0744 - val_MinusLogProbMetric: 34.0744 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 74/1000
2023-10-12 08:35:05.870 
Epoch 74/1000 
	 loss: 33.7084, MinusLogProbMetric: 33.7084, val_loss: 33.7886, val_MinusLogProbMetric: 33.7886

Epoch 74: val_loss improved from 34.07439 to 33.78862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 33.7084 - MinusLogProbMetric: 33.7084 - val_loss: 33.7886 - val_MinusLogProbMetric: 33.7886 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 75/1000
2023-10-12 08:35:40.215 
Epoch 75/1000 
	 loss: 34.4832, MinusLogProbMetric: 34.4832, val_loss: 33.5263, val_MinusLogProbMetric: 33.5263

Epoch 75: val_loss improved from 33.78862 to 33.52634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 34.4832 - MinusLogProbMetric: 34.4832 - val_loss: 33.5263 - val_MinusLogProbMetric: 33.5263 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 76/1000
2023-10-12 08:36:14.776 
Epoch 76/1000 
	 loss: 33.5437, MinusLogProbMetric: 33.5437, val_loss: 33.3112, val_MinusLogProbMetric: 33.3112

Epoch 76: val_loss improved from 33.52634 to 33.31120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 33.5437 - MinusLogProbMetric: 33.5437 - val_loss: 33.3112 - val_MinusLogProbMetric: 33.3112 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 77/1000
2023-10-12 08:36:49.910 
Epoch 77/1000 
	 loss: 33.4229, MinusLogProbMetric: 33.4229, val_loss: 35.5416, val_MinusLogProbMetric: 35.5416

Epoch 77: val_loss did not improve from 33.31120
196/196 - 35s - loss: 33.4229 - MinusLogProbMetric: 33.4229 - val_loss: 35.5416 - val_MinusLogProbMetric: 35.5416 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 78/1000
2023-10-12 08:37:24.272 
Epoch 78/1000 
	 loss: 33.6429, MinusLogProbMetric: 33.6429, val_loss: 33.7712, val_MinusLogProbMetric: 33.7712

Epoch 78: val_loss did not improve from 33.31120
196/196 - 34s - loss: 33.6429 - MinusLogProbMetric: 33.6429 - val_loss: 33.7712 - val_MinusLogProbMetric: 33.7712 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 79/1000
2023-10-12 08:37:57.860 
Epoch 79/1000 
	 loss: 33.8208, MinusLogProbMetric: 33.8208, val_loss: 33.8758, val_MinusLogProbMetric: 33.8758

Epoch 79: val_loss did not improve from 33.31120
196/196 - 34s - loss: 33.8208 - MinusLogProbMetric: 33.8208 - val_loss: 33.8758 - val_MinusLogProbMetric: 33.8758 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 80/1000
2023-10-12 08:38:32.166 
Epoch 80/1000 
	 loss: 33.2878, MinusLogProbMetric: 33.2878, val_loss: 33.5475, val_MinusLogProbMetric: 33.5475

Epoch 80: val_loss did not improve from 33.31120
196/196 - 34s - loss: 33.2878 - MinusLogProbMetric: 33.2878 - val_loss: 33.5475 - val_MinusLogProbMetric: 33.5475 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 81/1000
2023-10-12 08:39:05.798 
Epoch 81/1000 
	 loss: 33.1526, MinusLogProbMetric: 33.1526, val_loss: 33.3326, val_MinusLogProbMetric: 33.3326

Epoch 81: val_loss did not improve from 33.31120
196/196 - 34s - loss: 33.1526 - MinusLogProbMetric: 33.1526 - val_loss: 33.3326 - val_MinusLogProbMetric: 33.3326 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 82/1000
2023-10-12 08:39:39.646 
Epoch 82/1000 
	 loss: 33.2208, MinusLogProbMetric: 33.2208, val_loss: 33.5744, val_MinusLogProbMetric: 33.5744

Epoch 82: val_loss did not improve from 33.31120
196/196 - 34s - loss: 33.2208 - MinusLogProbMetric: 33.2208 - val_loss: 33.5744 - val_MinusLogProbMetric: 33.5744 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 83/1000
2023-10-12 08:40:14.135 
Epoch 83/1000 
	 loss: 33.2058, MinusLogProbMetric: 33.2058, val_loss: 34.3391, val_MinusLogProbMetric: 34.3391

Epoch 83: val_loss did not improve from 33.31120
196/196 - 34s - loss: 33.2058 - MinusLogProbMetric: 33.2058 - val_loss: 34.3391 - val_MinusLogProbMetric: 34.3391 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 84/1000
2023-10-12 08:40:48.484 
Epoch 84/1000 
	 loss: 33.6405, MinusLogProbMetric: 33.6405, val_loss: 33.2266, val_MinusLogProbMetric: 33.2266

Epoch 84: val_loss improved from 33.31120 to 33.22664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 33.6405 - MinusLogProbMetric: 33.6405 - val_loss: 33.2266 - val_MinusLogProbMetric: 33.2266 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 85/1000
2023-10-12 08:41:24.147 
Epoch 85/1000 
	 loss: 33.3931, MinusLogProbMetric: 33.3931, val_loss: 33.1855, val_MinusLogProbMetric: 33.1855

Epoch 85: val_loss improved from 33.22664 to 33.18550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 33.3931 - MinusLogProbMetric: 33.3931 - val_loss: 33.1855 - val_MinusLogProbMetric: 33.1855 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 86/1000
2023-10-12 08:41:58.012 
Epoch 86/1000 
	 loss: 33.0888, MinusLogProbMetric: 33.0888, val_loss: 33.5412, val_MinusLogProbMetric: 33.5412

Epoch 86: val_loss did not improve from 33.18550
196/196 - 33s - loss: 33.0888 - MinusLogProbMetric: 33.0888 - val_loss: 33.5412 - val_MinusLogProbMetric: 33.5412 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 87/1000
2023-10-12 08:42:34.902 
Epoch 87/1000 
	 loss: 32.8828, MinusLogProbMetric: 32.8828, val_loss: 33.7210, val_MinusLogProbMetric: 33.7210

Epoch 87: val_loss did not improve from 33.18550
196/196 - 37s - loss: 32.8828 - MinusLogProbMetric: 32.8828 - val_loss: 33.7210 - val_MinusLogProbMetric: 33.7210 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 88/1000
2023-10-12 08:43:08.926 
Epoch 88/1000 
	 loss: 33.1621, MinusLogProbMetric: 33.1621, val_loss: 33.5594, val_MinusLogProbMetric: 33.5594

Epoch 88: val_loss did not improve from 33.18550
196/196 - 34s - loss: 33.1621 - MinusLogProbMetric: 33.1621 - val_loss: 33.5594 - val_MinusLogProbMetric: 33.5594 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 89/1000
2023-10-12 08:43:43.068 
Epoch 89/1000 
	 loss: 33.1353, MinusLogProbMetric: 33.1353, val_loss: 33.0352, val_MinusLogProbMetric: 33.0352

Epoch 89: val_loss improved from 33.18550 to 33.03521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 33.1353 - MinusLogProbMetric: 33.1353 - val_loss: 33.0352 - val_MinusLogProbMetric: 33.0352 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 90/1000
2023-10-12 08:44:17.861 
Epoch 90/1000 
	 loss: 32.7203, MinusLogProbMetric: 32.7203, val_loss: 33.4655, val_MinusLogProbMetric: 33.4655

Epoch 90: val_loss did not improve from 33.03521
196/196 - 34s - loss: 32.7203 - MinusLogProbMetric: 32.7203 - val_loss: 33.4655 - val_MinusLogProbMetric: 33.4655 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 91/1000
2023-10-12 08:44:51.826 
Epoch 91/1000 
	 loss: 32.7438, MinusLogProbMetric: 32.7438, val_loss: 33.1656, val_MinusLogProbMetric: 33.1656

Epoch 91: val_loss did not improve from 33.03521
196/196 - 34s - loss: 32.7438 - MinusLogProbMetric: 32.7438 - val_loss: 33.1656 - val_MinusLogProbMetric: 33.1656 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 92/1000
2023-10-12 08:45:25.597 
Epoch 92/1000 
	 loss: 32.6873, MinusLogProbMetric: 32.6873, val_loss: 34.8019, val_MinusLogProbMetric: 34.8019

Epoch 92: val_loss did not improve from 33.03521
196/196 - 34s - loss: 32.6873 - MinusLogProbMetric: 32.6873 - val_loss: 34.8019 - val_MinusLogProbMetric: 34.8019 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 93/1000
2023-10-12 08:45:59.259 
Epoch 93/1000 
	 loss: 32.6020, MinusLogProbMetric: 32.6020, val_loss: 32.1712, val_MinusLogProbMetric: 32.1712

Epoch 93: val_loss improved from 33.03521 to 32.17123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 32.6020 - MinusLogProbMetric: 32.6020 - val_loss: 32.1712 - val_MinusLogProbMetric: 32.1712 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 94/1000
2023-10-12 08:46:31.596 
Epoch 94/1000 
	 loss: 32.6320, MinusLogProbMetric: 32.6320, val_loss: 32.9530, val_MinusLogProbMetric: 32.9530

Epoch 94: val_loss did not improve from 32.17123
196/196 - 32s - loss: 32.6320 - MinusLogProbMetric: 32.6320 - val_loss: 32.9530 - val_MinusLogProbMetric: 32.9530 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 95/1000
2023-10-12 08:47:04.770 
Epoch 95/1000 
	 loss: 32.5095, MinusLogProbMetric: 32.5095, val_loss: 34.3498, val_MinusLogProbMetric: 34.3498

Epoch 95: val_loss did not improve from 32.17123
196/196 - 33s - loss: 32.5095 - MinusLogProbMetric: 32.5095 - val_loss: 34.3498 - val_MinusLogProbMetric: 34.3498 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 96/1000
2023-10-12 08:47:39.218 
Epoch 96/1000 
	 loss: 32.7663, MinusLogProbMetric: 32.7663, val_loss: 32.5185, val_MinusLogProbMetric: 32.5185

Epoch 96: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.7663 - MinusLogProbMetric: 32.7663 - val_loss: 32.5185 - val_MinusLogProbMetric: 32.5185 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 97/1000
2023-10-12 08:48:14.037 
Epoch 97/1000 
	 loss: 32.8495, MinusLogProbMetric: 32.8495, val_loss: 35.7255, val_MinusLogProbMetric: 35.7255

Epoch 97: val_loss did not improve from 32.17123
196/196 - 35s - loss: 32.8495 - MinusLogProbMetric: 32.8495 - val_loss: 35.7255 - val_MinusLogProbMetric: 35.7255 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 98/1000
2023-10-12 08:48:48.400 
Epoch 98/1000 
	 loss: 32.3674, MinusLogProbMetric: 32.3674, val_loss: 32.4120, val_MinusLogProbMetric: 32.4120

Epoch 98: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.3674 - MinusLogProbMetric: 32.3674 - val_loss: 32.4120 - val_MinusLogProbMetric: 32.4120 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 99/1000
2023-10-12 08:49:22.349 
Epoch 99/1000 
	 loss: 32.7134, MinusLogProbMetric: 32.7134, val_loss: 32.9585, val_MinusLogProbMetric: 32.9585

Epoch 99: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.7134 - MinusLogProbMetric: 32.7134 - val_loss: 32.9585 - val_MinusLogProbMetric: 32.9585 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 100/1000
2023-10-12 08:49:56.053 
Epoch 100/1000 
	 loss: 34.5255, MinusLogProbMetric: 34.5255, val_loss: 32.9628, val_MinusLogProbMetric: 32.9628

Epoch 100: val_loss did not improve from 32.17123
196/196 - 34s - loss: 34.5255 - MinusLogProbMetric: 34.5255 - val_loss: 32.9628 - val_MinusLogProbMetric: 32.9628 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 101/1000
2023-10-12 08:50:29.929 
Epoch 101/1000 
	 loss: 32.3444, MinusLogProbMetric: 32.3444, val_loss: 32.4966, val_MinusLogProbMetric: 32.4966

Epoch 101: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.3444 - MinusLogProbMetric: 32.3444 - val_loss: 32.4966 - val_MinusLogProbMetric: 32.4966 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 102/1000
2023-10-12 08:51:04.513 
Epoch 102/1000 
	 loss: 32.3768, MinusLogProbMetric: 32.3768, val_loss: 32.9128, val_MinusLogProbMetric: 32.9128

Epoch 102: val_loss did not improve from 32.17123
196/196 - 35s - loss: 32.3768 - MinusLogProbMetric: 32.3768 - val_loss: 32.9128 - val_MinusLogProbMetric: 32.9128 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 103/1000
2023-10-12 08:51:38.369 
Epoch 103/1000 
	 loss: 32.5476, MinusLogProbMetric: 32.5476, val_loss: 33.4186, val_MinusLogProbMetric: 33.4186

Epoch 103: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.5476 - MinusLogProbMetric: 32.5476 - val_loss: 33.4186 - val_MinusLogProbMetric: 33.4186 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 104/1000
2023-10-12 08:52:12.063 
Epoch 104/1000 
	 loss: 32.4792, MinusLogProbMetric: 32.4792, val_loss: 32.4221, val_MinusLogProbMetric: 32.4221

Epoch 104: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.4792 - MinusLogProbMetric: 32.4792 - val_loss: 32.4221 - val_MinusLogProbMetric: 32.4221 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 105/1000
2023-10-12 08:52:45.603 
Epoch 105/1000 
	 loss: 32.0618, MinusLogProbMetric: 32.0618, val_loss: 33.4236, val_MinusLogProbMetric: 33.4236

Epoch 105: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.0618 - MinusLogProbMetric: 32.0618 - val_loss: 33.4236 - val_MinusLogProbMetric: 33.4236 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 106/1000
2023-10-12 08:53:19.531 
Epoch 106/1000 
	 loss: 32.4972, MinusLogProbMetric: 32.4972, val_loss: 33.1759, val_MinusLogProbMetric: 33.1759

Epoch 106: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.4972 - MinusLogProbMetric: 32.4972 - val_loss: 33.1759 - val_MinusLogProbMetric: 33.1759 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 107/1000
2023-10-12 08:53:53.004 
Epoch 107/1000 
	 loss: 32.2336, MinusLogProbMetric: 32.2336, val_loss: 32.5640, val_MinusLogProbMetric: 32.5640

Epoch 107: val_loss did not improve from 32.17123
196/196 - 33s - loss: 32.2336 - MinusLogProbMetric: 32.2336 - val_loss: 32.5640 - val_MinusLogProbMetric: 32.5640 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 108/1000
2023-10-12 08:54:26.304 
Epoch 108/1000 
	 loss: 32.0723, MinusLogProbMetric: 32.0723, val_loss: 32.8734, val_MinusLogProbMetric: 32.8734

Epoch 108: val_loss did not improve from 32.17123
196/196 - 33s - loss: 32.0723 - MinusLogProbMetric: 32.0723 - val_loss: 32.8734 - val_MinusLogProbMetric: 32.8734 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 109/1000
2023-10-12 08:54:59.776 
Epoch 109/1000 
	 loss: 32.0227, MinusLogProbMetric: 32.0227, val_loss: 32.2413, val_MinusLogProbMetric: 32.2413

Epoch 109: val_loss did not improve from 32.17123
196/196 - 33s - loss: 32.0227 - MinusLogProbMetric: 32.0227 - val_loss: 32.2413 - val_MinusLogProbMetric: 32.2413 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 110/1000
2023-10-12 08:55:33.354 
Epoch 110/1000 
	 loss: 32.4558, MinusLogProbMetric: 32.4558, val_loss: 36.8742, val_MinusLogProbMetric: 36.8742

Epoch 110: val_loss did not improve from 32.17123
196/196 - 34s - loss: 32.4558 - MinusLogProbMetric: 32.4558 - val_loss: 36.8742 - val_MinusLogProbMetric: 36.8742 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 111/1000
2023-10-12 08:56:07.062 
Epoch 111/1000 
	 loss: 32.2841, MinusLogProbMetric: 32.2841, val_loss: 32.0113, val_MinusLogProbMetric: 32.0113

Epoch 111: val_loss improved from 32.17123 to 32.01133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 32.2841 - MinusLogProbMetric: 32.2841 - val_loss: 32.0113 - val_MinusLogProbMetric: 32.0113 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 112/1000
2023-10-12 08:56:41.418 
Epoch 112/1000 
	 loss: 31.9806, MinusLogProbMetric: 31.9806, val_loss: 32.3156, val_MinusLogProbMetric: 32.3156

Epoch 112: val_loss did not improve from 32.01133
196/196 - 34s - loss: 31.9806 - MinusLogProbMetric: 31.9806 - val_loss: 32.3156 - val_MinusLogProbMetric: 32.3156 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 113/1000
2023-10-12 08:57:15.153 
Epoch 113/1000 
	 loss: 32.4084, MinusLogProbMetric: 32.4084, val_loss: 35.1964, val_MinusLogProbMetric: 35.1964

Epoch 113: val_loss did not improve from 32.01133
196/196 - 34s - loss: 32.4084 - MinusLogProbMetric: 32.4084 - val_loss: 35.1964 - val_MinusLogProbMetric: 35.1964 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 114/1000
2023-10-12 08:57:48.701 
Epoch 114/1000 
	 loss: 31.9861, MinusLogProbMetric: 31.9861, val_loss: 32.1110, val_MinusLogProbMetric: 32.1110

Epoch 114: val_loss did not improve from 32.01133
196/196 - 34s - loss: 31.9861 - MinusLogProbMetric: 31.9861 - val_loss: 32.1110 - val_MinusLogProbMetric: 32.1110 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 115/1000
2023-10-12 08:58:22.436 
Epoch 115/1000 
	 loss: 32.0660, MinusLogProbMetric: 32.0660, val_loss: 32.8155, val_MinusLogProbMetric: 32.8155

Epoch 115: val_loss did not improve from 32.01133
196/196 - 34s - loss: 32.0660 - MinusLogProbMetric: 32.0660 - val_loss: 32.8155 - val_MinusLogProbMetric: 32.8155 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 116/1000
2023-10-12 08:58:56.201 
Epoch 116/1000 
	 loss: 31.9818, MinusLogProbMetric: 31.9818, val_loss: 33.0142, val_MinusLogProbMetric: 33.0142

Epoch 116: val_loss did not improve from 32.01133
196/196 - 34s - loss: 31.9818 - MinusLogProbMetric: 31.9818 - val_loss: 33.0142 - val_MinusLogProbMetric: 33.0142 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 117/1000
2023-10-12 08:59:30.460 
Epoch 117/1000 
	 loss: 31.9794, MinusLogProbMetric: 31.9794, val_loss: 32.0005, val_MinusLogProbMetric: 32.0005

Epoch 117: val_loss improved from 32.01133 to 32.00047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 31.9794 - MinusLogProbMetric: 31.9794 - val_loss: 32.0005 - val_MinusLogProbMetric: 32.0005 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 118/1000
2023-10-12 09:00:05.575 
Epoch 118/1000 
	 loss: 32.1954, MinusLogProbMetric: 32.1954, val_loss: 31.8524, val_MinusLogProbMetric: 31.8524

Epoch 118: val_loss improved from 32.00047 to 31.85238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 32.1954 - MinusLogProbMetric: 32.1954 - val_loss: 31.8524 - val_MinusLogProbMetric: 31.8524 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 119/1000
2023-10-12 09:00:41.696 
Epoch 119/1000 
	 loss: 31.8426, MinusLogProbMetric: 31.8426, val_loss: 32.1623, val_MinusLogProbMetric: 32.1623

Epoch 119: val_loss did not improve from 31.85238
196/196 - 35s - loss: 31.8426 - MinusLogProbMetric: 31.8426 - val_loss: 32.1623 - val_MinusLogProbMetric: 32.1623 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 120/1000
2023-10-12 09:01:15.718 
Epoch 120/1000 
	 loss: 31.8182, MinusLogProbMetric: 31.8182, val_loss: 31.5703, val_MinusLogProbMetric: 31.5703

Epoch 120: val_loss improved from 31.85238 to 31.57027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 31.8182 - MinusLogProbMetric: 31.8182 - val_loss: 31.5703 - val_MinusLogProbMetric: 31.5703 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 121/1000
2023-10-12 09:01:50.255 
Epoch 121/1000 
	 loss: 31.7730, MinusLogProbMetric: 31.7730, val_loss: 32.5406, val_MinusLogProbMetric: 32.5406

Epoch 121: val_loss did not improve from 31.57027
196/196 - 34s - loss: 31.7730 - MinusLogProbMetric: 31.7730 - val_loss: 32.5406 - val_MinusLogProbMetric: 32.5406 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 122/1000
2023-10-12 09:02:24.653 
Epoch 122/1000 
	 loss: 31.8272, MinusLogProbMetric: 31.8272, val_loss: 31.4613, val_MinusLogProbMetric: 31.4613

Epoch 122: val_loss improved from 31.57027 to 31.46129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 31.8272 - MinusLogProbMetric: 31.8272 - val_loss: 31.4613 - val_MinusLogProbMetric: 31.4613 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 123/1000
2023-10-12 09:02:59.440 
Epoch 123/1000 
	 loss: 31.7382, MinusLogProbMetric: 31.7382, val_loss: 31.9597, val_MinusLogProbMetric: 31.9597

Epoch 123: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.7382 - MinusLogProbMetric: 31.7382 - val_loss: 31.9597 - val_MinusLogProbMetric: 31.9597 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 124/1000
2023-10-12 09:03:33.141 
Epoch 124/1000 
	 loss: 31.9251, MinusLogProbMetric: 31.9251, val_loss: 32.9893, val_MinusLogProbMetric: 32.9893

Epoch 124: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.9251 - MinusLogProbMetric: 31.9251 - val_loss: 32.9893 - val_MinusLogProbMetric: 32.9893 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 125/1000
2023-10-12 09:04:07.349 
Epoch 125/1000 
	 loss: 31.6393, MinusLogProbMetric: 31.6393, val_loss: 32.9059, val_MinusLogProbMetric: 32.9059

Epoch 125: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.6393 - MinusLogProbMetric: 31.6393 - val_loss: 32.9059 - val_MinusLogProbMetric: 32.9059 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 126/1000
2023-10-12 09:04:41.033 
Epoch 126/1000 
	 loss: 31.8109, MinusLogProbMetric: 31.8109, val_loss: 33.5010, val_MinusLogProbMetric: 33.5010

Epoch 126: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.8109 - MinusLogProbMetric: 31.8109 - val_loss: 33.5010 - val_MinusLogProbMetric: 33.5010 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 127/1000
2023-10-12 09:05:15.717 
Epoch 127/1000 
	 loss: 32.1846, MinusLogProbMetric: 32.1846, val_loss: 31.6217, val_MinusLogProbMetric: 31.6217

Epoch 127: val_loss did not improve from 31.46129
196/196 - 35s - loss: 32.1846 - MinusLogProbMetric: 32.1846 - val_loss: 31.6217 - val_MinusLogProbMetric: 31.6217 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 128/1000
2023-10-12 09:05:49.585 
Epoch 128/1000 
	 loss: 31.8806, MinusLogProbMetric: 31.8806, val_loss: 32.1037, val_MinusLogProbMetric: 32.1037

Epoch 128: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.8806 - MinusLogProbMetric: 31.8806 - val_loss: 32.1037 - val_MinusLogProbMetric: 32.1037 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 129/1000
2023-10-12 09:06:23.711 
Epoch 129/1000 
	 loss: 31.4239, MinusLogProbMetric: 31.4239, val_loss: 32.0869, val_MinusLogProbMetric: 32.0869

Epoch 129: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.4239 - MinusLogProbMetric: 31.4239 - val_loss: 32.0869 - val_MinusLogProbMetric: 32.0869 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 130/1000
2023-10-12 09:06:57.556 
Epoch 130/1000 
	 loss: 31.7212, MinusLogProbMetric: 31.7212, val_loss: 32.6361, val_MinusLogProbMetric: 32.6361

Epoch 130: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.7212 - MinusLogProbMetric: 31.7212 - val_loss: 32.6361 - val_MinusLogProbMetric: 32.6361 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 131/1000
2023-10-12 09:07:31.600 
Epoch 131/1000 
	 loss: 31.3961, MinusLogProbMetric: 31.3961, val_loss: 32.7416, val_MinusLogProbMetric: 32.7416

Epoch 131: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.3961 - MinusLogProbMetric: 31.3961 - val_loss: 32.7416 - val_MinusLogProbMetric: 32.7416 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 132/1000
2023-10-12 09:08:05.693 
Epoch 132/1000 
	 loss: 32.0668, MinusLogProbMetric: 32.0668, val_loss: 32.6103, val_MinusLogProbMetric: 32.6103

Epoch 132: val_loss did not improve from 31.46129
196/196 - 34s - loss: 32.0668 - MinusLogProbMetric: 32.0668 - val_loss: 32.6103 - val_MinusLogProbMetric: 32.6103 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 133/1000
2023-10-12 09:08:39.950 
Epoch 133/1000 
	 loss: 31.3288, MinusLogProbMetric: 31.3288, val_loss: 31.6324, val_MinusLogProbMetric: 31.6324

Epoch 133: val_loss did not improve from 31.46129
196/196 - 34s - loss: 31.3288 - MinusLogProbMetric: 31.3288 - val_loss: 31.6324 - val_MinusLogProbMetric: 31.6324 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 134/1000
2023-10-12 09:09:13.718 
Epoch 134/1000 
	 loss: 31.6492, MinusLogProbMetric: 31.6492, val_loss: 31.4022, val_MinusLogProbMetric: 31.4022

Epoch 134: val_loss improved from 31.46129 to 31.40225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 31.6492 - MinusLogProbMetric: 31.6492 - val_loss: 31.4022 - val_MinusLogProbMetric: 31.4022 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 135/1000
2023-10-12 09:09:48.408 
Epoch 135/1000 
	 loss: 32.3245, MinusLogProbMetric: 32.3245, val_loss: 32.2492, val_MinusLogProbMetric: 32.2492

Epoch 135: val_loss did not improve from 31.40225
196/196 - 34s - loss: 32.3245 - MinusLogProbMetric: 32.3245 - val_loss: 32.2492 - val_MinusLogProbMetric: 32.2492 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 136/1000
2023-10-12 09:10:22.586 
Epoch 136/1000 
	 loss: 31.6377, MinusLogProbMetric: 31.6377, val_loss: 31.5593, val_MinusLogProbMetric: 31.5593

Epoch 136: val_loss did not improve from 31.40225
196/196 - 34s - loss: 31.6377 - MinusLogProbMetric: 31.6377 - val_loss: 31.5593 - val_MinusLogProbMetric: 31.5593 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 137/1000
2023-10-12 09:10:56.677 
Epoch 137/1000 
	 loss: 31.4355, MinusLogProbMetric: 31.4355, val_loss: 32.5778, val_MinusLogProbMetric: 32.5778

Epoch 137: val_loss did not improve from 31.40225
196/196 - 34s - loss: 31.4355 - MinusLogProbMetric: 31.4355 - val_loss: 32.5778 - val_MinusLogProbMetric: 32.5778 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 138/1000
2023-10-12 09:11:30.272 
Epoch 138/1000 
	 loss: 31.5963, MinusLogProbMetric: 31.5963, val_loss: 31.3302, val_MinusLogProbMetric: 31.3302

Epoch 138: val_loss improved from 31.40225 to 31.33020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 31.5963 - MinusLogProbMetric: 31.5963 - val_loss: 31.3302 - val_MinusLogProbMetric: 31.3302 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 139/1000
2023-10-12 09:12:04.842 
Epoch 139/1000 
	 loss: 31.5100, MinusLogProbMetric: 31.5100, val_loss: 31.9945, val_MinusLogProbMetric: 31.9945

Epoch 139: val_loss did not improve from 31.33020
196/196 - 34s - loss: 31.5100 - MinusLogProbMetric: 31.5100 - val_loss: 31.9945 - val_MinusLogProbMetric: 31.9945 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 140/1000
2023-10-12 09:12:38.603 
Epoch 140/1000 
	 loss: 31.6496, MinusLogProbMetric: 31.6496, val_loss: 31.5577, val_MinusLogProbMetric: 31.5577

Epoch 140: val_loss did not improve from 31.33020
196/196 - 34s - loss: 31.6496 - MinusLogProbMetric: 31.6496 - val_loss: 31.5577 - val_MinusLogProbMetric: 31.5577 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 141/1000
2023-10-12 09:13:12.043 
Epoch 141/1000 
	 loss: 31.2962, MinusLogProbMetric: 31.2962, val_loss: 31.6024, val_MinusLogProbMetric: 31.6024

Epoch 141: val_loss did not improve from 31.33020
196/196 - 33s - loss: 31.2962 - MinusLogProbMetric: 31.2962 - val_loss: 31.6024 - val_MinusLogProbMetric: 31.6024 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 142/1000
2023-10-12 09:13:46.056 
Epoch 142/1000 
	 loss: 31.2271, MinusLogProbMetric: 31.2271, val_loss: 31.4827, val_MinusLogProbMetric: 31.4827

Epoch 142: val_loss did not improve from 31.33020
196/196 - 34s - loss: 31.2271 - MinusLogProbMetric: 31.2271 - val_loss: 31.4827 - val_MinusLogProbMetric: 31.4827 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 143/1000
2023-10-12 09:14:20.083 
Epoch 143/1000 
	 loss: 31.3332, MinusLogProbMetric: 31.3332, val_loss: 31.2149, val_MinusLogProbMetric: 31.2149

Epoch 143: val_loss improved from 31.33020 to 31.21487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 31.3332 - MinusLogProbMetric: 31.3332 - val_loss: 31.2149 - val_MinusLogProbMetric: 31.2149 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 144/1000
2023-10-12 09:14:54.468 
Epoch 144/1000 
	 loss: 31.5566, MinusLogProbMetric: 31.5566, val_loss: 32.0645, val_MinusLogProbMetric: 32.0645

Epoch 144: val_loss did not improve from 31.21487
196/196 - 34s - loss: 31.5566 - MinusLogProbMetric: 31.5566 - val_loss: 32.0645 - val_MinusLogProbMetric: 32.0645 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 145/1000
2023-10-12 09:15:28.078 
Epoch 145/1000 
	 loss: 31.5341, MinusLogProbMetric: 31.5341, val_loss: 31.5472, val_MinusLogProbMetric: 31.5472

Epoch 145: val_loss did not improve from 31.21487
196/196 - 34s - loss: 31.5341 - MinusLogProbMetric: 31.5341 - val_loss: 31.5472 - val_MinusLogProbMetric: 31.5472 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 146/1000
2023-10-12 09:16:01.883 
Epoch 146/1000 
	 loss: 31.2134, MinusLogProbMetric: 31.2134, val_loss: 31.1408, val_MinusLogProbMetric: 31.1408

Epoch 146: val_loss improved from 31.21487 to 31.14085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 31.2134 - MinusLogProbMetric: 31.2134 - val_loss: 31.1408 - val_MinusLogProbMetric: 31.1408 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 147/1000
2023-10-12 09:16:36.403 
Epoch 147/1000 
	 loss: 31.2823, MinusLogProbMetric: 31.2823, val_loss: 32.0608, val_MinusLogProbMetric: 32.0608

Epoch 147: val_loss did not improve from 31.14085
196/196 - 34s - loss: 31.2823 - MinusLogProbMetric: 31.2823 - val_loss: 32.0608 - val_MinusLogProbMetric: 32.0608 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 148/1000
2023-10-12 09:17:10.197 
Epoch 148/1000 
	 loss: 31.0016, MinusLogProbMetric: 31.0016, val_loss: 31.4950, val_MinusLogProbMetric: 31.4950

Epoch 148: val_loss did not improve from 31.14085
196/196 - 34s - loss: 31.0016 - MinusLogProbMetric: 31.0016 - val_loss: 31.4950 - val_MinusLogProbMetric: 31.4950 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 149/1000
2023-10-12 09:17:43.857 
Epoch 149/1000 
	 loss: 31.2375, MinusLogProbMetric: 31.2375, val_loss: 31.6700, val_MinusLogProbMetric: 31.6700

Epoch 149: val_loss did not improve from 31.14085
196/196 - 34s - loss: 31.2375 - MinusLogProbMetric: 31.2375 - val_loss: 31.6700 - val_MinusLogProbMetric: 31.6700 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 150/1000
2023-10-12 09:18:17.793 
Epoch 150/1000 
	 loss: 31.0482, MinusLogProbMetric: 31.0482, val_loss: 32.1064, val_MinusLogProbMetric: 32.1064

Epoch 150: val_loss did not improve from 31.14085
196/196 - 34s - loss: 31.0482 - MinusLogProbMetric: 31.0482 - val_loss: 32.1064 - val_MinusLogProbMetric: 32.1064 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 151/1000
2023-10-12 09:18:51.590 
Epoch 151/1000 
	 loss: 31.5206, MinusLogProbMetric: 31.5206, val_loss: 31.4269, val_MinusLogProbMetric: 31.4269

Epoch 151: val_loss did not improve from 31.14085
196/196 - 34s - loss: 31.5206 - MinusLogProbMetric: 31.5206 - val_loss: 31.4269 - val_MinusLogProbMetric: 31.4269 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 152/1000
2023-10-12 09:19:25.325 
Epoch 152/1000 
	 loss: 31.0254, MinusLogProbMetric: 31.0254, val_loss: 30.8984, val_MinusLogProbMetric: 30.8984

Epoch 152: val_loss improved from 31.14085 to 30.89837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 31.0254 - MinusLogProbMetric: 31.0254 - val_loss: 30.8984 - val_MinusLogProbMetric: 30.8984 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 153/1000
2023-10-12 09:19:59.181 
Epoch 153/1000 
	 loss: 31.2561, MinusLogProbMetric: 31.2561, val_loss: 31.2280, val_MinusLogProbMetric: 31.2280

Epoch 153: val_loss did not improve from 30.89837
196/196 - 33s - loss: 31.2561 - MinusLogProbMetric: 31.2561 - val_loss: 31.2280 - val_MinusLogProbMetric: 31.2280 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 154/1000
2023-10-12 09:20:33.006 
Epoch 154/1000 
	 loss: 31.4185, MinusLogProbMetric: 31.4185, val_loss: 32.1597, val_MinusLogProbMetric: 32.1597

Epoch 154: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.4185 - MinusLogProbMetric: 31.4185 - val_loss: 32.1597 - val_MinusLogProbMetric: 32.1597 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 155/1000
2023-10-12 09:21:05.374 
Epoch 155/1000 
	 loss: 30.9547, MinusLogProbMetric: 30.9547, val_loss: 31.6383, val_MinusLogProbMetric: 31.6383

Epoch 155: val_loss did not improve from 30.89837
196/196 - 32s - loss: 30.9547 - MinusLogProbMetric: 30.9547 - val_loss: 31.6383 - val_MinusLogProbMetric: 31.6383 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 156/1000
2023-10-12 09:21:37.593 
Epoch 156/1000 
	 loss: 31.2224, MinusLogProbMetric: 31.2224, val_loss: 31.7701, val_MinusLogProbMetric: 31.7701

Epoch 156: val_loss did not improve from 30.89837
196/196 - 32s - loss: 31.2224 - MinusLogProbMetric: 31.2224 - val_loss: 31.7701 - val_MinusLogProbMetric: 31.7701 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 157/1000
2023-10-12 09:22:08.462 
Epoch 157/1000 
	 loss: 31.2082, MinusLogProbMetric: 31.2082, val_loss: 31.6922, val_MinusLogProbMetric: 31.6922

Epoch 157: val_loss did not improve from 30.89837
196/196 - 31s - loss: 31.2082 - MinusLogProbMetric: 31.2082 - val_loss: 31.6922 - val_MinusLogProbMetric: 31.6922 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 158/1000
2023-10-12 09:22:39.276 
Epoch 158/1000 
	 loss: 31.2163, MinusLogProbMetric: 31.2163, val_loss: 31.0859, val_MinusLogProbMetric: 31.0859

Epoch 158: val_loss did not improve from 30.89837
196/196 - 31s - loss: 31.2163 - MinusLogProbMetric: 31.2163 - val_loss: 31.0859 - val_MinusLogProbMetric: 31.0859 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 159/1000
2023-10-12 09:23:10.075 
Epoch 159/1000 
	 loss: 31.4111, MinusLogProbMetric: 31.4111, val_loss: 31.9199, val_MinusLogProbMetric: 31.9199

Epoch 159: val_loss did not improve from 30.89837
196/196 - 31s - loss: 31.4111 - MinusLogProbMetric: 31.4111 - val_loss: 31.9199 - val_MinusLogProbMetric: 31.9199 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 160/1000
2023-10-12 09:23:41.812 
Epoch 160/1000 
	 loss: 31.0751, MinusLogProbMetric: 31.0751, val_loss: 31.3490, val_MinusLogProbMetric: 31.3490

Epoch 160: val_loss did not improve from 30.89837
196/196 - 32s - loss: 31.0751 - MinusLogProbMetric: 31.0751 - val_loss: 31.3490 - val_MinusLogProbMetric: 31.3490 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 161/1000
2023-10-12 09:24:15.341 
Epoch 161/1000 
	 loss: 31.0493, MinusLogProbMetric: 31.0493, val_loss: 31.0073, val_MinusLogProbMetric: 31.0073

Epoch 161: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.0493 - MinusLogProbMetric: 31.0493 - val_loss: 31.0073 - val_MinusLogProbMetric: 31.0073 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 162/1000
2023-10-12 09:24:48.936 
Epoch 162/1000 
	 loss: 31.3820, MinusLogProbMetric: 31.3820, val_loss: 31.4286, val_MinusLogProbMetric: 31.4286

Epoch 162: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.3820 - MinusLogProbMetric: 31.3820 - val_loss: 31.4286 - val_MinusLogProbMetric: 31.4286 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 163/1000
2023-10-12 09:25:23.005 
Epoch 163/1000 
	 loss: 31.1025, MinusLogProbMetric: 31.1025, val_loss: 31.0431, val_MinusLogProbMetric: 31.0431

Epoch 163: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.1025 - MinusLogProbMetric: 31.1025 - val_loss: 31.0431 - val_MinusLogProbMetric: 31.0431 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 164/1000
2023-10-12 09:25:56.883 
Epoch 164/1000 
	 loss: 31.3522, MinusLogProbMetric: 31.3522, val_loss: 31.2649, val_MinusLogProbMetric: 31.2649

Epoch 164: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.3522 - MinusLogProbMetric: 31.3522 - val_loss: 31.2649 - val_MinusLogProbMetric: 31.2649 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 165/1000
2023-10-12 09:26:30.459 
Epoch 165/1000 
	 loss: 30.7216, MinusLogProbMetric: 30.7216, val_loss: 34.1691, val_MinusLogProbMetric: 34.1691

Epoch 165: val_loss did not improve from 30.89837
196/196 - 34s - loss: 30.7216 - MinusLogProbMetric: 30.7216 - val_loss: 34.1691 - val_MinusLogProbMetric: 34.1691 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 166/1000
2023-10-12 09:27:04.051 
Epoch 166/1000 
	 loss: 31.0552, MinusLogProbMetric: 31.0552, val_loss: 32.2056, val_MinusLogProbMetric: 32.2056

Epoch 166: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.0552 - MinusLogProbMetric: 31.0552 - val_loss: 32.2056 - val_MinusLogProbMetric: 32.2056 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 167/1000
2023-10-12 09:27:38.027 
Epoch 167/1000 
	 loss: 30.8423, MinusLogProbMetric: 30.8423, val_loss: 31.0342, val_MinusLogProbMetric: 31.0342

Epoch 167: val_loss did not improve from 30.89837
196/196 - 34s - loss: 30.8423 - MinusLogProbMetric: 30.8423 - val_loss: 31.0342 - val_MinusLogProbMetric: 31.0342 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 168/1000
2023-10-12 09:28:11.806 
Epoch 168/1000 
	 loss: 30.8245, MinusLogProbMetric: 30.8245, val_loss: 31.6301, val_MinusLogProbMetric: 31.6301

Epoch 168: val_loss did not improve from 30.89837
196/196 - 34s - loss: 30.8245 - MinusLogProbMetric: 30.8245 - val_loss: 31.6301 - val_MinusLogProbMetric: 31.6301 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 169/1000
2023-10-12 09:28:45.719 
Epoch 169/1000 
	 loss: 31.2171, MinusLogProbMetric: 31.2171, val_loss: 31.3110, val_MinusLogProbMetric: 31.3110

Epoch 169: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.2171 - MinusLogProbMetric: 31.2171 - val_loss: 31.3110 - val_MinusLogProbMetric: 31.3110 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 170/1000
2023-10-12 09:29:19.568 
Epoch 170/1000 
	 loss: 30.8908, MinusLogProbMetric: 30.8908, val_loss: 31.4493, val_MinusLogProbMetric: 31.4493

Epoch 170: val_loss did not improve from 30.89837
196/196 - 34s - loss: 30.8908 - MinusLogProbMetric: 30.8908 - val_loss: 31.4493 - val_MinusLogProbMetric: 31.4493 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 171/1000
2023-10-12 09:29:53.314 
Epoch 171/1000 
	 loss: 31.1880, MinusLogProbMetric: 31.1880, val_loss: 31.3229, val_MinusLogProbMetric: 31.3229

Epoch 171: val_loss did not improve from 30.89837
196/196 - 34s - loss: 31.1880 - MinusLogProbMetric: 31.1880 - val_loss: 31.3229 - val_MinusLogProbMetric: 31.3229 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 172/1000
2023-10-12 09:30:27.059 
Epoch 172/1000 
	 loss: 30.7248, MinusLogProbMetric: 30.7248, val_loss: 31.3101, val_MinusLogProbMetric: 31.3101

Epoch 172: val_loss did not improve from 30.89837
196/196 - 34s - loss: 30.7248 - MinusLogProbMetric: 30.7248 - val_loss: 31.3101 - val_MinusLogProbMetric: 31.3101 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 173/1000
2023-10-12 09:31:00.197 
Epoch 173/1000 
	 loss: 30.8168, MinusLogProbMetric: 30.8168, val_loss: 30.8898, val_MinusLogProbMetric: 30.8898

Epoch 173: val_loss improved from 30.89837 to 30.88981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.8168 - MinusLogProbMetric: 30.8168 - val_loss: 30.8898 - val_MinusLogProbMetric: 30.8898 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 174/1000
2023-10-12 09:31:34.440 
Epoch 174/1000 
	 loss: 31.2960, MinusLogProbMetric: 31.2960, val_loss: 31.5108, val_MinusLogProbMetric: 31.5108

Epoch 174: val_loss did not improve from 30.88981
196/196 - 34s - loss: 31.2960 - MinusLogProbMetric: 31.2960 - val_loss: 31.5108 - val_MinusLogProbMetric: 31.5108 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 175/1000
2023-10-12 09:32:08.203 
Epoch 175/1000 
	 loss: 30.7515, MinusLogProbMetric: 30.7515, val_loss: 31.4179, val_MinusLogProbMetric: 31.4179

Epoch 175: val_loss did not improve from 30.88981
196/196 - 34s - loss: 30.7515 - MinusLogProbMetric: 30.7515 - val_loss: 31.4179 - val_MinusLogProbMetric: 31.4179 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 176/1000
2023-10-12 09:32:42.155 
Epoch 176/1000 
	 loss: 30.9214, MinusLogProbMetric: 30.9214, val_loss: 31.7694, val_MinusLogProbMetric: 31.7694

Epoch 176: val_loss did not improve from 30.88981
196/196 - 34s - loss: 30.9214 - MinusLogProbMetric: 30.9214 - val_loss: 31.7694 - val_MinusLogProbMetric: 31.7694 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 177/1000
2023-10-12 09:33:15.922 
Epoch 177/1000 
	 loss: 30.7282, MinusLogProbMetric: 30.7282, val_loss: 30.9191, val_MinusLogProbMetric: 30.9191

Epoch 177: val_loss did not improve from 30.88981
196/196 - 34s - loss: 30.7282 - MinusLogProbMetric: 30.7282 - val_loss: 30.9191 - val_MinusLogProbMetric: 30.9191 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 178/1000
2023-10-12 09:33:49.812 
Epoch 178/1000 
	 loss: 30.8614, MinusLogProbMetric: 30.8614, val_loss: 31.3223, val_MinusLogProbMetric: 31.3223

Epoch 178: val_loss did not improve from 30.88981
196/196 - 34s - loss: 30.8614 - MinusLogProbMetric: 30.8614 - val_loss: 31.3223 - val_MinusLogProbMetric: 31.3223 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 179/1000
2023-10-12 09:34:23.721 
Epoch 179/1000 
	 loss: 30.8203, MinusLogProbMetric: 30.8203, val_loss: 30.8473, val_MinusLogProbMetric: 30.8473

Epoch 179: val_loss improved from 30.88981 to 30.84732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 30.8203 - MinusLogProbMetric: 30.8203 - val_loss: 30.8473 - val_MinusLogProbMetric: 30.8473 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 180/1000
2023-10-12 09:34:57.932 
Epoch 180/1000 
	 loss: 31.1437, MinusLogProbMetric: 31.1437, val_loss: 30.9006, val_MinusLogProbMetric: 30.9006

Epoch 180: val_loss did not improve from 30.84732
196/196 - 34s - loss: 31.1437 - MinusLogProbMetric: 31.1437 - val_loss: 30.9006 - val_MinusLogProbMetric: 30.9006 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 181/1000
2023-10-12 09:35:31.635 
Epoch 181/1000 
	 loss: 30.7581, MinusLogProbMetric: 30.7581, val_loss: 30.7873, val_MinusLogProbMetric: 30.7873

Epoch 181: val_loss improved from 30.84732 to 30.78730, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.7581 - MinusLogProbMetric: 30.7581 - val_loss: 30.7873 - val_MinusLogProbMetric: 30.7873 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 182/1000
2023-10-12 09:36:05.790 
Epoch 182/1000 
	 loss: 31.2222, MinusLogProbMetric: 31.2222, val_loss: 31.0264, val_MinusLogProbMetric: 31.0264

Epoch 182: val_loss did not improve from 30.78730
196/196 - 34s - loss: 31.2222 - MinusLogProbMetric: 31.2222 - val_loss: 31.0264 - val_MinusLogProbMetric: 31.0264 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 183/1000
2023-10-12 09:36:39.215 
Epoch 183/1000 
	 loss: 30.6070, MinusLogProbMetric: 30.6070, val_loss: 32.6110, val_MinusLogProbMetric: 32.6110

Epoch 183: val_loss did not improve from 30.78730
196/196 - 33s - loss: 30.6070 - MinusLogProbMetric: 30.6070 - val_loss: 32.6110 - val_MinusLogProbMetric: 32.6110 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 184/1000
2023-10-12 09:37:12.768 
Epoch 184/1000 
	 loss: 30.8497, MinusLogProbMetric: 30.8497, val_loss: 30.8617, val_MinusLogProbMetric: 30.8617

Epoch 184: val_loss did not improve from 30.78730
196/196 - 34s - loss: 30.8497 - MinusLogProbMetric: 30.8497 - val_loss: 30.8617 - val_MinusLogProbMetric: 30.8617 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 185/1000
2023-10-12 09:37:46.476 
Epoch 185/1000 
	 loss: 30.5344, MinusLogProbMetric: 30.5344, val_loss: 30.9702, val_MinusLogProbMetric: 30.9702

Epoch 185: val_loss did not improve from 30.78730
196/196 - 34s - loss: 30.5344 - MinusLogProbMetric: 30.5344 - val_loss: 30.9702 - val_MinusLogProbMetric: 30.9702 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 186/1000
2023-10-12 09:38:20.229 
Epoch 186/1000 
	 loss: 30.7269, MinusLogProbMetric: 30.7269, val_loss: 31.0294, val_MinusLogProbMetric: 31.0294

Epoch 186: val_loss did not improve from 30.78730
196/196 - 34s - loss: 30.7269 - MinusLogProbMetric: 30.7269 - val_loss: 31.0294 - val_MinusLogProbMetric: 31.0294 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 187/1000
2023-10-12 09:38:53.731 
Epoch 187/1000 
	 loss: 30.6839, MinusLogProbMetric: 30.6839, val_loss: 31.2964, val_MinusLogProbMetric: 31.2964

Epoch 187: val_loss did not improve from 30.78730
196/196 - 33s - loss: 30.6839 - MinusLogProbMetric: 30.6839 - val_loss: 31.2964 - val_MinusLogProbMetric: 31.2964 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 188/1000
2023-10-12 09:39:27.339 
Epoch 188/1000 
	 loss: 30.6596, MinusLogProbMetric: 30.6596, val_loss: 30.7741, val_MinusLogProbMetric: 30.7741

Epoch 188: val_loss improved from 30.78730 to 30.77415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.6596 - MinusLogProbMetric: 30.6596 - val_loss: 30.7741 - val_MinusLogProbMetric: 30.7741 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 189/1000
2023-10-12 09:40:01.797 
Epoch 189/1000 
	 loss: 30.8906, MinusLogProbMetric: 30.8906, val_loss: 30.6980, val_MinusLogProbMetric: 30.6980

Epoch 189: val_loss improved from 30.77415 to 30.69804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.8906 - MinusLogProbMetric: 30.8906 - val_loss: 30.6980 - val_MinusLogProbMetric: 30.6980 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 190/1000
2023-10-12 09:40:35.889 
Epoch 190/1000 
	 loss: 30.7339, MinusLogProbMetric: 30.7339, val_loss: 31.2633, val_MinusLogProbMetric: 31.2633

Epoch 190: val_loss did not improve from 30.69804
196/196 - 34s - loss: 30.7339 - MinusLogProbMetric: 30.7339 - val_loss: 31.2633 - val_MinusLogProbMetric: 31.2633 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 191/1000
2023-10-12 09:41:09.509 
Epoch 191/1000 
	 loss: 30.8550, MinusLogProbMetric: 30.8550, val_loss: 34.7733, val_MinusLogProbMetric: 34.7733

Epoch 191: val_loss did not improve from 30.69804
196/196 - 34s - loss: 30.8550 - MinusLogProbMetric: 30.8550 - val_loss: 34.7733 - val_MinusLogProbMetric: 34.7733 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 192/1000
2023-10-12 09:41:43.186 
Epoch 192/1000 
	 loss: 30.8806, MinusLogProbMetric: 30.8806, val_loss: 30.5523, val_MinusLogProbMetric: 30.5523

Epoch 192: val_loss improved from 30.69804 to 30.55226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.8806 - MinusLogProbMetric: 30.8806 - val_loss: 30.5523 - val_MinusLogProbMetric: 30.5523 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 193/1000
2023-10-12 09:42:17.719 
Epoch 193/1000 
	 loss: 30.5987, MinusLogProbMetric: 30.5987, val_loss: 30.6355, val_MinusLogProbMetric: 30.6355

Epoch 193: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5987 - MinusLogProbMetric: 30.5987 - val_loss: 30.6355 - val_MinusLogProbMetric: 30.6355 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 194/1000
2023-10-12 09:42:51.523 
Epoch 194/1000 
	 loss: 30.6552, MinusLogProbMetric: 30.6552, val_loss: 30.7001, val_MinusLogProbMetric: 30.7001

Epoch 194: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.6552 - MinusLogProbMetric: 30.6552 - val_loss: 30.7001 - val_MinusLogProbMetric: 30.7001 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 195/1000
2023-10-12 09:43:24.906 
Epoch 195/1000 
	 loss: 30.6363, MinusLogProbMetric: 30.6363, val_loss: 31.1595, val_MinusLogProbMetric: 31.1595

Epoch 195: val_loss did not improve from 30.55226
196/196 - 33s - loss: 30.6363 - MinusLogProbMetric: 30.6363 - val_loss: 31.1595 - val_MinusLogProbMetric: 31.1595 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 196/1000
2023-10-12 09:43:58.894 
Epoch 196/1000 
	 loss: 30.7586, MinusLogProbMetric: 30.7586, val_loss: 30.9489, val_MinusLogProbMetric: 30.9489

Epoch 196: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.7586 - MinusLogProbMetric: 30.7586 - val_loss: 30.9489 - val_MinusLogProbMetric: 30.9489 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 197/1000
2023-10-12 09:44:32.398 
Epoch 197/1000 
	 loss: 30.5505, MinusLogProbMetric: 30.5505, val_loss: 30.8412, val_MinusLogProbMetric: 30.8412

Epoch 197: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5505 - MinusLogProbMetric: 30.5505 - val_loss: 30.8412 - val_MinusLogProbMetric: 30.8412 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 198/1000
2023-10-12 09:45:06.219 
Epoch 198/1000 
	 loss: 30.3495, MinusLogProbMetric: 30.3495, val_loss: 30.9205, val_MinusLogProbMetric: 30.9205

Epoch 198: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.3495 - MinusLogProbMetric: 30.3495 - val_loss: 30.9205 - val_MinusLogProbMetric: 30.9205 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 199/1000
2023-10-12 09:45:39.940 
Epoch 199/1000 
	 loss: 30.5851, MinusLogProbMetric: 30.5851, val_loss: 31.4900, val_MinusLogProbMetric: 31.4900

Epoch 199: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5851 - MinusLogProbMetric: 30.5851 - val_loss: 31.4900 - val_MinusLogProbMetric: 31.4900 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 200/1000
2023-10-12 09:46:13.642 
Epoch 200/1000 
	 loss: 30.5398, MinusLogProbMetric: 30.5398, val_loss: 30.8648, val_MinusLogProbMetric: 30.8648

Epoch 200: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5398 - MinusLogProbMetric: 30.5398 - val_loss: 30.8648 - val_MinusLogProbMetric: 30.8648 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 201/1000
2023-10-12 09:46:47.368 
Epoch 201/1000 
	 loss: 30.7105, MinusLogProbMetric: 30.7105, val_loss: 31.0723, val_MinusLogProbMetric: 31.0723

Epoch 201: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.7105 - MinusLogProbMetric: 30.7105 - val_loss: 31.0723 - val_MinusLogProbMetric: 31.0723 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 202/1000
2023-10-12 09:47:21.399 
Epoch 202/1000 
	 loss: 30.5238, MinusLogProbMetric: 30.5238, val_loss: 31.1000, val_MinusLogProbMetric: 31.1000

Epoch 202: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5238 - MinusLogProbMetric: 30.5238 - val_loss: 31.1000 - val_MinusLogProbMetric: 31.1000 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 203/1000
2023-10-12 09:47:54.776 
Epoch 203/1000 
	 loss: 30.7748, MinusLogProbMetric: 30.7748, val_loss: 30.5875, val_MinusLogProbMetric: 30.5875

Epoch 203: val_loss did not improve from 30.55226
196/196 - 33s - loss: 30.7748 - MinusLogProbMetric: 30.7748 - val_loss: 30.5875 - val_MinusLogProbMetric: 30.5875 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 204/1000
2023-10-12 09:48:28.469 
Epoch 204/1000 
	 loss: 30.4343, MinusLogProbMetric: 30.4343, val_loss: 30.9540, val_MinusLogProbMetric: 30.9540

Epoch 204: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.4343 - MinusLogProbMetric: 30.4343 - val_loss: 30.9540 - val_MinusLogProbMetric: 30.9540 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 205/1000
2023-10-12 09:49:02.336 
Epoch 205/1000 
	 loss: 30.3379, MinusLogProbMetric: 30.3379, val_loss: 31.5974, val_MinusLogProbMetric: 31.5974

Epoch 205: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.3379 - MinusLogProbMetric: 30.3379 - val_loss: 31.5974 - val_MinusLogProbMetric: 31.5974 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 206/1000
2023-10-12 09:49:36.238 
Epoch 206/1000 
	 loss: 30.5289, MinusLogProbMetric: 30.5289, val_loss: 30.7065, val_MinusLogProbMetric: 30.7065

Epoch 206: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5289 - MinusLogProbMetric: 30.5289 - val_loss: 30.7065 - val_MinusLogProbMetric: 30.7065 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 207/1000
2023-10-12 09:50:09.957 
Epoch 207/1000 
	 loss: 32.0954, MinusLogProbMetric: 32.0954, val_loss: 31.0199, val_MinusLogProbMetric: 31.0199

Epoch 207: val_loss did not improve from 30.55226
196/196 - 34s - loss: 32.0954 - MinusLogProbMetric: 32.0954 - val_loss: 31.0199 - val_MinusLogProbMetric: 31.0199 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 208/1000
2023-10-12 09:50:43.808 
Epoch 208/1000 
	 loss: 30.3885, MinusLogProbMetric: 30.3885, val_loss: 31.7303, val_MinusLogProbMetric: 31.7303

Epoch 208: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.3885 - MinusLogProbMetric: 30.3885 - val_loss: 31.7303 - val_MinusLogProbMetric: 31.7303 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 209/1000
2023-10-12 09:51:17.646 
Epoch 209/1000 
	 loss: 30.5377, MinusLogProbMetric: 30.5377, val_loss: 31.1546, val_MinusLogProbMetric: 31.1546

Epoch 209: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.5377 - MinusLogProbMetric: 30.5377 - val_loss: 31.1546 - val_MinusLogProbMetric: 31.1546 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 210/1000
2023-10-12 09:51:51.443 
Epoch 210/1000 
	 loss: 30.2288, MinusLogProbMetric: 30.2288, val_loss: 30.9696, val_MinusLogProbMetric: 30.9696

Epoch 210: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.2288 - MinusLogProbMetric: 30.2288 - val_loss: 30.9696 - val_MinusLogProbMetric: 30.9696 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 211/1000
2023-10-12 09:52:25.231 
Epoch 211/1000 
	 loss: 30.2727, MinusLogProbMetric: 30.2727, val_loss: 30.5658, val_MinusLogProbMetric: 30.5658

Epoch 211: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.2727 - MinusLogProbMetric: 30.2727 - val_loss: 30.5658 - val_MinusLogProbMetric: 30.5658 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 212/1000
2023-10-12 09:52:59.077 
Epoch 212/1000 
	 loss: 30.3371, MinusLogProbMetric: 30.3371, val_loss: 30.7337, val_MinusLogProbMetric: 30.7337

Epoch 212: val_loss did not improve from 30.55226
196/196 - 34s - loss: 30.3371 - MinusLogProbMetric: 30.3371 - val_loss: 30.7337 - val_MinusLogProbMetric: 30.7337 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 213/1000
2023-10-12 09:53:32.886 
Epoch 213/1000 
	 loss: 30.4524, MinusLogProbMetric: 30.4524, val_loss: 30.3460, val_MinusLogProbMetric: 30.3460

Epoch 213: val_loss improved from 30.55226 to 30.34602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.4524 - MinusLogProbMetric: 30.4524 - val_loss: 30.3460 - val_MinusLogProbMetric: 30.3460 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 214/1000
2023-10-12 09:54:07.544 
Epoch 214/1000 
	 loss: 30.7526, MinusLogProbMetric: 30.7526, val_loss: 30.7145, val_MinusLogProbMetric: 30.7145

Epoch 214: val_loss did not improve from 30.34602
196/196 - 34s - loss: 30.7526 - MinusLogProbMetric: 30.7526 - val_loss: 30.7145 - val_MinusLogProbMetric: 30.7145 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 215/1000
2023-10-12 09:54:41.216 
Epoch 215/1000 
	 loss: 30.4871, MinusLogProbMetric: 30.4871, val_loss: 30.8710, val_MinusLogProbMetric: 30.8710

Epoch 215: val_loss did not improve from 30.34602
196/196 - 34s - loss: 30.4871 - MinusLogProbMetric: 30.4871 - val_loss: 30.8710 - val_MinusLogProbMetric: 30.8710 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 216/1000
2023-10-12 09:55:14.705 
Epoch 216/1000 
	 loss: 30.5187, MinusLogProbMetric: 30.5187, val_loss: 30.4002, val_MinusLogProbMetric: 30.4002

Epoch 216: val_loss did not improve from 30.34602
196/196 - 33s - loss: 30.5187 - MinusLogProbMetric: 30.5187 - val_loss: 30.4002 - val_MinusLogProbMetric: 30.4002 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 217/1000
2023-10-12 09:55:48.505 
Epoch 217/1000 
	 loss: 30.2225, MinusLogProbMetric: 30.2225, val_loss: 30.3457, val_MinusLogProbMetric: 30.3457

Epoch 217: val_loss improved from 30.34602 to 30.34568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 30.2225 - MinusLogProbMetric: 30.2225 - val_loss: 30.3457 - val_MinusLogProbMetric: 30.3457 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 218/1000
2023-10-12 09:56:26.320 
Epoch 218/1000 
	 loss: 30.5771, MinusLogProbMetric: 30.5771, val_loss: 31.3154, val_MinusLogProbMetric: 31.3154

Epoch 218: val_loss did not improve from 30.34568
196/196 - 34s - loss: 30.5771 - MinusLogProbMetric: 30.5771 - val_loss: 31.3154 - val_MinusLogProbMetric: 31.3154 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 219/1000
2023-10-12 09:56:59.919 
Epoch 219/1000 
	 loss: 30.5413, MinusLogProbMetric: 30.5413, val_loss: 30.5243, val_MinusLogProbMetric: 30.5243

Epoch 219: val_loss did not improve from 30.34568
196/196 - 34s - loss: 30.5413 - MinusLogProbMetric: 30.5413 - val_loss: 30.5243 - val_MinusLogProbMetric: 30.5243 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 220/1000
2023-10-12 09:57:33.670 
Epoch 220/1000 
	 loss: 30.5782, MinusLogProbMetric: 30.5782, val_loss: 30.2799, val_MinusLogProbMetric: 30.2799

Epoch 220: val_loss improved from 30.34568 to 30.27993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.5782 - MinusLogProbMetric: 30.5782 - val_loss: 30.2799 - val_MinusLogProbMetric: 30.2799 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 221/1000
2023-10-12 09:58:08.398 
Epoch 221/1000 
	 loss: 30.2861, MinusLogProbMetric: 30.2861, val_loss: 30.6782, val_MinusLogProbMetric: 30.6782

Epoch 221: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.2861 - MinusLogProbMetric: 30.2861 - val_loss: 30.6782 - val_MinusLogProbMetric: 30.6782 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 222/1000
2023-10-12 09:58:41.868 
Epoch 222/1000 
	 loss: 30.8243, MinusLogProbMetric: 30.8243, val_loss: 31.2598, val_MinusLogProbMetric: 31.2598

Epoch 222: val_loss did not improve from 30.27993
196/196 - 33s - loss: 30.8243 - MinusLogProbMetric: 30.8243 - val_loss: 31.2598 - val_MinusLogProbMetric: 31.2598 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 223/1000
2023-10-12 09:59:15.488 
Epoch 223/1000 
	 loss: 30.3606, MinusLogProbMetric: 30.3606, val_loss: 30.3440, val_MinusLogProbMetric: 30.3440

Epoch 223: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3606 - MinusLogProbMetric: 30.3606 - val_loss: 30.3440 - val_MinusLogProbMetric: 30.3440 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 224/1000
2023-10-12 09:59:48.915 
Epoch 224/1000 
	 loss: 30.4467, MinusLogProbMetric: 30.4467, val_loss: 30.5043, val_MinusLogProbMetric: 30.5043

Epoch 224: val_loss did not improve from 30.27993
196/196 - 33s - loss: 30.4467 - MinusLogProbMetric: 30.4467 - val_loss: 30.5043 - val_MinusLogProbMetric: 30.5043 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 225/1000
2023-10-12 10:00:22.847 
Epoch 225/1000 
	 loss: 30.9068, MinusLogProbMetric: 30.9068, val_loss: 32.4023, val_MinusLogProbMetric: 32.4023

Epoch 225: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.9068 - MinusLogProbMetric: 30.9068 - val_loss: 32.4023 - val_MinusLogProbMetric: 32.4023 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 226/1000
2023-10-12 10:00:56.582 
Epoch 226/1000 
	 loss: 30.4889, MinusLogProbMetric: 30.4889, val_loss: 30.3243, val_MinusLogProbMetric: 30.3243

Epoch 226: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.4889 - MinusLogProbMetric: 30.4889 - val_loss: 30.3243 - val_MinusLogProbMetric: 30.3243 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 227/1000
2023-10-12 10:01:30.257 
Epoch 227/1000 
	 loss: 30.1394, MinusLogProbMetric: 30.1394, val_loss: 30.3471, val_MinusLogProbMetric: 30.3471

Epoch 227: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.1394 - MinusLogProbMetric: 30.1394 - val_loss: 30.3471 - val_MinusLogProbMetric: 30.3471 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 228/1000
2023-10-12 10:02:03.991 
Epoch 228/1000 
	 loss: 30.3028, MinusLogProbMetric: 30.3028, val_loss: 30.6331, val_MinusLogProbMetric: 30.6331

Epoch 228: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3028 - MinusLogProbMetric: 30.3028 - val_loss: 30.6331 - val_MinusLogProbMetric: 30.6331 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 229/1000
2023-10-12 10:02:37.639 
Epoch 229/1000 
	 loss: 30.5468, MinusLogProbMetric: 30.5468, val_loss: 31.8804, val_MinusLogProbMetric: 31.8804

Epoch 229: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.5468 - MinusLogProbMetric: 30.5468 - val_loss: 31.8804 - val_MinusLogProbMetric: 31.8804 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 230/1000
2023-10-12 10:03:11.528 
Epoch 230/1000 
	 loss: 30.2768, MinusLogProbMetric: 30.2768, val_loss: 31.0445, val_MinusLogProbMetric: 31.0445

Epoch 230: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.2768 - MinusLogProbMetric: 30.2768 - val_loss: 31.0445 - val_MinusLogProbMetric: 31.0445 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 231/1000
2023-10-12 10:03:44.890 
Epoch 231/1000 
	 loss: 30.5386, MinusLogProbMetric: 30.5386, val_loss: 31.0642, val_MinusLogProbMetric: 31.0642

Epoch 231: val_loss did not improve from 30.27993
196/196 - 33s - loss: 30.5386 - MinusLogProbMetric: 30.5386 - val_loss: 31.0642 - val_MinusLogProbMetric: 31.0642 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 232/1000
2023-10-12 10:04:18.572 
Epoch 232/1000 
	 loss: 30.3028, MinusLogProbMetric: 30.3028, val_loss: 30.5721, val_MinusLogProbMetric: 30.5721

Epoch 232: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3028 - MinusLogProbMetric: 30.3028 - val_loss: 30.5721 - val_MinusLogProbMetric: 30.5721 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 233/1000
2023-10-12 10:04:52.290 
Epoch 233/1000 
	 loss: 30.1845, MinusLogProbMetric: 30.1845, val_loss: 30.3327, val_MinusLogProbMetric: 30.3327

Epoch 233: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.1845 - MinusLogProbMetric: 30.1845 - val_loss: 30.3327 - val_MinusLogProbMetric: 30.3327 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 234/1000
2023-10-12 10:05:26.165 
Epoch 234/1000 
	 loss: 30.3337, MinusLogProbMetric: 30.3337, val_loss: 31.2829, val_MinusLogProbMetric: 31.2829

Epoch 234: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3337 - MinusLogProbMetric: 30.3337 - val_loss: 31.2829 - val_MinusLogProbMetric: 31.2829 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 235/1000
2023-10-12 10:06:00.116 
Epoch 235/1000 
	 loss: 30.2698, MinusLogProbMetric: 30.2698, val_loss: 34.5535, val_MinusLogProbMetric: 34.5535

Epoch 235: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.2698 - MinusLogProbMetric: 30.2698 - val_loss: 34.5535 - val_MinusLogProbMetric: 34.5535 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 236/1000
2023-10-12 10:06:33.782 
Epoch 236/1000 
	 loss: 30.5205, MinusLogProbMetric: 30.5205, val_loss: 30.3470, val_MinusLogProbMetric: 30.3470

Epoch 236: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.5205 - MinusLogProbMetric: 30.5205 - val_loss: 30.3470 - val_MinusLogProbMetric: 30.3470 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 237/1000
2023-10-12 10:07:07.583 
Epoch 237/1000 
	 loss: 30.3873, MinusLogProbMetric: 30.3873, val_loss: 30.7763, val_MinusLogProbMetric: 30.7763

Epoch 237: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3873 - MinusLogProbMetric: 30.3873 - val_loss: 30.7763 - val_MinusLogProbMetric: 30.7763 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 238/1000
2023-10-12 10:07:41.407 
Epoch 238/1000 
	 loss: 30.0642, MinusLogProbMetric: 30.0642, val_loss: 30.4213, val_MinusLogProbMetric: 30.4213

Epoch 238: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.0642 - MinusLogProbMetric: 30.0642 - val_loss: 30.4213 - val_MinusLogProbMetric: 30.4213 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 239/1000
2023-10-12 10:08:15.079 
Epoch 239/1000 
	 loss: 30.3196, MinusLogProbMetric: 30.3196, val_loss: 30.8654, val_MinusLogProbMetric: 30.8654

Epoch 239: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3196 - MinusLogProbMetric: 30.3196 - val_loss: 30.8654 - val_MinusLogProbMetric: 30.8654 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 240/1000
2023-10-12 10:08:48.903 
Epoch 240/1000 
	 loss: 30.1600, MinusLogProbMetric: 30.1600, val_loss: 30.6303, val_MinusLogProbMetric: 30.6303

Epoch 240: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.1600 - MinusLogProbMetric: 30.1600 - val_loss: 30.6303 - val_MinusLogProbMetric: 30.6303 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 241/1000
2023-10-12 10:09:22.674 
Epoch 241/1000 
	 loss: 30.3169, MinusLogProbMetric: 30.3169, val_loss: 30.3444, val_MinusLogProbMetric: 30.3444

Epoch 241: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.3169 - MinusLogProbMetric: 30.3169 - val_loss: 30.3444 - val_MinusLogProbMetric: 30.3444 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 242/1000
2023-10-12 10:09:56.467 
Epoch 242/1000 
	 loss: 30.1889, MinusLogProbMetric: 30.1889, val_loss: 30.6060, val_MinusLogProbMetric: 30.6060

Epoch 242: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.1889 - MinusLogProbMetric: 30.1889 - val_loss: 30.6060 - val_MinusLogProbMetric: 30.6060 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 243/1000
2023-10-12 10:10:30.120 
Epoch 243/1000 
	 loss: 30.2348, MinusLogProbMetric: 30.2348, val_loss: 30.4946, val_MinusLogProbMetric: 30.4946

Epoch 243: val_loss did not improve from 30.27993
196/196 - 34s - loss: 30.2348 - MinusLogProbMetric: 30.2348 - val_loss: 30.4946 - val_MinusLogProbMetric: 30.4946 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 244/1000
2023-10-12 10:11:03.976 
Epoch 244/1000 
	 loss: 30.6908, MinusLogProbMetric: 30.6908, val_loss: 30.2153, val_MinusLogProbMetric: 30.2153

Epoch 244: val_loss improved from 30.27993 to 30.21534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.6908 - MinusLogProbMetric: 30.6908 - val_loss: 30.2153 - val_MinusLogProbMetric: 30.2153 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 245/1000
2023-10-12 10:11:38.279 
Epoch 245/1000 
	 loss: 30.2421, MinusLogProbMetric: 30.2421, val_loss: 30.4665, val_MinusLogProbMetric: 30.4665

Epoch 245: val_loss did not improve from 30.21534
196/196 - 34s - loss: 30.2421 - MinusLogProbMetric: 30.2421 - val_loss: 30.4665 - val_MinusLogProbMetric: 30.4665 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 246/1000
2023-10-12 10:12:12.129 
Epoch 246/1000 
	 loss: 30.0229, MinusLogProbMetric: 30.0229, val_loss: 30.4219, val_MinusLogProbMetric: 30.4219

Epoch 246: val_loss did not improve from 30.21534
196/196 - 34s - loss: 30.0229 - MinusLogProbMetric: 30.0229 - val_loss: 30.4219 - val_MinusLogProbMetric: 30.4219 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 247/1000
2023-10-12 10:12:45.481 
Epoch 247/1000 
	 loss: 30.2587, MinusLogProbMetric: 30.2587, val_loss: 31.1479, val_MinusLogProbMetric: 31.1479

Epoch 247: val_loss did not improve from 30.21534
196/196 - 33s - loss: 30.2587 - MinusLogProbMetric: 30.2587 - val_loss: 31.1479 - val_MinusLogProbMetric: 31.1479 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 248/1000
2023-10-12 10:13:19.506 
Epoch 248/1000 
	 loss: 29.9524, MinusLogProbMetric: 29.9524, val_loss: 30.2154, val_MinusLogProbMetric: 30.2154

Epoch 248: val_loss did not improve from 30.21534
196/196 - 34s - loss: 29.9524 - MinusLogProbMetric: 29.9524 - val_loss: 30.2154 - val_MinusLogProbMetric: 30.2154 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 249/1000
2023-10-12 10:13:53.404 
Epoch 249/1000 
	 loss: 30.2245, MinusLogProbMetric: 30.2245, val_loss: 30.5103, val_MinusLogProbMetric: 30.5103

Epoch 249: val_loss did not improve from 30.21534
196/196 - 34s - loss: 30.2245 - MinusLogProbMetric: 30.2245 - val_loss: 30.5103 - val_MinusLogProbMetric: 30.5103 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 250/1000
2023-10-12 10:14:27.202 
Epoch 250/1000 
	 loss: 30.3896, MinusLogProbMetric: 30.3896, val_loss: 30.1012, val_MinusLogProbMetric: 30.1012

Epoch 250: val_loss improved from 30.21534 to 30.10121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.3896 - MinusLogProbMetric: 30.3896 - val_loss: 30.1012 - val_MinusLogProbMetric: 30.1012 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 251/1000
2023-10-12 10:15:01.292 
Epoch 251/1000 
	 loss: 30.1285, MinusLogProbMetric: 30.1285, val_loss: 30.3554, val_MinusLogProbMetric: 30.3554

Epoch 251: val_loss did not improve from 30.10121
196/196 - 33s - loss: 30.1285 - MinusLogProbMetric: 30.1285 - val_loss: 30.3554 - val_MinusLogProbMetric: 30.3554 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 252/1000
2023-10-12 10:15:34.919 
Epoch 252/1000 
	 loss: 30.2673, MinusLogProbMetric: 30.2673, val_loss: 30.9273, val_MinusLogProbMetric: 30.9273

Epoch 252: val_loss did not improve from 30.10121
196/196 - 34s - loss: 30.2673 - MinusLogProbMetric: 30.2673 - val_loss: 30.9273 - val_MinusLogProbMetric: 30.9273 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 253/1000
2023-10-12 10:16:08.456 
Epoch 253/1000 
	 loss: 30.3121, MinusLogProbMetric: 30.3121, val_loss: 30.2892, val_MinusLogProbMetric: 30.2892

Epoch 253: val_loss did not improve from 30.10121
196/196 - 34s - loss: 30.3121 - MinusLogProbMetric: 30.3121 - val_loss: 30.2892 - val_MinusLogProbMetric: 30.2892 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 254/1000
2023-10-12 10:16:42.513 
Epoch 254/1000 
	 loss: 30.1424, MinusLogProbMetric: 30.1424, val_loss: 30.3062, val_MinusLogProbMetric: 30.3062

Epoch 254: val_loss did not improve from 30.10121
196/196 - 34s - loss: 30.1424 - MinusLogProbMetric: 30.1424 - val_loss: 30.3062 - val_MinusLogProbMetric: 30.3062 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 255/1000
2023-10-12 10:17:16.595 
Epoch 255/1000 
	 loss: 30.0347, MinusLogProbMetric: 30.0347, val_loss: 30.9786, val_MinusLogProbMetric: 30.9786

Epoch 255: val_loss did not improve from 30.10121
196/196 - 34s - loss: 30.0347 - MinusLogProbMetric: 30.0347 - val_loss: 30.9786 - val_MinusLogProbMetric: 30.9786 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 256/1000
2023-10-12 10:17:50.612 
Epoch 256/1000 
	 loss: 30.1427, MinusLogProbMetric: 30.1427, val_loss: 30.1000, val_MinusLogProbMetric: 30.1000

Epoch 256: val_loss improved from 30.10121 to 30.10004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 30.1427 - MinusLogProbMetric: 30.1427 - val_loss: 30.1000 - val_MinusLogProbMetric: 30.1000 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 257/1000
2023-10-12 10:18:25.048 
Epoch 257/1000 
	 loss: 30.2136, MinusLogProbMetric: 30.2136, val_loss: 30.1147, val_MinusLogProbMetric: 30.1147

Epoch 257: val_loss did not improve from 30.10004
196/196 - 34s - loss: 30.2136 - MinusLogProbMetric: 30.2136 - val_loss: 30.1147 - val_MinusLogProbMetric: 30.1147 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 258/1000
2023-10-12 10:18:58.927 
Epoch 258/1000 
	 loss: 30.6239, MinusLogProbMetric: 30.6239, val_loss: 30.2710, val_MinusLogProbMetric: 30.2710

Epoch 258: val_loss did not improve from 30.10004
196/196 - 34s - loss: 30.6239 - MinusLogProbMetric: 30.6239 - val_loss: 30.2710 - val_MinusLogProbMetric: 30.2710 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 259/1000
2023-10-12 10:19:32.647 
Epoch 259/1000 
	 loss: 30.0205, MinusLogProbMetric: 30.0205, val_loss: 29.9898, val_MinusLogProbMetric: 29.9898

Epoch 259: val_loss improved from 30.10004 to 29.98977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 30.0205 - MinusLogProbMetric: 30.0205 - val_loss: 29.9898 - val_MinusLogProbMetric: 29.9898 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 260/1000
2023-10-12 10:20:06.988 
Epoch 260/1000 
	 loss: 29.9942, MinusLogProbMetric: 29.9942, val_loss: 30.0124, val_MinusLogProbMetric: 30.0124

Epoch 260: val_loss did not improve from 29.98977
196/196 - 34s - loss: 29.9942 - MinusLogProbMetric: 29.9942 - val_loss: 30.0124 - val_MinusLogProbMetric: 30.0124 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 261/1000
2023-10-12 10:20:40.758 
Epoch 261/1000 
	 loss: 30.2818, MinusLogProbMetric: 30.2818, val_loss: 30.1542, val_MinusLogProbMetric: 30.1542

Epoch 261: val_loss did not improve from 29.98977
196/196 - 34s - loss: 30.2818 - MinusLogProbMetric: 30.2818 - val_loss: 30.1542 - val_MinusLogProbMetric: 30.1542 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 262/1000
2023-10-12 10:21:14.592 
Epoch 262/1000 
	 loss: 30.9031, MinusLogProbMetric: 30.9031, val_loss: 30.3270, val_MinusLogProbMetric: 30.3270

Epoch 262: val_loss did not improve from 29.98977
196/196 - 34s - loss: 30.9031 - MinusLogProbMetric: 30.9031 - val_loss: 30.3270 - val_MinusLogProbMetric: 30.3270 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 263/1000
2023-10-12 10:21:48.515 
Epoch 263/1000 
	 loss: 30.3090, MinusLogProbMetric: 30.3090, val_loss: 30.2101, val_MinusLogProbMetric: 30.2101

Epoch 263: val_loss did not improve from 29.98977
196/196 - 34s - loss: 30.3090 - MinusLogProbMetric: 30.3090 - val_loss: 30.2101 - val_MinusLogProbMetric: 30.2101 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 264/1000
2023-10-12 10:22:22.325 
Epoch 264/1000 
	 loss: 29.8632, MinusLogProbMetric: 29.8632, val_loss: 30.0968, val_MinusLogProbMetric: 30.0968

Epoch 264: val_loss did not improve from 29.98977
196/196 - 34s - loss: 29.8632 - MinusLogProbMetric: 29.8632 - val_loss: 30.0968 - val_MinusLogProbMetric: 30.0968 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 265/1000
2023-10-12 10:22:56.202 
Epoch 265/1000 
	 loss: 30.0573, MinusLogProbMetric: 30.0573, val_loss: 30.0113, val_MinusLogProbMetric: 30.0113

Epoch 265: val_loss did not improve from 29.98977
196/196 - 34s - loss: 30.0573 - MinusLogProbMetric: 30.0573 - val_loss: 30.0113 - val_MinusLogProbMetric: 30.0113 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 266/1000
2023-10-12 10:23:30.041 
Epoch 266/1000 
	 loss: 29.9761, MinusLogProbMetric: 29.9761, val_loss: 30.1055, val_MinusLogProbMetric: 30.1055

Epoch 266: val_loss did not improve from 29.98977
196/196 - 34s - loss: 29.9761 - MinusLogProbMetric: 29.9761 - val_loss: 30.1055 - val_MinusLogProbMetric: 30.1055 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 267/1000
2023-10-12 10:24:03.200 
Epoch 267/1000 
	 loss: 30.0552, MinusLogProbMetric: 30.0552, val_loss: 30.0799, val_MinusLogProbMetric: 30.0799

Epoch 267: val_loss did not improve from 29.98977
196/196 - 33s - loss: 30.0552 - MinusLogProbMetric: 30.0552 - val_loss: 30.0799 - val_MinusLogProbMetric: 30.0799 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 268/1000
2023-10-12 10:24:35.035 
Epoch 268/1000 
	 loss: 30.0421, MinusLogProbMetric: 30.0421, val_loss: 30.4256, val_MinusLogProbMetric: 30.4256

Epoch 268: val_loss did not improve from 29.98977
196/196 - 32s - loss: 30.0421 - MinusLogProbMetric: 30.0421 - val_loss: 30.4256 - val_MinusLogProbMetric: 30.4256 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 269/1000
2023-10-12 10:25:08.981 
Epoch 269/1000 
	 loss: 30.1636, MinusLogProbMetric: 30.1636, val_loss: 31.4843, val_MinusLogProbMetric: 31.4843

Epoch 269: val_loss did not improve from 29.98977
196/196 - 34s - loss: 30.1636 - MinusLogProbMetric: 30.1636 - val_loss: 31.4843 - val_MinusLogProbMetric: 31.4843 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 270/1000
2023-10-12 10:25:42.562 
Epoch 270/1000 
	 loss: 30.3396, MinusLogProbMetric: 30.3396, val_loss: 30.1433, val_MinusLogProbMetric: 30.1433

Epoch 270: val_loss did not improve from 29.98977
196/196 - 34s - loss: 30.3396 - MinusLogProbMetric: 30.3396 - val_loss: 30.1433 - val_MinusLogProbMetric: 30.1433 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 271/1000
2023-10-12 10:26:16.501 
Epoch 271/1000 
	 loss: 29.9114, MinusLogProbMetric: 29.9114, val_loss: 30.0244, val_MinusLogProbMetric: 30.0244

Epoch 271: val_loss did not improve from 29.98977
196/196 - 34s - loss: 29.9114 - MinusLogProbMetric: 29.9114 - val_loss: 30.0244 - val_MinusLogProbMetric: 30.0244 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 272/1000
2023-10-12 10:26:50.214 
Epoch 272/1000 
	 loss: 29.9750, MinusLogProbMetric: 29.9750, val_loss: 30.2296, val_MinusLogProbMetric: 30.2296

Epoch 272: val_loss did not improve from 29.98977
196/196 - 34s - loss: 29.9750 - MinusLogProbMetric: 29.9750 - val_loss: 30.2296 - val_MinusLogProbMetric: 30.2296 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 273/1000
2023-10-12 10:27:23.782 
Epoch 273/1000 
	 loss: 29.8820, MinusLogProbMetric: 29.8820, val_loss: 29.8254, val_MinusLogProbMetric: 29.8254

Epoch 273: val_loss improved from 29.98977 to 29.82545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 29.8820 - MinusLogProbMetric: 29.8820 - val_loss: 29.8254 - val_MinusLogProbMetric: 29.8254 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 274/1000
2023-10-12 10:27:58.056 
Epoch 274/1000 
	 loss: 30.4308, MinusLogProbMetric: 30.4308, val_loss: 30.0385, val_MinusLogProbMetric: 30.0385

Epoch 274: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.4308 - MinusLogProbMetric: 30.4308 - val_loss: 30.0385 - val_MinusLogProbMetric: 30.0385 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 275/1000
2023-10-12 10:28:31.793 
Epoch 275/1000 
	 loss: 29.9689, MinusLogProbMetric: 29.9689, val_loss: 30.0980, val_MinusLogProbMetric: 30.0980

Epoch 275: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.9689 - MinusLogProbMetric: 29.9689 - val_loss: 30.0980 - val_MinusLogProbMetric: 30.0980 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 276/1000
2023-10-12 10:29:05.610 
Epoch 276/1000 
	 loss: 30.3754, MinusLogProbMetric: 30.3754, val_loss: 30.0928, val_MinusLogProbMetric: 30.0928

Epoch 276: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.3754 - MinusLogProbMetric: 30.3754 - val_loss: 30.0928 - val_MinusLogProbMetric: 30.0928 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 277/1000
2023-10-12 10:29:39.273 
Epoch 277/1000 
	 loss: 29.7327, MinusLogProbMetric: 29.7327, val_loss: 30.0147, val_MinusLogProbMetric: 30.0147

Epoch 277: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.7327 - MinusLogProbMetric: 29.7327 - val_loss: 30.0147 - val_MinusLogProbMetric: 30.0147 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 278/1000
2023-10-12 10:30:13.160 
Epoch 278/1000 
	 loss: 30.2484, MinusLogProbMetric: 30.2484, val_loss: 30.5959, val_MinusLogProbMetric: 30.5959

Epoch 278: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.2484 - MinusLogProbMetric: 30.2484 - val_loss: 30.5959 - val_MinusLogProbMetric: 30.5959 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 279/1000
2023-10-12 10:30:47.122 
Epoch 279/1000 
	 loss: 29.7394, MinusLogProbMetric: 29.7394, val_loss: 30.1214, val_MinusLogProbMetric: 30.1214

Epoch 279: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.7394 - MinusLogProbMetric: 29.7394 - val_loss: 30.1214 - val_MinusLogProbMetric: 30.1214 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 280/1000
2023-10-12 10:31:20.804 
Epoch 280/1000 
	 loss: 30.0256, MinusLogProbMetric: 30.0256, val_loss: 30.3154, val_MinusLogProbMetric: 30.3154

Epoch 280: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0256 - MinusLogProbMetric: 30.0256 - val_loss: 30.3154 - val_MinusLogProbMetric: 30.3154 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 281/1000
2023-10-12 10:31:54.458 
Epoch 281/1000 
	 loss: 30.1361, MinusLogProbMetric: 30.1361, val_loss: 30.1119, val_MinusLogProbMetric: 30.1119

Epoch 281: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.1361 - MinusLogProbMetric: 30.1361 - val_loss: 30.1119 - val_MinusLogProbMetric: 30.1119 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 282/1000
2023-10-12 10:32:28.445 
Epoch 282/1000 
	 loss: 30.1616, MinusLogProbMetric: 30.1616, val_loss: 31.9523, val_MinusLogProbMetric: 31.9523

Epoch 282: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.1616 - MinusLogProbMetric: 30.1616 - val_loss: 31.9523 - val_MinusLogProbMetric: 31.9523 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 283/1000
2023-10-12 10:33:02.239 
Epoch 283/1000 
	 loss: 30.0651, MinusLogProbMetric: 30.0651, val_loss: 30.4462, val_MinusLogProbMetric: 30.4462

Epoch 283: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0651 - MinusLogProbMetric: 30.0651 - val_loss: 30.4462 - val_MinusLogProbMetric: 30.4462 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 284/1000
2023-10-12 10:33:35.944 
Epoch 284/1000 
	 loss: 29.8514, MinusLogProbMetric: 29.8514, val_loss: 30.2850, val_MinusLogProbMetric: 30.2850

Epoch 284: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8514 - MinusLogProbMetric: 29.8514 - val_loss: 30.2850 - val_MinusLogProbMetric: 30.2850 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 285/1000
2023-10-12 10:34:09.916 
Epoch 285/1000 
	 loss: 30.3129, MinusLogProbMetric: 30.3129, val_loss: 30.0727, val_MinusLogProbMetric: 30.0727

Epoch 285: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.3129 - MinusLogProbMetric: 30.3129 - val_loss: 30.0727 - val_MinusLogProbMetric: 30.0727 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 286/1000
2023-10-12 10:34:44.059 
Epoch 286/1000 
	 loss: 29.8613, MinusLogProbMetric: 29.8613, val_loss: 29.9552, val_MinusLogProbMetric: 29.9552

Epoch 286: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8613 - MinusLogProbMetric: 29.8613 - val_loss: 29.9552 - val_MinusLogProbMetric: 29.9552 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 287/1000
2023-10-12 10:35:18.078 
Epoch 287/1000 
	 loss: 29.9605, MinusLogProbMetric: 29.9605, val_loss: 29.8944, val_MinusLogProbMetric: 29.8944

Epoch 287: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.9605 - MinusLogProbMetric: 29.9605 - val_loss: 29.8944 - val_MinusLogProbMetric: 29.8944 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 288/1000
2023-10-12 10:35:51.824 
Epoch 288/1000 
	 loss: 30.2330, MinusLogProbMetric: 30.2330, val_loss: 30.1839, val_MinusLogProbMetric: 30.1839

Epoch 288: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.2330 - MinusLogProbMetric: 30.2330 - val_loss: 30.1839 - val_MinusLogProbMetric: 30.1839 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 289/1000
2023-10-12 10:36:25.443 
Epoch 289/1000 
	 loss: 29.8404, MinusLogProbMetric: 29.8404, val_loss: 30.1307, val_MinusLogProbMetric: 30.1307

Epoch 289: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8404 - MinusLogProbMetric: 29.8404 - val_loss: 30.1307 - val_MinusLogProbMetric: 30.1307 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 290/1000
2023-10-12 10:36:59.487 
Epoch 290/1000 
	 loss: 29.9405, MinusLogProbMetric: 29.9405, val_loss: 30.3027, val_MinusLogProbMetric: 30.3027

Epoch 290: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.9405 - MinusLogProbMetric: 29.9405 - val_loss: 30.3027 - val_MinusLogProbMetric: 30.3027 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 291/1000
2023-10-12 10:37:33.228 
Epoch 291/1000 
	 loss: 29.9847, MinusLogProbMetric: 29.9847, val_loss: 30.2026, val_MinusLogProbMetric: 30.2026

Epoch 291: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.9847 - MinusLogProbMetric: 29.9847 - val_loss: 30.2026 - val_MinusLogProbMetric: 30.2026 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 292/1000
2023-10-12 10:38:07.195 
Epoch 292/1000 
	 loss: 29.8629, MinusLogProbMetric: 29.8629, val_loss: 30.3899, val_MinusLogProbMetric: 30.3899

Epoch 292: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8629 - MinusLogProbMetric: 29.8629 - val_loss: 30.3899 - val_MinusLogProbMetric: 30.3899 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 293/1000
2023-10-12 10:38:41.400 
Epoch 293/1000 
	 loss: 30.0442, MinusLogProbMetric: 30.0442, val_loss: 30.0561, val_MinusLogProbMetric: 30.0561

Epoch 293: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0442 - MinusLogProbMetric: 30.0442 - val_loss: 30.0561 - val_MinusLogProbMetric: 30.0561 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 294/1000
2023-10-12 10:39:15.086 
Epoch 294/1000 
	 loss: 29.8423, MinusLogProbMetric: 29.8423, val_loss: 31.0834, val_MinusLogProbMetric: 31.0834

Epoch 294: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8423 - MinusLogProbMetric: 29.8423 - val_loss: 31.0834 - val_MinusLogProbMetric: 31.0834 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 295/1000
2023-10-12 10:39:48.752 
Epoch 295/1000 
	 loss: 30.0525, MinusLogProbMetric: 30.0525, val_loss: 30.7135, val_MinusLogProbMetric: 30.7135

Epoch 295: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0525 - MinusLogProbMetric: 30.0525 - val_loss: 30.7135 - val_MinusLogProbMetric: 30.7135 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 296/1000
2023-10-12 10:40:23.079 
Epoch 296/1000 
	 loss: 29.8249, MinusLogProbMetric: 29.8249, val_loss: 29.8919, val_MinusLogProbMetric: 29.8919

Epoch 296: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8249 - MinusLogProbMetric: 29.8249 - val_loss: 29.8919 - val_MinusLogProbMetric: 29.8919 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 297/1000
2023-10-12 10:40:57.126 
Epoch 297/1000 
	 loss: 29.7632, MinusLogProbMetric: 29.7632, val_loss: 29.9897, val_MinusLogProbMetric: 29.9897

Epoch 297: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.7632 - MinusLogProbMetric: 29.7632 - val_loss: 29.9897 - val_MinusLogProbMetric: 29.9897 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 298/1000
2023-10-12 10:41:30.914 
Epoch 298/1000 
	 loss: 29.9677, MinusLogProbMetric: 29.9677, val_loss: 30.1223, val_MinusLogProbMetric: 30.1223

Epoch 298: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.9677 - MinusLogProbMetric: 29.9677 - val_loss: 30.1223 - val_MinusLogProbMetric: 30.1223 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 299/1000
2023-10-12 10:42:04.466 
Epoch 299/1000 
	 loss: 29.9577, MinusLogProbMetric: 29.9577, val_loss: 30.3264, val_MinusLogProbMetric: 30.3264

Epoch 299: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.9577 - MinusLogProbMetric: 29.9577 - val_loss: 30.3264 - val_MinusLogProbMetric: 30.3264 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 300/1000
2023-10-12 10:42:38.377 
Epoch 300/1000 
	 loss: 30.2170, MinusLogProbMetric: 30.2170, val_loss: 30.4266, val_MinusLogProbMetric: 30.4266

Epoch 300: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.2170 - MinusLogProbMetric: 30.2170 - val_loss: 30.4266 - val_MinusLogProbMetric: 30.4266 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 301/1000
2023-10-12 10:43:12.628 
Epoch 301/1000 
	 loss: 29.8307, MinusLogProbMetric: 29.8307, val_loss: 30.1122, val_MinusLogProbMetric: 30.1122

Epoch 301: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8307 - MinusLogProbMetric: 29.8307 - val_loss: 30.1122 - val_MinusLogProbMetric: 30.1122 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 302/1000
2023-10-12 10:43:46.325 
Epoch 302/1000 
	 loss: 30.0002, MinusLogProbMetric: 30.0002, val_loss: 30.3000, val_MinusLogProbMetric: 30.3000

Epoch 302: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0002 - MinusLogProbMetric: 30.0002 - val_loss: 30.3000 - val_MinusLogProbMetric: 30.3000 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 303/1000
2023-10-12 10:44:20.175 
Epoch 303/1000 
	 loss: 30.0252, MinusLogProbMetric: 30.0252, val_loss: 30.5621, val_MinusLogProbMetric: 30.5621

Epoch 303: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0252 - MinusLogProbMetric: 30.0252 - val_loss: 30.5621 - val_MinusLogProbMetric: 30.5621 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 304/1000
2023-10-12 10:44:54.485 
Epoch 304/1000 
	 loss: 29.8911, MinusLogProbMetric: 29.8911, val_loss: 30.5410, val_MinusLogProbMetric: 30.5410

Epoch 304: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8911 - MinusLogProbMetric: 29.8911 - val_loss: 30.5410 - val_MinusLogProbMetric: 30.5410 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 305/1000
2023-10-12 10:45:28.288 
Epoch 305/1000 
	 loss: 29.7514, MinusLogProbMetric: 29.7514, val_loss: 30.2791, val_MinusLogProbMetric: 30.2791

Epoch 305: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.7514 - MinusLogProbMetric: 29.7514 - val_loss: 30.2791 - val_MinusLogProbMetric: 30.2791 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 306/1000
2023-10-12 10:46:01.870 
Epoch 306/1000 
	 loss: 30.0715, MinusLogProbMetric: 30.0715, val_loss: 32.2123, val_MinusLogProbMetric: 32.2123

Epoch 306: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0715 - MinusLogProbMetric: 30.0715 - val_loss: 32.2123 - val_MinusLogProbMetric: 32.2123 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 307/1000
2023-10-12 10:46:35.901 
Epoch 307/1000 
	 loss: 30.0086, MinusLogProbMetric: 30.0086, val_loss: 30.2462, val_MinusLogProbMetric: 30.2462

Epoch 307: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.0086 - MinusLogProbMetric: 30.0086 - val_loss: 30.2462 - val_MinusLogProbMetric: 30.2462 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 308/1000
2023-10-12 10:47:10.102 
Epoch 308/1000 
	 loss: 29.8392, MinusLogProbMetric: 29.8392, val_loss: 31.5002, val_MinusLogProbMetric: 31.5002

Epoch 308: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.8392 - MinusLogProbMetric: 29.8392 - val_loss: 31.5002 - val_MinusLogProbMetric: 31.5002 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 309/1000
2023-10-12 10:47:44.105 
Epoch 309/1000 
	 loss: 30.1228, MinusLogProbMetric: 30.1228, val_loss: 30.2700, val_MinusLogProbMetric: 30.2700

Epoch 309: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.1228 - MinusLogProbMetric: 30.1228 - val_loss: 30.2700 - val_MinusLogProbMetric: 30.2700 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 310/1000
2023-10-12 10:48:17.876 
Epoch 310/1000 
	 loss: 29.7656, MinusLogProbMetric: 29.7656, val_loss: 31.4865, val_MinusLogProbMetric: 31.4865

Epoch 310: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.7656 - MinusLogProbMetric: 29.7656 - val_loss: 31.4865 - val_MinusLogProbMetric: 31.4865 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 311/1000
2023-10-12 10:48:51.573 
Epoch 311/1000 
	 loss: 29.7787, MinusLogProbMetric: 29.7787, val_loss: 29.9347, val_MinusLogProbMetric: 29.9347

Epoch 311: val_loss did not improve from 29.82545
196/196 - 34s - loss: 29.7787 - MinusLogProbMetric: 29.7787 - val_loss: 29.9347 - val_MinusLogProbMetric: 29.9347 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 312/1000
2023-10-12 10:49:25.270 
Epoch 312/1000 
	 loss: 30.1991, MinusLogProbMetric: 30.1991, val_loss: 30.2062, val_MinusLogProbMetric: 30.2062

Epoch 312: val_loss did not improve from 29.82545
196/196 - 34s - loss: 30.1991 - MinusLogProbMetric: 30.1991 - val_loss: 30.2062 - val_MinusLogProbMetric: 30.2062 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 313/1000
2023-10-12 10:49:59.181 
Epoch 313/1000 
	 loss: 29.7451, MinusLogProbMetric: 29.7451, val_loss: 29.7504, val_MinusLogProbMetric: 29.7504

Epoch 313: val_loss improved from 29.82545 to 29.75038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 29.7451 - MinusLogProbMetric: 29.7451 - val_loss: 29.7504 - val_MinusLogProbMetric: 29.7504 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 314/1000
2023-10-12 10:50:33.269 
Epoch 314/1000 
	 loss: 30.1110, MinusLogProbMetric: 30.1110, val_loss: 31.0311, val_MinusLogProbMetric: 31.0311

Epoch 314: val_loss did not improve from 29.75038
196/196 - 33s - loss: 30.1110 - MinusLogProbMetric: 30.1110 - val_loss: 31.0311 - val_MinusLogProbMetric: 31.0311 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 315/1000
2023-10-12 10:51:07.121 
Epoch 315/1000 
	 loss: 29.9482, MinusLogProbMetric: 29.9482, val_loss: 31.2976, val_MinusLogProbMetric: 31.2976

Epoch 315: val_loss did not improve from 29.75038
196/196 - 34s - loss: 29.9482 - MinusLogProbMetric: 29.9482 - val_loss: 31.2976 - val_MinusLogProbMetric: 31.2976 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 316/1000
2023-10-12 10:51:40.815 
Epoch 316/1000 
	 loss: 29.9618, MinusLogProbMetric: 29.9618, val_loss: 29.6467, val_MinusLogProbMetric: 29.6467

Epoch 316: val_loss improved from 29.75038 to 29.64672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 29.9618 - MinusLogProbMetric: 29.9618 - val_loss: 29.6467 - val_MinusLogProbMetric: 29.6467 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 317/1000
2023-10-12 10:52:15.267 
Epoch 317/1000 
	 loss: 29.9956, MinusLogProbMetric: 29.9956, val_loss: 29.9412, val_MinusLogProbMetric: 29.9412

Epoch 317: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.9956 - MinusLogProbMetric: 29.9956 - val_loss: 29.9412 - val_MinusLogProbMetric: 29.9412 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 318/1000
2023-10-12 10:52:49.209 
Epoch 318/1000 
	 loss: 29.7875, MinusLogProbMetric: 29.7875, val_loss: 30.0533, val_MinusLogProbMetric: 30.0533

Epoch 318: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.7875 - MinusLogProbMetric: 29.7875 - val_loss: 30.0533 - val_MinusLogProbMetric: 30.0533 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 319/1000
2023-10-12 10:53:22.895 
Epoch 319/1000 
	 loss: 29.7040, MinusLogProbMetric: 29.7040, val_loss: 29.7343, val_MinusLogProbMetric: 29.7343

Epoch 319: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.7040 - MinusLogProbMetric: 29.7040 - val_loss: 29.7343 - val_MinusLogProbMetric: 29.7343 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 320/1000
2023-10-12 10:53:56.933 
Epoch 320/1000 
	 loss: 29.8473, MinusLogProbMetric: 29.8473, val_loss: 29.8738, val_MinusLogProbMetric: 29.8738

Epoch 320: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.8473 - MinusLogProbMetric: 29.8473 - val_loss: 29.8738 - val_MinusLogProbMetric: 29.8738 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 321/1000
2023-10-12 10:54:30.972 
Epoch 321/1000 
	 loss: 29.6694, MinusLogProbMetric: 29.6694, val_loss: 31.0220, val_MinusLogProbMetric: 31.0220

Epoch 321: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.6694 - MinusLogProbMetric: 29.6694 - val_loss: 31.0220 - val_MinusLogProbMetric: 31.0220 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 322/1000
2023-10-12 10:55:05.252 
Epoch 322/1000 
	 loss: 29.9456, MinusLogProbMetric: 29.9456, val_loss: 29.9534, val_MinusLogProbMetric: 29.9534

Epoch 322: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.9456 - MinusLogProbMetric: 29.9456 - val_loss: 29.9534 - val_MinusLogProbMetric: 29.9534 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 323/1000
2023-10-12 10:55:38.683 
Epoch 323/1000 
	 loss: 29.7661, MinusLogProbMetric: 29.7661, val_loss: 29.6469, val_MinusLogProbMetric: 29.6469

Epoch 323: val_loss did not improve from 29.64672
196/196 - 33s - loss: 29.7661 - MinusLogProbMetric: 29.7661 - val_loss: 29.6469 - val_MinusLogProbMetric: 29.6469 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 324/1000
2023-10-12 10:56:12.775 
Epoch 324/1000 
	 loss: 29.5050, MinusLogProbMetric: 29.5050, val_loss: 30.2258, val_MinusLogProbMetric: 30.2258

Epoch 324: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.5050 - MinusLogProbMetric: 29.5050 - val_loss: 30.2258 - val_MinusLogProbMetric: 30.2258 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 325/1000
2023-10-12 10:56:46.601 
Epoch 325/1000 
	 loss: 30.0182, MinusLogProbMetric: 30.0182, val_loss: 29.7774, val_MinusLogProbMetric: 29.7774

Epoch 325: val_loss did not improve from 29.64672
196/196 - 34s - loss: 30.0182 - MinusLogProbMetric: 30.0182 - val_loss: 29.7774 - val_MinusLogProbMetric: 29.7774 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 326/1000
2023-10-12 10:57:20.271 
Epoch 326/1000 
	 loss: 29.7788, MinusLogProbMetric: 29.7788, val_loss: 30.0949, val_MinusLogProbMetric: 30.0949

Epoch 326: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.7788 - MinusLogProbMetric: 29.7788 - val_loss: 30.0949 - val_MinusLogProbMetric: 30.0949 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 327/1000
2023-10-12 10:57:54.157 
Epoch 327/1000 
	 loss: 29.6840, MinusLogProbMetric: 29.6840, val_loss: 29.8076, val_MinusLogProbMetric: 29.8076

Epoch 327: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.6840 - MinusLogProbMetric: 29.6840 - val_loss: 29.8076 - val_MinusLogProbMetric: 29.8076 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 328/1000
2023-10-12 10:58:27.908 
Epoch 328/1000 
	 loss: 29.6829, MinusLogProbMetric: 29.6829, val_loss: 29.8067, val_MinusLogProbMetric: 29.8067

Epoch 328: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.6829 - MinusLogProbMetric: 29.6829 - val_loss: 29.8067 - val_MinusLogProbMetric: 29.8067 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 329/1000
2023-10-12 10:59:01.537 
Epoch 329/1000 
	 loss: 29.6019, MinusLogProbMetric: 29.6019, val_loss: 34.7482, val_MinusLogProbMetric: 34.7482

Epoch 329: val_loss did not improve from 29.64672
196/196 - 34s - loss: 29.6019 - MinusLogProbMetric: 29.6019 - val_loss: 34.7482 - val_MinusLogProbMetric: 34.7482 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 330/1000
2023-10-12 10:59:35.115 
Epoch 330/1000 
	 loss: 29.7357, MinusLogProbMetric: 29.7357, val_loss: 29.5586, val_MinusLogProbMetric: 29.5586

Epoch 330: val_loss improved from 29.64672 to 29.55859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 29.7357 - MinusLogProbMetric: 29.7357 - val_loss: 29.5586 - val_MinusLogProbMetric: 29.5586 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 331/1000
2023-10-12 11:00:09.730 
Epoch 331/1000 
	 loss: 29.8837, MinusLogProbMetric: 29.8837, val_loss: 30.7936, val_MinusLogProbMetric: 30.7936

Epoch 331: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.8837 - MinusLogProbMetric: 29.8837 - val_loss: 30.7936 - val_MinusLogProbMetric: 30.7936 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 332/1000
2023-10-12 11:00:43.371 
Epoch 332/1000 
	 loss: 29.6242, MinusLogProbMetric: 29.6242, val_loss: 29.7849, val_MinusLogProbMetric: 29.7849

Epoch 332: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6242 - MinusLogProbMetric: 29.6242 - val_loss: 29.7849 - val_MinusLogProbMetric: 29.7849 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 333/1000
2023-10-12 11:01:17.086 
Epoch 333/1000 
	 loss: 29.7734, MinusLogProbMetric: 29.7734, val_loss: 30.3476, val_MinusLogProbMetric: 30.3476

Epoch 333: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7734 - MinusLogProbMetric: 29.7734 - val_loss: 30.3476 - val_MinusLogProbMetric: 30.3476 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 334/1000
2023-10-12 11:01:50.517 
Epoch 334/1000 
	 loss: 29.6928, MinusLogProbMetric: 29.6928, val_loss: 29.8926, val_MinusLogProbMetric: 29.8926

Epoch 334: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.6928 - MinusLogProbMetric: 29.6928 - val_loss: 29.8926 - val_MinusLogProbMetric: 29.8926 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 335/1000
2023-10-12 11:02:24.061 
Epoch 335/1000 
	 loss: 29.6502, MinusLogProbMetric: 29.6502, val_loss: 29.9520, val_MinusLogProbMetric: 29.9520

Epoch 335: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6502 - MinusLogProbMetric: 29.6502 - val_loss: 29.9520 - val_MinusLogProbMetric: 29.9520 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 336/1000
2023-10-12 11:02:57.536 
Epoch 336/1000 
	 loss: 29.8513, MinusLogProbMetric: 29.8513, val_loss: 30.7047, val_MinusLogProbMetric: 30.7047

Epoch 336: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.8513 - MinusLogProbMetric: 29.8513 - val_loss: 30.7047 - val_MinusLogProbMetric: 30.7047 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 337/1000
2023-10-12 11:03:31.246 
Epoch 337/1000 
	 loss: 29.7070, MinusLogProbMetric: 29.7070, val_loss: 30.2661, val_MinusLogProbMetric: 30.2661

Epoch 337: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7070 - MinusLogProbMetric: 29.7070 - val_loss: 30.2661 - val_MinusLogProbMetric: 30.2661 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 338/1000
2023-10-12 11:04:04.646 
Epoch 338/1000 
	 loss: 29.5694, MinusLogProbMetric: 29.5694, val_loss: 29.9604, val_MinusLogProbMetric: 29.9604

Epoch 338: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.5694 - MinusLogProbMetric: 29.5694 - val_loss: 29.9604 - val_MinusLogProbMetric: 29.9604 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 339/1000
2023-10-12 11:04:37.864 
Epoch 339/1000 
	 loss: 29.4398, MinusLogProbMetric: 29.4398, val_loss: 29.9549, val_MinusLogProbMetric: 29.9549

Epoch 339: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.4398 - MinusLogProbMetric: 29.4398 - val_loss: 29.9549 - val_MinusLogProbMetric: 29.9549 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 340/1000
2023-10-12 11:05:11.378 
Epoch 340/1000 
	 loss: 29.7682, MinusLogProbMetric: 29.7682, val_loss: 30.0655, val_MinusLogProbMetric: 30.0655

Epoch 340: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7682 - MinusLogProbMetric: 29.7682 - val_loss: 30.0655 - val_MinusLogProbMetric: 30.0655 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 341/1000
2023-10-12 11:05:45.112 
Epoch 341/1000 
	 loss: 29.7694, MinusLogProbMetric: 29.7694, val_loss: 29.8184, val_MinusLogProbMetric: 29.8184

Epoch 341: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7694 - MinusLogProbMetric: 29.7694 - val_loss: 29.8184 - val_MinusLogProbMetric: 29.8184 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 342/1000
2023-10-12 11:06:18.797 
Epoch 342/1000 
	 loss: 29.7792, MinusLogProbMetric: 29.7792, val_loss: 30.4429, val_MinusLogProbMetric: 30.4429

Epoch 342: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7792 - MinusLogProbMetric: 29.7792 - val_loss: 30.4429 - val_MinusLogProbMetric: 30.4429 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 343/1000
2023-10-12 11:06:52.564 
Epoch 343/1000 
	 loss: 29.8562, MinusLogProbMetric: 29.8562, val_loss: 29.7969, val_MinusLogProbMetric: 29.7969

Epoch 343: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.8562 - MinusLogProbMetric: 29.8562 - val_loss: 29.7969 - val_MinusLogProbMetric: 29.7969 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 344/1000
2023-10-12 11:07:26.421 
Epoch 344/1000 
	 loss: 29.9804, MinusLogProbMetric: 29.9804, val_loss: 29.8225, val_MinusLogProbMetric: 29.8225

Epoch 344: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.9804 - MinusLogProbMetric: 29.9804 - val_loss: 29.8225 - val_MinusLogProbMetric: 29.8225 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 345/1000
2023-10-12 11:08:00.151 
Epoch 345/1000 
	 loss: 29.7632, MinusLogProbMetric: 29.7632, val_loss: 30.1595, val_MinusLogProbMetric: 30.1595

Epoch 345: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7632 - MinusLogProbMetric: 29.7632 - val_loss: 30.1595 - val_MinusLogProbMetric: 30.1595 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 346/1000
2023-10-12 11:08:33.659 
Epoch 346/1000 
	 loss: 29.4810, MinusLogProbMetric: 29.4810, val_loss: 33.3271, val_MinusLogProbMetric: 33.3271

Epoch 346: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.4810 - MinusLogProbMetric: 29.4810 - val_loss: 33.3271 - val_MinusLogProbMetric: 33.3271 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 347/1000
2023-10-12 11:09:07.601 
Epoch 347/1000 
	 loss: 29.9594, MinusLogProbMetric: 29.9594, val_loss: 29.6281, val_MinusLogProbMetric: 29.6281

Epoch 347: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.9594 - MinusLogProbMetric: 29.9594 - val_loss: 29.6281 - val_MinusLogProbMetric: 29.6281 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 348/1000
2023-10-12 11:09:41.514 
Epoch 348/1000 
	 loss: 29.7131, MinusLogProbMetric: 29.7131, val_loss: 29.8080, val_MinusLogProbMetric: 29.8080

Epoch 348: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7131 - MinusLogProbMetric: 29.7131 - val_loss: 29.8080 - val_MinusLogProbMetric: 29.8080 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 349/1000
2023-10-12 11:10:15.550 
Epoch 349/1000 
	 loss: 29.6452, MinusLogProbMetric: 29.6452, val_loss: 29.7152, val_MinusLogProbMetric: 29.7152

Epoch 349: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6452 - MinusLogProbMetric: 29.6452 - val_loss: 29.7152 - val_MinusLogProbMetric: 29.7152 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 350/1000
2023-10-12 11:10:48.923 
Epoch 350/1000 
	 loss: 29.6580, MinusLogProbMetric: 29.6580, val_loss: 30.4766, val_MinusLogProbMetric: 30.4766

Epoch 350: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.6580 - MinusLogProbMetric: 29.6580 - val_loss: 30.4766 - val_MinusLogProbMetric: 30.4766 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 351/1000
2023-10-12 11:11:22.759 
Epoch 351/1000 
	 loss: 30.0184, MinusLogProbMetric: 30.0184, val_loss: 29.5865, val_MinusLogProbMetric: 29.5865

Epoch 351: val_loss did not improve from 29.55859
196/196 - 34s - loss: 30.0184 - MinusLogProbMetric: 30.0184 - val_loss: 29.5865 - val_MinusLogProbMetric: 29.5865 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 352/1000
2023-10-12 11:11:56.566 
Epoch 352/1000 
	 loss: 30.0005, MinusLogProbMetric: 30.0005, val_loss: 30.2305, val_MinusLogProbMetric: 30.2305

Epoch 352: val_loss did not improve from 29.55859
196/196 - 34s - loss: 30.0005 - MinusLogProbMetric: 30.0005 - val_loss: 30.2305 - val_MinusLogProbMetric: 30.2305 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 353/1000
2023-10-12 11:12:30.349 
Epoch 353/1000 
	 loss: 29.6437, MinusLogProbMetric: 29.6437, val_loss: 30.3107, val_MinusLogProbMetric: 30.3107

Epoch 353: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6437 - MinusLogProbMetric: 29.6437 - val_loss: 30.3107 - val_MinusLogProbMetric: 30.3107 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 354/1000
2023-10-12 11:13:03.985 
Epoch 354/1000 
	 loss: 29.7927, MinusLogProbMetric: 29.7927, val_loss: 30.4644, val_MinusLogProbMetric: 30.4644

Epoch 354: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7927 - MinusLogProbMetric: 29.7927 - val_loss: 30.4644 - val_MinusLogProbMetric: 30.4644 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 355/1000
2023-10-12 11:13:38.096 
Epoch 355/1000 
	 loss: 29.7775, MinusLogProbMetric: 29.7775, val_loss: 29.6827, val_MinusLogProbMetric: 29.6827

Epoch 355: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7775 - MinusLogProbMetric: 29.7775 - val_loss: 29.6827 - val_MinusLogProbMetric: 29.6827 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 356/1000
2023-10-12 11:14:11.887 
Epoch 356/1000 
	 loss: 29.6049, MinusLogProbMetric: 29.6049, val_loss: 30.7056, val_MinusLogProbMetric: 30.7056

Epoch 356: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6049 - MinusLogProbMetric: 29.6049 - val_loss: 30.7056 - val_MinusLogProbMetric: 30.7056 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 357/1000
2023-10-12 11:14:45.241 
Epoch 357/1000 
	 loss: 29.5765, MinusLogProbMetric: 29.5765, val_loss: 29.8242, val_MinusLogProbMetric: 29.8242

Epoch 357: val_loss did not improve from 29.55859
196/196 - 33s - loss: 29.5765 - MinusLogProbMetric: 29.5765 - val_loss: 29.8242 - val_MinusLogProbMetric: 29.8242 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 358/1000
2023-10-12 11:15:18.766 
Epoch 358/1000 
	 loss: 29.6174, MinusLogProbMetric: 29.6174, val_loss: 29.9926, val_MinusLogProbMetric: 29.9926

Epoch 358: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6174 - MinusLogProbMetric: 29.6174 - val_loss: 29.9926 - val_MinusLogProbMetric: 29.9926 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 359/1000
2023-10-12 11:15:52.510 
Epoch 359/1000 
	 loss: 29.7364, MinusLogProbMetric: 29.7364, val_loss: 29.7010, val_MinusLogProbMetric: 29.7010

Epoch 359: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7364 - MinusLogProbMetric: 29.7364 - val_loss: 29.7010 - val_MinusLogProbMetric: 29.7010 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 360/1000
2023-10-12 11:16:26.510 
Epoch 360/1000 
	 loss: 29.4613, MinusLogProbMetric: 29.4613, val_loss: 29.6319, val_MinusLogProbMetric: 29.6319

Epoch 360: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.4613 - MinusLogProbMetric: 29.4613 - val_loss: 29.6319 - val_MinusLogProbMetric: 29.6319 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 361/1000
2023-10-12 11:17:00.135 
Epoch 361/1000 
	 loss: 29.5208, MinusLogProbMetric: 29.5208, val_loss: 29.9719, val_MinusLogProbMetric: 29.9719

Epoch 361: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.5208 - MinusLogProbMetric: 29.5208 - val_loss: 29.9719 - val_MinusLogProbMetric: 29.9719 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 362/1000
2023-10-12 11:17:33.558 
Epoch 362/1000 
	 loss: 30.1692, MinusLogProbMetric: 30.1692, val_loss: 29.8401, val_MinusLogProbMetric: 29.8401

Epoch 362: val_loss did not improve from 29.55859
196/196 - 33s - loss: 30.1692 - MinusLogProbMetric: 30.1692 - val_loss: 29.8401 - val_MinusLogProbMetric: 29.8401 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 363/1000
2023-10-12 11:18:07.343 
Epoch 363/1000 
	 loss: 29.5772, MinusLogProbMetric: 29.5772, val_loss: 30.0441, val_MinusLogProbMetric: 30.0441

Epoch 363: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.5772 - MinusLogProbMetric: 29.5772 - val_loss: 30.0441 - val_MinusLogProbMetric: 30.0441 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 364/1000
2023-10-12 11:18:41.073 
Epoch 364/1000 
	 loss: 29.5371, MinusLogProbMetric: 29.5371, val_loss: 31.2257, val_MinusLogProbMetric: 31.2257

Epoch 364: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.5371 - MinusLogProbMetric: 29.5371 - val_loss: 31.2257 - val_MinusLogProbMetric: 31.2257 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 365/1000
2023-10-12 11:19:14.913 
Epoch 365/1000 
	 loss: 29.5775, MinusLogProbMetric: 29.5775, val_loss: 30.9142, val_MinusLogProbMetric: 30.9142

Epoch 365: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.5775 - MinusLogProbMetric: 29.5775 - val_loss: 30.9142 - val_MinusLogProbMetric: 30.9142 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 366/1000
2023-10-12 11:19:48.741 
Epoch 366/1000 
	 loss: 29.8152, MinusLogProbMetric: 29.8152, val_loss: 30.3512, val_MinusLogProbMetric: 30.3512

Epoch 366: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.8152 - MinusLogProbMetric: 29.8152 - val_loss: 30.3512 - val_MinusLogProbMetric: 30.3512 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 367/1000
2023-10-12 11:20:22.377 
Epoch 367/1000 
	 loss: 29.6024, MinusLogProbMetric: 29.6024, val_loss: 29.9093, val_MinusLogProbMetric: 29.9093

Epoch 367: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6024 - MinusLogProbMetric: 29.6024 - val_loss: 29.9093 - val_MinusLogProbMetric: 29.9093 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 368/1000
2023-10-12 11:20:55.938 
Epoch 368/1000 
	 loss: 29.6957, MinusLogProbMetric: 29.6957, val_loss: 29.6442, val_MinusLogProbMetric: 29.6442

Epoch 368: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6957 - MinusLogProbMetric: 29.6957 - val_loss: 29.6442 - val_MinusLogProbMetric: 29.6442 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 369/1000
2023-10-12 11:21:29.629 
Epoch 369/1000 
	 loss: 29.4914, MinusLogProbMetric: 29.4914, val_loss: 29.6261, val_MinusLogProbMetric: 29.6261

Epoch 369: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.4914 - MinusLogProbMetric: 29.4914 - val_loss: 29.6261 - val_MinusLogProbMetric: 29.6261 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 370/1000
2023-10-12 11:22:03.238 
Epoch 370/1000 
	 loss: 29.7801, MinusLogProbMetric: 29.7801, val_loss: 29.5958, val_MinusLogProbMetric: 29.5958

Epoch 370: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7801 - MinusLogProbMetric: 29.7801 - val_loss: 29.5958 - val_MinusLogProbMetric: 29.5958 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 371/1000
2023-10-12 11:22:36.919 
Epoch 371/1000 
	 loss: 29.6386, MinusLogProbMetric: 29.6386, val_loss: 29.8531, val_MinusLogProbMetric: 29.8531

Epoch 371: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6386 - MinusLogProbMetric: 29.6386 - val_loss: 29.8531 - val_MinusLogProbMetric: 29.8531 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 372/1000
2023-10-12 11:23:10.650 
Epoch 372/1000 
	 loss: 29.5646, MinusLogProbMetric: 29.5646, val_loss: 31.5779, val_MinusLogProbMetric: 31.5779

Epoch 372: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.5646 - MinusLogProbMetric: 29.5646 - val_loss: 31.5779 - val_MinusLogProbMetric: 31.5779 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 373/1000
2023-10-12 11:23:44.240 
Epoch 373/1000 
	 loss: 29.5697, MinusLogProbMetric: 29.5697, val_loss: 30.1229, val_MinusLogProbMetric: 30.1229

Epoch 373: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.5697 - MinusLogProbMetric: 29.5697 - val_loss: 30.1229 - val_MinusLogProbMetric: 30.1229 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 374/1000
2023-10-12 11:24:18.329 
Epoch 374/1000 
	 loss: 29.6144, MinusLogProbMetric: 29.6144, val_loss: 29.9267, val_MinusLogProbMetric: 29.9267

Epoch 374: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6144 - MinusLogProbMetric: 29.6144 - val_loss: 29.9267 - val_MinusLogProbMetric: 29.9267 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 375/1000
2023-10-12 11:24:52.006 
Epoch 375/1000 
	 loss: 29.4908, MinusLogProbMetric: 29.4908, val_loss: 29.8597, val_MinusLogProbMetric: 29.8597

Epoch 375: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.4908 - MinusLogProbMetric: 29.4908 - val_loss: 29.8597 - val_MinusLogProbMetric: 29.8597 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 376/1000
2023-10-12 11:25:25.809 
Epoch 376/1000 
	 loss: 29.8349, MinusLogProbMetric: 29.8349, val_loss: 30.3969, val_MinusLogProbMetric: 30.3969

Epoch 376: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.8349 - MinusLogProbMetric: 29.8349 - val_loss: 30.3969 - val_MinusLogProbMetric: 30.3969 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 377/1000
2023-10-12 11:25:59.455 
Epoch 377/1000 
	 loss: 29.8245, MinusLogProbMetric: 29.8245, val_loss: 29.8303, val_MinusLogProbMetric: 29.8303

Epoch 377: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.8245 - MinusLogProbMetric: 29.8245 - val_loss: 29.8303 - val_MinusLogProbMetric: 29.8303 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 378/1000
2023-10-12 11:26:33.239 
Epoch 378/1000 
	 loss: 29.6014, MinusLogProbMetric: 29.6014, val_loss: 29.6262, val_MinusLogProbMetric: 29.6262

Epoch 378: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.6014 - MinusLogProbMetric: 29.6014 - val_loss: 29.6262 - val_MinusLogProbMetric: 29.6262 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 379/1000
2023-10-12 11:27:06.769 
Epoch 379/1000 
	 loss: 29.3161, MinusLogProbMetric: 29.3161, val_loss: 30.7414, val_MinusLogProbMetric: 30.7414

Epoch 379: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.3161 - MinusLogProbMetric: 29.3161 - val_loss: 30.7414 - val_MinusLogProbMetric: 30.7414 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 380/1000
2023-10-12 11:27:40.715 
Epoch 380/1000 
	 loss: 29.7115, MinusLogProbMetric: 29.7115, val_loss: 30.1507, val_MinusLogProbMetric: 30.1507

Epoch 380: val_loss did not improve from 29.55859
196/196 - 34s - loss: 29.7115 - MinusLogProbMetric: 29.7115 - val_loss: 30.1507 - val_MinusLogProbMetric: 30.1507 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 381/1000
2023-10-12 11:28:14.481 
Epoch 381/1000 
	 loss: 28.8714, MinusLogProbMetric: 28.8714, val_loss: 29.3467, val_MinusLogProbMetric: 29.3467

Epoch 381: val_loss improved from 29.55859 to 29.34672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.8714 - MinusLogProbMetric: 28.8714 - val_loss: 29.3467 - val_MinusLogProbMetric: 29.3467 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 382/1000
2023-10-12 11:28:48.650 
Epoch 382/1000 
	 loss: 28.8397, MinusLogProbMetric: 28.8397, val_loss: 29.2213, val_MinusLogProbMetric: 29.2213

Epoch 382: val_loss improved from 29.34672 to 29.22130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.8397 - MinusLogProbMetric: 28.8397 - val_loss: 29.2213 - val_MinusLogProbMetric: 29.2213 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 383/1000
2023-10-12 11:29:22.879 
Epoch 383/1000 
	 loss: 28.8259, MinusLogProbMetric: 28.8259, val_loss: 29.2308, val_MinusLogProbMetric: 29.2308

Epoch 383: val_loss did not improve from 29.22130
196/196 - 34s - loss: 28.8259 - MinusLogProbMetric: 28.8259 - val_loss: 29.2308 - val_MinusLogProbMetric: 29.2308 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 384/1000
2023-10-12 11:29:56.368 
Epoch 384/1000 
	 loss: 28.8172, MinusLogProbMetric: 28.8172, val_loss: 29.1781, val_MinusLogProbMetric: 29.1781

Epoch 384: val_loss improved from 29.22130 to 29.17812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.8172 - MinusLogProbMetric: 28.8172 - val_loss: 29.1781 - val_MinusLogProbMetric: 29.1781 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 385/1000
2023-10-12 11:30:30.598 
Epoch 385/1000 
	 loss: 28.8221, MinusLogProbMetric: 28.8221, val_loss: 29.1853, val_MinusLogProbMetric: 29.1853

Epoch 385: val_loss did not improve from 29.17812
196/196 - 34s - loss: 28.8221 - MinusLogProbMetric: 28.8221 - val_loss: 29.1853 - val_MinusLogProbMetric: 29.1853 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 386/1000
2023-10-12 11:31:04.034 
Epoch 386/1000 
	 loss: 28.8382, MinusLogProbMetric: 28.8382, val_loss: 29.2380, val_MinusLogProbMetric: 29.2380

Epoch 386: val_loss did not improve from 29.17812
196/196 - 33s - loss: 28.8382 - MinusLogProbMetric: 28.8382 - val_loss: 29.2380 - val_MinusLogProbMetric: 29.2380 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 387/1000
2023-10-12 11:31:37.784 
Epoch 387/1000 
	 loss: 28.8629, MinusLogProbMetric: 28.8629, val_loss: 29.4786, val_MinusLogProbMetric: 29.4786

Epoch 387: val_loss did not improve from 29.17812
196/196 - 34s - loss: 28.8629 - MinusLogProbMetric: 28.8629 - val_loss: 29.4786 - val_MinusLogProbMetric: 29.4786 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 388/1000
2023-10-12 11:32:11.497 
Epoch 388/1000 
	 loss: 28.8809, MinusLogProbMetric: 28.8809, val_loss: 29.2395, val_MinusLogProbMetric: 29.2395

Epoch 388: val_loss did not improve from 29.17812
196/196 - 34s - loss: 28.8809 - MinusLogProbMetric: 28.8809 - val_loss: 29.2395 - val_MinusLogProbMetric: 29.2395 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 389/1000
2023-10-12 11:32:44.883 
Epoch 389/1000 
	 loss: 28.8134, MinusLogProbMetric: 28.8134, val_loss: 29.2342, val_MinusLogProbMetric: 29.2342

Epoch 389: val_loss did not improve from 29.17812
196/196 - 33s - loss: 28.8134 - MinusLogProbMetric: 28.8134 - val_loss: 29.2342 - val_MinusLogProbMetric: 29.2342 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 390/1000
2023-10-12 11:33:18.562 
Epoch 390/1000 
	 loss: 28.8498, MinusLogProbMetric: 28.8498, val_loss: 29.1015, val_MinusLogProbMetric: 29.1015

Epoch 390: val_loss improved from 29.17812 to 29.10154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.8498 - MinusLogProbMetric: 28.8498 - val_loss: 29.1015 - val_MinusLogProbMetric: 29.1015 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 391/1000
2023-10-12 11:33:52.841 
Epoch 391/1000 
	 loss: 28.8043, MinusLogProbMetric: 28.8043, val_loss: 29.1397, val_MinusLogProbMetric: 29.1397

Epoch 391: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8043 - MinusLogProbMetric: 28.8043 - val_loss: 29.1397 - val_MinusLogProbMetric: 29.1397 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 392/1000
2023-10-12 11:34:26.397 
Epoch 392/1000 
	 loss: 28.8435, MinusLogProbMetric: 28.8435, val_loss: 29.1483, val_MinusLogProbMetric: 29.1483

Epoch 392: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8435 - MinusLogProbMetric: 28.8435 - val_loss: 29.1483 - val_MinusLogProbMetric: 29.1483 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 393/1000
2023-10-12 11:35:00.272 
Epoch 393/1000 
	 loss: 28.7923, MinusLogProbMetric: 28.7923, val_loss: 29.2800, val_MinusLogProbMetric: 29.2800

Epoch 393: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7923 - MinusLogProbMetric: 28.7923 - val_loss: 29.2800 - val_MinusLogProbMetric: 29.2800 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 394/1000
2023-10-12 11:35:34.092 
Epoch 394/1000 
	 loss: 28.8668, MinusLogProbMetric: 28.8668, val_loss: 29.3111, val_MinusLogProbMetric: 29.3111

Epoch 394: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8668 - MinusLogProbMetric: 28.8668 - val_loss: 29.3111 - val_MinusLogProbMetric: 29.3111 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 395/1000
2023-10-12 11:36:08.181 
Epoch 395/1000 
	 loss: 28.8554, MinusLogProbMetric: 28.8554, val_loss: 29.1410, val_MinusLogProbMetric: 29.1410

Epoch 395: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8554 - MinusLogProbMetric: 28.8554 - val_loss: 29.1410 - val_MinusLogProbMetric: 29.1410 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 396/1000
2023-10-12 11:36:41.547 
Epoch 396/1000 
	 loss: 28.8893, MinusLogProbMetric: 28.8893, val_loss: 29.1430, val_MinusLogProbMetric: 29.1430

Epoch 396: val_loss did not improve from 29.10154
196/196 - 33s - loss: 28.8893 - MinusLogProbMetric: 28.8893 - val_loss: 29.1430 - val_MinusLogProbMetric: 29.1430 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 397/1000
2023-10-12 11:37:15.532 
Epoch 397/1000 
	 loss: 28.8530, MinusLogProbMetric: 28.8530, val_loss: 29.2986, val_MinusLogProbMetric: 29.2986

Epoch 397: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8530 - MinusLogProbMetric: 28.8530 - val_loss: 29.2986 - val_MinusLogProbMetric: 29.2986 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 398/1000
2023-10-12 11:37:49.577 
Epoch 398/1000 
	 loss: 28.8503, MinusLogProbMetric: 28.8503, val_loss: 29.1164, val_MinusLogProbMetric: 29.1164

Epoch 398: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8503 - MinusLogProbMetric: 28.8503 - val_loss: 29.1164 - val_MinusLogProbMetric: 29.1164 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 399/1000
2023-10-12 11:38:23.339 
Epoch 399/1000 
	 loss: 28.7950, MinusLogProbMetric: 28.7950, val_loss: 29.1332, val_MinusLogProbMetric: 29.1332

Epoch 399: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7950 - MinusLogProbMetric: 28.7950 - val_loss: 29.1332 - val_MinusLogProbMetric: 29.1332 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 400/1000
2023-10-12 11:38:56.863 
Epoch 400/1000 
	 loss: 28.8521, MinusLogProbMetric: 28.8521, val_loss: 29.1781, val_MinusLogProbMetric: 29.1781

Epoch 400: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8521 - MinusLogProbMetric: 28.8521 - val_loss: 29.1781 - val_MinusLogProbMetric: 29.1781 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 401/1000
2023-10-12 11:39:30.453 
Epoch 401/1000 
	 loss: 28.8039, MinusLogProbMetric: 28.8039, val_loss: 29.2978, val_MinusLogProbMetric: 29.2978

Epoch 401: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8039 - MinusLogProbMetric: 28.8039 - val_loss: 29.2978 - val_MinusLogProbMetric: 29.2978 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 402/1000
2023-10-12 11:40:04.456 
Epoch 402/1000 
	 loss: 28.8115, MinusLogProbMetric: 28.8115, val_loss: 29.3564, val_MinusLogProbMetric: 29.3564

Epoch 402: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8115 - MinusLogProbMetric: 28.8115 - val_loss: 29.3564 - val_MinusLogProbMetric: 29.3564 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 403/1000
2023-10-12 11:40:38.348 
Epoch 403/1000 
	 loss: 28.8231, MinusLogProbMetric: 28.8231, val_loss: 29.2163, val_MinusLogProbMetric: 29.2163

Epoch 403: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8231 - MinusLogProbMetric: 28.8231 - val_loss: 29.2163 - val_MinusLogProbMetric: 29.2163 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 404/1000
2023-10-12 11:41:12.266 
Epoch 404/1000 
	 loss: 28.8261, MinusLogProbMetric: 28.8261, val_loss: 29.2133, val_MinusLogProbMetric: 29.2133

Epoch 404: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8261 - MinusLogProbMetric: 28.8261 - val_loss: 29.2133 - val_MinusLogProbMetric: 29.2133 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 405/1000
2023-10-12 11:41:46.242 
Epoch 405/1000 
	 loss: 28.8610, MinusLogProbMetric: 28.8610, val_loss: 29.2522, val_MinusLogProbMetric: 29.2522

Epoch 405: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8610 - MinusLogProbMetric: 28.8610 - val_loss: 29.2522 - val_MinusLogProbMetric: 29.2522 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 406/1000
2023-10-12 11:42:20.012 
Epoch 406/1000 
	 loss: 28.8235, MinusLogProbMetric: 28.8235, val_loss: 29.4056, val_MinusLogProbMetric: 29.4056

Epoch 406: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8235 - MinusLogProbMetric: 28.8235 - val_loss: 29.4056 - val_MinusLogProbMetric: 29.4056 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 407/1000
2023-10-12 11:42:53.864 
Epoch 407/1000 
	 loss: 28.8895, MinusLogProbMetric: 28.8895, val_loss: 29.5399, val_MinusLogProbMetric: 29.5399

Epoch 407: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8895 - MinusLogProbMetric: 28.8895 - val_loss: 29.5399 - val_MinusLogProbMetric: 29.5399 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 408/1000
2023-10-12 11:43:27.836 
Epoch 408/1000 
	 loss: 28.8327, MinusLogProbMetric: 28.8327, val_loss: 29.1040, val_MinusLogProbMetric: 29.1040

Epoch 408: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8327 - MinusLogProbMetric: 28.8327 - val_loss: 29.1040 - val_MinusLogProbMetric: 29.1040 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 409/1000
2023-10-12 11:44:01.745 
Epoch 409/1000 
	 loss: 28.7798, MinusLogProbMetric: 28.7798, val_loss: 29.3478, val_MinusLogProbMetric: 29.3478

Epoch 409: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7798 - MinusLogProbMetric: 28.7798 - val_loss: 29.3478 - val_MinusLogProbMetric: 29.3478 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 410/1000
2023-10-12 11:44:35.161 
Epoch 410/1000 
	 loss: 28.7968, MinusLogProbMetric: 28.7968, val_loss: 29.1663, val_MinusLogProbMetric: 29.1663

Epoch 410: val_loss did not improve from 29.10154
196/196 - 33s - loss: 28.7968 - MinusLogProbMetric: 28.7968 - val_loss: 29.1663 - val_MinusLogProbMetric: 29.1663 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 411/1000
2023-10-12 11:45:08.940 
Epoch 411/1000 
	 loss: 28.8089, MinusLogProbMetric: 28.8089, val_loss: 29.2146, val_MinusLogProbMetric: 29.2146

Epoch 411: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8089 - MinusLogProbMetric: 28.8089 - val_loss: 29.2146 - val_MinusLogProbMetric: 29.2146 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 412/1000
2023-10-12 11:45:43.122 
Epoch 412/1000 
	 loss: 28.8304, MinusLogProbMetric: 28.8304, val_loss: 29.1072, val_MinusLogProbMetric: 29.1072

Epoch 412: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8304 - MinusLogProbMetric: 28.8304 - val_loss: 29.1072 - val_MinusLogProbMetric: 29.1072 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 413/1000
2023-10-12 11:46:17.317 
Epoch 413/1000 
	 loss: 28.7721, MinusLogProbMetric: 28.7721, val_loss: 29.1066, val_MinusLogProbMetric: 29.1066

Epoch 413: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7721 - MinusLogProbMetric: 28.7721 - val_loss: 29.1066 - val_MinusLogProbMetric: 29.1066 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 414/1000
2023-10-12 11:46:51.462 
Epoch 414/1000 
	 loss: 28.8729, MinusLogProbMetric: 28.8729, val_loss: 29.2688, val_MinusLogProbMetric: 29.2688

Epoch 414: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8729 - MinusLogProbMetric: 28.8729 - val_loss: 29.2688 - val_MinusLogProbMetric: 29.2688 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 415/1000
2023-10-12 11:47:25.492 
Epoch 415/1000 
	 loss: 28.8832, MinusLogProbMetric: 28.8832, val_loss: 29.3102, val_MinusLogProbMetric: 29.3102

Epoch 415: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8832 - MinusLogProbMetric: 28.8832 - val_loss: 29.3102 - val_MinusLogProbMetric: 29.3102 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 416/1000
2023-10-12 11:47:59.269 
Epoch 416/1000 
	 loss: 28.7920, MinusLogProbMetric: 28.7920, val_loss: 29.2041, val_MinusLogProbMetric: 29.2041

Epoch 416: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7920 - MinusLogProbMetric: 28.7920 - val_loss: 29.2041 - val_MinusLogProbMetric: 29.2041 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 417/1000
2023-10-12 11:48:33.238 
Epoch 417/1000 
	 loss: 28.7754, MinusLogProbMetric: 28.7754, val_loss: 29.2368, val_MinusLogProbMetric: 29.2368

Epoch 417: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7754 - MinusLogProbMetric: 28.7754 - val_loss: 29.2368 - val_MinusLogProbMetric: 29.2368 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 418/1000
2023-10-12 11:49:06.895 
Epoch 418/1000 
	 loss: 28.8052, MinusLogProbMetric: 28.8052, val_loss: 29.1547, val_MinusLogProbMetric: 29.1547

Epoch 418: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8052 - MinusLogProbMetric: 28.8052 - val_loss: 29.1547 - val_MinusLogProbMetric: 29.1547 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 419/1000
2023-10-12 11:49:40.776 
Epoch 419/1000 
	 loss: 28.8024, MinusLogProbMetric: 28.8024, val_loss: 29.1076, val_MinusLogProbMetric: 29.1076

Epoch 419: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8024 - MinusLogProbMetric: 28.8024 - val_loss: 29.1076 - val_MinusLogProbMetric: 29.1076 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 420/1000
2023-10-12 11:50:14.892 
Epoch 420/1000 
	 loss: 28.7652, MinusLogProbMetric: 28.7652, val_loss: 29.1157, val_MinusLogProbMetric: 29.1157

Epoch 420: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7652 - MinusLogProbMetric: 28.7652 - val_loss: 29.1157 - val_MinusLogProbMetric: 29.1157 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 421/1000
2023-10-12 11:50:49.873 
Epoch 421/1000 
	 loss: 28.8119, MinusLogProbMetric: 28.8119, val_loss: 29.2505, val_MinusLogProbMetric: 29.2505

Epoch 421: val_loss did not improve from 29.10154
196/196 - 35s - loss: 28.8119 - MinusLogProbMetric: 28.8119 - val_loss: 29.2505 - val_MinusLogProbMetric: 29.2505 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 422/1000
2023-10-12 11:51:27.863 
Epoch 422/1000 
	 loss: 28.8187, MinusLogProbMetric: 28.8187, val_loss: 29.5421, val_MinusLogProbMetric: 29.5421

Epoch 422: val_loss did not improve from 29.10154
196/196 - 38s - loss: 28.8187 - MinusLogProbMetric: 28.8187 - val_loss: 29.5421 - val_MinusLogProbMetric: 29.5421 - lr: 1.6667e-04 - 38s/epoch - 194ms/step
Epoch 423/1000
2023-10-12 11:52:00.922 
Epoch 423/1000 
	 loss: 28.8689, MinusLogProbMetric: 28.8689, val_loss: 29.1595, val_MinusLogProbMetric: 29.1595

Epoch 423: val_loss did not improve from 29.10154
196/196 - 33s - loss: 28.8689 - MinusLogProbMetric: 28.8689 - val_loss: 29.1595 - val_MinusLogProbMetric: 29.1595 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 424/1000
2023-10-12 11:52:34.512 
Epoch 424/1000 
	 loss: 28.7899, MinusLogProbMetric: 28.7899, val_loss: 29.1436, val_MinusLogProbMetric: 29.1436

Epoch 424: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7899 - MinusLogProbMetric: 28.7899 - val_loss: 29.1436 - val_MinusLogProbMetric: 29.1436 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 425/1000
2023-10-12 11:53:06.037 
Epoch 425/1000 
	 loss: 28.7498, MinusLogProbMetric: 28.7498, val_loss: 29.1332, val_MinusLogProbMetric: 29.1332

Epoch 425: val_loss did not improve from 29.10154
196/196 - 31s - loss: 28.7498 - MinusLogProbMetric: 28.7498 - val_loss: 29.1332 - val_MinusLogProbMetric: 29.1332 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 426/1000
2023-10-12 11:53:39.794 
Epoch 426/1000 
	 loss: 28.7957, MinusLogProbMetric: 28.7957, val_loss: 29.1996, val_MinusLogProbMetric: 29.1996

Epoch 426: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.7957 - MinusLogProbMetric: 28.7957 - val_loss: 29.1996 - val_MinusLogProbMetric: 29.1996 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 427/1000
2023-10-12 11:54:13.418 
Epoch 427/1000 
	 loss: 28.8036, MinusLogProbMetric: 28.8036, val_loss: 29.1829, val_MinusLogProbMetric: 29.1829

Epoch 427: val_loss did not improve from 29.10154
196/196 - 34s - loss: 28.8036 - MinusLogProbMetric: 28.8036 - val_loss: 29.1829 - val_MinusLogProbMetric: 29.1829 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 428/1000
2023-10-12 11:54:47.271 
Epoch 428/1000 
	 loss: 28.8871, MinusLogProbMetric: 28.8871, val_loss: 29.0875, val_MinusLogProbMetric: 29.0875

Epoch 428: val_loss improved from 29.10154 to 29.08749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 28.8871 - MinusLogProbMetric: 28.8871 - val_loss: 29.0875 - val_MinusLogProbMetric: 29.0875 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 429/1000
2023-10-12 11:55:20.712 
Epoch 429/1000 
	 loss: 28.7281, MinusLogProbMetric: 28.7281, val_loss: 29.4171, val_MinusLogProbMetric: 29.4171

Epoch 429: val_loss did not improve from 29.08749
196/196 - 33s - loss: 28.7281 - MinusLogProbMetric: 28.7281 - val_loss: 29.4171 - val_MinusLogProbMetric: 29.4171 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 430/1000
2023-10-12 11:55:54.277 
Epoch 430/1000 
	 loss: 28.8112, MinusLogProbMetric: 28.8112, val_loss: 29.3477, val_MinusLogProbMetric: 29.3477

Epoch 430: val_loss did not improve from 29.08749
196/196 - 34s - loss: 28.8112 - MinusLogProbMetric: 28.8112 - val_loss: 29.3477 - val_MinusLogProbMetric: 29.3477 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 431/1000
2023-10-12 11:56:28.311 
Epoch 431/1000 
	 loss: 28.8211, MinusLogProbMetric: 28.8211, val_loss: 29.3078, val_MinusLogProbMetric: 29.3078

Epoch 431: val_loss did not improve from 29.08749
196/196 - 34s - loss: 28.8211 - MinusLogProbMetric: 28.8211 - val_loss: 29.3078 - val_MinusLogProbMetric: 29.3078 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 432/1000
2023-10-12 11:57:02.062 
Epoch 432/1000 
	 loss: 28.8183, MinusLogProbMetric: 28.8183, val_loss: 29.1720, val_MinusLogProbMetric: 29.1720

Epoch 432: val_loss did not improve from 29.08749
196/196 - 34s - loss: 28.8183 - MinusLogProbMetric: 28.8183 - val_loss: 29.1720 - val_MinusLogProbMetric: 29.1720 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 433/1000
2023-10-12 11:57:35.753 
Epoch 433/1000 
	 loss: 28.8091, MinusLogProbMetric: 28.8091, val_loss: 29.0259, val_MinusLogProbMetric: 29.0259

Epoch 433: val_loss improved from 29.08749 to 29.02590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.8091 - MinusLogProbMetric: 28.8091 - val_loss: 29.0259 - val_MinusLogProbMetric: 29.0259 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 434/1000
2023-10-12 11:58:10.137 
Epoch 434/1000 
	 loss: 28.7933, MinusLogProbMetric: 28.7933, val_loss: 29.3881, val_MinusLogProbMetric: 29.3881

Epoch 434: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7933 - MinusLogProbMetric: 28.7933 - val_loss: 29.3881 - val_MinusLogProbMetric: 29.3881 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 435/1000
2023-10-12 11:58:44.093 
Epoch 435/1000 
	 loss: 28.8168, MinusLogProbMetric: 28.8168, val_loss: 29.3861, val_MinusLogProbMetric: 29.3861

Epoch 435: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.8168 - MinusLogProbMetric: 28.8168 - val_loss: 29.3861 - val_MinusLogProbMetric: 29.3861 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 436/1000
2023-10-12 11:59:17.898 
Epoch 436/1000 
	 loss: 28.8304, MinusLogProbMetric: 28.8304, val_loss: 29.2374, val_MinusLogProbMetric: 29.2374

Epoch 436: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.8304 - MinusLogProbMetric: 28.8304 - val_loss: 29.2374 - val_MinusLogProbMetric: 29.2374 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 437/1000
2023-10-12 11:59:51.716 
Epoch 437/1000 
	 loss: 28.7522, MinusLogProbMetric: 28.7522, val_loss: 29.1284, val_MinusLogProbMetric: 29.1284

Epoch 437: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7522 - MinusLogProbMetric: 28.7522 - val_loss: 29.1284 - val_MinusLogProbMetric: 29.1284 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 438/1000
2023-10-12 12:00:25.553 
Epoch 438/1000 
	 loss: 28.7553, MinusLogProbMetric: 28.7553, val_loss: 29.2108, val_MinusLogProbMetric: 29.2108

Epoch 438: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7553 - MinusLogProbMetric: 28.7553 - val_loss: 29.2108 - val_MinusLogProbMetric: 29.2108 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 439/1000
2023-10-12 12:00:58.982 
Epoch 439/1000 
	 loss: 28.7953, MinusLogProbMetric: 28.7953, val_loss: 29.1478, val_MinusLogProbMetric: 29.1478

Epoch 439: val_loss did not improve from 29.02590
196/196 - 33s - loss: 28.7953 - MinusLogProbMetric: 28.7953 - val_loss: 29.1478 - val_MinusLogProbMetric: 29.1478 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 440/1000
2023-10-12 12:01:32.598 
Epoch 440/1000 
	 loss: 28.7935, MinusLogProbMetric: 28.7935, val_loss: 29.2396, val_MinusLogProbMetric: 29.2396

Epoch 440: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7935 - MinusLogProbMetric: 28.7935 - val_loss: 29.2396 - val_MinusLogProbMetric: 29.2396 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 441/1000
2023-10-12 12:02:06.234 
Epoch 441/1000 
	 loss: 28.7653, MinusLogProbMetric: 28.7653, val_loss: 29.6318, val_MinusLogProbMetric: 29.6318

Epoch 441: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7653 - MinusLogProbMetric: 28.7653 - val_loss: 29.6318 - val_MinusLogProbMetric: 29.6318 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 442/1000
2023-10-12 12:02:39.965 
Epoch 442/1000 
	 loss: 28.7445, MinusLogProbMetric: 28.7445, val_loss: 29.1994, val_MinusLogProbMetric: 29.1994

Epoch 442: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7445 - MinusLogProbMetric: 28.7445 - val_loss: 29.1994 - val_MinusLogProbMetric: 29.1994 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 443/1000
2023-10-12 12:03:13.618 
Epoch 443/1000 
	 loss: 28.7945, MinusLogProbMetric: 28.7945, val_loss: 29.5929, val_MinusLogProbMetric: 29.5929

Epoch 443: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7945 - MinusLogProbMetric: 28.7945 - val_loss: 29.5929 - val_MinusLogProbMetric: 29.5929 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 444/1000
2023-10-12 12:03:47.154 
Epoch 444/1000 
	 loss: 28.7726, MinusLogProbMetric: 28.7726, val_loss: 29.1845, val_MinusLogProbMetric: 29.1845

Epoch 444: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7726 - MinusLogProbMetric: 28.7726 - val_loss: 29.1845 - val_MinusLogProbMetric: 29.1845 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 445/1000
2023-10-12 12:04:20.780 
Epoch 445/1000 
	 loss: 28.7891, MinusLogProbMetric: 28.7891, val_loss: 29.1246, val_MinusLogProbMetric: 29.1246

Epoch 445: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7891 - MinusLogProbMetric: 28.7891 - val_loss: 29.1246 - val_MinusLogProbMetric: 29.1246 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 446/1000
2023-10-12 12:04:54.415 
Epoch 446/1000 
	 loss: 28.7327, MinusLogProbMetric: 28.7327, val_loss: 29.0519, val_MinusLogProbMetric: 29.0519

Epoch 446: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7327 - MinusLogProbMetric: 28.7327 - val_loss: 29.0519 - val_MinusLogProbMetric: 29.0519 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 447/1000
2023-10-12 12:05:28.105 
Epoch 447/1000 
	 loss: 28.8328, MinusLogProbMetric: 28.8328, val_loss: 29.0565, val_MinusLogProbMetric: 29.0565

Epoch 447: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.8328 - MinusLogProbMetric: 28.8328 - val_loss: 29.0565 - val_MinusLogProbMetric: 29.0565 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 448/1000
2023-10-12 12:06:01.813 
Epoch 448/1000 
	 loss: 28.8142, MinusLogProbMetric: 28.8142, val_loss: 29.1218, val_MinusLogProbMetric: 29.1218

Epoch 448: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.8142 - MinusLogProbMetric: 28.8142 - val_loss: 29.1218 - val_MinusLogProbMetric: 29.1218 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 449/1000
2023-10-12 12:06:35.743 
Epoch 449/1000 
	 loss: 28.7668, MinusLogProbMetric: 28.7668, val_loss: 29.0931, val_MinusLogProbMetric: 29.0931

Epoch 449: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7668 - MinusLogProbMetric: 28.7668 - val_loss: 29.0931 - val_MinusLogProbMetric: 29.0931 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 450/1000
2023-10-12 12:07:09.607 
Epoch 450/1000 
	 loss: 28.7980, MinusLogProbMetric: 28.7980, val_loss: 29.1418, val_MinusLogProbMetric: 29.1418

Epoch 450: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7980 - MinusLogProbMetric: 28.7980 - val_loss: 29.1418 - val_MinusLogProbMetric: 29.1418 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 451/1000
2023-10-12 12:07:43.140 
Epoch 451/1000 
	 loss: 28.7453, MinusLogProbMetric: 28.7453, val_loss: 29.1582, val_MinusLogProbMetric: 29.1582

Epoch 451: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7453 - MinusLogProbMetric: 28.7453 - val_loss: 29.1582 - val_MinusLogProbMetric: 29.1582 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 452/1000
2023-10-12 12:08:16.843 
Epoch 452/1000 
	 loss: 28.7471, MinusLogProbMetric: 28.7471, val_loss: 29.2252, val_MinusLogProbMetric: 29.2252

Epoch 452: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7471 - MinusLogProbMetric: 28.7471 - val_loss: 29.2252 - val_MinusLogProbMetric: 29.2252 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 453/1000
2023-10-12 12:08:50.562 
Epoch 453/1000 
	 loss: 28.7570, MinusLogProbMetric: 28.7570, val_loss: 29.0941, val_MinusLogProbMetric: 29.0941

Epoch 453: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7570 - MinusLogProbMetric: 28.7570 - val_loss: 29.0941 - val_MinusLogProbMetric: 29.0941 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 454/1000
2023-10-12 12:09:24.466 
Epoch 454/1000 
	 loss: 28.7460, MinusLogProbMetric: 28.7460, val_loss: 29.1299, val_MinusLogProbMetric: 29.1299

Epoch 454: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7460 - MinusLogProbMetric: 28.7460 - val_loss: 29.1299 - val_MinusLogProbMetric: 29.1299 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 455/1000
2023-10-12 12:09:58.253 
Epoch 455/1000 
	 loss: 28.7311, MinusLogProbMetric: 28.7311, val_loss: 29.3862, val_MinusLogProbMetric: 29.3862

Epoch 455: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7311 - MinusLogProbMetric: 28.7311 - val_loss: 29.3862 - val_MinusLogProbMetric: 29.3862 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 456/1000
2023-10-12 12:10:31.839 
Epoch 456/1000 
	 loss: 28.8475, MinusLogProbMetric: 28.8475, val_loss: 29.1953, val_MinusLogProbMetric: 29.1953

Epoch 456: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.8475 - MinusLogProbMetric: 28.8475 - val_loss: 29.1953 - val_MinusLogProbMetric: 29.1953 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 457/1000
2023-10-12 12:11:06.087 
Epoch 457/1000 
	 loss: 28.7770, MinusLogProbMetric: 28.7770, val_loss: 29.0815, val_MinusLogProbMetric: 29.0815

Epoch 457: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7770 - MinusLogProbMetric: 28.7770 - val_loss: 29.0815 - val_MinusLogProbMetric: 29.0815 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 458/1000
2023-10-12 12:11:40.162 
Epoch 458/1000 
	 loss: 28.7651, MinusLogProbMetric: 28.7651, val_loss: 29.0947, val_MinusLogProbMetric: 29.0947

Epoch 458: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7651 - MinusLogProbMetric: 28.7651 - val_loss: 29.0947 - val_MinusLogProbMetric: 29.0947 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 459/1000
2023-10-12 12:12:13.800 
Epoch 459/1000 
	 loss: 28.7619, MinusLogProbMetric: 28.7619, val_loss: 29.2572, val_MinusLogProbMetric: 29.2572

Epoch 459: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7619 - MinusLogProbMetric: 28.7619 - val_loss: 29.2572 - val_MinusLogProbMetric: 29.2572 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 460/1000
2023-10-12 12:12:47.604 
Epoch 460/1000 
	 loss: 28.7155, MinusLogProbMetric: 28.7155, val_loss: 29.3003, val_MinusLogProbMetric: 29.3003

Epoch 460: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7155 - MinusLogProbMetric: 28.7155 - val_loss: 29.3003 - val_MinusLogProbMetric: 29.3003 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 461/1000
2023-10-12 12:13:21.405 
Epoch 461/1000 
	 loss: 28.7355, MinusLogProbMetric: 28.7355, val_loss: 29.0406, val_MinusLogProbMetric: 29.0406

Epoch 461: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7355 - MinusLogProbMetric: 28.7355 - val_loss: 29.0406 - val_MinusLogProbMetric: 29.0406 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 462/1000
2023-10-12 12:13:55.095 
Epoch 462/1000 
	 loss: 28.7917, MinusLogProbMetric: 28.7917, val_loss: 29.9508, val_MinusLogProbMetric: 29.9508

Epoch 462: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7917 - MinusLogProbMetric: 28.7917 - val_loss: 29.9508 - val_MinusLogProbMetric: 29.9508 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 463/1000
2023-10-12 12:14:28.726 
Epoch 463/1000 
	 loss: 28.7669, MinusLogProbMetric: 28.7669, val_loss: 29.1641, val_MinusLogProbMetric: 29.1641

Epoch 463: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7669 - MinusLogProbMetric: 28.7669 - val_loss: 29.1641 - val_MinusLogProbMetric: 29.1641 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 464/1000
2023-10-12 12:15:02.573 
Epoch 464/1000 
	 loss: 28.7169, MinusLogProbMetric: 28.7169, val_loss: 29.0691, val_MinusLogProbMetric: 29.0691

Epoch 464: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7169 - MinusLogProbMetric: 28.7169 - val_loss: 29.0691 - val_MinusLogProbMetric: 29.0691 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 465/1000
2023-10-12 12:15:36.216 
Epoch 465/1000 
	 loss: 28.7851, MinusLogProbMetric: 28.7851, val_loss: 29.1563, val_MinusLogProbMetric: 29.1563

Epoch 465: val_loss did not improve from 29.02590
196/196 - 34s - loss: 28.7851 - MinusLogProbMetric: 28.7851 - val_loss: 29.1563 - val_MinusLogProbMetric: 29.1563 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 466/1000
2023-10-12 12:16:09.703 
Epoch 466/1000 
	 loss: 28.7112, MinusLogProbMetric: 28.7112, val_loss: 29.0153, val_MinusLogProbMetric: 29.0153

Epoch 466: val_loss improved from 29.02590 to 29.01532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.7112 - MinusLogProbMetric: 28.7112 - val_loss: 29.0153 - val_MinusLogProbMetric: 29.0153 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 467/1000
2023-10-12 12:16:43.952 
Epoch 467/1000 
	 loss: 28.8073, MinusLogProbMetric: 28.8073, val_loss: 29.3087, val_MinusLogProbMetric: 29.3087

Epoch 467: val_loss did not improve from 29.01532
196/196 - 34s - loss: 28.8073 - MinusLogProbMetric: 28.8073 - val_loss: 29.3087 - val_MinusLogProbMetric: 29.3087 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 468/1000
2023-10-12 12:17:17.552 
Epoch 468/1000 
	 loss: 28.7937, MinusLogProbMetric: 28.7937, val_loss: 29.5167, val_MinusLogProbMetric: 29.5167

Epoch 468: val_loss did not improve from 29.01532
196/196 - 34s - loss: 28.7937 - MinusLogProbMetric: 28.7937 - val_loss: 29.5167 - val_MinusLogProbMetric: 29.5167 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 469/1000
2023-10-12 12:17:51.026 
Epoch 469/1000 
	 loss: 28.7280, MinusLogProbMetric: 28.7280, val_loss: 29.1612, val_MinusLogProbMetric: 29.1612

Epoch 469: val_loss did not improve from 29.01532
196/196 - 33s - loss: 28.7280 - MinusLogProbMetric: 28.7280 - val_loss: 29.1612 - val_MinusLogProbMetric: 29.1612 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 470/1000
2023-10-12 12:18:24.716 
Epoch 470/1000 
	 loss: 28.7355, MinusLogProbMetric: 28.7355, val_loss: 29.1789, val_MinusLogProbMetric: 29.1789

Epoch 470: val_loss did not improve from 29.01532
196/196 - 34s - loss: 28.7355 - MinusLogProbMetric: 28.7355 - val_loss: 29.1789 - val_MinusLogProbMetric: 29.1789 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 471/1000
2023-10-12 12:18:58.353 
Epoch 471/1000 
	 loss: 28.7580, MinusLogProbMetric: 28.7580, val_loss: 29.1279, val_MinusLogProbMetric: 29.1279

Epoch 471: val_loss did not improve from 29.01532
196/196 - 34s - loss: 28.7580 - MinusLogProbMetric: 28.7580 - val_loss: 29.1279 - val_MinusLogProbMetric: 29.1279 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 472/1000
2023-10-12 12:19:31.628 
Epoch 472/1000 
	 loss: 28.7322, MinusLogProbMetric: 28.7322, val_loss: 29.0637, val_MinusLogProbMetric: 29.0637

Epoch 472: val_loss did not improve from 29.01532
196/196 - 33s - loss: 28.7322 - MinusLogProbMetric: 28.7322 - val_loss: 29.0637 - val_MinusLogProbMetric: 29.0637 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 473/1000
2023-10-12 12:20:03.255 
Epoch 473/1000 
	 loss: 28.7224, MinusLogProbMetric: 28.7224, val_loss: 29.0086, val_MinusLogProbMetric: 29.0086

Epoch 473: val_loss improved from 29.01532 to 29.00860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 32s - loss: 28.7224 - MinusLogProbMetric: 28.7224 - val_loss: 29.0086 - val_MinusLogProbMetric: 29.0086 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 474/1000
2023-10-12 12:20:37.086 
Epoch 474/1000 
	 loss: 28.8185, MinusLogProbMetric: 28.8185, val_loss: 29.2983, val_MinusLogProbMetric: 29.2983

Epoch 474: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.8185 - MinusLogProbMetric: 28.8185 - val_loss: 29.2983 - val_MinusLogProbMetric: 29.2983 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 475/1000
2023-10-12 12:21:10.715 
Epoch 475/1000 
	 loss: 28.7258, MinusLogProbMetric: 28.7258, val_loss: 29.3224, val_MinusLogProbMetric: 29.3224

Epoch 475: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.7258 - MinusLogProbMetric: 28.7258 - val_loss: 29.3224 - val_MinusLogProbMetric: 29.3224 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 476/1000
2023-10-12 12:21:44.094 
Epoch 476/1000 
	 loss: 28.7554, MinusLogProbMetric: 28.7554, val_loss: 29.2217, val_MinusLogProbMetric: 29.2217

Epoch 476: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.7554 - MinusLogProbMetric: 28.7554 - val_loss: 29.2217 - val_MinusLogProbMetric: 29.2217 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 477/1000
2023-10-12 12:22:17.661 
Epoch 477/1000 
	 loss: 28.8575, MinusLogProbMetric: 28.8575, val_loss: 29.3333, val_MinusLogProbMetric: 29.3333

Epoch 477: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.8575 - MinusLogProbMetric: 28.8575 - val_loss: 29.3333 - val_MinusLogProbMetric: 29.3333 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 478/1000
2023-10-12 12:22:51.186 
Epoch 478/1000 
	 loss: 28.8497, MinusLogProbMetric: 28.8497, val_loss: 29.3157, val_MinusLogProbMetric: 29.3157

Epoch 478: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.8497 - MinusLogProbMetric: 28.8497 - val_loss: 29.3157 - val_MinusLogProbMetric: 29.3157 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 479/1000
2023-10-12 12:23:24.906 
Epoch 479/1000 
	 loss: 28.7627, MinusLogProbMetric: 28.7627, val_loss: 29.4247, val_MinusLogProbMetric: 29.4247

Epoch 479: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.7627 - MinusLogProbMetric: 28.7627 - val_loss: 29.4247 - val_MinusLogProbMetric: 29.4247 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 480/1000
2023-10-12 12:23:58.390 
Epoch 480/1000 
	 loss: 28.6879, MinusLogProbMetric: 28.6879, val_loss: 29.0832, val_MinusLogProbMetric: 29.0832

Epoch 480: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.6879 - MinusLogProbMetric: 28.6879 - val_loss: 29.0832 - val_MinusLogProbMetric: 29.0832 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 481/1000
2023-10-12 12:24:31.928 
Epoch 481/1000 
	 loss: 28.7121, MinusLogProbMetric: 28.7121, val_loss: 29.4366, val_MinusLogProbMetric: 29.4366

Epoch 481: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.7121 - MinusLogProbMetric: 28.7121 - val_loss: 29.4366 - val_MinusLogProbMetric: 29.4366 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 482/1000
2023-10-12 12:25:05.261 
Epoch 482/1000 
	 loss: 28.7203, MinusLogProbMetric: 28.7203, val_loss: 29.3550, val_MinusLogProbMetric: 29.3550

Epoch 482: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.7203 - MinusLogProbMetric: 28.7203 - val_loss: 29.3550 - val_MinusLogProbMetric: 29.3550 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 483/1000
2023-10-12 12:25:39.077 
Epoch 483/1000 
	 loss: 28.7916, MinusLogProbMetric: 28.7916, val_loss: 29.0639, val_MinusLogProbMetric: 29.0639

Epoch 483: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.7916 - MinusLogProbMetric: 28.7916 - val_loss: 29.0639 - val_MinusLogProbMetric: 29.0639 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 484/1000
2023-10-12 12:26:13.044 
Epoch 484/1000 
	 loss: 28.6845, MinusLogProbMetric: 28.6845, val_loss: 29.2325, val_MinusLogProbMetric: 29.2325

Epoch 484: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.6845 - MinusLogProbMetric: 28.6845 - val_loss: 29.2325 - val_MinusLogProbMetric: 29.2325 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 485/1000
2023-10-12 12:26:46.658 
Epoch 485/1000 
	 loss: 28.8389, MinusLogProbMetric: 28.8389, val_loss: 29.0803, val_MinusLogProbMetric: 29.0803

Epoch 485: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.8389 - MinusLogProbMetric: 28.8389 - val_loss: 29.0803 - val_MinusLogProbMetric: 29.0803 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 486/1000
2023-10-12 12:27:18.689 
Epoch 486/1000 
	 loss: 28.6789, MinusLogProbMetric: 28.6789, val_loss: 29.1761, val_MinusLogProbMetric: 29.1761

Epoch 486: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.6789 - MinusLogProbMetric: 28.6789 - val_loss: 29.1761 - val_MinusLogProbMetric: 29.1761 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 487/1000
2023-10-12 12:27:50.810 
Epoch 487/1000 
	 loss: 28.7583, MinusLogProbMetric: 28.7583, val_loss: 29.0503, val_MinusLogProbMetric: 29.0503

Epoch 487: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.7583 - MinusLogProbMetric: 28.7583 - val_loss: 29.0503 - val_MinusLogProbMetric: 29.0503 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 488/1000
2023-10-12 12:28:22.073 
Epoch 488/1000 
	 loss: 28.7197, MinusLogProbMetric: 28.7197, val_loss: 29.0704, val_MinusLogProbMetric: 29.0704

Epoch 488: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.7197 - MinusLogProbMetric: 28.7197 - val_loss: 29.0704 - val_MinusLogProbMetric: 29.0704 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 489/1000
2023-10-12 12:28:54.065 
Epoch 489/1000 
	 loss: 28.6840, MinusLogProbMetric: 28.6840, val_loss: 29.0397, val_MinusLogProbMetric: 29.0397

Epoch 489: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.6840 - MinusLogProbMetric: 28.6840 - val_loss: 29.0397 - val_MinusLogProbMetric: 29.0397 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 490/1000
2023-10-12 12:29:25.511 
Epoch 490/1000 
	 loss: 28.7424, MinusLogProbMetric: 28.7424, val_loss: 29.2891, val_MinusLogProbMetric: 29.2891

Epoch 490: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.7424 - MinusLogProbMetric: 28.7424 - val_loss: 29.2891 - val_MinusLogProbMetric: 29.2891 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 491/1000
2023-10-12 12:29:57.097 
Epoch 491/1000 
	 loss: 28.6765, MinusLogProbMetric: 28.6765, val_loss: 29.1921, val_MinusLogProbMetric: 29.1921

Epoch 491: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.6765 - MinusLogProbMetric: 28.6765 - val_loss: 29.1921 - val_MinusLogProbMetric: 29.1921 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 492/1000
2023-10-12 12:30:29.429 
Epoch 492/1000 
	 loss: 28.7103, MinusLogProbMetric: 28.7103, val_loss: 29.0121, val_MinusLogProbMetric: 29.0121

Epoch 492: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.7103 - MinusLogProbMetric: 28.7103 - val_loss: 29.0121 - val_MinusLogProbMetric: 29.0121 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 493/1000
2023-10-12 12:31:02.522 
Epoch 493/1000 
	 loss: 28.7673, MinusLogProbMetric: 28.7673, val_loss: 29.0933, val_MinusLogProbMetric: 29.0933

Epoch 493: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.7673 - MinusLogProbMetric: 28.7673 - val_loss: 29.0933 - val_MinusLogProbMetric: 29.0933 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 494/1000
2023-10-12 12:31:36.286 
Epoch 494/1000 
	 loss: 28.7381, MinusLogProbMetric: 28.7381, val_loss: 29.0777, val_MinusLogProbMetric: 29.0777

Epoch 494: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.7381 - MinusLogProbMetric: 28.7381 - val_loss: 29.0777 - val_MinusLogProbMetric: 29.0777 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 495/1000
2023-10-12 12:32:09.532 
Epoch 495/1000 
	 loss: 28.7283, MinusLogProbMetric: 28.7283, val_loss: 29.0479, val_MinusLogProbMetric: 29.0479

Epoch 495: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.7283 - MinusLogProbMetric: 28.7283 - val_loss: 29.0479 - val_MinusLogProbMetric: 29.0479 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 496/1000
2023-10-12 12:32:43.117 
Epoch 496/1000 
	 loss: 28.6896, MinusLogProbMetric: 28.6896, val_loss: 29.3555, val_MinusLogProbMetric: 29.3555

Epoch 496: val_loss did not improve from 29.00860
196/196 - 34s - loss: 28.6896 - MinusLogProbMetric: 28.6896 - val_loss: 29.3555 - val_MinusLogProbMetric: 29.3555 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 497/1000
2023-10-12 12:33:15.182 
Epoch 497/1000 
	 loss: 28.6813, MinusLogProbMetric: 28.6813, val_loss: 29.1604, val_MinusLogProbMetric: 29.1604

Epoch 497: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.6813 - MinusLogProbMetric: 28.6813 - val_loss: 29.1604 - val_MinusLogProbMetric: 29.1604 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 498/1000
2023-10-12 12:33:47.964 
Epoch 498/1000 
	 loss: 28.7156, MinusLogProbMetric: 28.7156, val_loss: 29.6293, val_MinusLogProbMetric: 29.6293

Epoch 498: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.7156 - MinusLogProbMetric: 28.7156 - val_loss: 29.6293 - val_MinusLogProbMetric: 29.6293 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 499/1000
2023-10-12 12:34:19.287 
Epoch 499/1000 
	 loss: 28.7203, MinusLogProbMetric: 28.7203, val_loss: 29.3341, val_MinusLogProbMetric: 29.3341

Epoch 499: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.7203 - MinusLogProbMetric: 28.7203 - val_loss: 29.3341 - val_MinusLogProbMetric: 29.3341 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 500/1000
2023-10-12 12:34:48.828 
Epoch 500/1000 
	 loss: 28.7498, MinusLogProbMetric: 28.7498, val_loss: 29.0712, val_MinusLogProbMetric: 29.0712

Epoch 500: val_loss did not improve from 29.00860
196/196 - 30s - loss: 28.7498 - MinusLogProbMetric: 28.7498 - val_loss: 29.0712 - val_MinusLogProbMetric: 29.0712 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 501/1000
2023-10-12 12:35:18.592 
Epoch 501/1000 
	 loss: 28.7048, MinusLogProbMetric: 28.7048, val_loss: 29.0666, val_MinusLogProbMetric: 29.0666

Epoch 501: val_loss did not improve from 29.00860
196/196 - 30s - loss: 28.7048 - MinusLogProbMetric: 28.7048 - val_loss: 29.0666 - val_MinusLogProbMetric: 29.0666 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 502/1000
2023-10-12 12:35:49.232 
Epoch 502/1000 
	 loss: 28.6453, MinusLogProbMetric: 28.6453, val_loss: 29.0654, val_MinusLogProbMetric: 29.0654

Epoch 502: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.6453 - MinusLogProbMetric: 28.6453 - val_loss: 29.0654 - val_MinusLogProbMetric: 29.0654 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 503/1000
2023-10-12 12:36:20.194 
Epoch 503/1000 
	 loss: 28.6616, MinusLogProbMetric: 28.6616, val_loss: 29.1597, val_MinusLogProbMetric: 29.1597

Epoch 503: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.6616 - MinusLogProbMetric: 28.6616 - val_loss: 29.1597 - val_MinusLogProbMetric: 29.1597 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 504/1000
2023-10-12 12:36:50.380 
Epoch 504/1000 
	 loss: 28.6555, MinusLogProbMetric: 28.6555, val_loss: 29.1349, val_MinusLogProbMetric: 29.1349

Epoch 504: val_loss did not improve from 29.00860
196/196 - 30s - loss: 28.6555 - MinusLogProbMetric: 28.6555 - val_loss: 29.1349 - val_MinusLogProbMetric: 29.1349 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 505/1000
2023-10-12 12:37:20.715 
Epoch 505/1000 
	 loss: 28.6403, MinusLogProbMetric: 28.6403, val_loss: 29.8685, val_MinusLogProbMetric: 29.8685

Epoch 505: val_loss did not improve from 29.00860
196/196 - 30s - loss: 28.6403 - MinusLogProbMetric: 28.6403 - val_loss: 29.8685 - val_MinusLogProbMetric: 29.8685 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 506/1000
2023-10-12 12:37:51.509 
Epoch 506/1000 
	 loss: 28.7528, MinusLogProbMetric: 28.7528, val_loss: 29.2347, val_MinusLogProbMetric: 29.2347

Epoch 506: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.7528 - MinusLogProbMetric: 28.7528 - val_loss: 29.2347 - val_MinusLogProbMetric: 29.2347 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 507/1000
2023-10-12 12:38:22.861 
Epoch 507/1000 
	 loss: 28.6963, MinusLogProbMetric: 28.6963, val_loss: 29.2811, val_MinusLogProbMetric: 29.2811

Epoch 507: val_loss did not improve from 29.00860
196/196 - 31s - loss: 28.6963 - MinusLogProbMetric: 28.6963 - val_loss: 29.2811 - val_MinusLogProbMetric: 29.2811 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 508/1000
2023-10-12 12:38:54.799 
Epoch 508/1000 
	 loss: 28.6847, MinusLogProbMetric: 28.6847, val_loss: 29.1722, val_MinusLogProbMetric: 29.1722

Epoch 508: val_loss did not improve from 29.00860
196/196 - 32s - loss: 28.6847 - MinusLogProbMetric: 28.6847 - val_loss: 29.1722 - val_MinusLogProbMetric: 29.1722 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 509/1000
2023-10-12 12:39:28.263 
Epoch 509/1000 
	 loss: 28.6597, MinusLogProbMetric: 28.6597, val_loss: 29.2233, val_MinusLogProbMetric: 29.2233

Epoch 509: val_loss did not improve from 29.00860
196/196 - 33s - loss: 28.6597 - MinusLogProbMetric: 28.6597 - val_loss: 29.2233 - val_MinusLogProbMetric: 29.2233 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 510/1000
2023-10-12 12:40:02.092 
Epoch 510/1000 
	 loss: 28.6308, MinusLogProbMetric: 28.6308, val_loss: 28.9660, val_MinusLogProbMetric: 28.9660

Epoch 510: val_loss improved from 29.00860 to 28.96599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.6308 - MinusLogProbMetric: 28.6308 - val_loss: 28.9660 - val_MinusLogProbMetric: 28.9660 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 511/1000
2023-10-12 12:40:36.582 
Epoch 511/1000 
	 loss: 28.7237, MinusLogProbMetric: 28.7237, val_loss: 29.2997, val_MinusLogProbMetric: 29.2997

Epoch 511: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7237 - MinusLogProbMetric: 28.7237 - val_loss: 29.2997 - val_MinusLogProbMetric: 29.2997 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 512/1000
2023-10-12 12:41:10.267 
Epoch 512/1000 
	 loss: 28.7213, MinusLogProbMetric: 28.7213, val_loss: 29.4160, val_MinusLogProbMetric: 29.4160

Epoch 512: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7213 - MinusLogProbMetric: 28.7213 - val_loss: 29.4160 - val_MinusLogProbMetric: 29.4160 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 513/1000
2023-10-12 12:41:44.318 
Epoch 513/1000 
	 loss: 28.6443, MinusLogProbMetric: 28.6443, val_loss: 29.0422, val_MinusLogProbMetric: 29.0422

Epoch 513: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6443 - MinusLogProbMetric: 28.6443 - val_loss: 29.0422 - val_MinusLogProbMetric: 29.0422 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 514/1000
2023-10-12 12:42:17.703 
Epoch 514/1000 
	 loss: 28.6421, MinusLogProbMetric: 28.6421, val_loss: 29.1349, val_MinusLogProbMetric: 29.1349

Epoch 514: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.6421 - MinusLogProbMetric: 28.6421 - val_loss: 29.1349 - val_MinusLogProbMetric: 29.1349 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 515/1000
2023-10-12 12:42:51.051 
Epoch 515/1000 
	 loss: 28.6908, MinusLogProbMetric: 28.6908, val_loss: 29.0784, val_MinusLogProbMetric: 29.0784

Epoch 515: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.6908 - MinusLogProbMetric: 28.6908 - val_loss: 29.0784 - val_MinusLogProbMetric: 29.0784 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 516/1000
2023-10-12 12:43:24.471 
Epoch 516/1000 
	 loss: 28.6854, MinusLogProbMetric: 28.6854, val_loss: 29.0342, val_MinusLogProbMetric: 29.0342

Epoch 516: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.6854 - MinusLogProbMetric: 28.6854 - val_loss: 29.0342 - val_MinusLogProbMetric: 29.0342 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 517/1000
2023-10-12 12:43:58.017 
Epoch 517/1000 
	 loss: 28.7021, MinusLogProbMetric: 28.7021, val_loss: 29.1304, val_MinusLogProbMetric: 29.1304

Epoch 517: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7021 - MinusLogProbMetric: 28.7021 - val_loss: 29.1304 - val_MinusLogProbMetric: 29.1304 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 518/1000
2023-10-12 12:44:31.649 
Epoch 518/1000 
	 loss: 28.6587, MinusLogProbMetric: 28.6587, val_loss: 28.9861, val_MinusLogProbMetric: 28.9861

Epoch 518: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6587 - MinusLogProbMetric: 28.6587 - val_loss: 28.9861 - val_MinusLogProbMetric: 28.9861 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 519/1000
2023-10-12 12:45:05.424 
Epoch 519/1000 
	 loss: 28.6544, MinusLogProbMetric: 28.6544, val_loss: 29.0781, val_MinusLogProbMetric: 29.0781

Epoch 519: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6544 - MinusLogProbMetric: 28.6544 - val_loss: 29.0781 - val_MinusLogProbMetric: 29.0781 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 520/1000
2023-10-12 12:45:39.057 
Epoch 520/1000 
	 loss: 28.6934, MinusLogProbMetric: 28.6934, val_loss: 29.1648, val_MinusLogProbMetric: 29.1648

Epoch 520: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6934 - MinusLogProbMetric: 28.6934 - val_loss: 29.1648 - val_MinusLogProbMetric: 29.1648 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 521/1000
2023-10-12 12:46:12.618 
Epoch 521/1000 
	 loss: 28.6918, MinusLogProbMetric: 28.6918, val_loss: 29.1055, val_MinusLogProbMetric: 29.1055

Epoch 521: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6918 - MinusLogProbMetric: 28.6918 - val_loss: 29.1055 - val_MinusLogProbMetric: 29.1055 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 522/1000
2023-10-12 12:46:46.250 
Epoch 522/1000 
	 loss: 28.7139, MinusLogProbMetric: 28.7139, val_loss: 29.0537, val_MinusLogProbMetric: 29.0537

Epoch 522: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7139 - MinusLogProbMetric: 28.7139 - val_loss: 29.0537 - val_MinusLogProbMetric: 29.0537 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 523/1000
2023-10-12 12:47:19.984 
Epoch 523/1000 
	 loss: 28.6701, MinusLogProbMetric: 28.6701, val_loss: 29.2080, val_MinusLogProbMetric: 29.2080

Epoch 523: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6701 - MinusLogProbMetric: 28.6701 - val_loss: 29.2080 - val_MinusLogProbMetric: 29.2080 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 524/1000
2023-10-12 12:47:53.703 
Epoch 524/1000 
	 loss: 28.7592, MinusLogProbMetric: 28.7592, val_loss: 28.9929, val_MinusLogProbMetric: 28.9929

Epoch 524: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7592 - MinusLogProbMetric: 28.7592 - val_loss: 28.9929 - val_MinusLogProbMetric: 28.9929 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 525/1000
2023-10-12 12:48:27.541 
Epoch 525/1000 
	 loss: 28.6791, MinusLogProbMetric: 28.6791, val_loss: 29.1825, val_MinusLogProbMetric: 29.1825

Epoch 525: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6791 - MinusLogProbMetric: 28.6791 - val_loss: 29.1825 - val_MinusLogProbMetric: 29.1825 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 526/1000
2023-10-12 12:49:01.168 
Epoch 526/1000 
	 loss: 28.6863, MinusLogProbMetric: 28.6863, val_loss: 28.9852, val_MinusLogProbMetric: 28.9852

Epoch 526: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6863 - MinusLogProbMetric: 28.6863 - val_loss: 28.9852 - val_MinusLogProbMetric: 28.9852 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 527/1000
2023-10-12 12:49:35.168 
Epoch 527/1000 
	 loss: 28.6825, MinusLogProbMetric: 28.6825, val_loss: 29.0895, val_MinusLogProbMetric: 29.0895

Epoch 527: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6825 - MinusLogProbMetric: 28.6825 - val_loss: 29.0895 - val_MinusLogProbMetric: 29.0895 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 528/1000
2023-10-12 12:50:09.060 
Epoch 528/1000 
	 loss: 28.6887, MinusLogProbMetric: 28.6887, val_loss: 29.1879, val_MinusLogProbMetric: 29.1879

Epoch 528: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6887 - MinusLogProbMetric: 28.6887 - val_loss: 29.1879 - val_MinusLogProbMetric: 29.1879 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 529/1000
2023-10-12 12:50:42.834 
Epoch 529/1000 
	 loss: 28.7204, MinusLogProbMetric: 28.7204, val_loss: 29.0869, val_MinusLogProbMetric: 29.0869

Epoch 529: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7204 - MinusLogProbMetric: 28.7204 - val_loss: 29.0869 - val_MinusLogProbMetric: 29.0869 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 530/1000
2023-10-12 12:51:16.510 
Epoch 530/1000 
	 loss: 28.6615, MinusLogProbMetric: 28.6615, val_loss: 29.8472, val_MinusLogProbMetric: 29.8472

Epoch 530: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6615 - MinusLogProbMetric: 28.6615 - val_loss: 29.8472 - val_MinusLogProbMetric: 29.8472 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 531/1000
2023-10-12 12:51:50.087 
Epoch 531/1000 
	 loss: 28.7241, MinusLogProbMetric: 28.7241, val_loss: 29.4209, val_MinusLogProbMetric: 29.4209

Epoch 531: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7241 - MinusLogProbMetric: 28.7241 - val_loss: 29.4209 - val_MinusLogProbMetric: 29.4209 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 532/1000
2023-10-12 12:52:24.207 
Epoch 532/1000 
	 loss: 28.6991, MinusLogProbMetric: 28.6991, val_loss: 29.2950, val_MinusLogProbMetric: 29.2950

Epoch 532: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6991 - MinusLogProbMetric: 28.6991 - val_loss: 29.2950 - val_MinusLogProbMetric: 29.2950 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 533/1000
2023-10-12 12:52:57.866 
Epoch 533/1000 
	 loss: 28.7078, MinusLogProbMetric: 28.7078, val_loss: 29.0717, val_MinusLogProbMetric: 29.0717

Epoch 533: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7078 - MinusLogProbMetric: 28.7078 - val_loss: 29.0717 - val_MinusLogProbMetric: 29.0717 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 534/1000
2023-10-12 12:53:31.669 
Epoch 534/1000 
	 loss: 28.7391, MinusLogProbMetric: 28.7391, val_loss: 29.0373, val_MinusLogProbMetric: 29.0373

Epoch 534: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7391 - MinusLogProbMetric: 28.7391 - val_loss: 29.0373 - val_MinusLogProbMetric: 29.0373 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 535/1000
2023-10-12 12:54:05.637 
Epoch 535/1000 
	 loss: 28.6686, MinusLogProbMetric: 28.6686, val_loss: 29.0851, val_MinusLogProbMetric: 29.0851

Epoch 535: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6686 - MinusLogProbMetric: 28.6686 - val_loss: 29.0851 - val_MinusLogProbMetric: 29.0851 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 536/1000
2023-10-12 12:54:39.540 
Epoch 536/1000 
	 loss: 28.7648, MinusLogProbMetric: 28.7648, val_loss: 29.1073, val_MinusLogProbMetric: 29.1073

Epoch 536: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7648 - MinusLogProbMetric: 28.7648 - val_loss: 29.1073 - val_MinusLogProbMetric: 29.1073 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 537/1000
2023-10-12 12:55:13.826 
Epoch 537/1000 
	 loss: 28.7068, MinusLogProbMetric: 28.7068, val_loss: 29.1222, val_MinusLogProbMetric: 29.1222

Epoch 537: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7068 - MinusLogProbMetric: 28.7068 - val_loss: 29.1222 - val_MinusLogProbMetric: 29.1222 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 538/1000
2023-10-12 12:55:47.629 
Epoch 538/1000 
	 loss: 28.6832, MinusLogProbMetric: 28.6832, val_loss: 29.3604, val_MinusLogProbMetric: 29.3604

Epoch 538: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6832 - MinusLogProbMetric: 28.6832 - val_loss: 29.3604 - val_MinusLogProbMetric: 29.3604 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 539/1000
2023-10-12 12:56:22.202 
Epoch 539/1000 
	 loss: 28.7205, MinusLogProbMetric: 28.7205, val_loss: 29.1845, val_MinusLogProbMetric: 29.1845

Epoch 539: val_loss did not improve from 28.96599
196/196 - 35s - loss: 28.7205 - MinusLogProbMetric: 28.7205 - val_loss: 29.1845 - val_MinusLogProbMetric: 29.1845 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 540/1000
2023-10-12 12:56:55.840 
Epoch 540/1000 
	 loss: 28.6426, MinusLogProbMetric: 28.6426, val_loss: 29.3433, val_MinusLogProbMetric: 29.3433

Epoch 540: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6426 - MinusLogProbMetric: 28.6426 - val_loss: 29.3433 - val_MinusLogProbMetric: 29.3433 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 541/1000
2023-10-12 12:57:29.702 
Epoch 541/1000 
	 loss: 28.6579, MinusLogProbMetric: 28.6579, val_loss: 29.2187, val_MinusLogProbMetric: 29.2187

Epoch 541: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6579 - MinusLogProbMetric: 28.6579 - val_loss: 29.2187 - val_MinusLogProbMetric: 29.2187 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 542/1000
2023-10-12 12:58:03.528 
Epoch 542/1000 
	 loss: 28.6328, MinusLogProbMetric: 28.6328, val_loss: 29.2414, val_MinusLogProbMetric: 29.2414

Epoch 542: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6328 - MinusLogProbMetric: 28.6328 - val_loss: 29.2414 - val_MinusLogProbMetric: 29.2414 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 543/1000
2023-10-12 12:58:36.664 
Epoch 543/1000 
	 loss: 28.6965, MinusLogProbMetric: 28.6965, val_loss: 29.1319, val_MinusLogProbMetric: 29.1319

Epoch 543: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.6965 - MinusLogProbMetric: 28.6965 - val_loss: 29.1319 - val_MinusLogProbMetric: 29.1319 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 544/1000
2023-10-12 12:59:10.560 
Epoch 544/1000 
	 loss: 28.6855, MinusLogProbMetric: 28.6855, val_loss: 29.3565, val_MinusLogProbMetric: 29.3565

Epoch 544: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6855 - MinusLogProbMetric: 28.6855 - val_loss: 29.3565 - val_MinusLogProbMetric: 29.3565 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 545/1000
2023-10-12 12:59:43.992 
Epoch 545/1000 
	 loss: 28.7018, MinusLogProbMetric: 28.7018, val_loss: 29.0064, val_MinusLogProbMetric: 29.0064

Epoch 545: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.7018 - MinusLogProbMetric: 28.7018 - val_loss: 29.0064 - val_MinusLogProbMetric: 29.0064 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 546/1000
2023-10-12 13:00:17.481 
Epoch 546/1000 
	 loss: 28.6543, MinusLogProbMetric: 28.6543, val_loss: 29.1810, val_MinusLogProbMetric: 29.1810

Epoch 546: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6543 - MinusLogProbMetric: 28.6543 - val_loss: 29.1810 - val_MinusLogProbMetric: 29.1810 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 547/1000
2023-10-12 13:00:51.012 
Epoch 547/1000 
	 loss: 28.6517, MinusLogProbMetric: 28.6517, val_loss: 29.2243, val_MinusLogProbMetric: 29.2243

Epoch 547: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6517 - MinusLogProbMetric: 28.6517 - val_loss: 29.2243 - val_MinusLogProbMetric: 29.2243 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 548/1000
2023-10-12 13:01:24.384 
Epoch 548/1000 
	 loss: 28.7004, MinusLogProbMetric: 28.7004, val_loss: 29.0289, val_MinusLogProbMetric: 29.0289

Epoch 548: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.7004 - MinusLogProbMetric: 28.7004 - val_loss: 29.0289 - val_MinusLogProbMetric: 29.0289 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 549/1000
2023-10-12 13:01:58.089 
Epoch 549/1000 
	 loss: 28.7185, MinusLogProbMetric: 28.7185, val_loss: 28.9838, val_MinusLogProbMetric: 28.9838

Epoch 549: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.7185 - MinusLogProbMetric: 28.7185 - val_loss: 28.9838 - val_MinusLogProbMetric: 28.9838 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 550/1000
2023-10-12 13:02:31.496 
Epoch 550/1000 
	 loss: 28.6365, MinusLogProbMetric: 28.6365, val_loss: 29.0096, val_MinusLogProbMetric: 29.0096

Epoch 550: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.6365 - MinusLogProbMetric: 28.6365 - val_loss: 29.0096 - val_MinusLogProbMetric: 29.0096 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 551/1000
2023-10-12 13:03:04.664 
Epoch 551/1000 
	 loss: 28.6111, MinusLogProbMetric: 28.6111, val_loss: 28.9749, val_MinusLogProbMetric: 28.9749

Epoch 551: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.6111 - MinusLogProbMetric: 28.6111 - val_loss: 28.9749 - val_MinusLogProbMetric: 28.9749 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 552/1000
2023-10-12 13:03:37.943 
Epoch 552/1000 
	 loss: 28.7103, MinusLogProbMetric: 28.7103, val_loss: 28.9829, val_MinusLogProbMetric: 28.9829

Epoch 552: val_loss did not improve from 28.96599
196/196 - 33s - loss: 28.7103 - MinusLogProbMetric: 28.7103 - val_loss: 28.9829 - val_MinusLogProbMetric: 28.9829 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 553/1000
2023-10-12 13:04:11.520 
Epoch 553/1000 
	 loss: 28.6442, MinusLogProbMetric: 28.6442, val_loss: 29.0913, val_MinusLogProbMetric: 29.0913

Epoch 553: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6442 - MinusLogProbMetric: 28.6442 - val_loss: 29.0913 - val_MinusLogProbMetric: 29.0913 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 554/1000
2023-10-12 13:04:45.106 
Epoch 554/1000 
	 loss: 28.6304, MinusLogProbMetric: 28.6304, val_loss: 29.1081, val_MinusLogProbMetric: 29.1081

Epoch 554: val_loss did not improve from 28.96599
196/196 - 34s - loss: 28.6304 - MinusLogProbMetric: 28.6304 - val_loss: 29.1081 - val_MinusLogProbMetric: 29.1081 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 555/1000
2023-10-12 13:05:19.121 
Epoch 555/1000 
	 loss: 28.6735, MinusLogProbMetric: 28.6735, val_loss: 28.9640, val_MinusLogProbMetric: 28.9640

Epoch 555: val_loss improved from 28.96599 to 28.96401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 28.6735 - MinusLogProbMetric: 28.6735 - val_loss: 28.9640 - val_MinusLogProbMetric: 28.9640 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 556/1000
2023-10-12 13:05:52.920 
Epoch 556/1000 
	 loss: 28.6341, MinusLogProbMetric: 28.6341, val_loss: 28.9791, val_MinusLogProbMetric: 28.9791

Epoch 556: val_loss did not improve from 28.96401
196/196 - 33s - loss: 28.6341 - MinusLogProbMetric: 28.6341 - val_loss: 28.9791 - val_MinusLogProbMetric: 28.9791 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 557/1000
2023-10-12 13:06:26.472 
Epoch 557/1000 
	 loss: 28.7237, MinusLogProbMetric: 28.7237, val_loss: 29.1034, val_MinusLogProbMetric: 29.1034

Epoch 557: val_loss did not improve from 28.96401
196/196 - 34s - loss: 28.7237 - MinusLogProbMetric: 28.7237 - val_loss: 29.1034 - val_MinusLogProbMetric: 29.1034 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 558/1000
2023-10-12 13:06:59.882 
Epoch 558/1000 
	 loss: 28.6570, MinusLogProbMetric: 28.6570, val_loss: 29.0174, val_MinusLogProbMetric: 29.0174

Epoch 558: val_loss did not improve from 28.96401
196/196 - 33s - loss: 28.6570 - MinusLogProbMetric: 28.6570 - val_loss: 29.0174 - val_MinusLogProbMetric: 29.0174 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 559/1000
2023-10-12 13:07:33.280 
Epoch 559/1000 
	 loss: 28.6103, MinusLogProbMetric: 28.6103, val_loss: 29.0518, val_MinusLogProbMetric: 29.0518

Epoch 559: val_loss did not improve from 28.96401
196/196 - 33s - loss: 28.6103 - MinusLogProbMetric: 28.6103 - val_loss: 29.0518 - val_MinusLogProbMetric: 29.0518 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 560/1000
2023-10-12 13:08:06.439 
Epoch 560/1000 
	 loss: 28.6000, MinusLogProbMetric: 28.6000, val_loss: 28.9420, val_MinusLogProbMetric: 28.9420

Epoch 560: val_loss improved from 28.96401 to 28.94204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.6000 - MinusLogProbMetric: 28.6000 - val_loss: 28.9420 - val_MinusLogProbMetric: 28.9420 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 561/1000
2023-10-12 13:08:40.962 
Epoch 561/1000 
	 loss: 28.6806, MinusLogProbMetric: 28.6806, val_loss: 29.0020, val_MinusLogProbMetric: 29.0020

Epoch 561: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.6806 - MinusLogProbMetric: 28.6806 - val_loss: 29.0020 - val_MinusLogProbMetric: 29.0020 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 562/1000
2023-10-12 13:09:14.618 
Epoch 562/1000 
	 loss: 28.6349, MinusLogProbMetric: 28.6349, val_loss: 28.9872, val_MinusLogProbMetric: 28.9872

Epoch 562: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.6349 - MinusLogProbMetric: 28.6349 - val_loss: 28.9872 - val_MinusLogProbMetric: 28.9872 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 563/1000
2023-10-12 13:09:47.999 
Epoch 563/1000 
	 loss: 28.6576, MinusLogProbMetric: 28.6576, val_loss: 29.1846, val_MinusLogProbMetric: 29.1846

Epoch 563: val_loss did not improve from 28.94204
196/196 - 33s - loss: 28.6576 - MinusLogProbMetric: 28.6576 - val_loss: 29.1846 - val_MinusLogProbMetric: 29.1846 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 564/1000
2023-10-12 13:10:21.397 
Epoch 564/1000 
	 loss: 28.6886, MinusLogProbMetric: 28.6886, val_loss: 29.0883, val_MinusLogProbMetric: 29.0883

Epoch 564: val_loss did not improve from 28.94204
196/196 - 33s - loss: 28.6886 - MinusLogProbMetric: 28.6886 - val_loss: 29.0883 - val_MinusLogProbMetric: 29.0883 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 565/1000
2023-10-12 13:10:55.082 
Epoch 565/1000 
	 loss: 28.6576, MinusLogProbMetric: 28.6576, val_loss: 29.2879, val_MinusLogProbMetric: 29.2879

Epoch 565: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.6576 - MinusLogProbMetric: 28.6576 - val_loss: 29.2879 - val_MinusLogProbMetric: 29.2879 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 566/1000
2023-10-12 13:11:28.761 
Epoch 566/1000 
	 loss: 28.6523, MinusLogProbMetric: 28.6523, val_loss: 29.3063, val_MinusLogProbMetric: 29.3063

Epoch 566: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.6523 - MinusLogProbMetric: 28.6523 - val_loss: 29.3063 - val_MinusLogProbMetric: 29.3063 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 567/1000
2023-10-12 13:12:02.477 
Epoch 567/1000 
	 loss: 28.6338, MinusLogProbMetric: 28.6338, val_loss: 28.9552, val_MinusLogProbMetric: 28.9552

Epoch 567: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.6338 - MinusLogProbMetric: 28.6338 - val_loss: 28.9552 - val_MinusLogProbMetric: 28.9552 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 568/1000
2023-10-12 13:12:35.819 
Epoch 568/1000 
	 loss: 28.6743, MinusLogProbMetric: 28.6743, val_loss: 28.9779, val_MinusLogProbMetric: 28.9779

Epoch 568: val_loss did not improve from 28.94204
196/196 - 33s - loss: 28.6743 - MinusLogProbMetric: 28.6743 - val_loss: 28.9779 - val_MinusLogProbMetric: 28.9779 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 569/1000
2023-10-12 13:13:09.591 
Epoch 569/1000 
	 loss: 28.7460, MinusLogProbMetric: 28.7460, val_loss: 29.3811, val_MinusLogProbMetric: 29.3811

Epoch 569: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.7460 - MinusLogProbMetric: 28.7460 - val_loss: 29.3811 - val_MinusLogProbMetric: 29.3811 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 570/1000
2023-10-12 13:13:43.140 
Epoch 570/1000 
	 loss: 28.6719, MinusLogProbMetric: 28.6719, val_loss: 29.0322, val_MinusLogProbMetric: 29.0322

Epoch 570: val_loss did not improve from 28.94204
196/196 - 34s - loss: 28.6719 - MinusLogProbMetric: 28.6719 - val_loss: 29.0322 - val_MinusLogProbMetric: 29.0322 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 571/1000
2023-10-12 13:14:16.681 
Epoch 571/1000 
	 loss: 28.6032, MinusLogProbMetric: 28.6032, val_loss: 28.9261, val_MinusLogProbMetric: 28.9261

Epoch 571: val_loss improved from 28.94204 to 28.92606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.6032 - MinusLogProbMetric: 28.6032 - val_loss: 28.9261 - val_MinusLogProbMetric: 28.9261 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 572/1000
2023-10-12 13:14:50.592 
Epoch 572/1000 
	 loss: 28.6431, MinusLogProbMetric: 28.6431, val_loss: 29.0408, val_MinusLogProbMetric: 29.0408

Epoch 572: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6431 - MinusLogProbMetric: 28.6431 - val_loss: 29.0408 - val_MinusLogProbMetric: 29.0408 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 573/1000
2023-10-12 13:15:24.203 
Epoch 573/1000 
	 loss: 28.6420, MinusLogProbMetric: 28.6420, val_loss: 29.0857, val_MinusLogProbMetric: 29.0857

Epoch 573: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6420 - MinusLogProbMetric: 28.6420 - val_loss: 29.0857 - val_MinusLogProbMetric: 29.0857 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 574/1000
2023-10-12 13:15:57.775 
Epoch 574/1000 
	 loss: 28.6676, MinusLogProbMetric: 28.6676, val_loss: 28.9632, val_MinusLogProbMetric: 28.9632

Epoch 574: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6676 - MinusLogProbMetric: 28.6676 - val_loss: 28.9632 - val_MinusLogProbMetric: 28.9632 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 575/1000
2023-10-12 13:16:31.356 
Epoch 575/1000 
	 loss: 28.6172, MinusLogProbMetric: 28.6172, val_loss: 29.0929, val_MinusLogProbMetric: 29.0929

Epoch 575: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6172 - MinusLogProbMetric: 28.6172 - val_loss: 29.0929 - val_MinusLogProbMetric: 29.0929 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 576/1000
2023-10-12 13:17:04.655 
Epoch 576/1000 
	 loss: 28.6489, MinusLogProbMetric: 28.6489, val_loss: 29.1095, val_MinusLogProbMetric: 29.1095

Epoch 576: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6489 - MinusLogProbMetric: 28.6489 - val_loss: 29.1095 - val_MinusLogProbMetric: 29.1095 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 577/1000
2023-10-12 13:17:38.328 
Epoch 577/1000 
	 loss: 28.6321, MinusLogProbMetric: 28.6321, val_loss: 29.1376, val_MinusLogProbMetric: 29.1376

Epoch 577: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6321 - MinusLogProbMetric: 28.6321 - val_loss: 29.1376 - val_MinusLogProbMetric: 29.1376 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 578/1000
2023-10-12 13:18:11.891 
Epoch 578/1000 
	 loss: 28.8398, MinusLogProbMetric: 28.8398, val_loss: 28.9479, val_MinusLogProbMetric: 28.9479

Epoch 578: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.8398 - MinusLogProbMetric: 28.8398 - val_loss: 28.9479 - val_MinusLogProbMetric: 28.9479 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 579/1000
2023-10-12 13:18:45.676 
Epoch 579/1000 
	 loss: 28.6534, MinusLogProbMetric: 28.6534, val_loss: 28.9677, val_MinusLogProbMetric: 28.9677

Epoch 579: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6534 - MinusLogProbMetric: 28.6534 - val_loss: 28.9677 - val_MinusLogProbMetric: 28.9677 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 580/1000
2023-10-12 13:19:19.048 
Epoch 580/1000 
	 loss: 28.6491, MinusLogProbMetric: 28.6491, val_loss: 29.0247, val_MinusLogProbMetric: 29.0247

Epoch 580: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6491 - MinusLogProbMetric: 28.6491 - val_loss: 29.0247 - val_MinusLogProbMetric: 29.0247 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 581/1000
2023-10-12 13:19:52.544 
Epoch 581/1000 
	 loss: 28.6341, MinusLogProbMetric: 28.6341, val_loss: 29.2158, val_MinusLogProbMetric: 29.2158

Epoch 581: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6341 - MinusLogProbMetric: 28.6341 - val_loss: 29.2158 - val_MinusLogProbMetric: 29.2158 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 582/1000
2023-10-12 13:20:26.031 
Epoch 582/1000 
	 loss: 28.6410, MinusLogProbMetric: 28.6410, val_loss: 28.9956, val_MinusLogProbMetric: 28.9956

Epoch 582: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6410 - MinusLogProbMetric: 28.6410 - val_loss: 28.9956 - val_MinusLogProbMetric: 28.9956 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 583/1000
2023-10-12 13:20:59.511 
Epoch 583/1000 
	 loss: 28.6823, MinusLogProbMetric: 28.6823, val_loss: 29.3424, val_MinusLogProbMetric: 29.3424

Epoch 583: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6823 - MinusLogProbMetric: 28.6823 - val_loss: 29.3424 - val_MinusLogProbMetric: 29.3424 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 584/1000
2023-10-12 13:21:31.796 
Epoch 584/1000 
	 loss: 28.7214, MinusLogProbMetric: 28.7214, val_loss: 29.1829, val_MinusLogProbMetric: 29.1829

Epoch 584: val_loss did not improve from 28.92606
196/196 - 32s - loss: 28.7214 - MinusLogProbMetric: 28.7214 - val_loss: 29.1829 - val_MinusLogProbMetric: 29.1829 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 585/1000
2023-10-12 13:22:03.570 
Epoch 585/1000 
	 loss: 28.6086, MinusLogProbMetric: 28.6086, val_loss: 29.0142, val_MinusLogProbMetric: 29.0142

Epoch 585: val_loss did not improve from 28.92606
196/196 - 32s - loss: 28.6086 - MinusLogProbMetric: 28.6086 - val_loss: 29.0142 - val_MinusLogProbMetric: 29.0142 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 586/1000
2023-10-12 13:22:35.171 
Epoch 586/1000 
	 loss: 28.6890, MinusLogProbMetric: 28.6890, val_loss: 29.0411, val_MinusLogProbMetric: 29.0411

Epoch 586: val_loss did not improve from 28.92606
196/196 - 32s - loss: 28.6890 - MinusLogProbMetric: 28.6890 - val_loss: 29.0411 - val_MinusLogProbMetric: 29.0411 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 587/1000
2023-10-12 13:23:06.562 
Epoch 587/1000 
	 loss: 28.6993, MinusLogProbMetric: 28.6993, val_loss: 29.1651, val_MinusLogProbMetric: 29.1651

Epoch 587: val_loss did not improve from 28.92606
196/196 - 31s - loss: 28.6993 - MinusLogProbMetric: 28.6993 - val_loss: 29.1651 - val_MinusLogProbMetric: 29.1651 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 588/1000
2023-10-12 13:23:38.692 
Epoch 588/1000 
	 loss: 28.6655, MinusLogProbMetric: 28.6655, val_loss: 29.1607, val_MinusLogProbMetric: 29.1607

Epoch 588: val_loss did not improve from 28.92606
196/196 - 32s - loss: 28.6655 - MinusLogProbMetric: 28.6655 - val_loss: 29.1607 - val_MinusLogProbMetric: 29.1607 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 589/1000
2023-10-12 13:24:10.471 
Epoch 589/1000 
	 loss: 28.7270, MinusLogProbMetric: 28.7270, val_loss: 29.6800, val_MinusLogProbMetric: 29.6800

Epoch 589: val_loss did not improve from 28.92606
196/196 - 32s - loss: 28.7270 - MinusLogProbMetric: 28.7270 - val_loss: 29.6800 - val_MinusLogProbMetric: 29.6800 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 590/1000
2023-10-12 13:24:42.013 
Epoch 590/1000 
	 loss: 28.7252, MinusLogProbMetric: 28.7252, val_loss: 29.0113, val_MinusLogProbMetric: 29.0113

Epoch 590: val_loss did not improve from 28.92606
196/196 - 32s - loss: 28.7252 - MinusLogProbMetric: 28.7252 - val_loss: 29.0113 - val_MinusLogProbMetric: 29.0113 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 591/1000
2023-10-12 13:25:15.266 
Epoch 591/1000 
	 loss: 28.6092, MinusLogProbMetric: 28.6092, val_loss: 29.1032, val_MinusLogProbMetric: 29.1032

Epoch 591: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.6092 - MinusLogProbMetric: 28.6092 - val_loss: 29.1032 - val_MinusLogProbMetric: 29.1032 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 592/1000
2023-10-12 13:25:48.921 
Epoch 592/1000 
	 loss: 28.6004, MinusLogProbMetric: 28.6004, val_loss: 28.9810, val_MinusLogProbMetric: 28.9810

Epoch 592: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6004 - MinusLogProbMetric: 28.6004 - val_loss: 28.9810 - val_MinusLogProbMetric: 28.9810 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 593/1000
2023-10-12 13:26:22.717 
Epoch 593/1000 
	 loss: 28.6126, MinusLogProbMetric: 28.6126, val_loss: 29.0085, val_MinusLogProbMetric: 29.0085

Epoch 593: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6126 - MinusLogProbMetric: 28.6126 - val_loss: 29.0085 - val_MinusLogProbMetric: 29.0085 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 594/1000
2023-10-12 13:26:56.400 
Epoch 594/1000 
	 loss: 28.6390, MinusLogProbMetric: 28.6390, val_loss: 29.0578, val_MinusLogProbMetric: 29.0578

Epoch 594: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6390 - MinusLogProbMetric: 28.6390 - val_loss: 29.0578 - val_MinusLogProbMetric: 29.0578 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 595/1000
2023-10-12 13:27:29.864 
Epoch 595/1000 
	 loss: 28.5694, MinusLogProbMetric: 28.5694, val_loss: 28.9952, val_MinusLogProbMetric: 28.9952

Epoch 595: val_loss did not improve from 28.92606
196/196 - 33s - loss: 28.5694 - MinusLogProbMetric: 28.5694 - val_loss: 28.9952 - val_MinusLogProbMetric: 28.9952 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 596/1000
2023-10-12 13:28:03.402 
Epoch 596/1000 
	 loss: 28.6353, MinusLogProbMetric: 28.6353, val_loss: 28.9723, val_MinusLogProbMetric: 28.9723

Epoch 596: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6353 - MinusLogProbMetric: 28.6353 - val_loss: 28.9723 - val_MinusLogProbMetric: 28.9723 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 597/1000
2023-10-12 13:28:37.187 
Epoch 597/1000 
	 loss: 28.5980, MinusLogProbMetric: 28.5980, val_loss: 28.9871, val_MinusLogProbMetric: 28.9871

Epoch 597: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.5980 - MinusLogProbMetric: 28.5980 - val_loss: 28.9871 - val_MinusLogProbMetric: 28.9871 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 598/1000
2023-10-12 13:29:10.869 
Epoch 598/1000 
	 loss: 28.6025, MinusLogProbMetric: 28.6025, val_loss: 29.1277, val_MinusLogProbMetric: 29.1277

Epoch 598: val_loss did not improve from 28.92606
196/196 - 34s - loss: 28.6025 - MinusLogProbMetric: 28.6025 - val_loss: 29.1277 - val_MinusLogProbMetric: 29.1277 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 599/1000
2023-10-12 13:29:44.453 
Epoch 599/1000 
	 loss: 28.5898, MinusLogProbMetric: 28.5898, val_loss: 28.9224, val_MinusLogProbMetric: 28.9224

Epoch 599: val_loss improved from 28.92606 to 28.92239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.5898 - MinusLogProbMetric: 28.5898 - val_loss: 28.9224 - val_MinusLogProbMetric: 28.9224 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 600/1000
2023-10-12 13:30:18.958 
Epoch 600/1000 
	 loss: 28.5906, MinusLogProbMetric: 28.5906, val_loss: 29.1849, val_MinusLogProbMetric: 29.1849

Epoch 600: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5906 - MinusLogProbMetric: 28.5906 - val_loss: 29.1849 - val_MinusLogProbMetric: 29.1849 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 601/1000
2023-10-12 13:30:52.498 
Epoch 601/1000 
	 loss: 28.6301, MinusLogProbMetric: 28.6301, val_loss: 29.3423, val_MinusLogProbMetric: 29.3423

Epoch 601: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6301 - MinusLogProbMetric: 28.6301 - val_loss: 29.3423 - val_MinusLogProbMetric: 29.3423 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 602/1000
2023-10-12 13:31:25.912 
Epoch 602/1000 
	 loss: 28.6661, MinusLogProbMetric: 28.6661, val_loss: 28.9964, val_MinusLogProbMetric: 28.9964

Epoch 602: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.6661 - MinusLogProbMetric: 28.6661 - val_loss: 28.9964 - val_MinusLogProbMetric: 28.9964 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 603/1000
2023-10-12 13:31:59.186 
Epoch 603/1000 
	 loss: 28.6250, MinusLogProbMetric: 28.6250, val_loss: 29.0557, val_MinusLogProbMetric: 29.0557

Epoch 603: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.6250 - MinusLogProbMetric: 28.6250 - val_loss: 29.0557 - val_MinusLogProbMetric: 29.0557 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 604/1000
2023-10-12 13:32:31.492 
Epoch 604/1000 
	 loss: 28.6168, MinusLogProbMetric: 28.6168, val_loss: 29.0498, val_MinusLogProbMetric: 29.0498

Epoch 604: val_loss did not improve from 28.92239
196/196 - 32s - loss: 28.6168 - MinusLogProbMetric: 28.6168 - val_loss: 29.0498 - val_MinusLogProbMetric: 29.0498 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 605/1000
2023-10-12 13:33:04.847 
Epoch 605/1000 
	 loss: 28.6105, MinusLogProbMetric: 28.6105, val_loss: 28.9475, val_MinusLogProbMetric: 28.9475

Epoch 605: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.6105 - MinusLogProbMetric: 28.6105 - val_loss: 28.9475 - val_MinusLogProbMetric: 28.9475 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 606/1000
2023-10-12 13:33:38.159 
Epoch 606/1000 
	 loss: 28.5786, MinusLogProbMetric: 28.5786, val_loss: 28.9687, val_MinusLogProbMetric: 28.9687

Epoch 606: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.5786 - MinusLogProbMetric: 28.5786 - val_loss: 28.9687 - val_MinusLogProbMetric: 28.9687 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 607/1000
2023-10-12 13:34:12.077 
Epoch 607/1000 
	 loss: 28.5634, MinusLogProbMetric: 28.5634, val_loss: 29.0509, val_MinusLogProbMetric: 29.0509

Epoch 607: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5634 - MinusLogProbMetric: 28.5634 - val_loss: 29.0509 - val_MinusLogProbMetric: 29.0509 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 608/1000
2023-10-12 13:34:45.954 
Epoch 608/1000 
	 loss: 28.5970, MinusLogProbMetric: 28.5970, val_loss: 28.9419, val_MinusLogProbMetric: 28.9419

Epoch 608: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5970 - MinusLogProbMetric: 28.5970 - val_loss: 28.9419 - val_MinusLogProbMetric: 28.9419 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 609/1000
2023-10-12 13:35:19.576 
Epoch 609/1000 
	 loss: 28.5469, MinusLogProbMetric: 28.5469, val_loss: 29.0814, val_MinusLogProbMetric: 29.0814

Epoch 609: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5469 - MinusLogProbMetric: 28.5469 - val_loss: 29.0814 - val_MinusLogProbMetric: 29.0814 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 610/1000
2023-10-12 13:35:53.179 
Epoch 610/1000 
	 loss: 28.5676, MinusLogProbMetric: 28.5676, val_loss: 29.1144, val_MinusLogProbMetric: 29.1144

Epoch 610: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5676 - MinusLogProbMetric: 28.5676 - val_loss: 29.1144 - val_MinusLogProbMetric: 29.1144 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 611/1000
2023-10-12 13:36:26.927 
Epoch 611/1000 
	 loss: 28.6304, MinusLogProbMetric: 28.6304, val_loss: 29.0359, val_MinusLogProbMetric: 29.0359

Epoch 611: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6304 - MinusLogProbMetric: 28.6304 - val_loss: 29.0359 - val_MinusLogProbMetric: 29.0359 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 612/1000
2023-10-12 13:37:00.601 
Epoch 612/1000 
	 loss: 28.6755, MinusLogProbMetric: 28.6755, val_loss: 29.1642, val_MinusLogProbMetric: 29.1642

Epoch 612: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6755 - MinusLogProbMetric: 28.6755 - val_loss: 29.1642 - val_MinusLogProbMetric: 29.1642 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 613/1000
2023-10-12 13:37:34.513 
Epoch 613/1000 
	 loss: 28.5886, MinusLogProbMetric: 28.5886, val_loss: 28.9457, val_MinusLogProbMetric: 28.9457

Epoch 613: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5886 - MinusLogProbMetric: 28.5886 - val_loss: 28.9457 - val_MinusLogProbMetric: 28.9457 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 614/1000
2023-10-12 13:38:08.201 
Epoch 614/1000 
	 loss: 28.6471, MinusLogProbMetric: 28.6471, val_loss: 29.0804, val_MinusLogProbMetric: 29.0804

Epoch 614: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6471 - MinusLogProbMetric: 28.6471 - val_loss: 29.0804 - val_MinusLogProbMetric: 29.0804 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 615/1000
2023-10-12 13:38:41.799 
Epoch 615/1000 
	 loss: 28.5825, MinusLogProbMetric: 28.5825, val_loss: 29.2792, val_MinusLogProbMetric: 29.2792

Epoch 615: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5825 - MinusLogProbMetric: 28.5825 - val_loss: 29.2792 - val_MinusLogProbMetric: 29.2792 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 616/1000
2023-10-12 13:39:15.588 
Epoch 616/1000 
	 loss: 28.6182, MinusLogProbMetric: 28.6182, val_loss: 28.9878, val_MinusLogProbMetric: 28.9878

Epoch 616: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6182 - MinusLogProbMetric: 28.6182 - val_loss: 28.9878 - val_MinusLogProbMetric: 28.9878 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 617/1000
2023-10-12 13:39:49.245 
Epoch 617/1000 
	 loss: 28.5968, MinusLogProbMetric: 28.5968, val_loss: 29.0948, val_MinusLogProbMetric: 29.0948

Epoch 617: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5968 - MinusLogProbMetric: 28.5968 - val_loss: 29.0948 - val_MinusLogProbMetric: 29.0948 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 618/1000
2023-10-12 13:40:22.660 
Epoch 618/1000 
	 loss: 28.6401, MinusLogProbMetric: 28.6401, val_loss: 29.4415, val_MinusLogProbMetric: 29.4415

Epoch 618: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.6401 - MinusLogProbMetric: 28.6401 - val_loss: 29.4415 - val_MinusLogProbMetric: 29.4415 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 619/1000
2023-10-12 13:40:56.220 
Epoch 619/1000 
	 loss: 28.5843, MinusLogProbMetric: 28.5843, val_loss: 29.0794, val_MinusLogProbMetric: 29.0794

Epoch 619: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.5843 - MinusLogProbMetric: 28.5843 - val_loss: 29.0794 - val_MinusLogProbMetric: 29.0794 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 620/1000
2023-10-12 13:41:29.824 
Epoch 620/1000 
	 loss: 28.6349, MinusLogProbMetric: 28.6349, val_loss: 28.9576, val_MinusLogProbMetric: 28.9576

Epoch 620: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6349 - MinusLogProbMetric: 28.6349 - val_loss: 28.9576 - val_MinusLogProbMetric: 28.9576 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 621/1000
2023-10-12 13:42:03.315 
Epoch 621/1000 
	 loss: 28.5555, MinusLogProbMetric: 28.5555, val_loss: 29.4529, val_MinusLogProbMetric: 29.4529

Epoch 621: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.5555 - MinusLogProbMetric: 28.5555 - val_loss: 29.4529 - val_MinusLogProbMetric: 29.4529 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 622/1000
2023-10-12 13:42:36.745 
Epoch 622/1000 
	 loss: 28.5810, MinusLogProbMetric: 28.5810, val_loss: 28.9613, val_MinusLogProbMetric: 28.9613

Epoch 622: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.5810 - MinusLogProbMetric: 28.5810 - val_loss: 28.9613 - val_MinusLogProbMetric: 28.9613 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 623/1000
2023-10-12 13:43:10.124 
Epoch 623/1000 
	 loss: 28.6040, MinusLogProbMetric: 28.6040, val_loss: 29.1670, val_MinusLogProbMetric: 29.1670

Epoch 623: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.6040 - MinusLogProbMetric: 28.6040 - val_loss: 29.1670 - val_MinusLogProbMetric: 29.1670 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 624/1000
2023-10-12 13:43:43.881 
Epoch 624/1000 
	 loss: 28.6153, MinusLogProbMetric: 28.6153, val_loss: 29.0226, val_MinusLogProbMetric: 29.0226

Epoch 624: val_loss did not improve from 28.92239
196/196 - 34s - loss: 28.6153 - MinusLogProbMetric: 28.6153 - val_loss: 29.0226 - val_MinusLogProbMetric: 29.0226 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 625/1000
2023-10-12 13:44:17.368 
Epoch 625/1000 
	 loss: 28.5883, MinusLogProbMetric: 28.5883, val_loss: 29.2293, val_MinusLogProbMetric: 29.2293

Epoch 625: val_loss did not improve from 28.92239
196/196 - 33s - loss: 28.5883 - MinusLogProbMetric: 28.5883 - val_loss: 29.2293 - val_MinusLogProbMetric: 29.2293 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 626/1000
2023-10-12 13:44:51.020 
Epoch 626/1000 
	 loss: 28.5918, MinusLogProbMetric: 28.5918, val_loss: 28.9065, val_MinusLogProbMetric: 28.9065

Epoch 626: val_loss improved from 28.92239 to 28.90654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.5918 - MinusLogProbMetric: 28.5918 - val_loss: 28.9065 - val_MinusLogProbMetric: 28.9065 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 627/1000
2023-10-12 13:45:25.425 
Epoch 627/1000 
	 loss: 28.5609, MinusLogProbMetric: 28.5609, val_loss: 29.6942, val_MinusLogProbMetric: 29.6942

Epoch 627: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.5609 - MinusLogProbMetric: 28.5609 - val_loss: 29.6942 - val_MinusLogProbMetric: 29.6942 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 628/1000
2023-10-12 13:45:58.922 
Epoch 628/1000 
	 loss: 28.6021, MinusLogProbMetric: 28.6021, val_loss: 28.9877, val_MinusLogProbMetric: 28.9877

Epoch 628: val_loss did not improve from 28.90654
196/196 - 33s - loss: 28.6021 - MinusLogProbMetric: 28.6021 - val_loss: 28.9877 - val_MinusLogProbMetric: 28.9877 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 629/1000
2023-10-12 13:46:32.610 
Epoch 629/1000 
	 loss: 28.6333, MinusLogProbMetric: 28.6333, val_loss: 29.0572, val_MinusLogProbMetric: 29.0572

Epoch 629: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.6333 - MinusLogProbMetric: 28.6333 - val_loss: 29.0572 - val_MinusLogProbMetric: 29.0572 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 630/1000
2023-10-12 13:47:06.353 
Epoch 630/1000 
	 loss: 28.6416, MinusLogProbMetric: 28.6416, val_loss: 29.0769, val_MinusLogProbMetric: 29.0769

Epoch 630: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.6416 - MinusLogProbMetric: 28.6416 - val_loss: 29.0769 - val_MinusLogProbMetric: 29.0769 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 631/1000
2023-10-12 13:47:40.321 
Epoch 631/1000 
	 loss: 28.5807, MinusLogProbMetric: 28.5807, val_loss: 29.2992, val_MinusLogProbMetric: 29.2992

Epoch 631: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.5807 - MinusLogProbMetric: 28.5807 - val_loss: 29.2992 - val_MinusLogProbMetric: 29.2992 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 632/1000
2023-10-12 13:48:13.879 
Epoch 632/1000 
	 loss: 28.5486, MinusLogProbMetric: 28.5486, val_loss: 28.9389, val_MinusLogProbMetric: 28.9389

Epoch 632: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.5486 - MinusLogProbMetric: 28.5486 - val_loss: 28.9389 - val_MinusLogProbMetric: 28.9389 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 633/1000
2023-10-12 13:48:47.669 
Epoch 633/1000 
	 loss: 28.6110, MinusLogProbMetric: 28.6110, val_loss: 28.9682, val_MinusLogProbMetric: 28.9682

Epoch 633: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.6110 - MinusLogProbMetric: 28.6110 - val_loss: 28.9682 - val_MinusLogProbMetric: 28.9682 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 634/1000
2023-10-12 13:49:21.194 
Epoch 634/1000 
	 loss: 28.7065, MinusLogProbMetric: 28.7065, val_loss: 28.9932, val_MinusLogProbMetric: 28.9932

Epoch 634: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.7065 - MinusLogProbMetric: 28.7065 - val_loss: 28.9932 - val_MinusLogProbMetric: 28.9932 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 635/1000
2023-10-12 13:49:54.656 
Epoch 635/1000 
	 loss: 28.5539, MinusLogProbMetric: 28.5539, val_loss: 29.0828, val_MinusLogProbMetric: 29.0828

Epoch 635: val_loss did not improve from 28.90654
196/196 - 33s - loss: 28.5539 - MinusLogProbMetric: 28.5539 - val_loss: 29.0828 - val_MinusLogProbMetric: 29.0828 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 636/1000
2023-10-12 13:50:28.688 
Epoch 636/1000 
	 loss: 28.5918, MinusLogProbMetric: 28.5918, val_loss: 28.9103, val_MinusLogProbMetric: 28.9103

Epoch 636: val_loss did not improve from 28.90654
196/196 - 34s - loss: 28.5918 - MinusLogProbMetric: 28.5918 - val_loss: 28.9103 - val_MinusLogProbMetric: 28.9103 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 637/1000
2023-10-12 13:51:02.246 
Epoch 637/1000 
	 loss: 28.5316, MinusLogProbMetric: 28.5316, val_loss: 28.8786, val_MinusLogProbMetric: 28.8786

Epoch 637: val_loss improved from 28.90654 to 28.87855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.5316 - MinusLogProbMetric: 28.5316 - val_loss: 28.8786 - val_MinusLogProbMetric: 28.8786 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 638/1000
2023-10-12 13:51:36.448 
Epoch 638/1000 
	 loss: 28.6307, MinusLogProbMetric: 28.6307, val_loss: 28.9666, val_MinusLogProbMetric: 28.9666

Epoch 638: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.6307 - MinusLogProbMetric: 28.6307 - val_loss: 28.9666 - val_MinusLogProbMetric: 28.9666 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 639/1000
2023-10-12 13:52:10.078 
Epoch 639/1000 
	 loss: 28.5493, MinusLogProbMetric: 28.5493, val_loss: 29.1177, val_MinusLogProbMetric: 29.1177

Epoch 639: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5493 - MinusLogProbMetric: 28.5493 - val_loss: 29.1177 - val_MinusLogProbMetric: 29.1177 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 640/1000
2023-10-12 13:52:44.008 
Epoch 640/1000 
	 loss: 28.6045, MinusLogProbMetric: 28.6045, val_loss: 28.9394, val_MinusLogProbMetric: 28.9394

Epoch 640: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.6045 - MinusLogProbMetric: 28.6045 - val_loss: 28.9394 - val_MinusLogProbMetric: 28.9394 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 641/1000
2023-10-12 13:53:18.007 
Epoch 641/1000 
	 loss: 28.6126, MinusLogProbMetric: 28.6126, val_loss: 28.9224, val_MinusLogProbMetric: 28.9224

Epoch 641: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.6126 - MinusLogProbMetric: 28.6126 - val_loss: 28.9224 - val_MinusLogProbMetric: 28.9224 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 642/1000
2023-10-12 13:53:51.867 
Epoch 642/1000 
	 loss: 28.5601, MinusLogProbMetric: 28.5601, val_loss: 29.0879, val_MinusLogProbMetric: 29.0879

Epoch 642: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5601 - MinusLogProbMetric: 28.5601 - val_loss: 29.0879 - val_MinusLogProbMetric: 29.0879 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 643/1000
2023-10-12 13:54:25.260 
Epoch 643/1000 
	 loss: 28.5826, MinusLogProbMetric: 28.5826, val_loss: 28.9377, val_MinusLogProbMetric: 28.9377

Epoch 643: val_loss did not improve from 28.87855
196/196 - 33s - loss: 28.5826 - MinusLogProbMetric: 28.5826 - val_loss: 28.9377 - val_MinusLogProbMetric: 28.9377 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 644/1000
2023-10-12 13:54:58.872 
Epoch 644/1000 
	 loss: 28.5952, MinusLogProbMetric: 28.5952, val_loss: 28.9705, val_MinusLogProbMetric: 28.9705

Epoch 644: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5952 - MinusLogProbMetric: 28.5952 - val_loss: 28.9705 - val_MinusLogProbMetric: 28.9705 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 645/1000
2023-10-12 13:55:32.620 
Epoch 645/1000 
	 loss: 28.5756, MinusLogProbMetric: 28.5756, val_loss: 28.9843, val_MinusLogProbMetric: 28.9843

Epoch 645: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5756 - MinusLogProbMetric: 28.5756 - val_loss: 28.9843 - val_MinusLogProbMetric: 28.9843 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 646/1000
2023-10-12 13:56:06.271 
Epoch 646/1000 
	 loss: 28.5086, MinusLogProbMetric: 28.5086, val_loss: 29.2726, val_MinusLogProbMetric: 29.2726

Epoch 646: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5086 - MinusLogProbMetric: 28.5086 - val_loss: 29.2726 - val_MinusLogProbMetric: 29.2726 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 647/1000
2023-10-12 13:56:39.787 
Epoch 647/1000 
	 loss: 28.6260, MinusLogProbMetric: 28.6260, val_loss: 29.0403, val_MinusLogProbMetric: 29.0403

Epoch 647: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.6260 - MinusLogProbMetric: 28.6260 - val_loss: 29.0403 - val_MinusLogProbMetric: 29.0403 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 648/1000
2023-10-12 13:57:13.486 
Epoch 648/1000 
	 loss: 28.5316, MinusLogProbMetric: 28.5316, val_loss: 29.1551, val_MinusLogProbMetric: 29.1551

Epoch 648: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5316 - MinusLogProbMetric: 28.5316 - val_loss: 29.1551 - val_MinusLogProbMetric: 29.1551 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 649/1000
2023-10-12 13:57:47.301 
Epoch 649/1000 
	 loss: 28.5527, MinusLogProbMetric: 28.5527, val_loss: 28.9972, val_MinusLogProbMetric: 28.9972

Epoch 649: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.5527 - MinusLogProbMetric: 28.5527 - val_loss: 28.9972 - val_MinusLogProbMetric: 28.9972 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 650/1000
2023-10-12 13:58:21.032 
Epoch 650/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 29.1797, val_MinusLogProbMetric: 29.1797

Epoch 650: val_loss did not improve from 28.87855
196/196 - 34s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 29.1797 - val_MinusLogProbMetric: 29.1797 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 651/1000
2023-10-12 13:58:54.378 
Epoch 651/1000 
	 loss: 28.5937, MinusLogProbMetric: 28.5937, val_loss: 28.8483, val_MinusLogProbMetric: 28.8483

Epoch 651: val_loss improved from 28.87855 to 28.84826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.5937 - MinusLogProbMetric: 28.5937 - val_loss: 28.8483 - val_MinusLogProbMetric: 28.8483 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 652/1000
2023-10-12 13:59:28.530 
Epoch 652/1000 
	 loss: 28.5314, MinusLogProbMetric: 28.5314, val_loss: 28.9931, val_MinusLogProbMetric: 28.9931

Epoch 652: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5314 - MinusLogProbMetric: 28.5314 - val_loss: 28.9931 - val_MinusLogProbMetric: 28.9931 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 653/1000
2023-10-12 14:00:01.820 
Epoch 653/1000 
	 loss: 28.5884, MinusLogProbMetric: 28.5884, val_loss: 28.9750, val_MinusLogProbMetric: 28.9750

Epoch 653: val_loss did not improve from 28.84826
196/196 - 33s - loss: 28.5884 - MinusLogProbMetric: 28.5884 - val_loss: 28.9750 - val_MinusLogProbMetric: 28.9750 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 654/1000
2023-10-12 14:00:35.554 
Epoch 654/1000 
	 loss: 28.6659, MinusLogProbMetric: 28.6659, val_loss: 29.0549, val_MinusLogProbMetric: 29.0549

Epoch 654: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.6659 - MinusLogProbMetric: 28.6659 - val_loss: 29.0549 - val_MinusLogProbMetric: 29.0549 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 655/1000
2023-10-12 14:01:09.315 
Epoch 655/1000 
	 loss: 28.6613, MinusLogProbMetric: 28.6613, val_loss: 29.1030, val_MinusLogProbMetric: 29.1030

Epoch 655: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.6613 - MinusLogProbMetric: 28.6613 - val_loss: 29.1030 - val_MinusLogProbMetric: 29.1030 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 656/1000
2023-10-12 14:01:43.056 
Epoch 656/1000 
	 loss: 28.5621, MinusLogProbMetric: 28.5621, val_loss: 29.0808, val_MinusLogProbMetric: 29.0808

Epoch 656: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5621 - MinusLogProbMetric: 28.5621 - val_loss: 29.0808 - val_MinusLogProbMetric: 29.0808 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 657/1000
2023-10-12 14:02:16.531 
Epoch 657/1000 
	 loss: 28.5844, MinusLogProbMetric: 28.5844, val_loss: 28.8636, val_MinusLogProbMetric: 28.8636

Epoch 657: val_loss did not improve from 28.84826
196/196 - 33s - loss: 28.5844 - MinusLogProbMetric: 28.5844 - val_loss: 28.8636 - val_MinusLogProbMetric: 28.8636 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 658/1000
2023-10-12 14:02:50.234 
Epoch 658/1000 
	 loss: 28.5906, MinusLogProbMetric: 28.5906, val_loss: 29.1109, val_MinusLogProbMetric: 29.1109

Epoch 658: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5906 - MinusLogProbMetric: 28.5906 - val_loss: 29.1109 - val_MinusLogProbMetric: 29.1109 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 659/1000
2023-10-12 14:03:23.629 
Epoch 659/1000 
	 loss: 28.5149, MinusLogProbMetric: 28.5149, val_loss: 29.0260, val_MinusLogProbMetric: 29.0260

Epoch 659: val_loss did not improve from 28.84826
196/196 - 33s - loss: 28.5149 - MinusLogProbMetric: 28.5149 - val_loss: 29.0260 - val_MinusLogProbMetric: 29.0260 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 660/1000
2023-10-12 14:03:57.592 
Epoch 660/1000 
	 loss: 28.5594, MinusLogProbMetric: 28.5594, val_loss: 28.8894, val_MinusLogProbMetric: 28.8894

Epoch 660: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5594 - MinusLogProbMetric: 28.5594 - val_loss: 28.8894 - val_MinusLogProbMetric: 28.8894 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 661/1000
2023-10-12 14:04:31.296 
Epoch 661/1000 
	 loss: 28.5807, MinusLogProbMetric: 28.5807, val_loss: 29.0916, val_MinusLogProbMetric: 29.0916

Epoch 661: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5807 - MinusLogProbMetric: 28.5807 - val_loss: 29.0916 - val_MinusLogProbMetric: 29.0916 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 662/1000
2023-10-12 14:05:04.996 
Epoch 662/1000 
	 loss: 28.5870, MinusLogProbMetric: 28.5870, val_loss: 28.8542, val_MinusLogProbMetric: 28.8542

Epoch 662: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5870 - MinusLogProbMetric: 28.5870 - val_loss: 28.8542 - val_MinusLogProbMetric: 28.8542 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 663/1000
2023-10-12 14:05:38.670 
Epoch 663/1000 
	 loss: 28.5899, MinusLogProbMetric: 28.5899, val_loss: 28.9986, val_MinusLogProbMetric: 28.9986

Epoch 663: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5899 - MinusLogProbMetric: 28.5899 - val_loss: 28.9986 - val_MinusLogProbMetric: 28.9986 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 664/1000
2023-10-12 14:06:12.570 
Epoch 664/1000 
	 loss: 28.5633, MinusLogProbMetric: 28.5633, val_loss: 28.8613, val_MinusLogProbMetric: 28.8613

Epoch 664: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5633 - MinusLogProbMetric: 28.5633 - val_loss: 28.8613 - val_MinusLogProbMetric: 28.8613 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 665/1000
2023-10-12 14:06:46.234 
Epoch 665/1000 
	 loss: 28.5914, MinusLogProbMetric: 28.5914, val_loss: 28.9499, val_MinusLogProbMetric: 28.9499

Epoch 665: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5914 - MinusLogProbMetric: 28.5914 - val_loss: 28.9499 - val_MinusLogProbMetric: 28.9499 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 666/1000
2023-10-12 14:07:19.942 
Epoch 666/1000 
	 loss: 28.5674, MinusLogProbMetric: 28.5674, val_loss: 28.9434, val_MinusLogProbMetric: 28.9434

Epoch 666: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5674 - MinusLogProbMetric: 28.5674 - val_loss: 28.9434 - val_MinusLogProbMetric: 28.9434 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 667/1000
2023-10-12 14:07:53.787 
Epoch 667/1000 
	 loss: 28.5722, MinusLogProbMetric: 28.5722, val_loss: 29.0515, val_MinusLogProbMetric: 29.0515

Epoch 667: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5722 - MinusLogProbMetric: 28.5722 - val_loss: 29.0515 - val_MinusLogProbMetric: 29.0515 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 668/1000
2023-10-12 14:08:27.614 
Epoch 668/1000 
	 loss: 28.5594, MinusLogProbMetric: 28.5594, val_loss: 29.1356, val_MinusLogProbMetric: 29.1356

Epoch 668: val_loss did not improve from 28.84826
196/196 - 34s - loss: 28.5594 - MinusLogProbMetric: 28.5594 - val_loss: 29.1356 - val_MinusLogProbMetric: 29.1356 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 669/1000
2023-10-12 14:09:01.089 
Epoch 669/1000 
	 loss: 28.5523, MinusLogProbMetric: 28.5523, val_loss: 28.8436, val_MinusLogProbMetric: 28.8436

Epoch 669: val_loss improved from 28.84826 to 28.84357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.5523 - MinusLogProbMetric: 28.5523 - val_loss: 28.8436 - val_MinusLogProbMetric: 28.8436 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 670/1000
2023-10-12 14:09:35.249 
Epoch 670/1000 
	 loss: 28.5079, MinusLogProbMetric: 28.5079, val_loss: 28.9991, val_MinusLogProbMetric: 28.9991

Epoch 670: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5079 - MinusLogProbMetric: 28.5079 - val_loss: 28.9991 - val_MinusLogProbMetric: 28.9991 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 671/1000
2023-10-12 14:10:08.955 
Epoch 671/1000 
	 loss: 28.5595, MinusLogProbMetric: 28.5595, val_loss: 28.9998, val_MinusLogProbMetric: 28.9998

Epoch 671: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5595 - MinusLogProbMetric: 28.5595 - val_loss: 28.9998 - val_MinusLogProbMetric: 28.9998 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 672/1000
2023-10-12 14:10:42.376 
Epoch 672/1000 
	 loss: 28.6653, MinusLogProbMetric: 28.6653, val_loss: 28.9869, val_MinusLogProbMetric: 28.9869

Epoch 672: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.6653 - MinusLogProbMetric: 28.6653 - val_loss: 28.9869 - val_MinusLogProbMetric: 28.9869 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 673/1000
2023-10-12 14:11:15.676 
Epoch 673/1000 
	 loss: 28.5617, MinusLogProbMetric: 28.5617, val_loss: 29.7152, val_MinusLogProbMetric: 29.7152

Epoch 673: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5617 - MinusLogProbMetric: 28.5617 - val_loss: 29.7152 - val_MinusLogProbMetric: 29.7152 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 674/1000
2023-10-12 14:11:49.389 
Epoch 674/1000 
	 loss: 28.6739, MinusLogProbMetric: 28.6739, val_loss: 29.0486, val_MinusLogProbMetric: 29.0486

Epoch 674: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.6739 - MinusLogProbMetric: 28.6739 - val_loss: 29.0486 - val_MinusLogProbMetric: 29.0486 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 675/1000
2023-10-12 14:12:23.270 
Epoch 675/1000 
	 loss: 28.5280, MinusLogProbMetric: 28.5280, val_loss: 28.9113, val_MinusLogProbMetric: 28.9113

Epoch 675: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5280 - MinusLogProbMetric: 28.5280 - val_loss: 28.9113 - val_MinusLogProbMetric: 28.9113 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 676/1000
2023-10-12 14:12:57.271 
Epoch 676/1000 
	 loss: 28.6111, MinusLogProbMetric: 28.6111, val_loss: 29.0602, val_MinusLogProbMetric: 29.0602

Epoch 676: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.6111 - MinusLogProbMetric: 28.6111 - val_loss: 29.0602 - val_MinusLogProbMetric: 29.0602 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 677/1000
2023-10-12 14:13:30.940 
Epoch 677/1000 
	 loss: 28.5035, MinusLogProbMetric: 28.5035, val_loss: 28.9509, val_MinusLogProbMetric: 28.9509

Epoch 677: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5035 - MinusLogProbMetric: 28.5035 - val_loss: 28.9509 - val_MinusLogProbMetric: 28.9509 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 678/1000
2023-10-12 14:14:03.554 
Epoch 678/1000 
	 loss: 28.5123, MinusLogProbMetric: 28.5123, val_loss: 29.0093, val_MinusLogProbMetric: 29.0093

Epoch 678: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5123 - MinusLogProbMetric: 28.5123 - val_loss: 29.0093 - val_MinusLogProbMetric: 29.0093 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 679/1000
2023-10-12 14:14:35.463 
Epoch 679/1000 
	 loss: 28.5033, MinusLogProbMetric: 28.5033, val_loss: 28.8984, val_MinusLogProbMetric: 28.8984

Epoch 679: val_loss did not improve from 28.84357
196/196 - 32s - loss: 28.5033 - MinusLogProbMetric: 28.5033 - val_loss: 28.8984 - val_MinusLogProbMetric: 28.8984 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 680/1000
2023-10-12 14:15:08.027 
Epoch 680/1000 
	 loss: 28.5628, MinusLogProbMetric: 28.5628, val_loss: 29.0138, val_MinusLogProbMetric: 29.0138

Epoch 680: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5628 - MinusLogProbMetric: 28.5628 - val_loss: 29.0138 - val_MinusLogProbMetric: 29.0138 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 681/1000
2023-10-12 14:15:41.738 
Epoch 681/1000 
	 loss: 28.5733, MinusLogProbMetric: 28.5733, val_loss: 29.1416, val_MinusLogProbMetric: 29.1416

Epoch 681: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5733 - MinusLogProbMetric: 28.5733 - val_loss: 29.1416 - val_MinusLogProbMetric: 29.1416 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 682/1000
2023-10-12 14:16:15.098 
Epoch 682/1000 
	 loss: 28.5658, MinusLogProbMetric: 28.5658, val_loss: 28.8961, val_MinusLogProbMetric: 28.8961

Epoch 682: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5658 - MinusLogProbMetric: 28.5658 - val_loss: 28.8961 - val_MinusLogProbMetric: 28.8961 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 683/1000
2023-10-12 14:16:48.579 
Epoch 683/1000 
	 loss: 28.5223, MinusLogProbMetric: 28.5223, val_loss: 29.0960, val_MinusLogProbMetric: 29.0960

Epoch 683: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5223 - MinusLogProbMetric: 28.5223 - val_loss: 29.0960 - val_MinusLogProbMetric: 29.0960 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 684/1000
2023-10-12 14:17:22.433 
Epoch 684/1000 
	 loss: 28.5027, MinusLogProbMetric: 28.5027, val_loss: 28.8823, val_MinusLogProbMetric: 28.8823

Epoch 684: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5027 - MinusLogProbMetric: 28.5027 - val_loss: 28.8823 - val_MinusLogProbMetric: 28.8823 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 685/1000
2023-10-12 14:17:56.149 
Epoch 685/1000 
	 loss: 28.5406, MinusLogProbMetric: 28.5406, val_loss: 28.9441, val_MinusLogProbMetric: 28.9441

Epoch 685: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5406 - MinusLogProbMetric: 28.5406 - val_loss: 28.9441 - val_MinusLogProbMetric: 28.9441 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 686/1000
2023-10-12 14:18:29.665 
Epoch 686/1000 
	 loss: 28.4928, MinusLogProbMetric: 28.4928, val_loss: 29.0183, val_MinusLogProbMetric: 29.0183

Epoch 686: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4928 - MinusLogProbMetric: 28.4928 - val_loss: 29.0183 - val_MinusLogProbMetric: 29.0183 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 687/1000
2023-10-12 14:19:03.551 
Epoch 687/1000 
	 loss: 28.5394, MinusLogProbMetric: 28.5394, val_loss: 28.8723, val_MinusLogProbMetric: 28.8723

Epoch 687: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5394 - MinusLogProbMetric: 28.5394 - val_loss: 28.8723 - val_MinusLogProbMetric: 28.8723 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 688/1000
2023-10-12 14:19:37.308 
Epoch 688/1000 
	 loss: 28.5638, MinusLogProbMetric: 28.5638, val_loss: 28.9645, val_MinusLogProbMetric: 28.9645

Epoch 688: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5638 - MinusLogProbMetric: 28.5638 - val_loss: 28.9645 - val_MinusLogProbMetric: 28.9645 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 689/1000
2023-10-12 14:20:10.863 
Epoch 689/1000 
	 loss: 28.5396, MinusLogProbMetric: 28.5396, val_loss: 29.0972, val_MinusLogProbMetric: 29.0972

Epoch 689: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5396 - MinusLogProbMetric: 28.5396 - val_loss: 29.0972 - val_MinusLogProbMetric: 29.0972 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 690/1000
2023-10-12 14:20:44.551 
Epoch 690/1000 
	 loss: 28.5255, MinusLogProbMetric: 28.5255, val_loss: 28.9887, val_MinusLogProbMetric: 28.9887

Epoch 690: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5255 - MinusLogProbMetric: 28.5255 - val_loss: 28.9887 - val_MinusLogProbMetric: 28.9887 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 691/1000
2023-10-12 14:21:18.159 
Epoch 691/1000 
	 loss: 28.5177, MinusLogProbMetric: 28.5177, val_loss: 28.9375, val_MinusLogProbMetric: 28.9375

Epoch 691: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5177 - MinusLogProbMetric: 28.5177 - val_loss: 28.9375 - val_MinusLogProbMetric: 28.9375 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 692/1000
2023-10-12 14:21:51.737 
Epoch 692/1000 
	 loss: 28.4992, MinusLogProbMetric: 28.4992, val_loss: 28.8924, val_MinusLogProbMetric: 28.8924

Epoch 692: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4992 - MinusLogProbMetric: 28.4992 - val_loss: 28.8924 - val_MinusLogProbMetric: 28.8924 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 693/1000
2023-10-12 14:22:25.431 
Epoch 693/1000 
	 loss: 28.5725, MinusLogProbMetric: 28.5725, val_loss: 28.8929, val_MinusLogProbMetric: 28.8929

Epoch 693: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5725 - MinusLogProbMetric: 28.5725 - val_loss: 28.8929 - val_MinusLogProbMetric: 28.8929 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 694/1000
2023-10-12 14:22:59.364 
Epoch 694/1000 
	 loss: 28.4648, MinusLogProbMetric: 28.4648, val_loss: 28.9856, val_MinusLogProbMetric: 28.9856

Epoch 694: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4648 - MinusLogProbMetric: 28.4648 - val_loss: 28.9856 - val_MinusLogProbMetric: 28.9856 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 695/1000
2023-10-12 14:23:32.890 
Epoch 695/1000 
	 loss: 28.5416, MinusLogProbMetric: 28.5416, val_loss: 28.9499, val_MinusLogProbMetric: 28.9499

Epoch 695: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5416 - MinusLogProbMetric: 28.5416 - val_loss: 28.9499 - val_MinusLogProbMetric: 28.9499 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 696/1000
2023-10-12 14:24:06.734 
Epoch 696/1000 
	 loss: 28.5112, MinusLogProbMetric: 28.5112, val_loss: 28.8441, val_MinusLogProbMetric: 28.8441

Epoch 696: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5112 - MinusLogProbMetric: 28.5112 - val_loss: 28.8441 - val_MinusLogProbMetric: 28.8441 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 697/1000
2023-10-12 14:24:39.865 
Epoch 697/1000 
	 loss: 28.5153, MinusLogProbMetric: 28.5153, val_loss: 29.1611, val_MinusLogProbMetric: 29.1611

Epoch 697: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5153 - MinusLogProbMetric: 28.5153 - val_loss: 29.1611 - val_MinusLogProbMetric: 29.1611 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 698/1000
2023-10-12 14:25:13.748 
Epoch 698/1000 
	 loss: 28.5472, MinusLogProbMetric: 28.5472, val_loss: 28.9520, val_MinusLogProbMetric: 28.9520

Epoch 698: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5472 - MinusLogProbMetric: 28.5472 - val_loss: 28.9520 - val_MinusLogProbMetric: 28.9520 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 699/1000
2023-10-12 14:25:47.333 
Epoch 699/1000 
	 loss: 28.4743, MinusLogProbMetric: 28.4743, val_loss: 28.9988, val_MinusLogProbMetric: 28.9988

Epoch 699: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4743 - MinusLogProbMetric: 28.4743 - val_loss: 28.9988 - val_MinusLogProbMetric: 28.9988 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 700/1000
2023-10-12 14:26:21.141 
Epoch 700/1000 
	 loss: 28.6643, MinusLogProbMetric: 28.6643, val_loss: 29.0116, val_MinusLogProbMetric: 29.0116

Epoch 700: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.6643 - MinusLogProbMetric: 28.6643 - val_loss: 29.0116 - val_MinusLogProbMetric: 29.0116 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 701/1000
2023-10-12 14:26:55.283 
Epoch 701/1000 
	 loss: 28.5106, MinusLogProbMetric: 28.5106, val_loss: 28.8591, val_MinusLogProbMetric: 28.8591

Epoch 701: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5106 - MinusLogProbMetric: 28.5106 - val_loss: 28.8591 - val_MinusLogProbMetric: 28.8591 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 702/1000
2023-10-12 14:27:29.077 
Epoch 702/1000 
	 loss: 28.4547, MinusLogProbMetric: 28.4547, val_loss: 28.9538, val_MinusLogProbMetric: 28.9538

Epoch 702: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4547 - MinusLogProbMetric: 28.4547 - val_loss: 28.9538 - val_MinusLogProbMetric: 28.9538 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 703/1000
2023-10-12 14:28:02.855 
Epoch 703/1000 
	 loss: 28.4818, MinusLogProbMetric: 28.4818, val_loss: 29.0518, val_MinusLogProbMetric: 29.0518

Epoch 703: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4818 - MinusLogProbMetric: 28.4818 - val_loss: 29.0518 - val_MinusLogProbMetric: 29.0518 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 704/1000
2023-10-12 14:28:36.620 
Epoch 704/1000 
	 loss: 28.6378, MinusLogProbMetric: 28.6378, val_loss: 28.9569, val_MinusLogProbMetric: 28.9569

Epoch 704: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.6378 - MinusLogProbMetric: 28.6378 - val_loss: 28.9569 - val_MinusLogProbMetric: 28.9569 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 705/1000
2023-10-12 14:29:11.021 
Epoch 705/1000 
	 loss: 28.4654, MinusLogProbMetric: 28.4654, val_loss: 28.9707, val_MinusLogProbMetric: 28.9707

Epoch 705: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4654 - MinusLogProbMetric: 28.4654 - val_loss: 28.9707 - val_MinusLogProbMetric: 28.9707 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 706/1000
2023-10-12 14:29:44.352 
Epoch 706/1000 
	 loss: 28.4954, MinusLogProbMetric: 28.4954, val_loss: 28.9998, val_MinusLogProbMetric: 28.9998

Epoch 706: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.4954 - MinusLogProbMetric: 28.4954 - val_loss: 28.9998 - val_MinusLogProbMetric: 28.9998 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 707/1000
2023-10-12 14:30:18.001 
Epoch 707/1000 
	 loss: 28.5420, MinusLogProbMetric: 28.5420, val_loss: 29.0141, val_MinusLogProbMetric: 29.0141

Epoch 707: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5420 - MinusLogProbMetric: 28.5420 - val_loss: 29.0141 - val_MinusLogProbMetric: 29.0141 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 708/1000
2023-10-12 14:30:51.798 
Epoch 708/1000 
	 loss: 28.5676, MinusLogProbMetric: 28.5676, val_loss: 28.9044, val_MinusLogProbMetric: 28.9044

Epoch 708: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5676 - MinusLogProbMetric: 28.5676 - val_loss: 28.9044 - val_MinusLogProbMetric: 28.9044 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 709/1000
2023-10-12 14:31:25.020 
Epoch 709/1000 
	 loss: 28.5042, MinusLogProbMetric: 28.5042, val_loss: 28.9036, val_MinusLogProbMetric: 28.9036

Epoch 709: val_loss did not improve from 28.84357
196/196 - 33s - loss: 28.5042 - MinusLogProbMetric: 28.5042 - val_loss: 28.9036 - val_MinusLogProbMetric: 28.9036 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 710/1000
2023-10-12 14:31:58.663 
Epoch 710/1000 
	 loss: 28.5177, MinusLogProbMetric: 28.5177, val_loss: 28.9630, val_MinusLogProbMetric: 28.9630

Epoch 710: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5177 - MinusLogProbMetric: 28.5177 - val_loss: 28.9630 - val_MinusLogProbMetric: 28.9630 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 711/1000
2023-10-12 14:32:32.351 
Epoch 711/1000 
	 loss: 28.4979, MinusLogProbMetric: 28.4979, val_loss: 29.0365, val_MinusLogProbMetric: 29.0365

Epoch 711: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4979 - MinusLogProbMetric: 28.4979 - val_loss: 29.0365 - val_MinusLogProbMetric: 29.0365 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 712/1000
2023-10-12 14:33:05.954 
Epoch 712/1000 
	 loss: 28.5018, MinusLogProbMetric: 28.5018, val_loss: 29.0680, val_MinusLogProbMetric: 29.0680

Epoch 712: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5018 - MinusLogProbMetric: 28.5018 - val_loss: 29.0680 - val_MinusLogProbMetric: 29.0680 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 713/1000
2023-10-12 14:33:39.939 
Epoch 713/1000 
	 loss: 28.4732, MinusLogProbMetric: 28.4732, val_loss: 29.3077, val_MinusLogProbMetric: 29.3077

Epoch 713: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4732 - MinusLogProbMetric: 28.4732 - val_loss: 29.3077 - val_MinusLogProbMetric: 29.3077 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 714/1000
2023-10-12 14:34:13.625 
Epoch 714/1000 
	 loss: 28.5642, MinusLogProbMetric: 28.5642, val_loss: 28.9062, val_MinusLogProbMetric: 28.9062

Epoch 714: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5642 - MinusLogProbMetric: 28.5642 - val_loss: 28.9062 - val_MinusLogProbMetric: 28.9062 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 715/1000
2023-10-12 14:34:47.267 
Epoch 715/1000 
	 loss: 28.4680, MinusLogProbMetric: 28.4680, val_loss: 28.8987, val_MinusLogProbMetric: 28.8987

Epoch 715: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4680 - MinusLogProbMetric: 28.4680 - val_loss: 28.8987 - val_MinusLogProbMetric: 28.8987 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 716/1000
2023-10-12 14:35:21.248 
Epoch 716/1000 
	 loss: 28.5008, MinusLogProbMetric: 28.5008, val_loss: 29.2129, val_MinusLogProbMetric: 29.2129

Epoch 716: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5008 - MinusLogProbMetric: 28.5008 - val_loss: 29.2129 - val_MinusLogProbMetric: 29.2129 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 717/1000
2023-10-12 14:35:54.961 
Epoch 717/1000 
	 loss: 28.5219, MinusLogProbMetric: 28.5219, val_loss: 29.0017, val_MinusLogProbMetric: 29.0017

Epoch 717: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.5219 - MinusLogProbMetric: 28.5219 - val_loss: 29.0017 - val_MinusLogProbMetric: 29.0017 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 718/1000
2023-10-12 14:36:28.886 
Epoch 718/1000 
	 loss: 28.4744, MinusLogProbMetric: 28.4744, val_loss: 28.9629, val_MinusLogProbMetric: 28.9629

Epoch 718: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4744 - MinusLogProbMetric: 28.4744 - val_loss: 28.9629 - val_MinusLogProbMetric: 28.9629 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 719/1000
2023-10-12 14:37:02.800 
Epoch 719/1000 
	 loss: 28.4907, MinusLogProbMetric: 28.4907, val_loss: 28.9732, val_MinusLogProbMetric: 28.9732

Epoch 719: val_loss did not improve from 28.84357
196/196 - 34s - loss: 28.4907 - MinusLogProbMetric: 28.4907 - val_loss: 28.9732 - val_MinusLogProbMetric: 28.9732 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 720/1000
2023-10-12 14:37:36.174 
Epoch 720/1000 
	 loss: 28.3052, MinusLogProbMetric: 28.3052, val_loss: 28.7968, val_MinusLogProbMetric: 28.7968

Epoch 720: val_loss improved from 28.84357 to 28.79679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.3052 - MinusLogProbMetric: 28.3052 - val_loss: 28.7968 - val_MinusLogProbMetric: 28.7968 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 721/1000
2023-10-12 14:38:10.624 
Epoch 721/1000 
	 loss: 28.3118, MinusLogProbMetric: 28.3118, val_loss: 28.7229, val_MinusLogProbMetric: 28.7229

Epoch 721: val_loss improved from 28.79679 to 28.72285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.3118 - MinusLogProbMetric: 28.3118 - val_loss: 28.7229 - val_MinusLogProbMetric: 28.7229 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 722/1000
2023-10-12 14:38:44.941 
Epoch 722/1000 
	 loss: 28.3030, MinusLogProbMetric: 28.3030, val_loss: 28.7557, val_MinusLogProbMetric: 28.7557

Epoch 722: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3030 - MinusLogProbMetric: 28.3030 - val_loss: 28.7557 - val_MinusLogProbMetric: 28.7557 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 723/1000
2023-10-12 14:39:18.651 
Epoch 723/1000 
	 loss: 28.3025, MinusLogProbMetric: 28.3025, val_loss: 28.7845, val_MinusLogProbMetric: 28.7845

Epoch 723: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3025 - MinusLogProbMetric: 28.3025 - val_loss: 28.7845 - val_MinusLogProbMetric: 28.7845 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 724/1000
2023-10-12 14:39:52.289 
Epoch 724/1000 
	 loss: 28.3098, MinusLogProbMetric: 28.3098, val_loss: 28.8368, val_MinusLogProbMetric: 28.8368

Epoch 724: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3098 - MinusLogProbMetric: 28.3098 - val_loss: 28.8368 - val_MinusLogProbMetric: 28.8368 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 725/1000
2023-10-12 14:40:25.666 
Epoch 725/1000 
	 loss: 28.3113, MinusLogProbMetric: 28.3113, val_loss: 28.7508, val_MinusLogProbMetric: 28.7508

Epoch 725: val_loss did not improve from 28.72285
196/196 - 33s - loss: 28.3113 - MinusLogProbMetric: 28.3113 - val_loss: 28.7508 - val_MinusLogProbMetric: 28.7508 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 726/1000
2023-10-12 14:40:59.733 
Epoch 726/1000 
	 loss: 28.3090, MinusLogProbMetric: 28.3090, val_loss: 28.7275, val_MinusLogProbMetric: 28.7275

Epoch 726: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3090 - MinusLogProbMetric: 28.3090 - val_loss: 28.7275 - val_MinusLogProbMetric: 28.7275 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 727/1000
2023-10-12 14:41:33.550 
Epoch 727/1000 
	 loss: 28.3095, MinusLogProbMetric: 28.3095, val_loss: 29.0192, val_MinusLogProbMetric: 29.0192

Epoch 727: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3095 - MinusLogProbMetric: 28.3095 - val_loss: 29.0192 - val_MinusLogProbMetric: 29.0192 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 728/1000
2023-10-12 14:42:07.146 
Epoch 728/1000 
	 loss: 28.3168, MinusLogProbMetric: 28.3168, val_loss: 28.7970, val_MinusLogProbMetric: 28.7970

Epoch 728: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3168 - MinusLogProbMetric: 28.3168 - val_loss: 28.7970 - val_MinusLogProbMetric: 28.7970 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 729/1000
2023-10-12 14:42:41.221 
Epoch 729/1000 
	 loss: 28.3021, MinusLogProbMetric: 28.3021, val_loss: 28.7309, val_MinusLogProbMetric: 28.7309

Epoch 729: val_loss did not improve from 28.72285
196/196 - 34s - loss: 28.3021 - MinusLogProbMetric: 28.3021 - val_loss: 28.7309 - val_MinusLogProbMetric: 28.7309 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 730/1000
2023-10-12 14:43:14.622 
Epoch 730/1000 
	 loss: 28.3078, MinusLogProbMetric: 28.3078, val_loss: 28.7053, val_MinusLogProbMetric: 28.7053

Epoch 730: val_loss improved from 28.72285 to 28.70526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.3078 - MinusLogProbMetric: 28.3078 - val_loss: 28.7053 - val_MinusLogProbMetric: 28.7053 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 731/1000
2023-10-12 14:43:48.338 
Epoch 731/1000 
	 loss: 28.3016, MinusLogProbMetric: 28.3016, val_loss: 28.7425, val_MinusLogProbMetric: 28.7425

Epoch 731: val_loss did not improve from 28.70526
196/196 - 33s - loss: 28.3016 - MinusLogProbMetric: 28.3016 - val_loss: 28.7425 - val_MinusLogProbMetric: 28.7425 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 732/1000
2023-10-12 14:44:22.226 
Epoch 732/1000 
	 loss: 28.2993, MinusLogProbMetric: 28.2993, val_loss: 28.8153, val_MinusLogProbMetric: 28.8153

Epoch 732: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2993 - MinusLogProbMetric: 28.2993 - val_loss: 28.8153 - val_MinusLogProbMetric: 28.8153 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 733/1000
2023-10-12 14:44:56.282 
Epoch 733/1000 
	 loss: 28.3046, MinusLogProbMetric: 28.3046, val_loss: 28.7839, val_MinusLogProbMetric: 28.7839

Epoch 733: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3046 - MinusLogProbMetric: 28.3046 - val_loss: 28.7839 - val_MinusLogProbMetric: 28.7839 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 734/1000
2023-10-12 14:45:29.941 
Epoch 734/1000 
	 loss: 28.3049, MinusLogProbMetric: 28.3049, val_loss: 28.7285, val_MinusLogProbMetric: 28.7285

Epoch 734: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3049 - MinusLogProbMetric: 28.3049 - val_loss: 28.7285 - val_MinusLogProbMetric: 28.7285 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 735/1000
2023-10-12 14:46:03.441 
Epoch 735/1000 
	 loss: 28.3197, MinusLogProbMetric: 28.3197, val_loss: 28.7907, val_MinusLogProbMetric: 28.7907

Epoch 735: val_loss did not improve from 28.70526
196/196 - 33s - loss: 28.3197 - MinusLogProbMetric: 28.3197 - val_loss: 28.7907 - val_MinusLogProbMetric: 28.7907 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 736/1000
2023-10-12 14:46:37.427 
Epoch 736/1000 
	 loss: 28.3044, MinusLogProbMetric: 28.3044, val_loss: 28.7197, val_MinusLogProbMetric: 28.7197

Epoch 736: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3044 - MinusLogProbMetric: 28.3044 - val_loss: 28.7197 - val_MinusLogProbMetric: 28.7197 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 737/1000
2023-10-12 14:47:11.283 
Epoch 737/1000 
	 loss: 28.3143, MinusLogProbMetric: 28.3143, val_loss: 28.8323, val_MinusLogProbMetric: 28.8323

Epoch 737: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3143 - MinusLogProbMetric: 28.3143 - val_loss: 28.8323 - val_MinusLogProbMetric: 28.8323 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 738/1000
2023-10-12 14:47:45.199 
Epoch 738/1000 
	 loss: 28.3042, MinusLogProbMetric: 28.3042, val_loss: 28.8283, val_MinusLogProbMetric: 28.8283

Epoch 738: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3042 - MinusLogProbMetric: 28.3042 - val_loss: 28.8283 - val_MinusLogProbMetric: 28.8283 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 739/1000
2023-10-12 14:48:19.079 
Epoch 739/1000 
	 loss: 28.3086, MinusLogProbMetric: 28.3086, val_loss: 28.7411, val_MinusLogProbMetric: 28.7411

Epoch 739: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3086 - MinusLogProbMetric: 28.3086 - val_loss: 28.7411 - val_MinusLogProbMetric: 28.7411 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 740/1000
2023-10-12 14:48:52.977 
Epoch 740/1000 
	 loss: 28.2947, MinusLogProbMetric: 28.2947, val_loss: 28.7616, val_MinusLogProbMetric: 28.7616

Epoch 740: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2947 - MinusLogProbMetric: 28.2947 - val_loss: 28.7616 - val_MinusLogProbMetric: 28.7616 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 741/1000
2023-10-12 14:49:26.198 
Epoch 741/1000 
	 loss: 28.3060, MinusLogProbMetric: 28.3060, val_loss: 28.7236, val_MinusLogProbMetric: 28.7236

Epoch 741: val_loss did not improve from 28.70526
196/196 - 33s - loss: 28.3060 - MinusLogProbMetric: 28.3060 - val_loss: 28.7236 - val_MinusLogProbMetric: 28.7236 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 742/1000
2023-10-12 14:49:59.963 
Epoch 742/1000 
	 loss: 28.2822, MinusLogProbMetric: 28.2822, val_loss: 28.7520, val_MinusLogProbMetric: 28.7520

Epoch 742: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2822 - MinusLogProbMetric: 28.2822 - val_loss: 28.7520 - val_MinusLogProbMetric: 28.7520 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 743/1000
2023-10-12 14:50:33.665 
Epoch 743/1000 
	 loss: 28.2842, MinusLogProbMetric: 28.2842, val_loss: 28.8471, val_MinusLogProbMetric: 28.8471

Epoch 743: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2842 - MinusLogProbMetric: 28.2842 - val_loss: 28.8471 - val_MinusLogProbMetric: 28.8471 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 744/1000
2023-10-12 14:51:07.705 
Epoch 744/1000 
	 loss: 28.2955, MinusLogProbMetric: 28.2955, val_loss: 28.7256, val_MinusLogProbMetric: 28.7256

Epoch 744: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2955 - MinusLogProbMetric: 28.2955 - val_loss: 28.7256 - val_MinusLogProbMetric: 28.7256 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 745/1000
2023-10-12 14:51:41.445 
Epoch 745/1000 
	 loss: 28.2871, MinusLogProbMetric: 28.2871, val_loss: 28.7990, val_MinusLogProbMetric: 28.7990

Epoch 745: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2871 - MinusLogProbMetric: 28.2871 - val_loss: 28.7990 - val_MinusLogProbMetric: 28.7990 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 746/1000
2023-10-12 14:52:15.394 
Epoch 746/1000 
	 loss: 28.2792, MinusLogProbMetric: 28.2792, val_loss: 28.7632, val_MinusLogProbMetric: 28.7632

Epoch 746: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2792 - MinusLogProbMetric: 28.2792 - val_loss: 28.7632 - val_MinusLogProbMetric: 28.7632 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 747/1000
2023-10-12 14:52:49.237 
Epoch 747/1000 
	 loss: 28.2940, MinusLogProbMetric: 28.2940, val_loss: 28.7205, val_MinusLogProbMetric: 28.7205

Epoch 747: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2940 - MinusLogProbMetric: 28.2940 - val_loss: 28.7205 - val_MinusLogProbMetric: 28.7205 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 748/1000
2023-10-12 14:53:23.130 
Epoch 748/1000 
	 loss: 28.3137, MinusLogProbMetric: 28.3137, val_loss: 28.7556, val_MinusLogProbMetric: 28.7556

Epoch 748: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3137 - MinusLogProbMetric: 28.3137 - val_loss: 28.7556 - val_MinusLogProbMetric: 28.7556 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 749/1000
2023-10-12 14:53:56.693 
Epoch 749/1000 
	 loss: 28.3035, MinusLogProbMetric: 28.3035, val_loss: 28.7297, val_MinusLogProbMetric: 28.7297

Epoch 749: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.3035 - MinusLogProbMetric: 28.3035 - val_loss: 28.7297 - val_MinusLogProbMetric: 28.7297 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 750/1000
2023-10-12 14:54:30.542 
Epoch 750/1000 
	 loss: 28.2997, MinusLogProbMetric: 28.2997, val_loss: 28.7408, val_MinusLogProbMetric: 28.7408

Epoch 750: val_loss did not improve from 28.70526
196/196 - 34s - loss: 28.2997 - MinusLogProbMetric: 28.2997 - val_loss: 28.7408 - val_MinusLogProbMetric: 28.7408 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 751/1000
2023-10-12 14:55:04.186 
Epoch 751/1000 
	 loss: 28.2979, MinusLogProbMetric: 28.2979, val_loss: 28.7051, val_MinusLogProbMetric: 28.7051

Epoch 751: val_loss improved from 28.70526 to 28.70511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.2979 - MinusLogProbMetric: 28.2979 - val_loss: 28.7051 - val_MinusLogProbMetric: 28.7051 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 752/1000
2023-10-12 14:55:38.694 
Epoch 752/1000 
	 loss: 28.3186, MinusLogProbMetric: 28.3186, val_loss: 28.8072, val_MinusLogProbMetric: 28.8072

Epoch 752: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.3186 - MinusLogProbMetric: 28.3186 - val_loss: 28.8072 - val_MinusLogProbMetric: 28.8072 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 753/1000
2023-10-12 14:56:12.518 
Epoch 753/1000 
	 loss: 28.2987, MinusLogProbMetric: 28.2987, val_loss: 28.7472, val_MinusLogProbMetric: 28.7472

Epoch 753: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.2987 - MinusLogProbMetric: 28.2987 - val_loss: 28.7472 - val_MinusLogProbMetric: 28.7472 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 754/1000
2023-10-12 14:56:46.058 
Epoch 754/1000 
	 loss: 28.2967, MinusLogProbMetric: 28.2967, val_loss: 28.7759, val_MinusLogProbMetric: 28.7759

Epoch 754: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.2967 - MinusLogProbMetric: 28.2967 - val_loss: 28.7759 - val_MinusLogProbMetric: 28.7759 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 755/1000
2023-10-12 14:57:19.418 
Epoch 755/1000 
	 loss: 28.2959, MinusLogProbMetric: 28.2959, val_loss: 28.7218, val_MinusLogProbMetric: 28.7218

Epoch 755: val_loss did not improve from 28.70511
196/196 - 33s - loss: 28.2959 - MinusLogProbMetric: 28.2959 - val_loss: 28.7218 - val_MinusLogProbMetric: 28.7218 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 756/1000
2023-10-12 14:57:53.554 
Epoch 756/1000 
	 loss: 28.2891, MinusLogProbMetric: 28.2891, val_loss: 28.9034, val_MinusLogProbMetric: 28.9034

Epoch 756: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.2891 - MinusLogProbMetric: 28.2891 - val_loss: 28.9034 - val_MinusLogProbMetric: 28.9034 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 757/1000
2023-10-12 14:58:27.082 
Epoch 757/1000 
	 loss: 28.2908, MinusLogProbMetric: 28.2908, val_loss: 28.7287, val_MinusLogProbMetric: 28.7287

Epoch 757: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.2908 - MinusLogProbMetric: 28.2908 - val_loss: 28.7287 - val_MinusLogProbMetric: 28.7287 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 758/1000
2023-10-12 14:59:00.938 
Epoch 758/1000 
	 loss: 28.2759, MinusLogProbMetric: 28.2759, val_loss: 28.7260, val_MinusLogProbMetric: 28.7260

Epoch 758: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.2759 - MinusLogProbMetric: 28.2759 - val_loss: 28.7260 - val_MinusLogProbMetric: 28.7260 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 759/1000
2023-10-12 14:59:34.674 
Epoch 759/1000 
	 loss: 28.3061, MinusLogProbMetric: 28.3061, val_loss: 28.7201, val_MinusLogProbMetric: 28.7201

Epoch 759: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.3061 - MinusLogProbMetric: 28.3061 - val_loss: 28.7201 - val_MinusLogProbMetric: 28.7201 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 760/1000
2023-10-12 15:00:08.530 
Epoch 760/1000 
	 loss: 28.2931, MinusLogProbMetric: 28.2931, val_loss: 28.7344, val_MinusLogProbMetric: 28.7344

Epoch 760: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.2931 - MinusLogProbMetric: 28.2931 - val_loss: 28.7344 - val_MinusLogProbMetric: 28.7344 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 761/1000
2023-10-12 15:00:42.074 
Epoch 761/1000 
	 loss: 28.3055, MinusLogProbMetric: 28.3055, val_loss: 28.7647, val_MinusLogProbMetric: 28.7647

Epoch 761: val_loss did not improve from 28.70511
196/196 - 34s - loss: 28.3055 - MinusLogProbMetric: 28.3055 - val_loss: 28.7647 - val_MinusLogProbMetric: 28.7647 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 762/1000
2023-10-12 15:01:15.594 
Epoch 762/1000 
	 loss: 28.3091, MinusLogProbMetric: 28.3091, val_loss: 28.6720, val_MinusLogProbMetric: 28.6720

Epoch 762: val_loss improved from 28.70511 to 28.67201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.3091 - MinusLogProbMetric: 28.3091 - val_loss: 28.6720 - val_MinusLogProbMetric: 28.6720 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 763/1000
2023-10-12 15:01:50.132 
Epoch 763/1000 
	 loss: 28.3026, MinusLogProbMetric: 28.3026, val_loss: 28.7101, val_MinusLogProbMetric: 28.7101

Epoch 763: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.3026 - MinusLogProbMetric: 28.3026 - val_loss: 28.7101 - val_MinusLogProbMetric: 28.7101 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 764/1000
2023-10-12 15:02:23.757 
Epoch 764/1000 
	 loss: 28.2813, MinusLogProbMetric: 28.2813, val_loss: 28.7476, val_MinusLogProbMetric: 28.7476

Epoch 764: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2813 - MinusLogProbMetric: 28.2813 - val_loss: 28.7476 - val_MinusLogProbMetric: 28.7476 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 765/1000
2023-10-12 15:02:57.456 
Epoch 765/1000 
	 loss: 28.2800, MinusLogProbMetric: 28.2800, val_loss: 28.6989, val_MinusLogProbMetric: 28.6989

Epoch 765: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2800 - MinusLogProbMetric: 28.2800 - val_loss: 28.6989 - val_MinusLogProbMetric: 28.6989 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 766/1000
2023-10-12 15:03:31.215 
Epoch 766/1000 
	 loss: 28.3034, MinusLogProbMetric: 28.3034, val_loss: 28.7531, val_MinusLogProbMetric: 28.7531

Epoch 766: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.3034 - MinusLogProbMetric: 28.3034 - val_loss: 28.7531 - val_MinusLogProbMetric: 28.7531 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 767/1000
2023-10-12 15:04:04.988 
Epoch 767/1000 
	 loss: 28.2910, MinusLogProbMetric: 28.2910, val_loss: 28.7546, val_MinusLogProbMetric: 28.7546

Epoch 767: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2910 - MinusLogProbMetric: 28.2910 - val_loss: 28.7546 - val_MinusLogProbMetric: 28.7546 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 768/1000
2023-10-12 15:04:38.618 
Epoch 768/1000 
	 loss: 28.2780, MinusLogProbMetric: 28.2780, val_loss: 28.7875, val_MinusLogProbMetric: 28.7875

Epoch 768: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2780 - MinusLogProbMetric: 28.2780 - val_loss: 28.7875 - val_MinusLogProbMetric: 28.7875 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 769/1000
2023-10-12 15:05:12.321 
Epoch 769/1000 
	 loss: 28.2825, MinusLogProbMetric: 28.2825, val_loss: 28.7139, val_MinusLogProbMetric: 28.7139

Epoch 769: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2825 - MinusLogProbMetric: 28.2825 - val_loss: 28.7139 - val_MinusLogProbMetric: 28.7139 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 770/1000
2023-10-12 15:05:45.729 
Epoch 770/1000 
	 loss: 28.2964, MinusLogProbMetric: 28.2964, val_loss: 28.8407, val_MinusLogProbMetric: 28.8407

Epoch 770: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2964 - MinusLogProbMetric: 28.2964 - val_loss: 28.8407 - val_MinusLogProbMetric: 28.8407 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 771/1000
2023-10-12 15:06:18.850 
Epoch 771/1000 
	 loss: 28.2829, MinusLogProbMetric: 28.2829, val_loss: 28.7263, val_MinusLogProbMetric: 28.7263

Epoch 771: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2829 - MinusLogProbMetric: 28.2829 - val_loss: 28.7263 - val_MinusLogProbMetric: 28.7263 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 772/1000
2023-10-12 15:06:52.611 
Epoch 772/1000 
	 loss: 28.2817, MinusLogProbMetric: 28.2817, val_loss: 28.8009, val_MinusLogProbMetric: 28.8009

Epoch 772: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2817 - MinusLogProbMetric: 28.2817 - val_loss: 28.8009 - val_MinusLogProbMetric: 28.8009 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 773/1000
2023-10-12 15:07:26.282 
Epoch 773/1000 
	 loss: 28.2963, MinusLogProbMetric: 28.2963, val_loss: 28.7371, val_MinusLogProbMetric: 28.7371

Epoch 773: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2963 - MinusLogProbMetric: 28.2963 - val_loss: 28.7371 - val_MinusLogProbMetric: 28.7371 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 774/1000
2023-10-12 15:07:59.979 
Epoch 774/1000 
	 loss: 28.2822, MinusLogProbMetric: 28.2822, val_loss: 28.7240, val_MinusLogProbMetric: 28.7240

Epoch 774: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2822 - MinusLogProbMetric: 28.2822 - val_loss: 28.7240 - val_MinusLogProbMetric: 28.7240 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 775/1000
2023-10-12 15:08:33.503 
Epoch 775/1000 
	 loss: 28.2838, MinusLogProbMetric: 28.2838, val_loss: 28.7518, val_MinusLogProbMetric: 28.7518

Epoch 775: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2838 - MinusLogProbMetric: 28.2838 - val_loss: 28.7518 - val_MinusLogProbMetric: 28.7518 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 776/1000
2023-10-12 15:09:06.900 
Epoch 776/1000 
	 loss: 28.2778, MinusLogProbMetric: 28.2778, val_loss: 28.7277, val_MinusLogProbMetric: 28.7277

Epoch 776: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2778 - MinusLogProbMetric: 28.2778 - val_loss: 28.7277 - val_MinusLogProbMetric: 28.7277 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 777/1000
2023-10-12 15:09:40.316 
Epoch 777/1000 
	 loss: 28.2832, MinusLogProbMetric: 28.2832, val_loss: 28.7306, val_MinusLogProbMetric: 28.7306

Epoch 777: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2832 - MinusLogProbMetric: 28.2832 - val_loss: 28.7306 - val_MinusLogProbMetric: 28.7306 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 778/1000
2023-10-12 15:10:13.686 
Epoch 778/1000 
	 loss: 28.2867, MinusLogProbMetric: 28.2867, val_loss: 28.7514, val_MinusLogProbMetric: 28.7514

Epoch 778: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2867 - MinusLogProbMetric: 28.2867 - val_loss: 28.7514 - val_MinusLogProbMetric: 28.7514 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 779/1000
2023-10-12 15:10:47.549 
Epoch 779/1000 
	 loss: 28.2677, MinusLogProbMetric: 28.2677, val_loss: 28.7777, val_MinusLogProbMetric: 28.7777

Epoch 779: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2677 - MinusLogProbMetric: 28.2677 - val_loss: 28.7777 - val_MinusLogProbMetric: 28.7777 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 780/1000
2023-10-12 15:11:21.213 
Epoch 780/1000 
	 loss: 28.2832, MinusLogProbMetric: 28.2832, val_loss: 28.9373, val_MinusLogProbMetric: 28.9373

Epoch 780: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2832 - MinusLogProbMetric: 28.2832 - val_loss: 28.9373 - val_MinusLogProbMetric: 28.9373 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 781/1000
2023-10-12 15:11:54.406 
Epoch 781/1000 
	 loss: 28.2873, MinusLogProbMetric: 28.2873, val_loss: 28.6925, val_MinusLogProbMetric: 28.6925

Epoch 781: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2873 - MinusLogProbMetric: 28.2873 - val_loss: 28.6925 - val_MinusLogProbMetric: 28.6925 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 782/1000
2023-10-12 15:12:28.140 
Epoch 782/1000 
	 loss: 28.2766, MinusLogProbMetric: 28.2766, val_loss: 28.7984, val_MinusLogProbMetric: 28.7984

Epoch 782: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2766 - MinusLogProbMetric: 28.2766 - val_loss: 28.7984 - val_MinusLogProbMetric: 28.7984 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 783/1000
2023-10-12 15:13:00.958 
Epoch 783/1000 
	 loss: 28.2993, MinusLogProbMetric: 28.2993, val_loss: 28.7803, val_MinusLogProbMetric: 28.7803

Epoch 783: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2993 - MinusLogProbMetric: 28.2993 - val_loss: 28.7803 - val_MinusLogProbMetric: 28.7803 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 784/1000
2023-10-12 15:13:32.696 
Epoch 784/1000 
	 loss: 28.2730, MinusLogProbMetric: 28.2730, val_loss: 28.8564, val_MinusLogProbMetric: 28.8564

Epoch 784: val_loss did not improve from 28.67201
196/196 - 32s - loss: 28.2730 - MinusLogProbMetric: 28.2730 - val_loss: 28.8564 - val_MinusLogProbMetric: 28.8564 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 785/1000
2023-10-12 15:14:04.302 
Epoch 785/1000 
	 loss: 28.2863, MinusLogProbMetric: 28.2863, val_loss: 28.6922, val_MinusLogProbMetric: 28.6922

Epoch 785: val_loss did not improve from 28.67201
196/196 - 32s - loss: 28.2863 - MinusLogProbMetric: 28.2863 - val_loss: 28.6922 - val_MinusLogProbMetric: 28.6922 - lr: 8.3333e-05 - 32s/epoch - 161ms/step
Epoch 786/1000
2023-10-12 15:14:37.617 
Epoch 786/1000 
	 loss: 28.2891, MinusLogProbMetric: 28.2891, val_loss: 28.7145, val_MinusLogProbMetric: 28.7145

Epoch 786: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2891 - MinusLogProbMetric: 28.2891 - val_loss: 28.7145 - val_MinusLogProbMetric: 28.7145 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 787/1000
2023-10-12 15:15:10.923 
Epoch 787/1000 
	 loss: 28.3002, MinusLogProbMetric: 28.3002, val_loss: 28.7472, val_MinusLogProbMetric: 28.7472

Epoch 787: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.3002 - MinusLogProbMetric: 28.3002 - val_loss: 28.7472 - val_MinusLogProbMetric: 28.7472 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 788/1000
2023-10-12 15:15:42.416 
Epoch 788/1000 
	 loss: 28.2857, MinusLogProbMetric: 28.2857, val_loss: 28.7501, val_MinusLogProbMetric: 28.7501

Epoch 788: val_loss did not improve from 28.67201
196/196 - 31s - loss: 28.2857 - MinusLogProbMetric: 28.2857 - val_loss: 28.7501 - val_MinusLogProbMetric: 28.7501 - lr: 8.3333e-05 - 31s/epoch - 161ms/step
Epoch 789/1000
2023-10-12 15:16:15.952 
Epoch 789/1000 
	 loss: 28.2722, MinusLogProbMetric: 28.2722, val_loss: 28.7477, val_MinusLogProbMetric: 28.7477

Epoch 789: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2722 - MinusLogProbMetric: 28.2722 - val_loss: 28.7477 - val_MinusLogProbMetric: 28.7477 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 790/1000
2023-10-12 15:16:49.000 
Epoch 790/1000 
	 loss: 28.2885, MinusLogProbMetric: 28.2885, val_loss: 28.7058, val_MinusLogProbMetric: 28.7058

Epoch 790: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2885 - MinusLogProbMetric: 28.2885 - val_loss: 28.7058 - val_MinusLogProbMetric: 28.7058 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 791/1000
2023-10-12 15:17:22.295 
Epoch 791/1000 
	 loss: 28.2894, MinusLogProbMetric: 28.2894, val_loss: 28.8051, val_MinusLogProbMetric: 28.8051

Epoch 791: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2894 - MinusLogProbMetric: 28.2894 - val_loss: 28.8051 - val_MinusLogProbMetric: 28.8051 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 792/1000
2023-10-12 15:17:56.145 
Epoch 792/1000 
	 loss: 28.2663, MinusLogProbMetric: 28.2663, val_loss: 28.7497, val_MinusLogProbMetric: 28.7497

Epoch 792: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2663 - MinusLogProbMetric: 28.2663 - val_loss: 28.7497 - val_MinusLogProbMetric: 28.7497 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 793/1000
2023-10-12 15:18:29.533 
Epoch 793/1000 
	 loss: 28.2900, MinusLogProbMetric: 28.2900, val_loss: 28.7155, val_MinusLogProbMetric: 28.7155

Epoch 793: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2900 - MinusLogProbMetric: 28.2900 - val_loss: 28.7155 - val_MinusLogProbMetric: 28.7155 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 794/1000
2023-10-12 15:19:03.212 
Epoch 794/1000 
	 loss: 28.2738, MinusLogProbMetric: 28.2738, val_loss: 28.7334, val_MinusLogProbMetric: 28.7334

Epoch 794: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2738 - MinusLogProbMetric: 28.2738 - val_loss: 28.7334 - val_MinusLogProbMetric: 28.7334 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 795/1000
2023-10-12 15:19:36.586 
Epoch 795/1000 
	 loss: 28.2755, MinusLogProbMetric: 28.2755, val_loss: 28.7337, val_MinusLogProbMetric: 28.7337

Epoch 795: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2755 - MinusLogProbMetric: 28.2755 - val_loss: 28.7337 - val_MinusLogProbMetric: 28.7337 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 796/1000
2023-10-12 15:20:10.241 
Epoch 796/1000 
	 loss: 28.2720, MinusLogProbMetric: 28.2720, val_loss: 28.7115, val_MinusLogProbMetric: 28.7115

Epoch 796: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2720 - MinusLogProbMetric: 28.2720 - val_loss: 28.7115 - val_MinusLogProbMetric: 28.7115 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 797/1000
2023-10-12 15:20:44.281 
Epoch 797/1000 
	 loss: 28.2874, MinusLogProbMetric: 28.2874, val_loss: 28.7358, val_MinusLogProbMetric: 28.7358

Epoch 797: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2874 - MinusLogProbMetric: 28.2874 - val_loss: 28.7358 - val_MinusLogProbMetric: 28.7358 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 798/1000
2023-10-12 15:21:18.089 
Epoch 798/1000 
	 loss: 28.2824, MinusLogProbMetric: 28.2824, val_loss: 28.7600, val_MinusLogProbMetric: 28.7600

Epoch 798: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2824 - MinusLogProbMetric: 28.2824 - val_loss: 28.7600 - val_MinusLogProbMetric: 28.7600 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 799/1000
2023-10-12 15:21:51.860 
Epoch 799/1000 
	 loss: 28.2875, MinusLogProbMetric: 28.2875, val_loss: 28.7160, val_MinusLogProbMetric: 28.7160

Epoch 799: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2875 - MinusLogProbMetric: 28.2875 - val_loss: 28.7160 - val_MinusLogProbMetric: 28.7160 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 800/1000
2023-10-12 15:22:25.641 
Epoch 800/1000 
	 loss: 28.2876, MinusLogProbMetric: 28.2876, val_loss: 28.7617, val_MinusLogProbMetric: 28.7617

Epoch 800: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2876 - MinusLogProbMetric: 28.2876 - val_loss: 28.7617 - val_MinusLogProbMetric: 28.7617 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 801/1000
2023-10-12 15:22:59.506 
Epoch 801/1000 
	 loss: 28.2743, MinusLogProbMetric: 28.2743, val_loss: 28.7589, val_MinusLogProbMetric: 28.7589

Epoch 801: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2743 - MinusLogProbMetric: 28.2743 - val_loss: 28.7589 - val_MinusLogProbMetric: 28.7589 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 802/1000
2023-10-12 15:23:33.043 
Epoch 802/1000 
	 loss: 28.2837, MinusLogProbMetric: 28.2837, val_loss: 28.8039, val_MinusLogProbMetric: 28.8039

Epoch 802: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2837 - MinusLogProbMetric: 28.2837 - val_loss: 28.8039 - val_MinusLogProbMetric: 28.8039 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 803/1000
2023-10-12 15:24:06.716 
Epoch 803/1000 
	 loss: 28.3060, MinusLogProbMetric: 28.3060, val_loss: 28.9929, val_MinusLogProbMetric: 28.9929

Epoch 803: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.3060 - MinusLogProbMetric: 28.3060 - val_loss: 28.9929 - val_MinusLogProbMetric: 28.9929 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 804/1000
2023-10-12 15:24:40.195 
Epoch 804/1000 
	 loss: 28.2793, MinusLogProbMetric: 28.2793, val_loss: 28.7178, val_MinusLogProbMetric: 28.7178

Epoch 804: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2793 - MinusLogProbMetric: 28.2793 - val_loss: 28.7178 - val_MinusLogProbMetric: 28.7178 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 805/1000
2023-10-12 15:25:13.683 
Epoch 805/1000 
	 loss: 28.2665, MinusLogProbMetric: 28.2665, val_loss: 28.6775, val_MinusLogProbMetric: 28.6775

Epoch 805: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2665 - MinusLogProbMetric: 28.2665 - val_loss: 28.6775 - val_MinusLogProbMetric: 28.6775 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 806/1000
2023-10-12 15:25:47.415 
Epoch 806/1000 
	 loss: 28.2730, MinusLogProbMetric: 28.2730, val_loss: 28.7883, val_MinusLogProbMetric: 28.7883

Epoch 806: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2730 - MinusLogProbMetric: 28.2730 - val_loss: 28.7883 - val_MinusLogProbMetric: 28.7883 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 807/1000
2023-10-12 15:26:20.979 
Epoch 807/1000 
	 loss: 28.3002, MinusLogProbMetric: 28.3002, val_loss: 28.7713, val_MinusLogProbMetric: 28.7713

Epoch 807: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.3002 - MinusLogProbMetric: 28.3002 - val_loss: 28.7713 - val_MinusLogProbMetric: 28.7713 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 808/1000
2023-10-12 15:26:54.555 
Epoch 808/1000 
	 loss: 28.2780, MinusLogProbMetric: 28.2780, val_loss: 28.7091, val_MinusLogProbMetric: 28.7091

Epoch 808: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2780 - MinusLogProbMetric: 28.2780 - val_loss: 28.7091 - val_MinusLogProbMetric: 28.7091 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 809/1000
2023-10-12 15:27:27.879 
Epoch 809/1000 
	 loss: 28.2635, MinusLogProbMetric: 28.2635, val_loss: 28.7434, val_MinusLogProbMetric: 28.7434

Epoch 809: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.2635 - MinusLogProbMetric: 28.2635 - val_loss: 28.7434 - val_MinusLogProbMetric: 28.7434 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 810/1000
2023-10-12 15:28:01.682 
Epoch 810/1000 
	 loss: 28.2840, MinusLogProbMetric: 28.2840, val_loss: 28.7835, val_MinusLogProbMetric: 28.7835

Epoch 810: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2840 - MinusLogProbMetric: 28.2840 - val_loss: 28.7835 - val_MinusLogProbMetric: 28.7835 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 811/1000
2023-10-12 15:28:35.251 
Epoch 811/1000 
	 loss: 28.2761, MinusLogProbMetric: 28.2761, val_loss: 28.8459, val_MinusLogProbMetric: 28.8459

Epoch 811: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.2761 - MinusLogProbMetric: 28.2761 - val_loss: 28.8459 - val_MinusLogProbMetric: 28.8459 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 812/1000
2023-10-12 15:29:08.613 
Epoch 812/1000 
	 loss: 28.3056, MinusLogProbMetric: 28.3056, val_loss: 28.6879, val_MinusLogProbMetric: 28.6879

Epoch 812: val_loss did not improve from 28.67201
196/196 - 33s - loss: 28.3056 - MinusLogProbMetric: 28.3056 - val_loss: 28.6879 - val_MinusLogProbMetric: 28.6879 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 813/1000
2023-10-12 15:29:42.538 
Epoch 813/1000 
	 loss: 28.1959, MinusLogProbMetric: 28.1959, val_loss: 28.6749, val_MinusLogProbMetric: 28.6749

Epoch 813: val_loss did not improve from 28.67201
196/196 - 34s - loss: 28.1959 - MinusLogProbMetric: 28.1959 - val_loss: 28.6749 - val_MinusLogProbMetric: 28.6749 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 814/1000
2023-10-12 15:30:16.141 
Epoch 814/1000 
	 loss: 28.1920, MinusLogProbMetric: 28.1920, val_loss: 28.6311, val_MinusLogProbMetric: 28.6311

Epoch 814: val_loss improved from 28.67201 to 28.63111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.1920 - MinusLogProbMetric: 28.1920 - val_loss: 28.6311 - val_MinusLogProbMetric: 28.6311 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 815/1000
2023-10-12 15:30:50.364 
Epoch 815/1000 
	 loss: 28.1920, MinusLogProbMetric: 28.1920, val_loss: 28.6748, val_MinusLogProbMetric: 28.6748

Epoch 815: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1920 - MinusLogProbMetric: 28.1920 - val_loss: 28.6748 - val_MinusLogProbMetric: 28.6748 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 816/1000
2023-10-12 15:31:23.946 
Epoch 816/1000 
	 loss: 28.2014, MinusLogProbMetric: 28.2014, val_loss: 28.6731, val_MinusLogProbMetric: 28.6731

Epoch 816: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.2014 - MinusLogProbMetric: 28.2014 - val_loss: 28.6731 - val_MinusLogProbMetric: 28.6731 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 817/1000
2023-10-12 15:31:57.558 
Epoch 817/1000 
	 loss: 28.1967, MinusLogProbMetric: 28.1967, val_loss: 28.6442, val_MinusLogProbMetric: 28.6442

Epoch 817: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1967 - MinusLogProbMetric: 28.1967 - val_loss: 28.6442 - val_MinusLogProbMetric: 28.6442 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 818/1000
2023-10-12 15:32:31.268 
Epoch 818/1000 
	 loss: 28.1896, MinusLogProbMetric: 28.1896, val_loss: 28.6866, val_MinusLogProbMetric: 28.6866

Epoch 818: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1896 - MinusLogProbMetric: 28.1896 - val_loss: 28.6866 - val_MinusLogProbMetric: 28.6866 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 819/1000
2023-10-12 15:33:04.790 
Epoch 819/1000 
	 loss: 28.1922, MinusLogProbMetric: 28.1922, val_loss: 28.6563, val_MinusLogProbMetric: 28.6563

Epoch 819: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1922 - MinusLogProbMetric: 28.1922 - val_loss: 28.6563 - val_MinusLogProbMetric: 28.6563 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 820/1000
2023-10-12 15:33:38.414 
Epoch 820/1000 
	 loss: 28.1941, MinusLogProbMetric: 28.1941, val_loss: 28.7096, val_MinusLogProbMetric: 28.7096

Epoch 820: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1941 - MinusLogProbMetric: 28.1941 - val_loss: 28.7096 - val_MinusLogProbMetric: 28.7096 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 821/1000
2023-10-12 15:34:11.940 
Epoch 821/1000 
	 loss: 28.1981, MinusLogProbMetric: 28.1981, val_loss: 28.6638, val_MinusLogProbMetric: 28.6638

Epoch 821: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1981 - MinusLogProbMetric: 28.1981 - val_loss: 28.6638 - val_MinusLogProbMetric: 28.6638 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 822/1000
2023-10-12 15:34:45.724 
Epoch 822/1000 
	 loss: 28.1983, MinusLogProbMetric: 28.1983, val_loss: 28.6607, val_MinusLogProbMetric: 28.6607

Epoch 822: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1983 - MinusLogProbMetric: 28.1983 - val_loss: 28.6607 - val_MinusLogProbMetric: 28.6607 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 823/1000
2023-10-12 15:35:19.614 
Epoch 823/1000 
	 loss: 28.1970, MinusLogProbMetric: 28.1970, val_loss: 28.6858, val_MinusLogProbMetric: 28.6858

Epoch 823: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1970 - MinusLogProbMetric: 28.1970 - val_loss: 28.6858 - val_MinusLogProbMetric: 28.6858 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 824/1000
2023-10-12 15:35:53.255 
Epoch 824/1000 
	 loss: 28.2008, MinusLogProbMetric: 28.2008, val_loss: 28.6841, val_MinusLogProbMetric: 28.6841

Epoch 824: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.2008 - MinusLogProbMetric: 28.2008 - val_loss: 28.6841 - val_MinusLogProbMetric: 28.6841 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 825/1000
2023-10-12 15:36:26.630 
Epoch 825/1000 
	 loss: 28.1933, MinusLogProbMetric: 28.1933, val_loss: 28.6605, val_MinusLogProbMetric: 28.6605

Epoch 825: val_loss did not improve from 28.63111
196/196 - 33s - loss: 28.1933 - MinusLogProbMetric: 28.1933 - val_loss: 28.6605 - val_MinusLogProbMetric: 28.6605 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 826/1000
2023-10-12 15:37:00.407 
Epoch 826/1000 
	 loss: 28.1931, MinusLogProbMetric: 28.1931, val_loss: 28.7002, val_MinusLogProbMetric: 28.7002

Epoch 826: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1931 - MinusLogProbMetric: 28.1931 - val_loss: 28.7002 - val_MinusLogProbMetric: 28.7002 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 827/1000
2023-10-12 15:37:34.160 
Epoch 827/1000 
	 loss: 28.1952, MinusLogProbMetric: 28.1952, val_loss: 28.6747, val_MinusLogProbMetric: 28.6747

Epoch 827: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1952 - MinusLogProbMetric: 28.1952 - val_loss: 28.6747 - val_MinusLogProbMetric: 28.6747 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 828/1000
2023-10-12 15:38:08.092 
Epoch 828/1000 
	 loss: 28.1866, MinusLogProbMetric: 28.1866, val_loss: 28.6833, val_MinusLogProbMetric: 28.6833

Epoch 828: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1866 - MinusLogProbMetric: 28.1866 - val_loss: 28.6833 - val_MinusLogProbMetric: 28.6833 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 829/1000
2023-10-12 15:38:41.498 
Epoch 829/1000 
	 loss: 28.1908, MinusLogProbMetric: 28.1908, val_loss: 28.6453, val_MinusLogProbMetric: 28.6453

Epoch 829: val_loss did not improve from 28.63111
196/196 - 33s - loss: 28.1908 - MinusLogProbMetric: 28.1908 - val_loss: 28.6453 - val_MinusLogProbMetric: 28.6453 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 830/1000
2023-10-12 15:39:15.302 
Epoch 830/1000 
	 loss: 28.1902, MinusLogProbMetric: 28.1902, val_loss: 28.7235, val_MinusLogProbMetric: 28.7235

Epoch 830: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1902 - MinusLogProbMetric: 28.1902 - val_loss: 28.7235 - val_MinusLogProbMetric: 28.7235 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 831/1000
2023-10-12 15:39:48.879 
Epoch 831/1000 
	 loss: 28.2036, MinusLogProbMetric: 28.2036, val_loss: 28.6641, val_MinusLogProbMetric: 28.6641

Epoch 831: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.2036 - MinusLogProbMetric: 28.2036 - val_loss: 28.6641 - val_MinusLogProbMetric: 28.6641 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 832/1000
2023-10-12 15:40:22.163 
Epoch 832/1000 
	 loss: 28.1977, MinusLogProbMetric: 28.1977, val_loss: 28.7169, val_MinusLogProbMetric: 28.7169

Epoch 832: val_loss did not improve from 28.63111
196/196 - 33s - loss: 28.1977 - MinusLogProbMetric: 28.1977 - val_loss: 28.7169 - val_MinusLogProbMetric: 28.7169 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 833/1000
2023-10-12 15:40:55.995 
Epoch 833/1000 
	 loss: 28.1941, MinusLogProbMetric: 28.1941, val_loss: 28.6874, val_MinusLogProbMetric: 28.6874

Epoch 833: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1941 - MinusLogProbMetric: 28.1941 - val_loss: 28.6874 - val_MinusLogProbMetric: 28.6874 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 834/1000
2023-10-12 15:41:30.224 
Epoch 834/1000 
	 loss: 28.1885, MinusLogProbMetric: 28.1885, val_loss: 28.6674, val_MinusLogProbMetric: 28.6674

Epoch 834: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1885 - MinusLogProbMetric: 28.1885 - val_loss: 28.6674 - val_MinusLogProbMetric: 28.6674 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 835/1000
2023-10-12 15:42:03.839 
Epoch 835/1000 
	 loss: 28.1872, MinusLogProbMetric: 28.1872, val_loss: 28.6747, val_MinusLogProbMetric: 28.6747

Epoch 835: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1872 - MinusLogProbMetric: 28.1872 - val_loss: 28.6747 - val_MinusLogProbMetric: 28.6747 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 836/1000
2023-10-12 15:42:37.684 
Epoch 836/1000 
	 loss: 28.1872, MinusLogProbMetric: 28.1872, val_loss: 28.6609, val_MinusLogProbMetric: 28.6609

Epoch 836: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1872 - MinusLogProbMetric: 28.1872 - val_loss: 28.6609 - val_MinusLogProbMetric: 28.6609 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 837/1000
2023-10-12 15:43:11.299 
Epoch 837/1000 
	 loss: 28.1904, MinusLogProbMetric: 28.1904, val_loss: 28.6526, val_MinusLogProbMetric: 28.6526

Epoch 837: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1904 - MinusLogProbMetric: 28.1904 - val_loss: 28.6526 - val_MinusLogProbMetric: 28.6526 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 838/1000
2023-10-12 15:43:44.813 
Epoch 838/1000 
	 loss: 28.1902, MinusLogProbMetric: 28.1902, val_loss: 28.6468, val_MinusLogProbMetric: 28.6468

Epoch 838: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1902 - MinusLogProbMetric: 28.1902 - val_loss: 28.6468 - val_MinusLogProbMetric: 28.6468 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 839/1000
2023-10-12 15:44:18.336 
Epoch 839/1000 
	 loss: 28.1847, MinusLogProbMetric: 28.1847, val_loss: 28.6885, val_MinusLogProbMetric: 28.6885

Epoch 839: val_loss did not improve from 28.63111
196/196 - 34s - loss: 28.1847 - MinusLogProbMetric: 28.1847 - val_loss: 28.6885 - val_MinusLogProbMetric: 28.6885 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 840/1000
2023-10-12 15:44:52.022 
Epoch 840/1000 
	 loss: 28.1853, MinusLogProbMetric: 28.1853, val_loss: 28.6257, val_MinusLogProbMetric: 28.6257

Epoch 840: val_loss improved from 28.63111 to 28.62574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.1853 - MinusLogProbMetric: 28.1853 - val_loss: 28.6257 - val_MinusLogProbMetric: 28.6257 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 841/1000
2023-10-12 15:45:26.061 
Epoch 841/1000 
	 loss: 28.1865, MinusLogProbMetric: 28.1865, val_loss: 28.6678, val_MinusLogProbMetric: 28.6678

Epoch 841: val_loss did not improve from 28.62574
196/196 - 33s - loss: 28.1865 - MinusLogProbMetric: 28.1865 - val_loss: 28.6678 - val_MinusLogProbMetric: 28.6678 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 842/1000
2023-10-12 15:45:59.831 
Epoch 842/1000 
	 loss: 28.1946, MinusLogProbMetric: 28.1946, val_loss: 28.6502, val_MinusLogProbMetric: 28.6502

Epoch 842: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1946 - MinusLogProbMetric: 28.1946 - val_loss: 28.6502 - val_MinusLogProbMetric: 28.6502 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 843/1000
2023-10-12 15:46:33.345 
Epoch 843/1000 
	 loss: 28.1993, MinusLogProbMetric: 28.1993, val_loss: 28.6966, val_MinusLogProbMetric: 28.6966

Epoch 843: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1993 - MinusLogProbMetric: 28.1993 - val_loss: 28.6966 - val_MinusLogProbMetric: 28.6966 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 844/1000
2023-10-12 15:47:07.091 
Epoch 844/1000 
	 loss: 28.1994, MinusLogProbMetric: 28.1994, val_loss: 28.6841, val_MinusLogProbMetric: 28.6841

Epoch 844: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1994 - MinusLogProbMetric: 28.1994 - val_loss: 28.6841 - val_MinusLogProbMetric: 28.6841 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 845/1000
2023-10-12 15:47:40.894 
Epoch 845/1000 
	 loss: 28.1896, MinusLogProbMetric: 28.1896, val_loss: 28.6615, val_MinusLogProbMetric: 28.6615

Epoch 845: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1896 - MinusLogProbMetric: 28.1896 - val_loss: 28.6615 - val_MinusLogProbMetric: 28.6615 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 846/1000
2023-10-12 15:48:15.028 
Epoch 846/1000 
	 loss: 28.2043, MinusLogProbMetric: 28.2043, val_loss: 28.7472, val_MinusLogProbMetric: 28.7472

Epoch 846: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.2043 - MinusLogProbMetric: 28.2043 - val_loss: 28.7472 - val_MinusLogProbMetric: 28.7472 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 847/1000
2023-10-12 15:48:48.441 
Epoch 847/1000 
	 loss: 28.1886, MinusLogProbMetric: 28.1886, val_loss: 28.6357, val_MinusLogProbMetric: 28.6357

Epoch 847: val_loss did not improve from 28.62574
196/196 - 33s - loss: 28.1886 - MinusLogProbMetric: 28.1886 - val_loss: 28.6357 - val_MinusLogProbMetric: 28.6357 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 848/1000
2023-10-12 15:49:21.670 
Epoch 848/1000 
	 loss: 28.1864, MinusLogProbMetric: 28.1864, val_loss: 28.6668, val_MinusLogProbMetric: 28.6668

Epoch 848: val_loss did not improve from 28.62574
196/196 - 33s - loss: 28.1864 - MinusLogProbMetric: 28.1864 - val_loss: 28.6668 - val_MinusLogProbMetric: 28.6668 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 849/1000
2023-10-12 15:49:55.308 
Epoch 849/1000 
	 loss: 28.2027, MinusLogProbMetric: 28.2027, val_loss: 28.6484, val_MinusLogProbMetric: 28.6484

Epoch 849: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.2027 - MinusLogProbMetric: 28.2027 - val_loss: 28.6484 - val_MinusLogProbMetric: 28.6484 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 850/1000
2023-10-12 15:50:28.887 
Epoch 850/1000 
	 loss: 28.1855, MinusLogProbMetric: 28.1855, val_loss: 28.6489, val_MinusLogProbMetric: 28.6489

Epoch 850: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1855 - MinusLogProbMetric: 28.1855 - val_loss: 28.6489 - val_MinusLogProbMetric: 28.6489 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 851/1000
2023-10-12 15:51:02.447 
Epoch 851/1000 
	 loss: 28.1917, MinusLogProbMetric: 28.1917, val_loss: 28.6892, val_MinusLogProbMetric: 28.6892

Epoch 851: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1917 - MinusLogProbMetric: 28.1917 - val_loss: 28.6892 - val_MinusLogProbMetric: 28.6892 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 852/1000
2023-10-12 15:51:36.285 
Epoch 852/1000 
	 loss: 28.1866, MinusLogProbMetric: 28.1866, val_loss: 28.6531, val_MinusLogProbMetric: 28.6531

Epoch 852: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1866 - MinusLogProbMetric: 28.1866 - val_loss: 28.6531 - val_MinusLogProbMetric: 28.6531 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 853/1000
2023-10-12 15:52:10.056 
Epoch 853/1000 
	 loss: 28.1912, MinusLogProbMetric: 28.1912, val_loss: 28.6753, val_MinusLogProbMetric: 28.6753

Epoch 853: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1912 - MinusLogProbMetric: 28.1912 - val_loss: 28.6753 - val_MinusLogProbMetric: 28.6753 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 854/1000
2023-10-12 15:52:43.994 
Epoch 854/1000 
	 loss: 28.1939, MinusLogProbMetric: 28.1939, val_loss: 28.7205, val_MinusLogProbMetric: 28.7205

Epoch 854: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1939 - MinusLogProbMetric: 28.1939 - val_loss: 28.7205 - val_MinusLogProbMetric: 28.7205 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 855/1000
2023-10-12 15:53:17.912 
Epoch 855/1000 
	 loss: 28.1959, MinusLogProbMetric: 28.1959, val_loss: 28.7487, val_MinusLogProbMetric: 28.7487

Epoch 855: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1959 - MinusLogProbMetric: 28.1959 - val_loss: 28.7487 - val_MinusLogProbMetric: 28.7487 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 856/1000
2023-10-12 15:53:51.531 
Epoch 856/1000 
	 loss: 28.1971, MinusLogProbMetric: 28.1971, val_loss: 28.6508, val_MinusLogProbMetric: 28.6508

Epoch 856: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1971 - MinusLogProbMetric: 28.1971 - val_loss: 28.6508 - val_MinusLogProbMetric: 28.6508 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 857/1000
2023-10-12 15:54:25.094 
Epoch 857/1000 
	 loss: 28.1847, MinusLogProbMetric: 28.1847, val_loss: 28.6659, val_MinusLogProbMetric: 28.6659

Epoch 857: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1847 - MinusLogProbMetric: 28.1847 - val_loss: 28.6659 - val_MinusLogProbMetric: 28.6659 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 858/1000
2023-10-12 15:54:58.822 
Epoch 858/1000 
	 loss: 28.1857, MinusLogProbMetric: 28.1857, val_loss: 28.6470, val_MinusLogProbMetric: 28.6470

Epoch 858: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1857 - MinusLogProbMetric: 28.1857 - val_loss: 28.6470 - val_MinusLogProbMetric: 28.6470 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 859/1000
2023-10-12 15:55:32.715 
Epoch 859/1000 
	 loss: 28.1894, MinusLogProbMetric: 28.1894, val_loss: 28.6879, val_MinusLogProbMetric: 28.6879

Epoch 859: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1894 - MinusLogProbMetric: 28.1894 - val_loss: 28.6879 - val_MinusLogProbMetric: 28.6879 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 860/1000
2023-10-12 15:56:06.247 
Epoch 860/1000 
	 loss: 28.1867, MinusLogProbMetric: 28.1867, val_loss: 28.6460, val_MinusLogProbMetric: 28.6460

Epoch 860: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1867 - MinusLogProbMetric: 28.1867 - val_loss: 28.6460 - val_MinusLogProbMetric: 28.6460 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 861/1000
2023-10-12 15:56:40.104 
Epoch 861/1000 
	 loss: 28.1858, MinusLogProbMetric: 28.1858, val_loss: 28.6863, val_MinusLogProbMetric: 28.6863

Epoch 861: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1858 - MinusLogProbMetric: 28.1858 - val_loss: 28.6863 - val_MinusLogProbMetric: 28.6863 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 862/1000
2023-10-12 15:57:13.563 
Epoch 862/1000 
	 loss: 28.1806, MinusLogProbMetric: 28.1806, val_loss: 28.7081, val_MinusLogProbMetric: 28.7081

Epoch 862: val_loss did not improve from 28.62574
196/196 - 33s - loss: 28.1806 - MinusLogProbMetric: 28.1806 - val_loss: 28.7081 - val_MinusLogProbMetric: 28.7081 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 863/1000
2023-10-12 15:57:47.020 
Epoch 863/1000 
	 loss: 28.1818, MinusLogProbMetric: 28.1818, val_loss: 28.6576, val_MinusLogProbMetric: 28.6576

Epoch 863: val_loss did not improve from 28.62574
196/196 - 33s - loss: 28.1818 - MinusLogProbMetric: 28.1818 - val_loss: 28.6576 - val_MinusLogProbMetric: 28.6576 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 864/1000
2023-10-12 15:58:20.751 
Epoch 864/1000 
	 loss: 28.1784, MinusLogProbMetric: 28.1784, val_loss: 28.6734, val_MinusLogProbMetric: 28.6734

Epoch 864: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1784 - MinusLogProbMetric: 28.1784 - val_loss: 28.6734 - val_MinusLogProbMetric: 28.6734 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 865/1000
2023-10-12 15:58:54.387 
Epoch 865/1000 
	 loss: 28.1833, MinusLogProbMetric: 28.1833, val_loss: 28.6359, val_MinusLogProbMetric: 28.6359

Epoch 865: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1833 - MinusLogProbMetric: 28.1833 - val_loss: 28.6359 - val_MinusLogProbMetric: 28.6359 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 866/1000
2023-10-12 15:59:28.395 
Epoch 866/1000 
	 loss: 28.1920, MinusLogProbMetric: 28.1920, val_loss: 28.6484, val_MinusLogProbMetric: 28.6484

Epoch 866: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1920 - MinusLogProbMetric: 28.1920 - val_loss: 28.6484 - val_MinusLogProbMetric: 28.6484 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 867/1000
2023-10-12 16:00:02.900 
Epoch 867/1000 
	 loss: 28.1857, MinusLogProbMetric: 28.1857, val_loss: 28.6573, val_MinusLogProbMetric: 28.6573

Epoch 867: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1857 - MinusLogProbMetric: 28.1857 - val_loss: 28.6573 - val_MinusLogProbMetric: 28.6573 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 868/1000
2023-10-12 16:00:36.984 
Epoch 868/1000 
	 loss: 28.1891, MinusLogProbMetric: 28.1891, val_loss: 28.6955, val_MinusLogProbMetric: 28.6955

Epoch 868: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1891 - MinusLogProbMetric: 28.1891 - val_loss: 28.6955 - val_MinusLogProbMetric: 28.6955 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 869/1000
2023-10-12 16:01:06.735 
Epoch 869/1000 
	 loss: 28.1870, MinusLogProbMetric: 28.1870, val_loss: 28.6691, val_MinusLogProbMetric: 28.6691

Epoch 869: val_loss did not improve from 28.62574
196/196 - 30s - loss: 28.1870 - MinusLogProbMetric: 28.1870 - val_loss: 28.6691 - val_MinusLogProbMetric: 28.6691 - lr: 4.1667e-05 - 30s/epoch - 152ms/step
Epoch 870/1000
2023-10-12 16:01:36.466 
Epoch 870/1000 
	 loss: 28.1829, MinusLogProbMetric: 28.1829, val_loss: 28.6421, val_MinusLogProbMetric: 28.6421

Epoch 870: val_loss did not improve from 28.62574
196/196 - 30s - loss: 28.1829 - MinusLogProbMetric: 28.1829 - val_loss: 28.6421 - val_MinusLogProbMetric: 28.6421 - lr: 4.1667e-05 - 30s/epoch - 152ms/step
Epoch 871/1000
2023-10-12 16:02:07.526 
Epoch 871/1000 
	 loss: 28.1827, MinusLogProbMetric: 28.1827, val_loss: 28.6371, val_MinusLogProbMetric: 28.6371

Epoch 871: val_loss did not improve from 28.62574
196/196 - 31s - loss: 28.1827 - MinusLogProbMetric: 28.1827 - val_loss: 28.6371 - val_MinusLogProbMetric: 28.6371 - lr: 4.1667e-05 - 31s/epoch - 158ms/step
Epoch 872/1000
2023-10-12 16:02:39.665 
Epoch 872/1000 
	 loss: 28.1842, MinusLogProbMetric: 28.1842, val_loss: 28.6299, val_MinusLogProbMetric: 28.6299

Epoch 872: val_loss did not improve from 28.62574
196/196 - 32s - loss: 28.1842 - MinusLogProbMetric: 28.1842 - val_loss: 28.6299 - val_MinusLogProbMetric: 28.6299 - lr: 4.1667e-05 - 32s/epoch - 164ms/step
Epoch 873/1000
2023-10-12 16:03:12.529 
Epoch 873/1000 
	 loss: 28.1848, MinusLogProbMetric: 28.1848, val_loss: 28.6846, val_MinusLogProbMetric: 28.6846

Epoch 873: val_loss did not improve from 28.62574
196/196 - 33s - loss: 28.1848 - MinusLogProbMetric: 28.1848 - val_loss: 28.6846 - val_MinusLogProbMetric: 28.6846 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 874/1000
2023-10-12 16:03:46.213 
Epoch 874/1000 
	 loss: 28.1929, MinusLogProbMetric: 28.1929, val_loss: 28.7046, val_MinusLogProbMetric: 28.7046

Epoch 874: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1929 - MinusLogProbMetric: 28.1929 - val_loss: 28.7046 - val_MinusLogProbMetric: 28.7046 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 875/1000
2023-10-12 16:04:20.395 
Epoch 875/1000 
	 loss: 28.1782, MinusLogProbMetric: 28.1782, val_loss: 28.7024, val_MinusLogProbMetric: 28.7024

Epoch 875: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1782 - MinusLogProbMetric: 28.1782 - val_loss: 28.7024 - val_MinusLogProbMetric: 28.7024 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 876/1000
2023-10-12 16:04:54.298 
Epoch 876/1000 
	 loss: 28.1792, MinusLogProbMetric: 28.1792, val_loss: 28.7137, val_MinusLogProbMetric: 28.7137

Epoch 876: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1792 - MinusLogProbMetric: 28.1792 - val_loss: 28.7137 - val_MinusLogProbMetric: 28.7137 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 877/1000
2023-10-12 16:05:27.996 
Epoch 877/1000 
	 loss: 28.1805, MinusLogProbMetric: 28.1805, val_loss: 28.6605, val_MinusLogProbMetric: 28.6605

Epoch 877: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1805 - MinusLogProbMetric: 28.1805 - val_loss: 28.6605 - val_MinusLogProbMetric: 28.6605 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 878/1000
2023-10-12 16:06:01.600 
Epoch 878/1000 
	 loss: 28.1857, MinusLogProbMetric: 28.1857, val_loss: 28.6741, val_MinusLogProbMetric: 28.6741

Epoch 878: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1857 - MinusLogProbMetric: 28.1857 - val_loss: 28.6741 - val_MinusLogProbMetric: 28.6741 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 879/1000
2023-10-12 16:06:35.408 
Epoch 879/1000 
	 loss: 28.1805, MinusLogProbMetric: 28.1805, val_loss: 28.6450, val_MinusLogProbMetric: 28.6450

Epoch 879: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1805 - MinusLogProbMetric: 28.1805 - val_loss: 28.6450 - val_MinusLogProbMetric: 28.6450 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 880/1000
2023-10-12 16:07:09.160 
Epoch 880/1000 
	 loss: 28.1819, MinusLogProbMetric: 28.1819, val_loss: 28.6598, val_MinusLogProbMetric: 28.6598

Epoch 880: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1819 - MinusLogProbMetric: 28.1819 - val_loss: 28.6598 - val_MinusLogProbMetric: 28.6598 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 881/1000
2023-10-12 16:07:42.798 
Epoch 881/1000 
	 loss: 28.1788, MinusLogProbMetric: 28.1788, val_loss: 28.6531, val_MinusLogProbMetric: 28.6531

Epoch 881: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1788 - MinusLogProbMetric: 28.1788 - val_loss: 28.6531 - val_MinusLogProbMetric: 28.6531 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 882/1000
2023-10-12 16:08:16.539 
Epoch 882/1000 
	 loss: 28.1866, MinusLogProbMetric: 28.1866, val_loss: 28.6732, val_MinusLogProbMetric: 28.6732

Epoch 882: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1866 - MinusLogProbMetric: 28.1866 - val_loss: 28.6732 - val_MinusLogProbMetric: 28.6732 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 883/1000
2023-10-12 16:08:50.396 
Epoch 883/1000 
	 loss: 28.1834, MinusLogProbMetric: 28.1834, val_loss: 28.6684, val_MinusLogProbMetric: 28.6684

Epoch 883: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1834 - MinusLogProbMetric: 28.1834 - val_loss: 28.6684 - val_MinusLogProbMetric: 28.6684 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 884/1000
2023-10-12 16:09:23.955 
Epoch 884/1000 
	 loss: 28.1875, MinusLogProbMetric: 28.1875, val_loss: 28.7254, val_MinusLogProbMetric: 28.7254

Epoch 884: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1875 - MinusLogProbMetric: 28.1875 - val_loss: 28.7254 - val_MinusLogProbMetric: 28.7254 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 885/1000
2023-10-12 16:09:57.846 
Epoch 885/1000 
	 loss: 28.1862, MinusLogProbMetric: 28.1862, val_loss: 28.6685, val_MinusLogProbMetric: 28.6685

Epoch 885: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1862 - MinusLogProbMetric: 28.1862 - val_loss: 28.6685 - val_MinusLogProbMetric: 28.6685 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 886/1000
2023-10-12 16:10:31.707 
Epoch 886/1000 
	 loss: 28.1816, MinusLogProbMetric: 28.1816, val_loss: 28.6608, val_MinusLogProbMetric: 28.6608

Epoch 886: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1816 - MinusLogProbMetric: 28.1816 - val_loss: 28.6608 - val_MinusLogProbMetric: 28.6608 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 887/1000
2023-10-12 16:11:05.795 
Epoch 887/1000 
	 loss: 28.1902, MinusLogProbMetric: 28.1902, val_loss: 28.6378, val_MinusLogProbMetric: 28.6378

Epoch 887: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1902 - MinusLogProbMetric: 28.1902 - val_loss: 28.6378 - val_MinusLogProbMetric: 28.6378 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 888/1000
2023-10-12 16:11:39.622 
Epoch 888/1000 
	 loss: 28.1756, MinusLogProbMetric: 28.1756, val_loss: 28.6737, val_MinusLogProbMetric: 28.6737

Epoch 888: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1756 - MinusLogProbMetric: 28.1756 - val_loss: 28.6737 - val_MinusLogProbMetric: 28.6737 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 889/1000
2023-10-12 16:12:13.236 
Epoch 889/1000 
	 loss: 28.1777, MinusLogProbMetric: 28.1777, val_loss: 28.6594, val_MinusLogProbMetric: 28.6594

Epoch 889: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1777 - MinusLogProbMetric: 28.1777 - val_loss: 28.6594 - val_MinusLogProbMetric: 28.6594 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 890/1000
2023-10-12 16:12:46.996 
Epoch 890/1000 
	 loss: 28.1803, MinusLogProbMetric: 28.1803, val_loss: 28.6789, val_MinusLogProbMetric: 28.6789

Epoch 890: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1803 - MinusLogProbMetric: 28.1803 - val_loss: 28.6789 - val_MinusLogProbMetric: 28.6789 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 891/1000
2023-10-12 16:13:20.896 
Epoch 891/1000 
	 loss: 28.1501, MinusLogProbMetric: 28.1501, val_loss: 28.6361, val_MinusLogProbMetric: 28.6361

Epoch 891: val_loss did not improve from 28.62574
196/196 - 34s - loss: 28.1501 - MinusLogProbMetric: 28.1501 - val_loss: 28.6361 - val_MinusLogProbMetric: 28.6361 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 892/1000
2023-10-12 16:13:54.544 
Epoch 892/1000 
	 loss: 28.1474, MinusLogProbMetric: 28.1474, val_loss: 28.6198, val_MinusLogProbMetric: 28.6198

Epoch 892: val_loss improved from 28.62574 to 28.61980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.1474 - MinusLogProbMetric: 28.1474 - val_loss: 28.6198 - val_MinusLogProbMetric: 28.6198 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 893/1000
2023-10-12 16:14:28.711 
Epoch 893/1000 
	 loss: 28.1438, MinusLogProbMetric: 28.1438, val_loss: 28.6379, val_MinusLogProbMetric: 28.6379

Epoch 893: val_loss did not improve from 28.61980
196/196 - 34s - loss: 28.1438 - MinusLogProbMetric: 28.1438 - val_loss: 28.6379 - val_MinusLogProbMetric: 28.6379 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 894/1000
2023-10-12 16:15:01.915 
Epoch 894/1000 
	 loss: 28.1476, MinusLogProbMetric: 28.1476, val_loss: 28.6290, val_MinusLogProbMetric: 28.6290

Epoch 894: val_loss did not improve from 28.61980
196/196 - 33s - loss: 28.1476 - MinusLogProbMetric: 28.1476 - val_loss: 28.6290 - val_MinusLogProbMetric: 28.6290 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 895/1000
2023-10-12 16:15:35.818 
Epoch 895/1000 
	 loss: 28.1470, MinusLogProbMetric: 28.1470, val_loss: 28.6137, val_MinusLogProbMetric: 28.6137

Epoch 895: val_loss improved from 28.61980 to 28.61367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 34s - loss: 28.1470 - MinusLogProbMetric: 28.1470 - val_loss: 28.6137 - val_MinusLogProbMetric: 28.6137 - lr: 2.0833e-05 - 34s/epoch - 176ms/step
Epoch 896/1000
2023-10-12 16:16:10.008 
Epoch 896/1000 
	 loss: 28.1477, MinusLogProbMetric: 28.1477, val_loss: 28.6161, val_MinusLogProbMetric: 28.6161

Epoch 896: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1477 - MinusLogProbMetric: 28.1477 - val_loss: 28.6161 - val_MinusLogProbMetric: 28.6161 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 897/1000
2023-10-12 16:16:43.734 
Epoch 897/1000 
	 loss: 28.1449, MinusLogProbMetric: 28.1449, val_loss: 28.6157, val_MinusLogProbMetric: 28.6157

Epoch 897: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1449 - MinusLogProbMetric: 28.1449 - val_loss: 28.6157 - val_MinusLogProbMetric: 28.6157 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 898/1000
2023-10-12 16:17:17.118 
Epoch 898/1000 
	 loss: 28.1462, MinusLogProbMetric: 28.1462, val_loss: 28.6308, val_MinusLogProbMetric: 28.6308

Epoch 898: val_loss did not improve from 28.61367
196/196 - 33s - loss: 28.1462 - MinusLogProbMetric: 28.1462 - val_loss: 28.6308 - val_MinusLogProbMetric: 28.6308 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 899/1000
2023-10-12 16:17:51.312 
Epoch 899/1000 
	 loss: 28.1455, MinusLogProbMetric: 28.1455, val_loss: 28.6398, val_MinusLogProbMetric: 28.6398

Epoch 899: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1455 - MinusLogProbMetric: 28.1455 - val_loss: 28.6398 - val_MinusLogProbMetric: 28.6398 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 900/1000
2023-10-12 16:18:25.116 
Epoch 900/1000 
	 loss: 28.1451, MinusLogProbMetric: 28.1451, val_loss: 28.6232, val_MinusLogProbMetric: 28.6232

Epoch 900: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1451 - MinusLogProbMetric: 28.1451 - val_loss: 28.6232 - val_MinusLogProbMetric: 28.6232 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 901/1000
2023-10-12 16:18:59.418 
Epoch 901/1000 
	 loss: 28.1441, MinusLogProbMetric: 28.1441, val_loss: 28.6304, val_MinusLogProbMetric: 28.6304

Epoch 901: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1441 - MinusLogProbMetric: 28.1441 - val_loss: 28.6304 - val_MinusLogProbMetric: 28.6304 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 902/1000
2023-10-12 16:19:33.106 
Epoch 902/1000 
	 loss: 28.1400, MinusLogProbMetric: 28.1400, val_loss: 28.6310, val_MinusLogProbMetric: 28.6310

Epoch 902: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1400 - MinusLogProbMetric: 28.1400 - val_loss: 28.6310 - val_MinusLogProbMetric: 28.6310 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 903/1000
2023-10-12 16:20:06.896 
Epoch 903/1000 
	 loss: 28.1423, MinusLogProbMetric: 28.1423, val_loss: 28.6254, val_MinusLogProbMetric: 28.6254

Epoch 903: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1423 - MinusLogProbMetric: 28.1423 - val_loss: 28.6254 - val_MinusLogProbMetric: 28.6254 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 904/1000
2023-10-12 16:20:40.666 
Epoch 904/1000 
	 loss: 28.1429, MinusLogProbMetric: 28.1429, val_loss: 28.6259, val_MinusLogProbMetric: 28.6259

Epoch 904: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1429 - MinusLogProbMetric: 28.1429 - val_loss: 28.6259 - val_MinusLogProbMetric: 28.6259 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 905/1000
2023-10-12 16:21:14.661 
Epoch 905/1000 
	 loss: 28.1419, MinusLogProbMetric: 28.1419, val_loss: 28.6231, val_MinusLogProbMetric: 28.6231

Epoch 905: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1419 - MinusLogProbMetric: 28.1419 - val_loss: 28.6231 - val_MinusLogProbMetric: 28.6231 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 906/1000
2023-10-12 16:21:48.089 
Epoch 906/1000 
	 loss: 28.1390, MinusLogProbMetric: 28.1390, val_loss: 28.6235, val_MinusLogProbMetric: 28.6235

Epoch 906: val_loss did not improve from 28.61367
196/196 - 33s - loss: 28.1390 - MinusLogProbMetric: 28.1390 - val_loss: 28.6235 - val_MinusLogProbMetric: 28.6235 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 907/1000
2023-10-12 16:22:21.818 
Epoch 907/1000 
	 loss: 28.1415, MinusLogProbMetric: 28.1415, val_loss: 28.6246, val_MinusLogProbMetric: 28.6246

Epoch 907: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1415 - MinusLogProbMetric: 28.1415 - val_loss: 28.6246 - val_MinusLogProbMetric: 28.6246 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 908/1000
2023-10-12 16:22:55.399 
Epoch 908/1000 
	 loss: 28.1443, MinusLogProbMetric: 28.1443, val_loss: 28.6641, val_MinusLogProbMetric: 28.6641

Epoch 908: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1443 - MinusLogProbMetric: 28.1443 - val_loss: 28.6641 - val_MinusLogProbMetric: 28.6641 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 909/1000
2023-10-12 16:23:29.486 
Epoch 909/1000 
	 loss: 28.1414, MinusLogProbMetric: 28.1414, val_loss: 28.6203, val_MinusLogProbMetric: 28.6203

Epoch 909: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1414 - MinusLogProbMetric: 28.1414 - val_loss: 28.6203 - val_MinusLogProbMetric: 28.6203 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 910/1000
2023-10-12 16:24:03.713 
Epoch 910/1000 
	 loss: 28.1384, MinusLogProbMetric: 28.1384, val_loss: 28.6277, val_MinusLogProbMetric: 28.6277

Epoch 910: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1384 - MinusLogProbMetric: 28.1384 - val_loss: 28.6277 - val_MinusLogProbMetric: 28.6277 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 911/1000
2023-10-12 16:24:37.817 
Epoch 911/1000 
	 loss: 28.1406, MinusLogProbMetric: 28.1406, val_loss: 28.6475, val_MinusLogProbMetric: 28.6475

Epoch 911: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1406 - MinusLogProbMetric: 28.1406 - val_loss: 28.6475 - val_MinusLogProbMetric: 28.6475 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 912/1000
2023-10-12 16:25:11.470 
Epoch 912/1000 
	 loss: 28.1404, MinusLogProbMetric: 28.1404, val_loss: 28.6221, val_MinusLogProbMetric: 28.6221

Epoch 912: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1404 - MinusLogProbMetric: 28.1404 - val_loss: 28.6221 - val_MinusLogProbMetric: 28.6221 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 913/1000
2023-10-12 16:25:45.052 
Epoch 913/1000 
	 loss: 28.1434, MinusLogProbMetric: 28.1434, val_loss: 28.6244, val_MinusLogProbMetric: 28.6244

Epoch 913: val_loss did not improve from 28.61367
196/196 - 34s - loss: 28.1434 - MinusLogProbMetric: 28.1434 - val_loss: 28.6244 - val_MinusLogProbMetric: 28.6244 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 914/1000
2023-10-12 16:26:20.098 
Epoch 914/1000 
	 loss: 28.1404, MinusLogProbMetric: 28.1404, val_loss: 28.6066, val_MinusLogProbMetric: 28.6066

Epoch 914: val_loss improved from 28.61367 to 28.60661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 28.1404 - MinusLogProbMetric: 28.1404 - val_loss: 28.6066 - val_MinusLogProbMetric: 28.6066 - lr: 2.0833e-05 - 36s/epoch - 182ms/step
Epoch 915/1000
2023-10-12 16:26:55.322 
Epoch 915/1000 
	 loss: 28.1392, MinusLogProbMetric: 28.1392, val_loss: 28.6304, val_MinusLogProbMetric: 28.6304

Epoch 915: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1392 - MinusLogProbMetric: 28.1392 - val_loss: 28.6304 - val_MinusLogProbMetric: 28.6304 - lr: 2.0833e-05 - 35s/epoch - 177ms/step
Epoch 916/1000
2023-10-12 16:27:29.673 
Epoch 916/1000 
	 loss: 28.1403, MinusLogProbMetric: 28.1403, val_loss: 28.6496, val_MinusLogProbMetric: 28.6496

Epoch 916: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1403 - MinusLogProbMetric: 28.1403 - val_loss: 28.6496 - val_MinusLogProbMetric: 28.6496 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 917/1000
2023-10-12 16:28:03.597 
Epoch 917/1000 
	 loss: 28.1436, MinusLogProbMetric: 28.1436, val_loss: 28.6163, val_MinusLogProbMetric: 28.6163

Epoch 917: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1436 - MinusLogProbMetric: 28.1436 - val_loss: 28.6163 - val_MinusLogProbMetric: 28.6163 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 918/1000
2023-10-12 16:28:37.748 
Epoch 918/1000 
	 loss: 28.1418, MinusLogProbMetric: 28.1418, val_loss: 28.6266, val_MinusLogProbMetric: 28.6266

Epoch 918: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1418 - MinusLogProbMetric: 28.1418 - val_loss: 28.6266 - val_MinusLogProbMetric: 28.6266 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 919/1000
2023-10-12 16:29:11.738 
Epoch 919/1000 
	 loss: 28.1421, MinusLogProbMetric: 28.1421, val_loss: 28.6647, val_MinusLogProbMetric: 28.6647

Epoch 919: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1421 - MinusLogProbMetric: 28.1421 - val_loss: 28.6647 - val_MinusLogProbMetric: 28.6647 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 920/1000
2023-10-12 16:29:45.441 
Epoch 920/1000 
	 loss: 28.1429, MinusLogProbMetric: 28.1429, val_loss: 28.6264, val_MinusLogProbMetric: 28.6264

Epoch 920: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1429 - MinusLogProbMetric: 28.1429 - val_loss: 28.6264 - val_MinusLogProbMetric: 28.6264 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 921/1000
2023-10-12 16:30:19.322 
Epoch 921/1000 
	 loss: 28.1409, MinusLogProbMetric: 28.1409, val_loss: 28.6168, val_MinusLogProbMetric: 28.6168

Epoch 921: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1409 - MinusLogProbMetric: 28.1409 - val_loss: 28.6168 - val_MinusLogProbMetric: 28.6168 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 922/1000
2023-10-12 16:30:53.027 
Epoch 922/1000 
	 loss: 28.1445, MinusLogProbMetric: 28.1445, val_loss: 28.6129, val_MinusLogProbMetric: 28.6129

Epoch 922: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1445 - MinusLogProbMetric: 28.1445 - val_loss: 28.6129 - val_MinusLogProbMetric: 28.6129 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 923/1000
2023-10-12 16:31:26.854 
Epoch 923/1000 
	 loss: 28.1423, MinusLogProbMetric: 28.1423, val_loss: 28.6255, val_MinusLogProbMetric: 28.6255

Epoch 923: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1423 - MinusLogProbMetric: 28.1423 - val_loss: 28.6255 - val_MinusLogProbMetric: 28.6255 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 924/1000
2023-10-12 16:32:00.661 
Epoch 924/1000 
	 loss: 28.1374, MinusLogProbMetric: 28.1374, val_loss: 28.6378, val_MinusLogProbMetric: 28.6378

Epoch 924: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1374 - MinusLogProbMetric: 28.1374 - val_loss: 28.6378 - val_MinusLogProbMetric: 28.6378 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 925/1000
2023-10-12 16:32:34.530 
Epoch 925/1000 
	 loss: 28.1468, MinusLogProbMetric: 28.1468, val_loss: 28.6170, val_MinusLogProbMetric: 28.6170

Epoch 925: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1468 - MinusLogProbMetric: 28.1468 - val_loss: 28.6170 - val_MinusLogProbMetric: 28.6170 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 926/1000
2023-10-12 16:33:08.658 
Epoch 926/1000 
	 loss: 28.1434, MinusLogProbMetric: 28.1434, val_loss: 28.6146, val_MinusLogProbMetric: 28.6146

Epoch 926: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1434 - MinusLogProbMetric: 28.1434 - val_loss: 28.6146 - val_MinusLogProbMetric: 28.6146 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 927/1000
2023-10-12 16:33:42.976 
Epoch 927/1000 
	 loss: 28.1387, MinusLogProbMetric: 28.1387, val_loss: 28.6389, val_MinusLogProbMetric: 28.6389

Epoch 927: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1387 - MinusLogProbMetric: 28.1387 - val_loss: 28.6389 - val_MinusLogProbMetric: 28.6389 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 928/1000
2023-10-12 16:34:16.798 
Epoch 928/1000 
	 loss: 28.1487, MinusLogProbMetric: 28.1487, val_loss: 28.6735, val_MinusLogProbMetric: 28.6735

Epoch 928: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1487 - MinusLogProbMetric: 28.1487 - val_loss: 28.6735 - val_MinusLogProbMetric: 28.6735 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 929/1000
2023-10-12 16:34:52.097 
Epoch 929/1000 
	 loss: 28.1396, MinusLogProbMetric: 28.1396, val_loss: 28.6343, val_MinusLogProbMetric: 28.6343

Epoch 929: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1396 - MinusLogProbMetric: 28.1396 - val_loss: 28.6343 - val_MinusLogProbMetric: 28.6343 - lr: 2.0833e-05 - 35s/epoch - 180ms/step
Epoch 930/1000
2023-10-12 16:35:26.858 
Epoch 930/1000 
	 loss: 28.1426, MinusLogProbMetric: 28.1426, val_loss: 28.6187, val_MinusLogProbMetric: 28.6187

Epoch 930: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1426 - MinusLogProbMetric: 28.1426 - val_loss: 28.6187 - val_MinusLogProbMetric: 28.6187 - lr: 2.0833e-05 - 35s/epoch - 177ms/step
Epoch 931/1000
2023-10-12 16:36:02.436 
Epoch 931/1000 
	 loss: 28.1439, MinusLogProbMetric: 28.1439, val_loss: 28.6220, val_MinusLogProbMetric: 28.6220

Epoch 931: val_loss did not improve from 28.60661
196/196 - 36s - loss: 28.1439 - MinusLogProbMetric: 28.1439 - val_loss: 28.6220 - val_MinusLogProbMetric: 28.6220 - lr: 2.0833e-05 - 36s/epoch - 181ms/step
Epoch 932/1000
2023-10-12 16:36:38.764 
Epoch 932/1000 
	 loss: 28.1409, MinusLogProbMetric: 28.1409, val_loss: 28.6360, val_MinusLogProbMetric: 28.6360

Epoch 932: val_loss did not improve from 28.60661
196/196 - 36s - loss: 28.1409 - MinusLogProbMetric: 28.1409 - val_loss: 28.6360 - val_MinusLogProbMetric: 28.6360 - lr: 2.0833e-05 - 36s/epoch - 186ms/step
Epoch 933/1000
2023-10-12 16:37:13.953 
Epoch 933/1000 
	 loss: 28.1443, MinusLogProbMetric: 28.1443, val_loss: 28.6337, val_MinusLogProbMetric: 28.6337

Epoch 933: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1443 - MinusLogProbMetric: 28.1443 - val_loss: 28.6337 - val_MinusLogProbMetric: 28.6337 - lr: 2.0833e-05 - 35s/epoch - 179ms/step
Epoch 934/1000
2023-10-12 16:37:48.649 
Epoch 934/1000 
	 loss: 28.1396, MinusLogProbMetric: 28.1396, val_loss: 28.6210, val_MinusLogProbMetric: 28.6210

Epoch 934: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1396 - MinusLogProbMetric: 28.1396 - val_loss: 28.6210 - val_MinusLogProbMetric: 28.6210 - lr: 2.0833e-05 - 35s/epoch - 177ms/step
Epoch 935/1000
2023-10-12 16:38:23.194 
Epoch 935/1000 
	 loss: 28.1417, MinusLogProbMetric: 28.1417, val_loss: 28.6427, val_MinusLogProbMetric: 28.6427

Epoch 935: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1417 - MinusLogProbMetric: 28.1417 - val_loss: 28.6427 - val_MinusLogProbMetric: 28.6427 - lr: 2.0833e-05 - 35s/epoch - 176ms/step
Epoch 936/1000
2023-10-12 16:38:57.069 
Epoch 936/1000 
	 loss: 28.1395, MinusLogProbMetric: 28.1395, val_loss: 28.6305, val_MinusLogProbMetric: 28.6305

Epoch 936: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1395 - MinusLogProbMetric: 28.1395 - val_loss: 28.6305 - val_MinusLogProbMetric: 28.6305 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 937/1000
2023-10-12 16:39:31.115 
Epoch 937/1000 
	 loss: 28.1403, MinusLogProbMetric: 28.1403, val_loss: 28.6341, val_MinusLogProbMetric: 28.6341

Epoch 937: val_loss did not improve from 28.60661
196/196 - 34s - loss: 28.1403 - MinusLogProbMetric: 28.1403 - val_loss: 28.6341 - val_MinusLogProbMetric: 28.6341 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 938/1000
2023-10-12 16:40:06.252 
Epoch 938/1000 
	 loss: 28.1443, MinusLogProbMetric: 28.1443, val_loss: 28.6268, val_MinusLogProbMetric: 28.6268

Epoch 938: val_loss did not improve from 28.60661
196/196 - 35s - loss: 28.1443 - MinusLogProbMetric: 28.1443 - val_loss: 28.6268 - val_MinusLogProbMetric: 28.6268 - lr: 2.0833e-05 - 35s/epoch - 179ms/step
Epoch 939/1000
2023-10-12 16:40:40.582 
Epoch 939/1000 
	 loss: 28.1433, MinusLogProbMetric: 28.1433, val_loss: 28.5991, val_MinusLogProbMetric: 28.5991

Epoch 939: val_loss improved from 28.60661 to 28.59914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 28.1433 - MinusLogProbMetric: 28.1433 - val_loss: 28.5991 - val_MinusLogProbMetric: 28.5991 - lr: 2.0833e-05 - 35s/epoch - 180ms/step
Epoch 940/1000
2023-10-12 16:41:15.272 
Epoch 940/1000 
	 loss: 28.1416, MinusLogProbMetric: 28.1416, val_loss: 28.6314, val_MinusLogProbMetric: 28.6314

Epoch 940: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1416 - MinusLogProbMetric: 28.1416 - val_loss: 28.6314 - val_MinusLogProbMetric: 28.6314 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 941/1000
2023-10-12 16:41:49.608 
Epoch 941/1000 
	 loss: 28.1410, MinusLogProbMetric: 28.1410, val_loss: 28.6256, val_MinusLogProbMetric: 28.6256

Epoch 941: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1410 - MinusLogProbMetric: 28.1410 - val_loss: 28.6256 - val_MinusLogProbMetric: 28.6256 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 942/1000
2023-10-12 16:42:24.001 
Epoch 942/1000 
	 loss: 28.1422, MinusLogProbMetric: 28.1422, val_loss: 28.6232, val_MinusLogProbMetric: 28.6232

Epoch 942: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1422 - MinusLogProbMetric: 28.1422 - val_loss: 28.6232 - val_MinusLogProbMetric: 28.6232 - lr: 2.0833e-05 - 34s/epoch - 176ms/step
Epoch 943/1000
2023-10-12 16:42:56.833 
Epoch 943/1000 
	 loss: 28.1415, MinusLogProbMetric: 28.1415, val_loss: 28.6338, val_MinusLogProbMetric: 28.6338

Epoch 943: val_loss did not improve from 28.59914
196/196 - 33s - loss: 28.1415 - MinusLogProbMetric: 28.1415 - val_loss: 28.6338 - val_MinusLogProbMetric: 28.6338 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 944/1000
2023-10-12 16:43:30.923 
Epoch 944/1000 
	 loss: 28.1403, MinusLogProbMetric: 28.1403, val_loss: 28.6347, val_MinusLogProbMetric: 28.6347

Epoch 944: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1403 - MinusLogProbMetric: 28.1403 - val_loss: 28.6347 - val_MinusLogProbMetric: 28.6347 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 945/1000
2023-10-12 16:44:04.402 
Epoch 945/1000 
	 loss: 28.1446, MinusLogProbMetric: 28.1446, val_loss: 28.6213, val_MinusLogProbMetric: 28.6213

Epoch 945: val_loss did not improve from 28.59914
196/196 - 33s - loss: 28.1446 - MinusLogProbMetric: 28.1446 - val_loss: 28.6213 - val_MinusLogProbMetric: 28.6213 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 946/1000
2023-10-12 16:44:38.583 
Epoch 946/1000 
	 loss: 28.1448, MinusLogProbMetric: 28.1448, val_loss: 28.6217, val_MinusLogProbMetric: 28.6217

Epoch 946: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1448 - MinusLogProbMetric: 28.1448 - val_loss: 28.6217 - val_MinusLogProbMetric: 28.6217 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 947/1000
2023-10-12 16:45:12.447 
Epoch 947/1000 
	 loss: 28.1390, MinusLogProbMetric: 28.1390, val_loss: 28.6202, val_MinusLogProbMetric: 28.6202

Epoch 947: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1390 - MinusLogProbMetric: 28.1390 - val_loss: 28.6202 - val_MinusLogProbMetric: 28.6202 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 948/1000
2023-10-12 16:45:46.391 
Epoch 948/1000 
	 loss: 28.1414, MinusLogProbMetric: 28.1414, val_loss: 28.6269, val_MinusLogProbMetric: 28.6269

Epoch 948: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1414 - MinusLogProbMetric: 28.1414 - val_loss: 28.6269 - val_MinusLogProbMetric: 28.6269 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 949/1000
2023-10-12 16:46:20.109 
Epoch 949/1000 
	 loss: 28.1378, MinusLogProbMetric: 28.1378, val_loss: 28.6090, val_MinusLogProbMetric: 28.6090

Epoch 949: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1378 - MinusLogProbMetric: 28.1378 - val_loss: 28.6090 - val_MinusLogProbMetric: 28.6090 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 950/1000
2023-10-12 16:46:54.261 
Epoch 950/1000 
	 loss: 28.1418, MinusLogProbMetric: 28.1418, val_loss: 28.6491, val_MinusLogProbMetric: 28.6491

Epoch 950: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1418 - MinusLogProbMetric: 28.1418 - val_loss: 28.6491 - val_MinusLogProbMetric: 28.6491 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 951/1000
2023-10-12 16:47:28.211 
Epoch 951/1000 
	 loss: 28.1399, MinusLogProbMetric: 28.1399, val_loss: 28.6305, val_MinusLogProbMetric: 28.6305

Epoch 951: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1399 - MinusLogProbMetric: 28.1399 - val_loss: 28.6305 - val_MinusLogProbMetric: 28.6305 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 952/1000
2023-10-12 16:48:02.247 
Epoch 952/1000 
	 loss: 28.1406, MinusLogProbMetric: 28.1406, val_loss: 28.6160, val_MinusLogProbMetric: 28.6160

Epoch 952: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1406 - MinusLogProbMetric: 28.1406 - val_loss: 28.6160 - val_MinusLogProbMetric: 28.6160 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 953/1000
2023-10-12 16:48:36.102 
Epoch 953/1000 
	 loss: 28.1383, MinusLogProbMetric: 28.1383, val_loss: 28.6431, val_MinusLogProbMetric: 28.6431

Epoch 953: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1383 - MinusLogProbMetric: 28.1383 - val_loss: 28.6431 - val_MinusLogProbMetric: 28.6431 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 954/1000
2023-10-12 16:49:10.135 
Epoch 954/1000 
	 loss: 28.1369, MinusLogProbMetric: 28.1369, val_loss: 28.6298, val_MinusLogProbMetric: 28.6298

Epoch 954: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1369 - MinusLogProbMetric: 28.1369 - val_loss: 28.6298 - val_MinusLogProbMetric: 28.6298 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 955/1000
2023-10-12 16:49:43.995 
Epoch 955/1000 
	 loss: 28.1376, MinusLogProbMetric: 28.1376, val_loss: 28.6336, val_MinusLogProbMetric: 28.6336

Epoch 955: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1376 - MinusLogProbMetric: 28.1376 - val_loss: 28.6336 - val_MinusLogProbMetric: 28.6336 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 956/1000
2023-10-12 16:50:18.133 
Epoch 956/1000 
	 loss: 28.1406, MinusLogProbMetric: 28.1406, val_loss: 28.6238, val_MinusLogProbMetric: 28.6238

Epoch 956: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1406 - MinusLogProbMetric: 28.1406 - val_loss: 28.6238 - val_MinusLogProbMetric: 28.6238 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 957/1000
2023-10-12 16:50:51.639 
Epoch 957/1000 
	 loss: 28.1424, MinusLogProbMetric: 28.1424, val_loss: 28.6531, val_MinusLogProbMetric: 28.6531

Epoch 957: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1424 - MinusLogProbMetric: 28.1424 - val_loss: 28.6531 - val_MinusLogProbMetric: 28.6531 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 958/1000
2023-10-12 16:51:25.434 
Epoch 958/1000 
	 loss: 28.1369, MinusLogProbMetric: 28.1369, val_loss: 28.6267, val_MinusLogProbMetric: 28.6267

Epoch 958: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1369 - MinusLogProbMetric: 28.1369 - val_loss: 28.6267 - val_MinusLogProbMetric: 28.6267 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 959/1000
2023-10-12 16:51:59.687 
Epoch 959/1000 
	 loss: 28.1380, MinusLogProbMetric: 28.1380, val_loss: 28.6242, val_MinusLogProbMetric: 28.6242

Epoch 959: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1380 - MinusLogProbMetric: 28.1380 - val_loss: 28.6242 - val_MinusLogProbMetric: 28.6242 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 960/1000
2023-10-12 16:52:33.434 
Epoch 960/1000 
	 loss: 28.1357, MinusLogProbMetric: 28.1357, val_loss: 28.6445, val_MinusLogProbMetric: 28.6445

Epoch 960: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1357 - MinusLogProbMetric: 28.1357 - val_loss: 28.6445 - val_MinusLogProbMetric: 28.6445 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 961/1000
2023-10-12 16:53:07.517 
Epoch 961/1000 
	 loss: 28.1391, MinusLogProbMetric: 28.1391, val_loss: 28.6207, val_MinusLogProbMetric: 28.6207

Epoch 961: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1391 - MinusLogProbMetric: 28.1391 - val_loss: 28.6207 - val_MinusLogProbMetric: 28.6207 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 962/1000
2023-10-12 16:53:40.960 
Epoch 962/1000 
	 loss: 28.1377, MinusLogProbMetric: 28.1377, val_loss: 28.6199, val_MinusLogProbMetric: 28.6199

Epoch 962: val_loss did not improve from 28.59914
196/196 - 33s - loss: 28.1377 - MinusLogProbMetric: 28.1377 - val_loss: 28.6199 - val_MinusLogProbMetric: 28.6199 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 963/1000
2023-10-12 16:54:13.305 
Epoch 963/1000 
	 loss: 28.1411, MinusLogProbMetric: 28.1411, val_loss: 28.6502, val_MinusLogProbMetric: 28.6502

Epoch 963: val_loss did not improve from 28.59914
196/196 - 32s - loss: 28.1411 - MinusLogProbMetric: 28.1411 - val_loss: 28.6502 - val_MinusLogProbMetric: 28.6502 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 964/1000
2023-10-12 16:54:45.695 
Epoch 964/1000 
	 loss: 28.1328, MinusLogProbMetric: 28.1328, val_loss: 28.6206, val_MinusLogProbMetric: 28.6206

Epoch 964: val_loss did not improve from 28.59914
196/196 - 32s - loss: 28.1328 - MinusLogProbMetric: 28.1328 - val_loss: 28.6206 - val_MinusLogProbMetric: 28.6206 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 965/1000
2023-10-12 16:55:18.011 
Epoch 965/1000 
	 loss: 28.1349, MinusLogProbMetric: 28.1349, val_loss: 28.6163, val_MinusLogProbMetric: 28.6163

Epoch 965: val_loss did not improve from 28.59914
196/196 - 32s - loss: 28.1349 - MinusLogProbMetric: 28.1349 - val_loss: 28.6163 - val_MinusLogProbMetric: 28.6163 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 966/1000
2023-10-12 16:55:51.641 
Epoch 966/1000 
	 loss: 28.1366, MinusLogProbMetric: 28.1366, val_loss: 28.6215, val_MinusLogProbMetric: 28.6215

Epoch 966: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1366 - MinusLogProbMetric: 28.1366 - val_loss: 28.6215 - val_MinusLogProbMetric: 28.6215 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 967/1000
2023-10-12 16:56:25.159 
Epoch 967/1000 
	 loss: 28.1381, MinusLogProbMetric: 28.1381, val_loss: 28.6172, val_MinusLogProbMetric: 28.6172

Epoch 967: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1381 - MinusLogProbMetric: 28.1381 - val_loss: 28.6172 - val_MinusLogProbMetric: 28.6172 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 968/1000
2023-10-12 16:56:59.043 
Epoch 968/1000 
	 loss: 28.1390, MinusLogProbMetric: 28.1390, val_loss: 28.6458, val_MinusLogProbMetric: 28.6458

Epoch 968: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1390 - MinusLogProbMetric: 28.1390 - val_loss: 28.6458 - val_MinusLogProbMetric: 28.6458 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 969/1000
2023-10-12 16:57:32.852 
Epoch 969/1000 
	 loss: 28.1374, MinusLogProbMetric: 28.1374, val_loss: 28.6390, val_MinusLogProbMetric: 28.6390

Epoch 969: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1374 - MinusLogProbMetric: 28.1374 - val_loss: 28.6390 - val_MinusLogProbMetric: 28.6390 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 970/1000
2023-10-12 16:58:06.721 
Epoch 970/1000 
	 loss: 28.1351, MinusLogProbMetric: 28.1351, val_loss: 28.6312, val_MinusLogProbMetric: 28.6312

Epoch 970: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1351 - MinusLogProbMetric: 28.1351 - val_loss: 28.6312 - val_MinusLogProbMetric: 28.6312 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 971/1000
2023-10-12 16:58:40.551 
Epoch 971/1000 
	 loss: 28.1363, MinusLogProbMetric: 28.1363, val_loss: 28.6155, val_MinusLogProbMetric: 28.6155

Epoch 971: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1363 - MinusLogProbMetric: 28.1363 - val_loss: 28.6155 - val_MinusLogProbMetric: 28.6155 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 972/1000
2023-10-12 16:59:14.611 
Epoch 972/1000 
	 loss: 28.1412, MinusLogProbMetric: 28.1412, val_loss: 28.6288, val_MinusLogProbMetric: 28.6288

Epoch 972: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1412 - MinusLogProbMetric: 28.1412 - val_loss: 28.6288 - val_MinusLogProbMetric: 28.6288 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 973/1000
2023-10-12 16:59:48.331 
Epoch 973/1000 
	 loss: 28.1367, MinusLogProbMetric: 28.1367, val_loss: 28.6176, val_MinusLogProbMetric: 28.6176

Epoch 973: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1367 - MinusLogProbMetric: 28.1367 - val_loss: 28.6176 - val_MinusLogProbMetric: 28.6176 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 974/1000
2023-10-12 17:00:21.904 
Epoch 974/1000 
	 loss: 28.1381, MinusLogProbMetric: 28.1381, val_loss: 28.6334, val_MinusLogProbMetric: 28.6334

Epoch 974: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1381 - MinusLogProbMetric: 28.1381 - val_loss: 28.6334 - val_MinusLogProbMetric: 28.6334 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 975/1000
2023-10-12 17:00:55.509 
Epoch 975/1000 
	 loss: 28.1335, MinusLogProbMetric: 28.1335, val_loss: 28.6133, val_MinusLogProbMetric: 28.6133

Epoch 975: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1335 - MinusLogProbMetric: 28.1335 - val_loss: 28.6133 - val_MinusLogProbMetric: 28.6133 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 976/1000
2023-10-12 17:01:28.556 
Epoch 976/1000 
	 loss: 28.1376, MinusLogProbMetric: 28.1376, val_loss: 28.6216, val_MinusLogProbMetric: 28.6216

Epoch 976: val_loss did not improve from 28.59914
196/196 - 33s - loss: 28.1376 - MinusLogProbMetric: 28.1376 - val_loss: 28.6216 - val_MinusLogProbMetric: 28.6216 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 977/1000
2023-10-12 17:02:00.775 
Epoch 977/1000 
	 loss: 28.1375, MinusLogProbMetric: 28.1375, val_loss: 28.6249, val_MinusLogProbMetric: 28.6249

Epoch 977: val_loss did not improve from 28.59914
196/196 - 32s - loss: 28.1375 - MinusLogProbMetric: 28.1375 - val_loss: 28.6249 - val_MinusLogProbMetric: 28.6249 - lr: 2.0833e-05 - 32s/epoch - 164ms/step
Epoch 978/1000
2023-10-12 17:02:34.558 
Epoch 978/1000 
	 loss: 28.1360, MinusLogProbMetric: 28.1360, val_loss: 28.6199, val_MinusLogProbMetric: 28.6199

Epoch 978: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1360 - MinusLogProbMetric: 28.1360 - val_loss: 28.6199 - val_MinusLogProbMetric: 28.6199 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 979/1000
2023-10-12 17:03:08.398 
Epoch 979/1000 
	 loss: 28.1369, MinusLogProbMetric: 28.1369, val_loss: 28.6630, val_MinusLogProbMetric: 28.6630

Epoch 979: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1369 - MinusLogProbMetric: 28.1369 - val_loss: 28.6630 - val_MinusLogProbMetric: 28.6630 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 980/1000
2023-10-12 17:03:42.425 
Epoch 980/1000 
	 loss: 28.1423, MinusLogProbMetric: 28.1423, val_loss: 28.6294, val_MinusLogProbMetric: 28.6294

Epoch 980: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1423 - MinusLogProbMetric: 28.1423 - val_loss: 28.6294 - val_MinusLogProbMetric: 28.6294 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 981/1000
2023-10-12 17:04:16.313 
Epoch 981/1000 
	 loss: 28.1361, MinusLogProbMetric: 28.1361, val_loss: 28.6247, val_MinusLogProbMetric: 28.6247

Epoch 981: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1361 - MinusLogProbMetric: 28.1361 - val_loss: 28.6247 - val_MinusLogProbMetric: 28.6247 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 982/1000
2023-10-12 17:04:50.573 
Epoch 982/1000 
	 loss: 28.1329, MinusLogProbMetric: 28.1329, val_loss: 28.6338, val_MinusLogProbMetric: 28.6338

Epoch 982: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1329 - MinusLogProbMetric: 28.1329 - val_loss: 28.6338 - val_MinusLogProbMetric: 28.6338 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 983/1000
2023-10-12 17:05:24.709 
Epoch 983/1000 
	 loss: 28.1363, MinusLogProbMetric: 28.1363, val_loss: 28.6406, val_MinusLogProbMetric: 28.6406

Epoch 983: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1363 - MinusLogProbMetric: 28.1363 - val_loss: 28.6406 - val_MinusLogProbMetric: 28.6406 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 984/1000
2023-10-12 17:05:58.356 
Epoch 984/1000 
	 loss: 28.1398, MinusLogProbMetric: 28.1398, val_loss: 28.6402, val_MinusLogProbMetric: 28.6402

Epoch 984: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1398 - MinusLogProbMetric: 28.1398 - val_loss: 28.6402 - val_MinusLogProbMetric: 28.6402 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 985/1000
2023-10-12 17:06:32.190 
Epoch 985/1000 
	 loss: 28.1366, MinusLogProbMetric: 28.1366, val_loss: 28.6315, val_MinusLogProbMetric: 28.6315

Epoch 985: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1366 - MinusLogProbMetric: 28.1366 - val_loss: 28.6315 - val_MinusLogProbMetric: 28.6315 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 986/1000
2023-10-12 17:07:06.081 
Epoch 986/1000 
	 loss: 28.1402, MinusLogProbMetric: 28.1402, val_loss: 28.6086, val_MinusLogProbMetric: 28.6086

Epoch 986: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1402 - MinusLogProbMetric: 28.1402 - val_loss: 28.6086 - val_MinusLogProbMetric: 28.6086 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 987/1000
2023-10-12 17:07:40.301 
Epoch 987/1000 
	 loss: 28.1392, MinusLogProbMetric: 28.1392, val_loss: 28.6441, val_MinusLogProbMetric: 28.6441

Epoch 987: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1392 - MinusLogProbMetric: 28.1392 - val_loss: 28.6441 - val_MinusLogProbMetric: 28.6441 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 988/1000
2023-10-12 17:08:14.236 
Epoch 988/1000 
	 loss: 28.1356, MinusLogProbMetric: 28.1356, val_loss: 28.6318, val_MinusLogProbMetric: 28.6318

Epoch 988: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1356 - MinusLogProbMetric: 28.1356 - val_loss: 28.6318 - val_MinusLogProbMetric: 28.6318 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 989/1000
2023-10-12 17:08:48.173 
Epoch 989/1000 
	 loss: 28.1367, MinusLogProbMetric: 28.1367, val_loss: 28.6406, val_MinusLogProbMetric: 28.6406

Epoch 989: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1367 - MinusLogProbMetric: 28.1367 - val_loss: 28.6406 - val_MinusLogProbMetric: 28.6406 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 990/1000
2023-10-12 17:09:21.801 
Epoch 990/1000 
	 loss: 28.1185, MinusLogProbMetric: 28.1185, val_loss: 28.6049, val_MinusLogProbMetric: 28.6049

Epoch 990: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1185 - MinusLogProbMetric: 28.1185 - val_loss: 28.6049 - val_MinusLogProbMetric: 28.6049 - lr: 1.0417e-05 - 34s/epoch - 172ms/step
Epoch 991/1000
2023-10-12 17:09:55.671 
Epoch 991/1000 
	 loss: 28.1207, MinusLogProbMetric: 28.1207, val_loss: 28.6092, val_MinusLogProbMetric: 28.6092

Epoch 991: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1207 - MinusLogProbMetric: 28.1207 - val_loss: 28.6092 - val_MinusLogProbMetric: 28.6092 - lr: 1.0417e-05 - 34s/epoch - 173ms/step
Epoch 992/1000
2023-10-12 17:10:29.680 
Epoch 992/1000 
	 loss: 28.1186, MinusLogProbMetric: 28.1186, val_loss: 28.6264, val_MinusLogProbMetric: 28.6264

Epoch 992: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1186 - MinusLogProbMetric: 28.1186 - val_loss: 28.6264 - val_MinusLogProbMetric: 28.6264 - lr: 1.0417e-05 - 34s/epoch - 174ms/step
Epoch 993/1000
2023-10-12 17:11:03.481 
Epoch 993/1000 
	 loss: 28.1195, MinusLogProbMetric: 28.1195, val_loss: 28.6088, val_MinusLogProbMetric: 28.6088

Epoch 993: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1195 - MinusLogProbMetric: 28.1195 - val_loss: 28.6088 - val_MinusLogProbMetric: 28.6088 - lr: 1.0417e-05 - 34s/epoch - 172ms/step
Epoch 994/1000
2023-10-12 17:11:37.428 
Epoch 994/1000 
	 loss: 28.1165, MinusLogProbMetric: 28.1165, val_loss: 28.6210, val_MinusLogProbMetric: 28.6210

Epoch 994: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1165 - MinusLogProbMetric: 28.1165 - val_loss: 28.6210 - val_MinusLogProbMetric: 28.6210 - lr: 1.0417e-05 - 34s/epoch - 173ms/step
Epoch 995/1000
2023-10-12 17:12:11.142 
Epoch 995/1000 
	 loss: 28.1181, MinusLogProbMetric: 28.1181, val_loss: 28.6112, val_MinusLogProbMetric: 28.6112

Epoch 995: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1181 - MinusLogProbMetric: 28.1181 - val_loss: 28.6112 - val_MinusLogProbMetric: 28.6112 - lr: 1.0417e-05 - 34s/epoch - 172ms/step
Epoch 996/1000
2023-10-12 17:12:45.120 
Epoch 996/1000 
	 loss: 28.1176, MinusLogProbMetric: 28.1176, val_loss: 28.6069, val_MinusLogProbMetric: 28.6069

Epoch 996: val_loss did not improve from 28.59914
196/196 - 34s - loss: 28.1176 - MinusLogProbMetric: 28.1176 - val_loss: 28.6069 - val_MinusLogProbMetric: 28.6069 - lr: 1.0417e-05 - 34s/epoch - 173ms/step
Epoch 997/1000
