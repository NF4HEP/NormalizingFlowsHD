2023-09-09 22:36:15.746024: Importing os...
2023-09-09 22:36:15.746106: Importing sys...
2023-09-09 22:36:15.746113: Importing and initializing argparse...
Visible devices: [1]
2023-09-09 22:36:15.778330: Importing timer from timeit...
2023-09-09 22:36:15.779051: Setting env variables for tf import (only device [1] will be available)...
2023-09-09 22:36:15.779105: Importing numpy...
2023-09-09 22:36:16.004245: Importing pandas...
2023-09-09 22:36:16.528087: Importing shutil...
2023-09-09 22:36:16.528259: Importing subprocess...
2023-09-09 22:36:16.528276: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-09 22:36:23.081009: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-09 22:36:24.151489: Importing textwrap...
2023-09-09 22:36:24.151536: Importing timeit...
2023-09-09 22:36:24.151547: Importing traceback...
2023-09-09 22:36:24.151556: Importing typing...
2023-09-09 22:36:24.151571: Setting tf configs...
2023-09-09 22:36:24.586135: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-09 22:36:26.753970: All modues imported successfully.
Directory ../../results/MAFN_new/ already exists.
Directory ../../results/MAFN_new/run_1/ already exists.
Skipping it.
===========
Run 1/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_2/ already exists.
Skipping it.
===========
Run 2/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_3/ already exists.
Skipping it.
===========
Run 3/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_4/ already exists.
Skipping it.
===========
Run 4/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_5/ already exists.
Skipping it.
===========
Run 5/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_6/ already exists.
Skipping it.
===========
Run 6/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_7/ already exists.
Skipping it.
===========
Run 7/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_8/ already exists.
Skipping it.
===========
Run 8/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_9/ already exists.
Skipping it.
===========
Run 9/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_10/ already exists.
Skipping it.
===========
Run 10/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_11/ already exists.
Skipping it.
===========
Run 11/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_12/ already exists.
Skipping it.
===========
Run 12/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_13/ already exists.
Skipping it.
===========
Run 13/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_14/ already exists.
Skipping it.
===========
Run 14/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_15/ already exists.
Skipping it.
===========
Run 15/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_16/ already exists.
Skipping it.
===========
Run 16/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_17/ already exists.
Skipping it.
===========
Run 17/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_18/ already exists.
Skipping it.
===========
Run 18/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_19/ already exists.
Skipping it.
===========
Run 19/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_20/ already exists.
Skipping it.
===========
Run 20/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_21/ already exists.
Skipping it.
===========
Run 21/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_22/ already exists.
Skipping it.
===========
Run 22/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_23/ already exists.
Skipping it.
===========
Run 23/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_24/ already exists.
Skipping it.
===========
Run 24/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_25/ already exists.
Skipping it.
===========
Run 25/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_26/ already exists.
Skipping it.
===========
Run 26/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_27/ already exists.
Skipping it.
===========
Run 27/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_28/ already exists.
Skipping it.
===========
Run 28/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_29/ already exists.
Skipping it.
===========
Run 29/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_30/ already exists.
Skipping it.
===========
Run 30/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_31/ already exists.
Skipping it.
===========
Run 31/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_32/ already exists.
Skipping it.
===========
Run 32/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_33/ already exists.
Skipping it.
===========
Run 33/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_34/ already exists.
Skipping it.
===========
Run 34/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_35/ already exists.
Skipping it.
===========
Run 35/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_36/ already exists.
Skipping it.
===========
Run 36/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_37/ already exists.
Skipping it.
===========
Run 37/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_38/ already exists.
Skipping it.
===========
Run 38/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_39/ already exists.
Skipping it.
===========
Run 39/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_40/ already exists.
Skipping it.
===========
Run 40/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_41/ already exists.
Skipping it.
===========
Run 41/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_42/ already exists.
Skipping it.
===========
Run 42/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_43/ already exists.
Skipping it.
===========
Run 43/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_44/ already exists.
Skipping it.
===========
Run 44/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_45/ already exists.
Skipping it.
===========
Run 45/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_46/ already exists.
Skipping it.
===========
Run 46/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_47/ already exists.
Skipping it.
===========
Run 47/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_48/ already exists.
Skipping it.
===========
Run 48/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_49/ already exists.
Skipping it.
===========
Run 49/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_50/ already exists.
Skipping it.
===========
Run 50/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_51/ already exists.
Skipping it.
===========
Run 51/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_52/ already exists.
Skipping it.
===========
Run 52/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_53/ already exists.
Skipping it.
===========
Run 53/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_54/ already exists.
Skipping it.
===========
Run 54/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_55/ already exists.
Skipping it.
===========
Run 55/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_56/ already exists.
Skipping it.
===========
Run 56/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_57/ already exists.
Skipping it.
===========
Run 57/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_58/ already exists.
Skipping it.
===========
Run 58/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_59/ already exists.
Skipping it.
===========
Run 59/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_60/ already exists.
Skipping it.
===========
Run 60/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_61/ already exists.
Skipping it.
===========
Run 61/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_62/ already exists.
Skipping it.
===========
Run 62/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_63/ already exists.
Skipping it.
===========
Run 63/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_64/ already exists.
Skipping it.
===========
Run 64/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_65/ already exists.
Skipping it.
===========
Run 65/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_66/ already exists.
Skipping it.
===========
Run 66/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_67/ already exists.
Skipping it.
===========
Run 67/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_68/ already exists.
Skipping it.
===========
Run 68/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_69/ already exists.
Skipping it.
===========
Run 69/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_70/ already exists.
Skipping it.
===========
Run 70/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_71/ already exists.
Skipping it.
===========
Run 71/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_72/ already exists.
Skipping it.
===========
Run 72/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_73/ already exists.
Skipping it.
===========
Run 73/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_74/ already exists.
Skipping it.
===========
Run 74/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_75/ already exists.
Skipping it.
===========
Run 75/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_76/ already exists.
Skipping it.
===========
Run 76/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_77/ already exists.
Skipping it.
===========
Run 77/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_78/ already exists.
Skipping it.
===========
Run 78/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_79/ already exists.
Skipping it.
===========
Run 79/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_80/ already exists.
Skipping it.
===========
Run 80/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_81/ already exists.
Skipping it.
===========
Run 81/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_82/ already exists.
Skipping it.
===========
Run 82/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_83/ already exists.
Skipping it.
===========
Run 83/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_84/ already exists.
Skipping it.
===========
Run 84/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_85/ already exists.
Skipping it.
===========
Run 85/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_86/ already exists.
Skipping it.
===========
Run 86/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_87/ already exists.
Skipping it.
===========
Run 87/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_88/ already exists.
Skipping it.
===========
Run 88/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_89/ already exists.
Skipping it.
===========
Run 89/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_90/ already exists.
Skipping it.
===========
Run 90/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_91/ already exists.
Skipping it.
===========
Run 91/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_92/ already exists.
Skipping it.
===========
Run 92/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_93/ already exists.
Skipping it.
===========
Run 93/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_94/ already exists.
Skipping it.
===========
Run 94/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_95/ already exists.
Skipping it.
===========
Run 95/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_96/ already exists.
Skipping it.
===========
Run 96/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_97/ already exists.
Skipping it.
===========
Run 97/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_98/ already exists.
Skipping it.
===========
Run 98/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_99/ already exists.
Skipping it.
===========
Run 99/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_100/ already exists.
Skipping it.
===========
Run 100/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_101/ already exists.
Skipping it.
===========
Run 101/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_102/ already exists.
Skipping it.
===========
Run 102/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_103/ already exists.
Skipping it.
===========
Run 103/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_104/ already exists.
Skipping it.
===========
Run 104/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_105/ already exists.
Skipping it.
===========
Run 105/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_106/ already exists.
Skipping it.
===========
Run 106/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_107/ already exists.
Skipping it.
===========
Run 107/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_108/ already exists.
Skipping it.
===========
Run 108/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_109/ already exists.
Skipping it.
===========
Run 109/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_110/ already exists.
Skipping it.
===========
Run 110/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_111/ already exists.
Skipping it.
===========
Run 111/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_112/ already exists.
Skipping it.
===========
Run 112/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_113/ already exists.
Skipping it.
===========
Run 113/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_114/ already exists.
Skipping it.
===========
Run 114/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_115/ already exists.
Skipping it.
===========
Run 115/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_116/ already exists.
Skipping it.
===========
Run 116/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_117/ already exists.
Skipping it.
===========
Run 117/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_118/ already exists.
Skipping it.
===========
Run 118/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_119/ already exists.
Skipping it.
===========
Run 119/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_120/ already exists.
Skipping it.
===========
Run 120/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_121/ already exists.
Skipping it.
===========
Run 121/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_122/ already exists.
Skipping it.
===========
Run 122/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_123/ already exists.
Skipping it.
===========
Run 123/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_124/ already exists.
Skipping it.
===========
Run 124/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_125/ already exists.
Skipping it.
===========
Run 125/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_126/ already exists.
Skipping it.
===========
Run 126/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_127/ already exists.
Skipping it.
===========
Run 127/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_128/ already exists.
Skipping it.
===========
Run 128/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_129/ already exists.
Skipping it.
===========
Run 129/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_130/ already exists.
Skipping it.
===========
Run 130/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_131/ already exists.
Skipping it.
===========
Run 131/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_132/ already exists.
Skipping it.
===========
Run 132/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_133/ already exists.
Skipping it.
===========
Run 133/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_134/ already exists.
Skipping it.
===========
Run 134/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_135/ already exists.
Skipping it.
===========
Run 135/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_136/ already exists.
Skipping it.
===========
Run 136/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_137/ already exists.
Skipping it.
===========
Run 137/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_138/ already exists.
Skipping it.
===========
Run 138/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_139/ already exists.
Skipping it.
===========
Run 139/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_140/ already exists.
Skipping it.
===========
Run 140/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_141/ already exists.
Skipping it.
===========
Run 141/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_142/ already exists.
Skipping it.
===========
Run 142/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_143/ already exists.
Skipping it.
===========
Run 143/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_144/ already exists.
Skipping it.
===========
Run 144/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_145/ already exists.
Skipping it.
===========
Run 145/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_146/ already exists.
Skipping it.
===========
Run 146/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_147/ already exists.
Skipping it.
===========
Run 147/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_148/ already exists.
Skipping it.
===========
Run 148/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_149/ already exists.
Skipping it.
===========
Run 149/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_150/ already exists.
Skipping it.
===========
Run 150/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_151/ already exists.
Skipping it.
===========
Run 151/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_152/ already exists.
Skipping it.
===========
Run 152/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_153/ already exists.
Skipping it.
===========
Run 153/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_154/ already exists.
Skipping it.
===========
Run 154/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_155/ already exists.
Skipping it.
===========
Run 155/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_156/ already exists.
Skipping it.
===========
Run 156/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_157/ already exists.
Skipping it.
===========
Run 157/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_158/ already exists.
Skipping it.
===========
Run 158/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_159/ already exists.
Skipping it.
===========
Run 159/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_160/ already exists.
Skipping it.
===========
Run 160/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_161/ already exists.
Skipping it.
===========
Run 161/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_162/ already exists.
Skipping it.
===========
Run 162/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_163/ already exists.
Skipping it.
===========
Run 163/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_164/ already exists.
Skipping it.
===========
Run 164/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_165/ already exists.
Skipping it.
===========
Run 165/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_166/ already exists.
Skipping it.
===========
Run 166/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_167/ already exists.
Skipping it.
===========
Run 167/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_168/ already exists.
Skipping it.
===========
Run 168/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_169/ already exists.
Skipping it.
===========
Run 169/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_170/ already exists.
Skipping it.
===========
Run 170/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_171/ already exists.
Skipping it.
===========
Run 171/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_172/ already exists.
Skipping it.
===========
Run 172/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_173/ already exists.
Skipping it.
===========
Run 173/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_174/ already exists.
Skipping it.
===========
Run 174/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_175/ already exists.
Skipping it.
===========
Run 175/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_176/ already exists.
Skipping it.
===========
Run 176/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_177/ already exists.
Skipping it.
===========
Run 177/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_178/ already exists.
Skipping it.
===========
Run 178/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_179/ already exists.
Skipping it.
===========
Run 179/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_180/ already exists.
Skipping it.
===========
Run 180/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_181/ already exists.
Skipping it.
===========
Run 181/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_182/ already exists.
Skipping it.
===========
Run 182/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_183/ already exists.
Skipping it.
===========
Run 183/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_184/ already exists.
Skipping it.
===========
Run 184/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_185/ already exists.
Skipping it.
===========
Run 185/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_186/ already exists.
Skipping it.
===========
Run 186/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_187/ already exists.
Skipping it.
===========
Run 187/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_188/ already exists.
Skipping it.
===========
Run 188/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_189/ already exists.
Skipping it.
===========
Run 189/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_190/ already exists.
Skipping it.
===========
Run 190/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_191/ already exists.
Skipping it.
===========
Run 191/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_192/ already exists.
Skipping it.
===========
Run 192/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_193/ already exists.
Skipping it.
===========
Run 193/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_194/ already exists.
Skipping it.
===========
Run 194/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_195/ already exists.
Skipping it.
===========
Run 195/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_196/ already exists.
Skipping it.
===========
Run 196/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_197/ already exists.
Skipping it.
===========
Run 197/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_198/ already exists.
Skipping it.
===========
Run 198/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_199/ already exists.
Skipping it.
===========
Run 199/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_200/ already exists.
Skipping it.
===========
Run 200/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_201/ already exists.
Skipping it.
===========
Run 201/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_202/ already exists.
Skipping it.
===========
Run 202/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_203/ already exists.
Skipping it.
===========
Run 203/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_204/ already exists.
Skipping it.
===========
Run 204/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_205/ already exists.
Skipping it.
===========
Run 205/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_206/ already exists.
Skipping it.
===========
Run 206/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_207/ already exists.
Skipping it.
===========
Run 207/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_208/ already exists.
Skipping it.
===========
Run 208/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_209/ already exists.
Skipping it.
===========
Run 209/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_210/ already exists.
Skipping it.
===========
Run 210/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_211/ already exists.
Skipping it.
===========
Run 211/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_212/ already exists.
Skipping it.
===========
Run 212/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_213/ already exists.
Skipping it.
===========
Run 213/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_214/ already exists.
Skipping it.
===========
Run 214/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_215/ already exists.
Skipping it.
===========
Run 215/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_216/ already exists.
Skipping it.
===========
Run 216/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_217/ already exists.
Skipping it.
===========
Run 217/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_218/ already exists.
Skipping it.
===========
Run 218/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_219/ already exists.
Skipping it.
===========
Run 219/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_220/ already exists.
Skipping it.
===========
Run 220/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_221/ already exists.
Skipping it.
===========
Run 221/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_222/ already exists.
Skipping it.
===========
Run 222/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_223/ already exists.
Skipping it.
===========
Run 223/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_224/ already exists.
Skipping it.
===========
Run 224/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_225/ already exists.
Skipping it.
===========
Run 225/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_226/ already exists.
Skipping it.
===========
Run 226/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_227/ already exists.
Skipping it.
===========
Run 227/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_228/ already exists.
Skipping it.
===========
Run 228/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_229/ already exists.
Skipping it.
===========
Run 229/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_230/ already exists.
Skipping it.
===========
Run 230/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_231/ already exists.
Skipping it.
===========
Run 231/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_232/ already exists.
Skipping it.
===========
Run 232/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_233/ already exists.
Skipping it.
===========
Run 233/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_234/ already exists.
Skipping it.
===========
Run 234/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_235/ already exists.
Skipping it.
===========
Run 235/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_236/ already exists.
Skipping it.
===========
Run 236/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_237/ already exists.
Skipping it.
===========
Run 237/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_238/ already exists.
Skipping it.
===========
Run 238/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_239/ already exists.
Skipping it.
===========
Run 239/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_240/ already exists.
Skipping it.
===========
Run 240/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_241/ already exists.
Skipping it.
===========
Run 241/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_242/ already exists.
Skipping it.
===========
Run 242/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_243/ already exists.
Skipping it.
===========
Run 243/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_244/ already exists.
Skipping it.
===========
Run 244/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_245/ already exists.
Skipping it.
===========
Run 245/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_246/ already exists.
Skipping it.
===========
Run 246/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_247/ already exists.
Skipping it.
===========
Run 247/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_248/ already exists.
Skipping it.
===========
Run 248/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_249/ already exists.
Skipping it.
===========
Run 249/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_250/ already exists.
Skipping it.
===========
Run 250/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_251/ already exists.
Skipping it.
===========
Run 251/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_252/ already exists.
Skipping it.
===========
Run 252/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_253/ already exists.
Skipping it.
===========
Run 253/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_254/ already exists.
Skipping it.
===========
Run 254/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_255/ already exists.
Skipping it.
===========
Run 255/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_256/ already exists.
Skipping it.
===========
Run 256/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_257/ already exists.
Skipping it.
===========
Run 257/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_258/ already exists.
Skipping it.
===========
Run 258/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_259/ already exists.
Skipping it.
===========
Run 259/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_260/ already exists.
Skipping it.
===========
Run 260/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_261/ already exists.
Skipping it.
===========
Run 261/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_262/ already exists.
Skipping it.
===========
Run 262/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_263/ already exists.
Skipping it.
===========
Run 263/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_264/ already exists.
Skipping it.
===========
Run 264/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_265/ already exists.
Skipping it.
===========
Run 265/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_266/ already exists.
Skipping it.
===========
Run 266/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_267/ already exists.
Skipping it.
===========
Run 267/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_268/ already exists.
Skipping it.
===========
Run 268/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_269/ already exists.
Skipping it.
===========
Run 269/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_270/ already exists.
Skipping it.
===========
Run 270/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_271/ already exists.
Skipping it.
===========
Run 271/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_272/ already exists.
Skipping it.
===========
Run 272/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_273/ already exists.
Skipping it.
===========
Run 273/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_274/ already exists.
Skipping it.
===========
Run 274/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_275/ already exists.
Skipping it.
===========
Run 275/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_276/ already exists.
Skipping it.
===========
Run 276/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_277/ already exists.
Skipping it.
===========
Run 277/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_278/ already exists.
Skipping it.
===========
Run 278/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_279/ already exists.
Skipping it.
===========
Run 279/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_280/ already exists.
Skipping it.
===========
Run 280/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_281/ already exists.
Skipping it.
===========
Run 281/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_282/ already exists.
Skipping it.
===========
Run 282/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_283/ already exists.
Skipping it.
===========
Run 283/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_284/ already exists.
Skipping it.
===========
Run 284/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_285/ already exists.
Skipping it.
===========
Run 285/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_286/ already exists.
Skipping it.
===========
Run 286/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_287/ already exists.
Skipping it.
===========
Run 287/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_288/ already exists.
Skipping it.
===========
Run 288/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_289/ already exists.
Skipping it.
===========
Run 289/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_290/ already exists.
Skipping it.
===========
Run 290/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_291/ already exists.
Skipping it.
===========
Run 291/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_292/ already exists.
Skipping it.
===========
Run 292/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_293/ already exists.
Skipping it.
===========
Run 293/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_294/ already exists.
Skipping it.
===========
Run 294/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_295/ already exists.
Skipping it.
===========
Run 295/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_296/ already exists.
Skipping it.
===========
Run 296/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_297/ already exists.
Skipping it.
===========
Run 297/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_298/ already exists.
Skipping it.
===========
Run 298/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_299/ already exists.
Skipping it.
===========
Run 299/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_300/ already exists.
Skipping it.
===========
Run 300/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_301/ already exists.
Skipping it.
===========
Run 301/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_302/ already exists.
Skipping it.
===========
Run 302/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_303/ already exists.
Skipping it.
===========
Run 303/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_304/ already exists.
Skipping it.
===========
Run 304/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_305/ already exists.
Skipping it.
===========
Run 305/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_306/ already exists.
Skipping it.
===========
Run 306/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_307/ already exists.
Skipping it.
===========
Run 307/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_308/ already exists.
Skipping it.
===========
Run 308/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_309/ already exists.
Skipping it.
===========
Run 309/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_310/ already exists.
Skipping it.
===========
Run 310/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_311/ already exists.
Skipping it.
===========
Run 311/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_312/ already exists.
Skipping it.
===========
Run 312/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_313/ already exists.
Skipping it.
===========
Run 313/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_314/ already exists.
Skipping it.
===========
Run 314/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_315/ already exists.
Skipping it.
===========
Run 315/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_316/ already exists.
Skipping it.
===========
Run 316/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_317/ already exists.
Skipping it.
===========
Run 317/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_318/ already exists.
Skipping it.
===========
Run 318/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_319/ already exists.
Skipping it.
===========
Run 319/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_320/ already exists.
Skipping it.
===========
Run 320/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_321/ already exists.
Skipping it.
===========
Run 321/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_322/ already exists.
Skipping it.
===========
Run 322/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_323/ already exists.
Skipping it.
===========
Run 323/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_324/ already exists.
Skipping it.
===========
Run 324/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_325/ already exists.
Skipping it.
===========
Run 325/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_326/ already exists.
Skipping it.
===========
Run 326/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_327/ already exists.
Skipping it.
===========
Run 327/360 already exists. Skipping it.
===========

===========
Generating train data for run 328.
===========
Train data generated in 2.51 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.6836276 , -0.60898554,  4.901356  , ...,  4.578798  ,
         5.78743   ,  5.8673487 ],
       [ 8.249707  ,  4.460014  ,  5.2482004 , ...,  1.7374016 ,
         8.867281  ,  6.8796477 ],
       [ 6.103276  ,  7.069252  ,  6.2995415 , ..., 11.163949  ,
         2.6241696 ,  6.7860346 ],
       ...,
       [ 5.7729836 ,  0.39295188,  4.752078  , ...,  4.8545365 ,
         6.277194  ,  5.1011395 ],
       [ 8.14715   ,  4.7503    ,  5.215126  , ...,  3.7450724 ,
         8.238651  ,  6.647562  ],
       [ 5.6715584 ,  7.2547503 ,  6.700116  , ..., 10.850491  ,
         1.1671181 ,  6.458376  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_328
self.data_kwargs: {'seed': 187}
self.x_data: [[8.405046   4.6301365  5.2287283  ... 2.9590435  8.640119   7.4559712 ]
 [6.027003   0.2016806  4.6617384  ... 4.692235   6.373698   5.3374834 ]
 [5.680177   0.10050362 4.564641   ... 4.4050727  6.0966487  6.0310545 ]
 ...
 [6.204324   0.5308406  4.687448   ... 4.736321   6.117902   4.3074207 ]
 [7.884271   4.844002   5.2654357  ... 2.2312212  7.9686017  6.4498177 ]
 [5.7032723  1.704168   4.78966    ... 5.263417   6.7690263  5.8227935 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1000)]            0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  9018400   
 r)                                                              
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fab4038b250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fab1c1e3e50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fab1c1e3e50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fab1c12e080>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fab1c15c0d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fab1c15ded0>, <keras.callbacks.ModelCheckpoint object at 0x7fab1c15e020>, <keras.callbacks.EarlyStopping object at 0x7fab1c15e230>, <keras.callbacks.ReduceLROnPlateau object at 0x7fab1c15e260>, <keras.callbacks.TerminateOnNaN object at 0x7fab1c15df90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.6836276 , -0.60898554,  4.901356  , ...,  4.578798  ,
         5.78743   ,  5.8673487 ],
       [ 8.249707  ,  4.460014  ,  5.2482004 , ...,  1.7374016 ,
         8.867281  ,  6.8796477 ],
       [ 6.103276  ,  7.069252  ,  6.2995415 , ..., 11.163949  ,
         2.6241696 ,  6.7860346 ],
       ...,
       [ 5.7729836 ,  0.39295188,  4.752078  , ...,  4.8545365 ,
         6.277194  ,  5.1011395 ],
       [ 8.14715   ,  4.7503    ,  5.215126  , ...,  3.7450724 ,
         8.238651  ,  6.647562  ],
       [ 5.6715584 ,  7.2547503 ,  6.700116  , ..., 10.850491  ,
         1.1671181 ,  6.458376  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_328/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 328/360 with hyperparameters:
timestamp = 2023-09-09 22:36:38.833164
ndims = 1000
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 8.405046    4.6301365   5.2287283   2.4556472   6.2928953   3.594312
  6.0004463   1.8052834   1.8765529   4.0951214   4.4015846   3.3011546
  1.1913271   0.29790348  0.444705    2.0839415   4.8839054   7.6232376
  5.0511703  10.082624    1.0256902   1.8862149   3.3338983   5.485959
  8.678817    4.2863455   2.746828    0.5995625   8.487818    4.326282
  5.4844646   9.668893    0.9005003   2.511495    7.4595304   8.076063
  7.482936    5.8244066  10.038453    4.2971616   6.199124    0.14504078
  4.115744    7.085761    2.2268302   8.105376    6.8798943   3.1965656
  7.803503    1.4473212   9.836955    1.2955343   9.292721    2.2835047
  7.347313   -0.344647    2.3793294   7.1599417   6.239404    1.8819436
  3.6855946   3.1096044   3.9889503   6.345884    8.7655735   3.506798
  3.8915956   1.4826899   7.7798705   6.4561133   3.7245452   5.022784
  6.785575    1.8807405  -0.53786993  3.331518    6.3648286   5.060461
  5.671335    8.188019    5.050044    0.8326328   9.021144    5.101296
  8.440869    7.8830547   9.847927    2.9928007   7.1174664   4.000934
  4.0642667   3.5513847   3.3835356   9.485252    2.8910272   2.857944
  5.1031704   2.365148    4.392073    7.9044094   2.2182727   3.5326295
  6.429046    5.444268    9.853491    6.801449    7.615357    9.654012
  4.2561636   3.94246     0.5266583   1.6963573   0.9076003   1.6972958
  8.415306    6.55893     6.517854    7.7323823   2.263416    3.867008
  7.0743175   4.310728    0.44073427  5.454958    8.770797    4.023472
  3.8313503   9.876201    6.083322    4.588916    0.97578037  8.022623
  6.904659    9.723064    1.5232677   1.2301477   7.2426834   2.4050941
  1.7633336   1.3129089   8.420942    5.7998257   0.4667193   4.692241
  6.5908155   5.257252    5.725936    2.6134639   8.439922    5.6147094
  7.455604    9.777677    4.190657    9.502805    0.30458957  0.2189286
  5.797137    3.0544481   2.6193647   2.5525932   3.284805    7.4161353
  5.749028    7.889573    7.898816    8.574444    3.8091307   4.298687
  9.481366    6.0068207   1.523428    5.6352916   4.365734    9.867473
  1.3835607   6.134872    7.2581935   2.8856063   9.735012    7.5791473
  9.23718     0.3085858   6.8750105   3.0606773   9.2376585   6.005112
  5.5536275   0.9032261   6.0153728   6.3190417   9.369196    5.911083
  9.520276   10.025198    7.444127    4.7846828   6.6327243  10.038699
  0.737067    1.1364235   0.7629767   7.3339915   3.2413385   6.3641744
  4.723883    3.890898    5.1846333   6.4865723   9.4478655   5.8353424
  6.1596503   0.8805034  10.229148    5.1357536   8.719589    0.61422366
  4.6646357   2.5584815   2.6442034   7.3017416   5.692237    3.2181895
  0.7464954   9.628433    9.805298    0.8460964   0.74706554  6.1736875
  9.794054    9.817206    5.73489     2.8118153   2.995823    9.004283
  3.2283762   1.9297047   2.5467188   2.732583    2.62711     5.3906684
  8.859532    7.221753    0.22360349  0.31767347  3.3918247   3.5905416
  8.748679    9.150864    4.0089087   6.819023    9.95121     7.2592616
  6.618907    0.7986359   3.617743    8.477222    2.5446649   8.356348
  6.784008    2.7764812   5.149644    5.879593    6.8204975   4.7749815
  1.5671438   7.5464864   2.9128666   9.640756    2.7195537   2.422054
  1.9658555   5.744532   10.17906     1.88257     3.3741965   5.759674
  4.4506254   5.799042    8.574516    9.521442    4.848262    7.099014
  7.410928    4.176586    2.7669337   6.7037125   6.396343    9.116149
  0.6767711   8.802082    5.400854   -0.46128303  4.315248    7.679634
  4.337188    9.473796    4.3057194   3.8800864   8.321836    5.464394
  6.8586717   7.8846817   6.758233    4.5121193   1.5240693   5.1997232
  7.966815    3.110014    9.149139    5.2918534   6.043538    8.5325165
  7.029521    0.71871924  9.851924    2.0979629   7.107039    5.8480177
  4.6838837   2.2520099   7.556054    1.2082794  10.557993    8.230412
  5.3685956   3.7290514   3.748278    4.2430515   8.3245535   2.619837
  3.433112    1.6606942   2.8829854   1.0669343   9.924688    5.8388543
  6.973813    0.9312429   3.8876076   8.887293    0.8722724   2.881507
  6.283689    6.159739    8.345073    9.115302    1.4750379   2.7552269
  4.2736816   0.15881406 -0.24159682  2.4190745   4.887695    2.6831913
  8.220071    3.2441287   8.8378105   9.509058    3.559123    3.1608875
  3.3725164   7.9289885   2.9474692   4.059424    4.7409554   5.2013454
  9.994335    2.323355    3.3355927   2.5336983   3.2941818   2.2210457
  3.2414346   9.904477    2.334639    4.492616   10.475705    7.142849
  4.6010633   7.570345    1.3164223   4.2845144   0.5250243   2.3362412
  2.8345227   3.6286557   4.130509    9.905742    2.1830826   4.842215
  6.807653    7.5327077   7.2363663   2.9334166   5.234727    1.7384282
  2.0550947   4.500183    7.394765    4.995599    7.815772   10.710524
 -0.19277403  5.792584    6.1900487   9.671367    0.28583023  5.668808
  1.2390957   0.5417446   6.8533115   7.9632573   8.180385   -0.4858576
  4.0685763   2.4839914   2.1516795   1.9025909   2.110797    2.6641178
  2.6061893   9.366816    4.2622356   8.9237175   1.1875978   7.9966984
  0.44503674  4.7554154  -0.20823273  7.945292   10.080656    0.3404049
  8.932869    8.365952    3.7764497   9.093311    2.3442857   4.1149993
  0.99684995  6.8027267   0.3532521   4.712874    6.197236    8.98079
  0.7532523   7.324172    6.816783    7.1629744   5.789584    1.8008664
  6.6962724   3.5133753   8.672127    1.7033234   5.757915    3.8499484
  7.6907835   2.515215    0.675339    6.6755958   4.1274943   5.9000664
  0.6306852  10.170123    8.690909    5.6241055   0.83715045  3.7537289
  9.811078    7.1486425   1.5089558   4.5211      9.977777    5.041157
  0.20099962  7.1491528   2.411008    0.98729235  4.0264      6.1107965
  0.8201466   6.400518    5.2138658   8.338983    4.008616    5.4276114
  7.5568547   9.328188    3.5043116   5.993576    6.732445    0.1456024
  8.195436    4.0946035   6.8117595   5.4540176   7.008695    7.6902666
  1.121738    6.662947    9.421039    3.6054578   8.282917    6.2551546
  8.867136    2.7717266   1.666508    1.9828001   2.582295    7.401628
  7.3309445   6.1640267   2.2886527   8.620049    9.976319    5.427213
  6.492129    5.227495    2.6098144   3.1037233   2.451418    1.547883
  9.130067    9.089639    3.8307672   2.809064    9.412396    3.0656943
  5.384379    1.269433    2.1198947   5.4160995   5.9553137   6.8595047
  8.024387    9.932428    7.275275    5.4964776   9.2686615  -0.8320414
  6.634356    3.5646963   1.6928086   9.683453    3.4941626   6.2850795
  8.140037    5.0819607   1.6098254   8.515681    3.746112   10.339378
  9.723754    8.560821    3.4616127   8.136909    8.474316    5.334234
  4.3071146   0.3017891   7.5740643   8.525242    1.961537    2.785501
  4.6199455   0.20740837  0.8326539   4.594327    2.569767    3.3214014
  4.017394    9.219309   10.736801    1.9790065   8.832557    0.9678319
  5.233325   -0.19063437  3.1312854   8.050418    4.191532    9.204013
  7.268445    9.203028    4.342827    8.019755    3.3341756   3.743745
  3.9635062   3.979517    0.6836107   5.098467    0.71361184  8.4552765
  0.5069707   4.833144    3.1548538   1.8944778   7.8488803   7.0419393
  1.2548511   6.628743    7.354838    8.050705    1.1345416   6.8048496
  6.528121    8.252401    4.5538      4.949585    8.730731    7.4312305
  5.643753    2.5072966   8.962284    4.0641007   0.7590125   5.4753785
  9.738958    4.561652   -0.360509    6.1097317   9.957747    7.0762954
  5.690881    8.773466    8.0799      5.2899666   6.263382    7.4875503
  8.585341    0.6384524   8.029185    4.3010435   6.264173    7.7155027
  5.2268887   1.5704446   4.016696    0.5981313   6.7822347   9.855953
  7.53499     5.1501827   5.5134096   2.8642232   5.220349    3.4915414
  4.7451196   2.6037292   9.620164    6.549827    0.7027252  -0.15775523
 10.00958     3.973167    3.273789    1.7160401   9.183787    7.1865473
  5.1355295   4.8474526   3.5554662   6.6154428   4.344493    2.1984468
  5.4696417   4.251729    8.8025255   4.26232     4.9216294   7.9483533
  4.526991    9.588897    8.017048    8.2831135   9.821576    4.1731563
  5.8485947   6.667628    7.001741    9.226937    2.9446237   2.5925434
  8.618449    8.161273    1.7101693   5.5401206   8.14539     2.020844
  0.89211583  5.7496552   4.987388    7.4813085   0.39198843  4.8122907
  2.6751356   5.1997175   7.827039    0.7617685   1.984582    6.211319
  7.497485    5.6864862   9.833999    4.343862    0.91428226  6.860134
  8.022065    5.7952833   5.549028    8.938169    0.7987054   1.0077511
  1.547055    2.4538176   5.900641    5.950689    2.172312    4.864265
  5.3084598   2.74433     7.4509587   4.642633    7.352649    1.6610278
  5.3932858   8.224104    1.7825346   4.6216807   6.3162665   2.1527483
  7.82622     0.4708147   9.143722    4.7161274   2.9691823   0.9679443
  3.9266372   6.638867    0.10056549  3.118843    7.3452816   6.4795094
  2.0939577   2.478408    8.811012    3.114293    2.1261327  10.111931
  4.855735    1.2398901   5.2605786   5.4892864   9.44359     1.744517
  0.8859111   0.84614795  8.730755    4.4898634   1.9797392   5.9254265
  5.903251    2.0259151   7.1823764   4.0827475   2.3360302   5.4425554
  3.0566494   1.579066    8.333391    1.4978724   3.288137    3.852397
  2.935655    2.1213212   6.9459066   3.5336087   1.8826928   5.0965667
  1.2400447   4.7354097   8.616984    2.6134915   5.8834224   5.1864443
 -0.13750565  6.549661    8.984314    0.7847537   7.2001395   6.8008256
  8.44678     2.6799302   3.2838757   7.0237226  -0.23040357  7.540698
  8.530038    4.6869135   3.2353654   3.3326626  -2.8252068   1.808005
  8.638641    9.090046    2.7026813   9.352778    3.2715847   2.764985
  2.5781574   8.643513    9.775191    3.6196568   6.3544054   2.67319
 -0.27903366  3.0261974   5.7475586   1.8039718   8.191788    4.8235297
  2.4119723   4.80837     7.8066583   3.14657     7.254359    3.8570547
  8.983632    8.136105    3.6973224   0.29414394  6.2837157   1.7246333
  1.2347451   8.727795    9.613968    2.8966768   3.0174162   1.6892313
  4.788908    8.051952    6.6716456   8.591153    1.2762189   7.83716
 10.090976    2.9488695   6.9059463   3.4287705   2.638495    9.951833
  3.8517146   1.0833437   3.8833115   4.4572515   7.453829    9.261691
 10.299599    8.9711075   4.912658    7.687476    2.8381176   0.8803614
  3.6029894   7.0204277   3.0225866   9.093265    9.016605    1.5790088
  2.4555895   8.069907    3.3349042   3.3253474   3.304729    8.906204
  0.14018849 -0.09102672  5.6968327   2.8784204   6.3043637   1.5840611
  9.080589    7.872192    0.68985736  8.022463    9.62591     9.730082
  6.885892    1.2239211   6.788951    3.103189    8.581392    7.224449
  6.0487485   6.9063663   4.2010665   4.5798826   9.059492    3.4417286
  0.25157684  4.0677304   8.905279    6.148643   11.682095    0.95629424
  9.730315    5.5144877   4.490063    4.5312085   1.2480376   3.6158848
  7.4273086  -0.96325046  1.452391    0.25324896  8.457037    6.1265316
  1.3514678   8.127087   10.009373    9.350058    8.951299   -0.29766494
  6.0847034   2.4157875   5.6192784   2.8229573   0.7195138   4.5521054
  4.8054247   9.962674    0.39720708  3.1612184   8.577424    7.115589
  7.7674336   6.298065    7.663883    4.695144    1.1639674   7.7367616
  5.669413    1.2667739   7.7379103   9.650189    3.731491    3.391371
  0.73656756  6.0238733   9.167239    1.2704167   4.995278    0.9358564
  0.38653633  6.6325297   8.341552    3.6262543   7.874937    2.0914352
  2.2575343   7.0230904   4.9926057   4.8522496   0.38922572  1.2943935
  0.4778885   8.9588175   1.688771    8.19581     3.4283693   8.456239
  2.8242724   2.5323164   0.56487453  2.687169    3.4400296   8.293358
  4.435119    8.654662    5.423925   -0.7138738   3.3961737   6.6198845
  2.3316503   8.031245    4.494654    0.59436065  7.459087    5.8713436
  0.84994775  5.0717716   3.7515552   9.185751    2.3369966   9.342911
  5.7315216   2.9590435   8.640119    7.4559712 ]
Epoch 1/1000
2023-09-09 22:38:33.920 
Epoch 1/1000 
	 loss: 2208.6831, MinusLogProbMetric: 2208.6831, val_loss: 665.1425, val_MinusLogProbMetric: 665.1425

Epoch 1: val_loss improved from inf to 665.14252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 116s - loss: 2208.6831 - MinusLogProbMetric: 2208.6831 - val_loss: 665.1425 - val_MinusLogProbMetric: 665.1425 - lr: 0.0010 - 116s/epoch - 590ms/step
Epoch 2/1000
2023-09-09 22:38:54.951 
Epoch 2/1000 
	 loss: 572.6438, MinusLogProbMetric: 572.6438, val_loss: 526.2288, val_MinusLogProbMetric: 526.2288

Epoch 2: val_loss improved from 665.14252 to 526.22876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 21s - loss: 572.6438 - MinusLogProbMetric: 572.6438 - val_loss: 526.2288 - val_MinusLogProbMetric: 526.2288 - lr: 0.0010 - 21s/epoch - 105ms/step
Epoch 3/1000
2023-09-09 22:39:13.712 
Epoch 3/1000 
	 loss: 508.5689, MinusLogProbMetric: 508.5689, val_loss: 496.5662, val_MinusLogProbMetric: 496.5662

Epoch 3: val_loss improved from 526.22876 to 496.56616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 508.5689 - MinusLogProbMetric: 508.5689 - val_loss: 496.5662 - val_MinusLogProbMetric: 496.5662 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 4/1000
2023-09-09 22:39:32.858 
Epoch 4/1000 
	 loss: 488.7541, MinusLogProbMetric: 488.7541, val_loss: 484.2020, val_MinusLogProbMetric: 484.2020

Epoch 4: val_loss improved from 496.56616 to 484.20203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 488.7541 - MinusLogProbMetric: 488.7541 - val_loss: 484.2020 - val_MinusLogProbMetric: 484.2020 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 5/1000
2023-09-09 22:39:51.071 
Epoch 5/1000 
	 loss: 475.5402, MinusLogProbMetric: 475.5402, val_loss: 467.2866, val_MinusLogProbMetric: 467.2866

Epoch 5: val_loss improved from 484.20203 to 467.28659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 475.5402 - MinusLogProbMetric: 475.5402 - val_loss: 467.2866 - val_MinusLogProbMetric: 467.2866 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 6/1000
2023-09-09 22:40:08.896 
Epoch 6/1000 
	 loss: 469.7375, MinusLogProbMetric: 469.7375, val_loss: 470.0402, val_MinusLogProbMetric: 470.0402

Epoch 6: val_loss did not improve from 467.28659
196/196 - 17s - loss: 469.7375 - MinusLogProbMetric: 469.7375 - val_loss: 470.0402 - val_MinusLogProbMetric: 470.0402 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 7/1000
2023-09-09 22:40:25.965 
Epoch 7/1000 
	 loss: 461.8330, MinusLogProbMetric: 461.8330, val_loss: 460.0364, val_MinusLogProbMetric: 460.0364

Epoch 7: val_loss improved from 467.28659 to 460.03638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 461.8330 - MinusLogProbMetric: 461.8330 - val_loss: 460.0364 - val_MinusLogProbMetric: 460.0364 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 8/1000
2023-09-09 22:40:43.966 
Epoch 8/1000 
	 loss: 457.2236, MinusLogProbMetric: 457.2236, val_loss: 479.3258, val_MinusLogProbMetric: 479.3258

Epoch 8: val_loss did not improve from 460.03638
196/196 - 17s - loss: 457.2236 - MinusLogProbMetric: 457.2236 - val_loss: 479.3258 - val_MinusLogProbMetric: 479.3258 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 9/1000
2023-09-09 22:41:00.794 
Epoch 9/1000 
	 loss: 452.3454, MinusLogProbMetric: 452.3454, val_loss: 481.0828, val_MinusLogProbMetric: 481.0828

Epoch 9: val_loss did not improve from 460.03638
196/196 - 17s - loss: 452.3454 - MinusLogProbMetric: 452.3454 - val_loss: 481.0828 - val_MinusLogProbMetric: 481.0828 - lr: 0.0010 - 17s/epoch - 86ms/step
Epoch 10/1000
2023-09-09 22:41:18.315 
Epoch 10/1000 
	 loss: 449.9809, MinusLogProbMetric: 449.9809, val_loss: 460.6804, val_MinusLogProbMetric: 460.6804

Epoch 10: val_loss did not improve from 460.03638
196/196 - 18s - loss: 449.9809 - MinusLogProbMetric: 449.9809 - val_loss: 460.6804 - val_MinusLogProbMetric: 460.6804 - lr: 0.0010 - 18s/epoch - 89ms/step
Epoch 11/1000
2023-09-09 22:41:35.739 
Epoch 11/1000 
	 loss: 445.9518, MinusLogProbMetric: 445.9518, val_loss: 463.4381, val_MinusLogProbMetric: 463.4381

Epoch 11: val_loss did not improve from 460.03638
196/196 - 17s - loss: 445.9518 - MinusLogProbMetric: 445.9518 - val_loss: 463.4381 - val_MinusLogProbMetric: 463.4381 - lr: 0.0010 - 17s/epoch - 89ms/step
Epoch 12/1000
2023-09-09 22:41:52.799 
Epoch 12/1000 
	 loss: 444.4464, MinusLogProbMetric: 444.4464, val_loss: 441.4001, val_MinusLogProbMetric: 441.4001

Epoch 12: val_loss improved from 460.03638 to 441.40009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 444.4464 - MinusLogProbMetric: 444.4464 - val_loss: 441.4001 - val_MinusLogProbMetric: 441.4001 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 13/1000
2023-09-09 22:42:11.155 
Epoch 13/1000 
	 loss: 439.7160, MinusLogProbMetric: 439.7160, val_loss: 436.7582, val_MinusLogProbMetric: 436.7582

Epoch 13: val_loss improved from 441.40009 to 436.75818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 439.7160 - MinusLogProbMetric: 439.7160 - val_loss: 436.7582 - val_MinusLogProbMetric: 436.7582 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 14/1000
2023-09-09 22:42:29.653 
Epoch 14/1000 
	 loss: 439.5519, MinusLogProbMetric: 439.5519, val_loss: 449.6214, val_MinusLogProbMetric: 449.6214

Epoch 14: val_loss did not improve from 436.75818
196/196 - 17s - loss: 439.5519 - MinusLogProbMetric: 439.5519 - val_loss: 449.6214 - val_MinusLogProbMetric: 449.6214 - lr: 0.0010 - 17s/epoch - 87ms/step
Epoch 15/1000
2023-09-09 22:42:46.667 
Epoch 15/1000 
	 loss: 448.5904, MinusLogProbMetric: 448.5904, val_loss: 444.3703, val_MinusLogProbMetric: 444.3703

Epoch 15: val_loss did not improve from 436.75818
196/196 - 17s - loss: 448.5904 - MinusLogProbMetric: 448.5904 - val_loss: 444.3703 - val_MinusLogProbMetric: 444.3703 - lr: 0.0010 - 17s/epoch - 87ms/step
Epoch 16/1000
2023-09-09 22:43:03.811 
Epoch 16/1000 
	 loss: 437.2484, MinusLogProbMetric: 437.2484, val_loss: 467.8994, val_MinusLogProbMetric: 467.8994

Epoch 16: val_loss did not improve from 436.75818
196/196 - 17s - loss: 437.2484 - MinusLogProbMetric: 437.2484 - val_loss: 467.8994 - val_MinusLogProbMetric: 467.8994 - lr: 0.0010 - 17s/epoch - 87ms/step
Epoch 17/1000
2023-09-09 22:43:21.488 
Epoch 17/1000 
	 loss: 434.7236, MinusLogProbMetric: 434.7236, val_loss: 437.4411, val_MinusLogProbMetric: 437.4411

Epoch 17: val_loss did not improve from 436.75818
196/196 - 18s - loss: 434.7236 - MinusLogProbMetric: 434.7236 - val_loss: 437.4411 - val_MinusLogProbMetric: 437.4411 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 18/1000
2023-09-09 22:43:38.742 
Epoch 18/1000 
	 loss: 432.5162, MinusLogProbMetric: 432.5162, val_loss: 434.6524, val_MinusLogProbMetric: 434.6524

Epoch 18: val_loss improved from 436.75818 to 434.65240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 432.5162 - MinusLogProbMetric: 432.5162 - val_loss: 434.6524 - val_MinusLogProbMetric: 434.6524 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 19/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 58: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-09 22:43:46.777 
Epoch 19/1000 
	 loss: nan, MinusLogProbMetric: 526.4371, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 19: val_loss did not improve from 434.65240
196/196 - 7s - loss: nan - MinusLogProbMetric: 526.4371 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 7s/epoch - 38ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 328.
===========
Train data generated in 2.64 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.6836276 , -0.60898554,  4.901356  , ...,  4.578798  ,
         5.78743   ,  5.8673487 ],
       [ 8.249707  ,  4.460014  ,  5.2482004 , ...,  1.7374016 ,
         8.867281  ,  6.8796477 ],
       [ 6.103276  ,  7.069252  ,  6.2995415 , ..., 11.163949  ,
         2.6241696 ,  6.7860346 ],
       ...,
       [ 5.7729836 ,  0.39295188,  4.752078  , ...,  4.8545365 ,
         6.277194  ,  5.1011395 ],
       [ 8.14715   ,  4.7503    ,  5.215126  , ...,  3.7450724 ,
         8.238651  ,  6.647562  ],
       [ 5.6715584 ,  7.2547503 ,  6.700116  , ..., 10.850491  ,
         1.1671181 ,  6.458376  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_328
self.data_kwargs: {'seed': 187}
self.x_data: [[8.405046   4.6301365  5.2287283  ... 2.9590435  8.640119   7.4559712 ]
 [6.027003   0.2016806  4.6617384  ... 4.692235   6.373698   5.3374834 ]
 [5.680177   0.10050362 4.564641   ... 4.4050727  6.0966487  6.0310545 ]
 ...
 [6.204324   0.5308406  4.687448   ... 4.736321   6.117902   4.3074207 ]
 [7.884271   4.844002   5.2654357  ... 2.2312212  7.9686017  6.4498177 ]
 [5.7032723  1.704168   4.78966    ... 5.263417   6.7690263  5.8227935 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7faa9c4bd630>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7faed8cf17b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7faed8cf17b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7faed7b886a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7faed8a349d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faed8a34eb0>, <keras.callbacks.ModelCheckpoint object at 0x7faed8a34f70>, <keras.callbacks.EarlyStopping object at 0x7faed8a351e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7faed8a35210>, <keras.callbacks.TerminateOnNaN object at 0x7faed8a34e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.6836276 , -0.60898554,  4.901356  , ...,  4.578798  ,
         5.78743   ,  5.8673487 ],
       [ 8.249707  ,  4.460014  ,  5.2482004 , ...,  1.7374016 ,
         8.867281  ,  6.8796477 ],
       [ 6.103276  ,  7.069252  ,  6.2995415 , ..., 11.163949  ,
         2.6241696 ,  6.7860346 ],
       ...,
       [ 5.7729836 ,  0.39295188,  4.752078  , ...,  4.8545365 ,
         6.277194  ,  5.1011395 ],
       [ 8.14715   ,  4.7503    ,  5.215126  , ...,  3.7450724 ,
         8.238651  ,  6.647562  ],
       [ 5.6715584 ,  7.2547503 ,  6.700116  , ..., 10.850491  ,
         1.1671181 ,  6.458376  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 328/360 with hyperparameters:
timestamp = 2023-09-09 22:43:56.167069
ndims = 1000
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 8.405046    4.6301365   5.2287283   2.4556472   6.2928953   3.594312
  6.0004463   1.8052834   1.8765529   4.0951214   4.4015846   3.3011546
  1.1913271   0.29790348  0.444705    2.0839415   4.8839054   7.6232376
  5.0511703  10.082624    1.0256902   1.8862149   3.3338983   5.485959
  8.678817    4.2863455   2.746828    0.5995625   8.487818    4.326282
  5.4844646   9.668893    0.9005003   2.511495    7.4595304   8.076063
  7.482936    5.8244066  10.038453    4.2971616   6.199124    0.14504078
  4.115744    7.085761    2.2268302   8.105376    6.8798943   3.1965656
  7.803503    1.4473212   9.836955    1.2955343   9.292721    2.2835047
  7.347313   -0.344647    2.3793294   7.1599417   6.239404    1.8819436
  3.6855946   3.1096044   3.9889503   6.345884    8.7655735   3.506798
  3.8915956   1.4826899   7.7798705   6.4561133   3.7245452   5.022784
  6.785575    1.8807405  -0.53786993  3.331518    6.3648286   5.060461
  5.671335    8.188019    5.050044    0.8326328   9.021144    5.101296
  8.440869    7.8830547   9.847927    2.9928007   7.1174664   4.000934
  4.0642667   3.5513847   3.3835356   9.485252    2.8910272   2.857944
  5.1031704   2.365148    4.392073    7.9044094   2.2182727   3.5326295
  6.429046    5.444268    9.853491    6.801449    7.615357    9.654012
  4.2561636   3.94246     0.5266583   1.6963573   0.9076003   1.6972958
  8.415306    6.55893     6.517854    7.7323823   2.263416    3.867008
  7.0743175   4.310728    0.44073427  5.454958    8.770797    4.023472
  3.8313503   9.876201    6.083322    4.588916    0.97578037  8.022623
  6.904659    9.723064    1.5232677   1.2301477   7.2426834   2.4050941
  1.7633336   1.3129089   8.420942    5.7998257   0.4667193   4.692241
  6.5908155   5.257252    5.725936    2.6134639   8.439922    5.6147094
  7.455604    9.777677    4.190657    9.502805    0.30458957  0.2189286
  5.797137    3.0544481   2.6193647   2.5525932   3.284805    7.4161353
  5.749028    7.889573    7.898816    8.574444    3.8091307   4.298687
  9.481366    6.0068207   1.523428    5.6352916   4.365734    9.867473
  1.3835607   6.134872    7.2581935   2.8856063   9.735012    7.5791473
  9.23718     0.3085858   6.8750105   3.0606773   9.2376585   6.005112
  5.5536275   0.9032261   6.0153728   6.3190417   9.369196    5.911083
  9.520276   10.025198    7.444127    4.7846828   6.6327243  10.038699
  0.737067    1.1364235   0.7629767   7.3339915   3.2413385   6.3641744
  4.723883    3.890898    5.1846333   6.4865723   9.4478655   5.8353424
  6.1596503   0.8805034  10.229148    5.1357536   8.719589    0.61422366
  4.6646357   2.5584815   2.6442034   7.3017416   5.692237    3.2181895
  0.7464954   9.628433    9.805298    0.8460964   0.74706554  6.1736875
  9.794054    9.817206    5.73489     2.8118153   2.995823    9.004283
  3.2283762   1.9297047   2.5467188   2.732583    2.62711     5.3906684
  8.859532    7.221753    0.22360349  0.31767347  3.3918247   3.5905416
  8.748679    9.150864    4.0089087   6.819023    9.95121     7.2592616
  6.618907    0.7986359   3.617743    8.477222    2.5446649   8.356348
  6.784008    2.7764812   5.149644    5.879593    6.8204975   4.7749815
  1.5671438   7.5464864   2.9128666   9.640756    2.7195537   2.422054
  1.9658555   5.744532   10.17906     1.88257     3.3741965   5.759674
  4.4506254   5.799042    8.574516    9.521442    4.848262    7.099014
  7.410928    4.176586    2.7669337   6.7037125   6.396343    9.116149
  0.6767711   8.802082    5.400854   -0.46128303  4.315248    7.679634
  4.337188    9.473796    4.3057194   3.8800864   8.321836    5.464394
  6.8586717   7.8846817   6.758233    4.5121193   1.5240693   5.1997232
  7.966815    3.110014    9.149139    5.2918534   6.043538    8.5325165
  7.029521    0.71871924  9.851924    2.0979629   7.107039    5.8480177
  4.6838837   2.2520099   7.556054    1.2082794  10.557993    8.230412
  5.3685956   3.7290514   3.748278    4.2430515   8.3245535   2.619837
  3.433112    1.6606942   2.8829854   1.0669343   9.924688    5.8388543
  6.973813    0.9312429   3.8876076   8.887293    0.8722724   2.881507
  6.283689    6.159739    8.345073    9.115302    1.4750379   2.7552269
  4.2736816   0.15881406 -0.24159682  2.4190745   4.887695    2.6831913
  8.220071    3.2441287   8.8378105   9.509058    3.559123    3.1608875
  3.3725164   7.9289885   2.9474692   4.059424    4.7409554   5.2013454
  9.994335    2.323355    3.3355927   2.5336983   3.2941818   2.2210457
  3.2414346   9.904477    2.334639    4.492616   10.475705    7.142849
  4.6010633   7.570345    1.3164223   4.2845144   0.5250243   2.3362412
  2.8345227   3.6286557   4.130509    9.905742    2.1830826   4.842215
  6.807653    7.5327077   7.2363663   2.9334166   5.234727    1.7384282
  2.0550947   4.500183    7.394765    4.995599    7.815772   10.710524
 -0.19277403  5.792584    6.1900487   9.671367    0.28583023  5.668808
  1.2390957   0.5417446   6.8533115   7.9632573   8.180385   -0.4858576
  4.0685763   2.4839914   2.1516795   1.9025909   2.110797    2.6641178
  2.6061893   9.366816    4.2622356   8.9237175   1.1875978   7.9966984
  0.44503674  4.7554154  -0.20823273  7.945292   10.080656    0.3404049
  8.932869    8.365952    3.7764497   9.093311    2.3442857   4.1149993
  0.99684995  6.8027267   0.3532521   4.712874    6.197236    8.98079
  0.7532523   7.324172    6.816783    7.1629744   5.789584    1.8008664
  6.6962724   3.5133753   8.672127    1.7033234   5.757915    3.8499484
  7.6907835   2.515215    0.675339    6.6755958   4.1274943   5.9000664
  0.6306852  10.170123    8.690909    5.6241055   0.83715045  3.7537289
  9.811078    7.1486425   1.5089558   4.5211      9.977777    5.041157
  0.20099962  7.1491528   2.411008    0.98729235  4.0264      6.1107965
  0.8201466   6.400518    5.2138658   8.338983    4.008616    5.4276114
  7.5568547   9.328188    3.5043116   5.993576    6.732445    0.1456024
  8.195436    4.0946035   6.8117595   5.4540176   7.008695    7.6902666
  1.121738    6.662947    9.421039    3.6054578   8.282917    6.2551546
  8.867136    2.7717266   1.666508    1.9828001   2.582295    7.401628
  7.3309445   6.1640267   2.2886527   8.620049    9.976319    5.427213
  6.492129    5.227495    2.6098144   3.1037233   2.451418    1.547883
  9.130067    9.089639    3.8307672   2.809064    9.412396    3.0656943
  5.384379    1.269433    2.1198947   5.4160995   5.9553137   6.8595047
  8.024387    9.932428    7.275275    5.4964776   9.2686615  -0.8320414
  6.634356    3.5646963   1.6928086   9.683453    3.4941626   6.2850795
  8.140037    5.0819607   1.6098254   8.515681    3.746112   10.339378
  9.723754    8.560821    3.4616127   8.136909    8.474316    5.334234
  4.3071146   0.3017891   7.5740643   8.525242    1.961537    2.785501
  4.6199455   0.20740837  0.8326539   4.594327    2.569767    3.3214014
  4.017394    9.219309   10.736801    1.9790065   8.832557    0.9678319
  5.233325   -0.19063437  3.1312854   8.050418    4.191532    9.204013
  7.268445    9.203028    4.342827    8.019755    3.3341756   3.743745
  3.9635062   3.979517    0.6836107   5.098467    0.71361184  8.4552765
  0.5069707   4.833144    3.1548538   1.8944778   7.8488803   7.0419393
  1.2548511   6.628743    7.354838    8.050705    1.1345416   6.8048496
  6.528121    8.252401    4.5538      4.949585    8.730731    7.4312305
  5.643753    2.5072966   8.962284    4.0641007   0.7590125   5.4753785
  9.738958    4.561652   -0.360509    6.1097317   9.957747    7.0762954
  5.690881    8.773466    8.0799      5.2899666   6.263382    7.4875503
  8.585341    0.6384524   8.029185    4.3010435   6.264173    7.7155027
  5.2268887   1.5704446   4.016696    0.5981313   6.7822347   9.855953
  7.53499     5.1501827   5.5134096   2.8642232   5.220349    3.4915414
  4.7451196   2.6037292   9.620164    6.549827    0.7027252  -0.15775523
 10.00958     3.973167    3.273789    1.7160401   9.183787    7.1865473
  5.1355295   4.8474526   3.5554662   6.6154428   4.344493    2.1984468
  5.4696417   4.251729    8.8025255   4.26232     4.9216294   7.9483533
  4.526991    9.588897    8.017048    8.2831135   9.821576    4.1731563
  5.8485947   6.667628    7.001741    9.226937    2.9446237   2.5925434
  8.618449    8.161273    1.7101693   5.5401206   8.14539     2.020844
  0.89211583  5.7496552   4.987388    7.4813085   0.39198843  4.8122907
  2.6751356   5.1997175   7.827039    0.7617685   1.984582    6.211319
  7.497485    5.6864862   9.833999    4.343862    0.91428226  6.860134
  8.022065    5.7952833   5.549028    8.938169    0.7987054   1.0077511
  1.547055    2.4538176   5.900641    5.950689    2.172312    4.864265
  5.3084598   2.74433     7.4509587   4.642633    7.352649    1.6610278
  5.3932858   8.224104    1.7825346   4.6216807   6.3162665   2.1527483
  7.82622     0.4708147   9.143722    4.7161274   2.9691823   0.9679443
  3.9266372   6.638867    0.10056549  3.118843    7.3452816   6.4795094
  2.0939577   2.478408    8.811012    3.114293    2.1261327  10.111931
  4.855735    1.2398901   5.2605786   5.4892864   9.44359     1.744517
  0.8859111   0.84614795  8.730755    4.4898634   1.9797392   5.9254265
  5.903251    2.0259151   7.1823764   4.0827475   2.3360302   5.4425554
  3.0566494   1.579066    8.333391    1.4978724   3.288137    3.852397
  2.935655    2.1213212   6.9459066   3.5336087   1.8826928   5.0965667
  1.2400447   4.7354097   8.616984    2.6134915   5.8834224   5.1864443
 -0.13750565  6.549661    8.984314    0.7847537   7.2001395   6.8008256
  8.44678     2.6799302   3.2838757   7.0237226  -0.23040357  7.540698
  8.530038    4.6869135   3.2353654   3.3326626  -2.8252068   1.808005
  8.638641    9.090046    2.7026813   9.352778    3.2715847   2.764985
  2.5781574   8.643513    9.775191    3.6196568   6.3544054   2.67319
 -0.27903366  3.0261974   5.7475586   1.8039718   8.191788    4.8235297
  2.4119723   4.80837     7.8066583   3.14657     7.254359    3.8570547
  8.983632    8.136105    3.6973224   0.29414394  6.2837157   1.7246333
  1.2347451   8.727795    9.613968    2.8966768   3.0174162   1.6892313
  4.788908    8.051952    6.6716456   8.591153    1.2762189   7.83716
 10.090976    2.9488695   6.9059463   3.4287705   2.638495    9.951833
  3.8517146   1.0833437   3.8833115   4.4572515   7.453829    9.261691
 10.299599    8.9711075   4.912658    7.687476    2.8381176   0.8803614
  3.6029894   7.0204277   3.0225866   9.093265    9.016605    1.5790088
  2.4555895   8.069907    3.3349042   3.3253474   3.304729    8.906204
  0.14018849 -0.09102672  5.6968327   2.8784204   6.3043637   1.5840611
  9.080589    7.872192    0.68985736  8.022463    9.62591     9.730082
  6.885892    1.2239211   6.788951    3.103189    8.581392    7.224449
  6.0487485   6.9063663   4.2010665   4.5798826   9.059492    3.4417286
  0.25157684  4.0677304   8.905279    6.148643   11.682095    0.95629424
  9.730315    5.5144877   4.490063    4.5312085   1.2480376   3.6158848
  7.4273086  -0.96325046  1.452391    0.25324896  8.457037    6.1265316
  1.3514678   8.127087   10.009373    9.350058    8.951299   -0.29766494
  6.0847034   2.4157875   5.6192784   2.8229573   0.7195138   4.5521054
  4.8054247   9.962674    0.39720708  3.1612184   8.577424    7.115589
  7.7674336   6.298065    7.663883    4.695144    1.1639674   7.7367616
  5.669413    1.2667739   7.7379103   9.650189    3.731491    3.391371
  0.73656756  6.0238733   9.167239    1.2704167   4.995278    0.9358564
  0.38653633  6.6325297   8.341552    3.6262543   7.874937    2.0914352
  2.2575343   7.0230904   4.9926057   4.8522496   0.38922572  1.2943935
  0.4778885   8.9588175   1.688771    8.19581     3.4283693   8.456239
  2.8242724   2.5323164   0.56487453  2.687169    3.4400296   8.293358
  4.435119    8.654662    5.423925   -0.7138738   3.3961737   6.6198845
  2.3316503   8.031245    4.494654    0.59436065  7.459087    5.8713436
  0.84994775  5.0717716   3.7515552   9.185751    2.3369966   9.342911
  5.7315216   2.9590435   8.640119    7.4559712 ]
Epoch 1/1000
2023-09-09 22:45:39.071 
Epoch 1/1000 
	 loss: 517.2377, MinusLogProbMetric: 517.2377, val_loss: 421.2802, val_MinusLogProbMetric: 421.2802

Epoch 1: val_loss improved from inf to 421.28024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 103s - loss: 517.2377 - MinusLogProbMetric: 517.2377 - val_loss: 421.2802 - val_MinusLogProbMetric: 421.2802 - lr: 3.3333e-04 - 103s/epoch - 527ms/step
Epoch 2/1000
2023-09-09 22:45:57.428 
Epoch 2/1000 
	 loss: 419.0507, MinusLogProbMetric: 419.0507, val_loss: 432.9575, val_MinusLogProbMetric: 432.9575

Epoch 2: val_loss did not improve from 421.28024
196/196 - 17s - loss: 419.0507 - MinusLogProbMetric: 419.0507 - val_loss: 432.9575 - val_MinusLogProbMetric: 432.9575 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 3/1000
2023-09-09 22:46:15.308 
Epoch 3/1000 
	 loss: 418.5141, MinusLogProbMetric: 418.5141, val_loss: 418.9100, val_MinusLogProbMetric: 418.9100

Epoch 3: val_loss improved from 421.28024 to 418.91003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 418.5141 - MinusLogProbMetric: 418.5141 - val_loss: 418.9100 - val_MinusLogProbMetric: 418.9100 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 4/1000
2023-09-09 22:46:33.442 
Epoch 4/1000 
	 loss: 422.3633, MinusLogProbMetric: 422.3633, val_loss: 419.4293, val_MinusLogProbMetric: 419.4293

Epoch 4: val_loss did not improve from 418.91003
196/196 - 17s - loss: 422.3633 - MinusLogProbMetric: 422.3633 - val_loss: 419.4293 - val_MinusLogProbMetric: 419.4293 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 5/1000
2023-09-09 22:46:51.256 
Epoch 5/1000 
	 loss: 417.9160, MinusLogProbMetric: 417.9160, val_loss: 416.6026, val_MinusLogProbMetric: 416.6026

Epoch 5: val_loss improved from 418.91003 to 416.60263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 417.9160 - MinusLogProbMetric: 417.9160 - val_loss: 416.6026 - val_MinusLogProbMetric: 416.6026 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 6/1000
2023-09-09 22:47:10.191 
Epoch 6/1000 
	 loss: 417.1593, MinusLogProbMetric: 417.1593, val_loss: 415.5744, val_MinusLogProbMetric: 415.5744

Epoch 6: val_loss improved from 416.60263 to 415.57443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 417.1593 - MinusLogProbMetric: 417.1593 - val_loss: 415.5744 - val_MinusLogProbMetric: 415.5744 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 7/1000
2023-09-09 22:47:29.837 
Epoch 7/1000 
	 loss: 416.4486, MinusLogProbMetric: 416.4486, val_loss: 415.7911, val_MinusLogProbMetric: 415.7911

Epoch 7: val_loss did not improve from 415.57443
196/196 - 19s - loss: 416.4486 - MinusLogProbMetric: 416.4486 - val_loss: 415.7911 - val_MinusLogProbMetric: 415.7911 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 8/1000
2023-09-09 22:47:47.927 
Epoch 8/1000 
	 loss: 422.4765, MinusLogProbMetric: 422.4765, val_loss: 416.2570, val_MinusLogProbMetric: 416.2570

Epoch 8: val_loss did not improve from 415.57443
196/196 - 18s - loss: 422.4765 - MinusLogProbMetric: 422.4765 - val_loss: 416.2570 - val_MinusLogProbMetric: 416.2570 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 9/1000
2023-09-09 22:48:05.540 
Epoch 9/1000 
	 loss: 415.7693, MinusLogProbMetric: 415.7693, val_loss: 468.3760, val_MinusLogProbMetric: 468.3760

Epoch 9: val_loss did not improve from 415.57443
196/196 - 18s - loss: 415.7693 - MinusLogProbMetric: 415.7693 - val_loss: 468.3760 - val_MinusLogProbMetric: 468.3760 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 10/1000
2023-09-09 22:48:23.497 
Epoch 10/1000 
	 loss: 415.7254, MinusLogProbMetric: 415.7254, val_loss: 416.2915, val_MinusLogProbMetric: 416.2915

Epoch 10: val_loss did not improve from 415.57443
196/196 - 18s - loss: 415.7254 - MinusLogProbMetric: 415.7254 - val_loss: 416.2915 - val_MinusLogProbMetric: 416.2915 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 11/1000
2023-09-09 22:48:41.164 
Epoch 11/1000 
	 loss: 414.3176, MinusLogProbMetric: 414.3176, val_loss: 415.7478, val_MinusLogProbMetric: 415.7478

Epoch 11: val_loss did not improve from 415.57443
196/196 - 18s - loss: 414.3176 - MinusLogProbMetric: 414.3176 - val_loss: 415.7478 - val_MinusLogProbMetric: 415.7478 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 12/1000
2023-09-09 22:48:58.823 
Epoch 12/1000 
	 loss: 414.5116, MinusLogProbMetric: 414.5116, val_loss: 417.0798, val_MinusLogProbMetric: 417.0798

Epoch 12: val_loss did not improve from 415.57443
196/196 - 18s - loss: 414.5116 - MinusLogProbMetric: 414.5116 - val_loss: 417.0798 - val_MinusLogProbMetric: 417.0798 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 13/1000
2023-09-09 22:49:16.088 
Epoch 13/1000 
	 loss: 413.7731, MinusLogProbMetric: 413.7731, val_loss: 412.2990, val_MinusLogProbMetric: 412.2990

Epoch 13: val_loss improved from 415.57443 to 412.29901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 413.7731 - MinusLogProbMetric: 413.7731 - val_loss: 412.2990 - val_MinusLogProbMetric: 412.2990 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 14/1000
2023-09-09 22:49:34.420 
Epoch 14/1000 
	 loss: 412.9398, MinusLogProbMetric: 412.9398, val_loss: 414.8060, val_MinusLogProbMetric: 414.8060

Epoch 14: val_loss did not improve from 412.29901
196/196 - 18s - loss: 412.9398 - MinusLogProbMetric: 412.9398 - val_loss: 414.8060 - val_MinusLogProbMetric: 414.8060 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 15/1000
2023-09-09 22:49:51.755 
Epoch 15/1000 
	 loss: 418.7257, MinusLogProbMetric: 418.7257, val_loss: 414.1962, val_MinusLogProbMetric: 414.1962

Epoch 15: val_loss did not improve from 412.29901
196/196 - 17s - loss: 418.7257 - MinusLogProbMetric: 418.7257 - val_loss: 414.1962 - val_MinusLogProbMetric: 414.1962 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 16/1000
2023-09-09 22:50:09.345 
Epoch 16/1000 
	 loss: 411.8442, MinusLogProbMetric: 411.8442, val_loss: 415.9993, val_MinusLogProbMetric: 415.9993

Epoch 16: val_loss did not improve from 412.29901
196/196 - 18s - loss: 411.8442 - MinusLogProbMetric: 411.8442 - val_loss: 415.9993 - val_MinusLogProbMetric: 415.9993 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 17/1000
2023-09-09 22:50:26.765 
Epoch 17/1000 
	 loss: 411.8512, MinusLogProbMetric: 411.8512, val_loss: 412.4141, val_MinusLogProbMetric: 412.4141

Epoch 17: val_loss did not improve from 412.29901
196/196 - 17s - loss: 411.8512 - MinusLogProbMetric: 411.8512 - val_loss: 412.4141 - val_MinusLogProbMetric: 412.4141 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 18/1000
2023-09-09 22:50:44.181 
Epoch 18/1000 
	 loss: 411.0429, MinusLogProbMetric: 411.0429, val_loss: 414.5525, val_MinusLogProbMetric: 414.5525

Epoch 18: val_loss did not improve from 412.29901
196/196 - 17s - loss: 411.0429 - MinusLogProbMetric: 411.0429 - val_loss: 414.5525 - val_MinusLogProbMetric: 414.5525 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 19/1000
2023-09-09 22:51:01.307 
Epoch 19/1000 
	 loss: 410.4765, MinusLogProbMetric: 410.4765, val_loss: 420.5880, val_MinusLogProbMetric: 420.5880

Epoch 19: val_loss did not improve from 412.29901
196/196 - 17s - loss: 410.4765 - MinusLogProbMetric: 410.4765 - val_loss: 420.5880 - val_MinusLogProbMetric: 420.5880 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 20/1000
2023-09-09 22:51:18.824 
Epoch 20/1000 
	 loss: 412.9044, MinusLogProbMetric: 412.9044, val_loss: 411.0354, val_MinusLogProbMetric: 411.0354

Epoch 20: val_loss improved from 412.29901 to 411.03537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 412.9044 - MinusLogProbMetric: 412.9044 - val_loss: 411.0354 - val_MinusLogProbMetric: 411.0354 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 21/1000
2023-09-09 22:51:36.946 
Epoch 21/1000 
	 loss: 411.8607, MinusLogProbMetric: 411.8607, val_loss: 411.0638, val_MinusLogProbMetric: 411.0638

Epoch 21: val_loss did not improve from 411.03537
196/196 - 17s - loss: 411.8607 - MinusLogProbMetric: 411.8607 - val_loss: 411.0638 - val_MinusLogProbMetric: 411.0638 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 22/1000
2023-09-09 22:51:54.228 
Epoch 22/1000 
	 loss: 410.7249, MinusLogProbMetric: 410.7249, val_loss: 412.1369, val_MinusLogProbMetric: 412.1369

Epoch 22: val_loss did not improve from 411.03537
196/196 - 17s - loss: 410.7249 - MinusLogProbMetric: 410.7249 - val_loss: 412.1369 - val_MinusLogProbMetric: 412.1369 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 23/1000
2023-09-09 22:52:11.742 
Epoch 23/1000 
	 loss: 409.9505, MinusLogProbMetric: 409.9505, val_loss: 410.2614, val_MinusLogProbMetric: 410.2614

Epoch 23: val_loss improved from 411.03537 to 410.26144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 409.9505 - MinusLogProbMetric: 409.9505 - val_loss: 410.2614 - val_MinusLogProbMetric: 410.2614 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 24/1000
2023-09-09 22:52:29.406 
Epoch 24/1000 
	 loss: 408.8939, MinusLogProbMetric: 408.8939, val_loss: 413.7040, val_MinusLogProbMetric: 413.7040

Epoch 24: val_loss did not improve from 410.26144
196/196 - 17s - loss: 408.8939 - MinusLogProbMetric: 408.8939 - val_loss: 413.7040 - val_MinusLogProbMetric: 413.7040 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 25/1000
2023-09-09 22:52:46.675 
Epoch 25/1000 
	 loss: 408.7817, MinusLogProbMetric: 408.7817, val_loss: 412.2891, val_MinusLogProbMetric: 412.2891

Epoch 25: val_loss did not improve from 410.26144
196/196 - 17s - loss: 408.7817 - MinusLogProbMetric: 408.7817 - val_loss: 412.2891 - val_MinusLogProbMetric: 412.2891 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 26/1000
2023-09-09 22:53:03.790 
Epoch 26/1000 
	 loss: 408.7793, MinusLogProbMetric: 408.7793, val_loss: 412.9258, val_MinusLogProbMetric: 412.9258

Epoch 26: val_loss did not improve from 410.26144
196/196 - 17s - loss: 408.7793 - MinusLogProbMetric: 408.7793 - val_loss: 412.9258 - val_MinusLogProbMetric: 412.9258 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 27/1000
2023-09-09 22:53:21.088 
Epoch 27/1000 
	 loss: 408.4120, MinusLogProbMetric: 408.4120, val_loss: 410.6821, val_MinusLogProbMetric: 410.6821

Epoch 27: val_loss did not improve from 410.26144
196/196 - 17s - loss: 408.4120 - MinusLogProbMetric: 408.4120 - val_loss: 410.6821 - val_MinusLogProbMetric: 410.6821 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 28/1000
2023-09-09 22:53:38.739 
Epoch 28/1000 
	 loss: 408.5763, MinusLogProbMetric: 408.5763, val_loss: 431.7830, val_MinusLogProbMetric: 431.7830

Epoch 28: val_loss did not improve from 410.26144
196/196 - 18s - loss: 408.5763 - MinusLogProbMetric: 408.5763 - val_loss: 431.7830 - val_MinusLogProbMetric: 431.7830 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 29/1000
2023-09-09 22:53:57.074 
Epoch 29/1000 
	 loss: 408.5233, MinusLogProbMetric: 408.5233, val_loss: 411.8388, val_MinusLogProbMetric: 411.8388

Epoch 29: val_loss did not improve from 410.26144
196/196 - 18s - loss: 408.5233 - MinusLogProbMetric: 408.5233 - val_loss: 411.8388 - val_MinusLogProbMetric: 411.8388 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 30/1000
2023-09-09 22:54:14.537 
Epoch 30/1000 
	 loss: 408.1171, MinusLogProbMetric: 408.1171, val_loss: 407.5744, val_MinusLogProbMetric: 407.5744

Epoch 30: val_loss improved from 410.26144 to 407.57443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 408.1171 - MinusLogProbMetric: 408.1171 - val_loss: 407.5744 - val_MinusLogProbMetric: 407.5744 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 31/1000
2023-09-09 22:54:32.898 
Epoch 31/1000 
	 loss: 407.6483, MinusLogProbMetric: 407.6483, val_loss: 408.3559, val_MinusLogProbMetric: 408.3559

Epoch 31: val_loss did not improve from 407.57443
196/196 - 18s - loss: 407.6483 - MinusLogProbMetric: 407.6483 - val_loss: 408.3559 - val_MinusLogProbMetric: 408.3559 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 32/1000
2023-09-09 22:54:50.269 
Epoch 32/1000 
	 loss: 407.0976, MinusLogProbMetric: 407.0976, val_loss: 408.3067, val_MinusLogProbMetric: 408.3067

Epoch 32: val_loss did not improve from 407.57443
196/196 - 17s - loss: 407.0976 - MinusLogProbMetric: 407.0976 - val_loss: 408.3067 - val_MinusLogProbMetric: 408.3067 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 33/1000
2023-09-09 22:55:08.321 
Epoch 33/1000 
	 loss: 406.7052, MinusLogProbMetric: 406.7052, val_loss: 412.7294, val_MinusLogProbMetric: 412.7294

Epoch 33: val_loss did not improve from 407.57443
196/196 - 18s - loss: 406.7052 - MinusLogProbMetric: 406.7052 - val_loss: 412.7294 - val_MinusLogProbMetric: 412.7294 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 34/1000
2023-09-09 22:55:26.186 
Epoch 34/1000 
	 loss: 407.1736, MinusLogProbMetric: 407.1736, val_loss: 417.4289, val_MinusLogProbMetric: 417.4289

Epoch 34: val_loss did not improve from 407.57443
196/196 - 18s - loss: 407.1736 - MinusLogProbMetric: 407.1736 - val_loss: 417.4289 - val_MinusLogProbMetric: 417.4289 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 35/1000
2023-09-09 22:55:44.715 
Epoch 35/1000 
	 loss: 406.5316, MinusLogProbMetric: 406.5316, val_loss: 409.1339, val_MinusLogProbMetric: 409.1339

Epoch 35: val_loss did not improve from 407.57443
196/196 - 19s - loss: 406.5316 - MinusLogProbMetric: 406.5316 - val_loss: 409.1339 - val_MinusLogProbMetric: 409.1339 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 36/1000
2023-09-09 22:56:02.562 
Epoch 36/1000 
	 loss: 406.3759, MinusLogProbMetric: 406.3759, val_loss: 418.2486, val_MinusLogProbMetric: 418.2486

Epoch 36: val_loss did not improve from 407.57443
196/196 - 18s - loss: 406.3759 - MinusLogProbMetric: 406.3759 - val_loss: 418.2486 - val_MinusLogProbMetric: 418.2486 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 37/1000
2023-09-09 22:56:20.683 
Epoch 37/1000 
	 loss: 406.1836, MinusLogProbMetric: 406.1836, val_loss: 412.7282, val_MinusLogProbMetric: 412.7282

Epoch 37: val_loss did not improve from 407.57443
196/196 - 18s - loss: 406.1836 - MinusLogProbMetric: 406.1836 - val_loss: 412.7282 - val_MinusLogProbMetric: 412.7282 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 38/1000
2023-09-09 22:56:38.981 
Epoch 38/1000 
	 loss: 405.7382, MinusLogProbMetric: 405.7382, val_loss: 408.3990, val_MinusLogProbMetric: 408.3990

Epoch 38: val_loss did not improve from 407.57443
196/196 - 18s - loss: 405.7382 - MinusLogProbMetric: 405.7382 - val_loss: 408.3990 - val_MinusLogProbMetric: 408.3990 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 39/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 113: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-09 22:56:50.461 
Epoch 39/1000 
	 loss: nan, MinusLogProbMetric: 358537979460834562146304.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 39: val_loss did not improve from 407.57443
196/196 - 11s - loss: nan - MinusLogProbMetric: 358537979460834562146304.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 11s/epoch - 59ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 328.
===========
Train data generated in 2.45 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.6836276 , -0.60898554,  4.901356  , ...,  4.578798  ,
         5.78743   ,  5.8673487 ],
       [ 8.249707  ,  4.460014  ,  5.2482004 , ...,  1.7374016 ,
         8.867281  ,  6.8796477 ],
       [ 6.103276  ,  7.069252  ,  6.2995415 , ..., 11.163949  ,
         2.6241696 ,  6.7860346 ],
       ...,
       [ 5.7729836 ,  0.39295188,  4.752078  , ...,  4.8545365 ,
         6.277194  ,  5.1011395 ],
       [ 8.14715   ,  4.7503    ,  5.215126  , ...,  3.7450724 ,
         8.238651  ,  6.647562  ],
       [ 5.6715584 ,  7.2547503 ,  6.700116  , ..., 10.850491  ,
         1.1671181 ,  6.458376  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_328
self.data_kwargs: {'seed': 187}
self.x_data: [[8.405046   4.6301365  5.2287283  ... 2.9590435  8.640119   7.4559712 ]
 [6.027003   0.2016806  4.6617384  ... 4.692235   6.373698   5.3374834 ]
 [5.680177   0.10050362 4.564641   ... 4.4050727  6.0966487  6.0310545 ]
 ...
 [6.204324   0.5308406  4.687448   ... 4.736321   6.117902   4.3074207 ]
 [7.884271   4.844002   5.2654357  ... 2.2312212  7.9686017  6.4498177 ]
 [5.7032723  1.704168   4.78966    ... 5.263417   6.7690263  5.8227935 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7faafc41c880>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7faabc47d8a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7faabc47d8a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7faafc41f2b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7faafc1b9c00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faafc1ba0e0>, <keras.callbacks.ModelCheckpoint object at 0x7faafc1ba1a0>, <keras.callbacks.EarlyStopping object at 0x7faafc1ba410>, <keras.callbacks.ReduceLROnPlateau object at 0x7faafc1ba440>, <keras.callbacks.TerminateOnNaN object at 0x7faafc1ba080>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.6836276 , -0.60898554,  4.901356  , ...,  4.578798  ,
         5.78743   ,  5.8673487 ],
       [ 8.249707  ,  4.460014  ,  5.2482004 , ...,  1.7374016 ,
         8.867281  ,  6.8796477 ],
       [ 6.103276  ,  7.069252  ,  6.2995415 , ..., 11.163949  ,
         2.6241696 ,  6.7860346 ],
       ...,
       [ 5.7729836 ,  0.39295188,  4.752078  , ...,  4.8545365 ,
         6.277194  ,  5.1011395 ],
       [ 8.14715   ,  4.7503    ,  5.215126  , ...,  3.7450724 ,
         8.238651  ,  6.647562  ],
       [ 5.6715584 ,  7.2547503 ,  6.700116  , ..., 10.850491  ,
         1.1671181 ,  6.458376  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 328/360 with hyperparameters:
timestamp = 2023-09-09 22:56:59.764382
ndims = 1000
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 8.405046    4.6301365   5.2287283   2.4556472   6.2928953   3.594312
  6.0004463   1.8052834   1.8765529   4.0951214   4.4015846   3.3011546
  1.1913271   0.29790348  0.444705    2.0839415   4.8839054   7.6232376
  5.0511703  10.082624    1.0256902   1.8862149   3.3338983   5.485959
  8.678817    4.2863455   2.746828    0.5995625   8.487818    4.326282
  5.4844646   9.668893    0.9005003   2.511495    7.4595304   8.076063
  7.482936    5.8244066  10.038453    4.2971616   6.199124    0.14504078
  4.115744    7.085761    2.2268302   8.105376    6.8798943   3.1965656
  7.803503    1.4473212   9.836955    1.2955343   9.292721    2.2835047
  7.347313   -0.344647    2.3793294   7.1599417   6.239404    1.8819436
  3.6855946   3.1096044   3.9889503   6.345884    8.7655735   3.506798
  3.8915956   1.4826899   7.7798705   6.4561133   3.7245452   5.022784
  6.785575    1.8807405  -0.53786993  3.331518    6.3648286   5.060461
  5.671335    8.188019    5.050044    0.8326328   9.021144    5.101296
  8.440869    7.8830547   9.847927    2.9928007   7.1174664   4.000934
  4.0642667   3.5513847   3.3835356   9.485252    2.8910272   2.857944
  5.1031704   2.365148    4.392073    7.9044094   2.2182727   3.5326295
  6.429046    5.444268    9.853491    6.801449    7.615357    9.654012
  4.2561636   3.94246     0.5266583   1.6963573   0.9076003   1.6972958
  8.415306    6.55893     6.517854    7.7323823   2.263416    3.867008
  7.0743175   4.310728    0.44073427  5.454958    8.770797    4.023472
  3.8313503   9.876201    6.083322    4.588916    0.97578037  8.022623
  6.904659    9.723064    1.5232677   1.2301477   7.2426834   2.4050941
  1.7633336   1.3129089   8.420942    5.7998257   0.4667193   4.692241
  6.5908155   5.257252    5.725936    2.6134639   8.439922    5.6147094
  7.455604    9.777677    4.190657    9.502805    0.30458957  0.2189286
  5.797137    3.0544481   2.6193647   2.5525932   3.284805    7.4161353
  5.749028    7.889573    7.898816    8.574444    3.8091307   4.298687
  9.481366    6.0068207   1.523428    5.6352916   4.365734    9.867473
  1.3835607   6.134872    7.2581935   2.8856063   9.735012    7.5791473
  9.23718     0.3085858   6.8750105   3.0606773   9.2376585   6.005112
  5.5536275   0.9032261   6.0153728   6.3190417   9.369196    5.911083
  9.520276   10.025198    7.444127    4.7846828   6.6327243  10.038699
  0.737067    1.1364235   0.7629767   7.3339915   3.2413385   6.3641744
  4.723883    3.890898    5.1846333   6.4865723   9.4478655   5.8353424
  6.1596503   0.8805034  10.229148    5.1357536   8.719589    0.61422366
  4.6646357   2.5584815   2.6442034   7.3017416   5.692237    3.2181895
  0.7464954   9.628433    9.805298    0.8460964   0.74706554  6.1736875
  9.794054    9.817206    5.73489     2.8118153   2.995823    9.004283
  3.2283762   1.9297047   2.5467188   2.732583    2.62711     5.3906684
  8.859532    7.221753    0.22360349  0.31767347  3.3918247   3.5905416
  8.748679    9.150864    4.0089087   6.819023    9.95121     7.2592616
  6.618907    0.7986359   3.617743    8.477222    2.5446649   8.356348
  6.784008    2.7764812   5.149644    5.879593    6.8204975   4.7749815
  1.5671438   7.5464864   2.9128666   9.640756    2.7195537   2.422054
  1.9658555   5.744532   10.17906     1.88257     3.3741965   5.759674
  4.4506254   5.799042    8.574516    9.521442    4.848262    7.099014
  7.410928    4.176586    2.7669337   6.7037125   6.396343    9.116149
  0.6767711   8.802082    5.400854   -0.46128303  4.315248    7.679634
  4.337188    9.473796    4.3057194   3.8800864   8.321836    5.464394
  6.8586717   7.8846817   6.758233    4.5121193   1.5240693   5.1997232
  7.966815    3.110014    9.149139    5.2918534   6.043538    8.5325165
  7.029521    0.71871924  9.851924    2.0979629   7.107039    5.8480177
  4.6838837   2.2520099   7.556054    1.2082794  10.557993    8.230412
  5.3685956   3.7290514   3.748278    4.2430515   8.3245535   2.619837
  3.433112    1.6606942   2.8829854   1.0669343   9.924688    5.8388543
  6.973813    0.9312429   3.8876076   8.887293    0.8722724   2.881507
  6.283689    6.159739    8.345073    9.115302    1.4750379   2.7552269
  4.2736816   0.15881406 -0.24159682  2.4190745   4.887695    2.6831913
  8.220071    3.2441287   8.8378105   9.509058    3.559123    3.1608875
  3.3725164   7.9289885   2.9474692   4.059424    4.7409554   5.2013454
  9.994335    2.323355    3.3355927   2.5336983   3.2941818   2.2210457
  3.2414346   9.904477    2.334639    4.492616   10.475705    7.142849
  4.6010633   7.570345    1.3164223   4.2845144   0.5250243   2.3362412
  2.8345227   3.6286557   4.130509    9.905742    2.1830826   4.842215
  6.807653    7.5327077   7.2363663   2.9334166   5.234727    1.7384282
  2.0550947   4.500183    7.394765    4.995599    7.815772   10.710524
 -0.19277403  5.792584    6.1900487   9.671367    0.28583023  5.668808
  1.2390957   0.5417446   6.8533115   7.9632573   8.180385   -0.4858576
  4.0685763   2.4839914   2.1516795   1.9025909   2.110797    2.6641178
  2.6061893   9.366816    4.2622356   8.9237175   1.1875978   7.9966984
  0.44503674  4.7554154  -0.20823273  7.945292   10.080656    0.3404049
  8.932869    8.365952    3.7764497   9.093311    2.3442857   4.1149993
  0.99684995  6.8027267   0.3532521   4.712874    6.197236    8.98079
  0.7532523   7.324172    6.816783    7.1629744   5.789584    1.8008664
  6.6962724   3.5133753   8.672127    1.7033234   5.757915    3.8499484
  7.6907835   2.515215    0.675339    6.6755958   4.1274943   5.9000664
  0.6306852  10.170123    8.690909    5.6241055   0.83715045  3.7537289
  9.811078    7.1486425   1.5089558   4.5211      9.977777    5.041157
  0.20099962  7.1491528   2.411008    0.98729235  4.0264      6.1107965
  0.8201466   6.400518    5.2138658   8.338983    4.008616    5.4276114
  7.5568547   9.328188    3.5043116   5.993576    6.732445    0.1456024
  8.195436    4.0946035   6.8117595   5.4540176   7.008695    7.6902666
  1.121738    6.662947    9.421039    3.6054578   8.282917    6.2551546
  8.867136    2.7717266   1.666508    1.9828001   2.582295    7.401628
  7.3309445   6.1640267   2.2886527   8.620049    9.976319    5.427213
  6.492129    5.227495    2.6098144   3.1037233   2.451418    1.547883
  9.130067    9.089639    3.8307672   2.809064    9.412396    3.0656943
  5.384379    1.269433    2.1198947   5.4160995   5.9553137   6.8595047
  8.024387    9.932428    7.275275    5.4964776   9.2686615  -0.8320414
  6.634356    3.5646963   1.6928086   9.683453    3.4941626   6.2850795
  8.140037    5.0819607   1.6098254   8.515681    3.746112   10.339378
  9.723754    8.560821    3.4616127   8.136909    8.474316    5.334234
  4.3071146   0.3017891   7.5740643   8.525242    1.961537    2.785501
  4.6199455   0.20740837  0.8326539   4.594327    2.569767    3.3214014
  4.017394    9.219309   10.736801    1.9790065   8.832557    0.9678319
  5.233325   -0.19063437  3.1312854   8.050418    4.191532    9.204013
  7.268445    9.203028    4.342827    8.019755    3.3341756   3.743745
  3.9635062   3.979517    0.6836107   5.098467    0.71361184  8.4552765
  0.5069707   4.833144    3.1548538   1.8944778   7.8488803   7.0419393
  1.2548511   6.628743    7.354838    8.050705    1.1345416   6.8048496
  6.528121    8.252401    4.5538      4.949585    8.730731    7.4312305
  5.643753    2.5072966   8.962284    4.0641007   0.7590125   5.4753785
  9.738958    4.561652   -0.360509    6.1097317   9.957747    7.0762954
  5.690881    8.773466    8.0799      5.2899666   6.263382    7.4875503
  8.585341    0.6384524   8.029185    4.3010435   6.264173    7.7155027
  5.2268887   1.5704446   4.016696    0.5981313   6.7822347   9.855953
  7.53499     5.1501827   5.5134096   2.8642232   5.220349    3.4915414
  4.7451196   2.6037292   9.620164    6.549827    0.7027252  -0.15775523
 10.00958     3.973167    3.273789    1.7160401   9.183787    7.1865473
  5.1355295   4.8474526   3.5554662   6.6154428   4.344493    2.1984468
  5.4696417   4.251729    8.8025255   4.26232     4.9216294   7.9483533
  4.526991    9.588897    8.017048    8.2831135   9.821576    4.1731563
  5.8485947   6.667628    7.001741    9.226937    2.9446237   2.5925434
  8.618449    8.161273    1.7101693   5.5401206   8.14539     2.020844
  0.89211583  5.7496552   4.987388    7.4813085   0.39198843  4.8122907
  2.6751356   5.1997175   7.827039    0.7617685   1.984582    6.211319
  7.497485    5.6864862   9.833999    4.343862    0.91428226  6.860134
  8.022065    5.7952833   5.549028    8.938169    0.7987054   1.0077511
  1.547055    2.4538176   5.900641    5.950689    2.172312    4.864265
  5.3084598   2.74433     7.4509587   4.642633    7.352649    1.6610278
  5.3932858   8.224104    1.7825346   4.6216807   6.3162665   2.1527483
  7.82622     0.4708147   9.143722    4.7161274   2.9691823   0.9679443
  3.9266372   6.638867    0.10056549  3.118843    7.3452816   6.4795094
  2.0939577   2.478408    8.811012    3.114293    2.1261327  10.111931
  4.855735    1.2398901   5.2605786   5.4892864   9.44359     1.744517
  0.8859111   0.84614795  8.730755    4.4898634   1.9797392   5.9254265
  5.903251    2.0259151   7.1823764   4.0827475   2.3360302   5.4425554
  3.0566494   1.579066    8.333391    1.4978724   3.288137    3.852397
  2.935655    2.1213212   6.9459066   3.5336087   1.8826928   5.0965667
  1.2400447   4.7354097   8.616984    2.6134915   5.8834224   5.1864443
 -0.13750565  6.549661    8.984314    0.7847537   7.2001395   6.8008256
  8.44678     2.6799302   3.2838757   7.0237226  -0.23040357  7.540698
  8.530038    4.6869135   3.2353654   3.3326626  -2.8252068   1.808005
  8.638641    9.090046    2.7026813   9.352778    3.2715847   2.764985
  2.5781574   8.643513    9.775191    3.6196568   6.3544054   2.67319
 -0.27903366  3.0261974   5.7475586   1.8039718   8.191788    4.8235297
  2.4119723   4.80837     7.8066583   3.14657     7.254359    3.8570547
  8.983632    8.136105    3.6973224   0.29414394  6.2837157   1.7246333
  1.2347451   8.727795    9.613968    2.8966768   3.0174162   1.6892313
  4.788908    8.051952    6.6716456   8.591153    1.2762189   7.83716
 10.090976    2.9488695   6.9059463   3.4287705   2.638495    9.951833
  3.8517146   1.0833437   3.8833115   4.4572515   7.453829    9.261691
 10.299599    8.9711075   4.912658    7.687476    2.8381176   0.8803614
  3.6029894   7.0204277   3.0225866   9.093265    9.016605    1.5790088
  2.4555895   8.069907    3.3349042   3.3253474   3.304729    8.906204
  0.14018849 -0.09102672  5.6968327   2.8784204   6.3043637   1.5840611
  9.080589    7.872192    0.68985736  8.022463    9.62591     9.730082
  6.885892    1.2239211   6.788951    3.103189    8.581392    7.224449
  6.0487485   6.9063663   4.2010665   4.5798826   9.059492    3.4417286
  0.25157684  4.0677304   8.905279    6.148643   11.682095    0.95629424
  9.730315    5.5144877   4.490063    4.5312085   1.2480376   3.6158848
  7.4273086  -0.96325046  1.452391    0.25324896  8.457037    6.1265316
  1.3514678   8.127087   10.009373    9.350058    8.951299   -0.29766494
  6.0847034   2.4157875   5.6192784   2.8229573   0.7195138   4.5521054
  4.8054247   9.962674    0.39720708  3.1612184   8.577424    7.115589
  7.7674336   6.298065    7.663883    4.695144    1.1639674   7.7367616
  5.669413    1.2667739   7.7379103   9.650189    3.731491    3.391371
  0.73656756  6.0238733   9.167239    1.2704167   4.995278    0.9358564
  0.38653633  6.6325297   8.341552    3.6262543   7.874937    2.0914352
  2.2575343   7.0230904   4.9926057   4.8522496   0.38922572  1.2943935
  0.4778885   8.9588175   1.688771    8.19581     3.4283693   8.456239
  2.8242724   2.5323164   0.56487453  2.687169    3.4400296   8.293358
  4.435119    8.654662    5.423925   -0.7138738   3.3961737   6.6198845
  2.3316503   8.031245    4.494654    0.59436065  7.459087    5.8713436
  0.84994775  5.0717716   3.7515552   9.185751    2.3369966   9.342911
  5.7315216   2.9590435   8.640119    7.4559712 ]
Epoch 1/1000
2023-09-09 22:58:39.557 
Epoch 1/1000 
	 loss: 409.9258, MinusLogProbMetric: 409.9258, val_loss: 404.0000, val_MinusLogProbMetric: 404.0000

Epoch 1: val_loss improved from inf to 404.00003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 100s - loss: 409.9258 - MinusLogProbMetric: 409.9258 - val_loss: 404.0000 - val_MinusLogProbMetric: 404.0000 - lr: 1.1111e-04 - 100s/epoch - 512ms/step
Epoch 2/1000
2023-09-09 22:58:59.484 
Epoch 2/1000 
	 loss: 400.1786, MinusLogProbMetric: 400.1786, val_loss: 403.7527, val_MinusLogProbMetric: 403.7527

Epoch 2: val_loss improved from 404.00003 to 403.75269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 400.1786 - MinusLogProbMetric: 400.1786 - val_loss: 403.7527 - val_MinusLogProbMetric: 403.7527 - lr: 1.1111e-04 - 19s/epoch - 99ms/step
Epoch 3/1000
2023-09-09 22:59:18.171 
Epoch 3/1000 
	 loss: 400.0321, MinusLogProbMetric: 400.0321, val_loss: 402.7279, val_MinusLogProbMetric: 402.7279

Epoch 3: val_loss improved from 403.75269 to 402.72787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 400.0321 - MinusLogProbMetric: 400.0321 - val_loss: 402.7279 - val_MinusLogProbMetric: 402.7279 - lr: 1.1111e-04 - 19s/epoch - 96ms/step
Epoch 4/1000
2023-09-09 22:59:36.770 
Epoch 4/1000 
	 loss: 400.0957, MinusLogProbMetric: 400.0957, val_loss: 403.2227, val_MinusLogProbMetric: 403.2227

Epoch 4: val_loss did not improve from 402.72787
196/196 - 18s - loss: 400.0957 - MinusLogProbMetric: 400.0957 - val_loss: 403.2227 - val_MinusLogProbMetric: 403.2227 - lr: 1.1111e-04 - 18s/epoch - 91ms/step
Epoch 5/1000
2023-09-09 22:59:54.617 
Epoch 5/1000 
	 loss: 399.5874, MinusLogProbMetric: 399.5874, val_loss: 403.3918, val_MinusLogProbMetric: 403.3918

Epoch 5: val_loss did not improve from 402.72787
196/196 - 18s - loss: 399.5874 - MinusLogProbMetric: 399.5874 - val_loss: 403.3918 - val_MinusLogProbMetric: 403.3918 - lr: 1.1111e-04 - 18s/epoch - 91ms/step
Epoch 6/1000
2023-09-09 23:00:11.895 
Epoch 6/1000 
	 loss: 399.5123, MinusLogProbMetric: 399.5123, val_loss: 403.8428, val_MinusLogProbMetric: 403.8428

Epoch 6: val_loss did not improve from 402.72787
196/196 - 17s - loss: 399.5123 - MinusLogProbMetric: 399.5123 - val_loss: 403.8428 - val_MinusLogProbMetric: 403.8428 - lr: 1.1111e-04 - 17s/epoch - 88ms/step
Epoch 7/1000
2023-09-09 23:00:29.126 
Epoch 7/1000 
	 loss: 400.5437, MinusLogProbMetric: 400.5437, val_loss: 400.9714, val_MinusLogProbMetric: 400.9714

Epoch 7: val_loss improved from 402.72787 to 400.97137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 400.5437 - MinusLogProbMetric: 400.5437 - val_loss: 400.9714 - val_MinusLogProbMetric: 400.9714 - lr: 1.1111e-04 - 18s/epoch - 92ms/step
Epoch 8/1000
2023-09-09 23:00:47.107 
Epoch 8/1000 
	 loss: 399.1844, MinusLogProbMetric: 399.1844, val_loss: 402.2153, val_MinusLogProbMetric: 402.2153

Epoch 8: val_loss did not improve from 400.97137
196/196 - 17s - loss: 399.1844 - MinusLogProbMetric: 399.1844 - val_loss: 402.2153 - val_MinusLogProbMetric: 402.2153 - lr: 1.1111e-04 - 17s/epoch - 88ms/step
Epoch 9/1000
2023-09-09 23:01:04.728 
Epoch 9/1000 
	 loss: 398.7753, MinusLogProbMetric: 398.7753, val_loss: 403.3760, val_MinusLogProbMetric: 403.3760

Epoch 9: val_loss did not improve from 400.97137
196/196 - 18s - loss: 398.7753 - MinusLogProbMetric: 398.7753 - val_loss: 403.3760 - val_MinusLogProbMetric: 403.3760 - lr: 1.1111e-04 - 18s/epoch - 90ms/step
Epoch 10/1000
2023-09-09 23:01:23.095 
Epoch 10/1000 
	 loss: 399.1978, MinusLogProbMetric: 399.1978, val_loss: 400.8038, val_MinusLogProbMetric: 400.8038

Epoch 10: val_loss improved from 400.97137 to 400.80383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 19s - loss: 399.1978 - MinusLogProbMetric: 399.1978 - val_loss: 400.8038 - val_MinusLogProbMetric: 400.8038 - lr: 1.1111e-04 - 19s/epoch - 98ms/step
Epoch 11/1000
2023-09-09 23:01:41.224 
Epoch 11/1000 
	 loss: 398.6557, MinusLogProbMetric: 398.6557, val_loss: 401.4290, val_MinusLogProbMetric: 401.4290

Epoch 11: val_loss did not improve from 400.80383
196/196 - 17s - loss: 398.6557 - MinusLogProbMetric: 398.6557 - val_loss: 401.4290 - val_MinusLogProbMetric: 401.4290 - lr: 1.1111e-04 - 17s/epoch - 88ms/step
Epoch 12/1000
2023-09-09 23:01:59.089 
Epoch 12/1000 
	 loss: 398.7681, MinusLogProbMetric: 398.7681, val_loss: 400.8387, val_MinusLogProbMetric: 400.8387

Epoch 12: val_loss did not improve from 400.80383
196/196 - 18s - loss: 398.7681 - MinusLogProbMetric: 398.7681 - val_loss: 400.8387 - val_MinusLogProbMetric: 400.8387 - lr: 1.1111e-04 - 18s/epoch - 91ms/step
Epoch 13/1000
2023-09-09 23:02:17.018 
Epoch 13/1000 
	 loss: 398.7624, MinusLogProbMetric: 398.7624, val_loss: 401.5327, val_MinusLogProbMetric: 401.5327

Epoch 13: val_loss did not improve from 400.80383
196/196 - 18s - loss: 398.7624 - MinusLogProbMetric: 398.7624 - val_loss: 401.5327 - val_MinusLogProbMetric: 401.5327 - lr: 1.1111e-04 - 18s/epoch - 92ms/step
Epoch 14/1000
2023-09-09 23:02:34.478 
Epoch 14/1000 
	 loss: 398.5461, MinusLogProbMetric: 398.5461, val_loss: 401.7472, val_MinusLogProbMetric: 401.7472

Epoch 14: val_loss did not improve from 400.80383
196/196 - 17s - loss: 398.5461 - MinusLogProbMetric: 398.5461 - val_loss: 401.7472 - val_MinusLogProbMetric: 401.7472 - lr: 1.1111e-04 - 17s/epoch - 89ms/step
Epoch 15/1000
2023-09-09 23:02:51.778 
Epoch 15/1000 
	 loss: 398.5009, MinusLogProbMetric: 398.5009, val_loss: 401.4723, val_MinusLogProbMetric: 401.4723

Epoch 15: val_loss did not improve from 400.80383
196/196 - 17s - loss: 398.5009 - MinusLogProbMetric: 398.5009 - val_loss: 401.4723 - val_MinusLogProbMetric: 401.4723 - lr: 1.1111e-04 - 17s/epoch - 88ms/step
Epoch 16/1000
2023-09-09 23:03:08.632 
Epoch 16/1000 
	 loss: 398.0228, MinusLogProbMetric: 398.0228, val_loss: 400.8734, val_MinusLogProbMetric: 400.8734

Epoch 16: val_loss did not improve from 400.80383
196/196 - 17s - loss: 398.0228 - MinusLogProbMetric: 398.0228 - val_loss: 400.8734 - val_MinusLogProbMetric: 400.8734 - lr: 1.1111e-04 - 17s/epoch - 86ms/step
Epoch 17/1000
2023-09-09 23:03:24.613 
Epoch 17/1000 
	 loss: 397.9698, MinusLogProbMetric: 397.9698, val_loss: 400.1923, val_MinusLogProbMetric: 400.1923

Epoch 17: val_loss improved from 400.80383 to 400.19226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 17s - loss: 397.9698 - MinusLogProbMetric: 397.9698 - val_loss: 400.1923 - val_MinusLogProbMetric: 400.1923 - lr: 1.1111e-04 - 17s/epoch - 87ms/step
Epoch 18/1000
2023-09-09 23:03:41.736 
Epoch 18/1000 
	 loss: 397.9491, MinusLogProbMetric: 397.9491, val_loss: 401.9201, val_MinusLogProbMetric: 401.9201

Epoch 18: val_loss did not improve from 400.19226
196/196 - 16s - loss: 397.9491 - MinusLogProbMetric: 397.9491 - val_loss: 401.9201 - val_MinusLogProbMetric: 401.9201 - lr: 1.1111e-04 - 16s/epoch - 82ms/step
Epoch 19/1000
2023-09-09 23:03:59.781 
Epoch 19/1000 
	 loss: 397.4425, MinusLogProbMetric: 397.4425, val_loss: 400.7199, val_MinusLogProbMetric: 400.7199

Epoch 19: val_loss did not improve from 400.19226
196/196 - 18s - loss: 397.4425 - MinusLogProbMetric: 397.4425 - val_loss: 400.7199 - val_MinusLogProbMetric: 400.7199 - lr: 1.1111e-04 - 18s/epoch - 92ms/step
Epoch 20/1000
2023-09-09 23:04:18.267 
Epoch 20/1000 
	 loss: 397.8364, MinusLogProbMetric: 397.8364, val_loss: 401.0792, val_MinusLogProbMetric: 401.0792

Epoch 20: val_loss did not improve from 400.19226
196/196 - 18s - loss: 397.8364 - MinusLogProbMetric: 397.8364 - val_loss: 401.0792 - val_MinusLogProbMetric: 401.0792 - lr: 1.1111e-04 - 18s/epoch - 94ms/step
Epoch 21/1000
2023-09-09 23:04:35.347 
Epoch 21/1000 
	 loss: 398.3296, MinusLogProbMetric: 398.3296, val_loss: 403.0977, val_MinusLogProbMetric: 403.0977

Epoch 21: val_loss did not improve from 400.19226
196/196 - 17s - loss: 398.3296 - MinusLogProbMetric: 398.3296 - val_loss: 403.0977 - val_MinusLogProbMetric: 403.0977 - lr: 1.1111e-04 - 17s/epoch - 87ms/step
Epoch 22/1000
2023-09-09 23:04:52.421 
Epoch 22/1000 
	 loss: 397.1713, MinusLogProbMetric: 397.1713, val_loss: 400.0262, val_MinusLogProbMetric: 400.0262

Epoch 22: val_loss improved from 400.19226 to 400.02625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 18s - loss: 397.1713 - MinusLogProbMetric: 397.1713 - val_loss: 400.0262 - val_MinusLogProbMetric: 400.0262 - lr: 1.1111e-04 - 18s/epoch - 91ms/step
Epoch 23/1000
2023-09-09 23:05:09.844 
Epoch 23/1000 
	 loss: 397.7164, MinusLogProbMetric: 397.7164, val_loss: 401.3403, val_MinusLogProbMetric: 401.3403

Epoch 23: val_loss did not improve from 400.02625
196/196 - 17s - loss: 397.7164 - MinusLogProbMetric: 397.7164 - val_loss: 401.3403 - val_MinusLogProbMetric: 401.3403 - lr: 1.1111e-04 - 17s/epoch - 85ms/step
Epoch 24/1000
2023-09-09 23:05:26.626 
Epoch 24/1000 
	 loss: 397.5652, MinusLogProbMetric: 397.5652, val_loss: 400.3055, val_MinusLogProbMetric: 400.3055

Epoch 24: val_loss did not improve from 400.02625
196/196 - 17s - loss: 397.5652 - MinusLogProbMetric: 397.5652 - val_loss: 400.3055 - val_MinusLogProbMetric: 400.3055 - lr: 1.1111e-04 - 17s/epoch - 86ms/step
Epoch 25/1000
2023-09-09 23:05:43.334 
Epoch 25/1000 
	 loss: 397.0808, MinusLogProbMetric: 397.0808, val_loss: 399.9687, val_MinusLogProbMetric: 399.9687

Epoch 25: val_loss improved from 400.02625 to 399.96866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 17s - loss: 397.0808 - MinusLogProbMetric: 397.0808 - val_loss: 399.9687 - val_MinusLogProbMetric: 399.9687 - lr: 1.1111e-04 - 17s/epoch - 89ms/step
Epoch 26/1000
2023-09-09 23:06:00.656 
Epoch 26/1000 
	 loss: 397.0238, MinusLogProbMetric: 397.0238, val_loss: 409.2533, val_MinusLogProbMetric: 409.2533

Epoch 26: val_loss did not improve from 399.96866
196/196 - 17s - loss: 397.0238 - MinusLogProbMetric: 397.0238 - val_loss: 409.2533 - val_MinusLogProbMetric: 409.2533 - lr: 1.1111e-04 - 17s/epoch - 85ms/step
Epoch 27/1000
2023-09-09 23:06:17.359 
Epoch 27/1000 
	 loss: 397.2428, MinusLogProbMetric: 397.2428, val_loss: 401.4650, val_MinusLogProbMetric: 401.4650

Epoch 27: val_loss did not improve from 399.96866
196/196 - 17s - loss: 397.2428 - MinusLogProbMetric: 397.2428 - val_loss: 401.4650 - val_MinusLogProbMetric: 401.4650 - lr: 1.1111e-04 - 17s/epoch - 85ms/step
Epoch 28/1000
2023-09-09 23:06:33.603 
Epoch 28/1000 
	 loss: 397.0617, MinusLogProbMetric: 397.0617, val_loss: 400.3744, val_MinusLogProbMetric: 400.3744

Epoch 28: val_loss did not improve from 399.96866
196/196 - 16s - loss: 397.0617 - MinusLogProbMetric: 397.0617 - val_loss: 400.3744 - val_MinusLogProbMetric: 400.3744 - lr: 1.1111e-04 - 16s/epoch - 83ms/step
Epoch 29/1000
2023-09-09 23:06:51.775 
Epoch 29/1000 
	 loss: 396.6910, MinusLogProbMetric: 396.6910, val_loss: 400.9629, val_MinusLogProbMetric: 400.9629

Epoch 29: val_loss did not improve from 399.96866
196/196 - 18s - loss: 396.6910 - MinusLogProbMetric: 396.6910 - val_loss: 400.9629 - val_MinusLogProbMetric: 400.9629 - lr: 1.1111e-04 - 18s/epoch - 93ms/step
Epoch 30/1000
2023-09-09 23:07:09.040 
Epoch 30/1000 
	 loss: 396.8898, MinusLogProbMetric: 396.8898, val_loss: 402.0026, val_MinusLogProbMetric: 402.0026

Epoch 30: val_loss did not improve from 399.96866
196/196 - 17s - loss: 396.8898 - MinusLogProbMetric: 396.8898 - val_loss: 402.0026 - val_MinusLogProbMetric: 402.0026 - lr: 1.1111e-04 - 17s/epoch - 88ms/step
Epoch 31/1000
2023-09-09 23:07:25.436 
Epoch 31/1000 
	 loss: 396.6725, MinusLogProbMetric: 396.6725, val_loss: 404.0475, val_MinusLogProbMetric: 404.0475

Epoch 31: val_loss did not improve from 399.96866
196/196 - 16s - loss: 396.6725 - MinusLogProbMetric: 396.6725 - val_loss: 404.0475 - val_MinusLogProbMetric: 404.0475 - lr: 1.1111e-04 - 16s/epoch - 84ms/step
Epoch 32/1000
2023-09-09 23:07:42.260 
Epoch 32/1000 
	 loss: 396.5233, MinusLogProbMetric: 396.5233, val_loss: 399.9041, val_MinusLogProbMetric: 399.9041

Epoch 32: val_loss improved from 399.96866 to 399.90414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 17s - loss: 396.5233 - MinusLogProbMetric: 396.5233 - val_loss: 399.9041 - val_MinusLogProbMetric: 399.9041 - lr: 1.1111e-04 - 17s/epoch - 89ms/step
Epoch 33/1000
2023-09-09 23:07:59.614 
Epoch 33/1000 
	 loss: 397.5600, MinusLogProbMetric: 397.5600, val_loss: 399.0053, val_MinusLogProbMetric: 399.0053

Epoch 33: val_loss improved from 399.90414 to 399.00528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 17s - loss: 397.5600 - MinusLogProbMetric: 397.5600 - val_loss: 399.0053 - val_MinusLogProbMetric: 399.0053 - lr: 1.1111e-04 - 17s/epoch - 89ms/step
Epoch 34/1000
2023-09-09 23:08:16.826 
Epoch 34/1000 
	 loss: 396.3591, MinusLogProbMetric: 396.3591, val_loss: 400.5984, val_MinusLogProbMetric: 400.5984

Epoch 34: val_loss did not improve from 399.00528
196/196 - 17s - loss: 396.3591 - MinusLogProbMetric: 396.3591 - val_loss: 400.5984 - val_MinusLogProbMetric: 400.5984 - lr: 1.1111e-04 - 17s/epoch - 84ms/step
Epoch 35/1000
2023-09-09 23:08:33.064 
Epoch 35/1000 
	 loss: 396.7896, MinusLogProbMetric: 396.7896, val_loss: 400.9433, val_MinusLogProbMetric: 400.9433

Epoch 35: val_loss did not improve from 399.00528
196/196 - 16s - loss: 396.7896 - MinusLogProbMetric: 396.7896 - val_loss: 400.9433 - val_MinusLogProbMetric: 400.9433 - lr: 1.1111e-04 - 16s/epoch - 83ms/step
Epoch 36/1000
2023-09-09 23:08:49.728 
Epoch 36/1000 
	 loss: 396.1053, MinusLogProbMetric: 396.1053, val_loss: 403.8662, val_MinusLogProbMetric: 403.8662

Epoch 36: val_loss did not improve from 399.00528
196/196 - 17s - loss: 396.1053 - MinusLogProbMetric: 396.1053 - val_loss: 403.8662 - val_MinusLogProbMetric: 403.8662 - lr: 1.1111e-04 - 17s/epoch - 85ms/step
Epoch 37/1000
2023-09-09 23:09:05.675 
Epoch 37/1000 
	 loss: 396.5741, MinusLogProbMetric: 396.5741, val_loss: 399.7217, val_MinusLogProbMetric: 399.7217

Epoch 37: val_loss did not improve from 399.00528
196/196 - 16s - loss: 396.5741 - MinusLogProbMetric: 396.5741 - val_loss: 399.7217 - val_MinusLogProbMetric: 399.7217 - lr: 1.1111e-04 - 16s/epoch - 81ms/step
Epoch 38/1000
2023-09-09 23:09:21.455 
Epoch 38/1000 
	 loss: 396.3426, MinusLogProbMetric: 396.3426, val_loss: 401.2642, val_MinusLogProbMetric: 401.2642

Epoch 38: val_loss did not improve from 399.00528
196/196 - 16s - loss: 396.3426 - MinusLogProbMetric: 396.3426 - val_loss: 401.2642 - val_MinusLogProbMetric: 401.2642 - lr: 1.1111e-04 - 16s/epoch - 81ms/step
Epoch 39/1000
2023-09-09 23:09:37.742 
Epoch 39/1000 
	 loss: 395.8298, MinusLogProbMetric: 395.8298, val_loss: 399.1715, val_MinusLogProbMetric: 399.1715

Epoch 39: val_loss did not improve from 399.00528
196/196 - 16s - loss: 395.8298 - MinusLogProbMetric: 395.8298 - val_loss: 399.1715 - val_MinusLogProbMetric: 399.1715 - lr: 1.1111e-04 - 16s/epoch - 83ms/step
Epoch 40/1000
2023-09-09 23:09:53.604 
Epoch 40/1000 
	 loss: 395.9975, MinusLogProbMetric: 395.9975, val_loss: 403.5708, val_MinusLogProbMetric: 403.5708

Epoch 40: val_loss did not improve from 399.00528
196/196 - 16s - loss: 395.9975 - MinusLogProbMetric: 395.9975 - val_loss: 403.5708 - val_MinusLogProbMetric: 403.5708 - lr: 1.1111e-04 - 16s/epoch - 81ms/step
Epoch 41/1000
2023-09-09 23:10:10.260 
Epoch 41/1000 
	 loss: 396.0899, MinusLogProbMetric: 396.0899, val_loss: 399.7573, val_MinusLogProbMetric: 399.7573

Epoch 41: val_loss did not improve from 399.00528
196/196 - 17s - loss: 396.0899 - MinusLogProbMetric: 396.0899 - val_loss: 399.7573 - val_MinusLogProbMetric: 399.7573 - lr: 1.1111e-04 - 17s/epoch - 85ms/step
Epoch 42/1000
2023-09-09 23:10:26.953 
Epoch 42/1000 
	 loss: 396.0899, MinusLogProbMetric: 396.0899, val_loss: 400.0005, val_MinusLogProbMetric: 400.0005

Epoch 42: val_loss did not improve from 399.00528
196/196 - 17s - loss: 396.0899 - MinusLogProbMetric: 396.0899 - val_loss: 400.0005 - val_MinusLogProbMetric: 400.0005 - lr: 1.1111e-04 - 17s/epoch - 85ms/step
Epoch 43/1000
2023-09-09 23:10:41.984 
Epoch 43/1000 
	 loss: 395.7977, MinusLogProbMetric: 395.7977, val_loss: 400.0055, val_MinusLogProbMetric: 400.0055

Epoch 43: val_loss did not improve from 399.00528
196/196 - 15s - loss: 395.7977 - MinusLogProbMetric: 395.7977 - val_loss: 400.0055 - val_MinusLogProbMetric: 400.0055 - lr: 1.1111e-04 - 15s/epoch - 77ms/step
Epoch 44/1000
2023-09-09 23:10:58.212 
Epoch 44/1000 
	 loss: 395.5058, MinusLogProbMetric: 395.5058, val_loss: 399.4494, val_MinusLogProbMetric: 399.4494

Epoch 44: val_loss did not improve from 399.00528
196/196 - 16s - loss: 395.5058 - MinusLogProbMetric: 395.5058 - val_loss: 399.4494 - val_MinusLogProbMetric: 399.4494 - lr: 1.1111e-04 - 16s/epoch - 83ms/step
Epoch 45/1000
2023-09-09 23:11:14.556 
Epoch 45/1000 
	 loss: 395.6875, MinusLogProbMetric: 395.6875, val_loss: 405.1101, val_MinusLogProbMetric: 405.1101

Epoch 45: val_loss did not improve from 399.00528
196/196 - 16s - loss: 395.6875 - MinusLogProbMetric: 395.6875 - val_loss: 405.1101 - val_MinusLogProbMetric: 405.1101 - lr: 1.1111e-04 - 16s/epoch - 83ms/step
Epoch 46/1000
2023-09-09 23:11:31.034 
Epoch 46/1000 
	 loss: 395.7215, MinusLogProbMetric: 395.7215, val_loss: 400.3689, val_MinusLogProbMetric: 400.3689

Epoch 46: val_loss did not improve from 399.00528
196/196 - 16s - loss: 395.7215 - MinusLogProbMetric: 395.7215 - val_loss: 400.3689 - val_MinusLogProbMetric: 400.3689 - lr: 1.1111e-04 - 16s/epoch - 84ms/step
Epoch 47/1000
2023-09-09 23:11:48.381 
Epoch 47/1000 
	 loss: 395.3101, MinusLogProbMetric: 395.3101, val_loss: 399.3653, val_MinusLogProbMetric: 399.3653

Epoch 47: val_loss did not improve from 399.00528
196/196 - 17s - loss: 395.3101 - MinusLogProbMetric: 395.3101 - val_loss: 399.3653 - val_MinusLogProbMetric: 399.3653 - lr: 1.1111e-04 - 17s/epoch - 89ms/step
Epoch 48/1000
2023-09-09 23:12:02.004 
Epoch 48/1000 
	 loss: 395.1312, MinusLogProbMetric: 395.1312, val_loss: 399.2895, val_MinusLogProbMetric: 399.2895

Epoch 48: val_loss did not improve from 399.00528
196/196 - 14s - loss: 395.1312 - MinusLogProbMetric: 395.1312 - val_loss: 399.2895 - val_MinusLogProbMetric: 399.2895 - lr: 1.1111e-04 - 14s/epoch - 69ms/step
Epoch 49/1000
2023-09-09 23:12:22.917 
Epoch 49/1000 
	 loss: 395.3035, MinusLogProbMetric: 395.3035, val_loss: 400.3255, val_MinusLogProbMetric: 400.3255

Epoch 49: val_loss did not improve from 399.00528
196/196 - 21s - loss: 395.3035 - MinusLogProbMetric: 395.3035 - val_loss: 400.3255 - val_MinusLogProbMetric: 400.3255 - lr: 1.1111e-04 - 21s/epoch - 107ms/step
Epoch 50/1000
2023-09-09 23:12:47.442 
Epoch 50/1000 
	 loss: 395.2379, MinusLogProbMetric: 395.2379, val_loss: 400.6199, val_MinusLogProbMetric: 400.6199

Epoch 50: val_loss did not improve from 399.00528
196/196 - 25s - loss: 395.2379 - MinusLogProbMetric: 395.2379 - val_loss: 400.6199 - val_MinusLogProbMetric: 400.6199 - lr: 1.1111e-04 - 25s/epoch - 125ms/step
Epoch 51/1000
2023-09-09 23:13:06.451 
Epoch 51/1000 
	 loss: 394.9875, MinusLogProbMetric: 394.9875, val_loss: 400.5373, val_MinusLogProbMetric: 400.5373

Epoch 51: val_loss did not improve from 399.00528
196/196 - 19s - loss: 394.9875 - MinusLogProbMetric: 394.9875 - val_loss: 400.5373 - val_MinusLogProbMetric: 400.5373 - lr: 1.1111e-04 - 19s/epoch - 97ms/step
Epoch 52/1000
2023-09-09 23:13:28.458 
Epoch 52/1000 
	 loss: 395.2906, MinusLogProbMetric: 395.2906, val_loss: 399.9331, val_MinusLogProbMetric: 399.9331

Epoch 52: val_loss did not improve from 399.00528
196/196 - 22s - loss: 395.2906 - MinusLogProbMetric: 395.2906 - val_loss: 399.9331 - val_MinusLogProbMetric: 399.9331 - lr: 1.1111e-04 - 22s/epoch - 112ms/step
Epoch 53/1000
2023-09-09 23:13:51.643 
Epoch 53/1000 
	 loss: 394.9778, MinusLogProbMetric: 394.9778, val_loss: 398.9976, val_MinusLogProbMetric: 398.9976

Epoch 53: val_loss improved from 399.00528 to 398.99759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 24s - loss: 394.9778 - MinusLogProbMetric: 394.9778 - val_loss: 398.9976 - val_MinusLogProbMetric: 398.9976 - lr: 1.1111e-04 - 24s/epoch - 124ms/step
Epoch 54/1000
2023-09-09 23:14:17.246 
Epoch 54/1000 
	 loss: 395.0373, MinusLogProbMetric: 395.0373, val_loss: 399.6784, val_MinusLogProbMetric: 399.6784

Epoch 54: val_loss did not improve from 398.99759
196/196 - 24s - loss: 395.0373 - MinusLogProbMetric: 395.0373 - val_loss: 399.6784 - val_MinusLogProbMetric: 399.6784 - lr: 1.1111e-04 - 24s/epoch - 125ms/step
Epoch 55/1000
2023-09-09 23:14:43.530 
Epoch 55/1000 
	 loss: 394.8849, MinusLogProbMetric: 394.8849, val_loss: 399.2023, val_MinusLogProbMetric: 399.2023

Epoch 55: val_loss did not improve from 398.99759
196/196 - 26s - loss: 394.8849 - MinusLogProbMetric: 394.8849 - val_loss: 399.2023 - val_MinusLogProbMetric: 399.2023 - lr: 1.1111e-04 - 26s/epoch - 134ms/step
Epoch 56/1000
2023-09-09 23:15:10.351 
Epoch 56/1000 
	 loss: 394.9680, MinusLogProbMetric: 394.9680, val_loss: 398.6991, val_MinusLogProbMetric: 398.6991

Epoch 56: val_loss improved from 398.99759 to 398.69913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 28s - loss: 394.9680 - MinusLogProbMetric: 394.9680 - val_loss: 398.6991 - val_MinusLogProbMetric: 398.6991 - lr: 1.1111e-04 - 28s/epoch - 142ms/step
Epoch 57/1000
2023-09-09 23:15:38.985 
Epoch 57/1000 
	 loss: 394.7587, MinusLogProbMetric: 394.7587, val_loss: 399.2670, val_MinusLogProbMetric: 399.2670

Epoch 57: val_loss did not improve from 398.69913
196/196 - 28s - loss: 394.7587 - MinusLogProbMetric: 394.7587 - val_loss: 399.2670 - val_MinusLogProbMetric: 399.2670 - lr: 1.1111e-04 - 28s/epoch - 140ms/step
Epoch 58/1000
2023-09-09 23:16:05.928 
Epoch 58/1000 
	 loss: 395.9716, MinusLogProbMetric: 395.9716, val_loss: 400.7421, val_MinusLogProbMetric: 400.7421

Epoch 58: val_loss did not improve from 398.69913
196/196 - 27s - loss: 395.9716 - MinusLogProbMetric: 395.9716 - val_loss: 400.7421 - val_MinusLogProbMetric: 400.7421 - lr: 1.1111e-04 - 27s/epoch - 137ms/step
Epoch 59/1000
2023-09-09 23:16:33.118 
Epoch 59/1000 
	 loss: 394.4052, MinusLogProbMetric: 394.4052, val_loss: 400.1892, val_MinusLogProbMetric: 400.1892

Epoch 59: val_loss did not improve from 398.69913
196/196 - 27s - loss: 394.4052 - MinusLogProbMetric: 394.4052 - val_loss: 400.1892 - val_MinusLogProbMetric: 400.1892 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 60/1000
2023-09-09 23:16:59.578 
Epoch 60/1000 
	 loss: 394.9140, MinusLogProbMetric: 394.9140, val_loss: 404.6111, val_MinusLogProbMetric: 404.6111

Epoch 60: val_loss did not improve from 398.69913
196/196 - 26s - loss: 394.9140 - MinusLogProbMetric: 394.9140 - val_loss: 404.6111 - val_MinusLogProbMetric: 404.6111 - lr: 1.1111e-04 - 26s/epoch - 135ms/step
Epoch 61/1000
2023-09-09 23:17:25.308 
Epoch 61/1000 
	 loss: 394.9890, MinusLogProbMetric: 394.9890, val_loss: 398.9813, val_MinusLogProbMetric: 398.9813

Epoch 61: val_loss did not improve from 398.69913
196/196 - 26s - loss: 394.9890 - MinusLogProbMetric: 394.9890 - val_loss: 398.9813 - val_MinusLogProbMetric: 398.9813 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 62/1000
2023-09-09 23:17:49.769 
Epoch 62/1000 
	 loss: 394.2304, MinusLogProbMetric: 394.2304, val_loss: 400.5266, val_MinusLogProbMetric: 400.5266

Epoch 62: val_loss did not improve from 398.69913
196/196 - 24s - loss: 394.2304 - MinusLogProbMetric: 394.2304 - val_loss: 400.5266 - val_MinusLogProbMetric: 400.5266 - lr: 1.1111e-04 - 24s/epoch - 125ms/step
Epoch 63/1000
2023-09-09 23:18:15.121 
Epoch 63/1000 
	 loss: 394.3829, MinusLogProbMetric: 394.3829, val_loss: 402.6220, val_MinusLogProbMetric: 402.6220

Epoch 63: val_loss did not improve from 398.69913
196/196 - 25s - loss: 394.3829 - MinusLogProbMetric: 394.3829 - val_loss: 402.6220 - val_MinusLogProbMetric: 402.6220 - lr: 1.1111e-04 - 25s/epoch - 129ms/step
Epoch 64/1000
2023-09-09 23:18:42.698 
Epoch 64/1000 
	 loss: 394.5303, MinusLogProbMetric: 394.5303, val_loss: 399.1966, val_MinusLogProbMetric: 399.1966

Epoch 64: val_loss did not improve from 398.69913
196/196 - 28s - loss: 394.5303 - MinusLogProbMetric: 394.5303 - val_loss: 399.1966 - val_MinusLogProbMetric: 399.1966 - lr: 1.1111e-04 - 28s/epoch - 141ms/step
Epoch 65/1000
2023-09-09 23:19:08.287 
Epoch 65/1000 
	 loss: 394.4134, MinusLogProbMetric: 394.4134, val_loss: 401.4473, val_MinusLogProbMetric: 401.4473

Epoch 65: val_loss did not improve from 398.69913
196/196 - 26s - loss: 394.4134 - MinusLogProbMetric: 394.4134 - val_loss: 401.4473 - val_MinusLogProbMetric: 401.4473 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 66/1000
2023-09-09 23:19:32.713 
Epoch 66/1000 
	 loss: 394.5844, MinusLogProbMetric: 394.5844, val_loss: 398.5249, val_MinusLogProbMetric: 398.5249

Epoch 66: val_loss improved from 398.69913 to 398.52490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 26s - loss: 394.5844 - MinusLogProbMetric: 394.5844 - val_loss: 398.5249 - val_MinusLogProbMetric: 398.5249 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 67/1000
2023-09-09 23:19:59.155 
Epoch 67/1000 
	 loss: 394.0259, MinusLogProbMetric: 394.0259, val_loss: 398.8058, val_MinusLogProbMetric: 398.8058

Epoch 67: val_loss did not improve from 398.52490
196/196 - 25s - loss: 394.0259 - MinusLogProbMetric: 394.0259 - val_loss: 398.8058 - val_MinusLogProbMetric: 398.8058 - lr: 1.1111e-04 - 25s/epoch - 128ms/step
Epoch 68/1000
2023-09-09 23:20:24.480 
Epoch 68/1000 
	 loss: 395.3973, MinusLogProbMetric: 395.3973, val_loss: 398.6531, val_MinusLogProbMetric: 398.6531

Epoch 68: val_loss did not improve from 398.52490
196/196 - 25s - loss: 395.3973 - MinusLogProbMetric: 395.3973 - val_loss: 398.6531 - val_MinusLogProbMetric: 398.6531 - lr: 1.1111e-04 - 25s/epoch - 129ms/step
Epoch 69/1000
2023-09-09 23:20:51.100 
Epoch 69/1000 
	 loss: 395.2487, MinusLogProbMetric: 395.2487, val_loss: 399.3152, val_MinusLogProbMetric: 399.3152

Epoch 69: val_loss did not improve from 398.52490
196/196 - 27s - loss: 395.2487 - MinusLogProbMetric: 395.2487 - val_loss: 399.3152 - val_MinusLogProbMetric: 399.3152 - lr: 1.1111e-04 - 27s/epoch - 136ms/step
Epoch 70/1000
2023-09-09 23:21:16.630 
Epoch 70/1000 
	 loss: 393.8223, MinusLogProbMetric: 393.8223, val_loss: 398.9853, val_MinusLogProbMetric: 398.9853

Epoch 70: val_loss did not improve from 398.52490
196/196 - 26s - loss: 393.8223 - MinusLogProbMetric: 393.8223 - val_loss: 398.9853 - val_MinusLogProbMetric: 398.9853 - lr: 1.1111e-04 - 26s/epoch - 130ms/step
Epoch 71/1000
2023-09-09 23:21:41.034 
Epoch 71/1000 
	 loss: 394.4617, MinusLogProbMetric: 394.4617, val_loss: 398.3950, val_MinusLogProbMetric: 398.3950

Epoch 71: val_loss improved from 398.52490 to 398.39496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 25s - loss: 394.4617 - MinusLogProbMetric: 394.4617 - val_loss: 398.3950 - val_MinusLogProbMetric: 398.3950 - lr: 1.1111e-04 - 25s/epoch - 130ms/step
Epoch 72/1000
2023-09-09 23:22:08.088 
Epoch 72/1000 
	 loss: 393.8144, MinusLogProbMetric: 393.8144, val_loss: 404.7218, val_MinusLogProbMetric: 404.7218

Epoch 72: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.8144 - MinusLogProbMetric: 393.8144 - val_loss: 404.7218 - val_MinusLogProbMetric: 404.7218 - lr: 1.1111e-04 - 26s/epoch - 133ms/step
Epoch 73/1000
2023-09-09 23:22:35.012 
Epoch 73/1000 
	 loss: 393.9675, MinusLogProbMetric: 393.9675, val_loss: 399.5813, val_MinusLogProbMetric: 399.5813

Epoch 73: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.9675 - MinusLogProbMetric: 393.9675 - val_loss: 399.5813 - val_MinusLogProbMetric: 399.5813 - lr: 1.1111e-04 - 27s/epoch - 137ms/step
Epoch 74/1000
2023-09-09 23:23:01.537 
Epoch 74/1000 
	 loss: 394.1449, MinusLogProbMetric: 394.1449, val_loss: 399.7229, val_MinusLogProbMetric: 399.7229

Epoch 74: val_loss did not improve from 398.39496
196/196 - 27s - loss: 394.1449 - MinusLogProbMetric: 394.1449 - val_loss: 399.7229 - val_MinusLogProbMetric: 399.7229 - lr: 1.1111e-04 - 27s/epoch - 135ms/step
Epoch 75/1000
2023-09-09 23:23:28.562 
Epoch 75/1000 
	 loss: 394.0183, MinusLogProbMetric: 394.0183, val_loss: 399.8365, val_MinusLogProbMetric: 399.8365

Epoch 75: val_loss did not improve from 398.39496
196/196 - 27s - loss: 394.0183 - MinusLogProbMetric: 394.0183 - val_loss: 399.8365 - val_MinusLogProbMetric: 399.8365 - lr: 1.1111e-04 - 27s/epoch - 138ms/step
Epoch 76/1000
2023-09-09 23:23:55.209 
Epoch 76/1000 
	 loss: 393.5553, MinusLogProbMetric: 393.5553, val_loss: 398.6804, val_MinusLogProbMetric: 398.6804

Epoch 76: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.5553 - MinusLogProbMetric: 393.5553 - val_loss: 398.6804 - val_MinusLogProbMetric: 398.6804 - lr: 1.1111e-04 - 27s/epoch - 136ms/step
Epoch 77/1000
2023-09-09 23:24:20.801 
Epoch 77/1000 
	 loss: 393.7820, MinusLogProbMetric: 393.7820, val_loss: 402.4861, val_MinusLogProbMetric: 402.4861

Epoch 77: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.7820 - MinusLogProbMetric: 393.7820 - val_loss: 402.4861 - val_MinusLogProbMetric: 402.4861 - lr: 1.1111e-04 - 26s/epoch - 130ms/step
Epoch 78/1000
2023-09-09 23:24:46.680 
Epoch 78/1000 
	 loss: 393.7204, MinusLogProbMetric: 393.7204, val_loss: 398.7028, val_MinusLogProbMetric: 398.7028

Epoch 78: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.7204 - MinusLogProbMetric: 393.7204 - val_loss: 398.7028 - val_MinusLogProbMetric: 398.7028 - lr: 1.1111e-04 - 26s/epoch - 132ms/step
Epoch 79/1000
2023-09-09 23:25:11.484 
Epoch 79/1000 
	 loss: 394.0575, MinusLogProbMetric: 394.0575, val_loss: 398.5421, val_MinusLogProbMetric: 398.5421

Epoch 79: val_loss did not improve from 398.39496
196/196 - 25s - loss: 394.0575 - MinusLogProbMetric: 394.0575 - val_loss: 398.5421 - val_MinusLogProbMetric: 398.5421 - lr: 1.1111e-04 - 25s/epoch - 127ms/step
Epoch 80/1000
2023-09-09 23:25:38.828 
Epoch 80/1000 
	 loss: 393.5322, MinusLogProbMetric: 393.5322, val_loss: 400.2089, val_MinusLogProbMetric: 400.2089

Epoch 80: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.5322 - MinusLogProbMetric: 393.5322 - val_loss: 400.2089 - val_MinusLogProbMetric: 400.2089 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 81/1000
2023-09-09 23:26:06.464 
Epoch 81/1000 
	 loss: 394.5237, MinusLogProbMetric: 394.5237, val_loss: 399.2333, val_MinusLogProbMetric: 399.2333

Epoch 81: val_loss did not improve from 398.39496
196/196 - 28s - loss: 394.5237 - MinusLogProbMetric: 394.5237 - val_loss: 399.2333 - val_MinusLogProbMetric: 399.2333 - lr: 1.1111e-04 - 28s/epoch - 141ms/step
Epoch 82/1000
2023-09-09 23:26:32.098 
Epoch 82/1000 
	 loss: 393.7663, MinusLogProbMetric: 393.7663, val_loss: 398.9446, val_MinusLogProbMetric: 398.9446

Epoch 82: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.7663 - MinusLogProbMetric: 393.7663 - val_loss: 398.9446 - val_MinusLogProbMetric: 398.9446 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 83/1000
2023-09-09 23:26:59.126 
Epoch 83/1000 
	 loss: 393.3100, MinusLogProbMetric: 393.3100, val_loss: 399.7199, val_MinusLogProbMetric: 399.7199

Epoch 83: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.3100 - MinusLogProbMetric: 393.3100 - val_loss: 399.7199 - val_MinusLogProbMetric: 399.7199 - lr: 1.1111e-04 - 27s/epoch - 138ms/step
Epoch 84/1000
2023-09-09 23:27:25.513 
Epoch 84/1000 
	 loss: 393.5390, MinusLogProbMetric: 393.5390, val_loss: 400.4110, val_MinusLogProbMetric: 400.4110

Epoch 84: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.5390 - MinusLogProbMetric: 393.5390 - val_loss: 400.4110 - val_MinusLogProbMetric: 400.4110 - lr: 1.1111e-04 - 26s/epoch - 135ms/step
Epoch 85/1000
2023-09-09 23:27:52.284 
Epoch 85/1000 
	 loss: 393.5521, MinusLogProbMetric: 393.5521, val_loss: 403.1357, val_MinusLogProbMetric: 403.1357

Epoch 85: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.5521 - MinusLogProbMetric: 393.5521 - val_loss: 403.1357 - val_MinusLogProbMetric: 403.1357 - lr: 1.1111e-04 - 27s/epoch - 137ms/step
Epoch 86/1000
2023-09-09 23:28:21.377 
Epoch 86/1000 
	 loss: 393.1967, MinusLogProbMetric: 393.1967, val_loss: 400.4613, val_MinusLogProbMetric: 400.4613

Epoch 86: val_loss did not improve from 398.39496
196/196 - 29s - loss: 393.1967 - MinusLogProbMetric: 393.1967 - val_loss: 400.4613 - val_MinusLogProbMetric: 400.4613 - lr: 1.1111e-04 - 29s/epoch - 148ms/step
Epoch 87/1000
2023-09-09 23:28:48.606 
Epoch 87/1000 
	 loss: 393.8078, MinusLogProbMetric: 393.8078, val_loss: 399.4987, val_MinusLogProbMetric: 399.4987

Epoch 87: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.8078 - MinusLogProbMetric: 393.8078 - val_loss: 399.4987 - val_MinusLogProbMetric: 399.4987 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 88/1000
2023-09-09 23:29:14.864 
Epoch 88/1000 
	 loss: 392.9282, MinusLogProbMetric: 392.9282, val_loss: 402.7753, val_MinusLogProbMetric: 402.7753

Epoch 88: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.9282 - MinusLogProbMetric: 392.9282 - val_loss: 402.7753 - val_MinusLogProbMetric: 402.7753 - lr: 1.1111e-04 - 26s/epoch - 134ms/step
Epoch 89/1000
2023-09-09 23:29:42.648 
Epoch 89/1000 
	 loss: 393.9675, MinusLogProbMetric: 393.9675, val_loss: 398.5183, val_MinusLogProbMetric: 398.5183

Epoch 89: val_loss did not improve from 398.39496
196/196 - 28s - loss: 393.9675 - MinusLogProbMetric: 393.9675 - val_loss: 398.5183 - val_MinusLogProbMetric: 398.5183 - lr: 1.1111e-04 - 28s/epoch - 142ms/step
Epoch 90/1000
2023-09-09 23:30:06.478 
Epoch 90/1000 
	 loss: 393.0882, MinusLogProbMetric: 393.0882, val_loss: 405.2214, val_MinusLogProbMetric: 405.2214

Epoch 90: val_loss did not improve from 398.39496
196/196 - 24s - loss: 393.0882 - MinusLogProbMetric: 393.0882 - val_loss: 405.2214 - val_MinusLogProbMetric: 405.2214 - lr: 1.1111e-04 - 24s/epoch - 122ms/step
Epoch 91/1000
2023-09-09 23:30:32.234 
Epoch 91/1000 
	 loss: 393.1569, MinusLogProbMetric: 393.1569, val_loss: 399.8740, val_MinusLogProbMetric: 399.8740

Epoch 91: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.1569 - MinusLogProbMetric: 393.1569 - val_loss: 399.8740 - val_MinusLogProbMetric: 399.8740 - lr: 1.1111e-04 - 26s/epoch - 132ms/step
Epoch 92/1000
2023-09-09 23:30:55.428 
Epoch 92/1000 
	 loss: 393.5612, MinusLogProbMetric: 393.5612, val_loss: 400.5995, val_MinusLogProbMetric: 400.5995

Epoch 92: val_loss did not improve from 398.39496
196/196 - 23s - loss: 393.5612 - MinusLogProbMetric: 393.5612 - val_loss: 400.5995 - val_MinusLogProbMetric: 400.5995 - lr: 1.1111e-04 - 23s/epoch - 118ms/step
Epoch 93/1000
2023-09-09 23:31:20.869 
Epoch 93/1000 
	 loss: 393.5196, MinusLogProbMetric: 393.5196, val_loss: 400.2396, val_MinusLogProbMetric: 400.2396

Epoch 93: val_loss did not improve from 398.39496
196/196 - 25s - loss: 393.5196 - MinusLogProbMetric: 393.5196 - val_loss: 400.2396 - val_MinusLogProbMetric: 400.2396 - lr: 1.1111e-04 - 25s/epoch - 130ms/step
Epoch 94/1000
2023-09-09 23:31:45.816 
Epoch 94/1000 
	 loss: 392.8476, MinusLogProbMetric: 392.8476, val_loss: 400.7432, val_MinusLogProbMetric: 400.7432

Epoch 94: val_loss did not improve from 398.39496
196/196 - 25s - loss: 392.8476 - MinusLogProbMetric: 392.8476 - val_loss: 400.7432 - val_MinusLogProbMetric: 400.7432 - lr: 1.1111e-04 - 25s/epoch - 127ms/step
Epoch 95/1000
2023-09-09 23:32:12.965 
Epoch 95/1000 
	 loss: 393.1230, MinusLogProbMetric: 393.1230, val_loss: 399.4358, val_MinusLogProbMetric: 399.4358

Epoch 95: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.1230 - MinusLogProbMetric: 393.1230 - val_loss: 399.4358 - val_MinusLogProbMetric: 399.4358 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 96/1000
2023-09-09 23:32:39.309 
Epoch 96/1000 
	 loss: 392.6920, MinusLogProbMetric: 392.6920, val_loss: 400.9615, val_MinusLogProbMetric: 400.9615

Epoch 96: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.6920 - MinusLogProbMetric: 392.6920 - val_loss: 400.9615 - val_MinusLogProbMetric: 400.9615 - lr: 1.1111e-04 - 26s/epoch - 134ms/step
Epoch 97/1000
2023-09-09 23:33:04.848 
Epoch 97/1000 
	 loss: 392.8544, MinusLogProbMetric: 392.8544, val_loss: 398.9659, val_MinusLogProbMetric: 398.9659

Epoch 97: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.8544 - MinusLogProbMetric: 392.8544 - val_loss: 398.9659 - val_MinusLogProbMetric: 398.9659 - lr: 1.1111e-04 - 26s/epoch - 130ms/step
Epoch 98/1000
2023-09-09 23:33:30.961 
Epoch 98/1000 
	 loss: 393.2835, MinusLogProbMetric: 393.2835, val_loss: 399.5458, val_MinusLogProbMetric: 399.5458

Epoch 98: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.2835 - MinusLogProbMetric: 393.2835 - val_loss: 399.5458 - val_MinusLogProbMetric: 399.5458 - lr: 1.1111e-04 - 26s/epoch - 133ms/step
Epoch 99/1000
2023-09-09 23:33:55.532 
Epoch 99/1000 
	 loss: 394.0397, MinusLogProbMetric: 394.0397, val_loss: 398.8422, val_MinusLogProbMetric: 398.8422

Epoch 99: val_loss did not improve from 398.39496
196/196 - 25s - loss: 394.0397 - MinusLogProbMetric: 394.0397 - val_loss: 398.8422 - val_MinusLogProbMetric: 398.8422 - lr: 1.1111e-04 - 25s/epoch - 125ms/step
Epoch 100/1000
2023-09-09 23:34:22.155 
Epoch 100/1000 
	 loss: 392.8756, MinusLogProbMetric: 392.8756, val_loss: 399.5864, val_MinusLogProbMetric: 399.5864

Epoch 100: val_loss did not improve from 398.39496
196/196 - 27s - loss: 392.8756 - MinusLogProbMetric: 392.8756 - val_loss: 399.5864 - val_MinusLogProbMetric: 399.5864 - lr: 1.1111e-04 - 27s/epoch - 136ms/step
Epoch 101/1000
2023-09-09 23:34:47.882 
Epoch 101/1000 
	 loss: 393.1107, MinusLogProbMetric: 393.1107, val_loss: 399.6030, val_MinusLogProbMetric: 399.6030

Epoch 101: val_loss did not improve from 398.39496
196/196 - 26s - loss: 393.1107 - MinusLogProbMetric: 393.1107 - val_loss: 399.6030 - val_MinusLogProbMetric: 399.6030 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 102/1000
2023-09-09 23:35:13.106 
Epoch 102/1000 
	 loss: 392.8674, MinusLogProbMetric: 392.8674, val_loss: 400.0224, val_MinusLogProbMetric: 400.0224

Epoch 102: val_loss did not improve from 398.39496
196/196 - 25s - loss: 392.8674 - MinusLogProbMetric: 392.8674 - val_loss: 400.0224 - val_MinusLogProbMetric: 400.0224 - lr: 1.1111e-04 - 25s/epoch - 129ms/step
Epoch 103/1000
2023-09-09 23:35:40.020 
Epoch 103/1000 
	 loss: 392.6271, MinusLogProbMetric: 392.6271, val_loss: 403.2101, val_MinusLogProbMetric: 403.2101

Epoch 103: val_loss did not improve from 398.39496
196/196 - 27s - loss: 392.6271 - MinusLogProbMetric: 392.6271 - val_loss: 403.2101 - val_MinusLogProbMetric: 403.2101 - lr: 1.1111e-04 - 27s/epoch - 137ms/step
Epoch 104/1000
2023-09-09 23:36:05.630 
Epoch 104/1000 
	 loss: 392.9386, MinusLogProbMetric: 392.9386, val_loss: 399.8170, val_MinusLogProbMetric: 399.8170

Epoch 104: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.9386 - MinusLogProbMetric: 392.9386 - val_loss: 399.8170 - val_MinusLogProbMetric: 399.8170 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 105/1000
2023-09-09 23:36:32.090 
Epoch 105/1000 
	 loss: 392.3712, MinusLogProbMetric: 392.3712, val_loss: 398.7849, val_MinusLogProbMetric: 398.7849

Epoch 105: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.3712 - MinusLogProbMetric: 392.3712 - val_loss: 398.7849 - val_MinusLogProbMetric: 398.7849 - lr: 1.1111e-04 - 26s/epoch - 135ms/step
Epoch 106/1000
2023-09-09 23:36:59.006 
Epoch 106/1000 
	 loss: 392.5018, MinusLogProbMetric: 392.5018, val_loss: 399.3225, val_MinusLogProbMetric: 399.3225

Epoch 106: val_loss did not improve from 398.39496
196/196 - 27s - loss: 392.5018 - MinusLogProbMetric: 392.5018 - val_loss: 399.3225 - val_MinusLogProbMetric: 399.3225 - lr: 1.1111e-04 - 27s/epoch - 137ms/step
Epoch 107/1000
2023-09-09 23:37:24.644 
Epoch 107/1000 
	 loss: 392.4405, MinusLogProbMetric: 392.4405, val_loss: 399.6745, val_MinusLogProbMetric: 399.6745

Epoch 107: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.4405 - MinusLogProbMetric: 392.4405 - val_loss: 399.6745 - val_MinusLogProbMetric: 399.6745 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 108/1000
2023-09-09 23:37:49.858 
Epoch 108/1000 
	 loss: 392.2678, MinusLogProbMetric: 392.2678, val_loss: 400.4684, val_MinusLogProbMetric: 400.4684

Epoch 108: val_loss did not improve from 398.39496
196/196 - 25s - loss: 392.2678 - MinusLogProbMetric: 392.2678 - val_loss: 400.4684 - val_MinusLogProbMetric: 400.4684 - lr: 1.1111e-04 - 25s/epoch - 129ms/step
Epoch 109/1000
2023-09-09 23:38:15.483 
Epoch 109/1000 
	 loss: 392.7143, MinusLogProbMetric: 392.7143, val_loss: 401.2724, val_MinusLogProbMetric: 401.2724

Epoch 109: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.7143 - MinusLogProbMetric: 392.7143 - val_loss: 401.2724 - val_MinusLogProbMetric: 401.2724 - lr: 1.1111e-04 - 26s/epoch - 131ms/step
Epoch 110/1000
2023-09-09 23:38:42.792 
Epoch 110/1000 
	 loss: 393.0694, MinusLogProbMetric: 393.0694, val_loss: 399.5963, val_MinusLogProbMetric: 399.5963

Epoch 110: val_loss did not improve from 398.39496
196/196 - 27s - loss: 393.0694 - MinusLogProbMetric: 393.0694 - val_loss: 399.5963 - val_MinusLogProbMetric: 399.5963 - lr: 1.1111e-04 - 27s/epoch - 139ms/step
Epoch 111/1000
2023-09-09 23:39:09.245 
Epoch 111/1000 
	 loss: 392.2356, MinusLogProbMetric: 392.2356, val_loss: 400.6944, val_MinusLogProbMetric: 400.6944

Epoch 111: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.2356 - MinusLogProbMetric: 392.2356 - val_loss: 400.6944 - val_MinusLogProbMetric: 400.6944 - lr: 1.1111e-04 - 26s/epoch - 135ms/step
Epoch 112/1000
2023-09-09 23:39:33.869 
Epoch 112/1000 
	 loss: 392.6842, MinusLogProbMetric: 392.6842, val_loss: 401.0291, val_MinusLogProbMetric: 401.0291

Epoch 112: val_loss did not improve from 398.39496
196/196 - 25s - loss: 392.6842 - MinusLogProbMetric: 392.6842 - val_loss: 401.0291 - val_MinusLogProbMetric: 401.0291 - lr: 1.1111e-04 - 25s/epoch - 126ms/step
Epoch 113/1000
2023-09-09 23:39:59.815 
Epoch 113/1000 
	 loss: 392.0194, MinusLogProbMetric: 392.0194, val_loss: 401.1302, val_MinusLogProbMetric: 401.1302

Epoch 113: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.0194 - MinusLogProbMetric: 392.0194 - val_loss: 401.1302 - val_MinusLogProbMetric: 401.1302 - lr: 1.1111e-04 - 26s/epoch - 132ms/step
Epoch 114/1000
2023-09-09 23:40:27.731 
Epoch 114/1000 
	 loss: 392.4843, MinusLogProbMetric: 392.4843, val_loss: 399.5052, val_MinusLogProbMetric: 399.5052

Epoch 114: val_loss did not improve from 398.39496
196/196 - 28s - loss: 392.4843 - MinusLogProbMetric: 392.4843 - val_loss: 399.5052 - val_MinusLogProbMetric: 399.5052 - lr: 1.1111e-04 - 28s/epoch - 142ms/step
Epoch 115/1000
2023-09-09 23:40:54.029 
Epoch 115/1000 
	 loss: 392.0580, MinusLogProbMetric: 392.0580, val_loss: 402.0179, val_MinusLogProbMetric: 402.0179

Epoch 115: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.0580 - MinusLogProbMetric: 392.0580 - val_loss: 402.0179 - val_MinusLogProbMetric: 402.0179 - lr: 1.1111e-04 - 26s/epoch - 134ms/step
Epoch 116/1000
2023-09-09 23:41:22.602 
Epoch 116/1000 
	 loss: 391.9966, MinusLogProbMetric: 391.9966, val_loss: 399.8133, val_MinusLogProbMetric: 399.8133

Epoch 116: val_loss did not improve from 398.39496
196/196 - 29s - loss: 391.9966 - MinusLogProbMetric: 391.9966 - val_loss: 399.8133 - val_MinusLogProbMetric: 399.8133 - lr: 1.1111e-04 - 29s/epoch - 146ms/step
Epoch 117/1000
2023-09-09 23:41:45.965 
Epoch 117/1000 
	 loss: 392.3911, MinusLogProbMetric: 392.3911, val_loss: 404.3017, val_MinusLogProbMetric: 404.3017

Epoch 117: val_loss did not improve from 398.39496
196/196 - 23s - loss: 392.3911 - MinusLogProbMetric: 392.3911 - val_loss: 404.3017 - val_MinusLogProbMetric: 404.3017 - lr: 1.1111e-04 - 23s/epoch - 119ms/step
Epoch 118/1000
2023-09-09 23:42:13.604 
Epoch 118/1000 
	 loss: 392.3676, MinusLogProbMetric: 392.3676, val_loss: 404.5488, val_MinusLogProbMetric: 404.5488

Epoch 118: val_loss did not improve from 398.39496
196/196 - 28s - loss: 392.3676 - MinusLogProbMetric: 392.3676 - val_loss: 404.5488 - val_MinusLogProbMetric: 404.5488 - lr: 1.1111e-04 - 28s/epoch - 141ms/step
Epoch 119/1000
2023-09-09 23:42:39.655 
Epoch 119/1000 
	 loss: 392.1103, MinusLogProbMetric: 392.1103, val_loss: 399.0270, val_MinusLogProbMetric: 399.0270

Epoch 119: val_loss did not improve from 398.39496
196/196 - 26s - loss: 392.1103 - MinusLogProbMetric: 392.1103 - val_loss: 399.0270 - val_MinusLogProbMetric: 399.0270 - lr: 1.1111e-04 - 26s/epoch - 133ms/step
Epoch 120/1000
2023-09-09 23:43:03.956 
Epoch 120/1000 
	 loss: 392.2262, MinusLogProbMetric: 392.2262, val_loss: 399.8975, val_MinusLogProbMetric: 399.8975

Epoch 120: val_loss did not improve from 398.39496
196/196 - 24s - loss: 392.2262 - MinusLogProbMetric: 392.2262 - val_loss: 399.8975 - val_MinusLogProbMetric: 399.8975 - lr: 1.1111e-04 - 24s/epoch - 124ms/step
Epoch 121/1000
2023-09-09 23:43:29.078 
Epoch 121/1000 
	 loss: 392.1828, MinusLogProbMetric: 392.1828, val_loss: 400.6023, val_MinusLogProbMetric: 400.6023

Epoch 121: val_loss did not improve from 398.39496
196/196 - 25s - loss: 392.1828 - MinusLogProbMetric: 392.1828 - val_loss: 400.6023 - val_MinusLogProbMetric: 400.6023 - lr: 1.1111e-04 - 25s/epoch - 128ms/step
Epoch 122/1000
2023-09-09 23:43:54.369 
Epoch 122/1000 
	 loss: 388.5981, MinusLogProbMetric: 388.5981, val_loss: 397.5316, val_MinusLogProbMetric: 397.5316

Epoch 122: val_loss improved from 398.39496 to 397.53165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 26s - loss: 388.5981 - MinusLogProbMetric: 388.5981 - val_loss: 397.5316 - val_MinusLogProbMetric: 397.5316 - lr: 5.5556e-05 - 26s/epoch - 133ms/step
Epoch 123/1000
2023-09-09 23:44:20.429 
Epoch 123/1000 
	 loss: 388.5284, MinusLogProbMetric: 388.5284, val_loss: 397.8428, val_MinusLogProbMetric: 397.8428

Epoch 123: val_loss did not improve from 397.53165
196/196 - 25s - loss: 388.5284 - MinusLogProbMetric: 388.5284 - val_loss: 397.8428 - val_MinusLogProbMetric: 397.8428 - lr: 5.5556e-05 - 25s/epoch - 128ms/step
Epoch 124/1000
2023-09-09 23:44:45.049 
Epoch 124/1000 
	 loss: 388.5650, MinusLogProbMetric: 388.5650, val_loss: 397.7771, val_MinusLogProbMetric: 397.7771

Epoch 124: val_loss did not improve from 397.53165
196/196 - 25s - loss: 388.5650 - MinusLogProbMetric: 388.5650 - val_loss: 397.7771 - val_MinusLogProbMetric: 397.7771 - lr: 5.5556e-05 - 25s/epoch - 126ms/step
Epoch 125/1000
2023-09-09 23:45:10.029 
Epoch 125/1000 
	 loss: 388.6429, MinusLogProbMetric: 388.6429, val_loss: 397.2363, val_MinusLogProbMetric: 397.2363

Epoch 125: val_loss improved from 397.53165 to 397.23633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 26s - loss: 388.6429 - MinusLogProbMetric: 388.6429 - val_loss: 397.2363 - val_MinusLogProbMetric: 397.2363 - lr: 5.5556e-05 - 26s/epoch - 134ms/step
Epoch 126/1000
2023-09-09 23:45:37.929 
Epoch 126/1000 
	 loss: 388.5989, MinusLogProbMetric: 388.5989, val_loss: 408.7972, val_MinusLogProbMetric: 408.7972

Epoch 126: val_loss did not improve from 397.23633
196/196 - 27s - loss: 388.5989 - MinusLogProbMetric: 388.5989 - val_loss: 408.7972 - val_MinusLogProbMetric: 408.7972 - lr: 5.5556e-05 - 27s/epoch - 136ms/step
Epoch 127/1000
2023-09-09 23:46:03.651 
Epoch 127/1000 
	 loss: 389.2700, MinusLogProbMetric: 389.2700, val_loss: 397.6682, val_MinusLogProbMetric: 397.6682

Epoch 127: val_loss did not improve from 397.23633
196/196 - 26s - loss: 389.2700 - MinusLogProbMetric: 389.2700 - val_loss: 397.6682 - val_MinusLogProbMetric: 397.6682 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 128/1000
2023-09-09 23:46:27.793 
Epoch 128/1000 
	 loss: 388.5307, MinusLogProbMetric: 388.5307, val_loss: 399.5534, val_MinusLogProbMetric: 399.5534

Epoch 128: val_loss did not improve from 397.23633
196/196 - 24s - loss: 388.5307 - MinusLogProbMetric: 388.5307 - val_loss: 399.5534 - val_MinusLogProbMetric: 399.5534 - lr: 5.5556e-05 - 24s/epoch - 123ms/step
Epoch 129/1000
2023-09-09 23:46:52.140 
Epoch 129/1000 
	 loss: 389.0848, MinusLogProbMetric: 389.0848, val_loss: 397.4887, val_MinusLogProbMetric: 397.4887

Epoch 129: val_loss did not improve from 397.23633
196/196 - 24s - loss: 389.0848 - MinusLogProbMetric: 389.0848 - val_loss: 397.4887 - val_MinusLogProbMetric: 397.4887 - lr: 5.5556e-05 - 24s/epoch - 124ms/step
Epoch 130/1000
2023-09-09 23:47:16.905 
Epoch 130/1000 
	 loss: 388.5178, MinusLogProbMetric: 388.5178, val_loss: 397.1266, val_MinusLogProbMetric: 397.1266

Epoch 130: val_loss improved from 397.23633 to 397.12659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 26s - loss: 388.5178 - MinusLogProbMetric: 388.5178 - val_loss: 397.1266 - val_MinusLogProbMetric: 397.1266 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 131/1000
2023-09-09 23:47:43.568 
Epoch 131/1000 
	 loss: 388.3742, MinusLogProbMetric: 388.3742, val_loss: 397.3856, val_MinusLogProbMetric: 397.3856

Epoch 131: val_loss did not improve from 397.12659
196/196 - 26s - loss: 388.3742 - MinusLogProbMetric: 388.3742 - val_loss: 397.3856 - val_MinusLogProbMetric: 397.3856 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 132/1000
2023-09-09 23:48:07.979 
Epoch 132/1000 
	 loss: 388.5826, MinusLogProbMetric: 388.5826, val_loss: 397.7495, val_MinusLogProbMetric: 397.7495

Epoch 132: val_loss did not improve from 397.12659
196/196 - 24s - loss: 388.5826 - MinusLogProbMetric: 388.5826 - val_loss: 397.7495 - val_MinusLogProbMetric: 397.7495 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 133/1000
2023-09-09 23:48:31.935 
Epoch 133/1000 
	 loss: 388.4998, MinusLogProbMetric: 388.4998, val_loss: 397.9098, val_MinusLogProbMetric: 397.9098

Epoch 133: val_loss did not improve from 397.12659
196/196 - 24s - loss: 388.4998 - MinusLogProbMetric: 388.4998 - val_loss: 397.9098 - val_MinusLogProbMetric: 397.9098 - lr: 5.5556e-05 - 24s/epoch - 122ms/step
Epoch 134/1000
2023-09-09 23:48:58.692 
Epoch 134/1000 
	 loss: 388.6120, MinusLogProbMetric: 388.6120, val_loss: 397.1763, val_MinusLogProbMetric: 397.1763

Epoch 134: val_loss did not improve from 397.12659
196/196 - 27s - loss: 388.6120 - MinusLogProbMetric: 388.6120 - val_loss: 397.1763 - val_MinusLogProbMetric: 397.1763 - lr: 5.5556e-05 - 27s/epoch - 136ms/step
Epoch 135/1000
2023-09-09 23:49:25.592 
Epoch 135/1000 
	 loss: 388.6160, MinusLogProbMetric: 388.6160, val_loss: 397.2347, val_MinusLogProbMetric: 397.2347

Epoch 135: val_loss did not improve from 397.12659
196/196 - 27s - loss: 388.6160 - MinusLogProbMetric: 388.6160 - val_loss: 397.2347 - val_MinusLogProbMetric: 397.2347 - lr: 5.5556e-05 - 27s/epoch - 137ms/step
Epoch 136/1000
2023-09-09 23:49:49.568 
Epoch 136/1000 
	 loss: 388.7357, MinusLogProbMetric: 388.7357, val_loss: 397.4220, val_MinusLogProbMetric: 397.4220

Epoch 136: val_loss did not improve from 397.12659
196/196 - 24s - loss: 388.7357 - MinusLogProbMetric: 388.7357 - val_loss: 397.4220 - val_MinusLogProbMetric: 397.4220 - lr: 5.5556e-05 - 24s/epoch - 122ms/step
Epoch 137/1000
2023-09-09 23:50:12.364 
Epoch 137/1000 
	 loss: 388.4033, MinusLogProbMetric: 388.4033, val_loss: 397.3116, val_MinusLogProbMetric: 397.3116

Epoch 137: val_loss did not improve from 397.12659
196/196 - 23s - loss: 388.4033 - MinusLogProbMetric: 388.4033 - val_loss: 397.3116 - val_MinusLogProbMetric: 397.3116 - lr: 5.5556e-05 - 23s/epoch - 116ms/step
Epoch 138/1000
2023-09-09 23:50:37.245 
Epoch 138/1000 
	 loss: 388.7876, MinusLogProbMetric: 388.7876, val_loss: 398.1956, val_MinusLogProbMetric: 398.1956

Epoch 138: val_loss did not improve from 397.12659
196/196 - 25s - loss: 388.7876 - MinusLogProbMetric: 388.7876 - val_loss: 398.1956 - val_MinusLogProbMetric: 398.1956 - lr: 5.5556e-05 - 25s/epoch - 127ms/step
Epoch 139/1000
2023-09-09 23:51:02.098 
Epoch 139/1000 
	 loss: 388.6835, MinusLogProbMetric: 388.6835, val_loss: 397.4652, val_MinusLogProbMetric: 397.4652

Epoch 139: val_loss did not improve from 397.12659
196/196 - 25s - loss: 388.6835 - MinusLogProbMetric: 388.6835 - val_loss: 397.4652 - val_MinusLogProbMetric: 397.4652 - lr: 5.5556e-05 - 25s/epoch - 127ms/step
Epoch 140/1000
2023-09-09 23:51:27.083 
Epoch 140/1000 
	 loss: 388.5825, MinusLogProbMetric: 388.5825, val_loss: 399.8111, val_MinusLogProbMetric: 399.8111

Epoch 140: val_loss did not improve from 397.12659
196/196 - 25s - loss: 388.5825 - MinusLogProbMetric: 388.5825 - val_loss: 399.8111 - val_MinusLogProbMetric: 399.8111 - lr: 5.5556e-05 - 25s/epoch - 128ms/step
Epoch 141/1000
2023-09-09 23:51:52.289 
Epoch 141/1000 
	 loss: 388.6772, MinusLogProbMetric: 388.6772, val_loss: 397.4298, val_MinusLogProbMetric: 397.4298

Epoch 141: val_loss did not improve from 397.12659
196/196 - 25s - loss: 388.6772 - MinusLogProbMetric: 388.6772 - val_loss: 397.4298 - val_MinusLogProbMetric: 397.4298 - lr: 5.5556e-05 - 25s/epoch - 129ms/step
Epoch 142/1000
2023-09-09 23:52:16.140 
Epoch 142/1000 
	 loss: 388.3190, MinusLogProbMetric: 388.3190, val_loss: 397.9941, val_MinusLogProbMetric: 397.9941

Epoch 142: val_loss did not improve from 397.12659
196/196 - 24s - loss: 388.3190 - MinusLogProbMetric: 388.3190 - val_loss: 397.9941 - val_MinusLogProbMetric: 397.9941 - lr: 5.5556e-05 - 24s/epoch - 122ms/step
Epoch 143/1000
2023-09-09 23:52:39.433 
Epoch 143/1000 
	 loss: 388.3034, MinusLogProbMetric: 388.3034, val_loss: 398.8025, val_MinusLogProbMetric: 398.8025

Epoch 143: val_loss did not improve from 397.12659
196/196 - 23s - loss: 388.3034 - MinusLogProbMetric: 388.3034 - val_loss: 398.8025 - val_MinusLogProbMetric: 398.8025 - lr: 5.5556e-05 - 23s/epoch - 119ms/step
Epoch 144/1000
2023-09-09 23:53:05.941 
Epoch 144/1000 
	 loss: 388.4678, MinusLogProbMetric: 388.4678, val_loss: 397.9750, val_MinusLogProbMetric: 397.9750

Epoch 144: val_loss did not improve from 397.12659
196/196 - 27s - loss: 388.4678 - MinusLogProbMetric: 388.4678 - val_loss: 397.9750 - val_MinusLogProbMetric: 397.9750 - lr: 5.5556e-05 - 27s/epoch - 135ms/step
Epoch 145/1000
2023-09-09 23:53:31.904 
Epoch 145/1000 
	 loss: 388.4112, MinusLogProbMetric: 388.4112, val_loss: 397.5111, val_MinusLogProbMetric: 397.5111

Epoch 145: val_loss did not improve from 397.12659
196/196 - 26s - loss: 388.4112 - MinusLogProbMetric: 388.4112 - val_loss: 397.5111 - val_MinusLogProbMetric: 397.5111 - lr: 5.5556e-05 - 26s/epoch - 132ms/step
Epoch 146/1000
2023-09-09 23:53:56.386 
Epoch 146/1000 
	 loss: 389.1273, MinusLogProbMetric: 389.1273, val_loss: 396.9007, val_MinusLogProbMetric: 396.9007

Epoch 146: val_loss improved from 397.12659 to 396.90073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 25s - loss: 389.1273 - MinusLogProbMetric: 389.1273 - val_loss: 396.9007 - val_MinusLogProbMetric: 396.9007 - lr: 5.5556e-05 - 25s/epoch - 130ms/step
Epoch 147/1000
2023-09-09 23:54:22.929 
Epoch 147/1000 
	 loss: 388.0808, MinusLogProbMetric: 388.0808, val_loss: 397.6837, val_MinusLogProbMetric: 397.6837

Epoch 147: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.0808 - MinusLogProbMetric: 388.0808 - val_loss: 397.6837 - val_MinusLogProbMetric: 397.6837 - lr: 5.5556e-05 - 26s/epoch - 130ms/step
Epoch 148/1000
2023-09-09 23:54:47.403 
Epoch 148/1000 
	 loss: 388.6892, MinusLogProbMetric: 388.6892, val_loss: 397.7035, val_MinusLogProbMetric: 397.7035

Epoch 148: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.6892 - MinusLogProbMetric: 388.6892 - val_loss: 397.7035 - val_MinusLogProbMetric: 397.7035 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 149/1000
2023-09-09 23:55:13.000 
Epoch 149/1000 
	 loss: 388.3099, MinusLogProbMetric: 388.3099, val_loss: 397.1340, val_MinusLogProbMetric: 397.1340

Epoch 149: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.3099 - MinusLogProbMetric: 388.3099 - val_loss: 397.1340 - val_MinusLogProbMetric: 397.1340 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 150/1000
2023-09-09 23:55:37.781 
Epoch 150/1000 
	 loss: 388.8339, MinusLogProbMetric: 388.8339, val_loss: 398.1337, val_MinusLogProbMetric: 398.1337

Epoch 150: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.8339 - MinusLogProbMetric: 388.8339 - val_loss: 398.1337 - val_MinusLogProbMetric: 398.1337 - lr: 5.5556e-05 - 25s/epoch - 126ms/step
Epoch 151/1000
2023-09-09 23:56:02.387 
Epoch 151/1000 
	 loss: 388.1640, MinusLogProbMetric: 388.1640, val_loss: 397.3881, val_MinusLogProbMetric: 397.3881

Epoch 151: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.1640 - MinusLogProbMetric: 388.1640 - val_loss: 397.3881 - val_MinusLogProbMetric: 397.3881 - lr: 5.5556e-05 - 25s/epoch - 126ms/step
Epoch 152/1000
2023-09-09 23:56:27.944 
Epoch 152/1000 
	 loss: 388.3571, MinusLogProbMetric: 388.3571, val_loss: 400.1029, val_MinusLogProbMetric: 400.1029

Epoch 152: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.3571 - MinusLogProbMetric: 388.3571 - val_loss: 400.1029 - val_MinusLogProbMetric: 400.1029 - lr: 5.5556e-05 - 26s/epoch - 130ms/step
Epoch 153/1000
2023-09-09 23:56:54.300 
Epoch 153/1000 
	 loss: 388.1366, MinusLogProbMetric: 388.1366, val_loss: 397.7125, val_MinusLogProbMetric: 397.7125

Epoch 153: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.1366 - MinusLogProbMetric: 388.1366 - val_loss: 397.7125 - val_MinusLogProbMetric: 397.7125 - lr: 5.5556e-05 - 26s/epoch - 134ms/step
Epoch 154/1000
2023-09-09 23:57:18.904 
Epoch 154/1000 
	 loss: 388.2107, MinusLogProbMetric: 388.2107, val_loss: 397.5062, val_MinusLogProbMetric: 397.5062

Epoch 154: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.2107 - MinusLogProbMetric: 388.2107 - val_loss: 397.5062 - val_MinusLogProbMetric: 397.5062 - lr: 5.5556e-05 - 25s/epoch - 125ms/step
Epoch 155/1000
2023-09-09 23:57:44.542 
Epoch 155/1000 
	 loss: 388.1610, MinusLogProbMetric: 388.1610, val_loss: 397.5256, val_MinusLogProbMetric: 397.5256

Epoch 155: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.1610 - MinusLogProbMetric: 388.1610 - val_loss: 397.5256 - val_MinusLogProbMetric: 397.5256 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 156/1000
2023-09-09 23:58:09.186 
Epoch 156/1000 
	 loss: 388.5560, MinusLogProbMetric: 388.5560, val_loss: 398.2602, val_MinusLogProbMetric: 398.2602

Epoch 156: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.5560 - MinusLogProbMetric: 388.5560 - val_loss: 398.2602 - val_MinusLogProbMetric: 398.2602 - lr: 5.5556e-05 - 25s/epoch - 126ms/step
Epoch 157/1000
2023-09-09 23:58:33.755 
Epoch 157/1000 
	 loss: 388.1572, MinusLogProbMetric: 388.1572, val_loss: 398.9489, val_MinusLogProbMetric: 398.9489

Epoch 157: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.1572 - MinusLogProbMetric: 388.1572 - val_loss: 398.9489 - val_MinusLogProbMetric: 398.9489 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 158/1000
2023-09-09 23:59:00.009 
Epoch 158/1000 
	 loss: 388.1721, MinusLogProbMetric: 388.1721, val_loss: 397.9033, val_MinusLogProbMetric: 397.9033

Epoch 158: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.1721 - MinusLogProbMetric: 388.1721 - val_loss: 397.9033 - val_MinusLogProbMetric: 397.9033 - lr: 5.5556e-05 - 26s/epoch - 134ms/step
Epoch 159/1000
2023-09-09 23:59:24.399 
Epoch 159/1000 
	 loss: 388.2186, MinusLogProbMetric: 388.2186, val_loss: 398.4389, val_MinusLogProbMetric: 398.4389

Epoch 159: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.2186 - MinusLogProbMetric: 388.2186 - val_loss: 398.4389 - val_MinusLogProbMetric: 398.4389 - lr: 5.5556e-05 - 24s/epoch - 124ms/step
Epoch 160/1000
2023-09-09 23:59:49.689 
Epoch 160/1000 
	 loss: 388.3152, MinusLogProbMetric: 388.3152, val_loss: 397.9162, val_MinusLogProbMetric: 397.9162

Epoch 160: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.3152 - MinusLogProbMetric: 388.3152 - val_loss: 397.9162 - val_MinusLogProbMetric: 397.9162 - lr: 5.5556e-05 - 25s/epoch - 129ms/step
Epoch 161/1000
2023-09-10 00:00:15.192 
Epoch 161/1000 
	 loss: 388.0603, MinusLogProbMetric: 388.0603, val_loss: 397.2723, val_MinusLogProbMetric: 397.2723

Epoch 161: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.0603 - MinusLogProbMetric: 388.0603 - val_loss: 397.2723 - val_MinusLogProbMetric: 397.2723 - lr: 5.5556e-05 - 26s/epoch - 130ms/step
Epoch 162/1000
2023-09-10 00:00:39.134 
Epoch 162/1000 
	 loss: 388.1344, MinusLogProbMetric: 388.1344, val_loss: 397.4473, val_MinusLogProbMetric: 397.4473

Epoch 162: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.1344 - MinusLogProbMetric: 388.1344 - val_loss: 397.4473 - val_MinusLogProbMetric: 397.4473 - lr: 5.5556e-05 - 24s/epoch - 122ms/step
Epoch 163/1000
2023-09-10 00:01:06.298 
Epoch 163/1000 
	 loss: 388.1239, MinusLogProbMetric: 388.1239, val_loss: 396.9070, val_MinusLogProbMetric: 396.9070

Epoch 163: val_loss did not improve from 396.90073
196/196 - 27s - loss: 388.1239 - MinusLogProbMetric: 388.1239 - val_loss: 396.9070 - val_MinusLogProbMetric: 396.9070 - lr: 5.5556e-05 - 27s/epoch - 139ms/step
Epoch 164/1000
2023-09-10 00:01:30.435 
Epoch 164/1000 
	 loss: 387.9351, MinusLogProbMetric: 387.9351, val_loss: 397.6101, val_MinusLogProbMetric: 397.6101

Epoch 164: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.9351 - MinusLogProbMetric: 387.9351 - val_loss: 397.6101 - val_MinusLogProbMetric: 397.6101 - lr: 5.5556e-05 - 24s/epoch - 123ms/step
Epoch 165/1000
2023-09-10 00:01:54.789 
Epoch 165/1000 
	 loss: 388.0817, MinusLogProbMetric: 388.0817, val_loss: 398.4139, val_MinusLogProbMetric: 398.4139

Epoch 165: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.0817 - MinusLogProbMetric: 388.0817 - val_loss: 398.4139 - val_MinusLogProbMetric: 398.4139 - lr: 5.5556e-05 - 24s/epoch - 124ms/step
Epoch 166/1000
2023-09-10 00:02:19.650 
Epoch 166/1000 
	 loss: 388.2254, MinusLogProbMetric: 388.2254, val_loss: 397.7527, val_MinusLogProbMetric: 397.7527

Epoch 166: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.2254 - MinusLogProbMetric: 388.2254 - val_loss: 397.7527 - val_MinusLogProbMetric: 397.7527 - lr: 5.5556e-05 - 25s/epoch - 127ms/step
Epoch 167/1000
2023-09-10 00:02:45.409 
Epoch 167/1000 
	 loss: 387.9945, MinusLogProbMetric: 387.9945, val_loss: 397.9294, val_MinusLogProbMetric: 397.9294

Epoch 167: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.9945 - MinusLogProbMetric: 387.9945 - val_loss: 397.9294 - val_MinusLogProbMetric: 397.9294 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 168/1000
2023-09-10 00:03:11.552 
Epoch 168/1000 
	 loss: 387.9968, MinusLogProbMetric: 387.9968, val_loss: 398.4537, val_MinusLogProbMetric: 398.4537

Epoch 168: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.9968 - MinusLogProbMetric: 387.9968 - val_loss: 398.4537 - val_MinusLogProbMetric: 398.4537 - lr: 5.5556e-05 - 26s/epoch - 133ms/step
Epoch 169/1000
2023-09-10 00:03:36.523 
Epoch 169/1000 
	 loss: 388.0807, MinusLogProbMetric: 388.0807, val_loss: 399.9519, val_MinusLogProbMetric: 399.9519

Epoch 169: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.0807 - MinusLogProbMetric: 388.0807 - val_loss: 399.9519 - val_MinusLogProbMetric: 399.9519 - lr: 5.5556e-05 - 25s/epoch - 127ms/step
Epoch 170/1000
2023-09-10 00:04:01.507 
Epoch 170/1000 
	 loss: 388.2957, MinusLogProbMetric: 388.2957, val_loss: 398.9233, val_MinusLogProbMetric: 398.9233

Epoch 170: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.2957 - MinusLogProbMetric: 388.2957 - val_loss: 398.9233 - val_MinusLogProbMetric: 398.9233 - lr: 5.5556e-05 - 25s/epoch - 127ms/step
Epoch 171/1000
2023-09-10 00:04:25.611 
Epoch 171/1000 
	 loss: 388.3356, MinusLogProbMetric: 388.3356, val_loss: 397.2207, val_MinusLogProbMetric: 397.2207

Epoch 171: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.3356 - MinusLogProbMetric: 388.3356 - val_loss: 397.2207 - val_MinusLogProbMetric: 397.2207 - lr: 5.5556e-05 - 24s/epoch - 123ms/step
Epoch 172/1000
2023-09-10 00:04:51.409 
Epoch 172/1000 
	 loss: 387.8265, MinusLogProbMetric: 387.8265, val_loss: 399.4108, val_MinusLogProbMetric: 399.4108

Epoch 172: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.8265 - MinusLogProbMetric: 387.8265 - val_loss: 399.4108 - val_MinusLogProbMetric: 399.4108 - lr: 5.5556e-05 - 26s/epoch - 132ms/step
Epoch 173/1000
2023-09-10 00:05:15.935 
Epoch 173/1000 
	 loss: 388.0686, MinusLogProbMetric: 388.0686, val_loss: 398.9640, val_MinusLogProbMetric: 398.9640

Epoch 173: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.0686 - MinusLogProbMetric: 388.0686 - val_loss: 398.9640 - val_MinusLogProbMetric: 398.9640 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 174/1000
2023-09-10 00:05:41.767 
Epoch 174/1000 
	 loss: 388.1353, MinusLogProbMetric: 388.1353, val_loss: 398.8715, val_MinusLogProbMetric: 398.8715

Epoch 174: val_loss did not improve from 396.90073
196/196 - 26s - loss: 388.1353 - MinusLogProbMetric: 388.1353 - val_loss: 398.8715 - val_MinusLogProbMetric: 398.8715 - lr: 5.5556e-05 - 26s/epoch - 132ms/step
Epoch 175/1000
2023-09-10 00:06:07.168 
Epoch 175/1000 
	 loss: 388.5965, MinusLogProbMetric: 388.5965, val_loss: 397.1663, val_MinusLogProbMetric: 397.1663

Epoch 175: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.5965 - MinusLogProbMetric: 388.5965 - val_loss: 397.1663 - val_MinusLogProbMetric: 397.1663 - lr: 5.5556e-05 - 25s/epoch - 129ms/step
Epoch 176/1000
2023-09-10 00:06:31.566 
Epoch 176/1000 
	 loss: 388.3177, MinusLogProbMetric: 388.3177, val_loss: 397.1978, val_MinusLogProbMetric: 397.1978

Epoch 176: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.3177 - MinusLogProbMetric: 388.3177 - val_loss: 397.1978 - val_MinusLogProbMetric: 397.1978 - lr: 5.5556e-05 - 24s/epoch - 124ms/step
Epoch 177/1000
2023-09-10 00:06:55.589 
Epoch 177/1000 
	 loss: 388.0943, MinusLogProbMetric: 388.0943, val_loss: 398.2880, val_MinusLogProbMetric: 398.2880

Epoch 177: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.0943 - MinusLogProbMetric: 388.0943 - val_loss: 398.2880 - val_MinusLogProbMetric: 398.2880 - lr: 5.5556e-05 - 24s/epoch - 123ms/step
Epoch 178/1000
2023-09-10 00:07:21.201 
Epoch 178/1000 
	 loss: 387.9934, MinusLogProbMetric: 387.9934, val_loss: 398.0774, val_MinusLogProbMetric: 398.0774

Epoch 178: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.9934 - MinusLogProbMetric: 387.9934 - val_loss: 398.0774 - val_MinusLogProbMetric: 398.0774 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 179/1000
2023-09-10 00:07:45.140 
Epoch 179/1000 
	 loss: 387.7880, MinusLogProbMetric: 387.7880, val_loss: 397.7151, val_MinusLogProbMetric: 397.7151

Epoch 179: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.7880 - MinusLogProbMetric: 387.7880 - val_loss: 397.7151 - val_MinusLogProbMetric: 397.7151 - lr: 5.5556e-05 - 24s/epoch - 122ms/step
Epoch 180/1000
2023-09-10 00:08:10.446 
Epoch 180/1000 
	 loss: 387.9748, MinusLogProbMetric: 387.9748, val_loss: 398.0273, val_MinusLogProbMetric: 398.0273

Epoch 180: val_loss did not improve from 396.90073
196/196 - 25s - loss: 387.9748 - MinusLogProbMetric: 387.9748 - val_loss: 398.0273 - val_MinusLogProbMetric: 398.0273 - lr: 5.5556e-05 - 25s/epoch - 129ms/step
Epoch 181/1000
2023-09-10 00:08:36.026 
Epoch 181/1000 
	 loss: 387.8522, MinusLogProbMetric: 387.8522, val_loss: 397.9160, val_MinusLogProbMetric: 397.9160

Epoch 181: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.8522 - MinusLogProbMetric: 387.8522 - val_loss: 397.9160 - val_MinusLogProbMetric: 397.9160 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 182/1000
2023-09-10 00:08:59.269 
Epoch 182/1000 
	 loss: 388.3853, MinusLogProbMetric: 388.3853, val_loss: 397.3591, val_MinusLogProbMetric: 397.3591

Epoch 182: val_loss did not improve from 396.90073
196/196 - 23s - loss: 388.3853 - MinusLogProbMetric: 388.3853 - val_loss: 397.3591 - val_MinusLogProbMetric: 397.3591 - lr: 5.5556e-05 - 23s/epoch - 118ms/step
Epoch 183/1000
2023-09-10 00:09:23.698 
Epoch 183/1000 
	 loss: 388.3521, MinusLogProbMetric: 388.3521, val_loss: 397.6200, val_MinusLogProbMetric: 397.6200

Epoch 183: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.3521 - MinusLogProbMetric: 388.3521 - val_loss: 397.6200 - val_MinusLogProbMetric: 397.6200 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 184/1000
2023-09-10 00:09:48.248 
Epoch 184/1000 
	 loss: 387.8290, MinusLogProbMetric: 387.8290, val_loss: 397.8557, val_MinusLogProbMetric: 397.8557

Epoch 184: val_loss did not improve from 396.90073
196/196 - 25s - loss: 387.8290 - MinusLogProbMetric: 387.8290 - val_loss: 397.8557 - val_MinusLogProbMetric: 397.8557 - lr: 5.5556e-05 - 25s/epoch - 125ms/step
Epoch 185/1000
2023-09-10 00:10:12.705 
Epoch 185/1000 
	 loss: 387.5580, MinusLogProbMetric: 387.5580, val_loss: 398.1166, val_MinusLogProbMetric: 398.1166

Epoch 185: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.5580 - MinusLogProbMetric: 387.5580 - val_loss: 398.1166 - val_MinusLogProbMetric: 398.1166 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 186/1000
2023-09-10 00:10:39.090 
Epoch 186/1000 
	 loss: 387.5530, MinusLogProbMetric: 387.5530, val_loss: 397.9492, val_MinusLogProbMetric: 397.9492

Epoch 186: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.5530 - MinusLogProbMetric: 387.5530 - val_loss: 397.9492 - val_MinusLogProbMetric: 397.9492 - lr: 5.5556e-05 - 26s/epoch - 135ms/step
Epoch 187/1000
2023-09-10 00:11:03.289 
Epoch 187/1000 
	 loss: 387.6155, MinusLogProbMetric: 387.6155, val_loss: 397.8325, val_MinusLogProbMetric: 397.8325

Epoch 187: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.6155 - MinusLogProbMetric: 387.6155 - val_loss: 397.8325 - val_MinusLogProbMetric: 397.8325 - lr: 5.5556e-05 - 24s/epoch - 123ms/step
Epoch 188/1000
2023-09-10 00:11:28.239 
Epoch 188/1000 
	 loss: 388.0818, MinusLogProbMetric: 388.0818, val_loss: 400.9754, val_MinusLogProbMetric: 400.9754

Epoch 188: val_loss did not improve from 396.90073
196/196 - 25s - loss: 388.0818 - MinusLogProbMetric: 388.0818 - val_loss: 400.9754 - val_MinusLogProbMetric: 400.9754 - lr: 5.5556e-05 - 25s/epoch - 127ms/step
Epoch 189/1000
2023-09-10 00:11:53.650 
Epoch 189/1000 
	 loss: 387.7342, MinusLogProbMetric: 387.7342, val_loss: 397.7990, val_MinusLogProbMetric: 397.7990

Epoch 189: val_loss did not improve from 396.90073
196/196 - 25s - loss: 387.7342 - MinusLogProbMetric: 387.7342 - val_loss: 397.7990 - val_MinusLogProbMetric: 397.7990 - lr: 5.5556e-05 - 25s/epoch - 130ms/step
Epoch 190/1000
2023-09-10 00:12:19.266 
Epoch 190/1000 
	 loss: 387.5980, MinusLogProbMetric: 387.5980, val_loss: 399.9722, val_MinusLogProbMetric: 399.9722

Epoch 190: val_loss did not improve from 396.90073
196/196 - 26s - loss: 387.5980 - MinusLogProbMetric: 387.5980 - val_loss: 399.9722 - val_MinusLogProbMetric: 399.9722 - lr: 5.5556e-05 - 26s/epoch - 131ms/step
Epoch 191/1000
2023-09-10 00:12:42.992 
Epoch 191/1000 
	 loss: 387.8252, MinusLogProbMetric: 387.8252, val_loss: 397.3660, val_MinusLogProbMetric: 397.3660

Epoch 191: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.8252 - MinusLogProbMetric: 387.8252 - val_loss: 397.3660 - val_MinusLogProbMetric: 397.3660 - lr: 5.5556e-05 - 24s/epoch - 121ms/step
Epoch 192/1000
2023-09-10 00:13:07.321 
Epoch 192/1000 
	 loss: 387.6817, MinusLogProbMetric: 387.6817, val_loss: 398.3588, val_MinusLogProbMetric: 398.3588

Epoch 192: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.6817 - MinusLogProbMetric: 387.6817 - val_loss: 398.3588 - val_MinusLogProbMetric: 398.3588 - lr: 5.5556e-05 - 24s/epoch - 124ms/step
Epoch 193/1000
2023-09-10 00:13:31.787 
Epoch 193/1000 
	 loss: 388.1171, MinusLogProbMetric: 388.1171, val_loss: 397.9526, val_MinusLogProbMetric: 397.9526

Epoch 193: val_loss did not improve from 396.90073
196/196 - 24s - loss: 388.1171 - MinusLogProbMetric: 388.1171 - val_loss: 397.9526 - val_MinusLogProbMetric: 397.9526 - lr: 5.5556e-05 - 24s/epoch - 125ms/step
Epoch 194/1000
2023-09-10 00:13:55.809 
Epoch 194/1000 
	 loss: 387.8328, MinusLogProbMetric: 387.8328, val_loss: 397.8695, val_MinusLogProbMetric: 397.8695

Epoch 194: val_loss did not improve from 396.90073
196/196 - 24s - loss: 387.8328 - MinusLogProbMetric: 387.8328 - val_loss: 397.8695 - val_MinusLogProbMetric: 397.8695 - lr: 5.5556e-05 - 24s/epoch - 122ms/step
Epoch 195/1000
2023-09-10 00:14:22.379 
Epoch 195/1000 
	 loss: 387.5439, MinusLogProbMetric: 387.5439, val_loss: 397.8572, val_MinusLogProbMetric: 397.8572

Epoch 195: val_loss did not improve from 396.90073
196/196 - 27s - loss: 387.5439 - MinusLogProbMetric: 387.5439 - val_loss: 397.8572 - val_MinusLogProbMetric: 397.8572 - lr: 5.5556e-05 - 27s/epoch - 136ms/step
Epoch 196/1000
2023-09-10 00:14:49.100 
Epoch 196/1000 
	 loss: 388.2604, MinusLogProbMetric: 388.2604, val_loss: 397.5379, val_MinusLogProbMetric: 397.5379

Epoch 196: val_loss did not improve from 396.90073
196/196 - 27s - loss: 388.2604 - MinusLogProbMetric: 388.2604 - val_loss: 397.5379 - val_MinusLogProbMetric: 397.5379 - lr: 5.5556e-05 - 27s/epoch - 136ms/step
Epoch 197/1000
2023-09-10 00:15:15.830 
Epoch 197/1000 
	 loss: 385.8293, MinusLogProbMetric: 385.8293, val_loss: 396.6462, val_MinusLogProbMetric: 396.6462

Epoch 197: val_loss improved from 396.90073 to 396.64621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 28s - loss: 385.8293 - MinusLogProbMetric: 385.8293 - val_loss: 396.6462 - val_MinusLogProbMetric: 396.6462 - lr: 2.7778e-05 - 28s/epoch - 143ms/step
Epoch 198/1000
2023-09-10 00:15:43.721 
Epoch 198/1000 
	 loss: 385.5919, MinusLogProbMetric: 385.5919, val_loss: 396.7590, val_MinusLogProbMetric: 396.7590

Epoch 198: val_loss did not improve from 396.64621
196/196 - 27s - loss: 385.5919 - MinusLogProbMetric: 385.5919 - val_loss: 396.7590 - val_MinusLogProbMetric: 396.7590 - lr: 2.7778e-05 - 27s/epoch - 135ms/step
Epoch 199/1000
2023-09-10 00:16:09.669 
Epoch 199/1000 
	 loss: 385.6086, MinusLogProbMetric: 385.6086, val_loss: 396.9571, val_MinusLogProbMetric: 396.9571

Epoch 199: val_loss did not improve from 396.64621
196/196 - 26s - loss: 385.6086 - MinusLogProbMetric: 385.6086 - val_loss: 396.9571 - val_MinusLogProbMetric: 396.9571 - lr: 2.7778e-05 - 26s/epoch - 132ms/step
Epoch 200/1000
2023-09-10 00:16:35.289 
Epoch 200/1000 
	 loss: 385.7481, MinusLogProbMetric: 385.7481, val_loss: 398.1050, val_MinusLogProbMetric: 398.1050

Epoch 200: val_loss did not improve from 396.64621
196/196 - 26s - loss: 385.7481 - MinusLogProbMetric: 385.7481 - val_loss: 398.1050 - val_MinusLogProbMetric: 398.1050 - lr: 2.7778e-05 - 26s/epoch - 131ms/step
Epoch 201/1000
2023-09-10 00:17:02.249 
Epoch 201/1000 
	 loss: 385.7816, MinusLogProbMetric: 385.7816, val_loss: 396.8395, val_MinusLogProbMetric: 396.8395

Epoch 201: val_loss did not improve from 396.64621
196/196 - 27s - loss: 385.7816 - MinusLogProbMetric: 385.7816 - val_loss: 396.8395 - val_MinusLogProbMetric: 396.8395 - lr: 2.7778e-05 - 27s/epoch - 138ms/step
Epoch 202/1000
2023-09-10 00:17:26.862 
Epoch 202/1000 
	 loss: 385.6622, MinusLogProbMetric: 385.6622, val_loss: 397.1938, val_MinusLogProbMetric: 397.1938

Epoch 202: val_loss did not improve from 396.64621
196/196 - 25s - loss: 385.6622 - MinusLogProbMetric: 385.6622 - val_loss: 397.1938 - val_MinusLogProbMetric: 397.1938 - lr: 2.7778e-05 - 25s/epoch - 126ms/step
Epoch 203/1000
2023-09-10 00:17:50.695 
Epoch 203/1000 
	 loss: 385.6272, MinusLogProbMetric: 385.6272, val_loss: 396.8214, val_MinusLogProbMetric: 396.8214

Epoch 203: val_loss did not improve from 396.64621
196/196 - 24s - loss: 385.6272 - MinusLogProbMetric: 385.6272 - val_loss: 396.8214 - val_MinusLogProbMetric: 396.8214 - lr: 2.7778e-05 - 24s/epoch - 122ms/step
Epoch 204/1000
2023-09-10 00:18:15.627 
Epoch 204/1000 
	 loss: 385.5365, MinusLogProbMetric: 385.5365, val_loss: 397.2269, val_MinusLogProbMetric: 397.2269

Epoch 204: val_loss did not improve from 396.64621
196/196 - 25s - loss: 385.5365 - MinusLogProbMetric: 385.5365 - val_loss: 397.2269 - val_MinusLogProbMetric: 397.2269 - lr: 2.7778e-05 - 25s/epoch - 127ms/step
Epoch 205/1000
2023-09-10 00:18:42.048 
Epoch 205/1000 
	 loss: 385.9297, MinusLogProbMetric: 385.9297, val_loss: 397.6329, val_MinusLogProbMetric: 397.6329

Epoch 205: val_loss did not improve from 396.64621
196/196 - 26s - loss: 385.9297 - MinusLogProbMetric: 385.9297 - val_loss: 397.6329 - val_MinusLogProbMetric: 397.6329 - lr: 2.7778e-05 - 26s/epoch - 135ms/step
Epoch 206/1000
2023-09-10 00:19:06.590 
Epoch 206/1000 
	 loss: 385.7307, MinusLogProbMetric: 385.7307, val_loss: 397.2084, val_MinusLogProbMetric: 397.2084

Epoch 206: val_loss did not improve from 396.64621
196/196 - 25s - loss: 385.7307 - MinusLogProbMetric: 385.7307 - val_loss: 397.2084 - val_MinusLogProbMetric: 397.2084 - lr: 2.7778e-05 - 25s/epoch - 125ms/step
Epoch 207/1000
2023-09-10 00:19:32.844 
Epoch 207/1000 
	 loss: 385.6914, MinusLogProbMetric: 385.6914, val_loss: 397.0320, val_MinusLogProbMetric: 397.0320

Epoch 207: val_loss did not improve from 396.64621
196/196 - 26s - loss: 385.6914 - MinusLogProbMetric: 385.6914 - val_loss: 397.0320 - val_MinusLogProbMetric: 397.0320 - lr: 2.7778e-05 - 26s/epoch - 134ms/step
Epoch 208/1000
2023-09-10 00:20:00.090 
Epoch 208/1000 
	 loss: 385.6832, MinusLogProbMetric: 385.6832, val_loss: 397.5201, val_MinusLogProbMetric: 397.5201

Epoch 208: val_loss did not improve from 396.64621
196/196 - 27s - loss: 385.6832 - MinusLogProbMetric: 385.6832 - val_loss: 397.5201 - val_MinusLogProbMetric: 397.5201 - lr: 2.7778e-05 - 27s/epoch - 139ms/step
Epoch 209/1000
2023-09-10 00:20:25.696 
Epoch 209/1000 
	 loss: 385.7520, MinusLogProbMetric: 385.7520, val_loss: 397.3506, val_MinusLogProbMetric: 397.3506

Epoch 209: val_loss did not improve from 396.64621
196/196 - 26s - loss: 385.7520 - MinusLogProbMetric: 385.7520 - val_loss: 397.3506 - val_MinusLogProbMetric: 397.3506 - lr: 2.7778e-05 - 26s/epoch - 130ms/step
Epoch 210/1000
2023-09-10 00:20:50.019 
Epoch 210/1000 
	 loss: 385.6743, MinusLogProbMetric: 385.6743, val_loss: 396.9279, val_MinusLogProbMetric: 396.9279

Epoch 210: val_loss did not improve from 396.64621
196/196 - 24s - loss: 385.6743 - MinusLogProbMetric: 385.6743 - val_loss: 396.9279 - val_MinusLogProbMetric: 396.9279 - lr: 2.7778e-05 - 24s/epoch - 124ms/step
Epoch 211/1000
2023-09-10 00:21:15.914 
Epoch 211/1000 
	 loss: 385.4494, MinusLogProbMetric: 385.4494, val_loss: 397.2321, val_MinusLogProbMetric: 397.2321

Epoch 211: val_loss did not improve from 396.64621
196/196 - 26s - loss: 385.4494 - MinusLogProbMetric: 385.4494 - val_loss: 397.2321 - val_MinusLogProbMetric: 397.2321 - lr: 2.7778e-05 - 26s/epoch - 132ms/step
Epoch 212/1000
2023-09-10 00:21:39.831 
Epoch 212/1000 
	 loss: 385.5579, MinusLogProbMetric: 385.5579, val_loss: 398.1690, val_MinusLogProbMetric: 398.1690

Epoch 212: val_loss did not improve from 396.64621
196/196 - 24s - loss: 385.5579 - MinusLogProbMetric: 385.5579 - val_loss: 398.1690 - val_MinusLogProbMetric: 398.1690 - lr: 2.7778e-05 - 24s/epoch - 122ms/step
Epoch 213/1000
2023-09-10 00:22:07.158 
Epoch 213/1000 
	 loss: 385.6514, MinusLogProbMetric: 385.6514, val_loss: 397.1520, val_MinusLogProbMetric: 397.1520

Epoch 213: val_loss did not improve from 396.64621
196/196 - 27s - loss: 385.6514 - MinusLogProbMetric: 385.6514 - val_loss: 397.1520 - val_MinusLogProbMetric: 397.1520 - lr: 2.7778e-05 - 27s/epoch - 139ms/step
Epoch 214/1000
2023-09-10 00:22:31.137 
Epoch 214/1000 
	 loss: 385.6214, MinusLogProbMetric: 385.6214, val_loss: 397.6854, val_MinusLogProbMetric: 397.6854

Epoch 214: val_loss did not improve from 396.64621
196/196 - 24s - loss: 385.6214 - MinusLogProbMetric: 385.6214 - val_loss: 397.6854 - val_MinusLogProbMetric: 397.6854 - lr: 2.7778e-05 - 24s/epoch - 122ms/step
Epoch 215/1000
2023-09-10 00:22:54.233 
Epoch 215/1000 
	 loss: 385.6795, MinusLogProbMetric: 385.6795, val_loss: 396.7114, val_MinusLogProbMetric: 396.7114

Epoch 215: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.6795 - MinusLogProbMetric: 385.6795 - val_loss: 396.7114 - val_MinusLogProbMetric: 396.7114 - lr: 2.7778e-05 - 23s/epoch - 118ms/step
Epoch 216/1000
2023-09-10 00:23:15.654 
Epoch 216/1000 
	 loss: 385.6581, MinusLogProbMetric: 385.6581, val_loss: 397.5508, val_MinusLogProbMetric: 397.5508

Epoch 216: val_loss did not improve from 396.64621
196/196 - 21s - loss: 385.6581 - MinusLogProbMetric: 385.6581 - val_loss: 397.5508 - val_MinusLogProbMetric: 397.5508 - lr: 2.7778e-05 - 21s/epoch - 109ms/step
Epoch 217/1000
2023-09-10 00:23:43.302 
Epoch 217/1000 
	 loss: 386.0663, MinusLogProbMetric: 386.0663, val_loss: 396.8221, val_MinusLogProbMetric: 396.8221

Epoch 217: val_loss did not improve from 396.64621
196/196 - 28s - loss: 386.0663 - MinusLogProbMetric: 386.0663 - val_loss: 396.8221 - val_MinusLogProbMetric: 396.8221 - lr: 2.7778e-05 - 28s/epoch - 141ms/step
Epoch 218/1000
2023-09-10 00:24:08.231 
Epoch 218/1000 
	 loss: 385.5380, MinusLogProbMetric: 385.5380, val_loss: 397.0672, val_MinusLogProbMetric: 397.0672

Epoch 218: val_loss did not improve from 396.64621
196/196 - 25s - loss: 385.5380 - MinusLogProbMetric: 385.5380 - val_loss: 397.0672 - val_MinusLogProbMetric: 397.0672 - lr: 2.7778e-05 - 25s/epoch - 127ms/step
Epoch 219/1000
2023-09-10 00:24:31.604 
Epoch 219/1000 
	 loss: 385.6538, MinusLogProbMetric: 385.6538, val_loss: 397.6231, val_MinusLogProbMetric: 397.6231

Epoch 219: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.6538 - MinusLogProbMetric: 385.6538 - val_loss: 397.6231 - val_MinusLogProbMetric: 397.6231 - lr: 2.7778e-05 - 23s/epoch - 119ms/step
Epoch 220/1000
2023-09-10 00:24:54.155 
Epoch 220/1000 
	 loss: 385.7525, MinusLogProbMetric: 385.7525, val_loss: 397.0555, val_MinusLogProbMetric: 397.0555

Epoch 220: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.7525 - MinusLogProbMetric: 385.7525 - val_loss: 397.0555 - val_MinusLogProbMetric: 397.0555 - lr: 2.7778e-05 - 23s/epoch - 115ms/step
Epoch 221/1000
2023-09-10 00:25:17.015 
Epoch 221/1000 
	 loss: 385.6710, MinusLogProbMetric: 385.6710, val_loss: 396.8151, val_MinusLogProbMetric: 396.8151

Epoch 221: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.6710 - MinusLogProbMetric: 385.6710 - val_loss: 396.8151 - val_MinusLogProbMetric: 396.8151 - lr: 2.7778e-05 - 23s/epoch - 117ms/step
Epoch 222/1000
2023-09-10 00:25:39.417 
Epoch 222/1000 
	 loss: 385.5408, MinusLogProbMetric: 385.5408, val_loss: 396.9583, val_MinusLogProbMetric: 396.9583

Epoch 222: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.5408 - MinusLogProbMetric: 385.5408 - val_loss: 396.9583 - val_MinusLogProbMetric: 396.9583 - lr: 2.7778e-05 - 22s/epoch - 114ms/step
Epoch 223/1000
2023-09-10 00:26:01.513 
Epoch 223/1000 
	 loss: 385.9410, MinusLogProbMetric: 385.9410, val_loss: 397.9948, val_MinusLogProbMetric: 397.9948

Epoch 223: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.9410 - MinusLogProbMetric: 385.9410 - val_loss: 397.9948 - val_MinusLogProbMetric: 397.9948 - lr: 2.7778e-05 - 22s/epoch - 113ms/step
Epoch 224/1000
2023-09-10 00:26:24.054 
Epoch 224/1000 
	 loss: 385.5944, MinusLogProbMetric: 385.5944, val_loss: 396.9885, val_MinusLogProbMetric: 396.9885

Epoch 224: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.5944 - MinusLogProbMetric: 385.5944 - val_loss: 396.9885 - val_MinusLogProbMetric: 396.9885 - lr: 2.7778e-05 - 23s/epoch - 115ms/step
Epoch 225/1000
2023-09-10 00:26:46.668 
Epoch 225/1000 
	 loss: 385.7653, MinusLogProbMetric: 385.7653, val_loss: 397.2061, val_MinusLogProbMetric: 397.2061

Epoch 225: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.7653 - MinusLogProbMetric: 385.7653 - val_loss: 397.2061 - val_MinusLogProbMetric: 397.2061 - lr: 2.7778e-05 - 23s/epoch - 115ms/step
Epoch 226/1000
2023-09-10 00:27:10.142 
Epoch 226/1000 
	 loss: 385.5943, MinusLogProbMetric: 385.5943, val_loss: 405.2834, val_MinusLogProbMetric: 405.2834

Epoch 226: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.5943 - MinusLogProbMetric: 385.5943 - val_loss: 405.2834 - val_MinusLogProbMetric: 405.2834 - lr: 2.7778e-05 - 23s/epoch - 120ms/step
Epoch 227/1000
2023-09-10 00:27:32.177 
Epoch 227/1000 
	 loss: 386.2012, MinusLogProbMetric: 386.2012, val_loss: 398.4915, val_MinusLogProbMetric: 398.4915

Epoch 227: val_loss did not improve from 396.64621
196/196 - 22s - loss: 386.2012 - MinusLogProbMetric: 386.2012 - val_loss: 398.4915 - val_MinusLogProbMetric: 398.4915 - lr: 2.7778e-05 - 22s/epoch - 112ms/step
Epoch 228/1000
2023-09-10 00:27:53.363 
Epoch 228/1000 
	 loss: 385.4758, MinusLogProbMetric: 385.4758, val_loss: 396.9141, val_MinusLogProbMetric: 396.9141

Epoch 228: val_loss did not improve from 396.64621
196/196 - 21s - loss: 385.4758 - MinusLogProbMetric: 385.4758 - val_loss: 396.9141 - val_MinusLogProbMetric: 396.9141 - lr: 2.7778e-05 - 21s/epoch - 108ms/step
Epoch 229/1000
2023-09-10 00:28:15.101 
Epoch 229/1000 
	 loss: 385.4967, MinusLogProbMetric: 385.4967, val_loss: 397.1496, val_MinusLogProbMetric: 397.1496

Epoch 229: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.4967 - MinusLogProbMetric: 385.4967 - val_loss: 397.1496 - val_MinusLogProbMetric: 397.1496 - lr: 2.7778e-05 - 22s/epoch - 111ms/step
Epoch 230/1000
2023-09-10 00:28:37.289 
Epoch 230/1000 
	 loss: 385.4174, MinusLogProbMetric: 385.4174, val_loss: 397.1088, val_MinusLogProbMetric: 397.1088

Epoch 230: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.4174 - MinusLogProbMetric: 385.4174 - val_loss: 397.1088 - val_MinusLogProbMetric: 397.1088 - lr: 2.7778e-05 - 22s/epoch - 113ms/step
Epoch 231/1000
2023-09-10 00:29:00.655 
Epoch 231/1000 
	 loss: 385.5742, MinusLogProbMetric: 385.5742, val_loss: 397.0955, val_MinusLogProbMetric: 397.0955

Epoch 231: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.5742 - MinusLogProbMetric: 385.5742 - val_loss: 397.0955 - val_MinusLogProbMetric: 397.0955 - lr: 2.7778e-05 - 23s/epoch - 119ms/step
Epoch 232/1000
2023-09-10 00:29:23.121 
Epoch 232/1000 
	 loss: 385.6895, MinusLogProbMetric: 385.6895, val_loss: 397.1235, val_MinusLogProbMetric: 397.1235

Epoch 232: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.6895 - MinusLogProbMetric: 385.6895 - val_loss: 397.1235 - val_MinusLogProbMetric: 397.1235 - lr: 2.7778e-05 - 22s/epoch - 114ms/step
Epoch 233/1000
2023-09-10 00:29:45.785 
Epoch 233/1000 
	 loss: 385.4166, MinusLogProbMetric: 385.4166, val_loss: 397.0325, val_MinusLogProbMetric: 397.0325

Epoch 233: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.4166 - MinusLogProbMetric: 385.4166 - val_loss: 397.0325 - val_MinusLogProbMetric: 397.0325 - lr: 2.7778e-05 - 23s/epoch - 116ms/step
Epoch 234/1000
2023-09-10 00:30:07.713 
Epoch 234/1000 
	 loss: 385.4793, MinusLogProbMetric: 385.4793, val_loss: 396.9877, val_MinusLogProbMetric: 396.9877

Epoch 234: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.4793 - MinusLogProbMetric: 385.4793 - val_loss: 396.9877 - val_MinusLogProbMetric: 396.9877 - lr: 2.7778e-05 - 22s/epoch - 112ms/step
Epoch 235/1000
2023-09-10 00:30:30.403 
Epoch 235/1000 
	 loss: 385.3276, MinusLogProbMetric: 385.3276, val_loss: 397.3272, val_MinusLogProbMetric: 397.3272

Epoch 235: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.3276 - MinusLogProbMetric: 385.3276 - val_loss: 397.3272 - val_MinusLogProbMetric: 397.3272 - lr: 2.7778e-05 - 23s/epoch - 116ms/step
Epoch 236/1000
2023-09-10 00:30:52.988 
Epoch 236/1000 
	 loss: 385.5775, MinusLogProbMetric: 385.5775, val_loss: 397.2550, val_MinusLogProbMetric: 397.2550

Epoch 236: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.5775 - MinusLogProbMetric: 385.5775 - val_loss: 397.2550 - val_MinusLogProbMetric: 397.2550 - lr: 2.7778e-05 - 23s/epoch - 115ms/step
Epoch 237/1000
2023-09-10 00:31:16.209 
Epoch 237/1000 
	 loss: 385.9814, MinusLogProbMetric: 385.9814, val_loss: 396.9001, val_MinusLogProbMetric: 396.9001

Epoch 237: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.9814 - MinusLogProbMetric: 385.9814 - val_loss: 396.9001 - val_MinusLogProbMetric: 396.9001 - lr: 2.7778e-05 - 23s/epoch - 119ms/step
Epoch 238/1000
2023-09-10 00:31:40.028 
Epoch 238/1000 
	 loss: 385.4154, MinusLogProbMetric: 385.4154, val_loss: 397.6029, val_MinusLogProbMetric: 397.6029

Epoch 238: val_loss did not improve from 396.64621
196/196 - 24s - loss: 385.4154 - MinusLogProbMetric: 385.4154 - val_loss: 397.6029 - val_MinusLogProbMetric: 397.6029 - lr: 2.7778e-05 - 24s/epoch - 122ms/step
Epoch 239/1000
2023-09-10 00:32:02.915 
Epoch 239/1000 
	 loss: 385.4176, MinusLogProbMetric: 385.4176, val_loss: 396.9853, val_MinusLogProbMetric: 396.9853

Epoch 239: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.4176 - MinusLogProbMetric: 385.4176 - val_loss: 396.9853 - val_MinusLogProbMetric: 396.9853 - lr: 2.7778e-05 - 23s/epoch - 117ms/step
Epoch 240/1000
2023-09-10 00:32:25.268 
Epoch 240/1000 
	 loss: 385.4445, MinusLogProbMetric: 385.4445, val_loss: 397.1754, val_MinusLogProbMetric: 397.1754

Epoch 240: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.4445 - MinusLogProbMetric: 385.4445 - val_loss: 397.1754 - val_MinusLogProbMetric: 397.1754 - lr: 2.7778e-05 - 22s/epoch - 114ms/step
Epoch 241/1000
2023-09-10 00:32:46.397 
Epoch 241/1000 
	 loss: 385.3855, MinusLogProbMetric: 385.3855, val_loss: 398.0248, val_MinusLogProbMetric: 398.0248

Epoch 241: val_loss did not improve from 396.64621
196/196 - 21s - loss: 385.3855 - MinusLogProbMetric: 385.3855 - val_loss: 398.0248 - val_MinusLogProbMetric: 398.0248 - lr: 2.7778e-05 - 21s/epoch - 108ms/step
Epoch 242/1000
2023-09-10 00:33:07.095 
Epoch 242/1000 
	 loss: 385.5411, MinusLogProbMetric: 385.5411, val_loss: 396.6961, val_MinusLogProbMetric: 396.6961

Epoch 242: val_loss did not improve from 396.64621
196/196 - 21s - loss: 385.5411 - MinusLogProbMetric: 385.5411 - val_loss: 396.6961 - val_MinusLogProbMetric: 396.6961 - lr: 2.7778e-05 - 21s/epoch - 106ms/step
Epoch 243/1000
2023-09-10 00:33:31.338 
Epoch 243/1000 
	 loss: 385.3807, MinusLogProbMetric: 385.3807, val_loss: 397.4338, val_MinusLogProbMetric: 397.4338

Epoch 243: val_loss did not improve from 396.64621
196/196 - 24s - loss: 385.3807 - MinusLogProbMetric: 385.3807 - val_loss: 397.4338 - val_MinusLogProbMetric: 397.4338 - lr: 2.7778e-05 - 24s/epoch - 124ms/step
Epoch 244/1000
2023-09-10 00:33:54.144 
Epoch 244/1000 
	 loss: 385.4247, MinusLogProbMetric: 385.4247, val_loss: 397.2954, val_MinusLogProbMetric: 397.2954

Epoch 244: val_loss did not improve from 396.64621
196/196 - 23s - loss: 385.4247 - MinusLogProbMetric: 385.4247 - val_loss: 397.2954 - val_MinusLogProbMetric: 397.2954 - lr: 2.7778e-05 - 23s/epoch - 116ms/step
Epoch 245/1000
2023-09-10 00:34:16.315 
Epoch 245/1000 
	 loss: 385.2346, MinusLogProbMetric: 385.2346, val_loss: 397.1927, val_MinusLogProbMetric: 397.1927

Epoch 245: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.2346 - MinusLogProbMetric: 385.2346 - val_loss: 397.1927 - val_MinusLogProbMetric: 397.1927 - lr: 2.7778e-05 - 22s/epoch - 113ms/step
Epoch 246/1000
2023-09-10 00:34:38.516 
Epoch 246/1000 
	 loss: 385.4458, MinusLogProbMetric: 385.4458, val_loss: 397.0160, val_MinusLogProbMetric: 397.0160

Epoch 246: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.4458 - MinusLogProbMetric: 385.4458 - val_loss: 397.0160 - val_MinusLogProbMetric: 397.0160 - lr: 2.7778e-05 - 22s/epoch - 113ms/step
Epoch 247/1000
2023-09-10 00:35:00.950 
Epoch 247/1000 
	 loss: 385.6416, MinusLogProbMetric: 385.6416, val_loss: 397.7258, val_MinusLogProbMetric: 397.7258

Epoch 247: val_loss did not improve from 396.64621
196/196 - 22s - loss: 385.6416 - MinusLogProbMetric: 385.6416 - val_loss: 397.7258 - val_MinusLogProbMetric: 397.7258 - lr: 2.7778e-05 - 22s/epoch - 114ms/step
Epoch 248/1000
2023-09-10 00:35:23.270 
Epoch 248/1000 
	 loss: 384.6288, MinusLogProbMetric: 384.6288, val_loss: 396.6975, val_MinusLogProbMetric: 396.6975

Epoch 248: val_loss did not improve from 396.64621
196/196 - 22s - loss: 384.6288 - MinusLogProbMetric: 384.6288 - val_loss: 396.6975 - val_MinusLogProbMetric: 396.6975 - lr: 1.3889e-05 - 22s/epoch - 114ms/step
Epoch 249/1000
2023-09-10 00:35:45.953 
Epoch 249/1000 
	 loss: 384.5471, MinusLogProbMetric: 384.5471, val_loss: 396.6460, val_MinusLogProbMetric: 396.6460

Epoch 249: val_loss improved from 396.64621 to 396.64600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_328/weights/best_weights.h5
196/196 - 24s - loss: 384.5471 - MinusLogProbMetric: 384.5471 - val_loss: 396.6460 - val_MinusLogProbMetric: 396.6460 - lr: 1.3889e-05 - 24s/epoch - 122ms/step
Epoch 250/1000
2023-09-10 00:36:09.500 
Epoch 250/1000 
	 loss: 384.6164, MinusLogProbMetric: 384.6164, val_loss: 396.9405, val_MinusLogProbMetric: 396.9405

Epoch 250: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.6164 - MinusLogProbMetric: 384.6164 - val_loss: 396.9405 - val_MinusLogProbMetric: 396.9405 - lr: 1.3889e-05 - 22s/epoch - 114ms/step
Epoch 251/1000
2023-09-10 00:36:31.982 
Epoch 251/1000 
	 loss: 384.5551, MinusLogProbMetric: 384.5551, val_loss: 396.7975, val_MinusLogProbMetric: 396.7975

Epoch 251: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5551 - MinusLogProbMetric: 384.5551 - val_loss: 396.7975 - val_MinusLogProbMetric: 396.7975 - lr: 1.3889e-05 - 22s/epoch - 115ms/step
Epoch 252/1000
2023-09-10 00:36:53.686 
Epoch 252/1000 
	 loss: 384.5386, MinusLogProbMetric: 384.5386, val_loss: 396.8434, val_MinusLogProbMetric: 396.8434

Epoch 252: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5386 - MinusLogProbMetric: 384.5386 - val_loss: 396.8434 - val_MinusLogProbMetric: 396.8434 - lr: 1.3889e-05 - 22s/epoch - 111ms/step
Epoch 253/1000
2023-09-10 00:37:17.331 
Epoch 253/1000 
	 loss: 384.5981, MinusLogProbMetric: 384.5981, val_loss: 396.9294, val_MinusLogProbMetric: 396.9294

Epoch 253: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.5981 - MinusLogProbMetric: 384.5981 - val_loss: 396.9294 - val_MinusLogProbMetric: 396.9294 - lr: 1.3889e-05 - 24s/epoch - 121ms/step
Epoch 254/1000
2023-09-10 00:37:41.525 
Epoch 254/1000 
	 loss: 384.6471, MinusLogProbMetric: 384.6471, val_loss: 396.7379, val_MinusLogProbMetric: 396.7379

Epoch 254: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.6471 - MinusLogProbMetric: 384.6471 - val_loss: 396.7379 - val_MinusLogProbMetric: 396.7379 - lr: 1.3889e-05 - 24s/epoch - 123ms/step
Epoch 255/1000
2023-09-10 00:38:04.949 
Epoch 255/1000 
	 loss: 384.6090, MinusLogProbMetric: 384.6090, val_loss: 396.8485, val_MinusLogProbMetric: 396.8485

Epoch 255: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.6090 - MinusLogProbMetric: 384.6090 - val_loss: 396.8485 - val_MinusLogProbMetric: 396.8485 - lr: 1.3889e-05 - 23s/epoch - 119ms/step
Epoch 256/1000
2023-09-10 00:38:27.226 
Epoch 256/1000 
	 loss: 384.5834, MinusLogProbMetric: 384.5834, val_loss: 396.8609, val_MinusLogProbMetric: 396.8609

Epoch 256: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5834 - MinusLogProbMetric: 384.5834 - val_loss: 396.8609 - val_MinusLogProbMetric: 396.8609 - lr: 1.3889e-05 - 22s/epoch - 113ms/step
Epoch 257/1000
2023-09-10 00:38:51.340 
Epoch 257/1000 
	 loss: 384.6285, MinusLogProbMetric: 384.6285, val_loss: 396.7533, val_MinusLogProbMetric: 396.7533

Epoch 257: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.6285 - MinusLogProbMetric: 384.6285 - val_loss: 396.7533 - val_MinusLogProbMetric: 396.7533 - lr: 1.3889e-05 - 24s/epoch - 123ms/step
Epoch 258/1000
2023-09-10 00:39:13.795 
Epoch 258/1000 
	 loss: 384.4870, MinusLogProbMetric: 384.4870, val_loss: 396.9032, val_MinusLogProbMetric: 396.9032

Epoch 258: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.4870 - MinusLogProbMetric: 384.4870 - val_loss: 396.9032 - val_MinusLogProbMetric: 396.9032 - lr: 1.3889e-05 - 22s/epoch - 115ms/step
Epoch 259/1000
2023-09-10 00:39:36.516 
Epoch 259/1000 
	 loss: 384.5027, MinusLogProbMetric: 384.5027, val_loss: 396.9813, val_MinusLogProbMetric: 396.9813

Epoch 259: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5027 - MinusLogProbMetric: 384.5027 - val_loss: 396.9813 - val_MinusLogProbMetric: 396.9813 - lr: 1.3889e-05 - 23s/epoch - 116ms/step
Epoch 260/1000
2023-09-10 00:39:59.431 
Epoch 260/1000 
	 loss: 384.5304, MinusLogProbMetric: 384.5304, val_loss: 396.7477, val_MinusLogProbMetric: 396.7477

Epoch 260: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5304 - MinusLogProbMetric: 384.5304 - val_loss: 396.7477 - val_MinusLogProbMetric: 396.7477 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 261/1000
2023-09-10 00:40:21.730 
Epoch 261/1000 
	 loss: 384.4929, MinusLogProbMetric: 384.4929, val_loss: 396.8289, val_MinusLogProbMetric: 396.8289

Epoch 261: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.4929 - MinusLogProbMetric: 384.4929 - val_loss: 396.8289 - val_MinusLogProbMetric: 396.8289 - lr: 1.3889e-05 - 22s/epoch - 114ms/step
Epoch 262/1000
2023-09-10 00:40:44.487 
Epoch 262/1000 
	 loss: 384.4731, MinusLogProbMetric: 384.4731, val_loss: 396.9584, val_MinusLogProbMetric: 396.9584

Epoch 262: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4731 - MinusLogProbMetric: 384.4731 - val_loss: 396.9584 - val_MinusLogProbMetric: 396.9584 - lr: 1.3889e-05 - 23s/epoch - 116ms/step
Epoch 263/1000
2023-09-10 00:41:07.974 
Epoch 263/1000 
	 loss: 384.5007, MinusLogProbMetric: 384.5007, val_loss: 397.2913, val_MinusLogProbMetric: 397.2913

Epoch 263: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5007 - MinusLogProbMetric: 384.5007 - val_loss: 397.2913 - val_MinusLogProbMetric: 397.2913 - lr: 1.3889e-05 - 23s/epoch - 120ms/step
Epoch 264/1000
2023-09-10 00:41:30.970 
Epoch 264/1000 
	 loss: 384.5017, MinusLogProbMetric: 384.5017, val_loss: 396.7493, val_MinusLogProbMetric: 396.7493

Epoch 264: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5017 - MinusLogProbMetric: 384.5017 - val_loss: 396.7493 - val_MinusLogProbMetric: 396.7493 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 265/1000
2023-09-10 00:41:54.030 
Epoch 265/1000 
	 loss: 384.4305, MinusLogProbMetric: 384.4305, val_loss: 397.0768, val_MinusLogProbMetric: 397.0768

Epoch 265: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4305 - MinusLogProbMetric: 384.4305 - val_loss: 397.0768 - val_MinusLogProbMetric: 397.0768 - lr: 1.3889e-05 - 23s/epoch - 118ms/step
Epoch 266/1000
2023-09-10 00:42:16.116 
Epoch 266/1000 
	 loss: 384.5098, MinusLogProbMetric: 384.5098, val_loss: 396.7618, val_MinusLogProbMetric: 396.7618

Epoch 266: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5098 - MinusLogProbMetric: 384.5098 - val_loss: 396.7618 - val_MinusLogProbMetric: 396.7618 - lr: 1.3889e-05 - 22s/epoch - 113ms/step
Epoch 267/1000
2023-09-10 00:42:39.889 
Epoch 267/1000 
	 loss: 384.4598, MinusLogProbMetric: 384.4598, val_loss: 396.8914, val_MinusLogProbMetric: 396.8914

Epoch 267: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.4598 - MinusLogProbMetric: 384.4598 - val_loss: 396.8914 - val_MinusLogProbMetric: 396.8914 - lr: 1.3889e-05 - 24s/epoch - 121ms/step
Epoch 268/1000
2023-09-10 00:43:02.322 
Epoch 268/1000 
	 loss: 384.4939, MinusLogProbMetric: 384.4939, val_loss: 397.0304, val_MinusLogProbMetric: 397.0304

Epoch 268: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.4939 - MinusLogProbMetric: 384.4939 - val_loss: 397.0304 - val_MinusLogProbMetric: 397.0304 - lr: 1.3889e-05 - 22s/epoch - 114ms/step
Epoch 269/1000
2023-09-10 00:43:25.049 
Epoch 269/1000 
	 loss: 384.4615, MinusLogProbMetric: 384.4615, val_loss: 396.8283, val_MinusLogProbMetric: 396.8283

Epoch 269: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4615 - MinusLogProbMetric: 384.4615 - val_loss: 396.8283 - val_MinusLogProbMetric: 396.8283 - lr: 1.3889e-05 - 23s/epoch - 116ms/step
Epoch 270/1000
2023-09-10 00:43:47.678 
Epoch 270/1000 
	 loss: 384.5430, MinusLogProbMetric: 384.5430, val_loss: 397.1888, val_MinusLogProbMetric: 397.1888

Epoch 270: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5430 - MinusLogProbMetric: 384.5430 - val_loss: 397.1888 - val_MinusLogProbMetric: 397.1888 - lr: 1.3889e-05 - 23s/epoch - 115ms/step
Epoch 271/1000
2023-09-10 00:44:08.935 
Epoch 271/1000 
	 loss: 384.4687, MinusLogProbMetric: 384.4687, val_loss: 396.9225, val_MinusLogProbMetric: 396.9225

Epoch 271: val_loss did not improve from 396.64600
196/196 - 21s - loss: 384.4687 - MinusLogProbMetric: 384.4687 - val_loss: 396.9225 - val_MinusLogProbMetric: 396.9225 - lr: 1.3889e-05 - 21s/epoch - 108ms/step
Epoch 272/1000
2023-09-10 00:44:30.809 
Epoch 272/1000 
	 loss: 384.5735, MinusLogProbMetric: 384.5735, val_loss: 396.8133, val_MinusLogProbMetric: 396.8133

Epoch 272: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5735 - MinusLogProbMetric: 384.5735 - val_loss: 396.8133 - val_MinusLogProbMetric: 396.8133 - lr: 1.3889e-05 - 22s/epoch - 112ms/step
Epoch 273/1000
2023-09-10 00:44:53.687 
Epoch 273/1000 
	 loss: 384.4972, MinusLogProbMetric: 384.4972, val_loss: 396.8718, val_MinusLogProbMetric: 396.8718

Epoch 273: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4972 - MinusLogProbMetric: 384.4972 - val_loss: 396.8718 - val_MinusLogProbMetric: 396.8718 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 274/1000
2023-09-10 00:45:16.690 
Epoch 274/1000 
	 loss: 384.4443, MinusLogProbMetric: 384.4443, val_loss: 396.9438, val_MinusLogProbMetric: 396.9438

Epoch 274: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4443 - MinusLogProbMetric: 384.4443 - val_loss: 396.9438 - val_MinusLogProbMetric: 396.9438 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 275/1000
2023-09-10 00:45:39.610 
Epoch 275/1000 
	 loss: 384.5783, MinusLogProbMetric: 384.5783, val_loss: 397.0361, val_MinusLogProbMetric: 397.0361

Epoch 275: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5783 - MinusLogProbMetric: 384.5783 - val_loss: 397.0361 - val_MinusLogProbMetric: 397.0361 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 276/1000
2023-09-10 00:46:01.932 
Epoch 276/1000 
	 loss: 384.5452, MinusLogProbMetric: 384.5452, val_loss: 398.1276, val_MinusLogProbMetric: 398.1276

Epoch 276: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5452 - MinusLogProbMetric: 384.5452 - val_loss: 398.1276 - val_MinusLogProbMetric: 398.1276 - lr: 1.3889e-05 - 22s/epoch - 114ms/step
Epoch 277/1000
2023-09-10 00:46:24.757 
Epoch 277/1000 
	 loss: 384.5782, MinusLogProbMetric: 384.5782, val_loss: 397.0667, val_MinusLogProbMetric: 397.0667

Epoch 277: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.5782 - MinusLogProbMetric: 384.5782 - val_loss: 397.0667 - val_MinusLogProbMetric: 397.0667 - lr: 1.3889e-05 - 23s/epoch - 116ms/step
Epoch 278/1000
2023-09-10 00:46:47.602 
Epoch 278/1000 
	 loss: 384.4427, MinusLogProbMetric: 384.4427, val_loss: 396.9683, val_MinusLogProbMetric: 396.9683

Epoch 278: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4427 - MinusLogProbMetric: 384.4427 - val_loss: 396.9683 - val_MinusLogProbMetric: 396.9683 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 279/1000
2023-09-10 00:47:09.835 
Epoch 279/1000 
	 loss: 384.4445, MinusLogProbMetric: 384.4445, val_loss: 396.8763, val_MinusLogProbMetric: 396.8763

Epoch 279: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.4445 - MinusLogProbMetric: 384.4445 - val_loss: 396.8763 - val_MinusLogProbMetric: 396.8763 - lr: 1.3889e-05 - 22s/epoch - 113ms/step
Epoch 280/1000
2023-09-10 00:47:33.250 
Epoch 280/1000 
	 loss: 384.4106, MinusLogProbMetric: 384.4106, val_loss: 396.8857, val_MinusLogProbMetric: 396.8857

Epoch 280: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4106 - MinusLogProbMetric: 384.4106 - val_loss: 396.8857 - val_MinusLogProbMetric: 396.8857 - lr: 1.3889e-05 - 23s/epoch - 119ms/step
Epoch 281/1000
2023-09-10 00:47:55.753 
Epoch 281/1000 
	 loss: 384.4254, MinusLogProbMetric: 384.4254, val_loss: 396.8039, val_MinusLogProbMetric: 396.8039

Epoch 281: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.4254 - MinusLogProbMetric: 384.4254 - val_loss: 396.8039 - val_MinusLogProbMetric: 396.8039 - lr: 1.3889e-05 - 22s/epoch - 115ms/step
Epoch 282/1000
2023-09-10 00:48:18.582 
Epoch 282/1000 
	 loss: 384.6018, MinusLogProbMetric: 384.6018, val_loss: 397.1946, val_MinusLogProbMetric: 397.1946

Epoch 282: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.6018 - MinusLogProbMetric: 384.6018 - val_loss: 397.1946 - val_MinusLogProbMetric: 397.1946 - lr: 1.3889e-05 - 23s/epoch - 116ms/step
Epoch 283/1000
2023-09-10 00:48:39.943 
Epoch 283/1000 
	 loss: 384.4594, MinusLogProbMetric: 384.4594, val_loss: 397.0949, val_MinusLogProbMetric: 397.0949

Epoch 283: val_loss did not improve from 396.64600
196/196 - 21s - loss: 384.4594 - MinusLogProbMetric: 384.4594 - val_loss: 397.0949 - val_MinusLogProbMetric: 397.0949 - lr: 1.3889e-05 - 21s/epoch - 109ms/step
Epoch 284/1000
2023-09-10 00:49:02.109 
Epoch 284/1000 
	 loss: 384.4424, MinusLogProbMetric: 384.4424, val_loss: 397.4617, val_MinusLogProbMetric: 397.4617

Epoch 284: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.4424 - MinusLogProbMetric: 384.4424 - val_loss: 397.4617 - val_MinusLogProbMetric: 397.4617 - lr: 1.3889e-05 - 22s/epoch - 113ms/step
Epoch 285/1000
2023-09-10 00:49:24.669 
Epoch 285/1000 
	 loss: 384.4833, MinusLogProbMetric: 384.4833, val_loss: 396.9553, val_MinusLogProbMetric: 396.9553

Epoch 285: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4833 - MinusLogProbMetric: 384.4833 - val_loss: 396.9553 - val_MinusLogProbMetric: 396.9553 - lr: 1.3889e-05 - 23s/epoch - 115ms/step
Epoch 286/1000
2023-09-10 00:49:47.224 
Epoch 286/1000 
	 loss: 384.4805, MinusLogProbMetric: 384.4805, val_loss: 396.8667, val_MinusLogProbMetric: 396.8667

Epoch 286: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4805 - MinusLogProbMetric: 384.4805 - val_loss: 396.8667 - val_MinusLogProbMetric: 396.8667 - lr: 1.3889e-05 - 23s/epoch - 115ms/step
Epoch 287/1000
2023-09-10 00:50:11.194 
Epoch 287/1000 
	 loss: 384.3891, MinusLogProbMetric: 384.3891, val_loss: 396.8496, val_MinusLogProbMetric: 396.8496

Epoch 287: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.3891 - MinusLogProbMetric: 384.3891 - val_loss: 396.8496 - val_MinusLogProbMetric: 396.8496 - lr: 1.3889e-05 - 24s/epoch - 122ms/step
Epoch 288/1000
2023-09-10 00:50:35.752 
Epoch 288/1000 
	 loss: 384.4116, MinusLogProbMetric: 384.4116, val_loss: 397.0999, val_MinusLogProbMetric: 397.0999

Epoch 288: val_loss did not improve from 396.64600
196/196 - 25s - loss: 384.4116 - MinusLogProbMetric: 384.4116 - val_loss: 397.0999 - val_MinusLogProbMetric: 397.0999 - lr: 1.3889e-05 - 25s/epoch - 125ms/step
Epoch 289/1000
2023-09-10 00:50:59.847 
Epoch 289/1000 
	 loss: 384.4905, MinusLogProbMetric: 384.4905, val_loss: 397.0448, val_MinusLogProbMetric: 397.0448

Epoch 289: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.4905 - MinusLogProbMetric: 384.4905 - val_loss: 397.0448 - val_MinusLogProbMetric: 397.0448 - lr: 1.3889e-05 - 24s/epoch - 123ms/step
Epoch 290/1000
2023-09-10 00:51:21.454 
Epoch 290/1000 
	 loss: 384.5271, MinusLogProbMetric: 384.5271, val_loss: 396.9396, val_MinusLogProbMetric: 396.9396

Epoch 290: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.5271 - MinusLogProbMetric: 384.5271 - val_loss: 396.9396 - val_MinusLogProbMetric: 396.9396 - lr: 1.3889e-05 - 22s/epoch - 110ms/step
Epoch 291/1000
2023-09-10 00:51:45.624 
Epoch 291/1000 
	 loss: 384.3456, MinusLogProbMetric: 384.3456, val_loss: 397.0846, val_MinusLogProbMetric: 397.0846

Epoch 291: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.3456 - MinusLogProbMetric: 384.3456 - val_loss: 397.0846 - val_MinusLogProbMetric: 397.0846 - lr: 1.3889e-05 - 24s/epoch - 123ms/step
Epoch 292/1000
2023-09-10 00:52:08.605 
Epoch 292/1000 
	 loss: 384.3817, MinusLogProbMetric: 384.3817, val_loss: 397.0642, val_MinusLogProbMetric: 397.0642

Epoch 292: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.3817 - MinusLogProbMetric: 384.3817 - val_loss: 397.0642 - val_MinusLogProbMetric: 397.0642 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 293/1000
2023-09-10 00:52:32.130 
Epoch 293/1000 
	 loss: 384.3098, MinusLogProbMetric: 384.3098, val_loss: 396.9636, val_MinusLogProbMetric: 396.9636

Epoch 293: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.3098 - MinusLogProbMetric: 384.3098 - val_loss: 396.9636 - val_MinusLogProbMetric: 396.9636 - lr: 1.3889e-05 - 24s/epoch - 120ms/step
Epoch 294/1000
2023-09-10 00:52:54.729 
Epoch 294/1000 
	 loss: 384.4127, MinusLogProbMetric: 384.4127, val_loss: 397.0711, val_MinusLogProbMetric: 397.0711

Epoch 294: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.4127 - MinusLogProbMetric: 384.4127 - val_loss: 397.0711 - val_MinusLogProbMetric: 397.0711 - lr: 1.3889e-05 - 23s/epoch - 115ms/step
Epoch 295/1000
2023-09-10 00:53:17.381 
Epoch 295/1000 
	 loss: 384.3798, MinusLogProbMetric: 384.3798, val_loss: 397.0019, val_MinusLogProbMetric: 397.0019

Epoch 295: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.3798 - MinusLogProbMetric: 384.3798 - val_loss: 397.0019 - val_MinusLogProbMetric: 397.0019 - lr: 1.3889e-05 - 23s/epoch - 116ms/step
Epoch 296/1000
2023-09-10 00:53:40.526 
Epoch 296/1000 
	 loss: 384.3802, MinusLogProbMetric: 384.3802, val_loss: 397.1847, val_MinusLogProbMetric: 397.1847

Epoch 296: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.3802 - MinusLogProbMetric: 384.3802 - val_loss: 397.1847 - val_MinusLogProbMetric: 397.1847 - lr: 1.3889e-05 - 23s/epoch - 118ms/step
Epoch 297/1000
2023-09-10 00:54:03.967 
Epoch 297/1000 
	 loss: 384.3407, MinusLogProbMetric: 384.3407, val_loss: 397.1116, val_MinusLogProbMetric: 397.1116

Epoch 297: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.3407 - MinusLogProbMetric: 384.3407 - val_loss: 397.1116 - val_MinusLogProbMetric: 397.1116 - lr: 1.3889e-05 - 23s/epoch - 120ms/step
Epoch 298/1000
2023-09-10 00:54:27.035 
Epoch 298/1000 
	 loss: 384.3036, MinusLogProbMetric: 384.3036, val_loss: 396.8799, val_MinusLogProbMetric: 396.8799

Epoch 298: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.3036 - MinusLogProbMetric: 384.3036 - val_loss: 396.8799 - val_MinusLogProbMetric: 396.8799 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 299/1000
2023-09-10 00:54:49.908 
Epoch 299/1000 
	 loss: 384.3242, MinusLogProbMetric: 384.3242, val_loss: 396.9105, val_MinusLogProbMetric: 396.9105

Epoch 299: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.3242 - MinusLogProbMetric: 384.3242 - val_loss: 396.9105 - val_MinusLogProbMetric: 396.9105 - lr: 1.3889e-05 - 23s/epoch - 117ms/step
Epoch 300/1000
2023-09-10 00:55:12.365 
Epoch 300/1000 
	 loss: 384.0273, MinusLogProbMetric: 384.0273, val_loss: 396.8544, val_MinusLogProbMetric: 396.8544

Epoch 300: val_loss did not improve from 396.64600
196/196 - 22s - loss: 384.0273 - MinusLogProbMetric: 384.0273 - val_loss: 396.8544 - val_MinusLogProbMetric: 396.8544 - lr: 6.9444e-06 - 22s/epoch - 114ms/step
Epoch 301/1000
2023-09-10 00:55:36.240 
Epoch 301/1000 
	 loss: 384.0028, MinusLogProbMetric: 384.0028, val_loss: 396.9422, val_MinusLogProbMetric: 396.9422

Epoch 301: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.0028 - MinusLogProbMetric: 384.0028 - val_loss: 396.9422 - val_MinusLogProbMetric: 396.9422 - lr: 6.9444e-06 - 24s/epoch - 122ms/step
Epoch 302/1000
2023-09-10 00:55:59.893 
Epoch 302/1000 
	 loss: 384.0089, MinusLogProbMetric: 384.0089, val_loss: 397.1067, val_MinusLogProbMetric: 397.1067

Epoch 302: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.0089 - MinusLogProbMetric: 384.0089 - val_loss: 397.1067 - val_MinusLogProbMetric: 397.1067 - lr: 6.9444e-06 - 24s/epoch - 121ms/step
Epoch 303/1000
2023-09-10 00:56:23.892 
Epoch 303/1000 
	 loss: 383.9892, MinusLogProbMetric: 383.9892, val_loss: 397.0782, val_MinusLogProbMetric: 397.0782

Epoch 303: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9892 - MinusLogProbMetric: 383.9892 - val_loss: 397.0782 - val_MinusLogProbMetric: 397.0782 - lr: 6.9444e-06 - 24s/epoch - 122ms/step
Epoch 304/1000
2023-09-10 00:56:48.465 
Epoch 304/1000 
	 loss: 383.9910, MinusLogProbMetric: 383.9910, val_loss: 396.8786, val_MinusLogProbMetric: 396.8786

Epoch 304: val_loss did not improve from 396.64600
196/196 - 25s - loss: 383.9910 - MinusLogProbMetric: 383.9910 - val_loss: 396.8786 - val_MinusLogProbMetric: 396.8786 - lr: 6.9444e-06 - 25s/epoch - 125ms/step
Epoch 305/1000
2023-09-10 00:57:11.303 
Epoch 305/1000 
	 loss: 384.0145, MinusLogProbMetric: 384.0145, val_loss: 396.9116, val_MinusLogProbMetric: 396.9116

Epoch 305: val_loss did not improve from 396.64600
196/196 - 23s - loss: 384.0145 - MinusLogProbMetric: 384.0145 - val_loss: 396.9116 - val_MinusLogProbMetric: 396.9116 - lr: 6.9444e-06 - 23s/epoch - 117ms/step
Epoch 306/1000
2023-09-10 00:57:35.223 
Epoch 306/1000 
	 loss: 384.0048, MinusLogProbMetric: 384.0048, val_loss: 396.9194, val_MinusLogProbMetric: 396.9194

Epoch 306: val_loss did not improve from 396.64600
196/196 - 24s - loss: 384.0048 - MinusLogProbMetric: 384.0048 - val_loss: 396.9194 - val_MinusLogProbMetric: 396.9194 - lr: 6.9444e-06 - 24s/epoch - 122ms/step
Epoch 307/1000
2023-09-10 00:57:58.984 
Epoch 307/1000 
	 loss: 383.9909, MinusLogProbMetric: 383.9909, val_loss: 397.1808, val_MinusLogProbMetric: 397.1808

Epoch 307: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9909 - MinusLogProbMetric: 383.9909 - val_loss: 397.1808 - val_MinusLogProbMetric: 397.1808 - lr: 6.9444e-06 - 24s/epoch - 121ms/step
Epoch 308/1000
2023-09-10 00:58:22.041 
Epoch 308/1000 
	 loss: 383.9904, MinusLogProbMetric: 383.9904, val_loss: 397.0279, val_MinusLogProbMetric: 397.0279

Epoch 308: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9904 - MinusLogProbMetric: 383.9904 - val_loss: 397.0279 - val_MinusLogProbMetric: 397.0279 - lr: 6.9444e-06 - 23s/epoch - 118ms/step
Epoch 309/1000
2023-09-10 00:58:46.860 
Epoch 309/1000 
	 loss: 383.9954, MinusLogProbMetric: 383.9954, val_loss: 396.9388, val_MinusLogProbMetric: 396.9388

Epoch 309: val_loss did not improve from 396.64600
196/196 - 25s - loss: 383.9954 - MinusLogProbMetric: 383.9954 - val_loss: 396.9388 - val_MinusLogProbMetric: 396.9388 - lr: 6.9444e-06 - 25s/epoch - 127ms/step
Epoch 310/1000
2023-09-10 00:59:10.280 
Epoch 310/1000 
	 loss: 383.9682, MinusLogProbMetric: 383.9682, val_loss: 397.0101, val_MinusLogProbMetric: 397.0101

Epoch 310: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9682 - MinusLogProbMetric: 383.9682 - val_loss: 397.0101 - val_MinusLogProbMetric: 397.0101 - lr: 6.9444e-06 - 23s/epoch - 119ms/step
Epoch 311/1000
2023-09-10 00:59:33.380 
Epoch 311/1000 
	 loss: 383.9858, MinusLogProbMetric: 383.9858, val_loss: 397.0149, val_MinusLogProbMetric: 397.0149

Epoch 311: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9858 - MinusLogProbMetric: 383.9858 - val_loss: 397.0149 - val_MinusLogProbMetric: 397.0149 - lr: 6.9444e-06 - 23s/epoch - 118ms/step
Epoch 312/1000
2023-09-10 00:59:55.428 
Epoch 312/1000 
	 loss: 383.9568, MinusLogProbMetric: 383.9568, val_loss: 397.0544, val_MinusLogProbMetric: 397.0544

Epoch 312: val_loss did not improve from 396.64600
196/196 - 22s - loss: 383.9568 - MinusLogProbMetric: 383.9568 - val_loss: 397.0544 - val_MinusLogProbMetric: 397.0544 - lr: 6.9444e-06 - 22s/epoch - 112ms/step
Epoch 313/1000
2023-09-10 01:00:18.930 
Epoch 313/1000 
	 loss: 383.9583, MinusLogProbMetric: 383.9583, val_loss: 396.9740, val_MinusLogProbMetric: 396.9740

Epoch 313: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9583 - MinusLogProbMetric: 383.9583 - val_loss: 396.9740 - val_MinusLogProbMetric: 396.9740 - lr: 6.9444e-06 - 24s/epoch - 120ms/step
Epoch 314/1000
2023-09-10 01:00:41.695 
Epoch 314/1000 
	 loss: 383.9611, MinusLogProbMetric: 383.9611, val_loss: 396.9622, val_MinusLogProbMetric: 396.9622

Epoch 314: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9611 - MinusLogProbMetric: 383.9611 - val_loss: 396.9622 - val_MinusLogProbMetric: 396.9622 - lr: 6.9444e-06 - 23s/epoch - 116ms/step
Epoch 315/1000
2023-09-10 01:01:05.269 
Epoch 315/1000 
	 loss: 383.9555, MinusLogProbMetric: 383.9555, val_loss: 396.8976, val_MinusLogProbMetric: 396.8976

Epoch 315: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9555 - MinusLogProbMetric: 383.9555 - val_loss: 396.8976 - val_MinusLogProbMetric: 396.8976 - lr: 6.9444e-06 - 24s/epoch - 120ms/step
Epoch 316/1000
2023-09-10 01:01:29.429 
Epoch 316/1000 
	 loss: 383.9546, MinusLogProbMetric: 383.9546, val_loss: 396.9477, val_MinusLogProbMetric: 396.9477

Epoch 316: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9546 - MinusLogProbMetric: 383.9546 - val_loss: 396.9477 - val_MinusLogProbMetric: 396.9477 - lr: 6.9444e-06 - 24s/epoch - 123ms/step
Epoch 317/1000
2023-09-10 01:01:52.136 
Epoch 317/1000 
	 loss: 383.9556, MinusLogProbMetric: 383.9556, val_loss: 397.0063, val_MinusLogProbMetric: 397.0063

Epoch 317: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9556 - MinusLogProbMetric: 383.9556 - val_loss: 397.0063 - val_MinusLogProbMetric: 397.0063 - lr: 6.9444e-06 - 23s/epoch - 116ms/step
Epoch 318/1000
2023-09-10 01:02:15.580 
Epoch 318/1000 
	 loss: 383.9551, MinusLogProbMetric: 383.9551, val_loss: 396.9217, val_MinusLogProbMetric: 396.9217

Epoch 318: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9551 - MinusLogProbMetric: 383.9551 - val_loss: 396.9217 - val_MinusLogProbMetric: 396.9217 - lr: 6.9444e-06 - 23s/epoch - 119ms/step
Epoch 319/1000
2023-09-10 01:02:39.150 
Epoch 319/1000 
	 loss: 383.9349, MinusLogProbMetric: 383.9349, val_loss: 396.9225, val_MinusLogProbMetric: 396.9225

Epoch 319: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9349 - MinusLogProbMetric: 383.9349 - val_loss: 396.9225 - val_MinusLogProbMetric: 396.9225 - lr: 6.9444e-06 - 24s/epoch - 120ms/step
Epoch 320/1000
2023-09-10 01:03:02.469 
Epoch 320/1000 
	 loss: 383.9543, MinusLogProbMetric: 383.9543, val_loss: 396.9890, val_MinusLogProbMetric: 396.9890

Epoch 320: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9543 - MinusLogProbMetric: 383.9543 - val_loss: 396.9890 - val_MinusLogProbMetric: 396.9890 - lr: 6.9444e-06 - 23s/epoch - 119ms/step
Epoch 321/1000
2023-09-10 01:03:26.087 
Epoch 321/1000 
	 loss: 383.9355, MinusLogProbMetric: 383.9355, val_loss: 396.8641, val_MinusLogProbMetric: 396.8641

Epoch 321: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9355 - MinusLogProbMetric: 383.9355 - val_loss: 396.8641 - val_MinusLogProbMetric: 396.8641 - lr: 6.9444e-06 - 24s/epoch - 120ms/step
Epoch 322/1000
2023-09-10 01:03:50.757 
Epoch 322/1000 
	 loss: 383.9332, MinusLogProbMetric: 383.9332, val_loss: 396.9019, val_MinusLogProbMetric: 396.9019

Epoch 322: val_loss did not improve from 396.64600
196/196 - 25s - loss: 383.9332 - MinusLogProbMetric: 383.9332 - val_loss: 396.9019 - val_MinusLogProbMetric: 396.9019 - lr: 6.9444e-06 - 25s/epoch - 126ms/step
Epoch 323/1000
2023-09-10 01:04:13.493 
Epoch 323/1000 
	 loss: 383.9356, MinusLogProbMetric: 383.9356, val_loss: 397.0918, val_MinusLogProbMetric: 397.0918

Epoch 323: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9356 - MinusLogProbMetric: 383.9356 - val_loss: 397.0918 - val_MinusLogProbMetric: 397.0918 - lr: 6.9444e-06 - 23s/epoch - 116ms/step
Epoch 324/1000
2023-09-10 01:04:35.760 
Epoch 324/1000 
	 loss: 383.9161, MinusLogProbMetric: 383.9161, val_loss: 397.1939, val_MinusLogProbMetric: 397.1939

Epoch 324: val_loss did not improve from 396.64600
196/196 - 22s - loss: 383.9161 - MinusLogProbMetric: 383.9161 - val_loss: 397.1939 - val_MinusLogProbMetric: 397.1939 - lr: 6.9444e-06 - 22s/epoch - 114ms/step
Epoch 325/1000
2023-09-10 01:04:59.614 
Epoch 325/1000 
	 loss: 383.9283, MinusLogProbMetric: 383.9283, val_loss: 396.9577, val_MinusLogProbMetric: 396.9577

Epoch 325: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9283 - MinusLogProbMetric: 383.9283 - val_loss: 396.9577 - val_MinusLogProbMetric: 396.9577 - lr: 6.9444e-06 - 24s/epoch - 122ms/step
Epoch 326/1000
2023-09-10 01:05:20.912 
Epoch 326/1000 
	 loss: 383.9607, MinusLogProbMetric: 383.9607, val_loss: 396.8309, val_MinusLogProbMetric: 396.8309

Epoch 326: val_loss did not improve from 396.64600
196/196 - 21s - loss: 383.9607 - MinusLogProbMetric: 383.9607 - val_loss: 396.8309 - val_MinusLogProbMetric: 396.8309 - lr: 6.9444e-06 - 21s/epoch - 109ms/step
Epoch 327/1000
2023-09-10 01:05:44.489 
Epoch 327/1000 
	 loss: 383.9023, MinusLogProbMetric: 383.9023, val_loss: 397.3099, val_MinusLogProbMetric: 397.3099

Epoch 327: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9023 - MinusLogProbMetric: 383.9023 - val_loss: 397.3099 - val_MinusLogProbMetric: 397.3099 - lr: 6.9444e-06 - 24s/epoch - 120ms/step
Epoch 328/1000
2023-09-10 01:06:08.300 
Epoch 328/1000 
	 loss: 383.9390, MinusLogProbMetric: 383.9390, val_loss: 396.9377, val_MinusLogProbMetric: 396.9377

Epoch 328: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9390 - MinusLogProbMetric: 383.9390 - val_loss: 396.9377 - val_MinusLogProbMetric: 396.9377 - lr: 6.9444e-06 - 24s/epoch - 121ms/step
Epoch 329/1000
2023-09-10 01:06:30.877 
Epoch 329/1000 
	 loss: 383.9361, MinusLogProbMetric: 383.9361, val_loss: 397.2376, val_MinusLogProbMetric: 397.2376

Epoch 329: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9361 - MinusLogProbMetric: 383.9361 - val_loss: 397.2376 - val_MinusLogProbMetric: 397.2376 - lr: 6.9444e-06 - 23s/epoch - 115ms/step
Epoch 330/1000
2023-09-10 01:06:54.229 
Epoch 330/1000 
	 loss: 383.9409, MinusLogProbMetric: 383.9409, val_loss: 396.9700, val_MinusLogProbMetric: 396.9700

Epoch 330: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9409 - MinusLogProbMetric: 383.9409 - val_loss: 396.9700 - val_MinusLogProbMetric: 396.9700 - lr: 6.9444e-06 - 23s/epoch - 119ms/step
Epoch 331/1000
2023-09-10 01:07:18.549 
Epoch 331/1000 
	 loss: 383.9164, MinusLogProbMetric: 383.9164, val_loss: 397.1265, val_MinusLogProbMetric: 397.1265

Epoch 331: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9164 - MinusLogProbMetric: 383.9164 - val_loss: 397.1265 - val_MinusLogProbMetric: 397.1265 - lr: 6.9444e-06 - 24s/epoch - 124ms/step
Epoch 332/1000
2023-09-10 01:07:41.385 
Epoch 332/1000 
	 loss: 383.9737, MinusLogProbMetric: 383.9737, val_loss: 397.0084, val_MinusLogProbMetric: 397.0084

Epoch 332: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9737 - MinusLogProbMetric: 383.9737 - val_loss: 397.0084 - val_MinusLogProbMetric: 397.0084 - lr: 6.9444e-06 - 23s/epoch - 117ms/step
Epoch 333/1000
2023-09-10 01:08:05.716 
Epoch 333/1000 
	 loss: 383.9265, MinusLogProbMetric: 383.9265, val_loss: 397.0767, val_MinusLogProbMetric: 397.0767

Epoch 333: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9265 - MinusLogProbMetric: 383.9265 - val_loss: 397.0767 - val_MinusLogProbMetric: 397.0767 - lr: 6.9444e-06 - 24s/epoch - 124ms/step
Epoch 334/1000
2023-09-10 01:08:28.720 
Epoch 334/1000 
	 loss: 383.9027, MinusLogProbMetric: 383.9027, val_loss: 397.1904, val_MinusLogProbMetric: 397.1904

Epoch 334: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9027 - MinusLogProbMetric: 383.9027 - val_loss: 397.1904 - val_MinusLogProbMetric: 397.1904 - lr: 6.9444e-06 - 23s/epoch - 117ms/step
Epoch 335/1000
2023-09-10 01:08:53.394 
Epoch 335/1000 
	 loss: 383.9105, MinusLogProbMetric: 383.9105, val_loss: 397.2059, val_MinusLogProbMetric: 397.2059

Epoch 335: val_loss did not improve from 396.64600
196/196 - 25s - loss: 383.9105 - MinusLogProbMetric: 383.9105 - val_loss: 397.2059 - val_MinusLogProbMetric: 397.2059 - lr: 6.9444e-06 - 25s/epoch - 126ms/step
Epoch 336/1000
2023-09-10 01:09:15.289 
Epoch 336/1000 
	 loss: 383.8928, MinusLogProbMetric: 383.8928, val_loss: 397.0910, val_MinusLogProbMetric: 397.0910

Epoch 336: val_loss did not improve from 396.64600
196/196 - 22s - loss: 383.8928 - MinusLogProbMetric: 383.8928 - val_loss: 397.0910 - val_MinusLogProbMetric: 397.0910 - lr: 6.9444e-06 - 22s/epoch - 112ms/step
Epoch 337/1000
2023-09-10 01:09:38.259 
Epoch 337/1000 
	 loss: 383.9057, MinusLogProbMetric: 383.9057, val_loss: 397.0255, val_MinusLogProbMetric: 397.0255

Epoch 337: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9057 - MinusLogProbMetric: 383.9057 - val_loss: 397.0255 - val_MinusLogProbMetric: 397.0255 - lr: 6.9444e-06 - 23s/epoch - 117ms/step
Epoch 338/1000
2023-09-10 01:10:02.775 
Epoch 338/1000 
	 loss: 383.9050, MinusLogProbMetric: 383.9050, val_loss: 397.0566, val_MinusLogProbMetric: 397.0566

Epoch 338: val_loss did not improve from 396.64600
196/196 - 25s - loss: 383.9050 - MinusLogProbMetric: 383.9050 - val_loss: 397.0566 - val_MinusLogProbMetric: 397.0566 - lr: 6.9444e-06 - 25s/epoch - 125ms/step
Epoch 339/1000
2023-09-10 01:10:25.632 
Epoch 339/1000 
	 loss: 383.9515, MinusLogProbMetric: 383.9515, val_loss: 397.0719, val_MinusLogProbMetric: 397.0719

Epoch 339: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9515 - MinusLogProbMetric: 383.9515 - val_loss: 397.0719 - val_MinusLogProbMetric: 397.0719 - lr: 6.9444e-06 - 23s/epoch - 117ms/step
Epoch 340/1000
2023-09-10 01:10:47.482 
Epoch 340/1000 
	 loss: 383.9240, MinusLogProbMetric: 383.9240, val_loss: 396.9631, val_MinusLogProbMetric: 396.9631

Epoch 340: val_loss did not improve from 396.64600
196/196 - 22s - loss: 383.9240 - MinusLogProbMetric: 383.9240 - val_loss: 396.9631 - val_MinusLogProbMetric: 396.9631 - lr: 6.9444e-06 - 22s/epoch - 111ms/step
Epoch 341/1000
2023-09-10 01:11:10.880 
Epoch 341/1000 
	 loss: 383.9272, MinusLogProbMetric: 383.9272, val_loss: 397.0494, val_MinusLogProbMetric: 397.0494

Epoch 341: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9272 - MinusLogProbMetric: 383.9272 - val_loss: 397.0494 - val_MinusLogProbMetric: 397.0494 - lr: 6.9444e-06 - 23s/epoch - 119ms/step
Epoch 342/1000
2023-09-10 01:11:33.605 
Epoch 342/1000 
	 loss: 383.9410, MinusLogProbMetric: 383.9410, val_loss: 396.8748, val_MinusLogProbMetric: 396.8748

Epoch 342: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9410 - MinusLogProbMetric: 383.9410 - val_loss: 396.8748 - val_MinusLogProbMetric: 396.8748 - lr: 6.9444e-06 - 23s/epoch - 116ms/step
Epoch 343/1000
2023-09-10 01:11:55.965 
Epoch 343/1000 
	 loss: 383.9044, MinusLogProbMetric: 383.9044, val_loss: 397.0086, val_MinusLogProbMetric: 397.0086

Epoch 343: val_loss did not improve from 396.64600
196/196 - 22s - loss: 383.9044 - MinusLogProbMetric: 383.9044 - val_loss: 397.0086 - val_MinusLogProbMetric: 397.0086 - lr: 6.9444e-06 - 22s/epoch - 114ms/step
Epoch 344/1000
2023-09-10 01:12:18.581 
Epoch 344/1000 
	 loss: 383.9168, MinusLogProbMetric: 383.9168, val_loss: 397.3604, val_MinusLogProbMetric: 397.3604

Epoch 344: val_loss did not improve from 396.64600
196/196 - 23s - loss: 383.9168 - MinusLogProbMetric: 383.9168 - val_loss: 397.3604 - val_MinusLogProbMetric: 397.3604 - lr: 6.9444e-06 - 23s/epoch - 115ms/step
Epoch 345/1000
2023-09-10 01:12:42.812 
Epoch 345/1000 
	 loss: 383.9091, MinusLogProbMetric: 383.9091, val_loss: 396.8822, val_MinusLogProbMetric: 396.8822

Epoch 345: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9091 - MinusLogProbMetric: 383.9091 - val_loss: 396.8822 - val_MinusLogProbMetric: 396.8822 - lr: 6.9444e-06 - 24s/epoch - 124ms/step
Epoch 346/1000
2023-09-10 01:13:06.867 
Epoch 346/1000 
	 loss: 383.9018, MinusLogProbMetric: 383.9018, val_loss: 397.1674, val_MinusLogProbMetric: 397.1674

Epoch 346: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9018 - MinusLogProbMetric: 383.9018 - val_loss: 397.1674 - val_MinusLogProbMetric: 397.1674 - lr: 6.9444e-06 - 24s/epoch - 123ms/step
Epoch 347/1000
2023-09-10 01:13:31.016 
Epoch 347/1000 
	 loss: 383.9049, MinusLogProbMetric: 383.9049, val_loss: 396.9957, val_MinusLogProbMetric: 396.9957

Epoch 347: val_loss did not improve from 396.64600
196/196 - 24s - loss: 383.9049 - MinusLogProbMetric: 383.9049 - val_loss: 396.9957 - val_MinusLogProbMetric: 396.9957 - lr: 6.9444e-06 - 24s/epoch - 123ms/step
Epoch 348/1000
2023-09-10 01:13:55.878 
Epoch 348/1000 
	 loss: 383.8782, MinusLogProbMetric: 383.8782, val_loss: 397.2660, val_MinusLogProbMetric: 397.2660

Epoch 348: val_loss did not improve from 396.64600
196/196 - 25s - loss: 383.8782 - MinusLogProbMetric: 383.8782 - val_loss: 397.2660 - val_MinusLogProbMetric: 397.2660 - lr: 6.9444e-06 - 25s/epoch - 127ms/step
Epoch 349/1000
2023-09-10 01:14:19.846 
Epoch 349/1000 
	 loss: 383.8720, MinusLogProbMetric: 383.8720, val_loss: 396.9807, val_MinusLogProbMetric: 396.9807

Epoch 349: val_loss did not improve from 396.64600
Restoring model weights from the end of the best epoch: 249.
196/196 - 25s - loss: 383.8720 - MinusLogProbMetric: 383.8720 - val_loss: 396.9807 - val_MinusLogProbMetric: 396.9807 - lr: 6.9444e-06 - 25s/epoch - 125ms/step
Epoch 349: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 3092.415368156042 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 3196.224900124944 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
SWD metric calculation completed in 3141.9624193579657 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
FN metric calculation completed in 3187.656892055995 seconds.
Training succeeded with seed 187.
Model trained in 8240.68 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 12865.74 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 470, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 12867.00 s.
===========
Run 328/360 done in 22336.25 s.
===========

Directory ../../results/MAFN_new/run_329/ already exists.
Skipping it.
===========
Run 329/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_330/ already exists.
Skipping it.
===========
Run 330/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_331/ already exists.
Skipping it.
===========
Run 331/360 already exists. Skipping it.
===========

===========
Generating train data for run 332.
===========
Train data generated in 3.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_332/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_332/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[8.229488 , 4.495837 , 5.2408857, ..., 2.4850268, 8.645066 ,
        6.947819 ],
       [6.4597087, 8.5036125, 6.4417224, ..., 9.769764 , 1.7685618,
        5.715251 ],
       [8.184484 , 4.6048226, 5.1865025, ..., 2.5379913, 8.714117 ,
        6.813641 ],
       ...,
       [8.467587 , 5.268347 , 5.1986547, ..., 3.5898209, 8.35342  ,
        6.779558 ],
       [8.0343275, 5.003659 , 5.2459483, ..., 3.075615 , 8.72342  ,
        6.6362123],
       [5.2588263, 7.861301 , 6.1851134, ..., 9.199393 , 2.3736775,
        6.457649 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_332/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_332
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.437488    7.1322474   6.533019   ...  9.559031    1.9697884
   6.253516  ]
 [ 6.105814   -0.33694893  4.861082   ...  4.728712    6.8399925
   2.9518385 ]
 [ 5.5099134   8.416618    3.6192868  ... 10.13037     0.5513004
   6.6116333 ]
 ...
 [ 8.378759    5.1539607   5.2823405  ...  4.1604185   7.9323363
   6.8563857 ]
 [ 8.088472    5.184855    5.311574   ...  3.8879027   7.132432
   7.66768   ]
 [ 5.6874776   6.932827    6.1243634  ...  9.540177    3.2688973
   7.1452303 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fae9d7e5a20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fae9da9a050>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fae9da9a050>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae7b8eb340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fae9d9df220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fae9d9df7f0>, <keras.callbacks.ModelCheckpoint object at 0x7fae9d9df8b0>, <keras.callbacks.EarlyStopping object at 0x7fae9d9dfb20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fae9d9dfb50>, <keras.callbacks.TerminateOnNaN object at 0x7fae9d9df790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[8.229488 , 4.495837 , 5.2408857, ..., 2.4850268, 8.645066 ,
        6.947819 ],
       [6.4597087, 8.5036125, 6.4417224, ..., 9.769764 , 1.7685618,
        5.715251 ],
       [8.184484 , 4.6048226, 5.1865025, ..., 2.5379913, 8.714117 ,
        6.813641 ],
       ...,
       [8.467587 , 5.268347 , 5.1986547, ..., 3.5898209, 8.35342  ,
        6.779558 ],
       [8.0343275, 5.003659 , 5.2459483, ..., 3.075615 , 8.72342  ,
        6.6362123],
       [5.2588263, 7.861301 , 6.1851134, ..., 9.199393 , 2.3736775,
        6.457649 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_332/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 332/360 with hyperparameters:
timestamp = 2023-09-10 04:48:58.096990
ndims = 1000
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.43748808e+00  7.13224745e+00  6.53301907e+00  5.31066608e+00
  3.79545021e+00  6.28738737e+00  2.71391177e+00  8.88872051e+00
  9.43951797e+00  4.12179470e+00  7.81367683e+00  5.36657619e+00
  5.55121899e+00  9.56866932e+00  7.94146359e-01  9.63486314e-01
 -1.46448463e-02  7.97977352e+00  8.35143089e+00  9.09444618e+00
  9.51050758e+00  8.15759563e+00  4.87677431e+00  8.12364483e+00
 -5.86649776e-01  6.22392607e+00  1.30506945e+00  9.45408440e+00
  7.08888865e+00  3.35072994e+00  2.25734115e+00  8.41814995e+00
  4.58319330e+00  5.61194181e+00  2.26602942e-01  5.45686245e+00
  6.14353561e+00  6.25023174e+00  9.57346344e+00  6.36795044e+00
  3.20757818e+00  4.83043623e+00  7.51926565e+00  6.34396672e-01
  6.89045191e+00  6.62100124e+00  2.44563007e+00  1.28772116e+00
  3.22250342e+00  4.52788544e+00  5.89035130e+00  4.86346960e+00
  9.48264980e+00 -1.13163722e+00  3.37091446e+00  9.05362010e-01
  6.78129101e+00  1.94818664e+00  4.17912102e+00  3.18960547e+00
  1.62776482e+00  1.21974039e+00  7.33607817e+00  1.62089002e+00
  2.96397400e+00  4.52560091e+00  8.39162827e+00  1.51216686e+00
  8.25380135e+00  5.98394752e-01  9.67017937e+00  4.48699141e+00
  1.02882853e+01  5.70898390e+00  6.28753185e+00  6.97938085e-01
  2.82613039e+00  1.18152845e+00  2.21910620e+00  1.18442059e-01
  3.30913496e+00  4.37123442e+00 -6.00820422e-01  7.13190508e+00
  5.83363104e+00  2.43325353e+00  5.20436668e+00  1.77731943e+00
  6.29233789e+00  9.14511585e+00  2.47849083e+00  7.14207220e+00
  1.59640992e+00  7.87721014e+00  3.72595000e+00  1.65804911e+00
  6.26306725e+00  6.14266574e-01  9.35802937e+00 -5.55100851e-02
  6.81794071e+00  2.07937002e+00  7.28380156e+00  1.05093136e+01
  1.40520501e+00  5.19018650e+00  6.29479265e+00  5.23412371e+00
  2.19359255e+00  9.70839596e+00  4.63605547e+00  9.80713272e+00
  6.71604156e+00  2.83194685e+00  8.21691513e+00  3.91888070e+00
  8.82857418e+00  5.65095377e+00  7.74853373e+00  6.66070604e+00
  8.72240067e+00  4.82063627e+00  9.65159893e+00  7.37593651e+00
  5.52394199e+00  6.06229258e+00  6.28078938e-01  3.63155842e+00
  6.63515043e+00  3.13720345e+00  6.10160494e+00  4.59917688e+00
  1.67009079e+00  2.67929316e+00  5.87469530e+00  5.35120296e+00
  5.20565653e+00  6.30662727e+00  7.39564276e+00  4.35430813e+00
  8.88551044e+00  2.87358689e+00  4.32844305e+00  9.07120323e+00
  8.34959602e+00  6.54305315e+00  6.36145771e-01  9.69855213e+00
  6.49582243e+00  1.02082977e+01  1.42218649e+00  7.77432966e+00
  1.58041847e+00  6.40702820e+00  1.07924449e+00  8.47589207e+00
  8.23396206e+00  5.41745853e+00  3.85215545e+00  7.45270491e-01
  6.41533756e+00  4.21811962e+00  7.45381451e+00  8.68435764e+00
  9.59165764e+00  9.22718906e+00 -1.01518297e+00  4.78479671e+00
  7.09206533e+00  2.16092372e+00  5.71304035e+00  1.19908273e-01
  3.16043782e+00  9.20669734e-02  8.17029667e+00  2.31537890e+00
  3.80825901e+00  9.06393814e+00  7.17630625e+00  1.20758474e-01
  1.57467747e+00  6.56413174e+00  5.80603886e+00  1.90417314e+00
  9.26850033e+00  5.71055651e+00  5.48348093e+00  5.70006657e+00
  7.34501457e+00  3.06974077e+00  4.17449951e+00  1.88637686e+00
  1.59649813e+00  8.74714470e+00  7.68149471e+00  5.14725018e+00
  1.94792962e+00  2.93671155e+00  6.94045722e-01  4.93041849e+00
  2.29451370e+00  6.96200371e+00  2.94133329e+00  7.65987992e-01
 -2.16490626e-02  6.85447574e-01  6.85941124e+00  4.85409117e+00
  5.72059441e+00  8.98723507e+00  9.62737179e+00  2.21571255e+00
  6.79449415e+00  2.29369974e+00  5.30113637e-01  7.87245512e+00
  3.63518763e+00  4.00529575e+00  5.20808411e+00  8.23005772e+00
  6.37014103e+00  8.26017952e+00  3.29657626e+00  7.93651295e+00
  1.78429985e+00  9.56696796e+00  7.06810617e+00  2.25382471e+00
  9.54646778e+00  7.23986912e+00  2.52804947e+00  2.58364177e+00
  5.52888250e+00  1.77395463e-01  2.06791592e+00  4.35356903e+00
  3.59637356e+00  3.74388838e+00  2.40446615e+00  5.81385517e+00
  8.67110729e+00  1.17576969e+00  4.94048452e+00  9.40345764e-01
  6.09276962e+00  3.86963511e+00  6.18902493e+00  1.56115341e+00
  2.90525699e+00  4.87162495e+00  3.71526504e+00  9.40074253e+00
  7.42161226e+00  7.27192879e+00  1.00699673e+01  1.18854022e+00
  5.78801632e+00  5.48251772e+00  9.94960403e+00  2.90021896e+00
  4.43213892e+00  4.77912188e-01  3.98015082e-01  1.12766476e+01
  6.74848080e+00  7.22801781e+00  2.82795882e+00  5.88993359e+00
  6.98136806e-01  3.23450828e+00  9.72003841e+00  8.30174351e+00
  3.07273555e+00  9.91379070e+00  1.91660452e+00  9.71661282e+00
  9.90089893e+00  8.15846062e+00  7.03740978e+00  9.58544540e+00
  2.93002963e+00  8.31967926e+00  5.99808025e+00  7.28550479e-02
  3.85735941e+00  9.70761538e-01  9.23753738e+00  5.46686459e+00
  5.50278473e+00  6.01728964e+00  4.34324074e+00  1.36993062e+00
  8.45484543e+00  4.68175888e-01  5.26911163e+00  1.84141123e+00
  3.81864011e-01  7.90763950e+00  9.70255852e+00  9.84720516e+00
  9.43443680e+00  7.68962574e+00  3.80397248e+00  6.65612042e-01
  5.16863966e+00  2.47927332e+00  4.73985672e-01  1.05347097e+00
  7.32991314e+00  9.36276674e-01  7.35460091e+00  1.67862105e+00
  4.33905751e-01  1.59291720e+00  7.84084034e+00  2.43011403e+00
  4.25165033e+00  5.68983269e+00  8.60872746e+00  7.16398525e+00
  2.72054410e+00  1.94252491e+00 -1.85395658e-01  3.34451747e+00
  1.99218881e+00  3.79499388e+00  5.96836662e+00  7.37168837e+00
  2.49210262e+00  3.98509121e+00  9.37832177e-01  8.80943108e+00
  2.57825583e-01  6.98963022e+00  7.78174019e+00  7.89891291e+00
  2.61566496e+00  3.04674053e+00  6.46863794e+00  2.90103054e+00
  4.06367350e+00  2.83596921e+00  5.16630077e+00  6.22896612e-01
  7.82478142e+00  3.80227089e-01  3.55100584e+00  3.55920887e+00
  5.84608507e+00  9.32784271e+00  6.44432068e+00  6.71383977e-01
  3.80863142e+00  5.12159157e+00  5.26213694e+00  6.50915527e+00
  3.31823993e+00  1.51588440e+00  3.74508786e+00  9.63789845e+00
  2.82012701e+00  9.30117702e+00  5.18297386e+00  5.08122492e+00
  9.10340309e+00  4.14017344e+00  8.16922951e+00  3.49889684e+00
  9.15036392e+00  7.04796791e+00  6.91863298e+00  3.43821239e+00
  7.49771976e+00  6.58094883e+00  1.52206314e+00  1.66219306e+00
  8.96467304e+00  9.71891689e+00  6.03888035e+00  6.12453365e+00
  8.36337280e+00  4.72068548e+00  7.40723658e+00  4.66089344e+00
  9.60812855e+00  1.02327881e+01  8.27917576e+00  1.76802504e+00
  5.86829615e+00  4.09585905e+00 -5.26962698e-01  4.35076141e+00
  2.46013212e+00  8.68481445e+00  3.86820614e-01  9.69470692e+00
  3.67586684e+00  3.92413521e+00 -9.84535664e-02  2.63078451e+00
  4.00670385e+00  1.10941935e+01  5.17772198e-01  9.30033970e+00
  9.24112511e+00  2.92216539e+00  3.37561965e+00  2.46589732e+00
  6.14229774e+00  4.67309088e-01 -8.67828280e-02  4.08560181e+00
  3.99175465e-01  2.54151607e+00  2.21575212e+00  2.50912642e+00
  1.27955842e+00 -7.25474238e-01  8.21681261e-01  6.53614998e+00
  1.04211264e+01  9.34315395e+00  3.89995766e+00  3.41503716e+00
  6.30898190e+00  4.78263617e+00  1.00384960e+01  1.44826126e+00
  8.01760387e+00  3.30487871e+00  2.22230029e+00  6.98749399e+00
  2.56673408e+00  6.51266956e+00  5.16357517e+00  4.21222687e+00
  5.53316402e+00  2.87564898e+00  6.88569880e+00  5.24295902e+00
  3.83784485e+00  8.63262177e+00  9.42399979e+00  4.50341552e-01
  2.33270240e+00  2.46700525e+00  8.57210445e+00  9.93498421e+00
  9.70180130e+00  9.04918671e+00  2.82848668e+00  1.01604681e+01
  2.13376069e+00  1.08001816e+00  1.01585007e+01  2.37143421e+00
  6.89639854e+00  6.13785625e-01  6.39277411e+00  8.98899841e+00
  3.29311657e+00  3.45347357e+00  3.81167507e+00  7.88749123e+00
  1.59779358e+00  5.66715002e-01  4.65487051e+00  4.15191126e+00
  8.22690773e+00  2.27221990e+00  5.74325514e+00  9.80365372e+00
  6.51945877e+00  8.45271111e+00  9.52017021e+00  7.24792719e+00
  6.43911409e+00  8.51561165e+00  1.02915831e+01  4.50498724e+00
  9.14542615e-01  2.86150455e+00  2.13800192e-01  3.66317129e+00
  1.32120097e+00  6.51088238e+00  3.09616232e+00  9.20569038e+00
  9.73210716e+00  5.21707201e+00  3.67960548e+00  5.34920645e+00
  8.44879818e+00  4.15654373e+00  5.51069403e+00  2.64717555e+00
  4.56229973e+00  4.03184319e+00  2.53720236e+00  4.99534702e+00
  2.31029892e+00  3.48865128e+00  8.25072670e+00  7.54355812e+00
  2.20225477e+00  1.13970642e+01  8.49588680e+00  7.10883319e-01
  2.74363828e+00  4.99293470e+00  1.34385705e-01  3.21501279e+00
  8.21650314e+00  6.79964018e+00  6.27488852e+00  7.14794111e+00
  1.90239477e+00  3.45294881e+00  7.35099411e+00  9.33995914e+00
  6.05294037e+00  2.90113711e+00 -1.24432337e+00  6.42705727e+00
  5.68964370e-02  3.37960458e+00  4.96043348e+00  9.90640545e+00
  1.23383534e+00  4.00741386e+00  6.40055537e-03  3.24708557e+00
  6.32790136e+00  2.19222999e+00  2.08447242e+00  1.32372677e+00
  5.92049932e+00  6.88256168e+00  7.01928997e+00  1.04399967e+01
 -2.88216442e-01  6.52927542e+00  6.16809750e+00  6.00324678e+00
  9.71869183e+00  1.13557257e-01  7.11323690e+00  7.71879911e+00
  5.03887224e+00  3.76561260e+00  8.05337811e+00  3.14987302e-01
  5.12668896e+00  5.27989054e+00  6.75082159e+00  1.43525863e+00
  2.57989788e+00  4.62727022e+00  6.04563522e+00  9.48542213e+00
  7.09511852e+00  1.17622685e+00  9.15441227e+00  9.80256653e+00
  5.09900427e+00  9.44795132e+00  5.51893759e+00  1.03330717e+01
  8.30469704e+00  1.07898369e+01  1.01827803e+01 -5.05745411e-04
  1.73885703e+00  3.52059174e+00  9.45347977e+00  3.41492081e+00
  1.14690316e+00  9.65928459e+00  4.69807243e+00  9.03562832e+00
  1.17234612e+00  6.69798565e+00  4.43081808e+00  8.74517798e-01
  6.92903471e+00  2.09807229e+00 -7.14968681e-01  5.73539436e-01
  6.28464758e-01  8.92654991e+00  8.03050327e+00  9.02817154e+00
  7.40084696e+00  5.17232370e+00  3.25738239e+00  1.01524630e+01
  3.55576420e+00  3.88637328e+00  4.20779133e+00  8.97262955e+00
  1.70729637e+00  3.30006480e+00  5.26616716e+00  9.60324168e-01
  6.40397358e+00  4.05875492e+00  2.62214065e+00  2.05707121e+00
  4.09347248e+00  3.94071198e+00  5.06777287e+00  3.40968561e+00
  8.99762726e+00  1.47003901e+00  8.83282781e-01  3.52675414e+00
  5.27389050e+00  6.61437607e+00  5.19436264e+00  9.38774395e+00
  3.59817815e+00  7.51900101e+00  6.17228937e+00  8.14072514e+00
  1.00964031e+01  8.67543316e+00  7.67522526e+00  6.90801048e+00
  3.51598859e+00  5.34878731e-01  6.36672795e-01  2.41127205e+00
  3.82626390e+00  5.18385172e+00  5.83661032e+00  9.15472126e+00
  8.23223293e-01  2.42483902e+00  5.41429329e+00  8.00633621e+00
  7.98597336e+00  9.41291142e+00  9.83840275e+00  4.71966314e+00
  4.63481140e+00  4.06945586e-01  6.68186712e+00  6.24139881e+00
  8.50881863e+00  7.10938215e-01  4.35715389e+00  5.98537350e+00
  2.39591932e+00  1.52024806e+00  9.15522194e+00  1.84586793e-01
  3.84848523e+00  1.13246572e+00  4.45568275e+00  9.84170437e+00
  4.21756744e+00  6.01763248e+00  7.27610350e-01  3.31445408e+00
  4.36259508e+00  4.26714516e+00  6.67291355e+00  3.47573233e+00
  6.96841526e+00  6.61320925e+00  4.78718519e+00  3.85241342e+00
  5.65875959e+00  9.63071108e-01  8.21956635e+00  7.05007601e+00
  7.35808372e+00  5.60682201e+00  8.74726832e-01  4.78749466e+00
  4.08274555e+00  3.05295515e+00 -2.82806456e-02  7.37880754e+00
  1.13245630e+00  6.21132517e+00  7.19738770e+00  6.62598848e+00
  8.44676971e+00  7.01955557e-01  8.75954437e+00  1.91597685e-01
  6.34029531e+00  4.09656525e+00  4.70814228e+00  3.66343403e+00
  1.92009354e+00 -2.97656119e-01  4.93795109e+00  7.03719902e+00
  3.54990387e+00  2.22650814e+00  1.47342825e+00  1.87733543e+00
  9.82225418e-01  1.97523093e+00  2.90856576e+00 -1.95377409e-01
  7.63740540e-01  8.23801708e+00  1.92524207e+00  4.57951069e+00
  8.07519722e+00  4.80337429e+00  1.63165092e+00  2.09296656e+00
  4.45628262e+00  5.24322176e+00  3.30323172e+00  7.77983236e+00
  7.60303879e+00  8.98269558e+00  3.17642242e-01  9.00031185e+00
  3.89518857e+00  8.87557316e+00  6.54464340e+00  9.40222740e+00
  7.76669884e+00  3.63010335e+00  4.89937687e+00  3.79815769e+00
  3.43109441e+00  2.11776972e+00  4.50608540e+00  6.73585749e+00
  2.77784801e+00  4.75118494e+00  1.41163933e+00  1.43678403e+00
  4.72844005e-01  1.01116915e+01  4.33705822e-02  1.87914848e+00
  6.03755856e+00  1.65911603e+00  9.08522511e+00  7.11956310e+00
  8.27528858e+00  4.67179298e+00  1.46821272e+00  5.57443333e+00
  9.03736591e+00  7.08154297e+00  2.92699647e+00  9.28951740e+00
  3.66812515e+00  1.79973990e-02  2.40762663e+00  1.92846382e+00
  8.69157314e+00  4.33280611e+00  4.23879671e+00  3.11560988e+00
  9.60638237e+00  1.10599823e+01  9.74016571e+00  3.66849303e+00
  9.42271328e+00  8.38017178e+00  6.39697123e+00  3.64649916e+00
  1.97412837e+00  2.19269991e+00  9.55885601e+00  2.80885458e+00
  4.91888142e+00  5.15744495e+00  8.31021786e+00  8.34845829e+00
  1.55545843e+00  3.92147779e+00  8.22108078e+00  7.88888502e+00
  1.22066307e+00  1.52498245e+00  3.15602779e+00  9.93907928e-01
  4.50527048e+00  1.36449265e+00  5.00522995e+00  2.42850351e+00
  5.87198400e+00 -1.04476917e+00  9.73143959e+00  9.51038170e+00
  4.31158495e+00  3.61805153e+00  1.88797522e+00  5.04972935e+00
  4.03085709e-01  4.60245562e+00  3.75540328e+00  4.01810455e+00
  8.59900475e+00  7.06478453e+00  9.62819099e+00  3.01856041e+00
  9.00697613e+00  8.57353497e+00  4.79125214e+00  6.26140499e+00
  2.85231853e+00  1.06102486e+01  1.06103802e+00  2.18488646e+00
  3.15353966e+00  4.29014683e+00  7.35634613e+00  5.12752581e+00
  4.86164331e+00  6.53155148e-01  5.14040470e+00  5.27053058e-01
  1.85232019e+00  9.85549164e+00  2.36561966e+00  8.83290672e+00
  8.07797718e+00  2.24759030e+00  5.88459301e+00  1.65719461e+00
  7.13856363e+00  6.36659193e+00  8.30550766e+00  4.64638376e+00
  8.86743259e+00 -1.78816348e-01  2.12690020e+00  7.22018147e+00
  4.37131548e+00  1.50182629e+00  1.44224501e+00  8.23597240e+00
  2.87912059e+00  8.82868195e+00  6.72185993e+00  4.10417318e+00
  3.72402120e+00  4.76707935e+00  8.69713497e+00  8.02846813e+00
 -5.91852479e-02  6.92536926e+00  9.99452686e+00  7.10508966e+00
  1.77003264e+00  5.62554789e+00  1.03404069e+00  4.70238781e+00
  9.31285954e+00  8.93695068e+00  2.82111669e+00  3.46559119e+00
  6.18987942e+00  9.39866257e+00  5.47782779e-01  2.72644186e+00
  8.94318962e+00  4.69057751e+00  1.03748589e+01  2.52786398e+00
  5.76243114e+00  3.48165631e+00  3.81163502e+00  3.76940107e+00
  4.03297091e+00  4.00298834e+00  8.04087162e+00  7.70173693e+00
  7.10223532e+00  3.21729660e+00  5.74228382e+00  6.87862539e+00
  7.01803017e+00  3.45543861e+00  7.26741600e+00  7.03988886e+00
  1.35023844e+00  8.91982746e+00  4.43680811e+00  5.25290489e+00
  3.51280975e+00  6.24143553e+00  6.52131414e+00  4.72280169e+00
  8.25355721e+00  6.25390244e+00  5.13515329e+00  4.87421942e+00
  7.67947769e+00  3.34088635e+00  2.31048179e+00  4.33013082e-01
  8.68751335e+00  9.47573662e+00  1.55903900e+00  3.07618022e+00
  1.50216174e+00  1.19446433e+00  9.29029846e+00  1.65662837e+00
  9.56382942e+00  9.47769403e-01  8.08623123e+00  5.85028315e+00
  4.43489981e+00  3.51404214e+00  7.59146261e+00  3.24519110e+00
  6.24453449e+00  3.84153080e+00  4.75549126e+00  8.96583271e+00
  5.54600143e+00  7.46122551e+00  6.96567345e+00  4.11713839e-01
  7.67649698e+00  2.08050108e+00  9.21236324e+00  1.41680741e+00
  3.03086996e+00  1.13182259e+00  4.03754902e+00  8.78156567e+00
  2.95456624e+00  1.61442590e+00  3.30749226e+00  8.14694023e+00
  1.37203860e+00  1.63373399e+00  6.45686090e-01  7.07791662e+00
  3.06980824e+00  4.66480160e+00  7.18863678e+00  3.41025472e+00
  7.26521921e+00  7.07599974e+00  2.82888842e+00  1.03278141e+01
  1.74131536e+00  2.78822184e+00  1.28855526e+00  3.68360829e+00
  5.39412928e+00  5.52329063e+00  8.01896572e+00  8.91389751e+00
  2.46595716e+00  6.16491985e+00  1.14002132e+00  3.73238707e+00
  3.28532362e+00  2.97972035e+00  4.97999954e+00  7.17574263e+00
  7.50538170e-01  1.02770720e+01  7.28148460e+00  2.06288123e+00
  9.35947418e+00  5.11205769e+00  9.92250729e+00  1.66801298e+00
  3.66859484e+00  3.06588221e+00  3.05286908e+00  5.87349463e+00
  9.89076328e+00  8.08929062e+00  6.18565559e+00  5.57329512e+00
  9.22900391e+00  3.96593451e+00  8.75203419e+00  8.35547829e+00
  2.17680025e+00  7.65000153e+00  1.08980224e-01  3.05595231e+00
  2.58785725e+00  5.23494005e+00  7.61446953e+00  4.45858717e-01
  4.63132048e+00  9.55903053e+00  1.96978843e+00  6.25351620e+00]
Epoch 1/1000
2023-09-10 04:51:04.410 
Epoch 1/1000 
	 loss: 1437.6387, MinusLogProbMetric: 1437.6387, val_loss: 606.7437, val_MinusLogProbMetric: 606.7437

Epoch 1: val_loss improved from inf to 606.74371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 127s - loss: 1437.6387 - MinusLogProbMetric: 1437.6387 - val_loss: 606.7437 - val_MinusLogProbMetric: 606.7437 - lr: 0.0010 - 127s/epoch - 649ms/step
Epoch 2/1000
2023-09-10 04:51:31.266 
Epoch 2/1000 
	 loss: 549.1440, MinusLogProbMetric: 549.1440, val_loss: 509.1637, val_MinusLogProbMetric: 509.1637

Epoch 2: val_loss improved from 606.74371 to 509.16367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 549.1440 - MinusLogProbMetric: 549.1440 - val_loss: 509.1637 - val_MinusLogProbMetric: 509.1637 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 3/1000
2023-09-10 04:51:58.262 
Epoch 3/1000 
	 loss: 514.1827, MinusLogProbMetric: 514.1827, val_loss: 501.5848, val_MinusLogProbMetric: 501.5848

Epoch 3: val_loss improved from 509.16367 to 501.58478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 514.1827 - MinusLogProbMetric: 514.1827 - val_loss: 501.5848 - val_MinusLogProbMetric: 501.5848 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 4/1000
2023-09-10 04:52:24.790 
Epoch 4/1000 
	 loss: 490.2717, MinusLogProbMetric: 490.2717, val_loss: 480.2407, val_MinusLogProbMetric: 480.2407

Epoch 4: val_loss improved from 501.58478 to 480.24072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 490.2717 - MinusLogProbMetric: 490.2717 - val_loss: 480.2407 - val_MinusLogProbMetric: 480.2407 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 5/1000
2023-09-10 04:52:52.307 
Epoch 5/1000 
	 loss: 480.7577, MinusLogProbMetric: 480.7577, val_loss: 465.0171, val_MinusLogProbMetric: 465.0171

Epoch 5: val_loss improved from 480.24072 to 465.01709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 480.7577 - MinusLogProbMetric: 480.7577 - val_loss: 465.0171 - val_MinusLogProbMetric: 465.0171 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 6/1000
2023-09-10 04:53:18.836 
Epoch 6/1000 
	 loss: 504.2998, MinusLogProbMetric: 504.2998, val_loss: 477.0372, val_MinusLogProbMetric: 477.0372

Epoch 6: val_loss did not improve from 465.01709
196/196 - 26s - loss: 504.2998 - MinusLogProbMetric: 504.2998 - val_loss: 477.0372 - val_MinusLogProbMetric: 477.0372 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 7/1000
2023-09-10 04:53:45.200 
Epoch 7/1000 
	 loss: 470.2957, MinusLogProbMetric: 470.2957, val_loss: 462.3189, val_MinusLogProbMetric: 462.3189

Epoch 7: val_loss improved from 465.01709 to 462.31888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 470.2957 - MinusLogProbMetric: 470.2957 - val_loss: 462.3189 - val_MinusLogProbMetric: 462.3189 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 8/1000
2023-09-10 04:54:09.546 
Epoch 8/1000 
	 loss: 464.2164, MinusLogProbMetric: 464.2164, val_loss: 453.4917, val_MinusLogProbMetric: 453.4917

Epoch 8: val_loss improved from 462.31888 to 453.49170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 24s - loss: 464.2164 - MinusLogProbMetric: 464.2164 - val_loss: 453.4917 - val_MinusLogProbMetric: 453.4917 - lr: 0.0010 - 24s/epoch - 124ms/step
Epoch 9/1000
2023-09-10 04:54:35.759 
Epoch 9/1000 
	 loss: 453.0404, MinusLogProbMetric: 453.0404, val_loss: 459.5088, val_MinusLogProbMetric: 459.5088

Epoch 9: val_loss did not improve from 453.49170
196/196 - 25s - loss: 453.0404 - MinusLogProbMetric: 453.0404 - val_loss: 459.5088 - val_MinusLogProbMetric: 459.5088 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 10/1000
2023-09-10 04:55:00.650 
Epoch 10/1000 
	 loss: 463.6805, MinusLogProbMetric: 463.6805, val_loss: 445.6991, val_MinusLogProbMetric: 445.6991

Epoch 10: val_loss improved from 453.49170 to 445.69907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 463.6805 - MinusLogProbMetric: 463.6805 - val_loss: 445.6991 - val_MinusLogProbMetric: 445.6991 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 11/1000
2023-09-10 04:55:27.147 
Epoch 11/1000 
	 loss: 446.4720, MinusLogProbMetric: 446.4720, val_loss: 448.2944, val_MinusLogProbMetric: 448.2944

Epoch 11: val_loss did not improve from 445.69907
196/196 - 25s - loss: 446.4720 - MinusLogProbMetric: 446.4720 - val_loss: 448.2944 - val_MinusLogProbMetric: 448.2944 - lr: 0.0010 - 25s/epoch - 127ms/step
Epoch 12/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 115: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-10 04:55:42.668 
Epoch 12/1000 
	 loss: nan, MinusLogProbMetric: 2164186119491815630645444267212800.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 12: val_loss did not improve from 445.69907
196/196 - 16s - loss: nan - MinusLogProbMetric: 2164186119491815630645444267212800.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 16s/epoch - 79ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 332.
===========
Train data generated in 2.83 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_332/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_332/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[8.229488 , 4.495837 , 5.2408857, ..., 2.4850268, 8.645066 ,
        6.947819 ],
       [6.4597087, 8.5036125, 6.4417224, ..., 9.769764 , 1.7685618,
        5.715251 ],
       [8.184484 , 4.6048226, 5.1865025, ..., 2.5379913, 8.714117 ,
        6.813641 ],
       ...,
       [8.467587 , 5.268347 , 5.1986547, ..., 3.5898209, 8.35342  ,
        6.779558 ],
       [8.0343275, 5.003659 , 5.2459483, ..., 3.075615 , 8.72342  ,
        6.6362123],
       [5.2588263, 7.861301 , 6.1851134, ..., 9.199393 , 2.3736775,
        6.457649 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_332/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_332
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.437488    7.1322474   6.533019   ...  9.559031    1.9697884
   6.253516  ]
 [ 6.105814   -0.33694893  4.861082   ...  4.728712    6.8399925
   2.9518385 ]
 [ 5.5099134   8.416618    3.6192868  ... 10.13037     0.5513004
   6.6116333 ]
 ...
 [ 8.378759    5.1539607   5.2823405  ...  4.1604185   7.9323363
   6.8563857 ]
 [ 8.088472    5.184855    5.311574   ...  3.8879027   7.132432
   7.66768   ]
 [ 5.6874776   6.932827    6.1243634  ...  9.540177    3.2688973
   7.1452303 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7faed8301de0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7faed8373700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7faed8373700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae83f658d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fae9ed2ac20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fae83f39600>, <keras.callbacks.ModelCheckpoint object at 0x7faed83016c0>, <keras.callbacks.EarlyStopping object at 0x7fae83f85c00>, <keras.callbacks.ReduceLROnPlateau object at 0x7faed83024d0>, <keras.callbacks.TerminateOnNaN object at 0x7faed8302650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[8.229488 , 4.495837 , 5.2408857, ..., 2.4850268, 8.645066 ,
        6.947819 ],
       [6.4597087, 8.5036125, 6.4417224, ..., 9.769764 , 1.7685618,
        5.715251 ],
       [8.184484 , 4.6048226, 5.1865025, ..., 2.5379913, 8.714117 ,
        6.813641 ],
       ...,
       [8.467587 , 5.268347 , 5.1986547, ..., 3.5898209, 8.35342  ,
        6.779558 ],
       [8.0343275, 5.003659 , 5.2459483, ..., 3.075615 , 8.72342  ,
        6.6362123],
       [5.2588263, 7.861301 , 6.1851134, ..., 9.199393 , 2.3736775,
        6.457649 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 332/360 with hyperparameters:
timestamp = 2023-09-10 04:55:54.333748
ndims = 1000
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.43748808e+00  7.13224745e+00  6.53301907e+00  5.31066608e+00
  3.79545021e+00  6.28738737e+00  2.71391177e+00  8.88872051e+00
  9.43951797e+00  4.12179470e+00  7.81367683e+00  5.36657619e+00
  5.55121899e+00  9.56866932e+00  7.94146359e-01  9.63486314e-01
 -1.46448463e-02  7.97977352e+00  8.35143089e+00  9.09444618e+00
  9.51050758e+00  8.15759563e+00  4.87677431e+00  8.12364483e+00
 -5.86649776e-01  6.22392607e+00  1.30506945e+00  9.45408440e+00
  7.08888865e+00  3.35072994e+00  2.25734115e+00  8.41814995e+00
  4.58319330e+00  5.61194181e+00  2.26602942e-01  5.45686245e+00
  6.14353561e+00  6.25023174e+00  9.57346344e+00  6.36795044e+00
  3.20757818e+00  4.83043623e+00  7.51926565e+00  6.34396672e-01
  6.89045191e+00  6.62100124e+00  2.44563007e+00  1.28772116e+00
  3.22250342e+00  4.52788544e+00  5.89035130e+00  4.86346960e+00
  9.48264980e+00 -1.13163722e+00  3.37091446e+00  9.05362010e-01
  6.78129101e+00  1.94818664e+00  4.17912102e+00  3.18960547e+00
  1.62776482e+00  1.21974039e+00  7.33607817e+00  1.62089002e+00
  2.96397400e+00  4.52560091e+00  8.39162827e+00  1.51216686e+00
  8.25380135e+00  5.98394752e-01  9.67017937e+00  4.48699141e+00
  1.02882853e+01  5.70898390e+00  6.28753185e+00  6.97938085e-01
  2.82613039e+00  1.18152845e+00  2.21910620e+00  1.18442059e-01
  3.30913496e+00  4.37123442e+00 -6.00820422e-01  7.13190508e+00
  5.83363104e+00  2.43325353e+00  5.20436668e+00  1.77731943e+00
  6.29233789e+00  9.14511585e+00  2.47849083e+00  7.14207220e+00
  1.59640992e+00  7.87721014e+00  3.72595000e+00  1.65804911e+00
  6.26306725e+00  6.14266574e-01  9.35802937e+00 -5.55100851e-02
  6.81794071e+00  2.07937002e+00  7.28380156e+00  1.05093136e+01
  1.40520501e+00  5.19018650e+00  6.29479265e+00  5.23412371e+00
  2.19359255e+00  9.70839596e+00  4.63605547e+00  9.80713272e+00
  6.71604156e+00  2.83194685e+00  8.21691513e+00  3.91888070e+00
  8.82857418e+00  5.65095377e+00  7.74853373e+00  6.66070604e+00
  8.72240067e+00  4.82063627e+00  9.65159893e+00  7.37593651e+00
  5.52394199e+00  6.06229258e+00  6.28078938e-01  3.63155842e+00
  6.63515043e+00  3.13720345e+00  6.10160494e+00  4.59917688e+00
  1.67009079e+00  2.67929316e+00  5.87469530e+00  5.35120296e+00
  5.20565653e+00  6.30662727e+00  7.39564276e+00  4.35430813e+00
  8.88551044e+00  2.87358689e+00  4.32844305e+00  9.07120323e+00
  8.34959602e+00  6.54305315e+00  6.36145771e-01  9.69855213e+00
  6.49582243e+00  1.02082977e+01  1.42218649e+00  7.77432966e+00
  1.58041847e+00  6.40702820e+00  1.07924449e+00  8.47589207e+00
  8.23396206e+00  5.41745853e+00  3.85215545e+00  7.45270491e-01
  6.41533756e+00  4.21811962e+00  7.45381451e+00  8.68435764e+00
  9.59165764e+00  9.22718906e+00 -1.01518297e+00  4.78479671e+00
  7.09206533e+00  2.16092372e+00  5.71304035e+00  1.19908273e-01
  3.16043782e+00  9.20669734e-02  8.17029667e+00  2.31537890e+00
  3.80825901e+00  9.06393814e+00  7.17630625e+00  1.20758474e-01
  1.57467747e+00  6.56413174e+00  5.80603886e+00  1.90417314e+00
  9.26850033e+00  5.71055651e+00  5.48348093e+00  5.70006657e+00
  7.34501457e+00  3.06974077e+00  4.17449951e+00  1.88637686e+00
  1.59649813e+00  8.74714470e+00  7.68149471e+00  5.14725018e+00
  1.94792962e+00  2.93671155e+00  6.94045722e-01  4.93041849e+00
  2.29451370e+00  6.96200371e+00  2.94133329e+00  7.65987992e-01
 -2.16490626e-02  6.85447574e-01  6.85941124e+00  4.85409117e+00
  5.72059441e+00  8.98723507e+00  9.62737179e+00  2.21571255e+00
  6.79449415e+00  2.29369974e+00  5.30113637e-01  7.87245512e+00
  3.63518763e+00  4.00529575e+00  5.20808411e+00  8.23005772e+00
  6.37014103e+00  8.26017952e+00  3.29657626e+00  7.93651295e+00
  1.78429985e+00  9.56696796e+00  7.06810617e+00  2.25382471e+00
  9.54646778e+00  7.23986912e+00  2.52804947e+00  2.58364177e+00
  5.52888250e+00  1.77395463e-01  2.06791592e+00  4.35356903e+00
  3.59637356e+00  3.74388838e+00  2.40446615e+00  5.81385517e+00
  8.67110729e+00  1.17576969e+00  4.94048452e+00  9.40345764e-01
  6.09276962e+00  3.86963511e+00  6.18902493e+00  1.56115341e+00
  2.90525699e+00  4.87162495e+00  3.71526504e+00  9.40074253e+00
  7.42161226e+00  7.27192879e+00  1.00699673e+01  1.18854022e+00
  5.78801632e+00  5.48251772e+00  9.94960403e+00  2.90021896e+00
  4.43213892e+00  4.77912188e-01  3.98015082e-01  1.12766476e+01
  6.74848080e+00  7.22801781e+00  2.82795882e+00  5.88993359e+00
  6.98136806e-01  3.23450828e+00  9.72003841e+00  8.30174351e+00
  3.07273555e+00  9.91379070e+00  1.91660452e+00  9.71661282e+00
  9.90089893e+00  8.15846062e+00  7.03740978e+00  9.58544540e+00
  2.93002963e+00  8.31967926e+00  5.99808025e+00  7.28550479e-02
  3.85735941e+00  9.70761538e-01  9.23753738e+00  5.46686459e+00
  5.50278473e+00  6.01728964e+00  4.34324074e+00  1.36993062e+00
  8.45484543e+00  4.68175888e-01  5.26911163e+00  1.84141123e+00
  3.81864011e-01  7.90763950e+00  9.70255852e+00  9.84720516e+00
  9.43443680e+00  7.68962574e+00  3.80397248e+00  6.65612042e-01
  5.16863966e+00  2.47927332e+00  4.73985672e-01  1.05347097e+00
  7.32991314e+00  9.36276674e-01  7.35460091e+00  1.67862105e+00
  4.33905751e-01  1.59291720e+00  7.84084034e+00  2.43011403e+00
  4.25165033e+00  5.68983269e+00  8.60872746e+00  7.16398525e+00
  2.72054410e+00  1.94252491e+00 -1.85395658e-01  3.34451747e+00
  1.99218881e+00  3.79499388e+00  5.96836662e+00  7.37168837e+00
  2.49210262e+00  3.98509121e+00  9.37832177e-01  8.80943108e+00
  2.57825583e-01  6.98963022e+00  7.78174019e+00  7.89891291e+00
  2.61566496e+00  3.04674053e+00  6.46863794e+00  2.90103054e+00
  4.06367350e+00  2.83596921e+00  5.16630077e+00  6.22896612e-01
  7.82478142e+00  3.80227089e-01  3.55100584e+00  3.55920887e+00
  5.84608507e+00  9.32784271e+00  6.44432068e+00  6.71383977e-01
  3.80863142e+00  5.12159157e+00  5.26213694e+00  6.50915527e+00
  3.31823993e+00  1.51588440e+00  3.74508786e+00  9.63789845e+00
  2.82012701e+00  9.30117702e+00  5.18297386e+00  5.08122492e+00
  9.10340309e+00  4.14017344e+00  8.16922951e+00  3.49889684e+00
  9.15036392e+00  7.04796791e+00  6.91863298e+00  3.43821239e+00
  7.49771976e+00  6.58094883e+00  1.52206314e+00  1.66219306e+00
  8.96467304e+00  9.71891689e+00  6.03888035e+00  6.12453365e+00
  8.36337280e+00  4.72068548e+00  7.40723658e+00  4.66089344e+00
  9.60812855e+00  1.02327881e+01  8.27917576e+00  1.76802504e+00
  5.86829615e+00  4.09585905e+00 -5.26962698e-01  4.35076141e+00
  2.46013212e+00  8.68481445e+00  3.86820614e-01  9.69470692e+00
  3.67586684e+00  3.92413521e+00 -9.84535664e-02  2.63078451e+00
  4.00670385e+00  1.10941935e+01  5.17772198e-01  9.30033970e+00
  9.24112511e+00  2.92216539e+00  3.37561965e+00  2.46589732e+00
  6.14229774e+00  4.67309088e-01 -8.67828280e-02  4.08560181e+00
  3.99175465e-01  2.54151607e+00  2.21575212e+00  2.50912642e+00
  1.27955842e+00 -7.25474238e-01  8.21681261e-01  6.53614998e+00
  1.04211264e+01  9.34315395e+00  3.89995766e+00  3.41503716e+00
  6.30898190e+00  4.78263617e+00  1.00384960e+01  1.44826126e+00
  8.01760387e+00  3.30487871e+00  2.22230029e+00  6.98749399e+00
  2.56673408e+00  6.51266956e+00  5.16357517e+00  4.21222687e+00
  5.53316402e+00  2.87564898e+00  6.88569880e+00  5.24295902e+00
  3.83784485e+00  8.63262177e+00  9.42399979e+00  4.50341552e-01
  2.33270240e+00  2.46700525e+00  8.57210445e+00  9.93498421e+00
  9.70180130e+00  9.04918671e+00  2.82848668e+00  1.01604681e+01
  2.13376069e+00  1.08001816e+00  1.01585007e+01  2.37143421e+00
  6.89639854e+00  6.13785625e-01  6.39277411e+00  8.98899841e+00
  3.29311657e+00  3.45347357e+00  3.81167507e+00  7.88749123e+00
  1.59779358e+00  5.66715002e-01  4.65487051e+00  4.15191126e+00
  8.22690773e+00  2.27221990e+00  5.74325514e+00  9.80365372e+00
  6.51945877e+00  8.45271111e+00  9.52017021e+00  7.24792719e+00
  6.43911409e+00  8.51561165e+00  1.02915831e+01  4.50498724e+00
  9.14542615e-01  2.86150455e+00  2.13800192e-01  3.66317129e+00
  1.32120097e+00  6.51088238e+00  3.09616232e+00  9.20569038e+00
  9.73210716e+00  5.21707201e+00  3.67960548e+00  5.34920645e+00
  8.44879818e+00  4.15654373e+00  5.51069403e+00  2.64717555e+00
  4.56229973e+00  4.03184319e+00  2.53720236e+00  4.99534702e+00
  2.31029892e+00  3.48865128e+00  8.25072670e+00  7.54355812e+00
  2.20225477e+00  1.13970642e+01  8.49588680e+00  7.10883319e-01
  2.74363828e+00  4.99293470e+00  1.34385705e-01  3.21501279e+00
  8.21650314e+00  6.79964018e+00  6.27488852e+00  7.14794111e+00
  1.90239477e+00  3.45294881e+00  7.35099411e+00  9.33995914e+00
  6.05294037e+00  2.90113711e+00 -1.24432337e+00  6.42705727e+00
  5.68964370e-02  3.37960458e+00  4.96043348e+00  9.90640545e+00
  1.23383534e+00  4.00741386e+00  6.40055537e-03  3.24708557e+00
  6.32790136e+00  2.19222999e+00  2.08447242e+00  1.32372677e+00
  5.92049932e+00  6.88256168e+00  7.01928997e+00  1.04399967e+01
 -2.88216442e-01  6.52927542e+00  6.16809750e+00  6.00324678e+00
  9.71869183e+00  1.13557257e-01  7.11323690e+00  7.71879911e+00
  5.03887224e+00  3.76561260e+00  8.05337811e+00  3.14987302e-01
  5.12668896e+00  5.27989054e+00  6.75082159e+00  1.43525863e+00
  2.57989788e+00  4.62727022e+00  6.04563522e+00  9.48542213e+00
  7.09511852e+00  1.17622685e+00  9.15441227e+00  9.80256653e+00
  5.09900427e+00  9.44795132e+00  5.51893759e+00  1.03330717e+01
  8.30469704e+00  1.07898369e+01  1.01827803e+01 -5.05745411e-04
  1.73885703e+00  3.52059174e+00  9.45347977e+00  3.41492081e+00
  1.14690316e+00  9.65928459e+00  4.69807243e+00  9.03562832e+00
  1.17234612e+00  6.69798565e+00  4.43081808e+00  8.74517798e-01
  6.92903471e+00  2.09807229e+00 -7.14968681e-01  5.73539436e-01
  6.28464758e-01  8.92654991e+00  8.03050327e+00  9.02817154e+00
  7.40084696e+00  5.17232370e+00  3.25738239e+00  1.01524630e+01
  3.55576420e+00  3.88637328e+00  4.20779133e+00  8.97262955e+00
  1.70729637e+00  3.30006480e+00  5.26616716e+00  9.60324168e-01
  6.40397358e+00  4.05875492e+00  2.62214065e+00  2.05707121e+00
  4.09347248e+00  3.94071198e+00  5.06777287e+00  3.40968561e+00
  8.99762726e+00  1.47003901e+00  8.83282781e-01  3.52675414e+00
  5.27389050e+00  6.61437607e+00  5.19436264e+00  9.38774395e+00
  3.59817815e+00  7.51900101e+00  6.17228937e+00  8.14072514e+00
  1.00964031e+01  8.67543316e+00  7.67522526e+00  6.90801048e+00
  3.51598859e+00  5.34878731e-01  6.36672795e-01  2.41127205e+00
  3.82626390e+00  5.18385172e+00  5.83661032e+00  9.15472126e+00
  8.23223293e-01  2.42483902e+00  5.41429329e+00  8.00633621e+00
  7.98597336e+00  9.41291142e+00  9.83840275e+00  4.71966314e+00
  4.63481140e+00  4.06945586e-01  6.68186712e+00  6.24139881e+00
  8.50881863e+00  7.10938215e-01  4.35715389e+00  5.98537350e+00
  2.39591932e+00  1.52024806e+00  9.15522194e+00  1.84586793e-01
  3.84848523e+00  1.13246572e+00  4.45568275e+00  9.84170437e+00
  4.21756744e+00  6.01763248e+00  7.27610350e-01  3.31445408e+00
  4.36259508e+00  4.26714516e+00  6.67291355e+00  3.47573233e+00
  6.96841526e+00  6.61320925e+00  4.78718519e+00  3.85241342e+00
  5.65875959e+00  9.63071108e-01  8.21956635e+00  7.05007601e+00
  7.35808372e+00  5.60682201e+00  8.74726832e-01  4.78749466e+00
  4.08274555e+00  3.05295515e+00 -2.82806456e-02  7.37880754e+00
  1.13245630e+00  6.21132517e+00  7.19738770e+00  6.62598848e+00
  8.44676971e+00  7.01955557e-01  8.75954437e+00  1.91597685e-01
  6.34029531e+00  4.09656525e+00  4.70814228e+00  3.66343403e+00
  1.92009354e+00 -2.97656119e-01  4.93795109e+00  7.03719902e+00
  3.54990387e+00  2.22650814e+00  1.47342825e+00  1.87733543e+00
  9.82225418e-01  1.97523093e+00  2.90856576e+00 -1.95377409e-01
  7.63740540e-01  8.23801708e+00  1.92524207e+00  4.57951069e+00
  8.07519722e+00  4.80337429e+00  1.63165092e+00  2.09296656e+00
  4.45628262e+00  5.24322176e+00  3.30323172e+00  7.77983236e+00
  7.60303879e+00  8.98269558e+00  3.17642242e-01  9.00031185e+00
  3.89518857e+00  8.87557316e+00  6.54464340e+00  9.40222740e+00
  7.76669884e+00  3.63010335e+00  4.89937687e+00  3.79815769e+00
  3.43109441e+00  2.11776972e+00  4.50608540e+00  6.73585749e+00
  2.77784801e+00  4.75118494e+00  1.41163933e+00  1.43678403e+00
  4.72844005e-01  1.01116915e+01  4.33705822e-02  1.87914848e+00
  6.03755856e+00  1.65911603e+00  9.08522511e+00  7.11956310e+00
  8.27528858e+00  4.67179298e+00  1.46821272e+00  5.57443333e+00
  9.03736591e+00  7.08154297e+00  2.92699647e+00  9.28951740e+00
  3.66812515e+00  1.79973990e-02  2.40762663e+00  1.92846382e+00
  8.69157314e+00  4.33280611e+00  4.23879671e+00  3.11560988e+00
  9.60638237e+00  1.10599823e+01  9.74016571e+00  3.66849303e+00
  9.42271328e+00  8.38017178e+00  6.39697123e+00  3.64649916e+00
  1.97412837e+00  2.19269991e+00  9.55885601e+00  2.80885458e+00
  4.91888142e+00  5.15744495e+00  8.31021786e+00  8.34845829e+00
  1.55545843e+00  3.92147779e+00  8.22108078e+00  7.88888502e+00
  1.22066307e+00  1.52498245e+00  3.15602779e+00  9.93907928e-01
  4.50527048e+00  1.36449265e+00  5.00522995e+00  2.42850351e+00
  5.87198400e+00 -1.04476917e+00  9.73143959e+00  9.51038170e+00
  4.31158495e+00  3.61805153e+00  1.88797522e+00  5.04972935e+00
  4.03085709e-01  4.60245562e+00  3.75540328e+00  4.01810455e+00
  8.59900475e+00  7.06478453e+00  9.62819099e+00  3.01856041e+00
  9.00697613e+00  8.57353497e+00  4.79125214e+00  6.26140499e+00
  2.85231853e+00  1.06102486e+01  1.06103802e+00  2.18488646e+00
  3.15353966e+00  4.29014683e+00  7.35634613e+00  5.12752581e+00
  4.86164331e+00  6.53155148e-01  5.14040470e+00  5.27053058e-01
  1.85232019e+00  9.85549164e+00  2.36561966e+00  8.83290672e+00
  8.07797718e+00  2.24759030e+00  5.88459301e+00  1.65719461e+00
  7.13856363e+00  6.36659193e+00  8.30550766e+00  4.64638376e+00
  8.86743259e+00 -1.78816348e-01  2.12690020e+00  7.22018147e+00
  4.37131548e+00  1.50182629e+00  1.44224501e+00  8.23597240e+00
  2.87912059e+00  8.82868195e+00  6.72185993e+00  4.10417318e+00
  3.72402120e+00  4.76707935e+00  8.69713497e+00  8.02846813e+00
 -5.91852479e-02  6.92536926e+00  9.99452686e+00  7.10508966e+00
  1.77003264e+00  5.62554789e+00  1.03404069e+00  4.70238781e+00
  9.31285954e+00  8.93695068e+00  2.82111669e+00  3.46559119e+00
  6.18987942e+00  9.39866257e+00  5.47782779e-01  2.72644186e+00
  8.94318962e+00  4.69057751e+00  1.03748589e+01  2.52786398e+00
  5.76243114e+00  3.48165631e+00  3.81163502e+00  3.76940107e+00
  4.03297091e+00  4.00298834e+00  8.04087162e+00  7.70173693e+00
  7.10223532e+00  3.21729660e+00  5.74228382e+00  6.87862539e+00
  7.01803017e+00  3.45543861e+00  7.26741600e+00  7.03988886e+00
  1.35023844e+00  8.91982746e+00  4.43680811e+00  5.25290489e+00
  3.51280975e+00  6.24143553e+00  6.52131414e+00  4.72280169e+00
  8.25355721e+00  6.25390244e+00  5.13515329e+00  4.87421942e+00
  7.67947769e+00  3.34088635e+00  2.31048179e+00  4.33013082e-01
  8.68751335e+00  9.47573662e+00  1.55903900e+00  3.07618022e+00
  1.50216174e+00  1.19446433e+00  9.29029846e+00  1.65662837e+00
  9.56382942e+00  9.47769403e-01  8.08623123e+00  5.85028315e+00
  4.43489981e+00  3.51404214e+00  7.59146261e+00  3.24519110e+00
  6.24453449e+00  3.84153080e+00  4.75549126e+00  8.96583271e+00
  5.54600143e+00  7.46122551e+00  6.96567345e+00  4.11713839e-01
  7.67649698e+00  2.08050108e+00  9.21236324e+00  1.41680741e+00
  3.03086996e+00  1.13182259e+00  4.03754902e+00  8.78156567e+00
  2.95456624e+00  1.61442590e+00  3.30749226e+00  8.14694023e+00
  1.37203860e+00  1.63373399e+00  6.45686090e-01  7.07791662e+00
  3.06980824e+00  4.66480160e+00  7.18863678e+00  3.41025472e+00
  7.26521921e+00  7.07599974e+00  2.82888842e+00  1.03278141e+01
  1.74131536e+00  2.78822184e+00  1.28855526e+00  3.68360829e+00
  5.39412928e+00  5.52329063e+00  8.01896572e+00  8.91389751e+00
  2.46595716e+00  6.16491985e+00  1.14002132e+00  3.73238707e+00
  3.28532362e+00  2.97972035e+00  4.97999954e+00  7.17574263e+00
  7.50538170e-01  1.02770720e+01  7.28148460e+00  2.06288123e+00
  9.35947418e+00  5.11205769e+00  9.92250729e+00  1.66801298e+00
  3.66859484e+00  3.06588221e+00  3.05286908e+00  5.87349463e+00
  9.89076328e+00  8.08929062e+00  6.18565559e+00  5.57329512e+00
  9.22900391e+00  3.96593451e+00  8.75203419e+00  8.35547829e+00
  2.17680025e+00  7.65000153e+00  1.08980224e-01  3.05595231e+00
  2.58785725e+00  5.23494005e+00  7.61446953e+00  4.45858717e-01
  4.63132048e+00  9.55903053e+00  1.96978843e+00  6.25351620e+00]
Epoch 1/1000
2023-09-10 04:57:52.106 
Epoch 1/1000 
	 loss: 470.4269, MinusLogProbMetric: 470.4269, val_loss: 424.6051, val_MinusLogProbMetric: 424.6051

Epoch 1: val_loss improved from inf to 424.60507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 118s - loss: 470.4269 - MinusLogProbMetric: 470.4269 - val_loss: 424.6051 - val_MinusLogProbMetric: 424.6051 - lr: 3.3333e-04 - 118s/epoch - 603ms/step
Epoch 2/1000
2023-09-10 04:58:18.720 
Epoch 2/1000 
	 loss: 424.1070, MinusLogProbMetric: 424.1070, val_loss: 421.6802, val_MinusLogProbMetric: 421.6802

Epoch 2: val_loss improved from 424.60507 to 421.68018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 424.1070 - MinusLogProbMetric: 424.1070 - val_loss: 421.6802 - val_MinusLogProbMetric: 421.6802 - lr: 3.3333e-04 - 27s/epoch - 135ms/step
Epoch 3/1000
2023-09-10 04:58:43.915 
Epoch 3/1000 
	 loss: 425.1869, MinusLogProbMetric: 425.1869, val_loss: 422.4328, val_MinusLogProbMetric: 422.4328

Epoch 3: val_loss did not improve from 421.68018
196/196 - 24s - loss: 425.1869 - MinusLogProbMetric: 425.1869 - val_loss: 422.4328 - val_MinusLogProbMetric: 422.4328 - lr: 3.3333e-04 - 24s/epoch - 121ms/step
Epoch 4/1000
2023-09-10 04:59:08.360 
Epoch 4/1000 
	 loss: 421.5349, MinusLogProbMetric: 421.5349, val_loss: 420.8282, val_MinusLogProbMetric: 420.8282

Epoch 4: val_loss improved from 421.68018 to 420.82816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 25s - loss: 421.5349 - MinusLogProbMetric: 421.5349 - val_loss: 420.8282 - val_MinusLogProbMetric: 420.8282 - lr: 3.3333e-04 - 25s/epoch - 129ms/step
Epoch 5/1000
2023-09-10 04:59:34.385 
Epoch 5/1000 
	 loss: 420.5957, MinusLogProbMetric: 420.5957, val_loss: 419.8792, val_MinusLogProbMetric: 419.8792

Epoch 5: val_loss improved from 420.82816 to 419.87918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 420.5957 - MinusLogProbMetric: 420.5957 - val_loss: 419.8792 - val_MinusLogProbMetric: 419.8792 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 6/1000
2023-09-10 05:00:00.122 
Epoch 6/1000 
	 loss: 419.9314, MinusLogProbMetric: 419.9314, val_loss: 422.6677, val_MinusLogProbMetric: 422.6677

Epoch 6: val_loss did not improve from 419.87918
196/196 - 25s - loss: 419.9314 - MinusLogProbMetric: 419.9314 - val_loss: 422.6677 - val_MinusLogProbMetric: 422.6677 - lr: 3.3333e-04 - 25s/epoch - 125ms/step
Epoch 7/1000
2023-09-10 05:00:25.949 
Epoch 7/1000 
	 loss: 417.8266, MinusLogProbMetric: 417.8266, val_loss: 417.3034, val_MinusLogProbMetric: 417.3034

Epoch 7: val_loss improved from 419.87918 to 417.30338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 417.8266 - MinusLogProbMetric: 417.8266 - val_loss: 417.3034 - val_MinusLogProbMetric: 417.3034 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 8/1000
2023-09-10 05:00:51.693 
Epoch 8/1000 
	 loss: 419.0972, MinusLogProbMetric: 419.0972, val_loss: 415.2843, val_MinusLogProbMetric: 415.2843

Epoch 8: val_loss improved from 417.30338 to 415.28430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 419.0972 - MinusLogProbMetric: 419.0972 - val_loss: 415.2843 - val_MinusLogProbMetric: 415.2843 - lr: 3.3333e-04 - 26s/epoch - 132ms/step
Epoch 9/1000
2023-09-10 05:01:16.638 
Epoch 9/1000 
	 loss: 416.7289, MinusLogProbMetric: 416.7289, val_loss: 414.7238, val_MinusLogProbMetric: 414.7238

Epoch 9: val_loss improved from 415.28430 to 414.72375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 25s - loss: 416.7289 - MinusLogProbMetric: 416.7289 - val_loss: 414.7238 - val_MinusLogProbMetric: 414.7238 - lr: 3.3333e-04 - 25s/epoch - 127ms/step
Epoch 10/1000
2023-09-10 05:01:44.660 
Epoch 10/1000 
	 loss: 417.3969, MinusLogProbMetric: 417.3969, val_loss: 414.3498, val_MinusLogProbMetric: 414.3498

Epoch 10: val_loss improved from 414.72375 to 414.34979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 28s - loss: 417.3969 - MinusLogProbMetric: 417.3969 - val_loss: 414.3498 - val_MinusLogProbMetric: 414.3498 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 11/1000
2023-09-10 05:02:12.341 
Epoch 11/1000 
	 loss: 414.7722, MinusLogProbMetric: 414.7722, val_loss: 414.3306, val_MinusLogProbMetric: 414.3306

Epoch 11: val_loss improved from 414.34979 to 414.33060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 28s - loss: 414.7722 - MinusLogProbMetric: 414.7722 - val_loss: 414.3306 - val_MinusLogProbMetric: 414.3306 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 12/1000
2023-09-10 05:02:37.491 
Epoch 12/1000 
	 loss: 415.6362, MinusLogProbMetric: 415.6362, val_loss: 417.3882, val_MinusLogProbMetric: 417.3882

Epoch 12: val_loss did not improve from 414.33060
196/196 - 24s - loss: 415.6362 - MinusLogProbMetric: 415.6362 - val_loss: 417.3882 - val_MinusLogProbMetric: 417.3882 - lr: 3.3333e-04 - 24s/epoch - 123ms/step
Epoch 13/1000
2023-09-10 05:03:02.030 
Epoch 13/1000 
	 loss: 414.4600, MinusLogProbMetric: 414.4600, val_loss: 413.4758, val_MinusLogProbMetric: 413.4758

Epoch 13: val_loss improved from 414.33060 to 413.47583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 25s - loss: 414.4600 - MinusLogProbMetric: 414.4600 - val_loss: 413.4758 - val_MinusLogProbMetric: 413.4758 - lr: 3.3333e-04 - 25s/epoch - 129ms/step
Epoch 14/1000
2023-09-10 05:03:27.391 
Epoch 14/1000 
	 loss: 413.0695, MinusLogProbMetric: 413.0695, val_loss: 419.4416, val_MinusLogProbMetric: 419.4416

Epoch 14: val_loss did not improve from 413.47583
196/196 - 25s - loss: 413.0695 - MinusLogProbMetric: 413.0695 - val_loss: 419.4416 - val_MinusLogProbMetric: 419.4416 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 15/1000
2023-09-10 05:03:53.561 
Epoch 15/1000 
	 loss: 413.4174, MinusLogProbMetric: 413.4174, val_loss: 420.5936, val_MinusLogProbMetric: 420.5936

Epoch 15: val_loss did not improve from 413.47583
196/196 - 26s - loss: 413.4174 - MinusLogProbMetric: 413.4174 - val_loss: 420.5936 - val_MinusLogProbMetric: 420.5936 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 16/1000
2023-09-10 05:04:16.825 
Epoch 16/1000 
	 loss: 412.0279, MinusLogProbMetric: 412.0279, val_loss: 411.4760, val_MinusLogProbMetric: 411.4760

Epoch 16: val_loss improved from 413.47583 to 411.47595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 24s - loss: 412.0279 - MinusLogProbMetric: 412.0279 - val_loss: 411.4760 - val_MinusLogProbMetric: 411.4760 - lr: 3.3333e-04 - 24s/epoch - 123ms/step
Epoch 17/1000
2023-09-10 05:04:40.616 
Epoch 17/1000 
	 loss: 412.5734, MinusLogProbMetric: 412.5734, val_loss: 411.2722, val_MinusLogProbMetric: 411.2722

Epoch 17: val_loss improved from 411.47595 to 411.27219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 25s - loss: 412.5734 - MinusLogProbMetric: 412.5734 - val_loss: 411.2722 - val_MinusLogProbMetric: 411.2722 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 18/1000
2023-09-10 05:05:07.613 
Epoch 18/1000 
	 loss: 412.1691, MinusLogProbMetric: 412.1691, val_loss: 431.8379, val_MinusLogProbMetric: 431.8379

Epoch 18: val_loss did not improve from 411.27219
196/196 - 25s - loss: 412.1691 - MinusLogProbMetric: 412.1691 - val_loss: 431.8379 - val_MinusLogProbMetric: 431.8379 - lr: 3.3333e-04 - 25s/epoch - 129ms/step
Epoch 19/1000
2023-09-10 05:05:32.380 
Epoch 19/1000 
	 loss: 410.5606, MinusLogProbMetric: 410.5606, val_loss: 414.1357, val_MinusLogProbMetric: 414.1357

Epoch 19: val_loss did not improve from 411.27219
196/196 - 25s - loss: 410.5606 - MinusLogProbMetric: 410.5606 - val_loss: 414.1357 - val_MinusLogProbMetric: 414.1357 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 20/1000
2023-09-10 05:05:55.147 
Epoch 20/1000 
	 loss: 409.9532, MinusLogProbMetric: 409.9532, val_loss: 411.1274, val_MinusLogProbMetric: 411.1274

Epoch 20: val_loss improved from 411.27219 to 411.12741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 24s - loss: 409.9532 - MinusLogProbMetric: 409.9532 - val_loss: 411.1274 - val_MinusLogProbMetric: 411.1274 - lr: 3.3333e-04 - 24s/epoch - 121ms/step
Epoch 21/1000
2023-09-10 05:06:20.863 
Epoch 21/1000 
	 loss: 409.9169, MinusLogProbMetric: 409.9169, val_loss: 411.3544, val_MinusLogProbMetric: 411.3544

Epoch 21: val_loss did not improve from 411.12741
196/196 - 25s - loss: 409.9169 - MinusLogProbMetric: 409.9169 - val_loss: 411.3544 - val_MinusLogProbMetric: 411.3544 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 22/1000
2023-09-10 05:06:45.235 
Epoch 22/1000 
	 loss: 410.3149, MinusLogProbMetric: 410.3149, val_loss: 410.5829, val_MinusLogProbMetric: 410.5829

Epoch 22: val_loss improved from 411.12741 to 410.58289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 25s - loss: 410.3149 - MinusLogProbMetric: 410.3149 - val_loss: 410.5829 - val_MinusLogProbMetric: 410.5829 - lr: 3.3333e-04 - 25s/epoch - 128ms/step
Epoch 23/1000
2023-09-10 05:07:11.912 
Epoch 23/1000 
	 loss: 408.4640, MinusLogProbMetric: 408.4640, val_loss: 410.7127, val_MinusLogProbMetric: 410.7127

Epoch 23: val_loss did not improve from 410.58289
196/196 - 26s - loss: 408.4640 - MinusLogProbMetric: 408.4640 - val_loss: 410.7127 - val_MinusLogProbMetric: 410.7127 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 24/1000
2023-09-10 05:07:37.464 
Epoch 24/1000 
	 loss: 408.4752, MinusLogProbMetric: 408.4752, val_loss: 408.0731, val_MinusLogProbMetric: 408.0731

Epoch 24: val_loss improved from 410.58289 to 408.07312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 408.4752 - MinusLogProbMetric: 408.4752 - val_loss: 408.0731 - val_MinusLogProbMetric: 408.0731 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 25/1000
2023-09-10 05:08:03.506 
Epoch 25/1000 
	 loss: 409.0264, MinusLogProbMetric: 409.0264, val_loss: 411.3232, val_MinusLogProbMetric: 411.3232

Epoch 25: val_loss did not improve from 408.07312
196/196 - 25s - loss: 409.0264 - MinusLogProbMetric: 409.0264 - val_loss: 411.3232 - val_MinusLogProbMetric: 411.3232 - lr: 3.3333e-04 - 25s/epoch - 129ms/step
Epoch 26/1000
2023-09-10 05:08:28.263 
Epoch 26/1000 
	 loss: 407.7887, MinusLogProbMetric: 407.7887, val_loss: 417.9256, val_MinusLogProbMetric: 417.9256

Epoch 26: val_loss did not improve from 408.07312
196/196 - 25s - loss: 407.7887 - MinusLogProbMetric: 407.7887 - val_loss: 417.9256 - val_MinusLogProbMetric: 417.9256 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 27/1000
2023-09-10 05:08:53.251 
Epoch 27/1000 
	 loss: 408.9064, MinusLogProbMetric: 408.9064, val_loss: 410.7048, val_MinusLogProbMetric: 410.7048

Epoch 27: val_loss did not improve from 408.07312
196/196 - 25s - loss: 408.9064 - MinusLogProbMetric: 408.9064 - val_loss: 410.7048 - val_MinusLogProbMetric: 410.7048 - lr: 3.3333e-04 - 25s/epoch - 127ms/step
Epoch 28/1000
2023-09-10 05:09:17.496 
Epoch 28/1000 
	 loss: 407.4489, MinusLogProbMetric: 407.4489, val_loss: 408.6069, val_MinusLogProbMetric: 408.6069

Epoch 28: val_loss did not improve from 408.07312
196/196 - 24s - loss: 407.4489 - MinusLogProbMetric: 407.4489 - val_loss: 408.6069 - val_MinusLogProbMetric: 408.6069 - lr: 3.3333e-04 - 24s/epoch - 124ms/step
Epoch 29/1000
2023-09-10 05:09:44.284 
Epoch 29/1000 
	 loss: 407.3502, MinusLogProbMetric: 407.3502, val_loss: 409.3020, val_MinusLogProbMetric: 409.3020

Epoch 29: val_loss did not improve from 408.07312
196/196 - 27s - loss: 407.3502 - MinusLogProbMetric: 407.3502 - val_loss: 409.3020 - val_MinusLogProbMetric: 409.3020 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 30/1000
2023-09-10 05:10:08.421 
Epoch 30/1000 
	 loss: 406.2886, MinusLogProbMetric: 406.2886, val_loss: 405.7952, val_MinusLogProbMetric: 405.7952

Epoch 30: val_loss improved from 408.07312 to 405.79523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 406.2886 - MinusLogProbMetric: 406.2886 - val_loss: 405.7952 - val_MinusLogProbMetric: 405.7952 - lr: 3.3333e-04 - 26s/epoch - 130ms/step
Epoch 31/1000
2023-09-10 05:10:34.460 
Epoch 31/1000 
	 loss: 407.3652, MinusLogProbMetric: 407.3652, val_loss: 412.1928, val_MinusLogProbMetric: 412.1928

Epoch 31: val_loss did not improve from 405.79523
196/196 - 25s - loss: 407.3652 - MinusLogProbMetric: 407.3652 - val_loss: 412.1928 - val_MinusLogProbMetric: 412.1928 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 32/1000
2023-09-10 05:10:57.320 
Epoch 32/1000 
	 loss: 406.5500, MinusLogProbMetric: 406.5500, val_loss: 409.1127, val_MinusLogProbMetric: 409.1127

Epoch 32: val_loss did not improve from 405.79523
196/196 - 23s - loss: 406.5500 - MinusLogProbMetric: 406.5500 - val_loss: 409.1127 - val_MinusLogProbMetric: 409.1127 - lr: 3.3333e-04 - 23s/epoch - 117ms/step
Epoch 33/1000
2023-09-10 05:11:21.433 
Epoch 33/1000 
	 loss: 405.8190, MinusLogProbMetric: 405.8190, val_loss: 407.5387, val_MinusLogProbMetric: 407.5387

Epoch 33: val_loss did not improve from 405.79523
196/196 - 24s - loss: 405.8190 - MinusLogProbMetric: 405.8190 - val_loss: 407.5387 - val_MinusLogProbMetric: 407.5387 - lr: 3.3333e-04 - 24s/epoch - 123ms/step
Epoch 34/1000
2023-09-10 05:11:45.809 
Epoch 34/1000 
	 loss: 406.4046, MinusLogProbMetric: 406.4046, val_loss: 406.0522, val_MinusLogProbMetric: 406.0522

Epoch 34: val_loss did not improve from 405.79523
196/196 - 24s - loss: 406.4046 - MinusLogProbMetric: 406.4046 - val_loss: 406.0522 - val_MinusLogProbMetric: 406.0522 - lr: 3.3333e-04 - 24s/epoch - 124ms/step
Epoch 35/1000
2023-09-10 05:12:10.982 
Epoch 35/1000 
	 loss: 406.6514, MinusLogProbMetric: 406.6514, val_loss: 408.3306, val_MinusLogProbMetric: 408.3306

Epoch 35: val_loss did not improve from 405.79523
196/196 - 25s - loss: 406.6514 - MinusLogProbMetric: 406.6514 - val_loss: 408.3306 - val_MinusLogProbMetric: 408.3306 - lr: 3.3333e-04 - 25s/epoch - 128ms/step
Epoch 36/1000
2023-09-10 05:12:34.525 
Epoch 36/1000 
	 loss: 406.0617, MinusLogProbMetric: 406.0617, val_loss: 408.3186, val_MinusLogProbMetric: 408.3186

Epoch 36: val_loss did not improve from 405.79523
196/196 - 24s - loss: 406.0617 - MinusLogProbMetric: 406.0617 - val_loss: 408.3186 - val_MinusLogProbMetric: 408.3186 - lr: 3.3333e-04 - 24s/epoch - 120ms/step
Epoch 37/1000
2023-09-10 05:13:00.014 
Epoch 37/1000 
	 loss: 404.5973, MinusLogProbMetric: 404.5973, val_loss: 407.1213, val_MinusLogProbMetric: 407.1213

Epoch 37: val_loss did not improve from 405.79523
196/196 - 25s - loss: 404.5973 - MinusLogProbMetric: 404.5973 - val_loss: 407.1213 - val_MinusLogProbMetric: 407.1213 - lr: 3.3333e-04 - 25s/epoch - 130ms/step
Epoch 38/1000
2023-09-10 05:13:25.416 
Epoch 38/1000 
	 loss: 405.3543, MinusLogProbMetric: 405.3543, val_loss: 407.3619, val_MinusLogProbMetric: 407.3619

Epoch 38: val_loss did not improve from 405.79523
196/196 - 25s - loss: 405.3543 - MinusLogProbMetric: 405.3543 - val_loss: 407.3619 - val_MinusLogProbMetric: 407.3619 - lr: 3.3333e-04 - 25s/epoch - 130ms/step
Epoch 39/1000
2023-09-10 05:13:51.290 
Epoch 39/1000 
	 loss: 404.8402, MinusLogProbMetric: 404.8402, val_loss: 406.7163, val_MinusLogProbMetric: 406.7163

Epoch 39: val_loss did not improve from 405.79523
196/196 - 26s - loss: 404.8402 - MinusLogProbMetric: 404.8402 - val_loss: 406.7163 - val_MinusLogProbMetric: 406.7163 - lr: 3.3333e-04 - 26s/epoch - 132ms/step
Epoch 40/1000
2023-09-10 05:14:18.803 
Epoch 40/1000 
	 loss: 404.8832, MinusLogProbMetric: 404.8832, val_loss: 410.1868, val_MinusLogProbMetric: 410.1868

Epoch 40: val_loss did not improve from 405.79523
196/196 - 28s - loss: 404.8832 - MinusLogProbMetric: 404.8832 - val_loss: 410.1868 - val_MinusLogProbMetric: 410.1868 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 41/1000
2023-09-10 05:14:43.107 
Epoch 41/1000 
	 loss: 404.1407, MinusLogProbMetric: 404.1407, val_loss: 405.7607, val_MinusLogProbMetric: 405.7607

Epoch 41: val_loss improved from 405.79523 to 405.76068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 25s - loss: 404.1407 - MinusLogProbMetric: 404.1407 - val_loss: 405.7607 - val_MinusLogProbMetric: 405.7607 - lr: 3.3333e-04 - 25s/epoch - 129ms/step
Epoch 42/1000
2023-09-10 05:15:08.940 
Epoch 42/1000 
	 loss: 403.9042, MinusLogProbMetric: 403.9042, val_loss: 427.8866, val_MinusLogProbMetric: 427.8866

Epoch 42: val_loss did not improve from 405.76068
196/196 - 25s - loss: 403.9042 - MinusLogProbMetric: 403.9042 - val_loss: 427.8866 - val_MinusLogProbMetric: 427.8866 - lr: 3.3333e-04 - 25s/epoch - 127ms/step
Epoch 43/1000
2023-09-10 05:15:35.773 
Epoch 43/1000 
	 loss: 403.5173, MinusLogProbMetric: 403.5173, val_loss: 403.9941, val_MinusLogProbMetric: 403.9941

Epoch 43: val_loss improved from 405.76068 to 403.99408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 28s - loss: 403.5173 - MinusLogProbMetric: 403.5173 - val_loss: 403.9941 - val_MinusLogProbMetric: 403.9941 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 44/1000
2023-09-10 05:16:01.713 
Epoch 44/1000 
	 loss: 404.8846, MinusLogProbMetric: 404.8846, val_loss: 407.6464, val_MinusLogProbMetric: 407.6464

Epoch 44: val_loss did not improve from 403.99408
196/196 - 25s - loss: 404.8846 - MinusLogProbMetric: 404.8846 - val_loss: 407.6464 - val_MinusLogProbMetric: 407.6464 - lr: 3.3333e-04 - 25s/epoch - 125ms/step
Epoch 45/1000
2023-09-10 05:16:27.513 
Epoch 45/1000 
	 loss: 403.1204, MinusLogProbMetric: 403.1204, val_loss: 407.5510, val_MinusLogProbMetric: 407.5510

Epoch 45: val_loss did not improve from 403.99408
196/196 - 26s - loss: 403.1204 - MinusLogProbMetric: 403.1204 - val_loss: 407.5510 - val_MinusLogProbMetric: 407.5510 - lr: 3.3333e-04 - 26s/epoch - 131ms/step
Epoch 46/1000
2023-09-10 05:16:53.626 
Epoch 46/1000 
	 loss: 404.3739, MinusLogProbMetric: 404.3739, val_loss: 406.0691, val_MinusLogProbMetric: 406.0691

Epoch 46: val_loss did not improve from 403.99408
196/196 - 26s - loss: 404.3739 - MinusLogProbMetric: 404.3739 - val_loss: 406.0691 - val_MinusLogProbMetric: 406.0691 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 47/1000
2023-09-10 05:17:19.710 
Epoch 47/1000 
	 loss: 402.9217, MinusLogProbMetric: 402.9217, val_loss: 403.9279, val_MinusLogProbMetric: 403.9279

Epoch 47: val_loss improved from 403.99408 to 403.92786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 402.9217 - MinusLogProbMetric: 402.9217 - val_loss: 403.9279 - val_MinusLogProbMetric: 403.9279 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 48/1000
2023-09-10 05:17:46.429 
Epoch 48/1000 
	 loss: 403.3811, MinusLogProbMetric: 403.3811, val_loss: 407.3418, val_MinusLogProbMetric: 407.3418

Epoch 48: val_loss did not improve from 403.92786
196/196 - 26s - loss: 403.3811 - MinusLogProbMetric: 403.3811 - val_loss: 407.3418 - val_MinusLogProbMetric: 407.3418 - lr: 3.3333e-04 - 26s/epoch - 131ms/step
Epoch 49/1000
2023-09-10 05:18:11.203 
Epoch 49/1000 
	 loss: 402.8725, MinusLogProbMetric: 402.8725, val_loss: 404.1097, val_MinusLogProbMetric: 404.1097

Epoch 49: val_loss did not improve from 403.92786
196/196 - 25s - loss: 402.8725 - MinusLogProbMetric: 402.8725 - val_loss: 404.1097 - val_MinusLogProbMetric: 404.1097 - lr: 3.3333e-04 - 25s/epoch - 126ms/step
Epoch 50/1000
2023-09-10 05:18:36.411 
Epoch 50/1000 
	 loss: 402.5038, MinusLogProbMetric: 402.5038, val_loss: 403.8943, val_MinusLogProbMetric: 403.8943

Epoch 50: val_loss improved from 403.92786 to 403.89435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 26s - loss: 402.5038 - MinusLogProbMetric: 402.5038 - val_loss: 403.8943 - val_MinusLogProbMetric: 403.8943 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 51/1000
2023-09-10 05:19:04.870 
Epoch 51/1000 
	 loss: 404.4675, MinusLogProbMetric: 404.4675, val_loss: 406.7694, val_MinusLogProbMetric: 406.7694

Epoch 51: val_loss did not improve from 403.89435
196/196 - 27s - loss: 404.4675 - MinusLogProbMetric: 404.4675 - val_loss: 406.7694 - val_MinusLogProbMetric: 406.7694 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 52/1000
2023-09-10 05:19:29.902 
Epoch 52/1000 
	 loss: 402.0338, MinusLogProbMetric: 402.0338, val_loss: 404.2432, val_MinusLogProbMetric: 404.2432

Epoch 52: val_loss did not improve from 403.89435
196/196 - 25s - loss: 402.0338 - MinusLogProbMetric: 402.0338 - val_loss: 404.2432 - val_MinusLogProbMetric: 404.2432 - lr: 3.3333e-04 - 25s/epoch - 128ms/step
Epoch 53/1000
2023-09-10 05:19:55.010 
Epoch 53/1000 
	 loss: 401.9500, MinusLogProbMetric: 401.9500, val_loss: 410.2256, val_MinusLogProbMetric: 410.2256

Epoch 53: val_loss did not improve from 403.89435
196/196 - 25s - loss: 401.9500 - MinusLogProbMetric: 401.9500 - val_loss: 410.2256 - val_MinusLogProbMetric: 410.2256 - lr: 3.3333e-04 - 25s/epoch - 128ms/step
Epoch 54/1000
2023-09-10 05:20:19.388 
Epoch 54/1000 
	 loss: 401.8507, MinusLogProbMetric: 401.8507, val_loss: 405.2737, val_MinusLogProbMetric: 405.2737

Epoch 54: val_loss did not improve from 403.89435
196/196 - 24s - loss: 401.8507 - MinusLogProbMetric: 401.8507 - val_loss: 405.2737 - val_MinusLogProbMetric: 405.2737 - lr: 3.3333e-04 - 24s/epoch - 125ms/step
Epoch 55/1000
2023-09-10 05:20:45.420 
Epoch 55/1000 
	 loss: 401.9908, MinusLogProbMetric: 401.9908, val_loss: 405.6768, val_MinusLogProbMetric: 405.6768

Epoch 55: val_loss did not improve from 403.89435
196/196 - 26s - loss: 401.9908 - MinusLogProbMetric: 401.9908 - val_loss: 405.6768 - val_MinusLogProbMetric: 405.6768 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 56/1000
2023-09-10 05:21:10.244 
Epoch 56/1000 
	 loss: 402.3689, MinusLogProbMetric: 402.3689, val_loss: 411.5384, val_MinusLogProbMetric: 411.5384

Epoch 56: val_loss did not improve from 403.89435
196/196 - 25s - loss: 402.3689 - MinusLogProbMetric: 402.3689 - val_loss: 411.5384 - val_MinusLogProbMetric: 411.5384 - lr: 3.3333e-04 - 25s/epoch - 127ms/step
Epoch 57/1000
2023-09-10 05:21:35.400 
Epoch 57/1000 
	 loss: 401.7237, MinusLogProbMetric: 401.7237, val_loss: 405.0782, val_MinusLogProbMetric: 405.0782

Epoch 57: val_loss did not improve from 403.89435
196/196 - 25s - loss: 401.7237 - MinusLogProbMetric: 401.7237 - val_loss: 405.0782 - val_MinusLogProbMetric: 405.0782 - lr: 3.3333e-04 - 25s/epoch - 128ms/step
Epoch 58/1000
2023-09-10 05:21:59.081 
Epoch 58/1000 
	 loss: 402.2790, MinusLogProbMetric: 402.2790, val_loss: 405.1230, val_MinusLogProbMetric: 405.1230

Epoch 58: val_loss did not improve from 403.89435
196/196 - 24s - loss: 402.2790 - MinusLogProbMetric: 402.2790 - val_loss: 405.1230 - val_MinusLogProbMetric: 405.1230 - lr: 3.3333e-04 - 24s/epoch - 121ms/step
Epoch 59/1000
2023-09-10 05:22:23.518 
Epoch 59/1000 
	 loss: 400.9234, MinusLogProbMetric: 400.9234, val_loss: 405.7551, val_MinusLogProbMetric: 405.7551

Epoch 59: val_loss did not improve from 403.89435
196/196 - 24s - loss: 400.9234 - MinusLogProbMetric: 400.9234 - val_loss: 405.7551 - val_MinusLogProbMetric: 405.7551 - lr: 3.3333e-04 - 24s/epoch - 125ms/step
Epoch 60/1000
2023-09-10 05:22:47.911 
Epoch 60/1000 
	 loss: 403.2001, MinusLogProbMetric: 403.2001, val_loss: 404.6245, val_MinusLogProbMetric: 404.6245

Epoch 60: val_loss did not improve from 403.89435
196/196 - 24s - loss: 403.2001 - MinusLogProbMetric: 403.2001 - val_loss: 404.6245 - val_MinusLogProbMetric: 404.6245 - lr: 3.3333e-04 - 24s/epoch - 124ms/step
Epoch 61/1000
2023-09-10 05:23:13.009 
Epoch 61/1000 
	 loss: 401.0691, MinusLogProbMetric: 401.0691, val_loss: 403.8380, val_MinusLogProbMetric: 403.8380

Epoch 61: val_loss improved from 403.89435 to 403.83795, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 27s - loss: 401.0691 - MinusLogProbMetric: 401.0691 - val_loss: 403.8380 - val_MinusLogProbMetric: 403.8380 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 62/1000
2023-09-10 05:23:38.752 
Epoch 62/1000 
	 loss: 401.9861, MinusLogProbMetric: 401.9861, val_loss: 409.8549, val_MinusLogProbMetric: 409.8549

Epoch 62: val_loss did not improve from 403.83795
196/196 - 24s - loss: 401.9861 - MinusLogProbMetric: 401.9861 - val_loss: 409.8549 - val_MinusLogProbMetric: 409.8549 - lr: 3.3333e-04 - 24s/epoch - 123ms/step
Epoch 63/1000
2023-09-10 05:24:03.334 
Epoch 63/1000 
	 loss: 401.1712, MinusLogProbMetric: 401.1712, val_loss: 408.3391, val_MinusLogProbMetric: 408.3391

Epoch 63: val_loss did not improve from 403.83795
196/196 - 25s - loss: 401.1712 - MinusLogProbMetric: 401.1712 - val_loss: 408.3391 - val_MinusLogProbMetric: 408.3391 - lr: 3.3333e-04 - 25s/epoch - 125ms/step
Epoch 64/1000
2023-09-10 05:24:28.377 
Epoch 64/1000 
	 loss: 400.5075, MinusLogProbMetric: 400.5075, val_loss: 405.7057, val_MinusLogProbMetric: 405.7057

Epoch 64: val_loss did not improve from 403.83795
196/196 - 25s - loss: 400.5075 - MinusLogProbMetric: 400.5075 - val_loss: 405.7057 - val_MinusLogProbMetric: 405.7057 - lr: 3.3333e-04 - 25s/epoch - 128ms/step
Epoch 65/1000
2023-09-10 05:24:55.372 
Epoch 65/1000 
	 loss: 400.6324, MinusLogProbMetric: 400.6324, val_loss: 404.9038, val_MinusLogProbMetric: 404.9038

Epoch 65: val_loss did not improve from 403.83795
196/196 - 27s - loss: 400.6324 - MinusLogProbMetric: 400.6324 - val_loss: 404.9038 - val_MinusLogProbMetric: 404.9038 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 66/1000
2023-09-10 05:25:17.207 
Epoch 66/1000 
	 loss: 400.2579, MinusLogProbMetric: 400.2579, val_loss: 404.0087, val_MinusLogProbMetric: 404.0087

Epoch 66: val_loss did not improve from 403.83795
196/196 - 22s - loss: 400.2579 - MinusLogProbMetric: 400.2579 - val_loss: 404.0087 - val_MinusLogProbMetric: 404.0087 - lr: 3.3333e-04 - 22s/epoch - 111ms/step
Epoch 67/1000
2023-09-10 05:25:26.274 
Epoch 67/1000 
	 loss: 401.0165, MinusLogProbMetric: 401.0165, val_loss: 405.0376, val_MinusLogProbMetric: 405.0376

Epoch 67: val_loss did not improve from 403.83795
196/196 - 9s - loss: 401.0165 - MinusLogProbMetric: 401.0165 - val_loss: 405.0376 - val_MinusLogProbMetric: 405.0376 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 68/1000
2023-09-10 05:25:37.686 
Epoch 68/1000 
	 loss: 400.5496, MinusLogProbMetric: 400.5496, val_loss: 407.1147, val_MinusLogProbMetric: 407.1147

Epoch 68: val_loss did not improve from 403.83795
196/196 - 11s - loss: 400.5496 - MinusLogProbMetric: 400.5496 - val_loss: 407.1147 - val_MinusLogProbMetric: 407.1147 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 69/1000
2023-09-10 05:25:56.353 
Epoch 69/1000 
	 loss: 400.4565, MinusLogProbMetric: 400.4565, val_loss: 404.6159, val_MinusLogProbMetric: 404.6159

Epoch 69: val_loss did not improve from 403.83795
196/196 - 19s - loss: 400.4565 - MinusLogProbMetric: 400.4565 - val_loss: 404.6159 - val_MinusLogProbMetric: 404.6159 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 70/1000
2023-09-10 05:26:16.926 
Epoch 70/1000 
	 loss: 400.0336, MinusLogProbMetric: 400.0336, val_loss: 403.1985, val_MinusLogProbMetric: 403.1985

Epoch 70: val_loss improved from 403.83795 to 403.19846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 21s - loss: 400.0336 - MinusLogProbMetric: 400.0336 - val_loss: 403.1985 - val_MinusLogProbMetric: 403.1985 - lr: 3.3333e-04 - 21s/epoch - 109ms/step
Epoch 71/1000
2023-09-10 05:26:37.146 
Epoch 71/1000 
	 loss: 400.4494, MinusLogProbMetric: 400.4494, val_loss: 403.8521, val_MinusLogProbMetric: 403.8521

Epoch 71: val_loss did not improve from 403.19846
196/196 - 19s - loss: 400.4494 - MinusLogProbMetric: 400.4494 - val_loss: 403.8521 - val_MinusLogProbMetric: 403.8521 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 72/1000
2023-09-10 05:26:56.653 
Epoch 72/1000 
	 loss: 400.4295, MinusLogProbMetric: 400.4295, val_loss: 404.6309, val_MinusLogProbMetric: 404.6309

Epoch 72: val_loss did not improve from 403.19846
196/196 - 20s - loss: 400.4295 - MinusLogProbMetric: 400.4295 - val_loss: 404.6309 - val_MinusLogProbMetric: 404.6309 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 73/1000
2023-09-10 05:27:16.532 
Epoch 73/1000 
	 loss: 400.0321, MinusLogProbMetric: 400.0321, val_loss: 411.0863, val_MinusLogProbMetric: 411.0863

Epoch 73: val_loss did not improve from 403.19846
196/196 - 20s - loss: 400.0321 - MinusLogProbMetric: 400.0321 - val_loss: 411.0863 - val_MinusLogProbMetric: 411.0863 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 74/1000
2023-09-10 05:27:35.830 
Epoch 74/1000 
	 loss: 400.2094, MinusLogProbMetric: 400.2094, val_loss: 407.6045, val_MinusLogProbMetric: 407.6045

Epoch 74: val_loss did not improve from 403.19846
196/196 - 19s - loss: 400.2094 - MinusLogProbMetric: 400.2094 - val_loss: 407.6045 - val_MinusLogProbMetric: 407.6045 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 75/1000
2023-09-10 05:27:53.636 
Epoch 75/1000 
	 loss: 399.8304, MinusLogProbMetric: 399.8304, val_loss: 405.0332, val_MinusLogProbMetric: 405.0332

Epoch 75: val_loss did not improve from 403.19846
196/196 - 18s - loss: 399.8304 - MinusLogProbMetric: 399.8304 - val_loss: 405.0332 - val_MinusLogProbMetric: 405.0332 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 76/1000
2023-09-10 05:28:11.537 
Epoch 76/1000 
	 loss: 400.0298, MinusLogProbMetric: 400.0298, val_loss: 404.2009, val_MinusLogProbMetric: 404.2009

Epoch 76: val_loss did not improve from 403.19846
196/196 - 18s - loss: 400.0298 - MinusLogProbMetric: 400.0298 - val_loss: 404.2009 - val_MinusLogProbMetric: 404.2009 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 77/1000
2023-09-10 05:28:29.669 
Epoch 77/1000 
	 loss: 399.6960, MinusLogProbMetric: 399.6960, val_loss: 403.2992, val_MinusLogProbMetric: 403.2992

Epoch 77: val_loss did not improve from 403.19846
196/196 - 18s - loss: 399.6960 - MinusLogProbMetric: 399.6960 - val_loss: 403.2992 - val_MinusLogProbMetric: 403.2992 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 78/1000
2023-09-10 05:28:47.793 
Epoch 78/1000 
	 loss: 399.5532, MinusLogProbMetric: 399.5532, val_loss: 404.5540, val_MinusLogProbMetric: 404.5540

Epoch 78: val_loss did not improve from 403.19846
196/196 - 18s - loss: 399.5532 - MinusLogProbMetric: 399.5532 - val_loss: 404.5540 - val_MinusLogProbMetric: 404.5540 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 79/1000
2023-09-10 05:29:05.373 
Epoch 79/1000 
	 loss: 398.9081, MinusLogProbMetric: 398.9081, val_loss: 405.1885, val_MinusLogProbMetric: 405.1885

Epoch 79: val_loss did not improve from 403.19846
196/196 - 18s - loss: 398.9081 - MinusLogProbMetric: 398.9081 - val_loss: 405.1885 - val_MinusLogProbMetric: 405.1885 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 80/1000
2023-09-10 05:29:23.935 
Epoch 80/1000 
	 loss: 399.9294, MinusLogProbMetric: 399.9294, val_loss: 406.3225, val_MinusLogProbMetric: 406.3225

Epoch 80: val_loss did not improve from 403.19846
196/196 - 19s - loss: 399.9294 - MinusLogProbMetric: 399.9294 - val_loss: 406.3225 - val_MinusLogProbMetric: 406.3225 - lr: 3.3333e-04 - 19s/epoch - 94ms/step
Epoch 81/1000
2023-09-10 05:29:41.169 
Epoch 81/1000 
	 loss: 399.8217, MinusLogProbMetric: 399.8217, val_loss: 405.1349, val_MinusLogProbMetric: 405.1349

Epoch 81: val_loss did not improve from 403.19846
196/196 - 17s - loss: 399.8217 - MinusLogProbMetric: 399.8217 - val_loss: 405.1349 - val_MinusLogProbMetric: 405.1349 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 82/1000
2023-09-10 05:29:59.382 
Epoch 82/1000 
	 loss: 398.9099, MinusLogProbMetric: 398.9099, val_loss: 408.3402, val_MinusLogProbMetric: 408.3402

Epoch 82: val_loss did not improve from 403.19846
196/196 - 18s - loss: 398.9099 - MinusLogProbMetric: 398.9099 - val_loss: 408.3402 - val_MinusLogProbMetric: 408.3402 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 83/1000
2023-09-10 05:30:18.086 
Epoch 83/1000 
	 loss: 398.6082, MinusLogProbMetric: 398.6082, val_loss: 402.6643, val_MinusLogProbMetric: 402.6643

Epoch 83: val_loss improved from 403.19846 to 402.66431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 19s - loss: 398.6082 - MinusLogProbMetric: 398.6082 - val_loss: 402.6643 - val_MinusLogProbMetric: 402.6643 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 84/1000
2023-09-10 05:30:38.644 
Epoch 84/1000 
	 loss: 399.3638, MinusLogProbMetric: 399.3638, val_loss: 403.8345, val_MinusLogProbMetric: 403.8345

Epoch 84: val_loss did not improve from 402.66431
196/196 - 20s - loss: 399.3638 - MinusLogProbMetric: 399.3638 - val_loss: 403.8345 - val_MinusLogProbMetric: 403.8345 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 85/1000
2023-09-10 05:30:56.879 
Epoch 85/1000 
	 loss: 399.0143, MinusLogProbMetric: 399.0143, val_loss: 403.8648, val_MinusLogProbMetric: 403.8648

Epoch 85: val_loss did not improve from 402.66431
196/196 - 18s - loss: 399.0143 - MinusLogProbMetric: 399.0143 - val_loss: 403.8648 - val_MinusLogProbMetric: 403.8648 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 86/1000
2023-09-10 05:31:13.644 
Epoch 86/1000 
	 loss: 398.6107, MinusLogProbMetric: 398.6107, val_loss: 403.3482, val_MinusLogProbMetric: 403.3482

Epoch 86: val_loss did not improve from 402.66431
196/196 - 17s - loss: 398.6107 - MinusLogProbMetric: 398.6107 - val_loss: 403.3482 - val_MinusLogProbMetric: 403.3482 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 87/1000
2023-09-10 05:31:31.308 
Epoch 87/1000 
	 loss: 398.8378, MinusLogProbMetric: 398.8378, val_loss: 407.3702, val_MinusLogProbMetric: 407.3702

Epoch 87: val_loss did not improve from 402.66431
196/196 - 18s - loss: 398.8378 - MinusLogProbMetric: 398.8378 - val_loss: 407.3702 - val_MinusLogProbMetric: 407.3702 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 88/1000
2023-09-10 05:31:48.728 
Epoch 88/1000 
	 loss: 397.8507, MinusLogProbMetric: 397.8507, val_loss: 408.0188, val_MinusLogProbMetric: 408.0188

Epoch 88: val_loss did not improve from 402.66431
196/196 - 17s - loss: 397.8507 - MinusLogProbMetric: 397.8507 - val_loss: 408.0188 - val_MinusLogProbMetric: 408.0188 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 89/1000
2023-09-10 05:32:06.381 
Epoch 89/1000 
	 loss: 398.1988, MinusLogProbMetric: 398.1988, val_loss: 426.4132, val_MinusLogProbMetric: 426.4132

Epoch 89: val_loss did not improve from 402.66431
196/196 - 18s - loss: 398.1988 - MinusLogProbMetric: 398.1988 - val_loss: 426.4132 - val_MinusLogProbMetric: 426.4132 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 90/1000
2023-09-10 05:32:23.686 
Epoch 90/1000 
	 loss: 399.0567, MinusLogProbMetric: 399.0567, val_loss: 405.0720, val_MinusLogProbMetric: 405.0720

Epoch 90: val_loss did not improve from 402.66431
196/196 - 17s - loss: 399.0567 - MinusLogProbMetric: 399.0567 - val_loss: 405.0720 - val_MinusLogProbMetric: 405.0720 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 91/1000
2023-09-10 05:32:42.525 
Epoch 91/1000 
	 loss: 397.8995, MinusLogProbMetric: 397.8995, val_loss: 408.0846, val_MinusLogProbMetric: 408.0846

Epoch 91: val_loss did not improve from 402.66431
196/196 - 19s - loss: 397.8995 - MinusLogProbMetric: 397.8995 - val_loss: 408.0846 - val_MinusLogProbMetric: 408.0846 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 92/1000
2023-09-10 05:33:01.430 
Epoch 92/1000 
	 loss: 398.0369, MinusLogProbMetric: 398.0369, val_loss: 406.8883, val_MinusLogProbMetric: 406.8883

Epoch 92: val_loss did not improve from 402.66431
196/196 - 19s - loss: 398.0369 - MinusLogProbMetric: 398.0369 - val_loss: 406.8883 - val_MinusLogProbMetric: 406.8883 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 93/1000
2023-09-10 05:33:18.949 
Epoch 93/1000 
	 loss: 398.0964, MinusLogProbMetric: 398.0964, val_loss: 403.9646, val_MinusLogProbMetric: 403.9646

Epoch 93: val_loss did not improve from 402.66431
196/196 - 18s - loss: 398.0964 - MinusLogProbMetric: 398.0964 - val_loss: 403.9646 - val_MinusLogProbMetric: 403.9646 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 94/1000
2023-09-10 05:33:36.367 
Epoch 94/1000 
	 loss: 397.9370, MinusLogProbMetric: 397.9370, val_loss: 405.6334, val_MinusLogProbMetric: 405.6334

Epoch 94: val_loss did not improve from 402.66431
196/196 - 17s - loss: 397.9370 - MinusLogProbMetric: 397.9370 - val_loss: 405.6334 - val_MinusLogProbMetric: 405.6334 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 95/1000
2023-09-10 05:33:53.689 
Epoch 95/1000 
	 loss: 398.3466, MinusLogProbMetric: 398.3466, val_loss: 409.2829, val_MinusLogProbMetric: 409.2829

Epoch 95: val_loss did not improve from 402.66431
196/196 - 17s - loss: 398.3466 - MinusLogProbMetric: 398.3466 - val_loss: 409.2829 - val_MinusLogProbMetric: 409.2829 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 96/1000
2023-09-10 05:34:11.330 
Epoch 96/1000 
	 loss: 398.3451, MinusLogProbMetric: 398.3451, val_loss: 404.4178, val_MinusLogProbMetric: 404.4178

Epoch 96: val_loss did not improve from 402.66431
196/196 - 18s - loss: 398.3451 - MinusLogProbMetric: 398.3451 - val_loss: 404.4178 - val_MinusLogProbMetric: 404.4178 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 97/1000
2023-09-10 05:34:28.030 
Epoch 97/1000 
	 loss: 397.4420, MinusLogProbMetric: 397.4420, val_loss: 409.5957, val_MinusLogProbMetric: 409.5957

Epoch 97: val_loss did not improve from 402.66431
196/196 - 17s - loss: 397.4420 - MinusLogProbMetric: 397.4420 - val_loss: 409.5957 - val_MinusLogProbMetric: 409.5957 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 98/1000
2023-09-10 05:34:44.229 
Epoch 98/1000 
	 loss: 397.6701, MinusLogProbMetric: 397.6701, val_loss: 403.9193, val_MinusLogProbMetric: 403.9193

Epoch 98: val_loss did not improve from 402.66431
196/196 - 16s - loss: 397.6701 - MinusLogProbMetric: 397.6701 - val_loss: 403.9193 - val_MinusLogProbMetric: 403.9193 - lr: 3.3333e-04 - 16s/epoch - 83ms/step
Epoch 99/1000
2023-09-10 05:35:01.325 
Epoch 99/1000 
	 loss: 397.7010, MinusLogProbMetric: 397.7010, val_loss: 402.4576, val_MinusLogProbMetric: 402.4576

Epoch 99: val_loss improved from 402.66431 to 402.45764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 18s - loss: 397.7010 - MinusLogProbMetric: 397.7010 - val_loss: 402.4576 - val_MinusLogProbMetric: 402.4576 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 100/1000
2023-09-10 05:35:19.318 
Epoch 100/1000 
	 loss: 397.3669, MinusLogProbMetric: 397.3669, val_loss: 406.7762, val_MinusLogProbMetric: 406.7762

Epoch 100: val_loss did not improve from 402.45764
196/196 - 17s - loss: 397.3669 - MinusLogProbMetric: 397.3669 - val_loss: 406.7762 - val_MinusLogProbMetric: 406.7762 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 101/1000
2023-09-10 05:35:36.957 
Epoch 101/1000 
	 loss: 397.7226, MinusLogProbMetric: 397.7226, val_loss: 403.8974, val_MinusLogProbMetric: 403.8974

Epoch 101: val_loss did not improve from 402.45764
196/196 - 18s - loss: 397.7226 - MinusLogProbMetric: 397.7226 - val_loss: 403.8974 - val_MinusLogProbMetric: 403.8974 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 102/1000
2023-09-10 05:35:53.875 
Epoch 102/1000 
	 loss: 396.9865, MinusLogProbMetric: 396.9865, val_loss: 405.2470, val_MinusLogProbMetric: 405.2470

Epoch 102: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.9865 - MinusLogProbMetric: 396.9865 - val_loss: 405.2470 - val_MinusLogProbMetric: 405.2470 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 103/1000
2023-09-10 05:36:11.053 
Epoch 103/1000 
	 loss: 397.2894, MinusLogProbMetric: 397.2894, val_loss: 405.5155, val_MinusLogProbMetric: 405.5155

Epoch 103: val_loss did not improve from 402.45764
196/196 - 17s - loss: 397.2894 - MinusLogProbMetric: 397.2894 - val_loss: 405.5155 - val_MinusLogProbMetric: 405.5155 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 104/1000
2023-09-10 05:36:27.168 
Epoch 104/1000 
	 loss: 397.7366, MinusLogProbMetric: 397.7366, val_loss: 407.6068, val_MinusLogProbMetric: 407.6068

Epoch 104: val_loss did not improve from 402.45764
196/196 - 16s - loss: 397.7366 - MinusLogProbMetric: 397.7366 - val_loss: 407.6068 - val_MinusLogProbMetric: 407.6068 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 105/1000
2023-09-10 05:36:44.638 
Epoch 105/1000 
	 loss: 397.1297, MinusLogProbMetric: 397.1297, val_loss: 407.7467, val_MinusLogProbMetric: 407.7467

Epoch 105: val_loss did not improve from 402.45764
196/196 - 17s - loss: 397.1297 - MinusLogProbMetric: 397.1297 - val_loss: 407.7467 - val_MinusLogProbMetric: 407.7467 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 106/1000
2023-09-10 05:37:02.020 
Epoch 106/1000 
	 loss: 396.3625, MinusLogProbMetric: 396.3625, val_loss: 408.6623, val_MinusLogProbMetric: 408.6623

Epoch 106: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.3625 - MinusLogProbMetric: 396.3625 - val_loss: 408.6623 - val_MinusLogProbMetric: 408.6623 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 107/1000
2023-09-10 05:37:19.226 
Epoch 107/1000 
	 loss: 397.1376, MinusLogProbMetric: 397.1376, val_loss: 405.9918, val_MinusLogProbMetric: 405.9918

Epoch 107: val_loss did not improve from 402.45764
196/196 - 17s - loss: 397.1376 - MinusLogProbMetric: 397.1376 - val_loss: 405.9918 - val_MinusLogProbMetric: 405.9918 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 108/1000
2023-09-10 05:37:37.576 
Epoch 108/1000 
	 loss: 398.9967, MinusLogProbMetric: 398.9967, val_loss: 403.2555, val_MinusLogProbMetric: 403.2555

Epoch 108: val_loss did not improve from 402.45764
196/196 - 18s - loss: 398.9967 - MinusLogProbMetric: 398.9967 - val_loss: 403.2555 - val_MinusLogProbMetric: 403.2555 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 109/1000
2023-09-10 05:37:54.965 
Epoch 109/1000 
	 loss: 396.3331, MinusLogProbMetric: 396.3331, val_loss: 404.7509, val_MinusLogProbMetric: 404.7509

Epoch 109: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.3331 - MinusLogProbMetric: 396.3331 - val_loss: 404.7509 - val_MinusLogProbMetric: 404.7509 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 110/1000
2023-09-10 05:38:13.438 
Epoch 110/1000 
	 loss: 396.2035, MinusLogProbMetric: 396.2035, val_loss: 406.1446, val_MinusLogProbMetric: 406.1446

Epoch 110: val_loss did not improve from 402.45764
196/196 - 18s - loss: 396.2035 - MinusLogProbMetric: 396.2035 - val_loss: 406.1446 - val_MinusLogProbMetric: 406.1446 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 111/1000
2023-09-10 05:38:30.567 
Epoch 111/1000 
	 loss: 396.3569, MinusLogProbMetric: 396.3569, val_loss: 407.6937, val_MinusLogProbMetric: 407.6937

Epoch 111: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.3569 - MinusLogProbMetric: 396.3569 - val_loss: 407.6937 - val_MinusLogProbMetric: 407.6937 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 112/1000
2023-09-10 05:38:48.000 
Epoch 112/1000 
	 loss: 396.4811, MinusLogProbMetric: 396.4811, val_loss: 412.3930, val_MinusLogProbMetric: 412.3930

Epoch 112: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.4811 - MinusLogProbMetric: 396.4811 - val_loss: 412.3930 - val_MinusLogProbMetric: 412.3930 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 113/1000
2023-09-10 05:39:04.842 
Epoch 113/1000 
	 loss: 396.5803, MinusLogProbMetric: 396.5803, val_loss: 404.7453, val_MinusLogProbMetric: 404.7453

Epoch 113: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.5803 - MinusLogProbMetric: 396.5803 - val_loss: 404.7453 - val_MinusLogProbMetric: 404.7453 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 114/1000
2023-09-10 05:39:22.277 
Epoch 114/1000 
	 loss: 396.0924, MinusLogProbMetric: 396.0924, val_loss: 403.7242, val_MinusLogProbMetric: 403.7242

Epoch 114: val_loss did not improve from 402.45764
196/196 - 17s - loss: 396.0924 - MinusLogProbMetric: 396.0924 - val_loss: 403.7242 - val_MinusLogProbMetric: 403.7242 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 115/1000
2023-09-10 05:39:41.130 
Epoch 115/1000 
	 loss: 395.9690, MinusLogProbMetric: 395.9690, val_loss: 404.6934, val_MinusLogProbMetric: 404.6934

Epoch 115: val_loss did not improve from 402.45764
196/196 - 19s - loss: 395.9690 - MinusLogProbMetric: 395.9690 - val_loss: 404.6934 - val_MinusLogProbMetric: 404.6934 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 116/1000
2023-09-10 05:39:59.164 
Epoch 116/1000 
	 loss: 396.5746, MinusLogProbMetric: 396.5746, val_loss: 404.2093, val_MinusLogProbMetric: 404.2093

Epoch 116: val_loss did not improve from 402.45764
196/196 - 18s - loss: 396.5746 - MinusLogProbMetric: 396.5746 - val_loss: 404.2093 - val_MinusLogProbMetric: 404.2093 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 117/1000
2023-09-10 05:40:17.043 
Epoch 117/1000 
	 loss: 396.6851, MinusLogProbMetric: 396.6851, val_loss: 406.5812, val_MinusLogProbMetric: 406.5812

Epoch 117: val_loss did not improve from 402.45764
196/196 - 18s - loss: 396.6851 - MinusLogProbMetric: 396.6851 - val_loss: 406.5812 - val_MinusLogProbMetric: 406.5812 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 118/1000
2023-09-10 05:40:34.739 
Epoch 118/1000 
	 loss: 395.9611, MinusLogProbMetric: 395.9611, val_loss: 408.9643, val_MinusLogProbMetric: 408.9643

Epoch 118: val_loss did not improve from 402.45764
196/196 - 18s - loss: 395.9611 - MinusLogProbMetric: 395.9611 - val_loss: 408.9643 - val_MinusLogProbMetric: 408.9643 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 119/1000
2023-09-10 05:40:52.208 
Epoch 119/1000 
	 loss: 398.0171, MinusLogProbMetric: 398.0171, val_loss: 404.7516, val_MinusLogProbMetric: 404.7516

Epoch 119: val_loss did not improve from 402.45764
196/196 - 17s - loss: 398.0171 - MinusLogProbMetric: 398.0171 - val_loss: 404.7516 - val_MinusLogProbMetric: 404.7516 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 120/1000
2023-09-10 05:41:10.433 
Epoch 120/1000 
	 loss: 395.1273, MinusLogProbMetric: 395.1273, val_loss: 406.2178, val_MinusLogProbMetric: 406.2178

Epoch 120: val_loss did not improve from 402.45764
196/196 - 18s - loss: 395.1273 - MinusLogProbMetric: 395.1273 - val_loss: 406.2178 - val_MinusLogProbMetric: 406.2178 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 121/1000
2023-09-10 05:41:29.048 
Epoch 121/1000 
	 loss: 397.2601, MinusLogProbMetric: 397.2601, val_loss: 405.1405, val_MinusLogProbMetric: 405.1405

Epoch 121: val_loss did not improve from 402.45764
196/196 - 19s - loss: 397.2601 - MinusLogProbMetric: 397.2601 - val_loss: 405.1405 - val_MinusLogProbMetric: 405.1405 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 122/1000
2023-09-10 05:41:47.609 
Epoch 122/1000 
	 loss: 396.5074, MinusLogProbMetric: 396.5074, val_loss: 404.3207, val_MinusLogProbMetric: 404.3207

Epoch 122: val_loss did not improve from 402.45764
196/196 - 19s - loss: 396.5074 - MinusLogProbMetric: 396.5074 - val_loss: 404.3207 - val_MinusLogProbMetric: 404.3207 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 123/1000
2023-09-10 05:42:05.744 
Epoch 123/1000 
	 loss: 394.9380, MinusLogProbMetric: 394.9380, val_loss: 403.0832, val_MinusLogProbMetric: 403.0832

Epoch 123: val_loss did not improve from 402.45764
196/196 - 18s - loss: 394.9380 - MinusLogProbMetric: 394.9380 - val_loss: 403.0832 - val_MinusLogProbMetric: 403.0832 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 124/1000
2023-09-10 05:42:24.564 
Epoch 124/1000 
	 loss: 395.0084, MinusLogProbMetric: 395.0084, val_loss: 404.4260, val_MinusLogProbMetric: 404.4260

Epoch 124: val_loss did not improve from 402.45764
196/196 - 19s - loss: 395.0084 - MinusLogProbMetric: 395.0084 - val_loss: 404.4260 - val_MinusLogProbMetric: 404.4260 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 125/1000
2023-09-10 05:42:43.160 
Epoch 125/1000 
	 loss: 395.7420, MinusLogProbMetric: 395.7420, val_loss: 407.5496, val_MinusLogProbMetric: 407.5496

Epoch 125: val_loss did not improve from 402.45764
196/196 - 19s - loss: 395.7420 - MinusLogProbMetric: 395.7420 - val_loss: 407.5496 - val_MinusLogProbMetric: 407.5496 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 126/1000
2023-09-10 05:43:02.237 
Epoch 126/1000 
	 loss: 394.6512, MinusLogProbMetric: 394.6512, val_loss: 422.4846, val_MinusLogProbMetric: 422.4846

Epoch 126: val_loss did not improve from 402.45764
196/196 - 19s - loss: 394.6512 - MinusLogProbMetric: 394.6512 - val_loss: 422.4846 - val_MinusLogProbMetric: 422.4846 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 127/1000
2023-09-10 05:43:20.118 
Epoch 127/1000 
	 loss: 395.6044, MinusLogProbMetric: 395.6044, val_loss: 405.2171, val_MinusLogProbMetric: 405.2171

Epoch 127: val_loss did not improve from 402.45764
196/196 - 18s - loss: 395.6044 - MinusLogProbMetric: 395.6044 - val_loss: 405.2171 - val_MinusLogProbMetric: 405.2171 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 128/1000
2023-09-10 05:43:37.992 
Epoch 128/1000 
	 loss: 396.1773, MinusLogProbMetric: 396.1773, val_loss: 406.2978, val_MinusLogProbMetric: 406.2978

Epoch 128: val_loss did not improve from 402.45764
196/196 - 18s - loss: 396.1773 - MinusLogProbMetric: 396.1773 - val_loss: 406.2978 - val_MinusLogProbMetric: 406.2978 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 129/1000
2023-09-10 05:43:56.166 
Epoch 129/1000 
	 loss: 394.8203, MinusLogProbMetric: 394.8203, val_loss: 404.5812, val_MinusLogProbMetric: 404.5812

Epoch 129: val_loss did not improve from 402.45764
196/196 - 18s - loss: 394.8203 - MinusLogProbMetric: 394.8203 - val_loss: 404.5812 - val_MinusLogProbMetric: 404.5812 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 130/1000
2023-09-10 05:44:13.810 
Epoch 130/1000 
	 loss: 395.9997, MinusLogProbMetric: 395.9997, val_loss: 410.2485, val_MinusLogProbMetric: 410.2485

Epoch 130: val_loss did not improve from 402.45764
196/196 - 18s - loss: 395.9997 - MinusLogProbMetric: 395.9997 - val_loss: 410.2485 - val_MinusLogProbMetric: 410.2485 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 131/1000
2023-09-10 05:44:31.617 
Epoch 131/1000 
	 loss: 394.7680, MinusLogProbMetric: 394.7680, val_loss: 405.7065, val_MinusLogProbMetric: 405.7065

Epoch 131: val_loss did not improve from 402.45764
196/196 - 18s - loss: 394.7680 - MinusLogProbMetric: 394.7680 - val_loss: 405.7065 - val_MinusLogProbMetric: 405.7065 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 132/1000
2023-09-10 05:44:49.111 
Epoch 132/1000 
	 loss: 394.6794, MinusLogProbMetric: 394.6794, val_loss: 407.9522, val_MinusLogProbMetric: 407.9522

Epoch 132: val_loss did not improve from 402.45764
196/196 - 17s - loss: 394.6794 - MinusLogProbMetric: 394.6794 - val_loss: 407.9522 - val_MinusLogProbMetric: 407.9522 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 133/1000
2023-09-10 05:45:07.616 
Epoch 133/1000 
	 loss: 395.3429, MinusLogProbMetric: 395.3429, val_loss: 404.5561, val_MinusLogProbMetric: 404.5561

Epoch 133: val_loss did not improve from 402.45764
196/196 - 19s - loss: 395.3429 - MinusLogProbMetric: 395.3429 - val_loss: 404.5561 - val_MinusLogProbMetric: 404.5561 - lr: 3.3333e-04 - 19s/epoch - 94ms/step
Epoch 134/1000
2023-09-10 05:45:24.860 
Epoch 134/1000 
	 loss: 394.5494, MinusLogProbMetric: 394.5494, val_loss: 406.7589, val_MinusLogProbMetric: 406.7589

Epoch 134: val_loss did not improve from 402.45764
196/196 - 17s - loss: 394.5494 - MinusLogProbMetric: 394.5494 - val_loss: 406.7589 - val_MinusLogProbMetric: 406.7589 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 135/1000
2023-09-10 05:45:43.770 
Epoch 135/1000 
	 loss: 394.2287, MinusLogProbMetric: 394.2287, val_loss: 406.0273, val_MinusLogProbMetric: 406.0273

Epoch 135: val_loss did not improve from 402.45764
196/196 - 19s - loss: 394.2287 - MinusLogProbMetric: 394.2287 - val_loss: 406.0273 - val_MinusLogProbMetric: 406.0273 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 136/1000
2023-09-10 05:46:02.699 
Epoch 136/1000 
	 loss: 394.4570, MinusLogProbMetric: 394.4570, val_loss: 408.2653, val_MinusLogProbMetric: 408.2653

Epoch 136: val_loss did not improve from 402.45764
196/196 - 19s - loss: 394.4570 - MinusLogProbMetric: 394.4570 - val_loss: 408.2653 - val_MinusLogProbMetric: 408.2653 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 137/1000
2023-09-10 05:46:21.788 
Epoch 137/1000 
	 loss: 395.3344, MinusLogProbMetric: 395.3344, val_loss: 404.5044, val_MinusLogProbMetric: 404.5044

Epoch 137: val_loss did not improve from 402.45764
196/196 - 19s - loss: 395.3344 - MinusLogProbMetric: 395.3344 - val_loss: 404.5044 - val_MinusLogProbMetric: 404.5044 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 138/1000
2023-09-10 05:46:41.652 
Epoch 138/1000 
	 loss: 393.8224, MinusLogProbMetric: 393.8224, val_loss: 405.7568, val_MinusLogProbMetric: 405.7568

Epoch 138: val_loss did not improve from 402.45764
196/196 - 20s - loss: 393.8224 - MinusLogProbMetric: 393.8224 - val_loss: 405.7568 - val_MinusLogProbMetric: 405.7568 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 139/1000
2023-09-10 05:47:01.257 
Epoch 139/1000 
	 loss: 394.4715, MinusLogProbMetric: 394.4715, val_loss: 407.8704, val_MinusLogProbMetric: 407.8704

Epoch 139: val_loss did not improve from 402.45764
196/196 - 20s - loss: 394.4715 - MinusLogProbMetric: 394.4715 - val_loss: 407.8704 - val_MinusLogProbMetric: 407.8704 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 140/1000
2023-09-10 05:47:19.910 
Epoch 140/1000 
	 loss: 394.9077, MinusLogProbMetric: 394.9077, val_loss: 406.3553, val_MinusLogProbMetric: 406.3553

Epoch 140: val_loss did not improve from 402.45764
196/196 - 19s - loss: 394.9077 - MinusLogProbMetric: 394.9077 - val_loss: 406.3553 - val_MinusLogProbMetric: 406.3553 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 141/1000
2023-09-10 05:47:38.711 
Epoch 141/1000 
	 loss: 394.6161, MinusLogProbMetric: 394.6161, val_loss: 403.5773, val_MinusLogProbMetric: 403.5773

Epoch 141: val_loss did not improve from 402.45764
196/196 - 19s - loss: 394.6161 - MinusLogProbMetric: 394.6161 - val_loss: 403.5773 - val_MinusLogProbMetric: 403.5773 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 142/1000
2023-09-10 05:47:56.957 
Epoch 142/1000 
	 loss: 393.6943, MinusLogProbMetric: 393.6943, val_loss: 406.1790, val_MinusLogProbMetric: 406.1790

Epoch 142: val_loss did not improve from 402.45764
196/196 - 18s - loss: 393.6943 - MinusLogProbMetric: 393.6943 - val_loss: 406.1790 - val_MinusLogProbMetric: 406.1790 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 143/1000
2023-09-10 05:48:13.601 
Epoch 143/1000 
	 loss: 395.4857, MinusLogProbMetric: 395.4857, val_loss: 403.3058, val_MinusLogProbMetric: 403.3058

Epoch 143: val_loss did not improve from 402.45764
196/196 - 17s - loss: 395.4857 - MinusLogProbMetric: 395.4857 - val_loss: 403.3058 - val_MinusLogProbMetric: 403.3058 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 144/1000
2023-09-10 05:48:32.039 
Epoch 144/1000 
	 loss: 393.7623, MinusLogProbMetric: 393.7623, val_loss: 406.1566, val_MinusLogProbMetric: 406.1566

Epoch 144: val_loss did not improve from 402.45764
196/196 - 18s - loss: 393.7623 - MinusLogProbMetric: 393.7623 - val_loss: 406.1566 - val_MinusLogProbMetric: 406.1566 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 145/1000
2023-09-10 05:48:50.105 
Epoch 145/1000 
	 loss: 394.2236, MinusLogProbMetric: 394.2236, val_loss: 406.1052, val_MinusLogProbMetric: 406.1052

Epoch 145: val_loss did not improve from 402.45764
196/196 - 18s - loss: 394.2236 - MinusLogProbMetric: 394.2236 - val_loss: 406.1052 - val_MinusLogProbMetric: 406.1052 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 146/1000
2023-09-10 05:49:07.893 
Epoch 146/1000 
	 loss: 394.2157, MinusLogProbMetric: 394.2157, val_loss: 405.0595, val_MinusLogProbMetric: 405.0595

Epoch 146: val_loss did not improve from 402.45764
196/196 - 18s - loss: 394.2157 - MinusLogProbMetric: 394.2157 - val_loss: 405.0595 - val_MinusLogProbMetric: 405.0595 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 147/1000
2023-09-10 05:49:24.408 
Epoch 147/1000 
	 loss: 395.0206, MinusLogProbMetric: 395.0206, val_loss: 406.1239, val_MinusLogProbMetric: 406.1239

Epoch 147: val_loss did not improve from 402.45764
196/196 - 17s - loss: 395.0206 - MinusLogProbMetric: 395.0206 - val_loss: 406.1239 - val_MinusLogProbMetric: 406.1239 - lr: 3.3333e-04 - 17s/epoch - 84ms/step
Epoch 148/1000
2023-09-10 05:49:43.775 
Epoch 148/1000 
	 loss: 393.5103, MinusLogProbMetric: 393.5103, val_loss: 405.7195, val_MinusLogProbMetric: 405.7195

Epoch 148: val_loss did not improve from 402.45764
196/196 - 19s - loss: 393.5103 - MinusLogProbMetric: 393.5103 - val_loss: 405.7195 - val_MinusLogProbMetric: 405.7195 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 149/1000
2023-09-10 05:50:02.079 
Epoch 149/1000 
	 loss: 393.8121, MinusLogProbMetric: 393.8121, val_loss: 408.3849, val_MinusLogProbMetric: 408.3849

Epoch 149: val_loss did not improve from 402.45764
196/196 - 18s - loss: 393.8121 - MinusLogProbMetric: 393.8121 - val_loss: 408.3849 - val_MinusLogProbMetric: 408.3849 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 150/1000
2023-09-10 05:50:19.900 
Epoch 150/1000 
	 loss: 387.9528, MinusLogProbMetric: 387.9528, val_loss: 401.5118, val_MinusLogProbMetric: 401.5118

Epoch 150: val_loss improved from 402.45764 to 401.51181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 19s - loss: 387.9528 - MinusLogProbMetric: 387.9528 - val_loss: 401.5118 - val_MinusLogProbMetric: 401.5118 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 151/1000
2023-09-10 05:50:38.074 
Epoch 151/1000 
	 loss: 387.9392, MinusLogProbMetric: 387.9392, val_loss: 401.4748, val_MinusLogProbMetric: 401.4748

Epoch 151: val_loss improved from 401.51181 to 401.47476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 18s - loss: 387.9392 - MinusLogProbMetric: 387.9392 - val_loss: 401.4748 - val_MinusLogProbMetric: 401.4748 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 152/1000
2023-09-10 05:50:56.473 
Epoch 152/1000 
	 loss: 388.0078, MinusLogProbMetric: 388.0078, val_loss: 401.6923, val_MinusLogProbMetric: 401.6923

Epoch 152: val_loss did not improve from 401.47476
196/196 - 18s - loss: 388.0078 - MinusLogProbMetric: 388.0078 - val_loss: 401.6923 - val_MinusLogProbMetric: 401.6923 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 153/1000
2023-09-10 05:51:13.531 
Epoch 153/1000 
	 loss: 388.2032, MinusLogProbMetric: 388.2032, val_loss: 400.9516, val_MinusLogProbMetric: 400.9516

Epoch 153: val_loss improved from 401.47476 to 400.95160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_332/weights/best_weights.h5
196/196 - 18s - loss: 388.2032 - MinusLogProbMetric: 388.2032 - val_loss: 400.9516 - val_MinusLogProbMetric: 400.9516 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 154/1000
2023-09-10 05:51:32.277 
Epoch 154/1000 
	 loss: 388.1043, MinusLogProbMetric: 388.1043, val_loss: 402.8518, val_MinusLogProbMetric: 402.8518

Epoch 154: val_loss did not improve from 400.95160
196/196 - 18s - loss: 388.1043 - MinusLogProbMetric: 388.1043 - val_loss: 402.8518 - val_MinusLogProbMetric: 402.8518 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 155/1000
2023-09-10 05:51:50.269 
Epoch 155/1000 
	 loss: 387.8402, MinusLogProbMetric: 387.8402, val_loss: 402.2263, val_MinusLogProbMetric: 402.2263

Epoch 155: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.8402 - MinusLogProbMetric: 387.8402 - val_loss: 402.2263 - val_MinusLogProbMetric: 402.2263 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 156/1000
2023-09-10 05:52:07.025 
Epoch 156/1000 
	 loss: 387.8506, MinusLogProbMetric: 387.8506, val_loss: 402.2089, val_MinusLogProbMetric: 402.2089

Epoch 156: val_loss did not improve from 400.95160
196/196 - 17s - loss: 387.8506 - MinusLogProbMetric: 387.8506 - val_loss: 402.2089 - val_MinusLogProbMetric: 402.2089 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 157/1000
2023-09-10 05:52:25.310 
Epoch 157/1000 
	 loss: 387.9090, MinusLogProbMetric: 387.9090, val_loss: 402.2377, val_MinusLogProbMetric: 402.2377

Epoch 157: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.9090 - MinusLogProbMetric: 387.9090 - val_loss: 402.2377 - val_MinusLogProbMetric: 402.2377 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 158/1000
2023-09-10 05:52:43.556 
Epoch 158/1000 
	 loss: 387.6430, MinusLogProbMetric: 387.6430, val_loss: 402.0454, val_MinusLogProbMetric: 402.0454

Epoch 158: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.6430 - MinusLogProbMetric: 387.6430 - val_loss: 402.0454 - val_MinusLogProbMetric: 402.0454 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 159/1000
2023-09-10 05:53:01.302 
Epoch 159/1000 
	 loss: 388.1806, MinusLogProbMetric: 388.1806, val_loss: 403.0687, val_MinusLogProbMetric: 403.0687

Epoch 159: val_loss did not improve from 400.95160
196/196 - 18s - loss: 388.1806 - MinusLogProbMetric: 388.1806 - val_loss: 403.0687 - val_MinusLogProbMetric: 403.0687 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 160/1000
2023-09-10 05:53:19.624 
Epoch 160/1000 
	 loss: 387.9129, MinusLogProbMetric: 387.9129, val_loss: 403.9841, val_MinusLogProbMetric: 403.9841

Epoch 160: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.9129 - MinusLogProbMetric: 387.9129 - val_loss: 403.9841 - val_MinusLogProbMetric: 403.9841 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 161/1000
2023-09-10 05:53:38.136 
Epoch 161/1000 
	 loss: 387.7462, MinusLogProbMetric: 387.7462, val_loss: 403.8678, val_MinusLogProbMetric: 403.8678

Epoch 161: val_loss did not improve from 400.95160
196/196 - 19s - loss: 387.7462 - MinusLogProbMetric: 387.7462 - val_loss: 403.8678 - val_MinusLogProbMetric: 403.8678 - lr: 1.6667e-04 - 19s/epoch - 94ms/step
Epoch 162/1000
2023-09-10 05:53:57.433 
Epoch 162/1000 
	 loss: 387.6819, MinusLogProbMetric: 387.6819, val_loss: 403.4094, val_MinusLogProbMetric: 403.4094

Epoch 162: val_loss did not improve from 400.95160
196/196 - 19s - loss: 387.6819 - MinusLogProbMetric: 387.6819 - val_loss: 403.4094 - val_MinusLogProbMetric: 403.4094 - lr: 1.6667e-04 - 19s/epoch - 98ms/step
Epoch 163/1000
2023-09-10 05:54:15.445 
Epoch 163/1000 
	 loss: 387.5255, MinusLogProbMetric: 387.5255, val_loss: 403.8563, val_MinusLogProbMetric: 403.8563

Epoch 163: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.5255 - MinusLogProbMetric: 387.5255 - val_loss: 403.8563 - val_MinusLogProbMetric: 403.8563 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 164/1000
2023-09-10 05:54:34.004 
Epoch 164/1000 
	 loss: 388.1815, MinusLogProbMetric: 388.1815, val_loss: 402.4900, val_MinusLogProbMetric: 402.4900

Epoch 164: val_loss did not improve from 400.95160
196/196 - 19s - loss: 388.1815 - MinusLogProbMetric: 388.1815 - val_loss: 402.4900 - val_MinusLogProbMetric: 402.4900 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 165/1000
2023-09-10 05:54:52.648 
Epoch 165/1000 
	 loss: 387.4342, MinusLogProbMetric: 387.4342, val_loss: 402.9654, val_MinusLogProbMetric: 402.9654

Epoch 165: val_loss did not improve from 400.95160
196/196 - 19s - loss: 387.4342 - MinusLogProbMetric: 387.4342 - val_loss: 402.9654 - val_MinusLogProbMetric: 402.9654 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 166/1000
2023-09-10 05:55:11.307 
Epoch 166/1000 
	 loss: 387.4593, MinusLogProbMetric: 387.4593, val_loss: 401.9745, val_MinusLogProbMetric: 401.9745

Epoch 166: val_loss did not improve from 400.95160
196/196 - 19s - loss: 387.4593 - MinusLogProbMetric: 387.4593 - val_loss: 401.9745 - val_MinusLogProbMetric: 401.9745 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 167/1000
2023-09-10 05:55:29.027 
Epoch 167/1000 
	 loss: 387.5738, MinusLogProbMetric: 387.5738, val_loss: 403.2763, val_MinusLogProbMetric: 403.2763

Epoch 167: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.5738 - MinusLogProbMetric: 387.5738 - val_loss: 403.2763 - val_MinusLogProbMetric: 403.2763 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 168/1000
2023-09-10 05:55:46.621 
Epoch 168/1000 
	 loss: 387.6693, MinusLogProbMetric: 387.6693, val_loss: 401.3447, val_MinusLogProbMetric: 401.3447

Epoch 168: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.6693 - MinusLogProbMetric: 387.6693 - val_loss: 401.3447 - val_MinusLogProbMetric: 401.3447 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 169/1000
2023-09-10 05:56:04.224 
Epoch 169/1000 
	 loss: 387.4797, MinusLogProbMetric: 387.4797, val_loss: 402.5815, val_MinusLogProbMetric: 402.5815

Epoch 169: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.4797 - MinusLogProbMetric: 387.4797 - val_loss: 402.5815 - val_MinusLogProbMetric: 402.5815 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 170/1000
2023-09-10 05:56:22.191 
Epoch 170/1000 
	 loss: 387.7832, MinusLogProbMetric: 387.7832, val_loss: 402.6844, val_MinusLogProbMetric: 402.6844

Epoch 170: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.7832 - MinusLogProbMetric: 387.7832 - val_loss: 402.6844 - val_MinusLogProbMetric: 402.6844 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 171/1000
2023-09-10 05:56:40.145 
Epoch 171/1000 
	 loss: 387.2872, MinusLogProbMetric: 387.2872, val_loss: 403.0501, val_MinusLogProbMetric: 403.0501

Epoch 171: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.2872 - MinusLogProbMetric: 387.2872 - val_loss: 403.0501 - val_MinusLogProbMetric: 403.0501 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 172/1000
2023-09-10 05:56:57.426 
Epoch 172/1000 
	 loss: 387.2813, MinusLogProbMetric: 387.2813, val_loss: 402.1765, val_MinusLogProbMetric: 402.1765

Epoch 172: val_loss did not improve from 400.95160
196/196 - 17s - loss: 387.2813 - MinusLogProbMetric: 387.2813 - val_loss: 402.1765 - val_MinusLogProbMetric: 402.1765 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 173/1000
2023-09-10 05:57:14.883 
Epoch 173/1000 
	 loss: 387.2652, MinusLogProbMetric: 387.2652, val_loss: 403.8126, val_MinusLogProbMetric: 403.8126

Epoch 173: val_loss did not improve from 400.95160
196/196 - 17s - loss: 387.2652 - MinusLogProbMetric: 387.2652 - val_loss: 403.8126 - val_MinusLogProbMetric: 403.8126 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 174/1000
2023-09-10 05:57:32.547 
Epoch 174/1000 
	 loss: 387.5770, MinusLogProbMetric: 387.5770, val_loss: 404.6567, val_MinusLogProbMetric: 404.6567

Epoch 174: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.5770 - MinusLogProbMetric: 387.5770 - val_loss: 404.6567 - val_MinusLogProbMetric: 404.6567 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 175/1000
2023-09-10 05:57:51.230 
Epoch 175/1000 
	 loss: 387.2903, MinusLogProbMetric: 387.2903, val_loss: 403.8528, val_MinusLogProbMetric: 403.8528

Epoch 175: val_loss did not improve from 400.95160
196/196 - 19s - loss: 387.2903 - MinusLogProbMetric: 387.2903 - val_loss: 403.8528 - val_MinusLogProbMetric: 403.8528 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 176/1000
2023-09-10 05:58:09.215 
Epoch 176/1000 
	 loss: 387.0783, MinusLogProbMetric: 387.0783, val_loss: 404.3896, val_MinusLogProbMetric: 404.3896

Epoch 176: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.0783 - MinusLogProbMetric: 387.0783 - val_loss: 404.3896 - val_MinusLogProbMetric: 404.3896 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 177/1000
2023-09-10 05:58:27.316 
Epoch 177/1000 
	 loss: 387.2140, MinusLogProbMetric: 387.2140, val_loss: 404.0039, val_MinusLogProbMetric: 404.0039

Epoch 177: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.2140 - MinusLogProbMetric: 387.2140 - val_loss: 404.0039 - val_MinusLogProbMetric: 404.0039 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 178/1000
2023-09-10 05:58:43.994 
Epoch 178/1000 
	 loss: 387.5619, MinusLogProbMetric: 387.5619, val_loss: 403.0134, val_MinusLogProbMetric: 403.0134

Epoch 178: val_loss did not improve from 400.95160
196/196 - 17s - loss: 387.5619 - MinusLogProbMetric: 387.5619 - val_loss: 403.0134 - val_MinusLogProbMetric: 403.0134 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 179/1000
2023-09-10 05:59:02.482 
Epoch 179/1000 
	 loss: 387.3216, MinusLogProbMetric: 387.3216, val_loss: 402.3357, val_MinusLogProbMetric: 402.3357

Epoch 179: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.3216 - MinusLogProbMetric: 387.3216 - val_loss: 402.3357 - val_MinusLogProbMetric: 402.3357 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 180/1000
2023-09-10 05:59:20.169 
Epoch 180/1000 
	 loss: 386.9372, MinusLogProbMetric: 386.9372, val_loss: 401.9233, val_MinusLogProbMetric: 401.9233

Epoch 180: val_loss did not improve from 400.95160
196/196 - 18s - loss: 386.9372 - MinusLogProbMetric: 386.9372 - val_loss: 401.9233 - val_MinusLogProbMetric: 401.9233 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 181/1000
2023-09-10 05:59:37.824 
Epoch 181/1000 
	 loss: 386.9965, MinusLogProbMetric: 386.9965, val_loss: 404.8009, val_MinusLogProbMetric: 404.8009

Epoch 181: val_loss did not improve from 400.95160
196/196 - 18s - loss: 386.9965 - MinusLogProbMetric: 386.9965 - val_loss: 404.8009 - val_MinusLogProbMetric: 404.8009 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 182/1000
2023-09-10 05:59:56.355 
Epoch 182/1000 
	 loss: 387.3132, MinusLogProbMetric: 387.3132, val_loss: 402.9718, val_MinusLogProbMetric: 402.9718

Epoch 182: val_loss did not improve from 400.95160
196/196 - 19s - loss: 387.3132 - MinusLogProbMetric: 387.3132 - val_loss: 402.9718 - val_MinusLogProbMetric: 402.9718 - lr: 1.6667e-04 - 19s/epoch - 94ms/step
Epoch 183/1000
2023-09-10 06:00:14.916 
Epoch 183/1000 
	 loss: 386.8121, MinusLogProbMetric: 386.8121, val_loss: 404.0635, val_MinusLogProbMetric: 404.0635

Epoch 183: val_loss did not improve from 400.95160
196/196 - 19s - loss: 386.8121 - MinusLogProbMetric: 386.8121 - val_loss: 404.0635 - val_MinusLogProbMetric: 404.0635 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 184/1000
2023-09-10 06:00:33.125 
Epoch 184/1000 
	 loss: 386.8103, MinusLogProbMetric: 386.8103, val_loss: 404.1238, val_MinusLogProbMetric: 404.1238

Epoch 184: val_loss did not improve from 400.95160
196/196 - 18s - loss: 386.8103 - MinusLogProbMetric: 386.8103 - val_loss: 404.1238 - val_MinusLogProbMetric: 404.1238 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 185/1000
2023-09-10 06:00:51.610 
Epoch 185/1000 
	 loss: 386.9426, MinusLogProbMetric: 386.9426, val_loss: 403.9120, val_MinusLogProbMetric: 403.9120

Epoch 185: val_loss did not improve from 400.95160
196/196 - 18s - loss: 386.9426 - MinusLogProbMetric: 386.9426 - val_loss: 403.9120 - val_MinusLogProbMetric: 403.9120 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 186/1000
2023-09-10 06:01:10.319 
Epoch 186/1000 
	 loss: 386.9472, MinusLogProbMetric: 386.9472, val_loss: 404.0515, val_MinusLogProbMetric: 404.0515

Epoch 186: val_loss did not improve from 400.95160
196/196 - 19s - loss: 386.9472 - MinusLogProbMetric: 386.9472 - val_loss: 404.0515 - val_MinusLogProbMetric: 404.0515 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 187/1000
2023-09-10 06:01:29.141 
Epoch 187/1000 
	 loss: 386.6789, MinusLogProbMetric: 386.6789, val_loss: 411.4531, val_MinusLogProbMetric: 411.4531

Epoch 187: val_loss did not improve from 400.95160
196/196 - 19s - loss: 386.6789 - MinusLogProbMetric: 386.6789 - val_loss: 411.4531 - val_MinusLogProbMetric: 411.4531 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 188/1000
2023-09-10 06:01:47.275 
Epoch 188/1000 
	 loss: 387.4893, MinusLogProbMetric: 387.4893, val_loss: 403.0449, val_MinusLogProbMetric: 403.0449

Epoch 188: val_loss did not improve from 400.95160
196/196 - 18s - loss: 387.4893 - MinusLogProbMetric: 387.4893 - val_loss: 403.0449 - val_MinusLogProbMetric: 403.0449 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 189/1000
2023-09-10 06:02:06.196 
Epoch 189/1000 
	 loss: 386.6189, MinusLogProbMetric: 386.6189, val_loss: 403.3285, val_MinusLogProbMetric: 403.3285

Epoch 189: val_loss did not improve from 400.95160
196/196 - 19s - loss: 386.6189 - MinusLogProbMetric: 386.6189 - val_loss: 403.3285 - val_MinusLogProbMetric: 403.3285 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 190/1000
2023-09-10 06:02:24.795 
Epoch 190/1000 
	 loss: 386.9451, MinusLogProbMetric: 386.9451, val_loss: 406.4637, val_MinusLogProbMetric: 406.4637

Epoch 190: val_loss did not improve from 400.95160
196/196 - 19s - loss: 386.9451 - MinusLogProbMetric: 386.9451 - val_loss: 406.4637 - val_MinusLogProbMetric: 406.4637 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 191/1000
2023-09-10 06:02:42.220 
Epoch 191/1000 
	 loss: 386.6541, MinusLogProbMetric: 386.6541, val_loss: 406.1661, val_MinusLogProbMetric: 406.1661

Epoch 191: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.6541 - MinusLogProbMetric: 386.6541 - val_loss: 406.1661 - val_MinusLogProbMetric: 406.1661 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 192/1000
2023-09-10 06:02:58.857 
Epoch 192/1000 
	 loss: 386.6768, MinusLogProbMetric: 386.6768, val_loss: 404.2981, val_MinusLogProbMetric: 404.2981

Epoch 192: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.6768 - MinusLogProbMetric: 386.6768 - val_loss: 404.2981 - val_MinusLogProbMetric: 404.2981 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 193/1000
2023-09-10 06:03:14.719 
Epoch 193/1000 
	 loss: 386.7661, MinusLogProbMetric: 386.7661, val_loss: 404.0004, val_MinusLogProbMetric: 404.0004

Epoch 193: val_loss did not improve from 400.95160
196/196 - 16s - loss: 386.7661 - MinusLogProbMetric: 386.7661 - val_loss: 404.0004 - val_MinusLogProbMetric: 404.0004 - lr: 1.6667e-04 - 16s/epoch - 81ms/step
Epoch 194/1000
2023-09-10 06:03:32.016 
Epoch 194/1000 
	 loss: 386.7586, MinusLogProbMetric: 386.7586, val_loss: 403.9611, val_MinusLogProbMetric: 403.9611

Epoch 194: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.7586 - MinusLogProbMetric: 386.7586 - val_loss: 403.9611 - val_MinusLogProbMetric: 403.9611 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 195/1000
2023-09-10 06:03:49.755 
Epoch 195/1000 
	 loss: 386.5720, MinusLogProbMetric: 386.5720, val_loss: 408.1602, val_MinusLogProbMetric: 408.1602

Epoch 195: val_loss did not improve from 400.95160
196/196 - 18s - loss: 386.5720 - MinusLogProbMetric: 386.5720 - val_loss: 408.1602 - val_MinusLogProbMetric: 408.1602 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 196/1000
2023-09-10 06:04:05.456 
Epoch 196/1000 
	 loss: 386.6373, MinusLogProbMetric: 386.6373, val_loss: 403.3754, val_MinusLogProbMetric: 403.3754

Epoch 196: val_loss did not improve from 400.95160
196/196 - 16s - loss: 386.6373 - MinusLogProbMetric: 386.6373 - val_loss: 403.3754 - val_MinusLogProbMetric: 403.3754 - lr: 1.6667e-04 - 16s/epoch - 80ms/step
Epoch 197/1000
2023-09-10 06:04:22.938 
Epoch 197/1000 
	 loss: 386.8473, MinusLogProbMetric: 386.8473, val_loss: 404.4920, val_MinusLogProbMetric: 404.4920

Epoch 197: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.8473 - MinusLogProbMetric: 386.8473 - val_loss: 404.4920 - val_MinusLogProbMetric: 404.4920 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 198/1000
2023-09-10 06:04:39.766 
Epoch 198/1000 
	 loss: 387.0870, MinusLogProbMetric: 387.0870, val_loss: 403.9632, val_MinusLogProbMetric: 403.9632

Epoch 198: val_loss did not improve from 400.95160
196/196 - 17s - loss: 387.0870 - MinusLogProbMetric: 387.0870 - val_loss: 403.9632 - val_MinusLogProbMetric: 403.9632 - lr: 1.6667e-04 - 17s/epoch - 86ms/step
Epoch 199/1000
2023-09-10 06:04:56.393 
Epoch 199/1000 
	 loss: 386.1673, MinusLogProbMetric: 386.1673, val_loss: 405.0143, val_MinusLogProbMetric: 405.0143

Epoch 199: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.1673 - MinusLogProbMetric: 386.1673 - val_loss: 405.0143 - val_MinusLogProbMetric: 405.0143 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 200/1000
2023-09-10 06:05:12.927 
Epoch 200/1000 
	 loss: 386.4268, MinusLogProbMetric: 386.4268, val_loss: 404.8852, val_MinusLogProbMetric: 404.8852

Epoch 200: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.4268 - MinusLogProbMetric: 386.4268 - val_loss: 404.8852 - val_MinusLogProbMetric: 404.8852 - lr: 1.6667e-04 - 17s/epoch - 84ms/step
Epoch 201/1000
2023-09-10 06:05:29.553 
Epoch 201/1000 
	 loss: 386.5817, MinusLogProbMetric: 386.5817, val_loss: 404.7574, val_MinusLogProbMetric: 404.7574

Epoch 201: val_loss did not improve from 400.95160
196/196 - 17s - loss: 386.5817 - MinusLogProbMetric: 386.5817 - val_loss: 404.7574 - val_MinusLogProbMetric: 404.7574 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 202/1000
2023-09-10 06:05:46.194 
Epoch 202/1000 
	 loss: 387.6663, MinusLogProbMetric: 387.6663, val_loss: 405.6154, val_MinusLogProbMetric: 405.6154

Epoch 202: val_loss did not improve from 400.95160
196/196 - 17s - loss: 387.6663 - MinusLogProbMetric: 387.6663 - val_loss: 405.6154 - val_MinusLogProbMetric: 405.6154 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 203/1000
2023-09-10 06:06:05.014 
Epoch 203/1000 
	 loss: 386.9188, MinusLogProbMetric: 386.9188, val_loss: 403.8615, val_MinusLogProbMetric: 403.8615

Epoch 203: val_loss did not improve from 400.95160
196/196 - 19s - loss: 386.9188 - MinusLogProbMetric: 386.9188 - val_loss: 403.8615 - val_MinusLogProbMetric: 403.8615 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 204/1000
2023-09-10 06:06:22.523 
Epoch 204/1000 
	 loss: 383.5322, MinusLogProbMetric: 383.5322, val_loss: 402.9146, val_MinusLogProbMetric: 402.9146

Epoch 204: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.5322 - MinusLogProbMetric: 383.5322 - val_loss: 402.9146 - val_MinusLogProbMetric: 402.9146 - lr: 8.3333e-05 - 18s/epoch - 89ms/step
Epoch 205/1000
2023-09-10 06:06:41.240 
Epoch 205/1000 
	 loss: 383.2061, MinusLogProbMetric: 383.2061, val_loss: 402.7277, val_MinusLogProbMetric: 402.7277

Epoch 205: val_loss did not improve from 400.95160
196/196 - 19s - loss: 383.2061 - MinusLogProbMetric: 383.2061 - val_loss: 402.7277 - val_MinusLogProbMetric: 402.7277 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 206/1000
2023-09-10 06:06:59.555 
Epoch 206/1000 
	 loss: 383.2012, MinusLogProbMetric: 383.2012, val_loss: 403.3381, val_MinusLogProbMetric: 403.3381

Epoch 206: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.2012 - MinusLogProbMetric: 383.2012 - val_loss: 403.3381 - val_MinusLogProbMetric: 403.3381 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 207/1000
2023-09-10 06:07:17.440 
Epoch 207/1000 
	 loss: 383.2394, MinusLogProbMetric: 383.2394, val_loss: 403.0556, val_MinusLogProbMetric: 403.0556

Epoch 207: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.2394 - MinusLogProbMetric: 383.2394 - val_loss: 403.0556 - val_MinusLogProbMetric: 403.0556 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 208/1000
2023-09-10 06:07:35.050 
Epoch 208/1000 
	 loss: 383.4634, MinusLogProbMetric: 383.4634, val_loss: 403.3611, val_MinusLogProbMetric: 403.3611

Epoch 208: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.4634 - MinusLogProbMetric: 383.4634 - val_loss: 403.3611 - val_MinusLogProbMetric: 403.3611 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 209/1000
2023-09-10 06:07:52.495 
Epoch 209/1000 
	 loss: 383.3641, MinusLogProbMetric: 383.3641, val_loss: 404.0917, val_MinusLogProbMetric: 404.0917

Epoch 209: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.3641 - MinusLogProbMetric: 383.3641 - val_loss: 404.0917 - val_MinusLogProbMetric: 404.0917 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 210/1000
2023-09-10 06:08:09.189 
Epoch 210/1000 
	 loss: 383.2176, MinusLogProbMetric: 383.2176, val_loss: 403.9751, val_MinusLogProbMetric: 403.9751

Epoch 210: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.2176 - MinusLogProbMetric: 383.2176 - val_loss: 403.9751 - val_MinusLogProbMetric: 403.9751 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 211/1000
2023-09-10 06:08:25.368 
Epoch 211/1000 
	 loss: 383.4006, MinusLogProbMetric: 383.4006, val_loss: 404.3866, val_MinusLogProbMetric: 404.3866

Epoch 211: val_loss did not improve from 400.95160
196/196 - 16s - loss: 383.4006 - MinusLogProbMetric: 383.4006 - val_loss: 404.3866 - val_MinusLogProbMetric: 404.3866 - lr: 8.3333e-05 - 16s/epoch - 83ms/step
Epoch 212/1000
2023-09-10 06:08:42.383 
Epoch 212/1000 
	 loss: 383.3366, MinusLogProbMetric: 383.3366, val_loss: 403.2912, val_MinusLogProbMetric: 403.2912

Epoch 212: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.3366 - MinusLogProbMetric: 383.3366 - val_loss: 403.2912 - val_MinusLogProbMetric: 403.2912 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 213/1000
2023-09-10 06:08:59.030 
Epoch 213/1000 
	 loss: 383.3382, MinusLogProbMetric: 383.3382, val_loss: 404.1423, val_MinusLogProbMetric: 404.1423

Epoch 213: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.3382 - MinusLogProbMetric: 383.3382 - val_loss: 404.1423 - val_MinusLogProbMetric: 404.1423 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 214/1000
2023-09-10 06:09:15.861 
Epoch 214/1000 
	 loss: 383.2460, MinusLogProbMetric: 383.2460, val_loss: 403.4795, val_MinusLogProbMetric: 403.4795

Epoch 214: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.2460 - MinusLogProbMetric: 383.2460 - val_loss: 403.4795 - val_MinusLogProbMetric: 403.4795 - lr: 8.3333e-05 - 17s/epoch - 86ms/step
Epoch 215/1000
2023-09-10 06:09:32.563 
Epoch 215/1000 
	 loss: 383.0573, MinusLogProbMetric: 383.0573, val_loss: 403.2943, val_MinusLogProbMetric: 403.2943

Epoch 215: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.0573 - MinusLogProbMetric: 383.0573 - val_loss: 403.2943 - val_MinusLogProbMetric: 403.2943 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 216/1000
2023-09-10 06:09:49.608 
Epoch 216/1000 
	 loss: 383.0441, MinusLogProbMetric: 383.0441, val_loss: 404.8771, val_MinusLogProbMetric: 404.8771

Epoch 216: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.0441 - MinusLogProbMetric: 383.0441 - val_loss: 404.8771 - val_MinusLogProbMetric: 404.8771 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 217/1000
2023-09-10 06:10:07.270 
Epoch 217/1000 
	 loss: 383.2540, MinusLogProbMetric: 383.2540, val_loss: 403.8458, val_MinusLogProbMetric: 403.8458

Epoch 217: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.2540 - MinusLogProbMetric: 383.2540 - val_loss: 403.8458 - val_MinusLogProbMetric: 403.8458 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 218/1000
2023-09-10 06:10:25.360 
Epoch 218/1000 
	 loss: 383.2281, MinusLogProbMetric: 383.2281, val_loss: 403.5768, val_MinusLogProbMetric: 403.5768

Epoch 218: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.2281 - MinusLogProbMetric: 383.2281 - val_loss: 403.5768 - val_MinusLogProbMetric: 403.5768 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 219/1000
2023-09-10 06:10:42.498 
Epoch 219/1000 
	 loss: 383.3558, MinusLogProbMetric: 383.3558, val_loss: 403.4536, val_MinusLogProbMetric: 403.4536

Epoch 219: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.3558 - MinusLogProbMetric: 383.3558 - val_loss: 403.4536 - val_MinusLogProbMetric: 403.4536 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 220/1000
2023-09-10 06:11:00.616 
Epoch 220/1000 
	 loss: 383.1979, MinusLogProbMetric: 383.1979, val_loss: 404.2077, val_MinusLogProbMetric: 404.2077

Epoch 220: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.1979 - MinusLogProbMetric: 383.1979 - val_loss: 404.2077 - val_MinusLogProbMetric: 404.2077 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 221/1000
2023-09-10 06:11:18.355 
Epoch 221/1000 
	 loss: 383.3137, MinusLogProbMetric: 383.3137, val_loss: 403.6454, val_MinusLogProbMetric: 403.6454

Epoch 221: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.3137 - MinusLogProbMetric: 383.3137 - val_loss: 403.6454 - val_MinusLogProbMetric: 403.6454 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 222/1000
2023-09-10 06:11:36.995 
Epoch 222/1000 
	 loss: 383.1771, MinusLogProbMetric: 383.1771, val_loss: 404.1516, val_MinusLogProbMetric: 404.1516

Epoch 222: val_loss did not improve from 400.95160
196/196 - 19s - loss: 383.1771 - MinusLogProbMetric: 383.1771 - val_loss: 404.1516 - val_MinusLogProbMetric: 404.1516 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 223/1000
2023-09-10 06:11:54.787 
Epoch 223/1000 
	 loss: 383.2668, MinusLogProbMetric: 383.2668, val_loss: 404.8492, val_MinusLogProbMetric: 404.8492

Epoch 223: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.2668 - MinusLogProbMetric: 383.2668 - val_loss: 404.8492 - val_MinusLogProbMetric: 404.8492 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 224/1000
2023-09-10 06:12:13.128 
Epoch 224/1000 
	 loss: 382.9276, MinusLogProbMetric: 382.9276, val_loss: 404.4878, val_MinusLogProbMetric: 404.4878

Epoch 224: val_loss did not improve from 400.95160
196/196 - 18s - loss: 382.9276 - MinusLogProbMetric: 382.9276 - val_loss: 404.4878 - val_MinusLogProbMetric: 404.4878 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 225/1000
2023-09-10 06:12:31.769 
Epoch 225/1000 
	 loss: 383.5754, MinusLogProbMetric: 383.5754, val_loss: 403.4764, val_MinusLogProbMetric: 403.4764

Epoch 225: val_loss did not improve from 400.95160
196/196 - 19s - loss: 383.5754 - MinusLogProbMetric: 383.5754 - val_loss: 403.4764 - val_MinusLogProbMetric: 403.4764 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 226/1000
2023-09-10 06:12:50.355 
Epoch 226/1000 
	 loss: 382.9639, MinusLogProbMetric: 382.9639, val_loss: 404.2924, val_MinusLogProbMetric: 404.2924

Epoch 226: val_loss did not improve from 400.95160
196/196 - 19s - loss: 382.9639 - MinusLogProbMetric: 382.9639 - val_loss: 404.2924 - val_MinusLogProbMetric: 404.2924 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 227/1000
2023-09-10 06:13:08.803 
Epoch 227/1000 
	 loss: 383.2065, MinusLogProbMetric: 383.2065, val_loss: 404.1063, val_MinusLogProbMetric: 404.1063

Epoch 227: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.2065 - MinusLogProbMetric: 383.2065 - val_loss: 404.1063 - val_MinusLogProbMetric: 404.1063 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 228/1000
2023-09-10 06:13:26.055 
Epoch 228/1000 
	 loss: 382.9936, MinusLogProbMetric: 382.9936, val_loss: 405.2194, val_MinusLogProbMetric: 405.2194

Epoch 228: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9936 - MinusLogProbMetric: 382.9936 - val_loss: 405.2194 - val_MinusLogProbMetric: 405.2194 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 229/1000
2023-09-10 06:13:43.888 
Epoch 229/1000 
	 loss: 383.0778, MinusLogProbMetric: 383.0778, val_loss: 403.8980, val_MinusLogProbMetric: 403.8980

Epoch 229: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.0778 - MinusLogProbMetric: 383.0778 - val_loss: 403.8980 - val_MinusLogProbMetric: 403.8980 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 230/1000
2023-09-10 06:14:01.308 
Epoch 230/1000 
	 loss: 382.9641, MinusLogProbMetric: 382.9641, val_loss: 404.2284, val_MinusLogProbMetric: 404.2284

Epoch 230: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9641 - MinusLogProbMetric: 382.9641 - val_loss: 404.2284 - val_MinusLogProbMetric: 404.2284 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 231/1000
2023-09-10 06:14:18.470 
Epoch 231/1000 
	 loss: 382.9265, MinusLogProbMetric: 382.9265, val_loss: 404.9111, val_MinusLogProbMetric: 404.9111

Epoch 231: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9265 - MinusLogProbMetric: 382.9265 - val_loss: 404.9111 - val_MinusLogProbMetric: 404.9111 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 232/1000
2023-09-10 06:14:35.477 
Epoch 232/1000 
	 loss: 382.8217, MinusLogProbMetric: 382.8217, val_loss: 404.9945, val_MinusLogProbMetric: 404.9945

Epoch 232: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.8217 - MinusLogProbMetric: 382.8217 - val_loss: 404.9945 - val_MinusLogProbMetric: 404.9945 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 233/1000
2023-09-10 06:14:52.848 
Epoch 233/1000 
	 loss: 383.0095, MinusLogProbMetric: 383.0095, val_loss: 404.0079, val_MinusLogProbMetric: 404.0079

Epoch 233: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.0095 - MinusLogProbMetric: 383.0095 - val_loss: 404.0079 - val_MinusLogProbMetric: 404.0079 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 234/1000
2023-09-10 06:15:10.041 
Epoch 234/1000 
	 loss: 382.9593, MinusLogProbMetric: 382.9593, val_loss: 404.3640, val_MinusLogProbMetric: 404.3640

Epoch 234: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9593 - MinusLogProbMetric: 382.9593 - val_loss: 404.3640 - val_MinusLogProbMetric: 404.3640 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 235/1000
2023-09-10 06:15:27.320 
Epoch 235/1000 
	 loss: 382.9304, MinusLogProbMetric: 382.9304, val_loss: 405.5773, val_MinusLogProbMetric: 405.5773

Epoch 235: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9304 - MinusLogProbMetric: 382.9304 - val_loss: 405.5773 - val_MinusLogProbMetric: 405.5773 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 236/1000
2023-09-10 06:15:44.236 
Epoch 236/1000 
	 loss: 383.6617, MinusLogProbMetric: 383.6617, val_loss: 405.9578, val_MinusLogProbMetric: 405.9578

Epoch 236: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.6617 - MinusLogProbMetric: 383.6617 - val_loss: 405.9578 - val_MinusLogProbMetric: 405.9578 - lr: 8.3333e-05 - 17s/epoch - 86ms/step
Epoch 237/1000
2023-09-10 06:16:01.749 
Epoch 237/1000 
	 loss: 382.8817, MinusLogProbMetric: 382.8817, val_loss: 404.7686, val_MinusLogProbMetric: 404.7686

Epoch 237: val_loss did not improve from 400.95160
196/196 - 18s - loss: 382.8817 - MinusLogProbMetric: 382.8817 - val_loss: 404.7686 - val_MinusLogProbMetric: 404.7686 - lr: 8.3333e-05 - 18s/epoch - 89ms/step
Epoch 238/1000
2023-09-10 06:16:18.871 
Epoch 238/1000 
	 loss: 382.7262, MinusLogProbMetric: 382.7262, val_loss: 405.0119, val_MinusLogProbMetric: 405.0119

Epoch 238: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.7262 - MinusLogProbMetric: 382.7262 - val_loss: 405.0119 - val_MinusLogProbMetric: 405.0119 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 239/1000
2023-09-10 06:16:35.850 
Epoch 239/1000 
	 loss: 382.9073, MinusLogProbMetric: 382.9073, val_loss: 404.8435, val_MinusLogProbMetric: 404.8435

Epoch 239: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9073 - MinusLogProbMetric: 382.9073 - val_loss: 404.8435 - val_MinusLogProbMetric: 404.8435 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 240/1000
2023-09-10 06:16:52.501 
Epoch 240/1000 
	 loss: 382.7661, MinusLogProbMetric: 382.7661, val_loss: 404.9549, val_MinusLogProbMetric: 404.9549

Epoch 240: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.7661 - MinusLogProbMetric: 382.7661 - val_loss: 404.9549 - val_MinusLogProbMetric: 404.9549 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 241/1000
2023-09-10 06:17:09.432 
Epoch 241/1000 
	 loss: 383.2698, MinusLogProbMetric: 383.2698, val_loss: 405.0445, val_MinusLogProbMetric: 405.0445

Epoch 241: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.2698 - MinusLogProbMetric: 383.2698 - val_loss: 405.0445 - val_MinusLogProbMetric: 405.0445 - lr: 8.3333e-05 - 17s/epoch - 86ms/step
Epoch 242/1000
2023-09-10 06:17:26.959 
Epoch 242/1000 
	 loss: 382.6537, MinusLogProbMetric: 382.6537, val_loss: 405.7704, val_MinusLogProbMetric: 405.7704

Epoch 242: val_loss did not improve from 400.95160
196/196 - 18s - loss: 382.6537 - MinusLogProbMetric: 382.6537 - val_loss: 405.7704 - val_MinusLogProbMetric: 405.7704 - lr: 8.3333e-05 - 18s/epoch - 89ms/step
Epoch 243/1000
2023-09-10 06:17:44.174 
Epoch 243/1000 
	 loss: 382.8624, MinusLogProbMetric: 382.8624, val_loss: 406.5054, val_MinusLogProbMetric: 406.5054

Epoch 243: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.8624 - MinusLogProbMetric: 382.8624 - val_loss: 406.5054 - val_MinusLogProbMetric: 406.5054 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 244/1000
2023-09-10 06:18:00.909 
Epoch 244/1000 
	 loss: 382.8167, MinusLogProbMetric: 382.8167, val_loss: 405.8104, val_MinusLogProbMetric: 405.8104

Epoch 244: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.8167 - MinusLogProbMetric: 382.8167 - val_loss: 405.8104 - val_MinusLogProbMetric: 405.8104 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 245/1000
2023-09-10 06:18:18.271 
Epoch 245/1000 
	 loss: 382.9376, MinusLogProbMetric: 382.9376, val_loss: 406.7108, val_MinusLogProbMetric: 406.7108

Epoch 245: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9376 - MinusLogProbMetric: 382.9376 - val_loss: 406.7108 - val_MinusLogProbMetric: 406.7108 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 246/1000
2023-09-10 06:18:34.754 
Epoch 246/1000 
	 loss: 382.6778, MinusLogProbMetric: 382.6778, val_loss: 405.5001, val_MinusLogProbMetric: 405.5001

Epoch 246: val_loss did not improve from 400.95160
196/196 - 16s - loss: 382.6778 - MinusLogProbMetric: 382.6778 - val_loss: 405.5001 - val_MinusLogProbMetric: 405.5001 - lr: 8.3333e-05 - 16s/epoch - 84ms/step
Epoch 247/1000
2023-09-10 06:18:52.006 
Epoch 247/1000 
	 loss: 382.6861, MinusLogProbMetric: 382.6861, val_loss: 406.6807, val_MinusLogProbMetric: 406.6807

Epoch 247: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.6861 - MinusLogProbMetric: 382.6861 - val_loss: 406.6807 - val_MinusLogProbMetric: 406.6807 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 248/1000
2023-09-10 06:19:10.040 
Epoch 248/1000 
	 loss: 383.1760, MinusLogProbMetric: 383.1760, val_loss: 407.6643, val_MinusLogProbMetric: 407.6643

Epoch 248: val_loss did not improve from 400.95160
196/196 - 18s - loss: 383.1760 - MinusLogProbMetric: 383.1760 - val_loss: 407.6643 - val_MinusLogProbMetric: 407.6643 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 249/1000
2023-09-10 06:19:27.369 
Epoch 249/1000 
	 loss: 382.9162, MinusLogProbMetric: 382.9162, val_loss: 406.0206, val_MinusLogProbMetric: 406.0206

Epoch 249: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.9162 - MinusLogProbMetric: 382.9162 - val_loss: 406.0206 - val_MinusLogProbMetric: 406.0206 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 250/1000
2023-09-10 06:19:45.211 
Epoch 250/1000 
	 loss: 382.5957, MinusLogProbMetric: 382.5957, val_loss: 405.9200, val_MinusLogProbMetric: 405.9200

Epoch 250: val_loss did not improve from 400.95160
196/196 - 18s - loss: 382.5957 - MinusLogProbMetric: 382.5957 - val_loss: 405.9200 - val_MinusLogProbMetric: 405.9200 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 251/1000
2023-09-10 06:20:02.272 
Epoch 251/1000 
	 loss: 382.7314, MinusLogProbMetric: 382.7314, val_loss: 405.8457, val_MinusLogProbMetric: 405.8457

Epoch 251: val_loss did not improve from 400.95160
196/196 - 17s - loss: 382.7314 - MinusLogProbMetric: 382.7314 - val_loss: 405.8457 - val_MinusLogProbMetric: 405.8457 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 252/1000
2023-09-10 06:20:19.178 
Epoch 252/1000 
	 loss: 383.0242, MinusLogProbMetric: 383.0242, val_loss: 405.5313, val_MinusLogProbMetric: 405.5313

Epoch 252: val_loss did not improve from 400.95160
196/196 - 17s - loss: 383.0242 - MinusLogProbMetric: 383.0242 - val_loss: 405.5313 - val_MinusLogProbMetric: 405.5313 - lr: 8.3333e-05 - 17s/epoch - 86ms/step
Epoch 253/1000
2023-09-10 06:20:36.481 
Epoch 253/1000 
	 loss: 382.6803, MinusLogProbMetric: 382.6803, val_loss: 405.6356, val_MinusLogProbMetric: 405.6356

Epoch 253: val_loss did not improve from 400.95160
Restoring model weights from the end of the best epoch: 153.
196/196 - 18s - loss: 382.6803 - MinusLogProbMetric: 382.6803 - val_loss: 405.6356 - val_MinusLogProbMetric: 405.6356 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 253: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 3156.232371393009 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 3214.317387380055 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
SWD metric calculation completed in 3109.231023115921 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
FN metric calculation completed in 3178.916314102942 seconds.
Training succeeded with seed 377.
Model trained in 5082.51 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 12893.06 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 470, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 12893.56 s.
===========
Run 332/360 done in 18402.93 s.
===========

Directory ../../results/MAFN_new/run_333/ already exists.
Skipping it.
===========
Run 333/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_334/ already exists.
Skipping it.
===========
Run 334/360 already exists. Skipping it.
===========

===========
Generating train data for run 335.
===========
Train data generated in 2.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.079587 ,  6.1984696,  7.6198487, ...,  9.8733   ,  1.2133766,
         6.3345976],
       [ 5.234588 ,  7.498538 ,  5.7464294, ..., 10.933875 ,  0.4187709,
         6.5884576],
       [ 7.800186 ,  4.269176 ,  5.1269364, ...,  2.4002125,  8.245506 ,
         6.792781 ],
       ...,
       [ 5.657867 ,  0.4851762,  4.7903004, ...,  5.063982 ,  6.747285 ,
         3.2595367],
       [ 7.8507524,  4.4384494,  5.2666016, ...,  3.8509533,  8.5486555,
         6.834612 ],
       [ 8.079243 ,  4.798266 ,  5.2613087, ...,  3.0539343,  8.016419 ,
         6.717349 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_335
self.data_kwargs: {'seed': 440}
self.x_data: [[ 5.975928    0.18705393  4.579732   ...  5.052782    6.363114
   3.5951567 ]
 [ 8.245443    4.908681    5.163151   ...  2.2962263   8.159441
   6.519855  ]
 [ 5.979998    0.06695901  4.8433633  ...  4.708263    6.650848
   7.1221867 ]
 ...
 [ 8.616862    3.848415    5.171632   ...  3.7576704   8.165923
   7.0578055 ]
 [ 8.209467    4.7939653   5.2042055  ...  3.0320504   8.283145
   7.183658  ]
 [ 6.448749   -0.35270444  4.789608   ...  4.7734337   6.228376
   4.513551  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_56 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  4191520   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7faa1c4dda80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fae7ba3fe80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fae7ba3fe80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae7ba36410>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fae7ba6a410>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faa1c30c640>, <keras.callbacks.ModelCheckpoint object at 0x7fae83de1240>, <keras.callbacks.EarlyStopping object at 0x7fae7b9c7610>, <keras.callbacks.ReduceLROnPlateau object at 0x7fae7b9c40d0>, <keras.callbacks.TerminateOnNaN object at 0x7faa1c360ac0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.079587 ,  6.1984696,  7.6198487, ...,  9.8733   ,  1.2133766,
         6.3345976],
       [ 5.234588 ,  7.498538 ,  5.7464294, ..., 10.933875 ,  0.4187709,
         6.5884576],
       [ 7.800186 ,  4.269176 ,  5.1269364, ...,  2.4002125,  8.245506 ,
         6.792781 ],
       ...,
       [ 5.657867 ,  0.4851762,  4.7903004, ...,  5.063982 ,  6.747285 ,
         3.2595367],
       [ 7.8507524,  4.4384494,  5.2666016, ...,  3.8509533,  8.5486555,
         6.834612 ],
       [ 8.079243 ,  4.798266 ,  5.2613087, ...,  3.0539343,  8.016419 ,
         6.717349 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/360 with hyperparameters:
timestamp = 2023-09-10 09:55:38.392385
ndims = 1000
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.9759278e+00  1.8705393e-01  4.5797319e+00  7.3813486e+00
  4.9839023e-01  9.1128473e+00  5.1425128e+00  1.0601972e+00
  1.8690022e+00  9.7343407e+00  6.0127983e+00  1.2467406e+00
  2.1360984e+00  5.5741024e+00  8.1917816e-01  3.7718730e+00
  5.1906404e+00  3.0766895e+00  6.0415301e+00  7.3736296e+00
  2.8012948e+00  7.5884948e+00  7.1015620e+00  1.0307134e+01
  6.5797253e+00  7.9973593e+00  3.9854233e+00  1.8046517e+00
  9.7217255e+00  1.0974171e+00  5.1221895e+00  8.3442938e-01
  9.2890491e+00  9.2900143e+00  7.2703452e+00  3.3996055e+00
  2.9350464e+00  2.6254275e+00  1.3082062e+00  5.2701192e+00
  1.3725662e+00  3.2777114e+00  6.2085199e+00  7.2575483e+00
  5.7510644e-02  4.4865317e+00  6.2462301e+00  8.9988785e+00
  4.7538991e+00  9.9086142e+00  2.3183393e+00  4.7639337e-01
  4.5737805e+00  2.5859902e+00  8.1949062e+00  2.6238203e+00
  3.0378006e+00  7.9298201e+00  8.4647675e+00  4.9066219e+00
  6.4023814e+00  3.9823089e+00 -4.2869711e-01  1.5175290e-02
  6.1451683e+00 -2.2587168e-01 -9.5339119e-02  8.5243511e+00
  9.1139994e+00 -1.0624671e-01 -6.7571568e-01  9.4670458e+00
  2.6047506e+00  9.5776010e-01  6.6801319e+00  9.6847260e-01
  3.1678288e+00  9.7247944e+00  6.2540312e+00  6.3839302e+00
  3.4341021e+00  7.7830091e+00  1.6556205e+00  9.9951563e+00
  8.6061764e+00  6.2981930e+00  1.2968564e+00  8.2717495e+00
  9.2503586e+00  5.0443635e+00  4.8242106e+00  1.5439949e+00
  7.4578109e+00  9.9600182e+00  1.0782404e+00  7.4997787e+00
 -6.1716002e-01  1.0503052e+01  6.2509499e+00  5.6324120e+00
  2.8450089e+00  8.2806787e+00  2.7606368e+00  8.2886658e+00
  1.0619202e+01  7.7074742e+00  4.5604439e+00  1.0194874e+00
  4.4170108e+00  4.2977200e+00  1.7771544e+00  4.3124924e+00
  3.5204492e+00  7.5383079e-01  4.9013953e+00  1.8483464e+00
  5.2527041e+00  2.9501736e-01  9.0906477e+00  6.7014809e+00
  5.4151597e+00  8.2172470e+00  9.6032486e+00  1.1688474e+00
  7.4652319e+00  9.9523344e+00  1.0106757e+01  5.6442394e+00
  1.2893100e+00  6.2976708e+00 -6.0980135e-01  2.0239670e+00
  9.4813910e+00  9.1562080e-01  6.0650496e+00  3.3473954e+00
  9.4390936e+00  6.5065956e+00  2.8910985e+00  7.6608114e+00
  1.1822252e+00  2.8502061e+00  2.2126217e+00  7.5678649e+00
  1.3060629e+00  4.1730914e+00  2.7150269e+00  4.2508979e+00
  3.1311915e+00  8.0161762e+00  1.1140229e+00  6.6955643e+00
  2.3109643e+00  2.5608201e+00  9.0579319e+00  8.9470587e+00
  1.9467136e+00  7.7616472e+00  1.1755015e+00  6.9813056e+00
  7.1541829e+00  1.1629101e+00  4.0641637e+00  8.5495291e+00
  1.0037423e+00  3.7546790e-01  5.4117308e+00  3.8483400e+00
  7.5943928e+00  1.0048924e+01  3.0955133e+00  3.7507610e+00
  4.7484083e+00  5.6718163e+00  9.9414043e+00  6.6468272e+00
  5.5795021e+00  2.8773146e+00  9.3531322e+00  1.8336586e+00
  8.0805283e+00  9.5689878e+00  7.0799055e+00  4.9151144e+00
  2.3026161e+00  3.8051813e+00  7.9677577e+00  6.0892525e+00
  9.0358343e+00  8.3774691e+00  8.6793871e+00  8.7717085e+00
  2.5660608e+00  4.4672542e+00  3.7499821e+00  6.0084786e+00
  6.4533715e+00  2.9849510e+00  6.0209244e-01  4.3385081e+00
  3.7030458e+00  9.5341787e+00  9.2192812e+00  7.2649994e+00
  6.9098902e+00  2.3594141e+00  5.7481446e+00  8.3444004e+00
  1.5395805e+00  8.0469885e+00  7.2350705e-01  7.6298466e+00
  7.5203185e+00  6.8629438e-01  6.1138077e+00  5.6196861e+00
  1.5752215e+00  1.0186056e+00  7.5847526e+00  4.9043198e+00
  1.0365025e+01  1.0957176e+01  1.0342247e+01  2.2840269e+00
  8.8948154e+00  2.1656735e+00  4.4965944e+00  6.3290133e+00
  1.9084119e+00  6.1508875e+00  7.1696973e-01  1.0796700e+00
  1.2289739e+00  3.7995231e+00  4.2479050e-01  4.0374312e+00
  6.2082539e+00  3.8042982e+00  1.1126742e+00 -2.8491318e-02
  2.3989291e+00  1.5484333e+00  9.0736609e+00  2.2606084e+00
  7.8717704e+00  5.8197365e+00  5.3839073e+00  4.4864936e+00
  9.7097330e+00  5.1985941e+00  7.8766909e+00  4.8876739e+00
  3.9949443e+00  7.6952267e+00  6.1897888e+00  1.1930196e+00
  4.7831144e+00  7.5184402e+00  7.9867325e+00  9.8890352e+00
  2.8126411e+00  6.6819544e+00  2.9722786e+00  4.3866873e+00
  9.6697216e+00  9.3694468e+00  8.5144243e+00  4.4165196e+00
  8.8433590e+00  4.9158731e+00  4.2708116e+00  6.1693680e-01
  6.9947834e+00  4.8547955e+00  8.8329058e+00  7.1928735e+00
  8.0629263e+00  3.3316441e+00  8.6988659e+00  5.4551535e+00
  5.0940285e+00  7.1205068e+00 -7.6878732e-01  1.4425153e+00
  2.9650118e+00  9.9633141e+00  1.3754138e+00  3.5696523e+00
  1.2369231e+00  3.2220390e+00  4.1846347e+00  9.2791929e+00
  7.7624817e+00  3.6623871e+00  7.8092593e-01  7.3296189e+00
  5.7747946e+00  2.3126864e-01  9.6106710e+00  9.5668274e-01
  8.3702803e+00  2.9747019e+00  4.3602333e+00  7.8842506e+00
  6.2896223e+00  8.5025053e+00  6.1606364e+00  1.0106425e+00
  8.6921835e+00  7.3920555e+00  7.1117039e+00  1.2270844e+00
  4.3614955e+00  2.5761406e+00  8.1967859e+00  6.2408938e+00
  9.4475508e+00  9.6550465e+00  8.4041348e+00  1.0130975e+01
  5.6460967e+00  4.0990400e-01 -8.8367611e-03  3.5952890e+00
  9.2217665e+00  8.4936142e+00  9.9719982e+00  6.2442482e-01
  1.5329283e+00  8.9031639e+00  2.9812365e+00  4.6210904e+00
  9.6318512e+00  3.3671608e+00  6.6170344e+00  4.0481849e+00
  7.8299704e+00  4.1430693e+00  2.1972313e+00  2.7806988e+00
  2.4690957e+00  6.2265091e+00  8.0078650e+00  8.5526848e+00
  7.8363638e+00  6.6496010e+00  1.7129709e+00  3.3299735e+00
  4.1951280e+00  7.1778541e+00  2.9357438e+00  3.2394106e+00
  4.3329515e+00  7.8274932e+00  5.8496819e+00  4.6137288e-01
  7.5736916e-01  3.1835232e+00  7.2212210e+00  7.3863716e+00
  7.1284332e+00  7.4827847e+00  2.6439285e+00  5.7008662e+00
  5.0590334e+00  8.0088997e+00  9.6267881e+00  2.4991486e+00
  8.2448721e+00  6.5369444e+00  6.2472978e+00  7.7897358e+00
  9.4234190e+00  9.6731400e-01  1.6939137e+00  7.3830414e+00
  1.0644419e+00  5.3757925e+00  4.4202948e+00  1.9308488e+00
  8.0857773e+00 -3.4219015e-01  8.6868877e+00  4.4470925e+00
  4.9024949e+00  5.6183209e+00  6.1064873e+00  3.4613369e+00
  1.2080189e+00  3.4117126e+00  4.1149378e-02  6.4430194e+00
  1.9570980e+00  9.7712898e+00  9.6677971e+00  6.8640056e+00
  6.0994020e+00  1.0187636e+01  8.8685141e+00  8.2290039e+00
  3.1689963e+00  3.6186795e+00  2.5079422e+00  3.3114364e+00
  9.9208021e+00  9.1239557e+00  1.3143113e+00  6.8661511e-01
  8.9672060e+00  5.2539353e+00  1.7683913e+00  6.8201876e+00
  6.3931870e+00  1.2865118e+00  3.4232042e+00  1.2203555e+00
  2.0503523e+00  8.4730494e-01  1.9659302e+00  6.6408830e+00
  3.0317956e-01  8.8737220e-01  2.3188889e-01  3.0580165e+00
  3.1296854e+00  2.1522398e+00  7.4903617e+00  9.1460247e+00
  5.0951142e+00  8.5311861e+00  3.9055674e+00 -2.2732937e-01
  4.0364518e+00  2.6478162e+00  5.1365023e+00  7.4299526e+00
  4.6355362e+00  8.9175758e+00  8.7688503e+00  5.5298843e+00
  4.4609528e+00  1.2880317e+00  4.0175514e+00  6.7396054e+00
  3.7963538e+00  1.2283164e+00  9.0873432e+00  7.1056828e+00
  5.5581961e+00  8.0034761e+00  8.3522511e+00  4.6975536e+00
  8.0432367e+00  8.6975918e+00  9.9233341e-01  6.2826734e+00
  5.9927521e+00  5.0778217e+00  8.8290005e+00  2.5841575e+00
  9.9096069e+00  8.9677849e+00  4.5229397e+00  8.1403732e+00
  6.6729965e+00  6.2468095e+00  3.7025385e+00  2.1858666e+00
  7.0194511e+00  8.6221571e+00 -2.3119065e-01  6.4518013e+00
  4.6490231e+00  9.1139078e+00  2.6489720e+00  4.4286499e+00
  4.7887325e+00  1.6389720e+00  8.0717764e+00  8.0382366e+00
  1.3237777e+00  2.9469595e+00  6.6192156e-01  9.5147648e+00
  6.1208944e+00  9.9296083e+00  5.8978138e+00  6.4843559e+00
  4.2908926e+00  3.5816567e+00  3.7703094e+00  7.1296864e+00
  3.3137634e+00  9.1458826e+00  1.0168221e+01  4.6002930e-01
  1.9140388e+00  7.9359615e-01  5.7083476e-01  2.0637333e+00
  4.4295535e+00  8.4761515e+00  2.1790206e+00  4.8846216e+00
  1.1725900e+01  6.4370713e+00  7.5755668e+00  5.3716149e+00
  9.8037472e+00  6.0171165e+00 -1.0524909e+00  2.8919909e+00
  3.9641018e+00  5.4104433e+00  7.7196703e+00  4.5299621e+00
  7.0386343e+00  8.1372356e+00  2.8667083e+00  9.8477182e+00
  6.2275105e+00  8.9304132e+00  5.8799353e+00  7.2770109e+00
  7.7765956e+00  9.1447611e+00  1.3774345e+00  4.7782230e+00
  4.5103283e+00  9.6529636e+00  1.0817671e+00  9.6128826e+00
  8.8991022e+00  7.8995752e-01  5.2336302e+00  7.1597772e+00
  6.9343681e+00  9.0601816e+00  8.0068483e+00  3.5910506e+00
  5.0274186e+00  7.0993495e+00  3.2836149e+00  1.3527107e+00
  3.3847008e+00  3.9555850e+00  8.4203587e+00  5.7436528e+00
  4.4528852e+00  8.3769274e+00  8.4812574e+00  9.8149929e+00
  8.4825897e+00  4.3883948e+00  1.1693200e+00  8.5065060e+00
  1.7731776e+00  3.4153044e+00  9.9896975e+00  8.2860680e+00
  6.3954239e+00  7.1741295e+00  6.7204058e-01  5.1233587e+00
  6.6728873e+00  3.4099829e+00  3.0468628e+00  2.3115411e+00
  4.1901307e+00  4.4959798e+00  5.7105017e+00  7.4096904e+00
  2.9691739e+00  2.5698562e+00  3.1576818e-01 -5.9249759e-02
  2.4639745e+00  7.8125792e+00  7.1327791e+00  7.7584710e+00
  6.2746820e+00  8.4847488e+00  6.9726362e+00  1.0046572e+01
  8.4835949e+00  6.7527709e+00  5.9673371e+00  4.3935239e-01
  1.1263874e+01  1.3709677e+00  7.3908615e+00  7.9875464e+00
  3.9839177e+00  2.2393181e+00  8.4422034e-01  6.9838266e+00
  4.5242758e+00  7.4394574e+00  9.1695070e+00  1.5673304e+00
  9.3064995e+00  3.2363920e+00  3.0193369e+00  9.7506819e+00
  9.7927599e+00  2.1529837e+00  5.8980074e+00  1.1057787e+00
  6.5593362e+00  8.3812113e+00  5.5978317e+00  3.2321651e+00
  3.8758488e+00  4.0856094e+00  7.0214200e+00  8.6556015e+00
  9.3241930e+00  2.9402196e-02  1.4503651e+00  4.8388457e-01
  2.0156007e+00  4.3253636e+00  5.0806727e+00  6.2870202e+00
  8.3587704e+00  9.1714058e+00  1.1041972e+01  6.7473054e+00
  8.3084793e+00  7.5109200e+00  3.7499793e+00  7.7249527e+00
  3.9467406e-01  6.4965940e-01  2.9256980e+00  4.6755557e+00
  6.6328969e+00  1.9519567e-01  7.0285888e+00  8.4598532e+00
  1.9452683e+00  4.4380093e+00  4.9697065e+00  4.4265666e+00
  7.9045653e+00  9.4661942e+00  8.0525227e+00  8.8097310e-01
  6.8595657e+00  1.7693057e+00  2.4858186e+00  1.4309098e+00
  1.8431244e+00  4.7576222e+00  6.7128339e+00  4.3999271e+00
  9.4328070e+00  2.1000056e+00  7.5769453e+00  8.7397270e+00
  5.8346295e+00  4.8793106e+00  4.7135677e+00  4.4952607e+00
  5.7733207e+00  3.9166284e+00  4.1439590e+00  6.8955579e+00
  6.8403614e-01  2.0110247e+00  9.0629015e+00  8.0548077e+00
  2.7293277e+00  4.2169027e+00  9.2188635e+00  9.0293961e+00
  6.1455669e+00  2.1301951e+00  5.6030965e-01  4.4237075e+00
  7.3219604e+00  8.5027437e+00  8.5191278e+00  4.1766196e-01
  2.4718022e+00  3.2887347e+00  1.7133644e+00  7.1094151e+00
  7.3223224e+00  4.3398414e+00  9.8931112e+00  1.3441966e+00
  9.0235968e+00  1.2211851e+00  2.6455071e+00  7.5472803e+00
  1.1029721e+00  4.7136912e+00  2.8467078e+00  7.3037176e+00
  9.4062443e+00  7.6552663e+00  8.4975100e+00  6.3747257e-02
  2.3440433e+00  5.1499615e+00  3.5230517e+00  1.9831378e+00
  3.5230274e+00  9.6046629e+00  3.3157225e+00  4.3171234e+00
  1.3997978e+00  7.7749600e+00  3.3477159e+00  1.9058161e+00
  2.5061352e+00  9.0734358e+00  2.0535131e+00  8.7670698e+00
  7.5356512e+00  1.0142322e+01 -3.0124056e-01  2.2794275e+00
  7.4697652e+00  1.9831645e+00  2.1713405e+00  2.2061615e+00
  1.5869830e+00  4.8229289e+00  1.4017915e+00  3.4431477e+00
  4.9791493e+00  7.0362866e-01  4.3548961e+00  6.2790900e-01
  5.3626833e+00  9.2004957e+00  7.4519486e+00  9.5844626e-01
  5.9674959e+00  8.2974005e+00  2.5478227e+00  7.6181903e+00
  8.8209562e+00  7.9951472e+00  2.8410530e-01  4.9098748e-01
  2.0705619e+00  2.6533947e+00  5.7332134e+00  2.9440532e+00
  7.7568593e+00  3.6492581e+00  2.4384577e+00  2.8873272e+00
  9.4621639e+00  9.1508560e+00  2.0058784e+00  6.5649538e+00
  2.7415826e+00  8.4922266e+00  4.9448359e-01  7.5271797e+00
 -1.5808821e-01  9.7426481e+00  6.3803730e+00  8.4051380e+00
  5.7340975e+00  4.6641579e+00  6.2364306e+00  5.1810746e+00
  5.3571901e+00  8.9868603e+00  1.8257052e+00  8.8656920e-01
  4.3585706e+00  3.5430098e+00  7.3958582e-01  8.3649902e+00
  7.7803888e+00  5.7408032e+00  9.7680950e+00  6.9590604e-01
  7.2661672e+00  2.1069894e+00  3.6764739e+00  3.4602623e+00
  7.6012840e+00  2.0565724e-01  8.5318031e+00  7.2798491e+00
  1.4081079e+00  8.3596611e+00  7.2540979e+00  9.5279789e+00
  7.3339424e+00  5.2405219e+00  8.5954704e+00  2.9429979e+00
  5.2126360e+00  3.8612814e+00  9.0659313e+00  1.9871573e+00
  3.8985163e-01  3.2772667e+00  4.5025587e+00  1.2284964e-02
  6.7665458e+00  9.5286417e+00  7.1374736e+00  6.0702968e+00
  9.5389080e+00  9.5242043e+00  8.4405804e+00  6.5415311e+00
  4.7715144e+00  1.9164640e-01  6.2476826e+00  9.6162710e+00
  8.1386156e+00  3.4102411e+00  1.4985125e+00  7.9778295e+00
  7.4183102e+00  9.5704803e+00  7.8087254e+00  6.9231396e+00
  1.3841813e+00  4.7959905e+00  2.9273345e+00  5.1850367e+00
  3.0295143e+00  8.6389799e+00  9.0257702e+00  3.6582994e+00
  5.9157324e+00  4.1758962e+00  9.3391609e+00  1.7730776e+00
  7.6125379e+00  1.0046360e+01  2.1057978e+00  1.7478250e+00
  7.1132212e+00  7.9645081e+00  5.2505498e+00  6.4574647e+00
  3.2315047e+00  4.3446302e+00  1.2056727e+00  2.9674428e+00
  2.7837610e+00  4.7524495e+00  9.8256521e+00  1.5932522e+00
  4.8396316e+00  5.9174523e+00  9.4890871e+00  1.2948707e+00
  9.0378551e+00  1.3124785e+00  5.4293876e+00  4.4993534e+00
  2.0538125e+00  3.5617321e+00  5.4850354e+00  8.2304430e+00
  9.7063475e+00  4.5277615e+00  8.3297014e+00  6.6988935e+00
  3.0370085e+00  2.5210819e+00  4.3796597e+00  5.5016708e+00
  1.9651670e+00  2.4778838e+00  5.8789811e+00  3.1068649e+00
  8.4105711e+00  2.6629505e+00 -3.3057988e-01  9.0704960e-01
  9.3188791e+00  1.0806879e+00 -8.1481934e-03  9.1659679e+00
  2.1467557e+00  7.2326632e+00  8.2058630e+00  5.8045855e+00
  8.4636183e+00  6.8212204e+00  5.0721443e-01  5.9133248e+00
  7.4281392e+00  7.8691220e+00 -6.8311334e-02  7.2124262e+00
  8.6880360e+00  6.6583366e+00  5.0127316e+00  4.6381230e+00
  5.9561200e+00  6.8466635e+00  9.5994196e+00  6.0355244e+00
  8.7480297e+00  7.6578970e+00  1.2954912e+00  4.2609344e+00
  2.7702582e+00  3.0329621e+00  3.6327574e+00  3.4195125e+00
  5.5413127e+00  5.8191652e+00  2.7070973e+00  1.6614884e+00
  3.8416210e-01  6.2802663e+00  2.0622129e+00 -6.8790317e-01
  9.2128534e+00  4.7458625e+00  9.4553316e-01  8.3560362e+00
  9.9401188e+00  3.5302291e+00  4.7089720e-01  1.3786711e+00
  5.3229847e+00  9.2913418e+00  1.6849303e+00  8.2535496e+00
 -2.5821114e-01  4.1421123e+00  6.4032269e+00  6.6551906e-01
  5.3515515e+00  9.1520185e+00  1.5094744e+00  7.4785428e+00
  6.0626564e+00  1.8558639e+00  4.4214793e-02  7.5862879e-01
  4.2469244e+00  8.8614159e+00  6.5688152e+00  7.3740344e+00
  4.8993049e+00  5.7953315e+00  9.2521513e-01  8.7253218e+00
  7.7945423e+00  9.9947720e+00  3.9001837e+00  2.8082497e+00
  2.2034130e+00  9.5151176e+00  7.4653130e+00  5.4289103e-01
  5.5686536e+00  4.1999950e+00  1.3804873e+00  1.5220041e+00
  9.6445208e+00  3.1844208e+00  8.2594252e+00  5.8706766e-01
  3.8130655e+00  9.4813788e-01  1.4116070e-01  5.6962514e+00
  2.8564551e+00  7.0057955e+00  6.3632298e+00  1.4823904e+00
  9.5153828e+00  7.8791199e+00  2.7109826e+00  5.3147154e+00
  7.3835139e+00  3.6906118e+00  4.5995116e+00  4.1400166e+00
  6.1948090e+00  8.8411551e+00  1.6546024e+00  8.7375402e+00
  5.7898629e-01  6.4826312e+00  4.3135948e+00  7.4583459e+00
  6.8681207e+00  5.0527821e+00  6.3631139e+00  3.5951567e+00]
Epoch 1/1000
2023-09-10 09:57:04.371 
Epoch 1/1000 
	 loss: 1638.7583, MinusLogProbMetric: 1638.7583, val_loss: 615.0331, val_MinusLogProbMetric: 615.0331

Epoch 1: val_loss improved from inf to 615.03308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 86s - loss: 1638.7583 - MinusLogProbMetric: 1638.7583 - val_loss: 615.0331 - val_MinusLogProbMetric: 615.0331 - lr: 0.0010 - 86s/epoch - 440ms/step
Epoch 2/1000
2023-09-10 09:57:23.929 
Epoch 2/1000 
	 loss: 557.3962, MinusLogProbMetric: 557.3962, val_loss: 556.0186, val_MinusLogProbMetric: 556.0186

Epoch 2: val_loss improved from 615.03308 to 556.01862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 557.3962 - MinusLogProbMetric: 557.3962 - val_loss: 556.0186 - val_MinusLogProbMetric: 556.0186 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 3/1000
2023-09-10 09:57:41.324 
Epoch 3/1000 
	 loss: 511.0398, MinusLogProbMetric: 511.0398, val_loss: 499.0501, val_MinusLogProbMetric: 499.0501

Epoch 3: val_loss improved from 556.01862 to 499.05008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 17s - loss: 511.0398 - MinusLogProbMetric: 511.0398 - val_loss: 499.0501 - val_MinusLogProbMetric: 499.0501 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 4/1000
2023-09-10 09:57:59.041 
Epoch 4/1000 
	 loss: 492.3588, MinusLogProbMetric: 492.3588, val_loss: 486.6398, val_MinusLogProbMetric: 486.6398

Epoch 4: val_loss improved from 499.05008 to 486.63980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 492.3588 - MinusLogProbMetric: 492.3588 - val_loss: 486.6398 - val_MinusLogProbMetric: 486.6398 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 5/1000
2023-09-10 09:58:16.600 
Epoch 5/1000 
	 loss: 479.8795, MinusLogProbMetric: 479.8795, val_loss: 472.4250, val_MinusLogProbMetric: 472.4250

Epoch 5: val_loss improved from 486.63980 to 472.42502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 479.8795 - MinusLogProbMetric: 479.8795 - val_loss: 472.4250 - val_MinusLogProbMetric: 472.4250 - lr: 0.0010 - 18s/epoch - 89ms/step
Epoch 6/1000
2023-09-10 09:58:33.866 
Epoch 6/1000 
	 loss: 471.6506, MinusLogProbMetric: 471.6506, val_loss: 467.2442, val_MinusLogProbMetric: 467.2442

Epoch 6: val_loss improved from 472.42502 to 467.24423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 17s - loss: 471.6506 - MinusLogProbMetric: 471.6506 - val_loss: 467.2442 - val_MinusLogProbMetric: 467.2442 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 7/1000
2023-09-10 09:58:52.231 
Epoch 7/1000 
	 loss: 463.1447, MinusLogProbMetric: 463.1447, val_loss: 457.2586, val_MinusLogProbMetric: 457.2586

Epoch 7: val_loss improved from 467.24423 to 457.25861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 463.1447 - MinusLogProbMetric: 463.1447 - val_loss: 457.2586 - val_MinusLogProbMetric: 457.2586 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 8/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 58: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-10 09:58:58.855 
Epoch 8/1000 
	 loss: inf, MinusLogProbMetric: 2822403325912494732312805126438912.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 8: val_loss did not improve from 457.25861
196/196 - 6s - loss: inf - MinusLogProbMetric: 2822403325912494732312805126438912.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 6s/epoch - 31ms/step
The loss history contains Inf values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 335.
===========
Train data generated in 2.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.079587 ,  6.1984696,  7.6198487, ...,  9.8733   ,  1.2133766,
         6.3345976],
       [ 5.234588 ,  7.498538 ,  5.7464294, ..., 10.933875 ,  0.4187709,
         6.5884576],
       [ 7.800186 ,  4.269176 ,  5.1269364, ...,  2.4002125,  8.245506 ,
         6.792781 ],
       ...,
       [ 5.657867 ,  0.4851762,  4.7903004, ...,  5.063982 ,  6.747285 ,
         3.2595367],
       [ 7.8507524,  4.4384494,  5.2666016, ...,  3.8509533,  8.5486555,
         6.834612 ],
       [ 8.079243 ,  4.798266 ,  5.2613087, ...,  3.0539343,  8.016419 ,
         6.717349 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_335
self.data_kwargs: {'seed': 440}
self.x_data: [[ 5.975928    0.18705393  4.579732   ...  5.052782    6.363114
   3.5951567 ]
 [ 8.245443    4.908681    5.163151   ...  2.2962263   8.159441
   6.519855  ]
 [ 5.979998    0.06695901  4.8433633  ...  4.708263    6.650848
   7.1221867 ]
 ...
 [ 8.616862    3.848415    5.171632   ...  3.7576704   8.165923
   7.0578055 ]
 [ 8.209467    4.7939653   5.2042055  ...  3.0320504   8.283145
   7.183658  ]
 [ 6.448749   -0.35270444  4.789608   ...  4.7734337   6.228376
   4.513551  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  4191520   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7faa1c4dc040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fae9d8cd6f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fae9d8cd6f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae9db3ba30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7faa7c3548b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faa7c57d2d0>, <keras.callbacks.ModelCheckpoint object at 0x7faa7c57d3c0>, <keras.callbacks.EarlyStopping object at 0x7faa7c57d540>, <keras.callbacks.ReduceLROnPlateau object at 0x7faa7c57e650>, <keras.callbacks.TerminateOnNaN object at 0x7faa7c57d7b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.079587 ,  6.1984696,  7.6198487, ...,  9.8733   ,  1.2133766,
         6.3345976],
       [ 5.234588 ,  7.498538 ,  5.7464294, ..., 10.933875 ,  0.4187709,
         6.5884576],
       [ 7.800186 ,  4.269176 ,  5.1269364, ...,  2.4002125,  8.245506 ,
         6.792781 ],
       ...,
       [ 5.657867 ,  0.4851762,  4.7903004, ...,  5.063982 ,  6.747285 ,
         3.2595367],
       [ 7.8507524,  4.4384494,  5.2666016, ...,  3.8509533,  8.5486555,
         6.834612 ],
       [ 8.079243 ,  4.798266 ,  5.2613087, ...,  3.0539343,  8.016419 ,
         6.717349 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 335/360 with hyperparameters:
timestamp = 2023-09-10 09:59:08.064219
ndims = 1000
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.9759278e+00  1.8705393e-01  4.5797319e+00  7.3813486e+00
  4.9839023e-01  9.1128473e+00  5.1425128e+00  1.0601972e+00
  1.8690022e+00  9.7343407e+00  6.0127983e+00  1.2467406e+00
  2.1360984e+00  5.5741024e+00  8.1917816e-01  3.7718730e+00
  5.1906404e+00  3.0766895e+00  6.0415301e+00  7.3736296e+00
  2.8012948e+00  7.5884948e+00  7.1015620e+00  1.0307134e+01
  6.5797253e+00  7.9973593e+00  3.9854233e+00  1.8046517e+00
  9.7217255e+00  1.0974171e+00  5.1221895e+00  8.3442938e-01
  9.2890491e+00  9.2900143e+00  7.2703452e+00  3.3996055e+00
  2.9350464e+00  2.6254275e+00  1.3082062e+00  5.2701192e+00
  1.3725662e+00  3.2777114e+00  6.2085199e+00  7.2575483e+00
  5.7510644e-02  4.4865317e+00  6.2462301e+00  8.9988785e+00
  4.7538991e+00  9.9086142e+00  2.3183393e+00  4.7639337e-01
  4.5737805e+00  2.5859902e+00  8.1949062e+00  2.6238203e+00
  3.0378006e+00  7.9298201e+00  8.4647675e+00  4.9066219e+00
  6.4023814e+00  3.9823089e+00 -4.2869711e-01  1.5175290e-02
  6.1451683e+00 -2.2587168e-01 -9.5339119e-02  8.5243511e+00
  9.1139994e+00 -1.0624671e-01 -6.7571568e-01  9.4670458e+00
  2.6047506e+00  9.5776010e-01  6.6801319e+00  9.6847260e-01
  3.1678288e+00  9.7247944e+00  6.2540312e+00  6.3839302e+00
  3.4341021e+00  7.7830091e+00  1.6556205e+00  9.9951563e+00
  8.6061764e+00  6.2981930e+00  1.2968564e+00  8.2717495e+00
  9.2503586e+00  5.0443635e+00  4.8242106e+00  1.5439949e+00
  7.4578109e+00  9.9600182e+00  1.0782404e+00  7.4997787e+00
 -6.1716002e-01  1.0503052e+01  6.2509499e+00  5.6324120e+00
  2.8450089e+00  8.2806787e+00  2.7606368e+00  8.2886658e+00
  1.0619202e+01  7.7074742e+00  4.5604439e+00  1.0194874e+00
  4.4170108e+00  4.2977200e+00  1.7771544e+00  4.3124924e+00
  3.5204492e+00  7.5383079e-01  4.9013953e+00  1.8483464e+00
  5.2527041e+00  2.9501736e-01  9.0906477e+00  6.7014809e+00
  5.4151597e+00  8.2172470e+00  9.6032486e+00  1.1688474e+00
  7.4652319e+00  9.9523344e+00  1.0106757e+01  5.6442394e+00
  1.2893100e+00  6.2976708e+00 -6.0980135e-01  2.0239670e+00
  9.4813910e+00  9.1562080e-01  6.0650496e+00  3.3473954e+00
  9.4390936e+00  6.5065956e+00  2.8910985e+00  7.6608114e+00
  1.1822252e+00  2.8502061e+00  2.2126217e+00  7.5678649e+00
  1.3060629e+00  4.1730914e+00  2.7150269e+00  4.2508979e+00
  3.1311915e+00  8.0161762e+00  1.1140229e+00  6.6955643e+00
  2.3109643e+00  2.5608201e+00  9.0579319e+00  8.9470587e+00
  1.9467136e+00  7.7616472e+00  1.1755015e+00  6.9813056e+00
  7.1541829e+00  1.1629101e+00  4.0641637e+00  8.5495291e+00
  1.0037423e+00  3.7546790e-01  5.4117308e+00  3.8483400e+00
  7.5943928e+00  1.0048924e+01  3.0955133e+00  3.7507610e+00
  4.7484083e+00  5.6718163e+00  9.9414043e+00  6.6468272e+00
  5.5795021e+00  2.8773146e+00  9.3531322e+00  1.8336586e+00
  8.0805283e+00  9.5689878e+00  7.0799055e+00  4.9151144e+00
  2.3026161e+00  3.8051813e+00  7.9677577e+00  6.0892525e+00
  9.0358343e+00  8.3774691e+00  8.6793871e+00  8.7717085e+00
  2.5660608e+00  4.4672542e+00  3.7499821e+00  6.0084786e+00
  6.4533715e+00  2.9849510e+00  6.0209244e-01  4.3385081e+00
  3.7030458e+00  9.5341787e+00  9.2192812e+00  7.2649994e+00
  6.9098902e+00  2.3594141e+00  5.7481446e+00  8.3444004e+00
  1.5395805e+00  8.0469885e+00  7.2350705e-01  7.6298466e+00
  7.5203185e+00  6.8629438e-01  6.1138077e+00  5.6196861e+00
  1.5752215e+00  1.0186056e+00  7.5847526e+00  4.9043198e+00
  1.0365025e+01  1.0957176e+01  1.0342247e+01  2.2840269e+00
  8.8948154e+00  2.1656735e+00  4.4965944e+00  6.3290133e+00
  1.9084119e+00  6.1508875e+00  7.1696973e-01  1.0796700e+00
  1.2289739e+00  3.7995231e+00  4.2479050e-01  4.0374312e+00
  6.2082539e+00  3.8042982e+00  1.1126742e+00 -2.8491318e-02
  2.3989291e+00  1.5484333e+00  9.0736609e+00  2.2606084e+00
  7.8717704e+00  5.8197365e+00  5.3839073e+00  4.4864936e+00
  9.7097330e+00  5.1985941e+00  7.8766909e+00  4.8876739e+00
  3.9949443e+00  7.6952267e+00  6.1897888e+00  1.1930196e+00
  4.7831144e+00  7.5184402e+00  7.9867325e+00  9.8890352e+00
  2.8126411e+00  6.6819544e+00  2.9722786e+00  4.3866873e+00
  9.6697216e+00  9.3694468e+00  8.5144243e+00  4.4165196e+00
  8.8433590e+00  4.9158731e+00  4.2708116e+00  6.1693680e-01
  6.9947834e+00  4.8547955e+00  8.8329058e+00  7.1928735e+00
  8.0629263e+00  3.3316441e+00  8.6988659e+00  5.4551535e+00
  5.0940285e+00  7.1205068e+00 -7.6878732e-01  1.4425153e+00
  2.9650118e+00  9.9633141e+00  1.3754138e+00  3.5696523e+00
  1.2369231e+00  3.2220390e+00  4.1846347e+00  9.2791929e+00
  7.7624817e+00  3.6623871e+00  7.8092593e-01  7.3296189e+00
  5.7747946e+00  2.3126864e-01  9.6106710e+00  9.5668274e-01
  8.3702803e+00  2.9747019e+00  4.3602333e+00  7.8842506e+00
  6.2896223e+00  8.5025053e+00  6.1606364e+00  1.0106425e+00
  8.6921835e+00  7.3920555e+00  7.1117039e+00  1.2270844e+00
  4.3614955e+00  2.5761406e+00  8.1967859e+00  6.2408938e+00
  9.4475508e+00  9.6550465e+00  8.4041348e+00  1.0130975e+01
  5.6460967e+00  4.0990400e-01 -8.8367611e-03  3.5952890e+00
  9.2217665e+00  8.4936142e+00  9.9719982e+00  6.2442482e-01
  1.5329283e+00  8.9031639e+00  2.9812365e+00  4.6210904e+00
  9.6318512e+00  3.3671608e+00  6.6170344e+00  4.0481849e+00
  7.8299704e+00  4.1430693e+00  2.1972313e+00  2.7806988e+00
  2.4690957e+00  6.2265091e+00  8.0078650e+00  8.5526848e+00
  7.8363638e+00  6.6496010e+00  1.7129709e+00  3.3299735e+00
  4.1951280e+00  7.1778541e+00  2.9357438e+00  3.2394106e+00
  4.3329515e+00  7.8274932e+00  5.8496819e+00  4.6137288e-01
  7.5736916e-01  3.1835232e+00  7.2212210e+00  7.3863716e+00
  7.1284332e+00  7.4827847e+00  2.6439285e+00  5.7008662e+00
  5.0590334e+00  8.0088997e+00  9.6267881e+00  2.4991486e+00
  8.2448721e+00  6.5369444e+00  6.2472978e+00  7.7897358e+00
  9.4234190e+00  9.6731400e-01  1.6939137e+00  7.3830414e+00
  1.0644419e+00  5.3757925e+00  4.4202948e+00  1.9308488e+00
  8.0857773e+00 -3.4219015e-01  8.6868877e+00  4.4470925e+00
  4.9024949e+00  5.6183209e+00  6.1064873e+00  3.4613369e+00
  1.2080189e+00  3.4117126e+00  4.1149378e-02  6.4430194e+00
  1.9570980e+00  9.7712898e+00  9.6677971e+00  6.8640056e+00
  6.0994020e+00  1.0187636e+01  8.8685141e+00  8.2290039e+00
  3.1689963e+00  3.6186795e+00  2.5079422e+00  3.3114364e+00
  9.9208021e+00  9.1239557e+00  1.3143113e+00  6.8661511e-01
  8.9672060e+00  5.2539353e+00  1.7683913e+00  6.8201876e+00
  6.3931870e+00  1.2865118e+00  3.4232042e+00  1.2203555e+00
  2.0503523e+00  8.4730494e-01  1.9659302e+00  6.6408830e+00
  3.0317956e-01  8.8737220e-01  2.3188889e-01  3.0580165e+00
  3.1296854e+00  2.1522398e+00  7.4903617e+00  9.1460247e+00
  5.0951142e+00  8.5311861e+00  3.9055674e+00 -2.2732937e-01
  4.0364518e+00  2.6478162e+00  5.1365023e+00  7.4299526e+00
  4.6355362e+00  8.9175758e+00  8.7688503e+00  5.5298843e+00
  4.4609528e+00  1.2880317e+00  4.0175514e+00  6.7396054e+00
  3.7963538e+00  1.2283164e+00  9.0873432e+00  7.1056828e+00
  5.5581961e+00  8.0034761e+00  8.3522511e+00  4.6975536e+00
  8.0432367e+00  8.6975918e+00  9.9233341e-01  6.2826734e+00
  5.9927521e+00  5.0778217e+00  8.8290005e+00  2.5841575e+00
  9.9096069e+00  8.9677849e+00  4.5229397e+00  8.1403732e+00
  6.6729965e+00  6.2468095e+00  3.7025385e+00  2.1858666e+00
  7.0194511e+00  8.6221571e+00 -2.3119065e-01  6.4518013e+00
  4.6490231e+00  9.1139078e+00  2.6489720e+00  4.4286499e+00
  4.7887325e+00  1.6389720e+00  8.0717764e+00  8.0382366e+00
  1.3237777e+00  2.9469595e+00  6.6192156e-01  9.5147648e+00
  6.1208944e+00  9.9296083e+00  5.8978138e+00  6.4843559e+00
  4.2908926e+00  3.5816567e+00  3.7703094e+00  7.1296864e+00
  3.3137634e+00  9.1458826e+00  1.0168221e+01  4.6002930e-01
  1.9140388e+00  7.9359615e-01  5.7083476e-01  2.0637333e+00
  4.4295535e+00  8.4761515e+00  2.1790206e+00  4.8846216e+00
  1.1725900e+01  6.4370713e+00  7.5755668e+00  5.3716149e+00
  9.8037472e+00  6.0171165e+00 -1.0524909e+00  2.8919909e+00
  3.9641018e+00  5.4104433e+00  7.7196703e+00  4.5299621e+00
  7.0386343e+00  8.1372356e+00  2.8667083e+00  9.8477182e+00
  6.2275105e+00  8.9304132e+00  5.8799353e+00  7.2770109e+00
  7.7765956e+00  9.1447611e+00  1.3774345e+00  4.7782230e+00
  4.5103283e+00  9.6529636e+00  1.0817671e+00  9.6128826e+00
  8.8991022e+00  7.8995752e-01  5.2336302e+00  7.1597772e+00
  6.9343681e+00  9.0601816e+00  8.0068483e+00  3.5910506e+00
  5.0274186e+00  7.0993495e+00  3.2836149e+00  1.3527107e+00
  3.3847008e+00  3.9555850e+00  8.4203587e+00  5.7436528e+00
  4.4528852e+00  8.3769274e+00  8.4812574e+00  9.8149929e+00
  8.4825897e+00  4.3883948e+00  1.1693200e+00  8.5065060e+00
  1.7731776e+00  3.4153044e+00  9.9896975e+00  8.2860680e+00
  6.3954239e+00  7.1741295e+00  6.7204058e-01  5.1233587e+00
  6.6728873e+00  3.4099829e+00  3.0468628e+00  2.3115411e+00
  4.1901307e+00  4.4959798e+00  5.7105017e+00  7.4096904e+00
  2.9691739e+00  2.5698562e+00  3.1576818e-01 -5.9249759e-02
  2.4639745e+00  7.8125792e+00  7.1327791e+00  7.7584710e+00
  6.2746820e+00  8.4847488e+00  6.9726362e+00  1.0046572e+01
  8.4835949e+00  6.7527709e+00  5.9673371e+00  4.3935239e-01
  1.1263874e+01  1.3709677e+00  7.3908615e+00  7.9875464e+00
  3.9839177e+00  2.2393181e+00  8.4422034e-01  6.9838266e+00
  4.5242758e+00  7.4394574e+00  9.1695070e+00  1.5673304e+00
  9.3064995e+00  3.2363920e+00  3.0193369e+00  9.7506819e+00
  9.7927599e+00  2.1529837e+00  5.8980074e+00  1.1057787e+00
  6.5593362e+00  8.3812113e+00  5.5978317e+00  3.2321651e+00
  3.8758488e+00  4.0856094e+00  7.0214200e+00  8.6556015e+00
  9.3241930e+00  2.9402196e-02  1.4503651e+00  4.8388457e-01
  2.0156007e+00  4.3253636e+00  5.0806727e+00  6.2870202e+00
  8.3587704e+00  9.1714058e+00  1.1041972e+01  6.7473054e+00
  8.3084793e+00  7.5109200e+00  3.7499793e+00  7.7249527e+00
  3.9467406e-01  6.4965940e-01  2.9256980e+00  4.6755557e+00
  6.6328969e+00  1.9519567e-01  7.0285888e+00  8.4598532e+00
  1.9452683e+00  4.4380093e+00  4.9697065e+00  4.4265666e+00
  7.9045653e+00  9.4661942e+00  8.0525227e+00  8.8097310e-01
  6.8595657e+00  1.7693057e+00  2.4858186e+00  1.4309098e+00
  1.8431244e+00  4.7576222e+00  6.7128339e+00  4.3999271e+00
  9.4328070e+00  2.1000056e+00  7.5769453e+00  8.7397270e+00
  5.8346295e+00  4.8793106e+00  4.7135677e+00  4.4952607e+00
  5.7733207e+00  3.9166284e+00  4.1439590e+00  6.8955579e+00
  6.8403614e-01  2.0110247e+00  9.0629015e+00  8.0548077e+00
  2.7293277e+00  4.2169027e+00  9.2188635e+00  9.0293961e+00
  6.1455669e+00  2.1301951e+00  5.6030965e-01  4.4237075e+00
  7.3219604e+00  8.5027437e+00  8.5191278e+00  4.1766196e-01
  2.4718022e+00  3.2887347e+00  1.7133644e+00  7.1094151e+00
  7.3223224e+00  4.3398414e+00  9.8931112e+00  1.3441966e+00
  9.0235968e+00  1.2211851e+00  2.6455071e+00  7.5472803e+00
  1.1029721e+00  4.7136912e+00  2.8467078e+00  7.3037176e+00
  9.4062443e+00  7.6552663e+00  8.4975100e+00  6.3747257e-02
  2.3440433e+00  5.1499615e+00  3.5230517e+00  1.9831378e+00
  3.5230274e+00  9.6046629e+00  3.3157225e+00  4.3171234e+00
  1.3997978e+00  7.7749600e+00  3.3477159e+00  1.9058161e+00
  2.5061352e+00  9.0734358e+00  2.0535131e+00  8.7670698e+00
  7.5356512e+00  1.0142322e+01 -3.0124056e-01  2.2794275e+00
  7.4697652e+00  1.9831645e+00  2.1713405e+00  2.2061615e+00
  1.5869830e+00  4.8229289e+00  1.4017915e+00  3.4431477e+00
  4.9791493e+00  7.0362866e-01  4.3548961e+00  6.2790900e-01
  5.3626833e+00  9.2004957e+00  7.4519486e+00  9.5844626e-01
  5.9674959e+00  8.2974005e+00  2.5478227e+00  7.6181903e+00
  8.8209562e+00  7.9951472e+00  2.8410530e-01  4.9098748e-01
  2.0705619e+00  2.6533947e+00  5.7332134e+00  2.9440532e+00
  7.7568593e+00  3.6492581e+00  2.4384577e+00  2.8873272e+00
  9.4621639e+00  9.1508560e+00  2.0058784e+00  6.5649538e+00
  2.7415826e+00  8.4922266e+00  4.9448359e-01  7.5271797e+00
 -1.5808821e-01  9.7426481e+00  6.3803730e+00  8.4051380e+00
  5.7340975e+00  4.6641579e+00  6.2364306e+00  5.1810746e+00
  5.3571901e+00  8.9868603e+00  1.8257052e+00  8.8656920e-01
  4.3585706e+00  3.5430098e+00  7.3958582e-01  8.3649902e+00
  7.7803888e+00  5.7408032e+00  9.7680950e+00  6.9590604e-01
  7.2661672e+00  2.1069894e+00  3.6764739e+00  3.4602623e+00
  7.6012840e+00  2.0565724e-01  8.5318031e+00  7.2798491e+00
  1.4081079e+00  8.3596611e+00  7.2540979e+00  9.5279789e+00
  7.3339424e+00  5.2405219e+00  8.5954704e+00  2.9429979e+00
  5.2126360e+00  3.8612814e+00  9.0659313e+00  1.9871573e+00
  3.8985163e-01  3.2772667e+00  4.5025587e+00  1.2284964e-02
  6.7665458e+00  9.5286417e+00  7.1374736e+00  6.0702968e+00
  9.5389080e+00  9.5242043e+00  8.4405804e+00  6.5415311e+00
  4.7715144e+00  1.9164640e-01  6.2476826e+00  9.6162710e+00
  8.1386156e+00  3.4102411e+00  1.4985125e+00  7.9778295e+00
  7.4183102e+00  9.5704803e+00  7.8087254e+00  6.9231396e+00
  1.3841813e+00  4.7959905e+00  2.9273345e+00  5.1850367e+00
  3.0295143e+00  8.6389799e+00  9.0257702e+00  3.6582994e+00
  5.9157324e+00  4.1758962e+00  9.3391609e+00  1.7730776e+00
  7.6125379e+00  1.0046360e+01  2.1057978e+00  1.7478250e+00
  7.1132212e+00  7.9645081e+00  5.2505498e+00  6.4574647e+00
  3.2315047e+00  4.3446302e+00  1.2056727e+00  2.9674428e+00
  2.7837610e+00  4.7524495e+00  9.8256521e+00  1.5932522e+00
  4.8396316e+00  5.9174523e+00  9.4890871e+00  1.2948707e+00
  9.0378551e+00  1.3124785e+00  5.4293876e+00  4.4993534e+00
  2.0538125e+00  3.5617321e+00  5.4850354e+00  8.2304430e+00
  9.7063475e+00  4.5277615e+00  8.3297014e+00  6.6988935e+00
  3.0370085e+00  2.5210819e+00  4.3796597e+00  5.5016708e+00
  1.9651670e+00  2.4778838e+00  5.8789811e+00  3.1068649e+00
  8.4105711e+00  2.6629505e+00 -3.3057988e-01  9.0704960e-01
  9.3188791e+00  1.0806879e+00 -8.1481934e-03  9.1659679e+00
  2.1467557e+00  7.2326632e+00  8.2058630e+00  5.8045855e+00
  8.4636183e+00  6.8212204e+00  5.0721443e-01  5.9133248e+00
  7.4281392e+00  7.8691220e+00 -6.8311334e-02  7.2124262e+00
  8.6880360e+00  6.6583366e+00  5.0127316e+00  4.6381230e+00
  5.9561200e+00  6.8466635e+00  9.5994196e+00  6.0355244e+00
  8.7480297e+00  7.6578970e+00  1.2954912e+00  4.2609344e+00
  2.7702582e+00  3.0329621e+00  3.6327574e+00  3.4195125e+00
  5.5413127e+00  5.8191652e+00  2.7070973e+00  1.6614884e+00
  3.8416210e-01  6.2802663e+00  2.0622129e+00 -6.8790317e-01
  9.2128534e+00  4.7458625e+00  9.4553316e-01  8.3560362e+00
  9.9401188e+00  3.5302291e+00  4.7089720e-01  1.3786711e+00
  5.3229847e+00  9.2913418e+00  1.6849303e+00  8.2535496e+00
 -2.5821114e-01  4.1421123e+00  6.4032269e+00  6.6551906e-01
  5.3515515e+00  9.1520185e+00  1.5094744e+00  7.4785428e+00
  6.0626564e+00  1.8558639e+00  4.4214793e-02  7.5862879e-01
  4.2469244e+00  8.8614159e+00  6.5688152e+00  7.3740344e+00
  4.8993049e+00  5.7953315e+00  9.2521513e-01  8.7253218e+00
  7.7945423e+00  9.9947720e+00  3.9001837e+00  2.8082497e+00
  2.2034130e+00  9.5151176e+00  7.4653130e+00  5.4289103e-01
  5.5686536e+00  4.1999950e+00  1.3804873e+00  1.5220041e+00
  9.6445208e+00  3.1844208e+00  8.2594252e+00  5.8706766e-01
  3.8130655e+00  9.4813788e-01  1.4116070e-01  5.6962514e+00
  2.8564551e+00  7.0057955e+00  6.3632298e+00  1.4823904e+00
  9.5153828e+00  7.8791199e+00  2.7109826e+00  5.3147154e+00
  7.3835139e+00  3.6906118e+00  4.5995116e+00  4.1400166e+00
  6.1948090e+00  8.8411551e+00  1.6546024e+00  8.7375402e+00
  5.7898629e-01  6.4826312e+00  4.3135948e+00  7.4583459e+00
  6.8681207e+00  5.0527821e+00  6.3631139e+00  3.5951567e+00]
Epoch 1/1000
2023-09-10 10:00:33.698 
Epoch 1/1000 
	 loss: 463.0371, MinusLogProbMetric: 463.0371, val_loss: 435.9744, val_MinusLogProbMetric: 435.9744

Epoch 1: val_loss improved from inf to 435.97443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 86s - loss: 463.0371 - MinusLogProbMetric: 463.0371 - val_loss: 435.9744 - val_MinusLogProbMetric: 435.9744 - lr: 3.3333e-04 - 86s/epoch - 438ms/step
Epoch 2/1000
2023-09-10 10:00:52.467 
Epoch 2/1000 
	 loss: 433.6762, MinusLogProbMetric: 433.6762, val_loss: 432.0340, val_MinusLogProbMetric: 432.0340

Epoch 2: val_loss improved from 435.97443 to 432.03400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 433.6762 - MinusLogProbMetric: 433.6762 - val_loss: 432.0340 - val_MinusLogProbMetric: 432.0340 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 3/1000
2023-09-10 10:01:10.834 
Epoch 3/1000 
	 loss: 431.1775, MinusLogProbMetric: 431.1775, val_loss: 431.7144, val_MinusLogProbMetric: 431.7144

Epoch 3: val_loss improved from 432.03400 to 431.71436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 431.1775 - MinusLogProbMetric: 431.1775 - val_loss: 431.7144 - val_MinusLogProbMetric: 431.7144 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 4/1000
2023-09-10 10:01:29.118 
Epoch 4/1000 
	 loss: 428.9440, MinusLogProbMetric: 428.9440, val_loss: 430.2242, val_MinusLogProbMetric: 430.2242

Epoch 4: val_loss improved from 431.71436 to 430.22415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 428.9440 - MinusLogProbMetric: 428.9440 - val_loss: 430.2242 - val_MinusLogProbMetric: 430.2242 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 5/1000
2023-09-10 10:01:46.813 
Epoch 5/1000 
	 loss: 427.4726, MinusLogProbMetric: 427.4726, val_loss: 430.7905, val_MinusLogProbMetric: 430.7905

Epoch 5: val_loss did not improve from 430.22415
196/196 - 17s - loss: 427.4726 - MinusLogProbMetric: 427.4726 - val_loss: 430.7905 - val_MinusLogProbMetric: 430.7905 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 6/1000
2023-09-10 10:02:04.192 
Epoch 6/1000 
	 loss: 425.7966, MinusLogProbMetric: 425.7966, val_loss: 424.6325, val_MinusLogProbMetric: 424.6325

Epoch 6: val_loss improved from 430.22415 to 424.63254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 425.7966 - MinusLogProbMetric: 425.7966 - val_loss: 424.6325 - val_MinusLogProbMetric: 424.6325 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 7/1000
2023-09-10 10:02:21.986 
Epoch 7/1000 
	 loss: 423.7811, MinusLogProbMetric: 423.7811, val_loss: 427.5498, val_MinusLogProbMetric: 427.5498

Epoch 7: val_loss did not improve from 424.63254
196/196 - 17s - loss: 423.7811 - MinusLogProbMetric: 423.7811 - val_loss: 427.5498 - val_MinusLogProbMetric: 427.5498 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 8/1000
2023-09-10 10:02:39.180 
Epoch 8/1000 
	 loss: 424.4514, MinusLogProbMetric: 424.4514, val_loss: 424.2675, val_MinusLogProbMetric: 424.2675

Epoch 8: val_loss improved from 424.63254 to 424.26749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 424.4514 - MinusLogProbMetric: 424.4514 - val_loss: 424.2675 - val_MinusLogProbMetric: 424.2675 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 9/1000
2023-09-10 10:02:57.610 
Epoch 9/1000 
	 loss: 421.3696, MinusLogProbMetric: 421.3696, val_loss: 423.7984, val_MinusLogProbMetric: 423.7984

Epoch 9: val_loss improved from 424.26749 to 423.79840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 421.3696 - MinusLogProbMetric: 421.3696 - val_loss: 423.7984 - val_MinusLogProbMetric: 423.7984 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 10/1000
2023-09-10 10:03:16.788 
Epoch 10/1000 
	 loss: 421.7170, MinusLogProbMetric: 421.7170, val_loss: 424.6590, val_MinusLogProbMetric: 424.6590

Epoch 10: val_loss did not improve from 423.79840
196/196 - 19s - loss: 421.7170 - MinusLogProbMetric: 421.7170 - val_loss: 424.6590 - val_MinusLogProbMetric: 424.6590 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 11/1000
2023-09-10 10:03:35.349 
Epoch 11/1000 
	 loss: 421.2097, MinusLogProbMetric: 421.2097, val_loss: 419.2852, val_MinusLogProbMetric: 419.2852

Epoch 11: val_loss improved from 423.79840 to 419.28516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 421.2097 - MinusLogProbMetric: 421.2097 - val_loss: 419.2852 - val_MinusLogProbMetric: 419.2852 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 12/1000
2023-09-10 10:03:52.843 
Epoch 12/1000 
	 loss: 419.8667, MinusLogProbMetric: 419.8667, val_loss: 417.8543, val_MinusLogProbMetric: 417.8543

Epoch 12: val_loss improved from 419.28516 to 417.85428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 419.8667 - MinusLogProbMetric: 419.8667 - val_loss: 417.8543 - val_MinusLogProbMetric: 417.8543 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 13/1000
2023-09-10 10:04:10.850 
Epoch 13/1000 
	 loss: 419.2930, MinusLogProbMetric: 419.2930, val_loss: 418.9698, val_MinusLogProbMetric: 418.9698

Epoch 13: val_loss did not improve from 417.85428
196/196 - 17s - loss: 419.2930 - MinusLogProbMetric: 419.2930 - val_loss: 418.9698 - val_MinusLogProbMetric: 418.9698 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 14/1000
2023-09-10 10:04:31.956 
Epoch 14/1000 
	 loss: 418.8233, MinusLogProbMetric: 418.8233, val_loss: 417.2844, val_MinusLogProbMetric: 417.2844

Epoch 14: val_loss improved from 417.85428 to 417.28442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 22s - loss: 418.8233 - MinusLogProbMetric: 418.8233 - val_loss: 417.2844 - val_MinusLogProbMetric: 417.2844 - lr: 3.3333e-04 - 22s/epoch - 111ms/step
Epoch 15/1000
2023-09-10 10:04:51.061 
Epoch 15/1000 
	 loss: 418.4237, MinusLogProbMetric: 418.4237, val_loss: 419.3986, val_MinusLogProbMetric: 419.3986

Epoch 15: val_loss did not improve from 417.28442
196/196 - 18s - loss: 418.4237 - MinusLogProbMetric: 418.4237 - val_loss: 419.3986 - val_MinusLogProbMetric: 419.3986 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 16/1000
2023-09-10 10:05:08.213 
Epoch 16/1000 
	 loss: 416.8478, MinusLogProbMetric: 416.8478, val_loss: 421.7155, val_MinusLogProbMetric: 421.7155

Epoch 16: val_loss did not improve from 417.28442
196/196 - 17s - loss: 416.8478 - MinusLogProbMetric: 416.8478 - val_loss: 421.7155 - val_MinusLogProbMetric: 421.7155 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 17/1000
2023-09-10 10:05:25.630 
Epoch 17/1000 
	 loss: 415.9923, MinusLogProbMetric: 415.9923, val_loss: 417.0690, val_MinusLogProbMetric: 417.0690

Epoch 17: val_loss improved from 417.28442 to 417.06897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 415.9923 - MinusLogProbMetric: 415.9923 - val_loss: 417.0690 - val_MinusLogProbMetric: 417.0690 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 18/1000
2023-09-10 10:05:44.243 
Epoch 18/1000 
	 loss: 416.6191, MinusLogProbMetric: 416.6191, val_loss: 417.2695, val_MinusLogProbMetric: 417.2695

Epoch 18: val_loss did not improve from 417.06897
196/196 - 18s - loss: 416.6191 - MinusLogProbMetric: 416.6191 - val_loss: 417.2695 - val_MinusLogProbMetric: 417.2695 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 19/1000
2023-09-10 10:06:00.999 
Epoch 19/1000 
	 loss: 416.2401, MinusLogProbMetric: 416.2401, val_loss: 415.9852, val_MinusLogProbMetric: 415.9852

Epoch 19: val_loss improved from 417.06897 to 415.98523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 17s - loss: 416.2401 - MinusLogProbMetric: 416.2401 - val_loss: 415.9852 - val_MinusLogProbMetric: 415.9852 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 20/1000
2023-09-10 10:06:19.188 
Epoch 20/1000 
	 loss: 414.7670, MinusLogProbMetric: 414.7670, val_loss: 415.2128, val_MinusLogProbMetric: 415.2128

Epoch 20: val_loss improved from 415.98523 to 415.21283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 414.7670 - MinusLogProbMetric: 414.7670 - val_loss: 415.2128 - val_MinusLogProbMetric: 415.2128 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 21/1000
2023-09-10 10:06:38.528 
Epoch 21/1000 
	 loss: 415.2487, MinusLogProbMetric: 415.2487, val_loss: 416.5045, val_MinusLogProbMetric: 416.5045

Epoch 21: val_loss did not improve from 415.21283
196/196 - 19s - loss: 415.2487 - MinusLogProbMetric: 415.2487 - val_loss: 416.5045 - val_MinusLogProbMetric: 416.5045 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 22/1000
2023-09-10 10:06:55.429 
Epoch 22/1000 
	 loss: 413.3200, MinusLogProbMetric: 413.3200, val_loss: 414.1803, val_MinusLogProbMetric: 414.1803

Epoch 22: val_loss improved from 415.21283 to 414.18033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 413.3200 - MinusLogProbMetric: 413.3200 - val_loss: 414.1803 - val_MinusLogProbMetric: 414.1803 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 23/1000
2023-09-10 10:07:13.496 
Epoch 23/1000 
	 loss: 414.6809, MinusLogProbMetric: 414.6809, val_loss: 413.1889, val_MinusLogProbMetric: 413.1889

Epoch 23: val_loss improved from 414.18033 to 413.18887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 414.6809 - MinusLogProbMetric: 414.6809 - val_loss: 413.1889 - val_MinusLogProbMetric: 413.1889 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 24/1000
2023-09-10 10:07:32.061 
Epoch 24/1000 
	 loss: 412.9016, MinusLogProbMetric: 412.9016, val_loss: 414.2744, val_MinusLogProbMetric: 414.2744

Epoch 24: val_loss did not improve from 413.18887
196/196 - 18s - loss: 412.9016 - MinusLogProbMetric: 412.9016 - val_loss: 414.2744 - val_MinusLogProbMetric: 414.2744 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 25/1000
2023-09-10 10:07:49.699 
Epoch 25/1000 
	 loss: 412.8878, MinusLogProbMetric: 412.8878, val_loss: 411.5490, val_MinusLogProbMetric: 411.5490

Epoch 25: val_loss improved from 413.18887 to 411.54901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 412.8878 - MinusLogProbMetric: 412.8878 - val_loss: 411.5490 - val_MinusLogProbMetric: 411.5490 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 26/1000
2023-09-10 10:08:07.820 
Epoch 26/1000 
	 loss: 412.3625, MinusLogProbMetric: 412.3625, val_loss: 413.2640, val_MinusLogProbMetric: 413.2640

Epoch 26: val_loss did not improve from 411.54901
196/196 - 17s - loss: 412.3625 - MinusLogProbMetric: 412.3625 - val_loss: 413.2640 - val_MinusLogProbMetric: 413.2640 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 27/1000
2023-09-10 10:08:26.060 
Epoch 27/1000 
	 loss: 414.3449, MinusLogProbMetric: 414.3449, val_loss: 412.2121, val_MinusLogProbMetric: 412.2121

Epoch 27: val_loss did not improve from 411.54901
196/196 - 18s - loss: 414.3449 - MinusLogProbMetric: 414.3449 - val_loss: 412.2121 - val_MinusLogProbMetric: 412.2121 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 28/1000
2023-09-10 10:08:43.224 
Epoch 28/1000 
	 loss: 411.2823, MinusLogProbMetric: 411.2823, val_loss: 414.4773, val_MinusLogProbMetric: 414.4773

Epoch 28: val_loss did not improve from 411.54901
196/196 - 17s - loss: 411.2823 - MinusLogProbMetric: 411.2823 - val_loss: 414.4773 - val_MinusLogProbMetric: 414.4773 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 29/1000
2023-09-10 10:09:00.248 
Epoch 29/1000 
	 loss: 412.6656, MinusLogProbMetric: 412.6656, val_loss: 422.1225, val_MinusLogProbMetric: 422.1225

Epoch 29: val_loss did not improve from 411.54901
196/196 - 17s - loss: 412.6656 - MinusLogProbMetric: 412.6656 - val_loss: 422.1225 - val_MinusLogProbMetric: 422.1225 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 30/1000
2023-09-10 10:09:17.629 
Epoch 30/1000 
	 loss: 410.6737, MinusLogProbMetric: 410.6737, val_loss: 413.5676, val_MinusLogProbMetric: 413.5676

Epoch 30: val_loss did not improve from 411.54901
196/196 - 17s - loss: 410.6737 - MinusLogProbMetric: 410.6737 - val_loss: 413.5676 - val_MinusLogProbMetric: 413.5676 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 31/1000
2023-09-10 10:09:35.038 
Epoch 31/1000 
	 loss: 410.0789, MinusLogProbMetric: 410.0789, val_loss: 413.7519, val_MinusLogProbMetric: 413.7519

Epoch 31: val_loss did not improve from 411.54901
196/196 - 17s - loss: 410.0789 - MinusLogProbMetric: 410.0789 - val_loss: 413.7519 - val_MinusLogProbMetric: 413.7519 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 32/1000
2023-09-10 10:09:52.220 
Epoch 32/1000 
	 loss: 411.3300, MinusLogProbMetric: 411.3300, val_loss: 411.9595, val_MinusLogProbMetric: 411.9596

Epoch 32: val_loss did not improve from 411.54901
196/196 - 17s - loss: 411.3300 - MinusLogProbMetric: 411.3300 - val_loss: 411.9595 - val_MinusLogProbMetric: 411.9596 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 33/1000
2023-09-10 10:10:10.038 
Epoch 33/1000 
	 loss: 410.1212, MinusLogProbMetric: 410.1212, val_loss: 416.0058, val_MinusLogProbMetric: 416.0058

Epoch 33: val_loss did not improve from 411.54901
196/196 - 18s - loss: 410.1212 - MinusLogProbMetric: 410.1212 - val_loss: 416.0058 - val_MinusLogProbMetric: 416.0058 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 34/1000
2023-09-10 10:10:27.604 
Epoch 34/1000 
	 loss: 410.2242, MinusLogProbMetric: 410.2242, val_loss: 416.1229, val_MinusLogProbMetric: 416.1229

Epoch 34: val_loss did not improve from 411.54901
196/196 - 18s - loss: 410.2242 - MinusLogProbMetric: 410.2242 - val_loss: 416.1229 - val_MinusLogProbMetric: 416.1229 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 35/1000
2023-09-10 10:10:45.732 
Epoch 35/1000 
	 loss: 409.6306, MinusLogProbMetric: 409.6306, val_loss: 415.7554, val_MinusLogProbMetric: 415.7554

Epoch 35: val_loss did not improve from 411.54901
196/196 - 18s - loss: 409.6306 - MinusLogProbMetric: 409.6306 - val_loss: 415.7554 - val_MinusLogProbMetric: 415.7554 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 36/1000
2023-09-10 10:11:03.938 
Epoch 36/1000 
	 loss: 410.0522, MinusLogProbMetric: 410.0522, val_loss: 411.3716, val_MinusLogProbMetric: 411.3716

Epoch 36: val_loss improved from 411.54901 to 411.37161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 410.0522 - MinusLogProbMetric: 410.0522 - val_loss: 411.3716 - val_MinusLogProbMetric: 411.3716 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 37/1000
2023-09-10 10:11:22.524 
Epoch 37/1000 
	 loss: 409.6409, MinusLogProbMetric: 409.6409, val_loss: 414.3444, val_MinusLogProbMetric: 414.3444

Epoch 37: val_loss did not improve from 411.37161
196/196 - 18s - loss: 409.6409 - MinusLogProbMetric: 409.6409 - val_loss: 414.3444 - val_MinusLogProbMetric: 414.3444 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 38/1000
2023-09-10 10:11:40.445 
Epoch 38/1000 
	 loss: 408.7027, MinusLogProbMetric: 408.7027, val_loss: 417.1063, val_MinusLogProbMetric: 417.1063

Epoch 38: val_loss did not improve from 411.37161
196/196 - 18s - loss: 408.7027 - MinusLogProbMetric: 408.7027 - val_loss: 417.1063 - val_MinusLogProbMetric: 417.1063 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 39/1000
2023-09-10 10:11:58.607 
Epoch 39/1000 
	 loss: 408.6924, MinusLogProbMetric: 408.6924, val_loss: 410.3663, val_MinusLogProbMetric: 410.3663

Epoch 39: val_loss improved from 411.37161 to 410.36630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 408.6924 - MinusLogProbMetric: 408.6924 - val_loss: 410.3663 - val_MinusLogProbMetric: 410.3663 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 40/1000
2023-09-10 10:12:17.280 
Epoch 40/1000 
	 loss: 409.5956, MinusLogProbMetric: 409.5956, val_loss: 409.1894, val_MinusLogProbMetric: 409.1894

Epoch 40: val_loss improved from 410.36630 to 409.18936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 409.5956 - MinusLogProbMetric: 409.5956 - val_loss: 409.1894 - val_MinusLogProbMetric: 409.1894 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 41/1000
2023-09-10 10:12:35.641 
Epoch 41/1000 
	 loss: 408.4866, MinusLogProbMetric: 408.4866, val_loss: 409.5486, val_MinusLogProbMetric: 409.5486

Epoch 41: val_loss did not improve from 409.18936
196/196 - 18s - loss: 408.4866 - MinusLogProbMetric: 408.4866 - val_loss: 409.5486 - val_MinusLogProbMetric: 409.5486 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 42/1000
2023-09-10 10:12:53.447 
Epoch 42/1000 
	 loss: 408.2581, MinusLogProbMetric: 408.2581, val_loss: 410.2728, val_MinusLogProbMetric: 410.2728

Epoch 42: val_loss did not improve from 409.18936
196/196 - 18s - loss: 408.2581 - MinusLogProbMetric: 408.2581 - val_loss: 410.2728 - val_MinusLogProbMetric: 410.2728 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 43/1000
2023-09-10 10:13:10.924 
Epoch 43/1000 
	 loss: 409.9831, MinusLogProbMetric: 409.9831, val_loss: 413.9519, val_MinusLogProbMetric: 413.9519

Epoch 43: val_loss did not improve from 409.18936
196/196 - 17s - loss: 409.9831 - MinusLogProbMetric: 409.9831 - val_loss: 413.9519 - val_MinusLogProbMetric: 413.9519 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 44/1000
2023-09-10 10:13:28.625 
Epoch 44/1000 
	 loss: 407.6559, MinusLogProbMetric: 407.6559, val_loss: 409.2507, val_MinusLogProbMetric: 409.2507

Epoch 44: val_loss did not improve from 409.18936
196/196 - 18s - loss: 407.6559 - MinusLogProbMetric: 407.6559 - val_loss: 409.2507 - val_MinusLogProbMetric: 409.2507 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 45/1000
2023-09-10 10:13:46.420 
Epoch 45/1000 
	 loss: 407.5948, MinusLogProbMetric: 407.5948, val_loss: 407.6743, val_MinusLogProbMetric: 407.6743

Epoch 45: val_loss improved from 409.18936 to 407.67435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 407.5948 - MinusLogProbMetric: 407.5948 - val_loss: 407.6743 - val_MinusLogProbMetric: 407.6743 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 46/1000
2023-09-10 10:14:05.371 
Epoch 46/1000 
	 loss: 407.7025, MinusLogProbMetric: 407.7025, val_loss: 411.3365, val_MinusLogProbMetric: 411.3365

Epoch 46: val_loss did not improve from 407.67435
196/196 - 18s - loss: 407.7025 - MinusLogProbMetric: 407.7025 - val_loss: 411.3365 - val_MinusLogProbMetric: 411.3365 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 47/1000
2023-09-10 10:14:22.790 
Epoch 47/1000 
	 loss: 407.1909, MinusLogProbMetric: 407.1909, val_loss: 416.5596, val_MinusLogProbMetric: 416.5596

Epoch 47: val_loss did not improve from 407.67435
196/196 - 17s - loss: 407.1909 - MinusLogProbMetric: 407.1909 - val_loss: 416.5596 - val_MinusLogProbMetric: 416.5596 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 48/1000
2023-09-10 10:14:40.010 
Epoch 48/1000 
	 loss: 406.8865, MinusLogProbMetric: 406.8865, val_loss: 410.7845, val_MinusLogProbMetric: 410.7845

Epoch 48: val_loss did not improve from 407.67435
196/196 - 17s - loss: 406.8865 - MinusLogProbMetric: 406.8865 - val_loss: 410.7845 - val_MinusLogProbMetric: 410.7845 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 49/1000
2023-09-10 10:14:57.013 
Epoch 49/1000 
	 loss: 408.5438, MinusLogProbMetric: 408.5438, val_loss: 408.2262, val_MinusLogProbMetric: 408.2262

Epoch 49: val_loss did not improve from 407.67435
196/196 - 17s - loss: 408.5438 - MinusLogProbMetric: 408.5438 - val_loss: 408.2262 - val_MinusLogProbMetric: 408.2262 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 50/1000
2023-09-10 10:15:14.150 
Epoch 50/1000 
	 loss: 406.3683, MinusLogProbMetric: 406.3683, val_loss: 407.2755, val_MinusLogProbMetric: 407.2755

Epoch 50: val_loss improved from 407.67435 to 407.27554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 406.3683 - MinusLogProbMetric: 406.3683 - val_loss: 407.2755 - val_MinusLogProbMetric: 407.2755 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 51/1000
2023-09-10 10:15:32.138 
Epoch 51/1000 
	 loss: 406.6475, MinusLogProbMetric: 406.6475, val_loss: 406.8747, val_MinusLogProbMetric: 406.8747

Epoch 51: val_loss improved from 407.27554 to 406.87473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 406.6475 - MinusLogProbMetric: 406.6475 - val_loss: 406.8747 - val_MinusLogProbMetric: 406.8747 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 52/1000
2023-09-10 10:15:51.329 
Epoch 52/1000 
	 loss: 406.7230, MinusLogProbMetric: 406.7230, val_loss: 409.3182, val_MinusLogProbMetric: 409.3182

Epoch 52: val_loss did not improve from 406.87473
196/196 - 18s - loss: 406.7230 - MinusLogProbMetric: 406.7230 - val_loss: 409.3182 - val_MinusLogProbMetric: 409.3182 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 53/1000
2023-09-10 10:16:08.468 
Epoch 53/1000 
	 loss: 406.4765, MinusLogProbMetric: 406.4765, val_loss: 406.5574, val_MinusLogProbMetric: 406.5574

Epoch 53: val_loss improved from 406.87473 to 406.55743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 406.4765 - MinusLogProbMetric: 406.4765 - val_loss: 406.5574 - val_MinusLogProbMetric: 406.5574 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 54/1000
2023-09-10 10:16:26.029 
Epoch 54/1000 
	 loss: 405.7136, MinusLogProbMetric: 405.7136, val_loss: 411.3841, val_MinusLogProbMetric: 411.3841

Epoch 54: val_loss did not improve from 406.55743
196/196 - 17s - loss: 405.7136 - MinusLogProbMetric: 405.7136 - val_loss: 411.3841 - val_MinusLogProbMetric: 411.3841 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 55/1000
2023-09-10 10:16:44.587 
Epoch 55/1000 
	 loss: 406.0694, MinusLogProbMetric: 406.0694, val_loss: 407.9368, val_MinusLogProbMetric: 407.9368

Epoch 55: val_loss did not improve from 406.55743
196/196 - 19s - loss: 406.0694 - MinusLogProbMetric: 406.0694 - val_loss: 407.9368 - val_MinusLogProbMetric: 407.9368 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 56/1000
2023-09-10 10:17:02.940 
Epoch 56/1000 
	 loss: 406.3970, MinusLogProbMetric: 406.3970, val_loss: 407.2425, val_MinusLogProbMetric: 407.2425

Epoch 56: val_loss did not improve from 406.55743
196/196 - 18s - loss: 406.3970 - MinusLogProbMetric: 406.3970 - val_loss: 407.2425 - val_MinusLogProbMetric: 407.2425 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 57/1000
2023-09-10 10:17:21.168 
Epoch 57/1000 
	 loss: 406.3634, MinusLogProbMetric: 406.3634, val_loss: 407.7834, val_MinusLogProbMetric: 407.7834

Epoch 57: val_loss did not improve from 406.55743
196/196 - 18s - loss: 406.3634 - MinusLogProbMetric: 406.3634 - val_loss: 407.7834 - val_MinusLogProbMetric: 407.7834 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 58/1000
2023-09-10 10:17:40.336 
Epoch 58/1000 
	 loss: 405.6854, MinusLogProbMetric: 405.6854, val_loss: 414.4265, val_MinusLogProbMetric: 414.4265

Epoch 58: val_loss did not improve from 406.55743
196/196 - 19s - loss: 405.6854 - MinusLogProbMetric: 405.6854 - val_loss: 414.4265 - val_MinusLogProbMetric: 414.4265 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 59/1000
2023-09-10 10:17:58.932 
Epoch 59/1000 
	 loss: 405.4061, MinusLogProbMetric: 405.4061, val_loss: 407.7111, val_MinusLogProbMetric: 407.7111

Epoch 59: val_loss did not improve from 406.55743
196/196 - 19s - loss: 405.4061 - MinusLogProbMetric: 405.4061 - val_loss: 407.7111 - val_MinusLogProbMetric: 407.7111 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 60/1000
2023-09-10 10:18:17.238 
Epoch 60/1000 
	 loss: 405.8234, MinusLogProbMetric: 405.8234, val_loss: 405.6086, val_MinusLogProbMetric: 405.6086

Epoch 60: val_loss improved from 406.55743 to 405.60861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 405.8234 - MinusLogProbMetric: 405.8234 - val_loss: 405.6086 - val_MinusLogProbMetric: 405.6086 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 61/1000
2023-09-10 10:18:36.302 
Epoch 61/1000 
	 loss: 405.2286, MinusLogProbMetric: 405.2286, val_loss: 405.6250, val_MinusLogProbMetric: 405.6250

Epoch 61: val_loss did not improve from 405.60861
196/196 - 18s - loss: 405.2286 - MinusLogProbMetric: 405.2286 - val_loss: 405.6250 - val_MinusLogProbMetric: 405.6250 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 62/1000
2023-09-10 10:18:55.807 
Epoch 62/1000 
	 loss: 404.7231, MinusLogProbMetric: 404.7231, val_loss: 406.6297, val_MinusLogProbMetric: 406.6297

Epoch 62: val_loss did not improve from 405.60861
196/196 - 19s - loss: 404.7231 - MinusLogProbMetric: 404.7231 - val_loss: 406.6297 - val_MinusLogProbMetric: 406.6297 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 63/1000
2023-09-10 10:19:14.963 
Epoch 63/1000 
	 loss: 406.7477, MinusLogProbMetric: 406.7477, val_loss: 405.5068, val_MinusLogProbMetric: 405.5068

Epoch 63: val_loss improved from 405.60861 to 405.50681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 20s - loss: 406.7477 - MinusLogProbMetric: 406.7477 - val_loss: 405.5068 - val_MinusLogProbMetric: 405.5068 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 64/1000
2023-09-10 10:19:34.955 
Epoch 64/1000 
	 loss: 405.7639, MinusLogProbMetric: 405.7639, val_loss: 407.7546, val_MinusLogProbMetric: 407.7546

Epoch 64: val_loss did not improve from 405.50681
196/196 - 19s - loss: 405.7639 - MinusLogProbMetric: 405.7639 - val_loss: 407.7546 - val_MinusLogProbMetric: 407.7546 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 65/1000
2023-09-10 10:19:53.630 
Epoch 65/1000 
	 loss: 405.1288, MinusLogProbMetric: 405.1288, val_loss: 407.0027, val_MinusLogProbMetric: 407.0027

Epoch 65: val_loss did not improve from 405.50681
196/196 - 19s - loss: 405.1288 - MinusLogProbMetric: 405.1288 - val_loss: 407.0027 - val_MinusLogProbMetric: 407.0027 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 66/1000
2023-09-10 10:20:12.669 
Epoch 66/1000 
	 loss: 405.7532, MinusLogProbMetric: 405.7532, val_loss: 404.9730, val_MinusLogProbMetric: 404.9730

Epoch 66: val_loss improved from 405.50681 to 404.97302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 20s - loss: 405.7532 - MinusLogProbMetric: 405.7532 - val_loss: 404.9730 - val_MinusLogProbMetric: 404.9730 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 67/1000
2023-09-10 10:20:31.451 
Epoch 67/1000 
	 loss: 404.7086, MinusLogProbMetric: 404.7086, val_loss: 404.6280, val_MinusLogProbMetric: 404.6280

Epoch 67: val_loss improved from 404.97302 to 404.62802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 404.7086 - MinusLogProbMetric: 404.7086 - val_loss: 404.6280 - val_MinusLogProbMetric: 404.6280 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 68/1000
2023-09-10 10:20:50.226 
Epoch 68/1000 
	 loss: 404.0279, MinusLogProbMetric: 404.0279, val_loss: 407.0982, val_MinusLogProbMetric: 407.0982

Epoch 68: val_loss did not improve from 404.62802
196/196 - 18s - loss: 404.0279 - MinusLogProbMetric: 404.0279 - val_loss: 407.0982 - val_MinusLogProbMetric: 407.0982 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 69/1000
2023-09-10 10:21:07.823 
Epoch 69/1000 
	 loss: 403.9612, MinusLogProbMetric: 403.9612, val_loss: 404.0836, val_MinusLogProbMetric: 404.0836

Epoch 69: val_loss improved from 404.62802 to 404.08359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 403.9612 - MinusLogProbMetric: 403.9612 - val_loss: 404.0836 - val_MinusLogProbMetric: 404.0836 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 70/1000
2023-09-10 10:21:26.416 
Epoch 70/1000 
	 loss: 404.7836, MinusLogProbMetric: 404.7836, val_loss: 405.6489, val_MinusLogProbMetric: 405.6489

Epoch 70: val_loss did not improve from 404.08359
196/196 - 18s - loss: 404.7836 - MinusLogProbMetric: 404.7836 - val_loss: 405.6489 - val_MinusLogProbMetric: 405.6489 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 71/1000
2023-09-10 10:21:44.646 
Epoch 71/1000 
	 loss: 404.2527, MinusLogProbMetric: 404.2527, val_loss: 406.4847, val_MinusLogProbMetric: 406.4847

Epoch 71: val_loss did not improve from 404.08359
196/196 - 18s - loss: 404.2527 - MinusLogProbMetric: 404.2527 - val_loss: 406.4847 - val_MinusLogProbMetric: 406.4847 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 72/1000
2023-09-10 10:22:02.358 
Epoch 72/1000 
	 loss: 403.8353, MinusLogProbMetric: 403.8353, val_loss: 408.3229, val_MinusLogProbMetric: 408.3229

Epoch 72: val_loss did not improve from 404.08359
196/196 - 18s - loss: 403.8353 - MinusLogProbMetric: 403.8353 - val_loss: 408.3229 - val_MinusLogProbMetric: 408.3229 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 73/1000
2023-09-10 10:22:20.940 
Epoch 73/1000 
	 loss: 404.6417, MinusLogProbMetric: 404.6417, val_loss: 405.2367, val_MinusLogProbMetric: 405.2367

Epoch 73: val_loss did not improve from 404.08359
196/196 - 19s - loss: 404.6417 - MinusLogProbMetric: 404.6417 - val_loss: 405.2367 - val_MinusLogProbMetric: 405.2367 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 74/1000
2023-09-10 10:22:38.970 
Epoch 74/1000 
	 loss: 403.7249, MinusLogProbMetric: 403.7249, val_loss: 405.0683, val_MinusLogProbMetric: 405.0683

Epoch 74: val_loss did not improve from 404.08359
196/196 - 18s - loss: 403.7249 - MinusLogProbMetric: 403.7249 - val_loss: 405.0683 - val_MinusLogProbMetric: 405.0683 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 75/1000
2023-09-10 10:22:56.999 
Epoch 75/1000 
	 loss: 404.1805, MinusLogProbMetric: 404.1805, val_loss: 407.6604, val_MinusLogProbMetric: 407.6604

Epoch 75: val_loss did not improve from 404.08359
196/196 - 18s - loss: 404.1805 - MinusLogProbMetric: 404.1805 - val_loss: 407.6604 - val_MinusLogProbMetric: 407.6604 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 76/1000
2023-09-10 10:23:14.951 
Epoch 76/1000 
	 loss: 403.5371, MinusLogProbMetric: 403.5371, val_loss: 406.6405, val_MinusLogProbMetric: 406.6405

Epoch 76: val_loss did not improve from 404.08359
196/196 - 18s - loss: 403.5371 - MinusLogProbMetric: 403.5371 - val_loss: 406.6405 - val_MinusLogProbMetric: 406.6405 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 77/1000
2023-09-10 10:23:33.873 
Epoch 77/1000 
	 loss: 403.5195, MinusLogProbMetric: 403.5195, val_loss: 405.2213, val_MinusLogProbMetric: 405.2213

Epoch 77: val_loss did not improve from 404.08359
196/196 - 19s - loss: 403.5195 - MinusLogProbMetric: 403.5195 - val_loss: 405.2213 - val_MinusLogProbMetric: 405.2213 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 78/1000
2023-09-10 10:23:52.340 
Epoch 78/1000 
	 loss: 403.0710, MinusLogProbMetric: 403.0710, val_loss: 406.2004, val_MinusLogProbMetric: 406.2004

Epoch 78: val_loss did not improve from 404.08359
196/196 - 18s - loss: 403.0710 - MinusLogProbMetric: 403.0710 - val_loss: 406.2004 - val_MinusLogProbMetric: 406.2004 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 79/1000
2023-09-10 10:24:11.254 
Epoch 79/1000 
	 loss: 402.9957, MinusLogProbMetric: 402.9957, val_loss: 404.1870, val_MinusLogProbMetric: 404.1870

Epoch 79: val_loss did not improve from 404.08359
196/196 - 19s - loss: 402.9957 - MinusLogProbMetric: 402.9957 - val_loss: 404.1870 - val_MinusLogProbMetric: 404.1870 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 80/1000
2023-09-10 10:24:30.455 
Epoch 80/1000 
	 loss: 404.5708, MinusLogProbMetric: 404.5708, val_loss: 405.9065, val_MinusLogProbMetric: 405.9065

Epoch 80: val_loss did not improve from 404.08359
196/196 - 19s - loss: 404.5708 - MinusLogProbMetric: 404.5708 - val_loss: 405.9065 - val_MinusLogProbMetric: 405.9065 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 81/1000
2023-09-10 10:24:49.337 
Epoch 81/1000 
	 loss: 403.8177, MinusLogProbMetric: 403.8177, val_loss: 407.9026, val_MinusLogProbMetric: 407.9026

Epoch 81: val_loss did not improve from 404.08359
196/196 - 19s - loss: 403.8177 - MinusLogProbMetric: 403.8177 - val_loss: 407.9026 - val_MinusLogProbMetric: 407.9026 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 82/1000
2023-09-10 10:25:07.055 
Epoch 82/1000 
	 loss: 402.6158, MinusLogProbMetric: 402.6158, val_loss: 405.9221, val_MinusLogProbMetric: 405.9221

Epoch 82: val_loss did not improve from 404.08359
196/196 - 18s - loss: 402.6158 - MinusLogProbMetric: 402.6158 - val_loss: 405.9221 - val_MinusLogProbMetric: 405.9221 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 83/1000
2023-09-10 10:25:26.134 
Epoch 83/1000 
	 loss: 403.6570, MinusLogProbMetric: 403.6570, val_loss: 405.2729, val_MinusLogProbMetric: 405.2729

Epoch 83: val_loss did not improve from 404.08359
196/196 - 19s - loss: 403.6570 - MinusLogProbMetric: 403.6570 - val_loss: 405.2729 - val_MinusLogProbMetric: 405.2729 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 84/1000
2023-09-10 10:25:43.839 
Epoch 84/1000 
	 loss: 404.4711, MinusLogProbMetric: 404.4711, val_loss: 405.7383, val_MinusLogProbMetric: 405.7383

Epoch 84: val_loss did not improve from 404.08359
196/196 - 18s - loss: 404.4711 - MinusLogProbMetric: 404.4711 - val_loss: 405.7383 - val_MinusLogProbMetric: 405.7383 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 85/1000
2023-09-10 10:26:02.291 
Epoch 85/1000 
	 loss: 402.8091, MinusLogProbMetric: 402.8091, val_loss: 405.4914, val_MinusLogProbMetric: 405.4914

Epoch 85: val_loss did not improve from 404.08359
196/196 - 18s - loss: 402.8091 - MinusLogProbMetric: 402.8091 - val_loss: 405.4914 - val_MinusLogProbMetric: 405.4914 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 86/1000
2023-09-10 10:26:19.911 
Epoch 86/1000 
	 loss: 402.6504, MinusLogProbMetric: 402.6504, val_loss: 402.8194, val_MinusLogProbMetric: 402.8194

Epoch 86: val_loss improved from 404.08359 to 402.81940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 402.6504 - MinusLogProbMetric: 402.6504 - val_loss: 402.8194 - val_MinusLogProbMetric: 402.8194 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 87/1000
2023-09-10 10:26:37.803 
Epoch 87/1000 
	 loss: 402.8608, MinusLogProbMetric: 402.8608, val_loss: 403.7312, val_MinusLogProbMetric: 403.7312

Epoch 87: val_loss did not improve from 402.81940
196/196 - 17s - loss: 402.8608 - MinusLogProbMetric: 402.8608 - val_loss: 403.7312 - val_MinusLogProbMetric: 403.7312 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 88/1000
2023-09-10 10:26:55.737 
Epoch 88/1000 
	 loss: 402.6033, MinusLogProbMetric: 402.6033, val_loss: 406.3335, val_MinusLogProbMetric: 406.3335

Epoch 88: val_loss did not improve from 402.81940
196/196 - 18s - loss: 402.6033 - MinusLogProbMetric: 402.6033 - val_loss: 406.3335 - val_MinusLogProbMetric: 406.3335 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 89/1000
2023-09-10 10:27:13.838 
Epoch 89/1000 
	 loss: 402.6356, MinusLogProbMetric: 402.6356, val_loss: 406.6286, val_MinusLogProbMetric: 406.6286

Epoch 89: val_loss did not improve from 402.81940
196/196 - 18s - loss: 402.6356 - MinusLogProbMetric: 402.6356 - val_loss: 406.6286 - val_MinusLogProbMetric: 406.6286 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 90/1000
2023-09-10 10:27:31.569 
Epoch 90/1000 
	 loss: 402.3117, MinusLogProbMetric: 402.3117, val_loss: 405.6322, val_MinusLogProbMetric: 405.6322

Epoch 90: val_loss did not improve from 402.81940
196/196 - 18s - loss: 402.3117 - MinusLogProbMetric: 402.3117 - val_loss: 405.6322 - val_MinusLogProbMetric: 405.6322 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 91/1000
2023-09-10 10:27:49.719 
Epoch 91/1000 
	 loss: 402.1815, MinusLogProbMetric: 402.1815, val_loss: 404.3771, val_MinusLogProbMetric: 404.3771

Epoch 91: val_loss did not improve from 402.81940
196/196 - 18s - loss: 402.1815 - MinusLogProbMetric: 402.1815 - val_loss: 404.3771 - val_MinusLogProbMetric: 404.3771 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 92/1000
2023-09-10 10:28:08.021 
Epoch 92/1000 
	 loss: 402.1520, MinusLogProbMetric: 402.1520, val_loss: 405.6004, val_MinusLogProbMetric: 405.6004

Epoch 92: val_loss did not improve from 402.81940
196/196 - 18s - loss: 402.1520 - MinusLogProbMetric: 402.1520 - val_loss: 405.6004 - val_MinusLogProbMetric: 405.6004 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 93/1000
2023-09-10 10:28:25.903 
Epoch 93/1000 
	 loss: 401.8065, MinusLogProbMetric: 401.8065, val_loss: 403.1517, val_MinusLogProbMetric: 403.1517

Epoch 93: val_loss did not improve from 402.81940
196/196 - 18s - loss: 401.8065 - MinusLogProbMetric: 401.8065 - val_loss: 403.1517 - val_MinusLogProbMetric: 403.1517 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 94/1000
2023-09-10 10:28:42.731 
Epoch 94/1000 
	 loss: 402.5709, MinusLogProbMetric: 402.5709, val_loss: 408.6457, val_MinusLogProbMetric: 408.6457

Epoch 94: val_loss did not improve from 402.81940
196/196 - 17s - loss: 402.5709 - MinusLogProbMetric: 402.5709 - val_loss: 408.6457 - val_MinusLogProbMetric: 408.6457 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 95/1000
2023-09-10 10:29:00.481 
Epoch 95/1000 
	 loss: 401.9605, MinusLogProbMetric: 401.9605, val_loss: 404.2163, val_MinusLogProbMetric: 404.2163

Epoch 95: val_loss did not improve from 402.81940
196/196 - 18s - loss: 401.9605 - MinusLogProbMetric: 401.9605 - val_loss: 404.2163 - val_MinusLogProbMetric: 404.2163 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 96/1000
2023-09-10 10:29:17.953 
Epoch 96/1000 
	 loss: 401.6217, MinusLogProbMetric: 401.6217, val_loss: 402.8163, val_MinusLogProbMetric: 402.8163

Epoch 96: val_loss improved from 402.81940 to 402.81631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 401.6217 - MinusLogProbMetric: 401.6217 - val_loss: 402.8163 - val_MinusLogProbMetric: 402.8163 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 97/1000
2023-09-10 10:29:36.175 
Epoch 97/1000 
	 loss: 402.0580, MinusLogProbMetric: 402.0580, val_loss: 404.5051, val_MinusLogProbMetric: 404.5051

Epoch 97: val_loss did not improve from 402.81631
196/196 - 17s - loss: 402.0580 - MinusLogProbMetric: 402.0580 - val_loss: 404.5051 - val_MinusLogProbMetric: 404.5051 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 98/1000
2023-09-10 10:29:53.867 
Epoch 98/1000 
	 loss: 401.7977, MinusLogProbMetric: 401.7977, val_loss: 406.9218, val_MinusLogProbMetric: 406.9218

Epoch 98: val_loss did not improve from 402.81631
196/196 - 18s - loss: 401.7977 - MinusLogProbMetric: 401.7977 - val_loss: 406.9218 - val_MinusLogProbMetric: 406.9218 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 99/1000
2023-09-10 10:30:11.748 
Epoch 99/1000 
	 loss: 401.5375, MinusLogProbMetric: 401.5375, val_loss: 406.3292, val_MinusLogProbMetric: 406.3292

Epoch 99: val_loss did not improve from 402.81631
196/196 - 18s - loss: 401.5375 - MinusLogProbMetric: 401.5375 - val_loss: 406.3292 - val_MinusLogProbMetric: 406.3292 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 100/1000
2023-09-10 10:30:29.490 
Epoch 100/1000 
	 loss: 401.4166, MinusLogProbMetric: 401.4166, val_loss: 403.9515, val_MinusLogProbMetric: 403.9515

Epoch 100: val_loss did not improve from 402.81631
196/196 - 18s - loss: 401.4166 - MinusLogProbMetric: 401.4166 - val_loss: 403.9515 - val_MinusLogProbMetric: 403.9515 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 101/1000
2023-09-10 10:30:47.735 
Epoch 101/1000 
	 loss: 402.0660, MinusLogProbMetric: 402.0660, val_loss: 402.4298, val_MinusLogProbMetric: 402.4298

Epoch 101: val_loss improved from 402.81631 to 402.42984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 402.0660 - MinusLogProbMetric: 402.0660 - val_loss: 402.4298 - val_MinusLogProbMetric: 402.4298 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 102/1000
2023-09-10 10:31:06.618 
Epoch 102/1000 
	 loss: 402.1668, MinusLogProbMetric: 402.1668, val_loss: 404.6469, val_MinusLogProbMetric: 404.6469

Epoch 102: val_loss did not improve from 402.42984
196/196 - 18s - loss: 402.1668 - MinusLogProbMetric: 402.1668 - val_loss: 404.6469 - val_MinusLogProbMetric: 404.6469 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 103/1000
2023-09-10 10:31:24.628 
Epoch 103/1000 
	 loss: 401.0807, MinusLogProbMetric: 401.0807, val_loss: 402.6266, val_MinusLogProbMetric: 402.6266

Epoch 103: val_loss did not improve from 402.42984
196/196 - 18s - loss: 401.0807 - MinusLogProbMetric: 401.0807 - val_loss: 402.6266 - val_MinusLogProbMetric: 402.6266 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 104/1000
2023-09-10 10:31:42.165 
Epoch 104/1000 
	 loss: 401.4784, MinusLogProbMetric: 401.4784, val_loss: 402.0864, val_MinusLogProbMetric: 402.0864

Epoch 104: val_loss improved from 402.42984 to 402.08636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 401.4784 - MinusLogProbMetric: 401.4784 - val_loss: 402.0864 - val_MinusLogProbMetric: 402.0864 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 105/1000
2023-09-10 10:32:01.359 
Epoch 105/1000 
	 loss: 401.2168, MinusLogProbMetric: 401.2168, val_loss: 404.0332, val_MinusLogProbMetric: 404.0332

Epoch 105: val_loss did not improve from 402.08636
196/196 - 19s - loss: 401.2168 - MinusLogProbMetric: 401.2168 - val_loss: 404.0332 - val_MinusLogProbMetric: 404.0332 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 106/1000
2023-09-10 10:32:19.817 
Epoch 106/1000 
	 loss: 401.6645, MinusLogProbMetric: 401.6645, val_loss: 402.5832, val_MinusLogProbMetric: 402.5832

Epoch 106: val_loss did not improve from 402.08636
196/196 - 18s - loss: 401.6645 - MinusLogProbMetric: 401.6645 - val_loss: 402.5832 - val_MinusLogProbMetric: 402.5832 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 107/1000
2023-09-10 10:32:39.200 
Epoch 107/1000 
	 loss: 401.4220, MinusLogProbMetric: 401.4220, val_loss: 412.3759, val_MinusLogProbMetric: 412.3759

Epoch 107: val_loss did not improve from 402.08636
196/196 - 19s - loss: 401.4220 - MinusLogProbMetric: 401.4220 - val_loss: 412.3759 - val_MinusLogProbMetric: 412.3759 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 108/1000
2023-09-10 10:32:58.643 
Epoch 108/1000 
	 loss: 401.3945, MinusLogProbMetric: 401.3945, val_loss: 405.8735, val_MinusLogProbMetric: 405.8735

Epoch 108: val_loss did not improve from 402.08636
196/196 - 19s - loss: 401.3945 - MinusLogProbMetric: 401.3945 - val_loss: 405.8735 - val_MinusLogProbMetric: 405.8735 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 109/1000
2023-09-10 10:33:18.127 
Epoch 109/1000 
	 loss: 401.4525, MinusLogProbMetric: 401.4525, val_loss: 402.9200, val_MinusLogProbMetric: 402.9200

Epoch 109: val_loss did not improve from 402.08636
196/196 - 19s - loss: 401.4525 - MinusLogProbMetric: 401.4525 - val_loss: 402.9200 - val_MinusLogProbMetric: 402.9200 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 110/1000
2023-09-10 10:33:36.205 
Epoch 110/1000 
	 loss: 401.1764, MinusLogProbMetric: 401.1764, val_loss: 402.9949, val_MinusLogProbMetric: 402.9949

Epoch 110: val_loss did not improve from 402.08636
196/196 - 18s - loss: 401.1764 - MinusLogProbMetric: 401.1764 - val_loss: 402.9949 - val_MinusLogProbMetric: 402.9949 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 111/1000
2023-09-10 10:33:54.059 
Epoch 111/1000 
	 loss: 401.0206, MinusLogProbMetric: 401.0206, val_loss: 409.7813, val_MinusLogProbMetric: 409.7813

Epoch 111: val_loss did not improve from 402.08636
196/196 - 18s - loss: 401.0206 - MinusLogProbMetric: 401.0206 - val_loss: 409.7813 - val_MinusLogProbMetric: 409.7813 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 112/1000
2023-09-10 10:34:11.006 
Epoch 112/1000 
	 loss: 401.5334, MinusLogProbMetric: 401.5334, val_loss: 404.6148, val_MinusLogProbMetric: 404.6148

Epoch 112: val_loss did not improve from 402.08636
196/196 - 17s - loss: 401.5334 - MinusLogProbMetric: 401.5334 - val_loss: 404.6148 - val_MinusLogProbMetric: 404.6148 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 113/1000
2023-09-10 10:34:28.807 
Epoch 113/1000 
	 loss: 401.1597, MinusLogProbMetric: 401.1597, val_loss: 402.1724, val_MinusLogProbMetric: 402.1724

Epoch 113: val_loss did not improve from 402.08636
196/196 - 18s - loss: 401.1597 - MinusLogProbMetric: 401.1597 - val_loss: 402.1724 - val_MinusLogProbMetric: 402.1724 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 114/1000
2023-09-10 10:34:45.984 
Epoch 114/1000 
	 loss: 400.7382, MinusLogProbMetric: 400.7382, val_loss: 412.8930, val_MinusLogProbMetric: 412.8930

Epoch 114: val_loss did not improve from 402.08636
196/196 - 17s - loss: 400.7382 - MinusLogProbMetric: 400.7382 - val_loss: 412.8930 - val_MinusLogProbMetric: 412.8930 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 115/1000
2023-09-10 10:35:03.322 
Epoch 115/1000 
	 loss: 400.6182, MinusLogProbMetric: 400.6182, val_loss: 410.9516, val_MinusLogProbMetric: 410.9516

Epoch 115: val_loss did not improve from 402.08636
196/196 - 17s - loss: 400.6182 - MinusLogProbMetric: 400.6182 - val_loss: 410.9516 - val_MinusLogProbMetric: 410.9516 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 116/1000
2023-09-10 10:35:20.873 
Epoch 116/1000 
	 loss: 400.4740, MinusLogProbMetric: 400.4740, val_loss: 403.3482, val_MinusLogProbMetric: 403.3482

Epoch 116: val_loss did not improve from 402.08636
196/196 - 18s - loss: 400.4740 - MinusLogProbMetric: 400.4740 - val_loss: 403.3482 - val_MinusLogProbMetric: 403.3482 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 117/1000
2023-09-10 10:35:38.214 
Epoch 117/1000 
	 loss: 400.6503, MinusLogProbMetric: 400.6503, val_loss: 404.0657, val_MinusLogProbMetric: 404.0657

Epoch 117: val_loss did not improve from 402.08636
196/196 - 17s - loss: 400.6503 - MinusLogProbMetric: 400.6503 - val_loss: 404.0657 - val_MinusLogProbMetric: 404.0657 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 118/1000
2023-09-10 10:35:55.485 
Epoch 118/1000 
	 loss: 401.3710, MinusLogProbMetric: 401.3710, val_loss: 420.9047, val_MinusLogProbMetric: 420.9047

Epoch 118: val_loss did not improve from 402.08636
196/196 - 17s - loss: 401.3710 - MinusLogProbMetric: 401.3710 - val_loss: 420.9047 - val_MinusLogProbMetric: 420.9047 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 119/1000
2023-09-10 10:36:12.635 
Epoch 119/1000 
	 loss: 401.1535, MinusLogProbMetric: 401.1535, val_loss: 403.2330, val_MinusLogProbMetric: 403.2330

Epoch 119: val_loss did not improve from 402.08636
196/196 - 17s - loss: 401.1535 - MinusLogProbMetric: 401.1535 - val_loss: 403.2330 - val_MinusLogProbMetric: 403.2330 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 120/1000
2023-09-10 10:36:29.982 
Epoch 120/1000 
	 loss: 401.6726, MinusLogProbMetric: 401.6726, val_loss: 401.8843, val_MinusLogProbMetric: 401.8843

Epoch 120: val_loss improved from 402.08636 to 401.88428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 401.6726 - MinusLogProbMetric: 401.6726 - val_loss: 401.8843 - val_MinusLogProbMetric: 401.8843 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 121/1000
2023-09-10 10:36:47.485 
Epoch 121/1000 
	 loss: 399.9927, MinusLogProbMetric: 399.9927, val_loss: 407.6999, val_MinusLogProbMetric: 407.6999

Epoch 121: val_loss did not improve from 401.88428
196/196 - 17s - loss: 399.9927 - MinusLogProbMetric: 399.9927 - val_loss: 407.6999 - val_MinusLogProbMetric: 407.6999 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 122/1000
2023-09-10 10:37:04.708 
Epoch 122/1000 
	 loss: 400.4288, MinusLogProbMetric: 400.4288, val_loss: 406.1765, val_MinusLogProbMetric: 406.1765

Epoch 122: val_loss did not improve from 401.88428
196/196 - 17s - loss: 400.4288 - MinusLogProbMetric: 400.4288 - val_loss: 406.1765 - val_MinusLogProbMetric: 406.1765 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 123/1000
2023-09-10 10:37:21.904 
Epoch 123/1000 
	 loss: 401.0240, MinusLogProbMetric: 401.0240, val_loss: 404.8614, val_MinusLogProbMetric: 404.8614

Epoch 123: val_loss did not improve from 401.88428
196/196 - 17s - loss: 401.0240 - MinusLogProbMetric: 401.0240 - val_loss: 404.8614 - val_MinusLogProbMetric: 404.8614 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 124/1000
2023-09-10 10:37:38.588 
Epoch 124/1000 
	 loss: 400.3345, MinusLogProbMetric: 400.3345, val_loss: 407.0237, val_MinusLogProbMetric: 407.0237

Epoch 124: val_loss did not improve from 401.88428
196/196 - 17s - loss: 400.3345 - MinusLogProbMetric: 400.3345 - val_loss: 407.0237 - val_MinusLogProbMetric: 407.0237 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 125/1000
2023-09-10 10:37:55.902 
Epoch 125/1000 
	 loss: 400.3782, MinusLogProbMetric: 400.3782, val_loss: 404.1981, val_MinusLogProbMetric: 404.1981

Epoch 125: val_loss did not improve from 401.88428
196/196 - 17s - loss: 400.3782 - MinusLogProbMetric: 400.3782 - val_loss: 404.1981 - val_MinusLogProbMetric: 404.1981 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 126/1000
2023-09-10 10:38:13.451 
Epoch 126/1000 
	 loss: 400.2388, MinusLogProbMetric: 400.2388, val_loss: 403.1801, val_MinusLogProbMetric: 403.1801

Epoch 126: val_loss did not improve from 401.88428
196/196 - 18s - loss: 400.2388 - MinusLogProbMetric: 400.2388 - val_loss: 403.1801 - val_MinusLogProbMetric: 403.1801 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 127/1000
2023-09-10 10:38:30.810 
Epoch 127/1000 
	 loss: 400.0707, MinusLogProbMetric: 400.0707, val_loss: 405.8128, val_MinusLogProbMetric: 405.8128

Epoch 127: val_loss did not improve from 401.88428
196/196 - 17s - loss: 400.0707 - MinusLogProbMetric: 400.0707 - val_loss: 405.8128 - val_MinusLogProbMetric: 405.8128 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 128/1000
2023-09-10 10:38:48.018 
Epoch 128/1000 
	 loss: 400.5498, MinusLogProbMetric: 400.5498, val_loss: 401.1884, val_MinusLogProbMetric: 401.1884

Epoch 128: val_loss improved from 401.88428 to 401.18839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 400.5498 - MinusLogProbMetric: 400.5498 - val_loss: 401.1884 - val_MinusLogProbMetric: 401.1884 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 129/1000
2023-09-10 10:39:05.735 
Epoch 129/1000 
	 loss: 399.9967, MinusLogProbMetric: 399.9967, val_loss: 402.2088, val_MinusLogProbMetric: 402.2088

Epoch 129: val_loss did not improve from 401.18839
196/196 - 17s - loss: 399.9967 - MinusLogProbMetric: 399.9967 - val_loss: 402.2088 - val_MinusLogProbMetric: 402.2088 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 130/1000
2023-09-10 10:39:22.214 
Epoch 130/1000 
	 loss: 400.2710, MinusLogProbMetric: 400.2710, val_loss: 402.7886, val_MinusLogProbMetric: 402.7886

Epoch 130: val_loss did not improve from 401.18839
196/196 - 16s - loss: 400.2710 - MinusLogProbMetric: 400.2710 - val_loss: 402.7886 - val_MinusLogProbMetric: 402.7886 - lr: 3.3333e-04 - 16s/epoch - 84ms/step
Epoch 131/1000
2023-09-10 10:39:39.770 
Epoch 131/1000 
	 loss: 399.6573, MinusLogProbMetric: 399.6573, val_loss: 405.4171, val_MinusLogProbMetric: 405.4171

Epoch 131: val_loss did not improve from 401.18839
196/196 - 18s - loss: 399.6573 - MinusLogProbMetric: 399.6573 - val_loss: 405.4171 - val_MinusLogProbMetric: 405.4171 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 132/1000
2023-09-10 10:39:57.364 
Epoch 132/1000 
	 loss: 399.5748, MinusLogProbMetric: 399.5748, val_loss: 405.8928, val_MinusLogProbMetric: 405.8928

Epoch 132: val_loss did not improve from 401.18839
196/196 - 18s - loss: 399.5748 - MinusLogProbMetric: 399.5748 - val_loss: 405.8928 - val_MinusLogProbMetric: 405.8928 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 133/1000
2023-09-10 10:40:15.574 
Epoch 133/1000 
	 loss: 400.2206, MinusLogProbMetric: 400.2206, val_loss: 403.1396, val_MinusLogProbMetric: 403.1396

Epoch 133: val_loss did not improve from 401.18839
196/196 - 18s - loss: 400.2206 - MinusLogProbMetric: 400.2206 - val_loss: 403.1396 - val_MinusLogProbMetric: 403.1396 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 134/1000
2023-09-10 10:40:32.957 
Epoch 134/1000 
	 loss: 399.7715, MinusLogProbMetric: 399.7715, val_loss: 408.8229, val_MinusLogProbMetric: 408.8229

Epoch 134: val_loss did not improve from 401.18839
196/196 - 17s - loss: 399.7715 - MinusLogProbMetric: 399.7715 - val_loss: 408.8229 - val_MinusLogProbMetric: 408.8229 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 135/1000
2023-09-10 10:40:50.471 
Epoch 135/1000 
	 loss: 399.4471, MinusLogProbMetric: 399.4471, val_loss: 402.6124, val_MinusLogProbMetric: 402.6124

Epoch 135: val_loss did not improve from 401.18839
196/196 - 18s - loss: 399.4471 - MinusLogProbMetric: 399.4471 - val_loss: 402.6124 - val_MinusLogProbMetric: 402.6124 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 136/1000
2023-09-10 10:41:08.223 
Epoch 136/1000 
	 loss: 399.6700, MinusLogProbMetric: 399.6700, val_loss: 400.9643, val_MinusLogProbMetric: 400.9643

Epoch 136: val_loss improved from 401.18839 to 400.96432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 399.6700 - MinusLogProbMetric: 399.6700 - val_loss: 400.9643 - val_MinusLogProbMetric: 400.9643 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 137/1000
2023-09-10 10:41:26.464 
Epoch 137/1000 
	 loss: 399.3188, MinusLogProbMetric: 399.3188, val_loss: 402.1428, val_MinusLogProbMetric: 402.1428

Epoch 137: val_loss did not improve from 400.96432
196/196 - 18s - loss: 399.3188 - MinusLogProbMetric: 399.3188 - val_loss: 402.1428 - val_MinusLogProbMetric: 402.1428 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 138/1000
2023-09-10 10:41:43.852 
Epoch 138/1000 
	 loss: 399.4229, MinusLogProbMetric: 399.4229, val_loss: 402.3464, val_MinusLogProbMetric: 402.3464

Epoch 138: val_loss did not improve from 400.96432
196/196 - 17s - loss: 399.4229 - MinusLogProbMetric: 399.4229 - val_loss: 402.3464 - val_MinusLogProbMetric: 402.3464 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 139/1000
2023-09-10 10:42:01.237 
Epoch 139/1000 
	 loss: 399.9094, MinusLogProbMetric: 399.9094, val_loss: 402.4620, val_MinusLogProbMetric: 402.4620

Epoch 139: val_loss did not improve from 400.96432
196/196 - 17s - loss: 399.9094 - MinusLogProbMetric: 399.9094 - val_loss: 402.4620 - val_MinusLogProbMetric: 402.4620 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 140/1000
2023-09-10 10:42:18.495 
Epoch 140/1000 
	 loss: 399.7588, MinusLogProbMetric: 399.7588, val_loss: 403.6792, val_MinusLogProbMetric: 403.6792

Epoch 140: val_loss did not improve from 400.96432
196/196 - 17s - loss: 399.7588 - MinusLogProbMetric: 399.7588 - val_loss: 403.6792 - val_MinusLogProbMetric: 403.6792 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 141/1000
2023-09-10 10:42:37.412 
Epoch 141/1000 
	 loss: 399.5708, MinusLogProbMetric: 399.5708, val_loss: 401.0491, val_MinusLogProbMetric: 401.0491

Epoch 141: val_loss did not improve from 400.96432
196/196 - 19s - loss: 399.5708 - MinusLogProbMetric: 399.5708 - val_loss: 401.0491 - val_MinusLogProbMetric: 401.0491 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 142/1000
2023-09-10 10:42:55.465 
Epoch 142/1000 
	 loss: 400.2204, MinusLogProbMetric: 400.2204, val_loss: 402.3122, val_MinusLogProbMetric: 402.3122

Epoch 142: val_loss did not improve from 400.96432
196/196 - 18s - loss: 400.2204 - MinusLogProbMetric: 400.2204 - val_loss: 402.3122 - val_MinusLogProbMetric: 402.3122 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 143/1000
2023-09-10 10:43:13.460 
Epoch 143/1000 
	 loss: 399.1284, MinusLogProbMetric: 399.1284, val_loss: 403.1922, val_MinusLogProbMetric: 403.1922

Epoch 143: val_loss did not improve from 400.96432
196/196 - 18s - loss: 399.1284 - MinusLogProbMetric: 399.1284 - val_loss: 403.1922 - val_MinusLogProbMetric: 403.1922 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 144/1000
2023-09-10 10:43:32.025 
Epoch 144/1000 
	 loss: 399.3285, MinusLogProbMetric: 399.3285, val_loss: 409.9312, val_MinusLogProbMetric: 409.9312

Epoch 144: val_loss did not improve from 400.96432
196/196 - 19s - loss: 399.3285 - MinusLogProbMetric: 399.3285 - val_loss: 409.9312 - val_MinusLogProbMetric: 409.9312 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 145/1000
2023-09-10 10:43:50.569 
Epoch 145/1000 
	 loss: 399.3624, MinusLogProbMetric: 399.3624, val_loss: 405.7834, val_MinusLogProbMetric: 405.7834

Epoch 145: val_loss did not improve from 400.96432
196/196 - 19s - loss: 399.3624 - MinusLogProbMetric: 399.3624 - val_loss: 405.7834 - val_MinusLogProbMetric: 405.7834 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 146/1000
2023-09-10 10:44:11.682 
Epoch 146/1000 
	 loss: 399.8324, MinusLogProbMetric: 399.8324, val_loss: 401.7541, val_MinusLogProbMetric: 401.7541

Epoch 146: val_loss did not improve from 400.96432
196/196 - 21s - loss: 399.8324 - MinusLogProbMetric: 399.8324 - val_loss: 401.7541 - val_MinusLogProbMetric: 401.7541 - lr: 3.3333e-04 - 21s/epoch - 108ms/step
Epoch 147/1000
2023-09-10 10:44:30.320 
Epoch 147/1000 
	 loss: 399.6441, MinusLogProbMetric: 399.6441, val_loss: 403.9013, val_MinusLogProbMetric: 403.9013

Epoch 147: val_loss did not improve from 400.96432
196/196 - 19s - loss: 399.6441 - MinusLogProbMetric: 399.6441 - val_loss: 403.9013 - val_MinusLogProbMetric: 403.9013 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 148/1000
2023-09-10 10:44:48.242 
Epoch 148/1000 
	 loss: 398.9897, MinusLogProbMetric: 398.9897, val_loss: 404.1742, val_MinusLogProbMetric: 404.1742

Epoch 148: val_loss did not improve from 400.96432
196/196 - 18s - loss: 398.9897 - MinusLogProbMetric: 398.9897 - val_loss: 404.1742 - val_MinusLogProbMetric: 404.1742 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 149/1000
2023-09-10 10:45:06.610 
Epoch 149/1000 
	 loss: 402.3656, MinusLogProbMetric: 402.3656, val_loss: 402.1328, val_MinusLogProbMetric: 402.1328

Epoch 149: val_loss did not improve from 400.96432
196/196 - 18s - loss: 402.3656 - MinusLogProbMetric: 402.3656 - val_loss: 402.1328 - val_MinusLogProbMetric: 402.1328 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 150/1000
2023-09-10 10:45:24.835 
Epoch 150/1000 
	 loss: 399.3443, MinusLogProbMetric: 399.3443, val_loss: 401.8998, val_MinusLogProbMetric: 401.8998

Epoch 150: val_loss did not improve from 400.96432
196/196 - 18s - loss: 399.3443 - MinusLogProbMetric: 399.3443 - val_loss: 401.8998 - val_MinusLogProbMetric: 401.8998 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 151/1000
2023-09-10 10:45:43.710 
Epoch 151/1000 
	 loss: 399.7810, MinusLogProbMetric: 399.7810, val_loss: 400.4468, val_MinusLogProbMetric: 400.4468

Epoch 151: val_loss improved from 400.96432 to 400.44684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 399.7810 - MinusLogProbMetric: 399.7810 - val_loss: 400.4468 - val_MinusLogProbMetric: 400.4468 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 152/1000
2023-09-10 10:46:03.192 
Epoch 152/1000 
	 loss: 399.4802, MinusLogProbMetric: 399.4802, val_loss: 436.7218, val_MinusLogProbMetric: 436.7218

Epoch 152: val_loss did not improve from 400.44684
196/196 - 19s - loss: 399.4802 - MinusLogProbMetric: 399.4802 - val_loss: 436.7218 - val_MinusLogProbMetric: 436.7218 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 153/1000
2023-09-10 10:46:23.193 
Epoch 153/1000 
	 loss: 400.0615, MinusLogProbMetric: 400.0615, val_loss: 407.9916, val_MinusLogProbMetric: 407.9916

Epoch 153: val_loss did not improve from 400.44684
196/196 - 20s - loss: 400.0615 - MinusLogProbMetric: 400.0615 - val_loss: 407.9916 - val_MinusLogProbMetric: 407.9916 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 154/1000
2023-09-10 10:46:43.112 
Epoch 154/1000 
	 loss: 399.2997, MinusLogProbMetric: 399.2997, val_loss: 407.2733, val_MinusLogProbMetric: 407.2733

Epoch 154: val_loss did not improve from 400.44684
196/196 - 20s - loss: 399.2997 - MinusLogProbMetric: 399.2997 - val_loss: 407.2733 - val_MinusLogProbMetric: 407.2733 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 155/1000
2023-09-10 10:47:00.883 
Epoch 155/1000 
	 loss: 398.6192, MinusLogProbMetric: 398.6192, val_loss: 403.5379, val_MinusLogProbMetric: 403.5379

Epoch 155: val_loss did not improve from 400.44684
196/196 - 18s - loss: 398.6192 - MinusLogProbMetric: 398.6192 - val_loss: 403.5379 - val_MinusLogProbMetric: 403.5379 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 156/1000
2023-09-10 10:47:18.696 
Epoch 156/1000 
	 loss: 399.3941, MinusLogProbMetric: 399.3941, val_loss: 401.7144, val_MinusLogProbMetric: 401.7144

Epoch 156: val_loss did not improve from 400.44684
196/196 - 18s - loss: 399.3941 - MinusLogProbMetric: 399.3941 - val_loss: 401.7144 - val_MinusLogProbMetric: 401.7144 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 157/1000
2023-09-10 10:47:35.939 
Epoch 157/1000 
	 loss: 398.4824, MinusLogProbMetric: 398.4824, val_loss: 404.6866, val_MinusLogProbMetric: 404.6866

Epoch 157: val_loss did not improve from 400.44684
196/196 - 17s - loss: 398.4824 - MinusLogProbMetric: 398.4824 - val_loss: 404.6866 - val_MinusLogProbMetric: 404.6866 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 158/1000
2023-09-10 10:47:52.880 
Epoch 158/1000 
	 loss: 398.6092, MinusLogProbMetric: 398.6092, val_loss: 407.4581, val_MinusLogProbMetric: 407.4581

Epoch 158: val_loss did not improve from 400.44684
196/196 - 17s - loss: 398.6092 - MinusLogProbMetric: 398.6092 - val_loss: 407.4581 - val_MinusLogProbMetric: 407.4581 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 159/1000
2023-09-10 10:48:10.922 
Epoch 159/1000 
	 loss: 398.8046, MinusLogProbMetric: 398.8046, val_loss: 400.8516, val_MinusLogProbMetric: 400.8516

Epoch 159: val_loss did not improve from 400.44684
196/196 - 18s - loss: 398.8046 - MinusLogProbMetric: 398.8046 - val_loss: 400.8516 - val_MinusLogProbMetric: 400.8516 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 160/1000
2023-09-10 10:48:29.035 
Epoch 160/1000 
	 loss: 398.5882, MinusLogProbMetric: 398.5882, val_loss: 400.8559, val_MinusLogProbMetric: 400.8559

Epoch 160: val_loss did not improve from 400.44684
196/196 - 18s - loss: 398.5882 - MinusLogProbMetric: 398.5882 - val_loss: 400.8559 - val_MinusLogProbMetric: 400.8559 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 161/1000
2023-09-10 10:48:46.276 
Epoch 161/1000 
	 loss: 398.6064, MinusLogProbMetric: 398.6064, val_loss: 400.7956, val_MinusLogProbMetric: 400.7956

Epoch 161: val_loss did not improve from 400.44684
196/196 - 17s - loss: 398.6064 - MinusLogProbMetric: 398.6064 - val_loss: 400.7956 - val_MinusLogProbMetric: 400.7956 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 162/1000
2023-09-10 10:49:03.465 
Epoch 162/1000 
	 loss: 398.7791, MinusLogProbMetric: 398.7791, val_loss: 401.9763, val_MinusLogProbMetric: 401.9763

Epoch 162: val_loss did not improve from 400.44684
196/196 - 17s - loss: 398.7791 - MinusLogProbMetric: 398.7791 - val_loss: 401.9763 - val_MinusLogProbMetric: 401.9763 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 163/1000
2023-09-10 10:49:20.868 
Epoch 163/1000 
	 loss: 398.9626, MinusLogProbMetric: 398.9626, val_loss: 402.3103, val_MinusLogProbMetric: 402.3103

Epoch 163: val_loss did not improve from 400.44684
196/196 - 17s - loss: 398.9626 - MinusLogProbMetric: 398.9626 - val_loss: 402.3103 - val_MinusLogProbMetric: 402.3103 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 164/1000
2023-09-10 10:49:38.092 
Epoch 164/1000 
	 loss: 398.4659, MinusLogProbMetric: 398.4659, val_loss: 402.3554, val_MinusLogProbMetric: 402.3554

Epoch 164: val_loss did not improve from 400.44684
196/196 - 17s - loss: 398.4659 - MinusLogProbMetric: 398.4659 - val_loss: 402.3554 - val_MinusLogProbMetric: 402.3554 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 165/1000
2023-09-10 10:49:55.841 
Epoch 165/1000 
	 loss: 398.5466, MinusLogProbMetric: 398.5466, val_loss: 402.0125, val_MinusLogProbMetric: 402.0125

Epoch 165: val_loss did not improve from 400.44684
196/196 - 18s - loss: 398.5466 - MinusLogProbMetric: 398.5466 - val_loss: 402.0125 - val_MinusLogProbMetric: 402.0125 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 166/1000
2023-09-10 10:50:12.997 
Epoch 166/1000 
	 loss: 398.1567, MinusLogProbMetric: 398.1567, val_loss: 399.4202, val_MinusLogProbMetric: 399.4202

Epoch 166: val_loss improved from 400.44684 to 399.42020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 398.1567 - MinusLogProbMetric: 398.1567 - val_loss: 399.4202 - val_MinusLogProbMetric: 399.4202 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 167/1000
2023-09-10 10:50:30.174 
Epoch 167/1000 
	 loss: 399.1636, MinusLogProbMetric: 399.1636, val_loss: 400.7050, val_MinusLogProbMetric: 400.7050

Epoch 167: val_loss did not improve from 399.42020
196/196 - 17s - loss: 399.1636 - MinusLogProbMetric: 399.1636 - val_loss: 400.7050 - val_MinusLogProbMetric: 400.7050 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 168/1000
2023-09-10 10:50:48.079 
Epoch 168/1000 
	 loss: 398.0133, MinusLogProbMetric: 398.0133, val_loss: 401.9846, val_MinusLogProbMetric: 401.9846

Epoch 168: val_loss did not improve from 399.42020
196/196 - 18s - loss: 398.0133 - MinusLogProbMetric: 398.0133 - val_loss: 401.9846 - val_MinusLogProbMetric: 401.9846 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 169/1000
2023-09-10 10:51:06.378 
Epoch 169/1000 
	 loss: 399.0282, MinusLogProbMetric: 399.0282, val_loss: 410.3826, val_MinusLogProbMetric: 410.3826

Epoch 169: val_loss did not improve from 399.42020
196/196 - 18s - loss: 399.0282 - MinusLogProbMetric: 399.0282 - val_loss: 410.3826 - val_MinusLogProbMetric: 410.3826 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 170/1000
2023-09-10 10:51:24.018 
Epoch 170/1000 
	 loss: 398.4846, MinusLogProbMetric: 398.4846, val_loss: 400.3828, val_MinusLogProbMetric: 400.3828

Epoch 170: val_loss did not improve from 399.42020
196/196 - 18s - loss: 398.4846 - MinusLogProbMetric: 398.4846 - val_loss: 400.3828 - val_MinusLogProbMetric: 400.3828 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 171/1000
2023-09-10 10:51:42.328 
Epoch 171/1000 
	 loss: 399.1926, MinusLogProbMetric: 399.1926, val_loss: 399.8611, val_MinusLogProbMetric: 399.8611

Epoch 171: val_loss did not improve from 399.42020
196/196 - 18s - loss: 399.1926 - MinusLogProbMetric: 399.1926 - val_loss: 399.8611 - val_MinusLogProbMetric: 399.8611 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 172/1000
2023-09-10 10:52:00.952 
Epoch 172/1000 
	 loss: 398.1788, MinusLogProbMetric: 398.1788, val_loss: 412.0319, val_MinusLogProbMetric: 412.0319

Epoch 172: val_loss did not improve from 399.42020
196/196 - 19s - loss: 398.1788 - MinusLogProbMetric: 398.1788 - val_loss: 412.0319 - val_MinusLogProbMetric: 412.0319 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 173/1000
2023-09-10 10:52:19.523 
Epoch 173/1000 
	 loss: 398.5989, MinusLogProbMetric: 398.5989, val_loss: 403.1036, val_MinusLogProbMetric: 403.1036

Epoch 173: val_loss did not improve from 399.42020
196/196 - 19s - loss: 398.5989 - MinusLogProbMetric: 398.5989 - val_loss: 403.1036 - val_MinusLogProbMetric: 403.1036 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 174/1000
2023-09-10 10:52:38.114 
Epoch 174/1000 
	 loss: 398.1539, MinusLogProbMetric: 398.1539, val_loss: 401.6280, val_MinusLogProbMetric: 401.6280

Epoch 174: val_loss did not improve from 399.42020
196/196 - 19s - loss: 398.1539 - MinusLogProbMetric: 398.1539 - val_loss: 401.6280 - val_MinusLogProbMetric: 401.6280 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 175/1000
2023-09-10 10:52:56.475 
Epoch 175/1000 
	 loss: 397.9818, MinusLogProbMetric: 397.9818, val_loss: 401.0717, val_MinusLogProbMetric: 401.0717

Epoch 175: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.9818 - MinusLogProbMetric: 397.9818 - val_loss: 401.0717 - val_MinusLogProbMetric: 401.0717 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 176/1000
2023-09-10 10:53:15.600 
Epoch 176/1000 
	 loss: 398.2510, MinusLogProbMetric: 398.2510, val_loss: 403.0717, val_MinusLogProbMetric: 403.0717

Epoch 176: val_loss did not improve from 399.42020
196/196 - 19s - loss: 398.2510 - MinusLogProbMetric: 398.2510 - val_loss: 403.0717 - val_MinusLogProbMetric: 403.0717 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 177/1000
2023-09-10 10:53:34.510 
Epoch 177/1000 
	 loss: 397.9200, MinusLogProbMetric: 397.9200, val_loss: 416.4065, val_MinusLogProbMetric: 416.4065

Epoch 177: val_loss did not improve from 399.42020
196/196 - 19s - loss: 397.9200 - MinusLogProbMetric: 397.9200 - val_loss: 416.4065 - val_MinusLogProbMetric: 416.4065 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 178/1000
2023-09-10 10:53:54.919 
Epoch 178/1000 
	 loss: 400.2590, MinusLogProbMetric: 400.2590, val_loss: 404.9851, val_MinusLogProbMetric: 404.9851

Epoch 178: val_loss did not improve from 399.42020
196/196 - 20s - loss: 400.2590 - MinusLogProbMetric: 400.2590 - val_loss: 404.9851 - val_MinusLogProbMetric: 404.9851 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 179/1000
2023-09-10 10:54:14.165 
Epoch 179/1000 
	 loss: 397.9659, MinusLogProbMetric: 397.9659, val_loss: 403.4349, val_MinusLogProbMetric: 403.4349

Epoch 179: val_loss did not improve from 399.42020
196/196 - 19s - loss: 397.9659 - MinusLogProbMetric: 397.9659 - val_loss: 403.4349 - val_MinusLogProbMetric: 403.4349 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 180/1000
2023-09-10 10:54:33.040 
Epoch 180/1000 
	 loss: 398.2888, MinusLogProbMetric: 398.2888, val_loss: 401.8040, val_MinusLogProbMetric: 401.8040

Epoch 180: val_loss did not improve from 399.42020
196/196 - 19s - loss: 398.2888 - MinusLogProbMetric: 398.2888 - val_loss: 401.8040 - val_MinusLogProbMetric: 401.8040 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 181/1000
2023-09-10 10:54:51.624 
Epoch 181/1000 
	 loss: 400.3299, MinusLogProbMetric: 400.3299, val_loss: 401.1582, val_MinusLogProbMetric: 401.1582

Epoch 181: val_loss did not improve from 399.42020
196/196 - 19s - loss: 400.3299 - MinusLogProbMetric: 400.3299 - val_loss: 401.1582 - val_MinusLogProbMetric: 401.1582 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 182/1000
2023-09-10 10:55:12.156 
Epoch 182/1000 
	 loss: 397.9674, MinusLogProbMetric: 397.9674, val_loss: 400.4475, val_MinusLogProbMetric: 400.4475

Epoch 182: val_loss did not improve from 399.42020
196/196 - 21s - loss: 397.9674 - MinusLogProbMetric: 397.9674 - val_loss: 400.4475 - val_MinusLogProbMetric: 400.4475 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 183/1000
2023-09-10 10:55:31.244 
Epoch 183/1000 
	 loss: 397.4898, MinusLogProbMetric: 397.4898, val_loss: 400.9499, val_MinusLogProbMetric: 400.9499

Epoch 183: val_loss did not improve from 399.42020
196/196 - 19s - loss: 397.4898 - MinusLogProbMetric: 397.4898 - val_loss: 400.9499 - val_MinusLogProbMetric: 400.9499 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 184/1000
2023-09-10 10:55:50.695 
Epoch 184/1000 
	 loss: 397.9782, MinusLogProbMetric: 397.9782, val_loss: 400.5834, val_MinusLogProbMetric: 400.5834

Epoch 184: val_loss did not improve from 399.42020
196/196 - 19s - loss: 397.9782 - MinusLogProbMetric: 397.9782 - val_loss: 400.5834 - val_MinusLogProbMetric: 400.5834 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 185/1000
2023-09-10 10:56:10.974 
Epoch 185/1000 
	 loss: 397.3511, MinusLogProbMetric: 397.3511, val_loss: 401.5975, val_MinusLogProbMetric: 401.5975

Epoch 185: val_loss did not improve from 399.42020
196/196 - 20s - loss: 397.3511 - MinusLogProbMetric: 397.3511 - val_loss: 401.5975 - val_MinusLogProbMetric: 401.5975 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 186/1000
2023-09-10 10:56:29.694 
Epoch 186/1000 
	 loss: 397.6387, MinusLogProbMetric: 397.6387, val_loss: 400.2963, val_MinusLogProbMetric: 400.2963

Epoch 186: val_loss did not improve from 399.42020
196/196 - 19s - loss: 397.6387 - MinusLogProbMetric: 397.6387 - val_loss: 400.2963 - val_MinusLogProbMetric: 400.2963 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 187/1000
2023-09-10 10:56:47.116 
Epoch 187/1000 
	 loss: 398.4010, MinusLogProbMetric: 398.4010, val_loss: 403.2657, val_MinusLogProbMetric: 403.2657

Epoch 187: val_loss did not improve from 399.42020
196/196 - 17s - loss: 398.4010 - MinusLogProbMetric: 398.4010 - val_loss: 403.2657 - val_MinusLogProbMetric: 403.2657 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 188/1000
2023-09-10 10:57:04.692 
Epoch 188/1000 
	 loss: 397.9346, MinusLogProbMetric: 397.9346, val_loss: 406.5770, val_MinusLogProbMetric: 406.5770

Epoch 188: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.9346 - MinusLogProbMetric: 397.9346 - val_loss: 406.5770 - val_MinusLogProbMetric: 406.5770 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 189/1000
2023-09-10 10:57:22.335 
Epoch 189/1000 
	 loss: 397.5638, MinusLogProbMetric: 397.5638, val_loss: 412.0888, val_MinusLogProbMetric: 412.0888

Epoch 189: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.5638 - MinusLogProbMetric: 397.5638 - val_loss: 412.0888 - val_MinusLogProbMetric: 412.0888 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 190/1000
2023-09-10 10:57:39.966 
Epoch 190/1000 
	 loss: 397.6722, MinusLogProbMetric: 397.6722, val_loss: 403.0088, val_MinusLogProbMetric: 403.0088

Epoch 190: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.6722 - MinusLogProbMetric: 397.6722 - val_loss: 403.0088 - val_MinusLogProbMetric: 403.0088 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 191/1000
2023-09-10 10:57:57.414 
Epoch 191/1000 
	 loss: 397.5215, MinusLogProbMetric: 397.5215, val_loss: 402.4019, val_MinusLogProbMetric: 402.4019

Epoch 191: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.5215 - MinusLogProbMetric: 397.5215 - val_loss: 402.4019 - val_MinusLogProbMetric: 402.4019 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 192/1000
2023-09-10 10:58:14.482 
Epoch 192/1000 
	 loss: 397.3538, MinusLogProbMetric: 397.3538, val_loss: 399.8538, val_MinusLogProbMetric: 399.8538

Epoch 192: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.3538 - MinusLogProbMetric: 397.3538 - val_loss: 399.8538 - val_MinusLogProbMetric: 399.8538 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 193/1000
2023-09-10 10:58:31.888 
Epoch 193/1000 
	 loss: 397.6037, MinusLogProbMetric: 397.6037, val_loss: 400.5025, val_MinusLogProbMetric: 400.5025

Epoch 193: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.6037 - MinusLogProbMetric: 397.6037 - val_loss: 400.5025 - val_MinusLogProbMetric: 400.5025 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 194/1000
2023-09-10 10:58:49.086 
Epoch 194/1000 
	 loss: 397.5798, MinusLogProbMetric: 397.5798, val_loss: 401.1402, val_MinusLogProbMetric: 401.1402

Epoch 194: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.5798 - MinusLogProbMetric: 397.5798 - val_loss: 401.1402 - val_MinusLogProbMetric: 401.1402 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 195/1000
2023-09-10 10:59:06.718 
Epoch 195/1000 
	 loss: 397.5774, MinusLogProbMetric: 397.5774, val_loss: 406.6073, val_MinusLogProbMetric: 406.6073

Epoch 195: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.5774 - MinusLogProbMetric: 397.5774 - val_loss: 406.6073 - val_MinusLogProbMetric: 406.6073 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 196/1000
2023-09-10 10:59:24.086 
Epoch 196/1000 
	 loss: 399.1954, MinusLogProbMetric: 399.1954, val_loss: 400.2647, val_MinusLogProbMetric: 400.2647

Epoch 196: val_loss did not improve from 399.42020
196/196 - 17s - loss: 399.1954 - MinusLogProbMetric: 399.1954 - val_loss: 400.2647 - val_MinusLogProbMetric: 400.2647 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 197/1000
2023-09-10 10:59:40.080 
Epoch 197/1000 
	 loss: 398.3300, MinusLogProbMetric: 398.3300, val_loss: 410.8126, val_MinusLogProbMetric: 410.8126

Epoch 197: val_loss did not improve from 399.42020
196/196 - 16s - loss: 398.3300 - MinusLogProbMetric: 398.3300 - val_loss: 410.8126 - val_MinusLogProbMetric: 410.8126 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 198/1000
2023-09-10 10:59:57.131 
Epoch 198/1000 
	 loss: 397.2386, MinusLogProbMetric: 397.2386, val_loss: 400.9242, val_MinusLogProbMetric: 400.9242

Epoch 198: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.2386 - MinusLogProbMetric: 397.2386 - val_loss: 400.9242 - val_MinusLogProbMetric: 400.9242 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 199/1000
2023-09-10 11:00:14.199 
Epoch 199/1000 
	 loss: 397.3814, MinusLogProbMetric: 397.3814, val_loss: 402.1179, val_MinusLogProbMetric: 402.1179

Epoch 199: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.3814 - MinusLogProbMetric: 397.3814 - val_loss: 402.1179 - val_MinusLogProbMetric: 402.1179 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 200/1000
2023-09-10 11:00:31.980 
Epoch 200/1000 
	 loss: 398.8331, MinusLogProbMetric: 398.8331, val_loss: 401.0157, val_MinusLogProbMetric: 401.0157

Epoch 200: val_loss did not improve from 399.42020
196/196 - 18s - loss: 398.8331 - MinusLogProbMetric: 398.8331 - val_loss: 401.0157 - val_MinusLogProbMetric: 401.0157 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 201/1000
2023-09-10 11:00:49.401 
Epoch 201/1000 
	 loss: 397.0774, MinusLogProbMetric: 397.0774, val_loss: 421.3089, val_MinusLogProbMetric: 421.3089

Epoch 201: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.0774 - MinusLogProbMetric: 397.0774 - val_loss: 421.3089 - val_MinusLogProbMetric: 421.3089 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 202/1000
2023-09-10 11:01:06.876 
Epoch 202/1000 
	 loss: 398.0954, MinusLogProbMetric: 398.0954, val_loss: 401.6582, val_MinusLogProbMetric: 401.6582

Epoch 202: val_loss did not improve from 399.42020
196/196 - 17s - loss: 398.0954 - MinusLogProbMetric: 398.0954 - val_loss: 401.6582 - val_MinusLogProbMetric: 401.6582 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 203/1000
2023-09-10 11:01:24.339 
Epoch 203/1000 
	 loss: 397.1047, MinusLogProbMetric: 397.1047, val_loss: 399.9764, val_MinusLogProbMetric: 399.9764

Epoch 203: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.1047 - MinusLogProbMetric: 397.1047 - val_loss: 399.9764 - val_MinusLogProbMetric: 399.9764 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 204/1000
2023-09-10 11:01:41.847 
Epoch 204/1000 
	 loss: 396.9846, MinusLogProbMetric: 396.9846, val_loss: 402.7616, val_MinusLogProbMetric: 402.7616

Epoch 204: val_loss did not improve from 399.42020
196/196 - 18s - loss: 396.9846 - MinusLogProbMetric: 396.9846 - val_loss: 402.7616 - val_MinusLogProbMetric: 402.7616 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 205/1000
2023-09-10 11:01:58.717 
Epoch 205/1000 
	 loss: 397.5335, MinusLogProbMetric: 397.5335, val_loss: 400.3311, val_MinusLogProbMetric: 400.3311

Epoch 205: val_loss did not improve from 399.42020
196/196 - 17s - loss: 397.5335 - MinusLogProbMetric: 397.5335 - val_loss: 400.3311 - val_MinusLogProbMetric: 400.3311 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 206/1000
2023-09-10 11:02:17.174 
Epoch 206/1000 
	 loss: 396.9713, MinusLogProbMetric: 396.9713, val_loss: 400.6049, val_MinusLogProbMetric: 400.6049

Epoch 206: val_loss did not improve from 399.42020
196/196 - 18s - loss: 396.9713 - MinusLogProbMetric: 396.9713 - val_loss: 400.6049 - val_MinusLogProbMetric: 400.6049 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 207/1000
2023-09-10 11:02:34.870 
Epoch 207/1000 
	 loss: 398.0909, MinusLogProbMetric: 398.0909, val_loss: 400.0230, val_MinusLogProbMetric: 400.0230

Epoch 207: val_loss did not improve from 399.42020
196/196 - 18s - loss: 398.0909 - MinusLogProbMetric: 398.0909 - val_loss: 400.0230 - val_MinusLogProbMetric: 400.0230 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 208/1000
2023-09-10 11:02:52.388 
Epoch 208/1000 
	 loss: 396.5037, MinusLogProbMetric: 396.5037, val_loss: 400.9945, val_MinusLogProbMetric: 400.9945

Epoch 208: val_loss did not improve from 399.42020
196/196 - 18s - loss: 396.5037 - MinusLogProbMetric: 396.5037 - val_loss: 400.9945 - val_MinusLogProbMetric: 400.9945 - lr: 3.3333e-04 - 18s/epoch - 89ms/step
Epoch 209/1000
2023-09-10 11:03:09.961 
Epoch 209/1000 
	 loss: 397.1174, MinusLogProbMetric: 397.1174, val_loss: 400.5084, val_MinusLogProbMetric: 400.5084

Epoch 209: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.1174 - MinusLogProbMetric: 397.1174 - val_loss: 400.5084 - val_MinusLogProbMetric: 400.5084 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 210/1000
2023-09-10 11:03:27.242 
Epoch 210/1000 
	 loss: 396.8364, MinusLogProbMetric: 396.8364, val_loss: 403.3467, val_MinusLogProbMetric: 403.3467

Epoch 210: val_loss did not improve from 399.42020
196/196 - 17s - loss: 396.8364 - MinusLogProbMetric: 396.8364 - val_loss: 403.3467 - val_MinusLogProbMetric: 403.3467 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 211/1000
2023-09-10 11:03:44.579 
Epoch 211/1000 
	 loss: 396.7335, MinusLogProbMetric: 396.7335, val_loss: 400.4240, val_MinusLogProbMetric: 400.4240

Epoch 211: val_loss did not improve from 399.42020
196/196 - 17s - loss: 396.7335 - MinusLogProbMetric: 396.7335 - val_loss: 400.4240 - val_MinusLogProbMetric: 400.4240 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 212/1000
2023-09-10 11:04:02.516 
Epoch 212/1000 
	 loss: 396.8076, MinusLogProbMetric: 396.8076, val_loss: 401.5820, val_MinusLogProbMetric: 401.5820

Epoch 212: val_loss did not improve from 399.42020
196/196 - 18s - loss: 396.8076 - MinusLogProbMetric: 396.8076 - val_loss: 401.5820 - val_MinusLogProbMetric: 401.5820 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 213/1000
2023-09-10 11:04:19.913 
Epoch 213/1000 
	 loss: 396.9003, MinusLogProbMetric: 396.9003, val_loss: 400.6151, val_MinusLogProbMetric: 400.6151

Epoch 213: val_loss did not improve from 399.42020
196/196 - 17s - loss: 396.9003 - MinusLogProbMetric: 396.9003 - val_loss: 400.6151 - val_MinusLogProbMetric: 400.6151 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 214/1000
2023-09-10 11:04:37.688 
Epoch 214/1000 
	 loss: 396.9078, MinusLogProbMetric: 396.9078, val_loss: 401.3143, val_MinusLogProbMetric: 401.3143

Epoch 214: val_loss did not improve from 399.42020
196/196 - 18s - loss: 396.9078 - MinusLogProbMetric: 396.9078 - val_loss: 401.3143 - val_MinusLogProbMetric: 401.3143 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 215/1000
2023-09-10 11:04:55.516 
Epoch 215/1000 
	 loss: 397.3713, MinusLogProbMetric: 397.3713, val_loss: 402.6309, val_MinusLogProbMetric: 402.6309

Epoch 215: val_loss did not improve from 399.42020
196/196 - 18s - loss: 397.3713 - MinusLogProbMetric: 397.3713 - val_loss: 402.6309 - val_MinusLogProbMetric: 402.6309 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 216/1000
2023-09-10 11:05:13.773 
Epoch 216/1000 
	 loss: 396.6058, MinusLogProbMetric: 396.6058, val_loss: 400.7109, val_MinusLogProbMetric: 400.7109

Epoch 216: val_loss did not improve from 399.42020
196/196 - 18s - loss: 396.6058 - MinusLogProbMetric: 396.6058 - val_loss: 400.7109 - val_MinusLogProbMetric: 400.7109 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 217/1000
2023-09-10 11:05:31.200 
Epoch 217/1000 
	 loss: 391.6287, MinusLogProbMetric: 391.6287, val_loss: 396.4289, val_MinusLogProbMetric: 396.4289

Epoch 217: val_loss improved from 399.42020 to 396.42892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 391.6287 - MinusLogProbMetric: 391.6287 - val_loss: 396.4289 - val_MinusLogProbMetric: 396.4289 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 218/1000
2023-09-10 11:05:49.101 
Epoch 218/1000 
	 loss: 391.7289, MinusLogProbMetric: 391.7289, val_loss: 396.4266, val_MinusLogProbMetric: 396.4266

Epoch 218: val_loss improved from 396.42892 to 396.42664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 391.7289 - MinusLogProbMetric: 391.7289 - val_loss: 396.4266 - val_MinusLogProbMetric: 396.4266 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 219/1000
2023-09-10 11:06:07.485 
Epoch 219/1000 
	 loss: 391.7849, MinusLogProbMetric: 391.7849, val_loss: 396.8486, val_MinusLogProbMetric: 396.8486

Epoch 219: val_loss did not improve from 396.42664
196/196 - 18s - loss: 391.7849 - MinusLogProbMetric: 391.7849 - val_loss: 396.8486 - val_MinusLogProbMetric: 396.8486 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 220/1000
2023-09-10 11:06:25.147 
Epoch 220/1000 
	 loss: 392.0733, MinusLogProbMetric: 392.0733, val_loss: 396.7685, val_MinusLogProbMetric: 396.7685

Epoch 220: val_loss did not improve from 396.42664
196/196 - 18s - loss: 392.0733 - MinusLogProbMetric: 392.0733 - val_loss: 396.7685 - val_MinusLogProbMetric: 396.7685 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 221/1000
2023-09-10 11:06:43.226 
Epoch 221/1000 
	 loss: 392.3964, MinusLogProbMetric: 392.3964, val_loss: 396.0327, val_MinusLogProbMetric: 396.0327

Epoch 221: val_loss improved from 396.42664 to 396.03265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 392.3964 - MinusLogProbMetric: 392.3964 - val_loss: 396.0327 - val_MinusLogProbMetric: 396.0327 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 222/1000
2023-09-10 11:07:01.845 
Epoch 222/1000 
	 loss: 391.9437, MinusLogProbMetric: 391.9437, val_loss: 396.7461, val_MinusLogProbMetric: 396.7461

Epoch 222: val_loss did not improve from 396.03265
196/196 - 18s - loss: 391.9437 - MinusLogProbMetric: 391.9437 - val_loss: 396.7461 - val_MinusLogProbMetric: 396.7461 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 223/1000
2023-09-10 11:07:18.314 
Epoch 223/1000 
	 loss: 391.9725, MinusLogProbMetric: 391.9725, val_loss: 395.9772, val_MinusLogProbMetric: 395.9772

Epoch 223: val_loss improved from 396.03265 to 395.97723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 17s - loss: 391.9725 - MinusLogProbMetric: 391.9725 - val_loss: 395.9772 - val_MinusLogProbMetric: 395.9772 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 224/1000
2023-09-10 11:07:37.132 
Epoch 224/1000 
	 loss: 391.9611, MinusLogProbMetric: 391.9611, val_loss: 395.5334, val_MinusLogProbMetric: 395.5334

Epoch 224: val_loss improved from 395.97723 to 395.53339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 391.9611 - MinusLogProbMetric: 391.9611 - val_loss: 395.5334 - val_MinusLogProbMetric: 395.5334 - lr: 1.6667e-04 - 19s/epoch - 99ms/step
Epoch 225/1000
2023-09-10 11:07:55.114 
Epoch 225/1000 
	 loss: 391.8763, MinusLogProbMetric: 391.8763, val_loss: 395.9983, val_MinusLogProbMetric: 395.9983

Epoch 225: val_loss did not improve from 395.53339
196/196 - 17s - loss: 391.8763 - MinusLogProbMetric: 391.8763 - val_loss: 395.9983 - val_MinusLogProbMetric: 395.9983 - lr: 1.6667e-04 - 17s/epoch - 86ms/step
Epoch 226/1000
2023-09-10 11:08:13.074 
Epoch 226/1000 
	 loss: 392.0896, MinusLogProbMetric: 392.0896, val_loss: 396.0449, val_MinusLogProbMetric: 396.0449

Epoch 226: val_loss did not improve from 395.53339
196/196 - 18s - loss: 392.0896 - MinusLogProbMetric: 392.0896 - val_loss: 396.0449 - val_MinusLogProbMetric: 396.0449 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 227/1000
2023-09-10 11:08:31.184 
Epoch 227/1000 
	 loss: 392.1626, MinusLogProbMetric: 392.1626, val_loss: 395.6008, val_MinusLogProbMetric: 395.6008

Epoch 227: val_loss did not improve from 395.53339
196/196 - 18s - loss: 392.1626 - MinusLogProbMetric: 392.1626 - val_loss: 395.6008 - val_MinusLogProbMetric: 395.6008 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 228/1000
2023-09-10 11:08:48.719 
Epoch 228/1000 
	 loss: 391.7198, MinusLogProbMetric: 391.7198, val_loss: 397.4802, val_MinusLogProbMetric: 397.4802

Epoch 228: val_loss did not improve from 395.53339
196/196 - 18s - loss: 391.7198 - MinusLogProbMetric: 391.7198 - val_loss: 397.4802 - val_MinusLogProbMetric: 397.4802 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 229/1000
2023-09-10 11:09:06.174 
Epoch 229/1000 
	 loss: 391.7356, MinusLogProbMetric: 391.7356, val_loss: 399.0008, val_MinusLogProbMetric: 399.0008

Epoch 229: val_loss did not improve from 395.53339
196/196 - 17s - loss: 391.7356 - MinusLogProbMetric: 391.7356 - val_loss: 399.0008 - val_MinusLogProbMetric: 399.0008 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 230/1000
2023-09-10 11:09:24.336 
Epoch 230/1000 
	 loss: 392.0773, MinusLogProbMetric: 392.0773, val_loss: 396.4729, val_MinusLogProbMetric: 396.4729

Epoch 230: val_loss did not improve from 395.53339
196/196 - 18s - loss: 392.0773 - MinusLogProbMetric: 392.0773 - val_loss: 396.4729 - val_MinusLogProbMetric: 396.4729 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 231/1000
2023-09-10 11:09:42.335 
Epoch 231/1000 
	 loss: 391.6593, MinusLogProbMetric: 391.6593, val_loss: 395.6758, val_MinusLogProbMetric: 395.6758

Epoch 231: val_loss did not improve from 395.53339
196/196 - 18s - loss: 391.6593 - MinusLogProbMetric: 391.6593 - val_loss: 395.6758 - val_MinusLogProbMetric: 395.6758 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 232/1000
2023-09-10 11:09:59.860 
Epoch 232/1000 
	 loss: 391.7572, MinusLogProbMetric: 391.7572, val_loss: 395.5799, val_MinusLogProbMetric: 395.5799

Epoch 232: val_loss did not improve from 395.53339
196/196 - 18s - loss: 391.7572 - MinusLogProbMetric: 391.7572 - val_loss: 395.5799 - val_MinusLogProbMetric: 395.5799 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 233/1000
2023-09-10 11:10:17.797 
Epoch 233/1000 
	 loss: 392.2805, MinusLogProbMetric: 392.2805, val_loss: 395.9749, val_MinusLogProbMetric: 395.9749

Epoch 233: val_loss did not improve from 395.53339
196/196 - 18s - loss: 392.2805 - MinusLogProbMetric: 392.2805 - val_loss: 395.9749 - val_MinusLogProbMetric: 395.9749 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 234/1000
2023-09-10 11:10:35.276 
Epoch 234/1000 
	 loss: 391.8583, MinusLogProbMetric: 391.8583, val_loss: 395.8374, val_MinusLogProbMetric: 395.8374

Epoch 234: val_loss did not improve from 395.53339
196/196 - 17s - loss: 391.8583 - MinusLogProbMetric: 391.8583 - val_loss: 395.8374 - val_MinusLogProbMetric: 395.8374 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 235/1000
2023-09-10 11:10:53.138 
Epoch 235/1000 
	 loss: 391.7787, MinusLogProbMetric: 391.7787, val_loss: 395.8879, val_MinusLogProbMetric: 395.8879

Epoch 235: val_loss did not improve from 395.53339
196/196 - 18s - loss: 391.7787 - MinusLogProbMetric: 391.7787 - val_loss: 395.8879 - val_MinusLogProbMetric: 395.8879 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 236/1000
2023-09-10 11:11:10.873 
Epoch 236/1000 
	 loss: 391.8477, MinusLogProbMetric: 391.8477, val_loss: 395.3798, val_MinusLogProbMetric: 395.3798

Epoch 236: val_loss improved from 395.53339 to 395.37976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 391.8477 - MinusLogProbMetric: 391.8477 - val_loss: 395.3798 - val_MinusLogProbMetric: 395.3798 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 237/1000
2023-09-10 11:11:30.970 
Epoch 237/1000 
	 loss: 391.5869, MinusLogProbMetric: 391.5869, val_loss: 409.3282, val_MinusLogProbMetric: 409.3282

Epoch 237: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.5869 - MinusLogProbMetric: 391.5869 - val_loss: 409.3282 - val_MinusLogProbMetric: 409.3282 - lr: 1.6667e-04 - 19s/epoch - 98ms/step
Epoch 238/1000
2023-09-10 11:11:50.006 
Epoch 238/1000 
	 loss: 391.8782, MinusLogProbMetric: 391.8782, val_loss: 397.5671, val_MinusLogProbMetric: 397.5671

Epoch 238: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.8782 - MinusLogProbMetric: 391.8782 - val_loss: 397.5671 - val_MinusLogProbMetric: 397.5671 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 239/1000
2023-09-10 11:12:07.234 
Epoch 239/1000 
	 loss: 391.8200, MinusLogProbMetric: 391.8200, val_loss: 395.9218, val_MinusLogProbMetric: 395.9218

Epoch 239: val_loss did not improve from 395.37976
196/196 - 17s - loss: 391.8200 - MinusLogProbMetric: 391.8200 - val_loss: 395.9218 - val_MinusLogProbMetric: 395.9218 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 240/1000
2023-09-10 11:12:24.852 
Epoch 240/1000 
	 loss: 391.6395, MinusLogProbMetric: 391.6395, val_loss: 396.3711, val_MinusLogProbMetric: 396.3711

Epoch 240: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.6395 - MinusLogProbMetric: 391.6395 - val_loss: 396.3711 - val_MinusLogProbMetric: 396.3711 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 241/1000
2023-09-10 11:12:42.714 
Epoch 241/1000 
	 loss: 391.7879, MinusLogProbMetric: 391.7879, val_loss: 395.7150, val_MinusLogProbMetric: 395.7150

Epoch 241: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.7879 - MinusLogProbMetric: 391.7879 - val_loss: 395.7150 - val_MinusLogProbMetric: 395.7150 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 242/1000
2023-09-10 11:13:00.054 
Epoch 242/1000 
	 loss: 391.9212, MinusLogProbMetric: 391.9212, val_loss: 396.4250, val_MinusLogProbMetric: 396.4250

Epoch 242: val_loss did not improve from 395.37976
196/196 - 17s - loss: 391.9212 - MinusLogProbMetric: 391.9212 - val_loss: 396.4250 - val_MinusLogProbMetric: 396.4250 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 243/1000
2023-09-10 11:13:18.124 
Epoch 243/1000 
	 loss: 391.8319, MinusLogProbMetric: 391.8319, val_loss: 395.4635, val_MinusLogProbMetric: 395.4635

Epoch 243: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.8319 - MinusLogProbMetric: 391.8319 - val_loss: 395.4635 - val_MinusLogProbMetric: 395.4635 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 244/1000
2023-09-10 11:13:36.730 
Epoch 244/1000 
	 loss: 391.7518, MinusLogProbMetric: 391.7518, val_loss: 396.5843, val_MinusLogProbMetric: 396.5843

Epoch 244: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.7518 - MinusLogProbMetric: 391.7518 - val_loss: 396.5843 - val_MinusLogProbMetric: 396.5843 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 245/1000
2023-09-10 11:13:54.949 
Epoch 245/1000 
	 loss: 391.3826, MinusLogProbMetric: 391.3826, val_loss: 397.2996, val_MinusLogProbMetric: 397.2996

Epoch 245: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.3826 - MinusLogProbMetric: 391.3826 - val_loss: 397.2996 - val_MinusLogProbMetric: 397.2996 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 246/1000
2023-09-10 11:14:13.118 
Epoch 246/1000 
	 loss: 391.8420, MinusLogProbMetric: 391.8420, val_loss: 396.1718, val_MinusLogProbMetric: 396.1718

Epoch 246: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.8420 - MinusLogProbMetric: 391.8420 - val_loss: 396.1718 - val_MinusLogProbMetric: 396.1718 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 247/1000
2023-09-10 11:14:30.408 
Epoch 247/1000 
	 loss: 391.5645, MinusLogProbMetric: 391.5645, val_loss: 402.6144, val_MinusLogProbMetric: 402.6144

Epoch 247: val_loss did not improve from 395.37976
196/196 - 17s - loss: 391.5645 - MinusLogProbMetric: 391.5645 - val_loss: 402.6144 - val_MinusLogProbMetric: 402.6144 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 248/1000
2023-09-10 11:14:47.449 
Epoch 248/1000 
	 loss: 391.6765, MinusLogProbMetric: 391.6765, val_loss: 397.7252, val_MinusLogProbMetric: 397.7252

Epoch 248: val_loss did not improve from 395.37976
196/196 - 17s - loss: 391.6765 - MinusLogProbMetric: 391.6765 - val_loss: 397.7252 - val_MinusLogProbMetric: 397.7252 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 249/1000
2023-09-10 11:15:05.592 
Epoch 249/1000 
	 loss: 391.5154, MinusLogProbMetric: 391.5154, val_loss: 396.2081, val_MinusLogProbMetric: 396.2081

Epoch 249: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.5154 - MinusLogProbMetric: 391.5154 - val_loss: 396.2081 - val_MinusLogProbMetric: 396.2081 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 250/1000
2023-09-10 11:15:24.964 
Epoch 250/1000 
	 loss: 391.5174, MinusLogProbMetric: 391.5174, val_loss: 396.6288, val_MinusLogProbMetric: 396.6288

Epoch 250: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.5174 - MinusLogProbMetric: 391.5174 - val_loss: 396.6288 - val_MinusLogProbMetric: 396.6288 - lr: 1.6667e-04 - 19s/epoch - 99ms/step
Epoch 251/1000
2023-09-10 11:15:43.694 
Epoch 251/1000 
	 loss: 391.6721, MinusLogProbMetric: 391.6721, val_loss: 397.2717, val_MinusLogProbMetric: 397.2717

Epoch 251: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.6721 - MinusLogProbMetric: 391.6721 - val_loss: 397.2717 - val_MinusLogProbMetric: 397.2717 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 252/1000
2023-09-10 11:16:02.204 
Epoch 252/1000 
	 loss: 391.6303, MinusLogProbMetric: 391.6303, val_loss: 396.9012, val_MinusLogProbMetric: 396.9012

Epoch 252: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.6303 - MinusLogProbMetric: 391.6303 - val_loss: 396.9012 - val_MinusLogProbMetric: 396.9012 - lr: 1.6667e-04 - 19s/epoch - 94ms/step
Epoch 253/1000
2023-09-10 11:16:19.900 
Epoch 253/1000 
	 loss: 391.8150, MinusLogProbMetric: 391.8150, val_loss: 397.0764, val_MinusLogProbMetric: 397.0764

Epoch 253: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.8150 - MinusLogProbMetric: 391.8150 - val_loss: 397.0764 - val_MinusLogProbMetric: 397.0764 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 254/1000
2023-09-10 11:16:39.026 
Epoch 254/1000 
	 loss: 391.4338, MinusLogProbMetric: 391.4338, val_loss: 396.2143, val_MinusLogProbMetric: 396.2143

Epoch 254: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.4338 - MinusLogProbMetric: 391.4338 - val_loss: 396.2143 - val_MinusLogProbMetric: 396.2143 - lr: 1.6667e-04 - 19s/epoch - 98ms/step
Epoch 255/1000
2023-09-10 11:16:57.250 
Epoch 255/1000 
	 loss: 391.5136, MinusLogProbMetric: 391.5136, val_loss: 397.0223, val_MinusLogProbMetric: 397.0223

Epoch 255: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.5136 - MinusLogProbMetric: 391.5136 - val_loss: 397.0223 - val_MinusLogProbMetric: 397.0223 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 256/1000
2023-09-10 11:17:16.251 
Epoch 256/1000 
	 loss: 391.5292, MinusLogProbMetric: 391.5292, val_loss: 395.5399, val_MinusLogProbMetric: 395.5399

Epoch 256: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.5292 - MinusLogProbMetric: 391.5292 - val_loss: 395.5399 - val_MinusLogProbMetric: 395.5399 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 257/1000
2023-09-10 11:17:34.549 
Epoch 257/1000 
	 loss: 391.7226, MinusLogProbMetric: 391.7226, val_loss: 395.8571, val_MinusLogProbMetric: 395.8571

Epoch 257: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.7226 - MinusLogProbMetric: 391.7226 - val_loss: 395.8571 - val_MinusLogProbMetric: 395.8571 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 258/1000
2023-09-10 11:17:53.132 
Epoch 258/1000 
	 loss: 391.9370, MinusLogProbMetric: 391.9370, val_loss: 396.2017, val_MinusLogProbMetric: 396.2017

Epoch 258: val_loss did not improve from 395.37976
196/196 - 19s - loss: 391.9370 - MinusLogProbMetric: 391.9370 - val_loss: 396.2017 - val_MinusLogProbMetric: 396.2017 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 259/1000
2023-09-10 11:18:10.332 
Epoch 259/1000 
	 loss: 391.3759, MinusLogProbMetric: 391.3759, val_loss: 396.1087, val_MinusLogProbMetric: 396.1087

Epoch 259: val_loss did not improve from 395.37976
196/196 - 17s - loss: 391.3759 - MinusLogProbMetric: 391.3759 - val_loss: 396.1087 - val_MinusLogProbMetric: 396.1087 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 260/1000
2023-09-10 11:18:27.803 
Epoch 260/1000 
	 loss: 391.4528, MinusLogProbMetric: 391.4528, val_loss: 395.9769, val_MinusLogProbMetric: 395.9769

Epoch 260: val_loss did not improve from 395.37976
196/196 - 17s - loss: 391.4528 - MinusLogProbMetric: 391.4528 - val_loss: 395.9769 - val_MinusLogProbMetric: 395.9769 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 261/1000
2023-09-10 11:18:45.351 
Epoch 261/1000 
	 loss: 391.6000, MinusLogProbMetric: 391.6000, val_loss: 395.8145, val_MinusLogProbMetric: 395.8145

Epoch 261: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.6000 - MinusLogProbMetric: 391.6000 - val_loss: 395.8145 - val_MinusLogProbMetric: 395.8145 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 262/1000
2023-09-10 11:19:02.957 
Epoch 262/1000 
	 loss: 391.7079, MinusLogProbMetric: 391.7079, val_loss: 396.0574, val_MinusLogProbMetric: 396.0574

Epoch 262: val_loss did not improve from 395.37976
196/196 - 18s - loss: 391.7079 - MinusLogProbMetric: 391.7079 - val_loss: 396.0574 - val_MinusLogProbMetric: 396.0574 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 263/1000
2023-09-10 11:19:19.738 
Epoch 263/1000 
	 loss: 392.0458, MinusLogProbMetric: 392.0458, val_loss: 396.0932, val_MinusLogProbMetric: 396.0932

Epoch 263: val_loss did not improve from 395.37976
196/196 - 17s - loss: 392.0458 - MinusLogProbMetric: 392.0458 - val_loss: 396.0932 - val_MinusLogProbMetric: 396.0932 - lr: 1.6667e-04 - 17s/epoch - 86ms/step
Epoch 264/1000
2023-09-10 11:19:36.636 
Epoch 264/1000 
	 loss: 391.4150, MinusLogProbMetric: 391.4150, val_loss: 394.8655, val_MinusLogProbMetric: 394.8655

Epoch 264: val_loss improved from 395.37976 to 394.86548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 391.4150 - MinusLogProbMetric: 391.4150 - val_loss: 394.8655 - val_MinusLogProbMetric: 394.8655 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 265/1000
2023-09-10 11:19:54.921 
Epoch 265/1000 
	 loss: 391.3679, MinusLogProbMetric: 391.3679, val_loss: 396.5953, val_MinusLogProbMetric: 396.5953

Epoch 265: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.3679 - MinusLogProbMetric: 391.3679 - val_loss: 396.5953 - val_MinusLogProbMetric: 396.5953 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 266/1000
2023-09-10 11:20:12.896 
Epoch 266/1000 
	 loss: 391.9312, MinusLogProbMetric: 391.9312, val_loss: 396.4984, val_MinusLogProbMetric: 396.4984

Epoch 266: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.9312 - MinusLogProbMetric: 391.9312 - val_loss: 396.4984 - val_MinusLogProbMetric: 396.4984 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 267/1000
2023-09-10 11:20:30.567 
Epoch 267/1000 
	 loss: 391.4821, MinusLogProbMetric: 391.4821, val_loss: 395.5815, val_MinusLogProbMetric: 395.5815

Epoch 267: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.4821 - MinusLogProbMetric: 391.4821 - val_loss: 395.5815 - val_MinusLogProbMetric: 395.5815 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 268/1000
2023-09-10 11:20:48.774 
Epoch 268/1000 
	 loss: 391.0875, MinusLogProbMetric: 391.0875, val_loss: 396.2846, val_MinusLogProbMetric: 396.2846

Epoch 268: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.0875 - MinusLogProbMetric: 391.0875 - val_loss: 396.2846 - val_MinusLogProbMetric: 396.2846 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 269/1000
2023-09-10 11:21:06.527 
Epoch 269/1000 
	 loss: 391.5264, MinusLogProbMetric: 391.5264, val_loss: 395.2104, val_MinusLogProbMetric: 395.2104

Epoch 269: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.5264 - MinusLogProbMetric: 391.5264 - val_loss: 395.2104 - val_MinusLogProbMetric: 395.2104 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 270/1000
2023-09-10 11:21:24.476 
Epoch 270/1000 
	 loss: 391.2876, MinusLogProbMetric: 391.2876, val_loss: 395.2083, val_MinusLogProbMetric: 395.2083

Epoch 270: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.2876 - MinusLogProbMetric: 391.2876 - val_loss: 395.2083 - val_MinusLogProbMetric: 395.2083 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 271/1000
2023-09-10 11:21:42.340 
Epoch 271/1000 
	 loss: 391.7037, MinusLogProbMetric: 391.7037, val_loss: 396.0271, val_MinusLogProbMetric: 396.0271

Epoch 271: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.7037 - MinusLogProbMetric: 391.7037 - val_loss: 396.0271 - val_MinusLogProbMetric: 396.0271 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 272/1000
2023-09-10 11:22:00.178 
Epoch 272/1000 
	 loss: 391.5300, MinusLogProbMetric: 391.5300, val_loss: 396.0694, val_MinusLogProbMetric: 396.0694

Epoch 272: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.5300 - MinusLogProbMetric: 391.5300 - val_loss: 396.0694 - val_MinusLogProbMetric: 396.0694 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 273/1000
2023-09-10 11:22:18.083 
Epoch 273/1000 
	 loss: 391.0381, MinusLogProbMetric: 391.0381, val_loss: 396.2184, val_MinusLogProbMetric: 396.2184

Epoch 273: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.0381 - MinusLogProbMetric: 391.0381 - val_loss: 396.2184 - val_MinusLogProbMetric: 396.2184 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 274/1000
2023-09-10 11:22:35.723 
Epoch 274/1000 
	 loss: 391.5358, MinusLogProbMetric: 391.5358, val_loss: 395.5090, val_MinusLogProbMetric: 395.5090

Epoch 274: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.5358 - MinusLogProbMetric: 391.5358 - val_loss: 395.5090 - val_MinusLogProbMetric: 395.5090 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 275/1000
2023-09-10 11:22:53.568 
Epoch 275/1000 
	 loss: 391.3189, MinusLogProbMetric: 391.3189, val_loss: 397.1340, val_MinusLogProbMetric: 397.1340

Epoch 275: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.3189 - MinusLogProbMetric: 391.3189 - val_loss: 397.1340 - val_MinusLogProbMetric: 397.1340 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 276/1000
2023-09-10 11:23:10.388 
Epoch 276/1000 
	 loss: 391.3066, MinusLogProbMetric: 391.3066, val_loss: 395.4703, val_MinusLogProbMetric: 395.4703

Epoch 276: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.3066 - MinusLogProbMetric: 391.3066 - val_loss: 395.4703 - val_MinusLogProbMetric: 395.4703 - lr: 1.6667e-04 - 17s/epoch - 86ms/step
Epoch 277/1000
2023-09-10 11:23:27.658 
Epoch 277/1000 
	 loss: 391.2983, MinusLogProbMetric: 391.2983, val_loss: 396.2724, val_MinusLogProbMetric: 396.2724

Epoch 277: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.2983 - MinusLogProbMetric: 391.2983 - val_loss: 396.2724 - val_MinusLogProbMetric: 396.2724 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 278/1000
2023-09-10 11:23:46.974 
Epoch 278/1000 
	 loss: 391.5179, MinusLogProbMetric: 391.5179, val_loss: 395.5128, val_MinusLogProbMetric: 395.5128

Epoch 278: val_loss did not improve from 394.86548
196/196 - 19s - loss: 391.5179 - MinusLogProbMetric: 391.5179 - val_loss: 395.5128 - val_MinusLogProbMetric: 395.5128 - lr: 1.6667e-04 - 19s/epoch - 98ms/step
Epoch 279/1000
2023-09-10 11:24:05.590 
Epoch 279/1000 
	 loss: 391.8102, MinusLogProbMetric: 391.8102, val_loss: 395.7738, val_MinusLogProbMetric: 395.7738

Epoch 279: val_loss did not improve from 394.86548
196/196 - 19s - loss: 391.8102 - MinusLogProbMetric: 391.8102 - val_loss: 395.7738 - val_MinusLogProbMetric: 395.7738 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 280/1000
2023-09-10 11:24:23.843 
Epoch 280/1000 
	 loss: 391.3663, MinusLogProbMetric: 391.3663, val_loss: 395.3850, val_MinusLogProbMetric: 395.3850

Epoch 280: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.3663 - MinusLogProbMetric: 391.3663 - val_loss: 395.3850 - val_MinusLogProbMetric: 395.3850 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 281/1000
2023-09-10 11:24:40.875 
Epoch 281/1000 
	 loss: 391.3710, MinusLogProbMetric: 391.3710, val_loss: 395.2534, val_MinusLogProbMetric: 395.2534

Epoch 281: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.3710 - MinusLogProbMetric: 391.3710 - val_loss: 395.2534 - val_MinusLogProbMetric: 395.2534 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 282/1000
2023-09-10 11:24:58.507 
Epoch 282/1000 
	 loss: 391.2711, MinusLogProbMetric: 391.2711, val_loss: 395.5356, val_MinusLogProbMetric: 395.5356

Epoch 282: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.2711 - MinusLogProbMetric: 391.2711 - val_loss: 395.5356 - val_MinusLogProbMetric: 395.5356 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 283/1000
2023-09-10 11:25:16.539 
Epoch 283/1000 
	 loss: 391.1300, MinusLogProbMetric: 391.1300, val_loss: 395.0711, val_MinusLogProbMetric: 395.0711

Epoch 283: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.1300 - MinusLogProbMetric: 391.1300 - val_loss: 395.0711 - val_MinusLogProbMetric: 395.0711 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 284/1000
2023-09-10 11:25:33.931 
Epoch 284/1000 
	 loss: 391.5069, MinusLogProbMetric: 391.5069, val_loss: 395.5372, val_MinusLogProbMetric: 395.5372

Epoch 284: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.5069 - MinusLogProbMetric: 391.5069 - val_loss: 395.5372 - val_MinusLogProbMetric: 395.5372 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 285/1000
2023-09-10 11:25:50.205 
Epoch 285/1000 
	 loss: 391.7361, MinusLogProbMetric: 391.7361, val_loss: 396.2147, val_MinusLogProbMetric: 396.2147

Epoch 285: val_loss did not improve from 394.86548
196/196 - 16s - loss: 391.7361 - MinusLogProbMetric: 391.7361 - val_loss: 396.2147 - val_MinusLogProbMetric: 396.2147 - lr: 1.6667e-04 - 16s/epoch - 83ms/step
Epoch 286/1000
2023-09-10 11:26:07.635 
Epoch 286/1000 
	 loss: 391.1763, MinusLogProbMetric: 391.1763, val_loss: 395.8404, val_MinusLogProbMetric: 395.8404

Epoch 286: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.1763 - MinusLogProbMetric: 391.1763 - val_loss: 395.8404 - val_MinusLogProbMetric: 395.8404 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 287/1000
2023-09-10 11:26:25.772 
Epoch 287/1000 
	 loss: 391.1518, MinusLogProbMetric: 391.1518, val_loss: 396.1268, val_MinusLogProbMetric: 396.1268

Epoch 287: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.1518 - MinusLogProbMetric: 391.1518 - val_loss: 396.1268 - val_MinusLogProbMetric: 396.1268 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 288/1000
2023-09-10 11:26:42.836 
Epoch 288/1000 
	 loss: 391.2710, MinusLogProbMetric: 391.2710, val_loss: 395.4200, val_MinusLogProbMetric: 395.4200

Epoch 288: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.2710 - MinusLogProbMetric: 391.2710 - val_loss: 395.4200 - val_MinusLogProbMetric: 395.4200 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 289/1000
2023-09-10 11:26:59.506 
Epoch 289/1000 
	 loss: 391.2671, MinusLogProbMetric: 391.2671, val_loss: 396.2090, val_MinusLogProbMetric: 396.2090

Epoch 289: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.2671 - MinusLogProbMetric: 391.2671 - val_loss: 396.2090 - val_MinusLogProbMetric: 396.2090 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 290/1000
2023-09-10 11:27:16.095 
Epoch 290/1000 
	 loss: 391.0404, MinusLogProbMetric: 391.0404, val_loss: 395.4645, val_MinusLogProbMetric: 395.4645

Epoch 290: val_loss did not improve from 394.86548
196/196 - 17s - loss: 391.0404 - MinusLogProbMetric: 391.0404 - val_loss: 395.4645 - val_MinusLogProbMetric: 395.4645 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 291/1000
2023-09-10 11:27:33.726 
Epoch 291/1000 
	 loss: 391.6198, MinusLogProbMetric: 391.6198, val_loss: 397.0008, val_MinusLogProbMetric: 397.0008

Epoch 291: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.6198 - MinusLogProbMetric: 391.6198 - val_loss: 397.0008 - val_MinusLogProbMetric: 397.0008 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 292/1000
2023-09-10 11:27:51.322 
Epoch 292/1000 
	 loss: 391.3784, MinusLogProbMetric: 391.3784, val_loss: 395.7193, val_MinusLogProbMetric: 395.7193

Epoch 292: val_loss did not improve from 394.86548
196/196 - 18s - loss: 391.3784 - MinusLogProbMetric: 391.3784 - val_loss: 395.7193 - val_MinusLogProbMetric: 395.7193 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 293/1000
2023-09-10 11:28:08.687 
Epoch 293/1000 
	 loss: 391.6519, MinusLogProbMetric: 391.6519, val_loss: 394.6381, val_MinusLogProbMetric: 394.6381

Epoch 293: val_loss improved from 394.86548 to 394.63806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 391.6519 - MinusLogProbMetric: 391.6519 - val_loss: 394.6381 - val_MinusLogProbMetric: 394.6381 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 294/1000
2023-09-10 11:28:27.423 
Epoch 294/1000 
	 loss: 391.0416, MinusLogProbMetric: 391.0416, val_loss: 396.5256, val_MinusLogProbMetric: 396.5256

Epoch 294: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.0416 - MinusLogProbMetric: 391.0416 - val_loss: 396.5256 - val_MinusLogProbMetric: 396.5256 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 295/1000
2023-09-10 11:28:45.713 
Epoch 295/1000 
	 loss: 392.8873, MinusLogProbMetric: 392.8873, val_loss: 395.7948, val_MinusLogProbMetric: 395.7948

Epoch 295: val_loss did not improve from 394.63806
196/196 - 18s - loss: 392.8873 - MinusLogProbMetric: 392.8873 - val_loss: 395.7948 - val_MinusLogProbMetric: 395.7948 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 296/1000
2023-09-10 11:29:03.771 
Epoch 296/1000 
	 loss: 390.8871, MinusLogProbMetric: 390.8871, val_loss: 395.8132, val_MinusLogProbMetric: 395.8132

Epoch 296: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.8871 - MinusLogProbMetric: 390.8871 - val_loss: 395.8132 - val_MinusLogProbMetric: 395.8132 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 297/1000
2023-09-10 11:29:21.749 
Epoch 297/1000 
	 loss: 391.3031, MinusLogProbMetric: 391.3031, val_loss: 395.4203, val_MinusLogProbMetric: 395.4203

Epoch 297: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.3031 - MinusLogProbMetric: 391.3031 - val_loss: 395.4203 - val_MinusLogProbMetric: 395.4203 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 298/1000
2023-09-10 11:29:40.013 
Epoch 298/1000 
	 loss: 391.2607, MinusLogProbMetric: 391.2607, val_loss: 396.0259, val_MinusLogProbMetric: 396.0259

Epoch 298: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.2607 - MinusLogProbMetric: 391.2607 - val_loss: 396.0259 - val_MinusLogProbMetric: 396.0259 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 299/1000
2023-09-10 11:29:57.879 
Epoch 299/1000 
	 loss: 390.9954, MinusLogProbMetric: 390.9954, val_loss: 395.6593, val_MinusLogProbMetric: 395.6593

Epoch 299: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.9954 - MinusLogProbMetric: 390.9954 - val_loss: 395.6593 - val_MinusLogProbMetric: 395.6593 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 300/1000
2023-09-10 11:30:15.822 
Epoch 300/1000 
	 loss: 391.0259, MinusLogProbMetric: 391.0259, val_loss: 397.0888, val_MinusLogProbMetric: 397.0888

Epoch 300: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.0259 - MinusLogProbMetric: 391.0259 - val_loss: 397.0888 - val_MinusLogProbMetric: 397.0888 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 301/1000
2023-09-10 11:30:33.585 
Epoch 301/1000 
	 loss: 391.3392, MinusLogProbMetric: 391.3392, val_loss: 395.4298, val_MinusLogProbMetric: 395.4298

Epoch 301: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.3392 - MinusLogProbMetric: 391.3392 - val_loss: 395.4298 - val_MinusLogProbMetric: 395.4298 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 302/1000
2023-09-10 11:30:51.669 
Epoch 302/1000 
	 loss: 391.2484, MinusLogProbMetric: 391.2484, val_loss: 398.7434, val_MinusLogProbMetric: 398.7434

Epoch 302: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.2484 - MinusLogProbMetric: 391.2484 - val_loss: 398.7434 - val_MinusLogProbMetric: 398.7434 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 303/1000
2023-09-10 11:31:09.072 
Epoch 303/1000 
	 loss: 391.2032, MinusLogProbMetric: 391.2032, val_loss: 395.2712, val_MinusLogProbMetric: 395.2712

Epoch 303: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.2032 - MinusLogProbMetric: 391.2032 - val_loss: 395.2712 - val_MinusLogProbMetric: 395.2712 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 304/1000
2023-09-10 11:31:27.118 
Epoch 304/1000 
	 loss: 391.2556, MinusLogProbMetric: 391.2556, val_loss: 397.4659, val_MinusLogProbMetric: 397.4659

Epoch 304: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.2556 - MinusLogProbMetric: 391.2556 - val_loss: 397.4659 - val_MinusLogProbMetric: 397.4659 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 305/1000
2023-09-10 11:31:45.063 
Epoch 305/1000 
	 loss: 391.1055, MinusLogProbMetric: 391.1055, val_loss: 394.9196, val_MinusLogProbMetric: 394.9196

Epoch 305: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.1055 - MinusLogProbMetric: 391.1055 - val_loss: 394.9196 - val_MinusLogProbMetric: 394.9196 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 306/1000
2023-09-10 11:32:03.010 
Epoch 306/1000 
	 loss: 391.0739, MinusLogProbMetric: 391.0739, val_loss: 396.8159, val_MinusLogProbMetric: 396.8159

Epoch 306: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.0739 - MinusLogProbMetric: 391.0739 - val_loss: 396.8159 - val_MinusLogProbMetric: 396.8159 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 307/1000
2023-09-10 11:32:20.406 
Epoch 307/1000 
	 loss: 391.1422, MinusLogProbMetric: 391.1422, val_loss: 395.8099, val_MinusLogProbMetric: 395.8099

Epoch 307: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.1422 - MinusLogProbMetric: 391.1422 - val_loss: 395.8099 - val_MinusLogProbMetric: 395.8099 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 308/1000
2023-09-10 11:32:37.580 
Epoch 308/1000 
	 loss: 391.0580, MinusLogProbMetric: 391.0580, val_loss: 395.5025, val_MinusLogProbMetric: 395.5025

Epoch 308: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.0580 - MinusLogProbMetric: 391.0580 - val_loss: 395.5025 - val_MinusLogProbMetric: 395.5025 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 309/1000
2023-09-10 11:32:54.846 
Epoch 309/1000 
	 loss: 391.0316, MinusLogProbMetric: 391.0316, val_loss: 397.2926, val_MinusLogProbMetric: 397.2926

Epoch 309: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.0316 - MinusLogProbMetric: 391.0316 - val_loss: 397.2926 - val_MinusLogProbMetric: 397.2926 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 310/1000
2023-09-10 11:33:13.511 
Epoch 310/1000 
	 loss: 391.1526, MinusLogProbMetric: 391.1526, val_loss: 397.7996, val_MinusLogProbMetric: 397.7996

Epoch 310: val_loss did not improve from 394.63806
196/196 - 19s - loss: 391.1526 - MinusLogProbMetric: 391.1526 - val_loss: 397.7996 - val_MinusLogProbMetric: 397.7996 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 311/1000
2023-09-10 11:33:31.342 
Epoch 311/1000 
	 loss: 390.9179, MinusLogProbMetric: 390.9179, val_loss: 395.4793, val_MinusLogProbMetric: 395.4793

Epoch 311: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.9179 - MinusLogProbMetric: 390.9179 - val_loss: 395.4793 - val_MinusLogProbMetric: 395.4793 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 312/1000
2023-09-10 11:33:49.058 
Epoch 312/1000 
	 loss: 390.9385, MinusLogProbMetric: 390.9385, val_loss: 396.9870, val_MinusLogProbMetric: 396.9870

Epoch 312: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.9385 - MinusLogProbMetric: 390.9385 - val_loss: 396.9870 - val_MinusLogProbMetric: 396.9870 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 313/1000
2023-09-10 11:34:07.194 
Epoch 313/1000 
	 loss: 391.0613, MinusLogProbMetric: 391.0613, val_loss: 398.0134, val_MinusLogProbMetric: 398.0134

Epoch 313: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.0613 - MinusLogProbMetric: 391.0613 - val_loss: 398.0134 - val_MinusLogProbMetric: 398.0134 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 314/1000
2023-09-10 11:34:25.148 
Epoch 314/1000 
	 loss: 391.4973, MinusLogProbMetric: 391.4973, val_loss: 396.0188, val_MinusLogProbMetric: 396.0188

Epoch 314: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.4973 - MinusLogProbMetric: 391.4973 - val_loss: 396.0188 - val_MinusLogProbMetric: 396.0188 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 315/1000
2023-09-10 11:34:43.418 
Epoch 315/1000 
	 loss: 390.8920, MinusLogProbMetric: 390.8920, val_loss: 395.8885, val_MinusLogProbMetric: 395.8885

Epoch 315: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.8920 - MinusLogProbMetric: 390.8920 - val_loss: 395.8885 - val_MinusLogProbMetric: 395.8885 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 316/1000
2023-09-10 11:35:01.066 
Epoch 316/1000 
	 loss: 390.9532, MinusLogProbMetric: 390.9532, val_loss: 396.5417, val_MinusLogProbMetric: 396.5417

Epoch 316: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.9532 - MinusLogProbMetric: 390.9532 - val_loss: 396.5417 - val_MinusLogProbMetric: 396.5417 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 317/1000
2023-09-10 11:35:19.965 
Epoch 317/1000 
	 loss: 390.8777, MinusLogProbMetric: 390.8777, val_loss: 396.6314, val_MinusLogProbMetric: 396.6314

Epoch 317: val_loss did not improve from 394.63806
196/196 - 19s - loss: 390.8777 - MinusLogProbMetric: 390.8777 - val_loss: 396.6314 - val_MinusLogProbMetric: 396.6314 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 318/1000
2023-09-10 11:35:38.171 
Epoch 318/1000 
	 loss: 390.7696, MinusLogProbMetric: 390.7696, val_loss: 395.5835, val_MinusLogProbMetric: 395.5835

Epoch 318: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.7696 - MinusLogProbMetric: 390.7696 - val_loss: 395.5835 - val_MinusLogProbMetric: 395.5835 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 319/1000
2023-09-10 11:35:56.540 
Epoch 319/1000 
	 loss: 391.8192, MinusLogProbMetric: 391.8192, val_loss: 395.2035, val_MinusLogProbMetric: 395.2035

Epoch 319: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.8192 - MinusLogProbMetric: 391.8192 - val_loss: 395.2035 - val_MinusLogProbMetric: 395.2035 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 320/1000
2023-09-10 11:36:16.197 
Epoch 320/1000 
	 loss: 390.9319, MinusLogProbMetric: 390.9319, val_loss: 395.4950, val_MinusLogProbMetric: 395.4950

Epoch 320: val_loss did not improve from 394.63806
196/196 - 20s - loss: 390.9319 - MinusLogProbMetric: 390.9319 - val_loss: 395.4950 - val_MinusLogProbMetric: 395.4950 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 321/1000
2023-09-10 11:36:33.506 
Epoch 321/1000 
	 loss: 391.0354, MinusLogProbMetric: 391.0354, val_loss: 396.1815, val_MinusLogProbMetric: 396.1815

Epoch 321: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.0354 - MinusLogProbMetric: 391.0354 - val_loss: 396.1815 - val_MinusLogProbMetric: 396.1815 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 322/1000
2023-09-10 11:36:50.640 
Epoch 322/1000 
	 loss: 391.1400, MinusLogProbMetric: 391.1400, val_loss: 396.6475, val_MinusLogProbMetric: 396.6475

Epoch 322: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.1400 - MinusLogProbMetric: 391.1400 - val_loss: 396.6475 - val_MinusLogProbMetric: 396.6475 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 323/1000
2023-09-10 11:37:07.354 
Epoch 323/1000 
	 loss: 390.8853, MinusLogProbMetric: 390.8853, val_loss: 397.3019, val_MinusLogProbMetric: 397.3019

Epoch 323: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.8853 - MinusLogProbMetric: 390.8853 - val_loss: 397.3019 - val_MinusLogProbMetric: 397.3019 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 324/1000
2023-09-10 11:37:23.173 
Epoch 324/1000 
	 loss: 391.1319, MinusLogProbMetric: 391.1319, val_loss: 397.4398, val_MinusLogProbMetric: 397.4398

Epoch 324: val_loss did not improve from 394.63806
196/196 - 16s - loss: 391.1319 - MinusLogProbMetric: 391.1319 - val_loss: 397.4398 - val_MinusLogProbMetric: 397.4398 - lr: 1.6667e-04 - 16s/epoch - 81ms/step
Epoch 325/1000
2023-09-10 11:37:39.340 
Epoch 325/1000 
	 loss: 390.9525, MinusLogProbMetric: 390.9525, val_loss: 395.2222, val_MinusLogProbMetric: 395.2222

Epoch 325: val_loss did not improve from 394.63806
196/196 - 16s - loss: 390.9525 - MinusLogProbMetric: 390.9525 - val_loss: 395.2222 - val_MinusLogProbMetric: 395.2222 - lr: 1.6667e-04 - 16s/epoch - 82ms/step
Epoch 326/1000
2023-09-10 11:37:56.163 
Epoch 326/1000 
	 loss: 390.8481, MinusLogProbMetric: 390.8481, val_loss: 395.9803, val_MinusLogProbMetric: 395.9803

Epoch 326: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.8481 - MinusLogProbMetric: 390.8481 - val_loss: 395.9803 - val_MinusLogProbMetric: 395.9803 - lr: 1.6667e-04 - 17s/epoch - 86ms/step
Epoch 327/1000
2023-09-10 11:38:12.913 
Epoch 327/1000 
	 loss: 391.0745, MinusLogProbMetric: 391.0745, val_loss: 397.5420, val_MinusLogProbMetric: 397.5420

Epoch 327: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.0745 - MinusLogProbMetric: 391.0745 - val_loss: 397.5420 - val_MinusLogProbMetric: 397.5420 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 328/1000
2023-09-10 11:38:29.514 
Epoch 328/1000 
	 loss: 390.6812, MinusLogProbMetric: 390.6812, val_loss: 396.5061, val_MinusLogProbMetric: 396.5061

Epoch 328: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.6812 - MinusLogProbMetric: 390.6812 - val_loss: 396.5061 - val_MinusLogProbMetric: 396.5061 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 329/1000
2023-09-10 11:38:45.952 
Epoch 329/1000 
	 loss: 390.9821, MinusLogProbMetric: 390.9821, val_loss: 396.1658, val_MinusLogProbMetric: 396.1658

Epoch 329: val_loss did not improve from 394.63806
196/196 - 16s - loss: 390.9821 - MinusLogProbMetric: 390.9821 - val_loss: 396.1658 - val_MinusLogProbMetric: 396.1658 - lr: 1.6667e-04 - 16s/epoch - 84ms/step
Epoch 330/1000
2023-09-10 11:39:02.925 
Epoch 330/1000 
	 loss: 390.8826, MinusLogProbMetric: 390.8826, val_loss: 398.3831, val_MinusLogProbMetric: 398.3831

Epoch 330: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.8826 - MinusLogProbMetric: 390.8826 - val_loss: 398.3831 - val_MinusLogProbMetric: 398.3831 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 331/1000
2023-09-10 11:39:20.087 
Epoch 331/1000 
	 loss: 391.2856, MinusLogProbMetric: 391.2856, val_loss: 396.1910, val_MinusLogProbMetric: 396.1910

Epoch 331: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.2856 - MinusLogProbMetric: 391.2856 - val_loss: 396.1910 - val_MinusLogProbMetric: 396.1910 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 332/1000
2023-09-10 11:39:37.144 
Epoch 332/1000 
	 loss: 390.7822, MinusLogProbMetric: 390.7822, val_loss: 395.2381, val_MinusLogProbMetric: 395.2381

Epoch 332: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.7822 - MinusLogProbMetric: 390.7822 - val_loss: 395.2381 - val_MinusLogProbMetric: 395.2381 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 333/1000
2023-09-10 11:39:54.336 
Epoch 333/1000 
	 loss: 391.0146, MinusLogProbMetric: 391.0146, val_loss: 398.0337, val_MinusLogProbMetric: 398.0337

Epoch 333: val_loss did not improve from 394.63806
196/196 - 17s - loss: 391.0146 - MinusLogProbMetric: 391.0146 - val_loss: 398.0337 - val_MinusLogProbMetric: 398.0337 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 334/1000
2023-09-10 11:40:12.216 
Epoch 334/1000 
	 loss: 391.5663, MinusLogProbMetric: 391.5663, val_loss: 394.6595, val_MinusLogProbMetric: 394.6595

Epoch 334: val_loss did not improve from 394.63806
196/196 - 18s - loss: 391.5663 - MinusLogProbMetric: 391.5663 - val_loss: 394.6595 - val_MinusLogProbMetric: 394.6595 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 335/1000
2023-09-10 11:40:29.292 
Epoch 335/1000 
	 loss: 390.9482, MinusLogProbMetric: 390.9482, val_loss: 397.8877, val_MinusLogProbMetric: 397.8877

Epoch 335: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.9482 - MinusLogProbMetric: 390.9482 - val_loss: 397.8877 - val_MinusLogProbMetric: 397.8877 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 336/1000
2023-09-10 11:40:47.685 
Epoch 336/1000 
	 loss: 390.9635, MinusLogProbMetric: 390.9635, val_loss: 396.0669, val_MinusLogProbMetric: 396.0669

Epoch 336: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.9635 - MinusLogProbMetric: 390.9635 - val_loss: 396.0669 - val_MinusLogProbMetric: 396.0669 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 337/1000
2023-09-10 11:41:05.186 
Epoch 337/1000 
	 loss: 390.7518, MinusLogProbMetric: 390.7518, val_loss: 401.0915, val_MinusLogProbMetric: 401.0915

Epoch 337: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.7518 - MinusLogProbMetric: 390.7518 - val_loss: 401.0915 - val_MinusLogProbMetric: 401.0915 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 338/1000
2023-09-10 11:41:21.890 
Epoch 338/1000 
	 loss: 390.9130, MinusLogProbMetric: 390.9130, val_loss: 395.2479, val_MinusLogProbMetric: 395.2479

Epoch 338: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.9130 - MinusLogProbMetric: 390.9130 - val_loss: 395.2479 - val_MinusLogProbMetric: 395.2479 - lr: 1.6667e-04 - 17s/epoch - 85ms/step
Epoch 339/1000
2023-09-10 11:41:40.456 
Epoch 339/1000 
	 loss: 390.9405, MinusLogProbMetric: 390.9405, val_loss: 401.2331, val_MinusLogProbMetric: 401.2331

Epoch 339: val_loss did not improve from 394.63806
196/196 - 19s - loss: 390.9405 - MinusLogProbMetric: 390.9405 - val_loss: 401.2331 - val_MinusLogProbMetric: 401.2331 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 340/1000
2023-09-10 11:41:57.993 
Epoch 340/1000 
	 loss: 390.6941, MinusLogProbMetric: 390.6941, val_loss: 394.8669, val_MinusLogProbMetric: 394.8669

Epoch 340: val_loss did not improve from 394.63806
196/196 - 18s - loss: 390.6941 - MinusLogProbMetric: 390.6941 - val_loss: 394.8669 - val_MinusLogProbMetric: 394.8669 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 341/1000
2023-09-10 11:42:15.171 
Epoch 341/1000 
	 loss: 390.8685, MinusLogProbMetric: 390.8685, val_loss: 397.0452, val_MinusLogProbMetric: 397.0452

Epoch 341: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.8685 - MinusLogProbMetric: 390.8685 - val_loss: 397.0452 - val_MinusLogProbMetric: 397.0452 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 342/1000
2023-09-10 11:42:32.415 
Epoch 342/1000 
	 loss: 390.4726, MinusLogProbMetric: 390.4726, val_loss: 394.8491, val_MinusLogProbMetric: 394.8491

Epoch 342: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.4726 - MinusLogProbMetric: 390.4726 - val_loss: 394.8491 - val_MinusLogProbMetric: 394.8491 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 343/1000
2023-09-10 11:42:49.770 
Epoch 343/1000 
	 loss: 390.9420, MinusLogProbMetric: 390.9420, val_loss: 395.9729, val_MinusLogProbMetric: 395.9729

Epoch 343: val_loss did not improve from 394.63806
196/196 - 17s - loss: 390.9420 - MinusLogProbMetric: 390.9420 - val_loss: 395.9729 - val_MinusLogProbMetric: 395.9729 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 344/1000
2023-09-10 11:43:07.589 
Epoch 344/1000 
	 loss: 387.9558, MinusLogProbMetric: 387.9558, val_loss: 393.2363, val_MinusLogProbMetric: 393.2363

Epoch 344: val_loss improved from 394.63806 to 393.23627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 387.9558 - MinusLogProbMetric: 387.9558 - val_loss: 393.2363 - val_MinusLogProbMetric: 393.2363 - lr: 8.3333e-05 - 19s/epoch - 94ms/step
Epoch 345/1000
2023-09-10 11:43:25.683 
Epoch 345/1000 
	 loss: 387.6877, MinusLogProbMetric: 387.6877, val_loss: 393.0445, val_MinusLogProbMetric: 393.0445

Epoch 345: val_loss improved from 393.23627 to 393.04453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 387.6877 - MinusLogProbMetric: 387.6877 - val_loss: 393.0445 - val_MinusLogProbMetric: 393.0445 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 346/1000
2023-09-10 11:43:43.085 
Epoch 346/1000 
	 loss: 387.9045, MinusLogProbMetric: 387.9045, val_loss: 393.5676, val_MinusLogProbMetric: 393.5676

Epoch 346: val_loss did not improve from 393.04453
196/196 - 17s - loss: 387.9045 - MinusLogProbMetric: 387.9045 - val_loss: 393.5676 - val_MinusLogProbMetric: 393.5676 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 347/1000
2023-09-10 11:44:00.683 
Epoch 347/1000 
	 loss: 387.9399, MinusLogProbMetric: 387.9399, val_loss: 392.9542, val_MinusLogProbMetric: 392.9542

Epoch 347: val_loss improved from 393.04453 to 392.95419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 387.9399 - MinusLogProbMetric: 387.9399 - val_loss: 392.9542 - val_MinusLogProbMetric: 392.9542 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 348/1000
2023-09-10 11:44:17.324 
Epoch 348/1000 
	 loss: 388.5635, MinusLogProbMetric: 388.5635, val_loss: 393.6431, val_MinusLogProbMetric: 393.6431

Epoch 348: val_loss did not improve from 392.95419
196/196 - 16s - loss: 388.5635 - MinusLogProbMetric: 388.5635 - val_loss: 393.6431 - val_MinusLogProbMetric: 393.6431 - lr: 8.3333e-05 - 16s/epoch - 82ms/step
Epoch 349/1000
2023-09-10 11:44:34.944 
Epoch 349/1000 
	 loss: 388.2609, MinusLogProbMetric: 388.2609, val_loss: 393.4647, val_MinusLogProbMetric: 393.4647

Epoch 349: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.2609 - MinusLogProbMetric: 388.2609 - val_loss: 393.4647 - val_MinusLogProbMetric: 393.4647 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 350/1000
2023-09-10 11:44:52.488 
Epoch 350/1000 
	 loss: 387.8141, MinusLogProbMetric: 387.8141, val_loss: 393.5954, val_MinusLogProbMetric: 393.5954

Epoch 350: val_loss did not improve from 392.95419
196/196 - 18s - loss: 387.8141 - MinusLogProbMetric: 387.8141 - val_loss: 393.5954 - val_MinusLogProbMetric: 393.5954 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 351/1000
2023-09-10 11:45:09.970 
Epoch 351/1000 
	 loss: 387.9713, MinusLogProbMetric: 387.9713, val_loss: 393.1361, val_MinusLogProbMetric: 393.1361

Epoch 351: val_loss did not improve from 392.95419
196/196 - 17s - loss: 387.9713 - MinusLogProbMetric: 387.9713 - val_loss: 393.1361 - val_MinusLogProbMetric: 393.1361 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 352/1000
2023-09-10 11:45:27.894 
Epoch 352/1000 
	 loss: 387.9977, MinusLogProbMetric: 387.9977, val_loss: 394.7841, val_MinusLogProbMetric: 394.7841

Epoch 352: val_loss did not improve from 392.95419
196/196 - 18s - loss: 387.9977 - MinusLogProbMetric: 387.9977 - val_loss: 394.7841 - val_MinusLogProbMetric: 394.7841 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 353/1000
2023-09-10 11:45:45.967 
Epoch 353/1000 
	 loss: 388.3246, MinusLogProbMetric: 388.3246, val_loss: 393.8717, val_MinusLogProbMetric: 393.8717

Epoch 353: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.3246 - MinusLogProbMetric: 388.3246 - val_loss: 393.8717 - val_MinusLogProbMetric: 393.8717 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 354/1000
2023-09-10 11:46:03.800 
Epoch 354/1000 
	 loss: 388.1148, MinusLogProbMetric: 388.1148, val_loss: 393.2515, val_MinusLogProbMetric: 393.2515

Epoch 354: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.1148 - MinusLogProbMetric: 388.1148 - val_loss: 393.2515 - val_MinusLogProbMetric: 393.2515 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 355/1000
2023-09-10 11:46:21.310 
Epoch 355/1000 
	 loss: 387.8629, MinusLogProbMetric: 387.8629, val_loss: 393.4399, val_MinusLogProbMetric: 393.4399

Epoch 355: val_loss did not improve from 392.95419
196/196 - 17s - loss: 387.8629 - MinusLogProbMetric: 387.8629 - val_loss: 393.4399 - val_MinusLogProbMetric: 393.4399 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 356/1000
2023-09-10 11:46:39.782 
Epoch 356/1000 
	 loss: 388.1728, MinusLogProbMetric: 388.1728, val_loss: 393.6003, val_MinusLogProbMetric: 393.6003

Epoch 356: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.1728 - MinusLogProbMetric: 388.1728 - val_loss: 393.6003 - val_MinusLogProbMetric: 393.6003 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 357/1000
2023-09-10 11:46:57.790 
Epoch 357/1000 
	 loss: 387.9943, MinusLogProbMetric: 387.9943, val_loss: 394.4211, val_MinusLogProbMetric: 394.4211

Epoch 357: val_loss did not improve from 392.95419
196/196 - 18s - loss: 387.9943 - MinusLogProbMetric: 387.9943 - val_loss: 394.4211 - val_MinusLogProbMetric: 394.4211 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 358/1000
2023-09-10 11:47:15.609 
Epoch 358/1000 
	 loss: 388.1536, MinusLogProbMetric: 388.1536, val_loss: 393.2082, val_MinusLogProbMetric: 393.2082

Epoch 358: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.1536 - MinusLogProbMetric: 388.1536 - val_loss: 393.2082 - val_MinusLogProbMetric: 393.2082 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 359/1000
2023-09-10 11:47:33.813 
Epoch 359/1000 
	 loss: 388.1968, MinusLogProbMetric: 388.1968, val_loss: 393.6496, val_MinusLogProbMetric: 393.6496

Epoch 359: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.1968 - MinusLogProbMetric: 388.1968 - val_loss: 393.6496 - val_MinusLogProbMetric: 393.6496 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 360/1000
2023-09-10 11:47:52.239 
Epoch 360/1000 
	 loss: 388.4187, MinusLogProbMetric: 388.4187, val_loss: 394.2848, val_MinusLogProbMetric: 394.2848

Epoch 360: val_loss did not improve from 392.95419
196/196 - 18s - loss: 388.4187 - MinusLogProbMetric: 388.4187 - val_loss: 394.2848 - val_MinusLogProbMetric: 394.2848 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 361/1000
2023-09-10 11:48:10.469 
Epoch 361/1000 
	 loss: 387.9292, MinusLogProbMetric: 387.9292, val_loss: 392.8762, val_MinusLogProbMetric: 392.8762

Epoch 361: val_loss improved from 392.95419 to 392.87622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 19s - loss: 387.9292 - MinusLogProbMetric: 387.9292 - val_loss: 392.8762 - val_MinusLogProbMetric: 392.8762 - lr: 8.3333e-05 - 19s/epoch - 97ms/step
Epoch 362/1000
2023-09-10 11:48:29.142 
Epoch 362/1000 
	 loss: 388.6178, MinusLogProbMetric: 388.6178, val_loss: 393.2507, val_MinusLogProbMetric: 393.2507

Epoch 362: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.6178 - MinusLogProbMetric: 388.6178 - val_loss: 393.2507 - val_MinusLogProbMetric: 393.2507 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 363/1000
2023-09-10 11:48:47.347 
Epoch 363/1000 
	 loss: 388.2433, MinusLogProbMetric: 388.2433, val_loss: 393.5334, val_MinusLogProbMetric: 393.5334

Epoch 363: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.2433 - MinusLogProbMetric: 388.2433 - val_loss: 393.5334 - val_MinusLogProbMetric: 393.5334 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 364/1000
2023-09-10 11:49:05.850 
Epoch 364/1000 
	 loss: 388.1188, MinusLogProbMetric: 388.1188, val_loss: 393.2610, val_MinusLogProbMetric: 393.2610

Epoch 364: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.1188 - MinusLogProbMetric: 388.1188 - val_loss: 393.2610 - val_MinusLogProbMetric: 393.2610 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 365/1000
2023-09-10 11:49:23.731 
Epoch 365/1000 
	 loss: 388.0685, MinusLogProbMetric: 388.0685, val_loss: 393.1400, val_MinusLogProbMetric: 393.1400

Epoch 365: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.0685 - MinusLogProbMetric: 388.0685 - val_loss: 393.1400 - val_MinusLogProbMetric: 393.1400 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 366/1000
2023-09-10 11:49:42.126 
Epoch 366/1000 
	 loss: 388.1680, MinusLogProbMetric: 388.1680, val_loss: 393.0490, val_MinusLogProbMetric: 393.0490

Epoch 366: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.1680 - MinusLogProbMetric: 388.1680 - val_loss: 393.0490 - val_MinusLogProbMetric: 393.0490 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 367/1000
2023-09-10 11:49:59.759 
Epoch 367/1000 
	 loss: 388.1204, MinusLogProbMetric: 388.1204, val_loss: 393.9798, val_MinusLogProbMetric: 393.9798

Epoch 367: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.1204 - MinusLogProbMetric: 388.1204 - val_loss: 393.9798 - val_MinusLogProbMetric: 393.9798 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 368/1000
2023-09-10 11:50:18.717 
Epoch 368/1000 
	 loss: 388.1699, MinusLogProbMetric: 388.1699, val_loss: 393.8630, val_MinusLogProbMetric: 393.8630

Epoch 368: val_loss did not improve from 392.87622
196/196 - 19s - loss: 388.1699 - MinusLogProbMetric: 388.1699 - val_loss: 393.8630 - val_MinusLogProbMetric: 393.8630 - lr: 8.3333e-05 - 19s/epoch - 97ms/step
Epoch 369/1000
2023-09-10 11:50:35.929 
Epoch 369/1000 
	 loss: 387.7807, MinusLogProbMetric: 387.7807, val_loss: 393.9373, val_MinusLogProbMetric: 393.9373

Epoch 369: val_loss did not improve from 392.87622
196/196 - 17s - loss: 387.7807 - MinusLogProbMetric: 387.7807 - val_loss: 393.9373 - val_MinusLogProbMetric: 393.9373 - lr: 8.3333e-05 - 17s/epoch - 88ms/step
Epoch 370/1000
2023-09-10 11:50:53.844 
Epoch 370/1000 
	 loss: 388.0011, MinusLogProbMetric: 388.0011, val_loss: 393.4770, val_MinusLogProbMetric: 393.4770

Epoch 370: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.0011 - MinusLogProbMetric: 388.0011 - val_loss: 393.4770 - val_MinusLogProbMetric: 393.4770 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 371/1000
2023-09-10 11:51:11.947 
Epoch 371/1000 
	 loss: 388.3543, MinusLogProbMetric: 388.3543, val_loss: 393.0877, val_MinusLogProbMetric: 393.0877

Epoch 371: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.3543 - MinusLogProbMetric: 388.3543 - val_loss: 393.0877 - val_MinusLogProbMetric: 393.0877 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 372/1000
2023-09-10 11:51:29.627 
Epoch 372/1000 
	 loss: 388.4220, MinusLogProbMetric: 388.4220, val_loss: 395.3286, val_MinusLogProbMetric: 395.3286

Epoch 372: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.4220 - MinusLogProbMetric: 388.4220 - val_loss: 395.3286 - val_MinusLogProbMetric: 395.3286 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 373/1000
2023-09-10 11:51:47.652 
Epoch 373/1000 
	 loss: 388.1216, MinusLogProbMetric: 388.1216, val_loss: 393.8871, val_MinusLogProbMetric: 393.8871

Epoch 373: val_loss did not improve from 392.87622
196/196 - 18s - loss: 388.1216 - MinusLogProbMetric: 388.1216 - val_loss: 393.8871 - val_MinusLogProbMetric: 393.8871 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 374/1000
2023-09-10 11:52:05.405 
Epoch 374/1000 
	 loss: 387.9975, MinusLogProbMetric: 387.9975, val_loss: 394.2405, val_MinusLogProbMetric: 394.2405

Epoch 374: val_loss did not improve from 392.87622
196/196 - 18s - loss: 387.9975 - MinusLogProbMetric: 387.9975 - val_loss: 394.2405 - val_MinusLogProbMetric: 394.2405 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 375/1000
2023-09-10 11:52:23.851 
Epoch 375/1000 
	 loss: 387.8878, MinusLogProbMetric: 387.8878, val_loss: 393.5696, val_MinusLogProbMetric: 393.5696

Epoch 375: val_loss did not improve from 392.87622
196/196 - 18s - loss: 387.8878 - MinusLogProbMetric: 387.8878 - val_loss: 393.5696 - val_MinusLogProbMetric: 393.5696 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 376/1000
2023-09-10 11:52:41.552 
Epoch 376/1000 
	 loss: 387.7221, MinusLogProbMetric: 387.7221, val_loss: 392.8460, val_MinusLogProbMetric: 392.8460

Epoch 376: val_loss improved from 392.87622 to 392.84604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 18s - loss: 387.7221 - MinusLogProbMetric: 387.7221 - val_loss: 392.8460 - val_MinusLogProbMetric: 392.8460 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 377/1000
2023-09-10 11:53:00.181 
Epoch 377/1000 
	 loss: 388.1568, MinusLogProbMetric: 388.1568, val_loss: 393.3094, val_MinusLogProbMetric: 393.3094

Epoch 377: val_loss did not improve from 392.84604
196/196 - 18s - loss: 388.1568 - MinusLogProbMetric: 388.1568 - val_loss: 393.3094 - val_MinusLogProbMetric: 393.3094 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 378/1000
2023-09-10 11:53:18.444 
Epoch 378/1000 
	 loss: 387.7772, MinusLogProbMetric: 387.7772, val_loss: 393.3947, val_MinusLogProbMetric: 393.3947

Epoch 378: val_loss did not improve from 392.84604
196/196 - 18s - loss: 387.7772 - MinusLogProbMetric: 387.7772 - val_loss: 393.3947 - val_MinusLogProbMetric: 393.3947 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 379/1000
2023-09-10 11:53:37.121 
Epoch 379/1000 
	 loss: 387.9640, MinusLogProbMetric: 387.9640, val_loss: 394.3737, val_MinusLogProbMetric: 394.3737

Epoch 379: val_loss did not improve from 392.84604
196/196 - 19s - loss: 387.9640 - MinusLogProbMetric: 387.9640 - val_loss: 394.3737 - val_MinusLogProbMetric: 394.3737 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 380/1000
2023-09-10 11:53:56.068 
Epoch 380/1000 
	 loss: 388.3088, MinusLogProbMetric: 388.3088, val_loss: 393.9955, val_MinusLogProbMetric: 393.9955

Epoch 380: val_loss did not improve from 392.84604
196/196 - 19s - loss: 388.3088 - MinusLogProbMetric: 388.3088 - val_loss: 393.9955 - val_MinusLogProbMetric: 393.9955 - lr: 8.3333e-05 - 19s/epoch - 97ms/step
Epoch 381/1000
2023-09-10 11:54:14.416 
Epoch 381/1000 
	 loss: 387.8888, MinusLogProbMetric: 387.8888, val_loss: 392.6168, val_MinusLogProbMetric: 392.6168

Epoch 381: val_loss improved from 392.84604 to 392.61682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 20s - loss: 387.8888 - MinusLogProbMetric: 387.8888 - val_loss: 392.6168 - val_MinusLogProbMetric: 392.6168 - lr: 8.3333e-05 - 20s/epoch - 100ms/step
Epoch 382/1000
2023-09-10 11:54:34.792 
Epoch 382/1000 
	 loss: 387.8228, MinusLogProbMetric: 387.8228, val_loss: 393.3461, val_MinusLogProbMetric: 393.3461

Epoch 382: val_loss did not improve from 392.61682
196/196 - 19s - loss: 387.8228 - MinusLogProbMetric: 387.8228 - val_loss: 393.3461 - val_MinusLogProbMetric: 393.3461 - lr: 8.3333e-05 - 19s/epoch - 98ms/step
Epoch 383/1000
2023-09-10 11:54:53.230 
Epoch 383/1000 
	 loss: 388.1012, MinusLogProbMetric: 388.1012, val_loss: 394.4968, val_MinusLogProbMetric: 394.4968

Epoch 383: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.1012 - MinusLogProbMetric: 388.1012 - val_loss: 394.4968 - val_MinusLogProbMetric: 394.4968 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 384/1000
2023-09-10 11:55:10.701 
Epoch 384/1000 
	 loss: 388.0378, MinusLogProbMetric: 388.0378, val_loss: 395.1862, val_MinusLogProbMetric: 395.1862

Epoch 384: val_loss did not improve from 392.61682
196/196 - 17s - loss: 388.0378 - MinusLogProbMetric: 388.0378 - val_loss: 395.1862 - val_MinusLogProbMetric: 395.1862 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 385/1000
2023-09-10 11:55:28.350 
Epoch 385/1000 
	 loss: 388.0273, MinusLogProbMetric: 388.0273, val_loss: 392.9560, val_MinusLogProbMetric: 392.9560

Epoch 385: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.0273 - MinusLogProbMetric: 388.0273 - val_loss: 392.9560 - val_MinusLogProbMetric: 392.9560 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 386/1000
2023-09-10 11:55:45.983 
Epoch 386/1000 
	 loss: 387.8433, MinusLogProbMetric: 387.8433, val_loss: 393.5201, val_MinusLogProbMetric: 393.5201

Epoch 386: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.8433 - MinusLogProbMetric: 387.8433 - val_loss: 393.5201 - val_MinusLogProbMetric: 393.5201 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 387/1000
2023-09-10 11:56:03.742 
Epoch 387/1000 
	 loss: 388.2271, MinusLogProbMetric: 388.2271, val_loss: 393.3117, val_MinusLogProbMetric: 393.3117

Epoch 387: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.2271 - MinusLogProbMetric: 388.2271 - val_loss: 393.3117 - val_MinusLogProbMetric: 393.3117 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 388/1000
2023-09-10 11:56:21.958 
Epoch 388/1000 
	 loss: 388.0222, MinusLogProbMetric: 388.0222, val_loss: 393.6416, val_MinusLogProbMetric: 393.6416

Epoch 388: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.0222 - MinusLogProbMetric: 388.0222 - val_loss: 393.6416 - val_MinusLogProbMetric: 393.6416 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 389/1000
2023-09-10 11:56:40.670 
Epoch 389/1000 
	 loss: 387.9124, MinusLogProbMetric: 387.9124, val_loss: 393.3016, val_MinusLogProbMetric: 393.3016

Epoch 389: val_loss did not improve from 392.61682
196/196 - 19s - loss: 387.9124 - MinusLogProbMetric: 387.9124 - val_loss: 393.3016 - val_MinusLogProbMetric: 393.3016 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 390/1000
2023-09-10 11:56:58.914 
Epoch 390/1000 
	 loss: 388.0410, MinusLogProbMetric: 388.0410, val_loss: 393.3252, val_MinusLogProbMetric: 393.3252

Epoch 390: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.0410 - MinusLogProbMetric: 388.0410 - val_loss: 393.3252 - val_MinusLogProbMetric: 393.3252 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 391/1000
2023-09-10 11:57:18.222 
Epoch 391/1000 
	 loss: 387.8792, MinusLogProbMetric: 387.8792, val_loss: 393.6906, val_MinusLogProbMetric: 393.6906

Epoch 391: val_loss did not improve from 392.61682
196/196 - 19s - loss: 387.8792 - MinusLogProbMetric: 387.8792 - val_loss: 393.6906 - val_MinusLogProbMetric: 393.6906 - lr: 8.3333e-05 - 19s/epoch - 99ms/step
Epoch 392/1000
2023-09-10 11:57:36.493 
Epoch 392/1000 
	 loss: 388.1332, MinusLogProbMetric: 388.1332, val_loss: 393.9775, val_MinusLogProbMetric: 393.9775

Epoch 392: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.1332 - MinusLogProbMetric: 388.1332 - val_loss: 393.9775 - val_MinusLogProbMetric: 393.9775 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 393/1000
2023-09-10 11:57:54.651 
Epoch 393/1000 
	 loss: 387.9761, MinusLogProbMetric: 387.9761, val_loss: 393.3405, val_MinusLogProbMetric: 393.3405

Epoch 393: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.9761 - MinusLogProbMetric: 387.9761 - val_loss: 393.3405 - val_MinusLogProbMetric: 393.3405 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 394/1000
2023-09-10 11:58:13.299 
Epoch 394/1000 
	 loss: 387.8133, MinusLogProbMetric: 387.8133, val_loss: 393.2942, val_MinusLogProbMetric: 393.2942

Epoch 394: val_loss did not improve from 392.61682
196/196 - 19s - loss: 387.8133 - MinusLogProbMetric: 387.8133 - val_loss: 393.2942 - val_MinusLogProbMetric: 393.2942 - lr: 8.3333e-05 - 19s/epoch - 95ms/step
Epoch 395/1000
2023-09-10 11:58:32.869 
Epoch 395/1000 
	 loss: 387.8840, MinusLogProbMetric: 387.8840, val_loss: 394.0498, val_MinusLogProbMetric: 394.0498

Epoch 395: val_loss did not improve from 392.61682
196/196 - 20s - loss: 387.8840 - MinusLogProbMetric: 387.8840 - val_loss: 394.0498 - val_MinusLogProbMetric: 394.0498 - lr: 8.3333e-05 - 20s/epoch - 100ms/step
Epoch 396/1000
2023-09-10 11:58:52.642 
Epoch 396/1000 
	 loss: 387.6015, MinusLogProbMetric: 387.6015, val_loss: 393.0345, val_MinusLogProbMetric: 393.0345

Epoch 396: val_loss did not improve from 392.61682
196/196 - 20s - loss: 387.6015 - MinusLogProbMetric: 387.6015 - val_loss: 393.0345 - val_MinusLogProbMetric: 393.0345 - lr: 8.3333e-05 - 20s/epoch - 101ms/step
Epoch 397/1000
2023-09-10 11:59:10.206 
Epoch 397/1000 
	 loss: 388.1093, MinusLogProbMetric: 388.1093, val_loss: 394.6297, val_MinusLogProbMetric: 394.6297

Epoch 397: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.1093 - MinusLogProbMetric: 388.1093 - val_loss: 394.6297 - val_MinusLogProbMetric: 394.6297 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 398/1000
2023-09-10 11:59:27.637 
Epoch 398/1000 
	 loss: 388.1941, MinusLogProbMetric: 388.1941, val_loss: 393.4189, val_MinusLogProbMetric: 393.4189

Epoch 398: val_loss did not improve from 392.61682
196/196 - 17s - loss: 388.1941 - MinusLogProbMetric: 388.1941 - val_loss: 393.4189 - val_MinusLogProbMetric: 393.4189 - lr: 8.3333e-05 - 17s/epoch - 89ms/step
Epoch 399/1000
2023-09-10 11:59:44.764 
Epoch 399/1000 
	 loss: 387.8465, MinusLogProbMetric: 387.8465, val_loss: 393.9873, val_MinusLogProbMetric: 393.9873

Epoch 399: val_loss did not improve from 392.61682
196/196 - 17s - loss: 387.8465 - MinusLogProbMetric: 387.8465 - val_loss: 393.9873 - val_MinusLogProbMetric: 393.9873 - lr: 8.3333e-05 - 17s/epoch - 87ms/step
Epoch 400/1000
2023-09-10 12:00:02.520 
Epoch 400/1000 
	 loss: 387.7962, MinusLogProbMetric: 387.7962, val_loss: 393.4362, val_MinusLogProbMetric: 393.4362

Epoch 400: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.7962 - MinusLogProbMetric: 387.7962 - val_loss: 393.4362 - val_MinusLogProbMetric: 393.4362 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 401/1000
2023-09-10 12:00:20.873 
Epoch 401/1000 
	 loss: 388.1390, MinusLogProbMetric: 388.1390, val_loss: 393.1973, val_MinusLogProbMetric: 393.1973

Epoch 401: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.1390 - MinusLogProbMetric: 388.1390 - val_loss: 393.1973 - val_MinusLogProbMetric: 393.1973 - lr: 8.3333e-05 - 18s/epoch - 94ms/step
Epoch 402/1000
2023-09-10 12:00:39.179 
Epoch 402/1000 
	 loss: 387.7114, MinusLogProbMetric: 387.7114, val_loss: 393.2777, val_MinusLogProbMetric: 393.2777

Epoch 402: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.7114 - MinusLogProbMetric: 387.7114 - val_loss: 393.2777 - val_MinusLogProbMetric: 393.2777 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 403/1000
2023-09-10 12:00:57.161 
Epoch 403/1000 
	 loss: 387.5134, MinusLogProbMetric: 387.5134, val_loss: 393.0461, val_MinusLogProbMetric: 393.0461

Epoch 403: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.5134 - MinusLogProbMetric: 387.5134 - val_loss: 393.0461 - val_MinusLogProbMetric: 393.0461 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 404/1000
2023-09-10 12:01:15.904 
Epoch 404/1000 
	 loss: 387.7914, MinusLogProbMetric: 387.7914, val_loss: 394.1643, val_MinusLogProbMetric: 394.1643

Epoch 404: val_loss did not improve from 392.61682
196/196 - 19s - loss: 387.7914 - MinusLogProbMetric: 387.7914 - val_loss: 394.1643 - val_MinusLogProbMetric: 394.1643 - lr: 8.3333e-05 - 19s/epoch - 96ms/step
Epoch 405/1000
2023-09-10 12:01:34.676 
Epoch 405/1000 
	 loss: 387.7532, MinusLogProbMetric: 387.7532, val_loss: 393.1964, val_MinusLogProbMetric: 393.1964

Epoch 405: val_loss did not improve from 392.61682
196/196 - 19s - loss: 387.7532 - MinusLogProbMetric: 387.7532 - val_loss: 393.1964 - val_MinusLogProbMetric: 393.1964 - lr: 8.3333e-05 - 19s/epoch - 96ms/step
Epoch 406/1000
2023-09-10 12:01:52.950 
Epoch 406/1000 
	 loss: 388.2539, MinusLogProbMetric: 388.2539, val_loss: 393.4042, val_MinusLogProbMetric: 393.4042

Epoch 406: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.2539 - MinusLogProbMetric: 388.2539 - val_loss: 393.4042 - val_MinusLogProbMetric: 393.4042 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 407/1000
2023-09-10 12:02:10.988 
Epoch 407/1000 
	 loss: 388.0795, MinusLogProbMetric: 388.0795, val_loss: 393.1213, val_MinusLogProbMetric: 393.1213

Epoch 407: val_loss did not improve from 392.61682
196/196 - 18s - loss: 388.0795 - MinusLogProbMetric: 388.0795 - val_loss: 393.1213 - val_MinusLogProbMetric: 393.1213 - lr: 8.3333e-05 - 18s/epoch - 92ms/step
Epoch 408/1000
2023-09-10 12:02:28.553 
Epoch 408/1000 
	 loss: 387.6137, MinusLogProbMetric: 387.6137, val_loss: 393.6927, val_MinusLogProbMetric: 393.6927

Epoch 408: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.6137 - MinusLogProbMetric: 387.6137 - val_loss: 393.6927 - val_MinusLogProbMetric: 393.6927 - lr: 8.3333e-05 - 18s/epoch - 90ms/step
Epoch 409/1000
2023-09-10 12:02:46.392 
Epoch 409/1000 
	 loss: 387.8202, MinusLogProbMetric: 387.8202, val_loss: 393.4090, val_MinusLogProbMetric: 393.4090

Epoch 409: val_loss did not improve from 392.61682
196/196 - 18s - loss: 387.8202 - MinusLogProbMetric: 387.8202 - val_loss: 393.4090 - val_MinusLogProbMetric: 393.4090 - lr: 8.3333e-05 - 18s/epoch - 91ms/step
Epoch 410/1000
2023-09-10 12:02:54.884 
Epoch 410/1000 
	 loss: 388.0445, MinusLogProbMetric: 388.0445, val_loss: 392.8929, val_MinusLogProbMetric: 392.8929

Epoch 410: val_loss did not improve from 392.61682
196/196 - 8s - loss: 388.0445 - MinusLogProbMetric: 388.0445 - val_loss: 392.8929 - val_MinusLogProbMetric: 392.8929 - lr: 8.3333e-05 - 8s/epoch - 43ms/step
Epoch 411/1000
2023-09-10 12:03:19.367 
Epoch 411/1000 
	 loss: 387.9180, MinusLogProbMetric: 387.9180, val_loss: 393.2876, val_MinusLogProbMetric: 393.2876

Epoch 411: val_loss did not improve from 392.61682
196/196 - 24s - loss: 387.9180 - MinusLogProbMetric: 387.9180 - val_loss: 393.2876 - val_MinusLogProbMetric: 393.2876 - lr: 8.3333e-05 - 24s/epoch - 125ms/step
Epoch 412/1000
2023-09-10 12:03:42.432 
Epoch 412/1000 
	 loss: 387.7922, MinusLogProbMetric: 387.7922, val_loss: 394.9381, val_MinusLogProbMetric: 394.9381

Epoch 412: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.7922 - MinusLogProbMetric: 387.7922 - val_loss: 394.9381 - val_MinusLogProbMetric: 394.9381 - lr: 8.3333e-05 - 23s/epoch - 118ms/step
Epoch 413/1000
2023-09-10 12:04:05.873 
Epoch 413/1000 
	 loss: 389.1376, MinusLogProbMetric: 389.1376, val_loss: 392.8260, val_MinusLogProbMetric: 392.8260

Epoch 413: val_loss did not improve from 392.61682
196/196 - 23s - loss: 389.1376 - MinusLogProbMetric: 389.1376 - val_loss: 392.8260 - val_MinusLogProbMetric: 392.8260 - lr: 8.3333e-05 - 23s/epoch - 119ms/step
Epoch 414/1000
2023-09-10 12:04:28.021 
Epoch 414/1000 
	 loss: 387.4220, MinusLogProbMetric: 387.4220, val_loss: 394.4642, val_MinusLogProbMetric: 394.4642

Epoch 414: val_loss did not improve from 392.61682
196/196 - 22s - loss: 387.4220 - MinusLogProbMetric: 387.4220 - val_loss: 394.4642 - val_MinusLogProbMetric: 394.4642 - lr: 8.3333e-05 - 22s/epoch - 113ms/step
Epoch 415/1000
2023-09-10 12:04:49.759 
Epoch 415/1000 
	 loss: 387.5286, MinusLogProbMetric: 387.5286, val_loss: 395.9111, val_MinusLogProbMetric: 395.9111

Epoch 415: val_loss did not improve from 392.61682
196/196 - 22s - loss: 387.5286 - MinusLogProbMetric: 387.5286 - val_loss: 395.9111 - val_MinusLogProbMetric: 395.9111 - lr: 8.3333e-05 - 22s/epoch - 111ms/step
Epoch 416/1000
2023-09-10 12:05:11.435 
Epoch 416/1000 
	 loss: 387.7239, MinusLogProbMetric: 387.7239, val_loss: 393.9925, val_MinusLogProbMetric: 393.9925

Epoch 416: val_loss did not improve from 392.61682
196/196 - 22s - loss: 387.7239 - MinusLogProbMetric: 387.7239 - val_loss: 393.9925 - val_MinusLogProbMetric: 393.9925 - lr: 8.3333e-05 - 22s/epoch - 111ms/step
Epoch 417/1000
2023-09-10 12:05:34.047 
Epoch 417/1000 
	 loss: 387.6922, MinusLogProbMetric: 387.6922, val_loss: 393.9852, val_MinusLogProbMetric: 393.9852

Epoch 417: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.6922 - MinusLogProbMetric: 387.6922 - val_loss: 393.9852 - val_MinusLogProbMetric: 393.9852 - lr: 8.3333e-05 - 23s/epoch - 115ms/step
Epoch 418/1000
2023-09-10 12:05:58.824 
Epoch 418/1000 
	 loss: 387.9652, MinusLogProbMetric: 387.9652, val_loss: 392.9910, val_MinusLogProbMetric: 392.9910

Epoch 418: val_loss did not improve from 392.61682
196/196 - 25s - loss: 387.9652 - MinusLogProbMetric: 387.9652 - val_loss: 392.9910 - val_MinusLogProbMetric: 392.9910 - lr: 8.3333e-05 - 25s/epoch - 126ms/step
Epoch 419/1000
2023-09-10 12:06:22.771 
Epoch 419/1000 
	 loss: 387.6896, MinusLogProbMetric: 387.6896, val_loss: 392.9299, val_MinusLogProbMetric: 392.9299

Epoch 419: val_loss did not improve from 392.61682
196/196 - 24s - loss: 387.6896 - MinusLogProbMetric: 387.6896 - val_loss: 392.9299 - val_MinusLogProbMetric: 392.9299 - lr: 8.3333e-05 - 24s/epoch - 122ms/step
Epoch 420/1000
2023-09-10 12:06:46.384 
Epoch 420/1000 
	 loss: 387.6188, MinusLogProbMetric: 387.6188, val_loss: 395.4591, val_MinusLogProbMetric: 395.4591

Epoch 420: val_loss did not improve from 392.61682
196/196 - 24s - loss: 387.6188 - MinusLogProbMetric: 387.6188 - val_loss: 395.4591 - val_MinusLogProbMetric: 395.4591 - lr: 8.3333e-05 - 24s/epoch - 120ms/step
Epoch 421/1000
2023-09-10 12:07:09.459 
Epoch 421/1000 
	 loss: 387.7769, MinusLogProbMetric: 387.7769, val_loss: 394.4122, val_MinusLogProbMetric: 394.4122

Epoch 421: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.7769 - MinusLogProbMetric: 387.7769 - val_loss: 394.4122 - val_MinusLogProbMetric: 394.4122 - lr: 8.3333e-05 - 23s/epoch - 118ms/step
Epoch 422/1000
2023-09-10 12:07:32.037 
Epoch 422/1000 
	 loss: 387.9604, MinusLogProbMetric: 387.9604, val_loss: 394.9243, val_MinusLogProbMetric: 394.9243

Epoch 422: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.9604 - MinusLogProbMetric: 387.9604 - val_loss: 394.9243 - val_MinusLogProbMetric: 394.9243 - lr: 8.3333e-05 - 23s/epoch - 115ms/step
Epoch 423/1000
2023-09-10 12:07:55.082 
Epoch 423/1000 
	 loss: 387.9058, MinusLogProbMetric: 387.9058, val_loss: 392.8026, val_MinusLogProbMetric: 392.8026

Epoch 423: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.9058 - MinusLogProbMetric: 387.9058 - val_loss: 392.8026 - val_MinusLogProbMetric: 392.8026 - lr: 8.3333e-05 - 23s/epoch - 118ms/step
Epoch 424/1000
2023-09-10 12:08:18.677 
Epoch 424/1000 
	 loss: 387.6772, MinusLogProbMetric: 387.6772, val_loss: 395.6724, val_MinusLogProbMetric: 395.6724

Epoch 424: val_loss did not improve from 392.61682
196/196 - 24s - loss: 387.6772 - MinusLogProbMetric: 387.6772 - val_loss: 395.6724 - val_MinusLogProbMetric: 395.6724 - lr: 8.3333e-05 - 24s/epoch - 120ms/step
Epoch 425/1000
2023-09-10 12:08:41.661 
Epoch 425/1000 
	 loss: 388.0441, MinusLogProbMetric: 388.0441, val_loss: 394.0601, val_MinusLogProbMetric: 394.0601

Epoch 425: val_loss did not improve from 392.61682
196/196 - 23s - loss: 388.0441 - MinusLogProbMetric: 388.0441 - val_loss: 394.0601 - val_MinusLogProbMetric: 394.0601 - lr: 8.3333e-05 - 23s/epoch - 117ms/step
Epoch 426/1000
2023-09-10 12:09:05.083 
Epoch 426/1000 
	 loss: 387.8364, MinusLogProbMetric: 387.8364, val_loss: 395.1680, val_MinusLogProbMetric: 395.1680

Epoch 426: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.8364 - MinusLogProbMetric: 387.8364 - val_loss: 395.1680 - val_MinusLogProbMetric: 395.1680 - lr: 8.3333e-05 - 23s/epoch - 119ms/step
Epoch 427/1000
2023-09-10 12:09:27.709 
Epoch 427/1000 
	 loss: 388.3518, MinusLogProbMetric: 388.3518, val_loss: 393.1355, val_MinusLogProbMetric: 393.1355

Epoch 427: val_loss did not improve from 392.61682
196/196 - 23s - loss: 388.3518 - MinusLogProbMetric: 388.3518 - val_loss: 393.1355 - val_MinusLogProbMetric: 393.1355 - lr: 8.3333e-05 - 23s/epoch - 115ms/step
Epoch 428/1000
2023-09-10 12:09:49.909 
Epoch 428/1000 
	 loss: 387.9978, MinusLogProbMetric: 387.9978, val_loss: 393.5345, val_MinusLogProbMetric: 393.5345

Epoch 428: val_loss did not improve from 392.61682
196/196 - 22s - loss: 387.9978 - MinusLogProbMetric: 387.9978 - val_loss: 393.5345 - val_MinusLogProbMetric: 393.5345 - lr: 8.3333e-05 - 22s/epoch - 113ms/step
Epoch 429/1000
2023-09-10 12:10:13.058 
Epoch 429/1000 
	 loss: 387.7726, MinusLogProbMetric: 387.7726, val_loss: 393.0539, val_MinusLogProbMetric: 393.0539

Epoch 429: val_loss did not improve from 392.61682
196/196 - 23s - loss: 387.7726 - MinusLogProbMetric: 387.7726 - val_loss: 393.0539 - val_MinusLogProbMetric: 393.0539 - lr: 8.3333e-05 - 23s/epoch - 118ms/step
Epoch 430/1000
2023-09-10 12:10:35.389 
Epoch 430/1000 
	 loss: 387.7971, MinusLogProbMetric: 387.7971, val_loss: 393.0367, val_MinusLogProbMetric: 393.0367

Epoch 430: val_loss did not improve from 392.61682
196/196 - 22s - loss: 387.7971 - MinusLogProbMetric: 387.7971 - val_loss: 393.0367 - val_MinusLogProbMetric: 393.0367 - lr: 8.3333e-05 - 22s/epoch - 114ms/step
Epoch 431/1000
2023-09-10 12:10:57.305 
Epoch 431/1000 
	 loss: 388.4075, MinusLogProbMetric: 388.4075, val_loss: 394.2986, val_MinusLogProbMetric: 394.2986

Epoch 431: val_loss did not improve from 392.61682
196/196 - 22s - loss: 388.4075 - MinusLogProbMetric: 388.4075 - val_loss: 394.2986 - val_MinusLogProbMetric: 394.2986 - lr: 8.3333e-05 - 22s/epoch - 112ms/step
Epoch 432/1000
2023-09-10 12:11:19.191 
Epoch 432/1000 
	 loss: 386.4217, MinusLogProbMetric: 386.4217, val_loss: 392.2165, val_MinusLogProbMetric: 392.2165

Epoch 432: val_loss improved from 392.61682 to 392.21652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 386.4217 - MinusLogProbMetric: 386.4217 - val_loss: 392.2165 - val_MinusLogProbMetric: 392.2165 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 433/1000
2023-09-10 12:11:42.703 
Epoch 433/1000 
	 loss: 386.2756, MinusLogProbMetric: 386.2756, val_loss: 392.4371, val_MinusLogProbMetric: 392.4371

Epoch 433: val_loss did not improve from 392.21652
196/196 - 23s - loss: 386.2756 - MinusLogProbMetric: 386.2756 - val_loss: 392.4371 - val_MinusLogProbMetric: 392.4371 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 434/1000
2023-09-10 12:12:04.366 
Epoch 434/1000 
	 loss: 386.1570, MinusLogProbMetric: 386.1570, val_loss: 392.2436, val_MinusLogProbMetric: 392.2436

Epoch 434: val_loss did not improve from 392.21652
196/196 - 22s - loss: 386.1570 - MinusLogProbMetric: 386.1570 - val_loss: 392.2436 - val_MinusLogProbMetric: 392.2436 - lr: 4.1667e-05 - 22s/epoch - 110ms/step
Epoch 435/1000
2023-09-10 12:12:27.695 
Epoch 435/1000 
	 loss: 386.1696, MinusLogProbMetric: 386.1696, val_loss: 392.4309, val_MinusLogProbMetric: 392.4309

Epoch 435: val_loss did not improve from 392.21652
196/196 - 23s - loss: 386.1696 - MinusLogProbMetric: 386.1696 - val_loss: 392.4309 - val_MinusLogProbMetric: 392.4309 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 436/1000
2023-09-10 12:12:50.336 
Epoch 436/1000 
	 loss: 386.2384, MinusLogProbMetric: 386.2384, val_loss: 392.2133, val_MinusLogProbMetric: 392.2133

Epoch 436: val_loss improved from 392.21652 to 392.21326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 24s - loss: 386.2384 - MinusLogProbMetric: 386.2384 - val_loss: 392.2133 - val_MinusLogProbMetric: 392.2133 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 437/1000
2023-09-10 12:13:13.203 
Epoch 437/1000 
	 loss: 386.2003, MinusLogProbMetric: 386.2003, val_loss: 392.3371, val_MinusLogProbMetric: 392.3371

Epoch 437: val_loss did not improve from 392.21326
196/196 - 22s - loss: 386.2003 - MinusLogProbMetric: 386.2003 - val_loss: 392.3371 - val_MinusLogProbMetric: 392.3371 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 438/1000
2023-09-10 12:13:34.961 
Epoch 438/1000 
	 loss: 386.1844, MinusLogProbMetric: 386.1844, val_loss: 392.2307, val_MinusLogProbMetric: 392.2307

Epoch 438: val_loss did not improve from 392.21326
196/196 - 22s - loss: 386.1844 - MinusLogProbMetric: 386.1844 - val_loss: 392.2307 - val_MinusLogProbMetric: 392.2307 - lr: 4.1667e-05 - 22s/epoch - 111ms/step
Epoch 439/1000
2023-09-10 12:13:57.823 
Epoch 439/1000 
	 loss: 386.1064, MinusLogProbMetric: 386.1064, val_loss: 392.0746, val_MinusLogProbMetric: 392.0746

Epoch 439: val_loss improved from 392.21326 to 392.07465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 24s - loss: 386.1064 - MinusLogProbMetric: 386.1064 - val_loss: 392.0746 - val_MinusLogProbMetric: 392.0746 - lr: 4.1667e-05 - 24s/epoch - 122ms/step
Epoch 440/1000
2023-09-10 12:14:22.124 
Epoch 440/1000 
	 loss: 386.1494, MinusLogProbMetric: 386.1494, val_loss: 392.0777, val_MinusLogProbMetric: 392.0777

Epoch 440: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.1494 - MinusLogProbMetric: 386.1494 - val_loss: 392.0777 - val_MinusLogProbMetric: 392.0777 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 441/1000
2023-09-10 12:14:44.870 
Epoch 441/1000 
	 loss: 386.1757, MinusLogProbMetric: 386.1757, val_loss: 392.5660, val_MinusLogProbMetric: 392.5660

Epoch 441: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.1757 - MinusLogProbMetric: 386.1757 - val_loss: 392.5660 - val_MinusLogProbMetric: 392.5660 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 442/1000
2023-09-10 12:15:06.770 
Epoch 442/1000 
	 loss: 386.2502, MinusLogProbMetric: 386.2502, val_loss: 392.4483, val_MinusLogProbMetric: 392.4483

Epoch 442: val_loss did not improve from 392.07465
196/196 - 22s - loss: 386.2502 - MinusLogProbMetric: 386.2502 - val_loss: 392.4483 - val_MinusLogProbMetric: 392.4483 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 443/1000
2023-09-10 12:15:30.038 
Epoch 443/1000 
	 loss: 386.2953, MinusLogProbMetric: 386.2953, val_loss: 392.2511, val_MinusLogProbMetric: 392.2511

Epoch 443: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.2953 - MinusLogProbMetric: 386.2953 - val_loss: 392.2511 - val_MinusLogProbMetric: 392.2511 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 444/1000
2023-09-10 12:15:52.701 
Epoch 444/1000 
	 loss: 386.2899, MinusLogProbMetric: 386.2899, val_loss: 392.9156, val_MinusLogProbMetric: 392.9156

Epoch 444: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.2899 - MinusLogProbMetric: 386.2899 - val_loss: 392.9156 - val_MinusLogProbMetric: 392.9156 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 445/1000
2023-09-10 12:16:15.714 
Epoch 445/1000 
	 loss: 386.5674, MinusLogProbMetric: 386.5674, val_loss: 392.3504, val_MinusLogProbMetric: 392.3504

Epoch 445: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.5674 - MinusLogProbMetric: 386.5674 - val_loss: 392.3504 - val_MinusLogProbMetric: 392.3504 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 446/1000
2023-09-10 12:16:37.187 
Epoch 446/1000 
	 loss: 386.5087, MinusLogProbMetric: 386.5087, val_loss: 392.4947, val_MinusLogProbMetric: 392.4947

Epoch 446: val_loss did not improve from 392.07465
196/196 - 21s - loss: 386.5087 - MinusLogProbMetric: 386.5087 - val_loss: 392.4947 - val_MinusLogProbMetric: 392.4947 - lr: 4.1667e-05 - 21s/epoch - 109ms/step
Epoch 447/1000
2023-09-10 12:16:58.807 
Epoch 447/1000 
	 loss: 386.4389, MinusLogProbMetric: 386.4389, val_loss: 392.6917, val_MinusLogProbMetric: 392.6917

Epoch 447: val_loss did not improve from 392.07465
196/196 - 22s - loss: 386.4389 - MinusLogProbMetric: 386.4389 - val_loss: 392.6917 - val_MinusLogProbMetric: 392.6917 - lr: 4.1667e-05 - 22s/epoch - 110ms/step
Epoch 448/1000
2023-09-10 12:17:20.522 
Epoch 448/1000 
	 loss: 386.2281, MinusLogProbMetric: 386.2281, val_loss: 392.1534, val_MinusLogProbMetric: 392.1534

Epoch 448: val_loss did not improve from 392.07465
196/196 - 22s - loss: 386.2281 - MinusLogProbMetric: 386.2281 - val_loss: 392.1534 - val_MinusLogProbMetric: 392.1534 - lr: 4.1667e-05 - 22s/epoch - 111ms/step
Epoch 449/1000
2023-09-10 12:17:43.443 
Epoch 449/1000 
	 loss: 386.2088, MinusLogProbMetric: 386.2088, val_loss: 392.4376, val_MinusLogProbMetric: 392.4376

Epoch 449: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.2088 - MinusLogProbMetric: 386.2088 - val_loss: 392.4376 - val_MinusLogProbMetric: 392.4376 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 450/1000
2023-09-10 12:18:06.097 
Epoch 450/1000 
	 loss: 386.1920, MinusLogProbMetric: 386.1920, val_loss: 392.2560, val_MinusLogProbMetric: 392.2560

Epoch 450: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.1920 - MinusLogProbMetric: 386.1920 - val_loss: 392.2560 - val_MinusLogProbMetric: 392.2560 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 451/1000
2023-09-10 12:18:28.840 
Epoch 451/1000 
	 loss: 386.2621, MinusLogProbMetric: 386.2621, val_loss: 392.4794, val_MinusLogProbMetric: 392.4794

Epoch 451: val_loss did not improve from 392.07465
196/196 - 23s - loss: 386.2621 - MinusLogProbMetric: 386.2621 - val_loss: 392.4794 - val_MinusLogProbMetric: 392.4794 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 452/1000
2023-09-10 12:18:51.235 
Epoch 452/1000 
	 loss: 386.2460, MinusLogProbMetric: 386.2460, val_loss: 392.0522, val_MinusLogProbMetric: 392.0522

Epoch 452: val_loss improved from 392.07465 to 392.05219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 386.2460 - MinusLogProbMetric: 386.2460 - val_loss: 392.0522 - val_MinusLogProbMetric: 392.0522 - lr: 4.1667e-05 - 23s/epoch - 120ms/step
Epoch 453/1000
2023-09-10 12:19:15.682 
Epoch 453/1000 
	 loss: 386.1771, MinusLogProbMetric: 386.1771, val_loss: 392.3699, val_MinusLogProbMetric: 392.3699

Epoch 453: val_loss did not improve from 392.05219
196/196 - 23s - loss: 386.1771 - MinusLogProbMetric: 386.1771 - val_loss: 392.3699 - val_MinusLogProbMetric: 392.3699 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 454/1000
2023-09-10 12:19:38.474 
Epoch 454/1000 
	 loss: 386.1328, MinusLogProbMetric: 386.1328, val_loss: 392.2986, val_MinusLogProbMetric: 392.2986

Epoch 454: val_loss did not improve from 392.05219
196/196 - 23s - loss: 386.1328 - MinusLogProbMetric: 386.1328 - val_loss: 392.2986 - val_MinusLogProbMetric: 392.2986 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 455/1000
2023-09-10 12:20:01.224 
Epoch 455/1000 
	 loss: 386.0973, MinusLogProbMetric: 386.0973, val_loss: 392.3227, val_MinusLogProbMetric: 392.3227

Epoch 455: val_loss did not improve from 392.05219
196/196 - 23s - loss: 386.0973 - MinusLogProbMetric: 386.0973 - val_loss: 392.3227 - val_MinusLogProbMetric: 392.3227 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 456/1000
2023-09-10 12:20:22.888 
Epoch 456/1000 
	 loss: 386.1879, MinusLogProbMetric: 386.1879, val_loss: 392.2406, val_MinusLogProbMetric: 392.2406

Epoch 456: val_loss did not improve from 392.05219
196/196 - 22s - loss: 386.1879 - MinusLogProbMetric: 386.1879 - val_loss: 392.2406 - val_MinusLogProbMetric: 392.2406 - lr: 4.1667e-05 - 22s/epoch - 111ms/step
Epoch 457/1000
2023-09-10 12:20:45.372 
Epoch 457/1000 
	 loss: 386.3070, MinusLogProbMetric: 386.3070, val_loss: 392.7447, val_MinusLogProbMetric: 392.7447

Epoch 457: val_loss did not improve from 392.05219
196/196 - 22s - loss: 386.3070 - MinusLogProbMetric: 386.3070 - val_loss: 392.7447 - val_MinusLogProbMetric: 392.7447 - lr: 4.1667e-05 - 22s/epoch - 115ms/step
Epoch 458/1000
2023-09-10 12:21:08.858 
Epoch 458/1000 
	 loss: 386.2039, MinusLogProbMetric: 386.2039, val_loss: 392.4899, val_MinusLogProbMetric: 392.4899

Epoch 458: val_loss did not improve from 392.05219
196/196 - 23s - loss: 386.2039 - MinusLogProbMetric: 386.2039 - val_loss: 392.4899 - val_MinusLogProbMetric: 392.4899 - lr: 4.1667e-05 - 23s/epoch - 120ms/step
Epoch 459/1000
2023-09-10 12:21:32.207 
Epoch 459/1000 
	 loss: 386.2397, MinusLogProbMetric: 386.2397, val_loss: 392.0712, val_MinusLogProbMetric: 392.0712

Epoch 459: val_loss did not improve from 392.05219
196/196 - 23s - loss: 386.2397 - MinusLogProbMetric: 386.2397 - val_loss: 392.0712 - val_MinusLogProbMetric: 392.0712 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 460/1000
2023-09-10 12:21:54.553 
Epoch 460/1000 
	 loss: 386.2394, MinusLogProbMetric: 386.2394, val_loss: 392.6291, val_MinusLogProbMetric: 392.6291

Epoch 460: val_loss did not improve from 392.05219
196/196 - 22s - loss: 386.2394 - MinusLogProbMetric: 386.2394 - val_loss: 392.6291 - val_MinusLogProbMetric: 392.6291 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 461/1000
2023-09-10 12:22:17.409 
Epoch 461/1000 
	 loss: 386.4905, MinusLogProbMetric: 386.4905, val_loss: 393.5878, val_MinusLogProbMetric: 393.5878

Epoch 461: val_loss did not improve from 392.05219
196/196 - 23s - loss: 386.4905 - MinusLogProbMetric: 386.4905 - val_loss: 393.5878 - val_MinusLogProbMetric: 393.5878 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 462/1000
2023-09-10 12:22:41.149 
Epoch 462/1000 
	 loss: 386.2432, MinusLogProbMetric: 386.2432, val_loss: 392.4337, val_MinusLogProbMetric: 392.4337

Epoch 462: val_loss did not improve from 392.05219
196/196 - 24s - loss: 386.2432 - MinusLogProbMetric: 386.2432 - val_loss: 392.4337 - val_MinusLogProbMetric: 392.4337 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 463/1000
2023-09-10 12:23:03.831 
Epoch 463/1000 
	 loss: 386.2934, MinusLogProbMetric: 386.2934, val_loss: 391.9004, val_MinusLogProbMetric: 391.9004

Epoch 463: val_loss improved from 392.05219 to 391.90039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 386.2934 - MinusLogProbMetric: 386.2934 - val_loss: 391.9004 - val_MinusLogProbMetric: 391.9004 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 464/1000
2023-09-10 12:23:27.438 
Epoch 464/1000 
	 loss: 386.3103, MinusLogProbMetric: 386.3103, val_loss: 392.0754, val_MinusLogProbMetric: 392.0754

Epoch 464: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.3103 - MinusLogProbMetric: 386.3103 - val_loss: 392.0754 - val_MinusLogProbMetric: 392.0754 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 465/1000
2023-09-10 12:23:50.122 
Epoch 465/1000 
	 loss: 386.2709, MinusLogProbMetric: 386.2709, val_loss: 392.3516, val_MinusLogProbMetric: 392.3516

Epoch 465: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.2709 - MinusLogProbMetric: 386.2709 - val_loss: 392.3516 - val_MinusLogProbMetric: 392.3516 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 466/1000
2023-09-10 12:24:12.634 
Epoch 466/1000 
	 loss: 386.2333, MinusLogProbMetric: 386.2333, val_loss: 392.2868, val_MinusLogProbMetric: 392.2868

Epoch 466: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.2333 - MinusLogProbMetric: 386.2333 - val_loss: 392.2868 - val_MinusLogProbMetric: 392.2868 - lr: 4.1667e-05 - 22s/epoch - 115ms/step
Epoch 467/1000
2023-09-10 12:24:35.411 
Epoch 467/1000 
	 loss: 386.2158, MinusLogProbMetric: 386.2158, val_loss: 393.4355, val_MinusLogProbMetric: 393.4355

Epoch 467: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.2158 - MinusLogProbMetric: 386.2158 - val_loss: 393.4355 - val_MinusLogProbMetric: 393.4355 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 468/1000
2023-09-10 12:24:57.486 
Epoch 468/1000 
	 loss: 386.3976, MinusLogProbMetric: 386.3976, val_loss: 392.0410, val_MinusLogProbMetric: 392.0410

Epoch 468: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.3976 - MinusLogProbMetric: 386.3976 - val_loss: 392.0410 - val_MinusLogProbMetric: 392.0410 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 469/1000
2023-09-10 12:25:20.787 
Epoch 469/1000 
	 loss: 386.1848, MinusLogProbMetric: 386.1848, val_loss: 392.0986, val_MinusLogProbMetric: 392.0986

Epoch 469: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1848 - MinusLogProbMetric: 386.1848 - val_loss: 392.0986 - val_MinusLogProbMetric: 392.0986 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 470/1000
2023-09-10 12:25:43.174 
Epoch 470/1000 
	 loss: 386.1931, MinusLogProbMetric: 386.1931, val_loss: 392.1679, val_MinusLogProbMetric: 392.1679

Epoch 470: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.1931 - MinusLogProbMetric: 386.1931 - val_loss: 392.1679 - val_MinusLogProbMetric: 392.1679 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 471/1000
2023-09-10 12:26:07.204 
Epoch 471/1000 
	 loss: 386.1873, MinusLogProbMetric: 386.1873, val_loss: 392.8557, val_MinusLogProbMetric: 392.8557

Epoch 471: val_loss did not improve from 391.90039
196/196 - 24s - loss: 386.1873 - MinusLogProbMetric: 386.1873 - val_loss: 392.8557 - val_MinusLogProbMetric: 392.8557 - lr: 4.1667e-05 - 24s/epoch - 123ms/step
Epoch 472/1000
2023-09-10 12:26:29.834 
Epoch 472/1000 
	 loss: 386.1932, MinusLogProbMetric: 386.1932, val_loss: 392.1229, val_MinusLogProbMetric: 392.1229

Epoch 472: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1932 - MinusLogProbMetric: 386.1932 - val_loss: 392.1229 - val_MinusLogProbMetric: 392.1229 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 473/1000
2023-09-10 12:26:51.453 
Epoch 473/1000 
	 loss: 386.2598, MinusLogProbMetric: 386.2598, val_loss: 392.4620, val_MinusLogProbMetric: 392.4620

Epoch 473: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.2598 - MinusLogProbMetric: 386.2598 - val_loss: 392.4620 - val_MinusLogProbMetric: 392.4620 - lr: 4.1667e-05 - 22s/epoch - 110ms/step
Epoch 474/1000
2023-09-10 12:27:13.682 
Epoch 474/1000 
	 loss: 386.1473, MinusLogProbMetric: 386.1473, val_loss: 392.1391, val_MinusLogProbMetric: 392.1391

Epoch 474: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.1473 - MinusLogProbMetric: 386.1473 - val_loss: 392.1391 - val_MinusLogProbMetric: 392.1391 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 475/1000
2023-09-10 12:27:36.887 
Epoch 475/1000 
	 loss: 386.1884, MinusLogProbMetric: 386.1884, val_loss: 392.2594, val_MinusLogProbMetric: 392.2594

Epoch 475: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1884 - MinusLogProbMetric: 386.1884 - val_loss: 392.2594 - val_MinusLogProbMetric: 392.2594 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 476/1000
2023-09-10 12:27:58.821 
Epoch 476/1000 
	 loss: 386.0455, MinusLogProbMetric: 386.0455, val_loss: 391.9552, val_MinusLogProbMetric: 391.9552

Epoch 476: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.0455 - MinusLogProbMetric: 386.0455 - val_loss: 391.9552 - val_MinusLogProbMetric: 391.9552 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 477/1000
2023-09-10 12:28:20.739 
Epoch 477/1000 
	 loss: 386.2509, MinusLogProbMetric: 386.2509, val_loss: 392.2005, val_MinusLogProbMetric: 392.2005

Epoch 477: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.2509 - MinusLogProbMetric: 386.2509 - val_loss: 392.2005 - val_MinusLogProbMetric: 392.2005 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 478/1000
2023-09-10 12:28:42.839 
Epoch 478/1000 
	 loss: 386.2977, MinusLogProbMetric: 386.2977, val_loss: 392.9193, val_MinusLogProbMetric: 392.9193

Epoch 478: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.2977 - MinusLogProbMetric: 386.2977 - val_loss: 392.9193 - val_MinusLogProbMetric: 392.9193 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 479/1000
2023-09-10 12:29:05.657 
Epoch 479/1000 
	 loss: 386.1857, MinusLogProbMetric: 386.1857, val_loss: 392.1579, val_MinusLogProbMetric: 392.1579

Epoch 479: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1857 - MinusLogProbMetric: 386.1857 - val_loss: 392.1579 - val_MinusLogProbMetric: 392.1579 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 480/1000
2023-09-10 12:29:27.590 
Epoch 480/1000 
	 loss: 386.3177, MinusLogProbMetric: 386.3177, val_loss: 392.2732, val_MinusLogProbMetric: 392.2732

Epoch 480: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.3177 - MinusLogProbMetric: 386.3177 - val_loss: 392.2732 - val_MinusLogProbMetric: 392.2732 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 481/1000
2023-09-10 12:29:49.650 
Epoch 481/1000 
	 loss: 386.2582, MinusLogProbMetric: 386.2582, val_loss: 392.2197, val_MinusLogProbMetric: 392.2197

Epoch 481: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.2582 - MinusLogProbMetric: 386.2582 - val_loss: 392.2197 - val_MinusLogProbMetric: 392.2197 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 482/1000
2023-09-10 12:30:11.427 
Epoch 482/1000 
	 loss: 386.1467, MinusLogProbMetric: 386.1467, val_loss: 392.2042, val_MinusLogProbMetric: 392.2042

Epoch 482: val_loss did not improve from 391.90039
196/196 - 22s - loss: 386.1467 - MinusLogProbMetric: 386.1467 - val_loss: 392.2042 - val_MinusLogProbMetric: 392.2042 - lr: 4.1667e-05 - 22s/epoch - 111ms/step
Epoch 483/1000
2023-09-10 12:30:34.607 
Epoch 483/1000 
	 loss: 386.2504, MinusLogProbMetric: 386.2504, val_loss: 392.2319, val_MinusLogProbMetric: 392.2319

Epoch 483: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.2504 - MinusLogProbMetric: 386.2504 - val_loss: 392.2319 - val_MinusLogProbMetric: 392.2319 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 484/1000
2023-09-10 12:30:57.265 
Epoch 484/1000 
	 loss: 386.1122, MinusLogProbMetric: 386.1122, val_loss: 392.4707, val_MinusLogProbMetric: 392.4707

Epoch 484: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1122 - MinusLogProbMetric: 386.1122 - val_loss: 392.4707 - val_MinusLogProbMetric: 392.4707 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 485/1000
2023-09-10 12:31:20.002 
Epoch 485/1000 
	 loss: 386.1601, MinusLogProbMetric: 386.1601, val_loss: 393.4094, val_MinusLogProbMetric: 393.4094

Epoch 485: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1601 - MinusLogProbMetric: 386.1601 - val_loss: 393.4094 - val_MinusLogProbMetric: 393.4094 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 486/1000
2023-09-10 12:31:42.848 
Epoch 486/1000 
	 loss: 386.1652, MinusLogProbMetric: 386.1652, val_loss: 392.1162, val_MinusLogProbMetric: 392.1162

Epoch 486: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1652 - MinusLogProbMetric: 386.1652 - val_loss: 392.1162 - val_MinusLogProbMetric: 392.1162 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 487/1000
2023-09-10 12:32:06.097 
Epoch 487/1000 
	 loss: 386.1922, MinusLogProbMetric: 386.1922, val_loss: 392.4450, val_MinusLogProbMetric: 392.4450

Epoch 487: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1922 - MinusLogProbMetric: 386.1922 - val_loss: 392.4450 - val_MinusLogProbMetric: 392.4450 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 488/1000
2023-09-10 12:32:28.726 
Epoch 488/1000 
	 loss: 386.4794, MinusLogProbMetric: 386.4794, val_loss: 392.5804, val_MinusLogProbMetric: 392.5804

Epoch 488: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.4794 - MinusLogProbMetric: 386.4794 - val_loss: 392.5804 - val_MinusLogProbMetric: 392.5804 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 489/1000
2023-09-10 12:32:52.268 
Epoch 489/1000 
	 loss: 386.1206, MinusLogProbMetric: 386.1206, val_loss: 392.3240, val_MinusLogProbMetric: 392.3240

Epoch 489: val_loss did not improve from 391.90039
196/196 - 24s - loss: 386.1206 - MinusLogProbMetric: 386.1206 - val_loss: 392.3240 - val_MinusLogProbMetric: 392.3240 - lr: 4.1667e-05 - 24s/epoch - 120ms/step
Epoch 490/1000
2023-09-10 12:33:16.192 
Epoch 490/1000 
	 loss: 386.7512, MinusLogProbMetric: 386.7512, val_loss: 393.3872, val_MinusLogProbMetric: 393.3872

Epoch 490: val_loss did not improve from 391.90039
196/196 - 24s - loss: 386.7512 - MinusLogProbMetric: 386.7512 - val_loss: 393.3872 - val_MinusLogProbMetric: 393.3872 - lr: 4.1667e-05 - 24s/epoch - 122ms/step
Epoch 491/1000
2023-09-10 12:33:39.143 
Epoch 491/1000 
	 loss: 386.3689, MinusLogProbMetric: 386.3689, val_loss: 392.6173, val_MinusLogProbMetric: 392.6173

Epoch 491: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.3689 - MinusLogProbMetric: 386.3689 - val_loss: 392.6173 - val_MinusLogProbMetric: 392.6173 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 492/1000
2023-09-10 12:34:02.949 
Epoch 492/1000 
	 loss: 386.5634, MinusLogProbMetric: 386.5634, val_loss: 392.2686, val_MinusLogProbMetric: 392.2686

Epoch 492: val_loss did not improve from 391.90039
196/196 - 24s - loss: 386.5634 - MinusLogProbMetric: 386.5634 - val_loss: 392.2686 - val_MinusLogProbMetric: 392.2686 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 493/1000
2023-09-10 12:34:25.509 
Epoch 493/1000 
	 loss: 386.5694, MinusLogProbMetric: 386.5694, val_loss: 392.7948, val_MinusLogProbMetric: 392.7948

Epoch 493: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.5694 - MinusLogProbMetric: 386.5694 - val_loss: 392.7948 - val_MinusLogProbMetric: 392.7948 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 494/1000
2023-09-10 12:34:48.290 
Epoch 494/1000 
	 loss: 386.1582, MinusLogProbMetric: 386.1582, val_loss: 392.3003, val_MinusLogProbMetric: 392.3003

Epoch 494: val_loss did not improve from 391.90039
196/196 - 23s - loss: 386.1582 - MinusLogProbMetric: 386.1582 - val_loss: 392.3003 - val_MinusLogProbMetric: 392.3003 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 495/1000
2023-09-10 12:35:10.429 
Epoch 495/1000 
	 loss: 386.0300, MinusLogProbMetric: 386.0300, val_loss: 391.8963, val_MinusLogProbMetric: 391.8963

Epoch 495: val_loss improved from 391.90039 to 391.89633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 386.0300 - MinusLogProbMetric: 386.0300 - val_loss: 391.8963 - val_MinusLogProbMetric: 391.8963 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 496/1000
2023-09-10 12:35:34.702 
Epoch 496/1000 
	 loss: 386.1161, MinusLogProbMetric: 386.1161, val_loss: 392.3182, val_MinusLogProbMetric: 392.3182

Epoch 496: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.1161 - MinusLogProbMetric: 386.1161 - val_loss: 392.3182 - val_MinusLogProbMetric: 392.3182 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 497/1000
2023-09-10 12:35:58.869 
Epoch 497/1000 
	 loss: 386.1584, MinusLogProbMetric: 386.1584, val_loss: 392.1863, val_MinusLogProbMetric: 392.1863

Epoch 497: val_loss did not improve from 391.89633
196/196 - 24s - loss: 386.1584 - MinusLogProbMetric: 386.1584 - val_loss: 392.1863 - val_MinusLogProbMetric: 392.1863 - lr: 4.1667e-05 - 24s/epoch - 123ms/step
Epoch 498/1000
2023-09-10 12:36:21.137 
Epoch 498/1000 
	 loss: 386.0919, MinusLogProbMetric: 386.0919, val_loss: 392.6753, val_MinusLogProbMetric: 392.6753

Epoch 498: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.0919 - MinusLogProbMetric: 386.0919 - val_loss: 392.6753 - val_MinusLogProbMetric: 392.6753 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 499/1000
2023-09-10 12:36:43.111 
Epoch 499/1000 
	 loss: 386.1868, MinusLogProbMetric: 386.1868, val_loss: 392.0135, val_MinusLogProbMetric: 392.0135

Epoch 499: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.1868 - MinusLogProbMetric: 386.1868 - val_loss: 392.0135 - val_MinusLogProbMetric: 392.0135 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 500/1000
2023-09-10 12:37:06.653 
Epoch 500/1000 
	 loss: 386.1807, MinusLogProbMetric: 386.1807, val_loss: 392.4462, val_MinusLogProbMetric: 392.4462

Epoch 500: val_loss did not improve from 391.89633
196/196 - 24s - loss: 386.1807 - MinusLogProbMetric: 386.1807 - val_loss: 392.4462 - val_MinusLogProbMetric: 392.4462 - lr: 4.1667e-05 - 24s/epoch - 120ms/step
Epoch 501/1000
2023-09-10 12:37:29.414 
Epoch 501/1000 
	 loss: 386.1010, MinusLogProbMetric: 386.1010, val_loss: 392.0379, val_MinusLogProbMetric: 392.0379

Epoch 501: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.1010 - MinusLogProbMetric: 386.1010 - val_loss: 392.0379 - val_MinusLogProbMetric: 392.0379 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 502/1000
2023-09-10 12:37:51.558 
Epoch 502/1000 
	 loss: 386.1312, MinusLogProbMetric: 386.1312, val_loss: 393.2184, val_MinusLogProbMetric: 393.2184

Epoch 502: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.1312 - MinusLogProbMetric: 386.1312 - val_loss: 393.2184 - val_MinusLogProbMetric: 393.2184 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 503/1000
2023-09-10 12:38:13.005 
Epoch 503/1000 
	 loss: 386.3022, MinusLogProbMetric: 386.3022, val_loss: 392.2578, val_MinusLogProbMetric: 392.2578

Epoch 503: val_loss did not improve from 391.89633
196/196 - 21s - loss: 386.3022 - MinusLogProbMetric: 386.3022 - val_loss: 392.2578 - val_MinusLogProbMetric: 392.2578 - lr: 4.1667e-05 - 21s/epoch - 109ms/step
Epoch 504/1000
2023-09-10 12:38:35.091 
Epoch 504/1000 
	 loss: 386.0659, MinusLogProbMetric: 386.0659, val_loss: 392.3669, val_MinusLogProbMetric: 392.3669

Epoch 504: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.0659 - MinusLogProbMetric: 386.0659 - val_loss: 392.3669 - val_MinusLogProbMetric: 392.3669 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 505/1000
2023-09-10 12:38:57.740 
Epoch 505/1000 
	 loss: 386.3403, MinusLogProbMetric: 386.3403, val_loss: 392.3302, val_MinusLogProbMetric: 392.3302

Epoch 505: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.3403 - MinusLogProbMetric: 386.3403 - val_loss: 392.3302 - val_MinusLogProbMetric: 392.3302 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 506/1000
2023-09-10 12:39:20.855 
Epoch 506/1000 
	 loss: 386.3029, MinusLogProbMetric: 386.3029, val_loss: 392.7801, val_MinusLogProbMetric: 392.7801

Epoch 506: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.3029 - MinusLogProbMetric: 386.3029 - val_loss: 392.7801 - val_MinusLogProbMetric: 392.7801 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 507/1000
2023-09-10 12:39:42.478 
Epoch 507/1000 
	 loss: 386.3470, MinusLogProbMetric: 386.3470, val_loss: 392.1437, val_MinusLogProbMetric: 392.1437

Epoch 507: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.3470 - MinusLogProbMetric: 386.3470 - val_loss: 392.1437 - val_MinusLogProbMetric: 392.1437 - lr: 4.1667e-05 - 22s/epoch - 110ms/step
Epoch 508/1000
2023-09-10 12:40:04.703 
Epoch 508/1000 
	 loss: 386.3860, MinusLogProbMetric: 386.3860, val_loss: 392.2252, val_MinusLogProbMetric: 392.2252

Epoch 508: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.3860 - MinusLogProbMetric: 386.3860 - val_loss: 392.2252 - val_MinusLogProbMetric: 392.2252 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 509/1000
2023-09-10 12:40:27.329 
Epoch 509/1000 
	 loss: 386.2173, MinusLogProbMetric: 386.2173, val_loss: 393.0608, val_MinusLogProbMetric: 393.0608

Epoch 509: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.2173 - MinusLogProbMetric: 386.2173 - val_loss: 393.0608 - val_MinusLogProbMetric: 393.0608 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 510/1000
2023-09-10 12:40:49.525 
Epoch 510/1000 
	 loss: 386.1632, MinusLogProbMetric: 386.1632, val_loss: 392.1118, val_MinusLogProbMetric: 392.1118

Epoch 510: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.1632 - MinusLogProbMetric: 386.1632 - val_loss: 392.1118 - val_MinusLogProbMetric: 392.1118 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 511/1000
2023-09-10 12:41:11.872 
Epoch 511/1000 
	 loss: 386.0524, MinusLogProbMetric: 386.0524, val_loss: 392.0105, val_MinusLogProbMetric: 392.0105

Epoch 511: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.0524 - MinusLogProbMetric: 386.0524 - val_loss: 392.0105 - val_MinusLogProbMetric: 392.0105 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 512/1000
2023-09-10 12:41:35.132 
Epoch 512/1000 
	 loss: 385.9245, MinusLogProbMetric: 385.9245, val_loss: 392.0688, val_MinusLogProbMetric: 392.0688

Epoch 512: val_loss did not improve from 391.89633
196/196 - 23s - loss: 385.9245 - MinusLogProbMetric: 385.9245 - val_loss: 392.0688 - val_MinusLogProbMetric: 392.0688 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 513/1000
2023-09-10 12:41:58.654 
Epoch 513/1000 
	 loss: 386.1810, MinusLogProbMetric: 386.1810, val_loss: 392.1137, val_MinusLogProbMetric: 392.1137

Epoch 513: val_loss did not improve from 391.89633
196/196 - 24s - loss: 386.1810 - MinusLogProbMetric: 386.1810 - val_loss: 392.1137 - val_MinusLogProbMetric: 392.1137 - lr: 4.1667e-05 - 24s/epoch - 120ms/step
Epoch 514/1000
2023-09-10 12:42:21.194 
Epoch 514/1000 
	 loss: 386.2511, MinusLogProbMetric: 386.2511, val_loss: 392.3504, val_MinusLogProbMetric: 392.3504

Epoch 514: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.2511 - MinusLogProbMetric: 386.2511 - val_loss: 392.3504 - val_MinusLogProbMetric: 392.3504 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 515/1000
2023-09-10 12:42:43.551 
Epoch 515/1000 
	 loss: 386.2351, MinusLogProbMetric: 386.2351, val_loss: 392.5209, val_MinusLogProbMetric: 392.5209

Epoch 515: val_loss did not improve from 391.89633
196/196 - 22s - loss: 386.2351 - MinusLogProbMetric: 386.2351 - val_loss: 392.5209 - val_MinusLogProbMetric: 392.5209 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 516/1000
2023-09-10 12:43:06.270 
Epoch 516/1000 
	 loss: 386.1499, MinusLogProbMetric: 386.1499, val_loss: 392.2238, val_MinusLogProbMetric: 392.2238

Epoch 516: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.1499 - MinusLogProbMetric: 386.1499 - val_loss: 392.2238 - val_MinusLogProbMetric: 392.2238 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 517/1000
2023-09-10 12:43:29.184 
Epoch 517/1000 
	 loss: 386.0801, MinusLogProbMetric: 386.0801, val_loss: 392.1899, val_MinusLogProbMetric: 392.1899

Epoch 517: val_loss did not improve from 391.89633
196/196 - 23s - loss: 386.0801 - MinusLogProbMetric: 386.0801 - val_loss: 392.1899 - val_MinusLogProbMetric: 392.1899 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 518/1000
2023-09-10 12:43:51.226 
Epoch 518/1000 
	 loss: 385.9474, MinusLogProbMetric: 385.9474, val_loss: 391.8450, val_MinusLogProbMetric: 391.8450

Epoch 518: val_loss improved from 391.89633 to 391.84500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 385.9474 - MinusLogProbMetric: 385.9474 - val_loss: 391.8450 - val_MinusLogProbMetric: 391.8450 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 519/1000
2023-09-10 12:44:13.949 
Epoch 519/1000 
	 loss: 386.1175, MinusLogProbMetric: 386.1175, val_loss: 392.6022, val_MinusLogProbMetric: 392.6022

Epoch 519: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.1175 - MinusLogProbMetric: 386.1175 - val_loss: 392.6022 - val_MinusLogProbMetric: 392.6022 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 520/1000
2023-09-10 12:44:39.992 
Epoch 520/1000 
	 loss: 385.8506, MinusLogProbMetric: 385.8506, val_loss: 392.2691, val_MinusLogProbMetric: 392.2691

Epoch 520: val_loss did not improve from 391.84500
196/196 - 26s - loss: 385.8506 - MinusLogProbMetric: 385.8506 - val_loss: 392.2691 - val_MinusLogProbMetric: 392.2691 - lr: 4.1667e-05 - 26s/epoch - 133ms/step
Epoch 521/1000
2023-09-10 12:45:02.520 
Epoch 521/1000 
	 loss: 385.8827, MinusLogProbMetric: 385.8827, val_loss: 392.4344, val_MinusLogProbMetric: 392.4344

Epoch 521: val_loss did not improve from 391.84500
196/196 - 22s - loss: 385.8827 - MinusLogProbMetric: 385.8827 - val_loss: 392.4344 - val_MinusLogProbMetric: 392.4344 - lr: 4.1667e-05 - 22s/epoch - 115ms/step
Epoch 522/1000
2023-09-10 12:45:25.552 
Epoch 522/1000 
	 loss: 385.8784, MinusLogProbMetric: 385.8784, val_loss: 392.3403, val_MinusLogProbMetric: 392.3403

Epoch 522: val_loss did not improve from 391.84500
196/196 - 23s - loss: 385.8784 - MinusLogProbMetric: 385.8784 - val_loss: 392.3403 - val_MinusLogProbMetric: 392.3403 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 523/1000
2023-09-10 12:45:48.109 
Epoch 523/1000 
	 loss: 386.3951, MinusLogProbMetric: 386.3951, val_loss: 392.1894, val_MinusLogProbMetric: 392.1894

Epoch 523: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.3951 - MinusLogProbMetric: 386.3951 - val_loss: 392.1894 - val_MinusLogProbMetric: 392.1894 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 524/1000
2023-09-10 12:46:10.950 
Epoch 524/1000 
	 loss: 386.1143, MinusLogProbMetric: 386.1143, val_loss: 392.0473, val_MinusLogProbMetric: 392.0473

Epoch 524: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1143 - MinusLogProbMetric: 386.1143 - val_loss: 392.0473 - val_MinusLogProbMetric: 392.0473 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 525/1000
2023-09-10 12:46:33.834 
Epoch 525/1000 
	 loss: 386.1624, MinusLogProbMetric: 386.1624, val_loss: 392.3547, val_MinusLogProbMetric: 392.3547

Epoch 525: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1624 - MinusLogProbMetric: 386.1624 - val_loss: 392.3547 - val_MinusLogProbMetric: 392.3547 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 526/1000
2023-09-10 12:46:57.025 
Epoch 526/1000 
	 loss: 386.3049, MinusLogProbMetric: 386.3049, val_loss: 392.3188, val_MinusLogProbMetric: 392.3188

Epoch 526: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.3049 - MinusLogProbMetric: 386.3049 - val_loss: 392.3188 - val_MinusLogProbMetric: 392.3188 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 527/1000
2023-09-10 12:47:19.545 
Epoch 527/1000 
	 loss: 386.3141, MinusLogProbMetric: 386.3141, val_loss: 392.7122, val_MinusLogProbMetric: 392.7122

Epoch 527: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.3141 - MinusLogProbMetric: 386.3141 - val_loss: 392.7122 - val_MinusLogProbMetric: 392.7122 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 528/1000
2023-09-10 12:47:42.157 
Epoch 528/1000 
	 loss: 386.1791, MinusLogProbMetric: 386.1791, val_loss: 392.5176, val_MinusLogProbMetric: 392.5176

Epoch 528: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1791 - MinusLogProbMetric: 386.1791 - val_loss: 392.5176 - val_MinusLogProbMetric: 392.5176 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 529/1000
2023-09-10 12:48:04.558 
Epoch 529/1000 
	 loss: 386.3396, MinusLogProbMetric: 386.3396, val_loss: 392.6207, val_MinusLogProbMetric: 392.6207

Epoch 529: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.3396 - MinusLogProbMetric: 386.3396 - val_loss: 392.6207 - val_MinusLogProbMetric: 392.6207 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 530/1000
2023-09-10 12:48:27.475 
Epoch 530/1000 
	 loss: 386.2566, MinusLogProbMetric: 386.2566, val_loss: 393.0250, val_MinusLogProbMetric: 393.0250

Epoch 530: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.2566 - MinusLogProbMetric: 386.2566 - val_loss: 393.0250 - val_MinusLogProbMetric: 393.0250 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 531/1000
2023-09-10 12:48:51.369 
Epoch 531/1000 
	 loss: 386.3880, MinusLogProbMetric: 386.3880, val_loss: 392.1733, val_MinusLogProbMetric: 392.1733

Epoch 531: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.3880 - MinusLogProbMetric: 386.3880 - val_loss: 392.1733 - val_MinusLogProbMetric: 392.1733 - lr: 4.1667e-05 - 24s/epoch - 122ms/step
Epoch 532/1000
2023-09-10 12:49:13.374 
Epoch 532/1000 
	 loss: 386.1752, MinusLogProbMetric: 386.1752, val_loss: 392.0051, val_MinusLogProbMetric: 392.0051

Epoch 532: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.1752 - MinusLogProbMetric: 386.1752 - val_loss: 392.0051 - val_MinusLogProbMetric: 392.0051 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 533/1000
2023-09-10 12:49:37.402 
Epoch 533/1000 
	 loss: 386.1404, MinusLogProbMetric: 386.1404, val_loss: 392.3397, val_MinusLogProbMetric: 392.3397

Epoch 533: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.1404 - MinusLogProbMetric: 386.1404 - val_loss: 392.3397 - val_MinusLogProbMetric: 392.3397 - lr: 4.1667e-05 - 24s/epoch - 122ms/step
Epoch 534/1000
2023-09-10 12:50:00.387 
Epoch 534/1000 
	 loss: 386.0046, MinusLogProbMetric: 386.0046, val_loss: 392.7773, val_MinusLogProbMetric: 392.7773

Epoch 534: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.0046 - MinusLogProbMetric: 386.0046 - val_loss: 392.7773 - val_MinusLogProbMetric: 392.7773 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 535/1000
2023-09-10 12:50:24.589 
Epoch 535/1000 
	 loss: 386.0945, MinusLogProbMetric: 386.0945, val_loss: 393.6173, val_MinusLogProbMetric: 393.6173

Epoch 535: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.0945 - MinusLogProbMetric: 386.0945 - val_loss: 393.6173 - val_MinusLogProbMetric: 393.6173 - lr: 4.1667e-05 - 24s/epoch - 123ms/step
Epoch 536/1000
2023-09-10 12:50:47.229 
Epoch 536/1000 
	 loss: 386.1692, MinusLogProbMetric: 386.1692, val_loss: 393.2627, val_MinusLogProbMetric: 393.2627

Epoch 536: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1692 - MinusLogProbMetric: 386.1692 - val_loss: 393.2627 - val_MinusLogProbMetric: 393.2627 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 537/1000
2023-09-10 12:51:10.952 
Epoch 537/1000 
	 loss: 385.9714, MinusLogProbMetric: 385.9714, val_loss: 391.9859, val_MinusLogProbMetric: 391.9859

Epoch 537: val_loss did not improve from 391.84500
196/196 - 24s - loss: 385.9714 - MinusLogProbMetric: 385.9714 - val_loss: 391.9859 - val_MinusLogProbMetric: 391.9859 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 538/1000
2023-09-10 12:51:33.635 
Epoch 538/1000 
	 loss: 386.1904, MinusLogProbMetric: 386.1904, val_loss: 392.1529, val_MinusLogProbMetric: 392.1529

Epoch 538: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1904 - MinusLogProbMetric: 386.1904 - val_loss: 392.1529 - val_MinusLogProbMetric: 392.1529 - lr: 4.1667e-05 - 23s/epoch - 116ms/step
Epoch 539/1000
2023-09-10 12:51:55.851 
Epoch 539/1000 
	 loss: 386.1361, MinusLogProbMetric: 386.1361, val_loss: 393.6784, val_MinusLogProbMetric: 393.6784

Epoch 539: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.1361 - MinusLogProbMetric: 386.1361 - val_loss: 393.6784 - val_MinusLogProbMetric: 393.6784 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 540/1000
2023-09-10 12:52:18.862 
Epoch 540/1000 
	 loss: 386.0623, MinusLogProbMetric: 386.0623, val_loss: 392.4865, val_MinusLogProbMetric: 392.4865

Epoch 540: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.0623 - MinusLogProbMetric: 386.0623 - val_loss: 392.4865 - val_MinusLogProbMetric: 392.4865 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 541/1000
2023-09-10 12:52:41.079 
Epoch 541/1000 
	 loss: 386.1335, MinusLogProbMetric: 386.1335, val_loss: 392.4550, val_MinusLogProbMetric: 392.4550

Epoch 541: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.1335 - MinusLogProbMetric: 386.1335 - val_loss: 392.4550 - val_MinusLogProbMetric: 392.4550 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 542/1000
2023-09-10 12:53:04.260 
Epoch 542/1000 
	 loss: 386.1872, MinusLogProbMetric: 386.1872, val_loss: 392.5238, val_MinusLogProbMetric: 392.5238

Epoch 542: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1872 - MinusLogProbMetric: 386.1872 - val_loss: 392.5238 - val_MinusLogProbMetric: 392.5238 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 543/1000
2023-09-10 12:53:26.664 
Epoch 543/1000 
	 loss: 386.1559, MinusLogProbMetric: 386.1559, val_loss: 392.1412, val_MinusLogProbMetric: 392.1412

Epoch 543: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.1559 - MinusLogProbMetric: 386.1559 - val_loss: 392.1412 - val_MinusLogProbMetric: 392.1412 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 544/1000
2023-09-10 12:53:50.308 
Epoch 544/1000 
	 loss: 386.1663, MinusLogProbMetric: 386.1663, val_loss: 392.1340, val_MinusLogProbMetric: 392.1340

Epoch 544: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.1663 - MinusLogProbMetric: 386.1663 - val_loss: 392.1340 - val_MinusLogProbMetric: 392.1340 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 545/1000
2023-09-10 12:54:12.322 
Epoch 545/1000 
	 loss: 385.9333, MinusLogProbMetric: 385.9333, val_loss: 391.9886, val_MinusLogProbMetric: 391.9886

Epoch 545: val_loss did not improve from 391.84500
196/196 - 22s - loss: 385.9333 - MinusLogProbMetric: 385.9333 - val_loss: 391.9886 - val_MinusLogProbMetric: 391.9886 - lr: 4.1667e-05 - 22s/epoch - 112ms/step
Epoch 546/1000
2023-09-10 12:54:36.365 
Epoch 546/1000 
	 loss: 385.9458, MinusLogProbMetric: 385.9458, val_loss: 392.1892, val_MinusLogProbMetric: 392.1892

Epoch 546: val_loss did not improve from 391.84500
196/196 - 24s - loss: 385.9458 - MinusLogProbMetric: 385.9458 - val_loss: 392.1892 - val_MinusLogProbMetric: 392.1892 - lr: 4.1667e-05 - 24s/epoch - 123ms/step
Epoch 547/1000
2023-09-10 12:55:00.640 
Epoch 547/1000 
	 loss: 386.1389, MinusLogProbMetric: 386.1389, val_loss: 392.6170, val_MinusLogProbMetric: 392.6170

Epoch 547: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.1389 - MinusLogProbMetric: 386.1389 - val_loss: 392.6170 - val_MinusLogProbMetric: 392.6170 - lr: 4.1667e-05 - 24s/epoch - 124ms/step
Epoch 548/1000
2023-09-10 12:55:24.972 
Epoch 548/1000 
	 loss: 385.9997, MinusLogProbMetric: 385.9997, val_loss: 391.9472, val_MinusLogProbMetric: 391.9472

Epoch 548: val_loss did not improve from 391.84500
196/196 - 24s - loss: 385.9997 - MinusLogProbMetric: 385.9997 - val_loss: 391.9472 - val_MinusLogProbMetric: 391.9472 - lr: 4.1667e-05 - 24s/epoch - 124ms/step
Epoch 549/1000
2023-09-10 12:55:49.543 
Epoch 549/1000 
	 loss: 386.1456, MinusLogProbMetric: 386.1456, val_loss: 391.9898, val_MinusLogProbMetric: 391.9898

Epoch 549: val_loss did not improve from 391.84500
196/196 - 25s - loss: 386.1456 - MinusLogProbMetric: 386.1456 - val_loss: 391.9898 - val_MinusLogProbMetric: 391.9898 - lr: 4.1667e-05 - 25s/epoch - 125ms/step
Epoch 550/1000
2023-09-10 12:56:12.889 
Epoch 550/1000 
	 loss: 385.9657, MinusLogProbMetric: 385.9657, val_loss: 392.5262, val_MinusLogProbMetric: 392.5262

Epoch 550: val_loss did not improve from 391.84500
196/196 - 23s - loss: 385.9657 - MinusLogProbMetric: 385.9657 - val_loss: 392.5262 - val_MinusLogProbMetric: 392.5262 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 551/1000
2023-09-10 12:56:36.372 
Epoch 551/1000 
	 loss: 386.0694, MinusLogProbMetric: 386.0694, val_loss: 391.9398, val_MinusLogProbMetric: 391.9398

Epoch 551: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.0694 - MinusLogProbMetric: 386.0694 - val_loss: 391.9398 - val_MinusLogProbMetric: 391.9398 - lr: 4.1667e-05 - 23s/epoch - 120ms/step
Epoch 552/1000
2023-09-10 12:57:00.345 
Epoch 552/1000 
	 loss: 385.9443, MinusLogProbMetric: 385.9443, val_loss: 392.4204, val_MinusLogProbMetric: 392.4204

Epoch 552: val_loss did not improve from 391.84500
196/196 - 24s - loss: 385.9443 - MinusLogProbMetric: 385.9443 - val_loss: 392.4204 - val_MinusLogProbMetric: 392.4204 - lr: 4.1667e-05 - 24s/epoch - 122ms/step
Epoch 553/1000
2023-09-10 12:57:23.558 
Epoch 553/1000 
	 loss: 385.8678, MinusLogProbMetric: 385.8678, val_loss: 392.2315, val_MinusLogProbMetric: 392.2315

Epoch 553: val_loss did not improve from 391.84500
196/196 - 23s - loss: 385.8678 - MinusLogProbMetric: 385.8678 - val_loss: 392.2315 - val_MinusLogProbMetric: 392.2315 - lr: 4.1667e-05 - 23s/epoch - 118ms/step
Epoch 554/1000
2023-09-10 12:57:46.877 
Epoch 554/1000 
	 loss: 386.1090, MinusLogProbMetric: 386.1090, val_loss: 392.4237, val_MinusLogProbMetric: 392.4237

Epoch 554: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.1090 - MinusLogProbMetric: 386.1090 - val_loss: 392.4237 - val_MinusLogProbMetric: 392.4237 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 555/1000
2023-09-10 12:58:11.535 
Epoch 555/1000 
	 loss: 385.9502, MinusLogProbMetric: 385.9502, val_loss: 391.9810, val_MinusLogProbMetric: 391.9810

Epoch 555: val_loss did not improve from 391.84500
196/196 - 25s - loss: 385.9502 - MinusLogProbMetric: 385.9502 - val_loss: 391.9810 - val_MinusLogProbMetric: 391.9810 - lr: 4.1667e-05 - 25s/epoch - 126ms/step
Epoch 556/1000
2023-09-10 12:58:34.926 
Epoch 556/1000 
	 loss: 385.8845, MinusLogProbMetric: 385.8845, val_loss: 392.2726, val_MinusLogProbMetric: 392.2726

Epoch 556: val_loss did not improve from 391.84500
196/196 - 23s - loss: 385.8845 - MinusLogProbMetric: 385.8845 - val_loss: 392.2726 - val_MinusLogProbMetric: 392.2726 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 557/1000
2023-09-10 12:58:58.339 
Epoch 557/1000 
	 loss: 386.0820, MinusLogProbMetric: 386.0820, val_loss: 392.3566, val_MinusLogProbMetric: 392.3566

Epoch 557: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.0820 - MinusLogProbMetric: 386.0820 - val_loss: 392.3566 - val_MinusLogProbMetric: 392.3566 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 558/1000
2023-09-10 12:59:21.595 
Epoch 558/1000 
	 loss: 385.8191, MinusLogProbMetric: 385.8191, val_loss: 392.3109, val_MinusLogProbMetric: 392.3109

Epoch 558: val_loss did not improve from 391.84500
196/196 - 23s - loss: 385.8191 - MinusLogProbMetric: 385.8191 - val_loss: 392.3109 - val_MinusLogProbMetric: 392.3109 - lr: 4.1667e-05 - 23s/epoch - 119ms/step
Epoch 559/1000
2023-09-10 12:59:44.630 
Epoch 559/1000 
	 loss: 386.2056, MinusLogProbMetric: 386.2056, val_loss: 391.9650, val_MinusLogProbMetric: 391.9650

Epoch 559: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.2056 - MinusLogProbMetric: 386.2056 - val_loss: 391.9650 - val_MinusLogProbMetric: 391.9650 - lr: 4.1667e-05 - 23s/epoch - 117ms/step
Epoch 560/1000
2023-09-10 13:00:06.873 
Epoch 560/1000 
	 loss: 386.0937, MinusLogProbMetric: 386.0937, val_loss: 392.4154, val_MinusLogProbMetric: 392.4154

Epoch 560: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.0937 - MinusLogProbMetric: 386.0937 - val_loss: 392.4154 - val_MinusLogProbMetric: 392.4154 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 561/1000
2023-09-10 13:00:29.510 
Epoch 561/1000 
	 loss: 386.0264, MinusLogProbMetric: 386.0264, val_loss: 392.1516, val_MinusLogProbMetric: 392.1516

Epoch 561: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.0264 - MinusLogProbMetric: 386.0264 - val_loss: 392.1516 - val_MinusLogProbMetric: 392.1516 - lr: 4.1667e-05 - 23s/epoch - 115ms/step
Epoch 562/1000
2023-09-10 13:00:52.991 
Epoch 562/1000 
	 loss: 386.2254, MinusLogProbMetric: 386.2254, val_loss: 392.3593, val_MinusLogProbMetric: 392.3593

Epoch 562: val_loss did not improve from 391.84500
196/196 - 23s - loss: 386.2254 - MinusLogProbMetric: 386.2254 - val_loss: 392.3593 - val_MinusLogProbMetric: 392.3593 - lr: 4.1667e-05 - 23s/epoch - 120ms/step
Epoch 563/1000
2023-09-10 13:01:16.649 
Epoch 563/1000 
	 loss: 385.9991, MinusLogProbMetric: 385.9991, val_loss: 392.9161, val_MinusLogProbMetric: 392.9161

Epoch 563: val_loss did not improve from 391.84500
196/196 - 24s - loss: 385.9991 - MinusLogProbMetric: 385.9991 - val_loss: 392.9161 - val_MinusLogProbMetric: 392.9161 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 564/1000
2023-09-10 13:01:38.871 
Epoch 564/1000 
	 loss: 385.9631, MinusLogProbMetric: 385.9631, val_loss: 392.2738, val_MinusLogProbMetric: 392.2738

Epoch 564: val_loss did not improve from 391.84500
196/196 - 22s - loss: 385.9631 - MinusLogProbMetric: 385.9631 - val_loss: 392.2738 - val_MinusLogProbMetric: 392.2738 - lr: 4.1667e-05 - 22s/epoch - 113ms/step
Epoch 565/1000
2023-09-10 13:02:02.890 
Epoch 565/1000 
	 loss: 386.1057, MinusLogProbMetric: 386.1057, val_loss: 392.2471, val_MinusLogProbMetric: 392.2471

Epoch 565: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.1057 - MinusLogProbMetric: 386.1057 - val_loss: 392.2471 - val_MinusLogProbMetric: 392.2471 - lr: 4.1667e-05 - 24s/epoch - 123ms/step
Epoch 566/1000
2023-09-10 13:02:25.314 
Epoch 566/1000 
	 loss: 386.3402, MinusLogProbMetric: 386.3402, val_loss: 391.9630, val_MinusLogProbMetric: 391.9630

Epoch 566: val_loss did not improve from 391.84500
196/196 - 22s - loss: 386.3402 - MinusLogProbMetric: 386.3402 - val_loss: 391.9630 - val_MinusLogProbMetric: 391.9630 - lr: 4.1667e-05 - 22s/epoch - 114ms/step
Epoch 567/1000
2023-09-10 13:02:49.948 
Epoch 567/1000 
	 loss: 386.2326, MinusLogProbMetric: 386.2326, val_loss: 394.3701, val_MinusLogProbMetric: 394.3701

Epoch 567: val_loss did not improve from 391.84500
196/196 - 25s - loss: 386.2326 - MinusLogProbMetric: 386.2326 - val_loss: 394.3701 - val_MinusLogProbMetric: 394.3701 - lr: 4.1667e-05 - 25s/epoch - 126ms/step
Epoch 568/1000
2023-09-10 13:03:13.709 
Epoch 568/1000 
	 loss: 386.3383, MinusLogProbMetric: 386.3383, val_loss: 393.5765, val_MinusLogProbMetric: 393.5765

Epoch 568: val_loss did not improve from 391.84500
196/196 - 24s - loss: 386.3383 - MinusLogProbMetric: 386.3383 - val_loss: 393.5765 - val_MinusLogProbMetric: 393.5765 - lr: 4.1667e-05 - 24s/epoch - 121ms/step
Epoch 569/1000
2023-09-10 13:03:36.141 
Epoch 569/1000 
	 loss: 385.4265, MinusLogProbMetric: 385.4265, val_loss: 391.7236, val_MinusLogProbMetric: 391.7236

Epoch 569: val_loss improved from 391.84500 to 391.72363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 385.4265 - MinusLogProbMetric: 385.4265 - val_loss: 391.7236 - val_MinusLogProbMetric: 391.7236 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 570/1000
2023-09-10 13:03:59.895 
Epoch 570/1000 
	 loss: 385.2643, MinusLogProbMetric: 385.2643, val_loss: 391.6953, val_MinusLogProbMetric: 391.6953

Epoch 570: val_loss improved from 391.72363 to 391.69525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 24s - loss: 385.2643 - MinusLogProbMetric: 385.2643 - val_loss: 391.6953 - val_MinusLogProbMetric: 391.6953 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 571/1000
2023-09-10 13:04:24.437 
Epoch 571/1000 
	 loss: 385.2944, MinusLogProbMetric: 385.2944, val_loss: 391.7522, val_MinusLogProbMetric: 391.7522

Epoch 571: val_loss did not improve from 391.69525
196/196 - 24s - loss: 385.2944 - MinusLogProbMetric: 385.2944 - val_loss: 391.7522 - val_MinusLogProbMetric: 391.7522 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 572/1000
2023-09-10 13:04:46.306 
Epoch 572/1000 
	 loss: 385.2762, MinusLogProbMetric: 385.2762, val_loss: 391.6681, val_MinusLogProbMetric: 391.6681

Epoch 572: val_loss improved from 391.69525 to 391.66806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 385.2762 - MinusLogProbMetric: 385.2762 - val_loss: 391.6681 - val_MinusLogProbMetric: 391.6681 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 573/1000
2023-09-10 13:05:12.067 
Epoch 573/1000 
	 loss: 385.2841, MinusLogProbMetric: 385.2841, val_loss: 391.6569, val_MinusLogProbMetric: 391.6569

Epoch 573: val_loss improved from 391.66806 to 391.65692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 26s - loss: 385.2841 - MinusLogProbMetric: 385.2841 - val_loss: 391.6569 - val_MinusLogProbMetric: 391.6569 - lr: 2.0833e-05 - 26s/epoch - 132ms/step
Epoch 574/1000
2023-09-10 13:05:35.609 
Epoch 574/1000 
	 loss: 385.2682, MinusLogProbMetric: 385.2682, val_loss: 391.7063, val_MinusLogProbMetric: 391.7063

Epoch 574: val_loss did not improve from 391.65692
196/196 - 23s - loss: 385.2682 - MinusLogProbMetric: 385.2682 - val_loss: 391.7063 - val_MinusLogProbMetric: 391.7063 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 575/1000
2023-09-10 13:05:57.988 
Epoch 575/1000 
	 loss: 385.2407, MinusLogProbMetric: 385.2407, val_loss: 391.7689, val_MinusLogProbMetric: 391.7689

Epoch 575: val_loss did not improve from 391.65692
196/196 - 22s - loss: 385.2407 - MinusLogProbMetric: 385.2407 - val_loss: 391.7689 - val_MinusLogProbMetric: 391.7689 - lr: 2.0833e-05 - 22s/epoch - 114ms/step
Epoch 576/1000
2023-09-10 13:06:21.498 
Epoch 576/1000 
	 loss: 385.2707, MinusLogProbMetric: 385.2707, val_loss: 391.7459, val_MinusLogProbMetric: 391.7459

Epoch 576: val_loss did not improve from 391.65692
196/196 - 24s - loss: 385.2707 - MinusLogProbMetric: 385.2707 - val_loss: 391.7459 - val_MinusLogProbMetric: 391.7459 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 577/1000
2023-09-10 13:06:44.109 
Epoch 577/1000 
	 loss: 385.2580, MinusLogProbMetric: 385.2580, val_loss: 391.8180, val_MinusLogProbMetric: 391.8180

Epoch 577: val_loss did not improve from 391.65692
196/196 - 23s - loss: 385.2580 - MinusLogProbMetric: 385.2580 - val_loss: 391.8180 - val_MinusLogProbMetric: 391.8180 - lr: 2.0833e-05 - 23s/epoch - 115ms/step
Epoch 578/1000
2023-09-10 13:07:07.056 
Epoch 578/1000 
	 loss: 385.2012, MinusLogProbMetric: 385.2012, val_loss: 391.6441, val_MinusLogProbMetric: 391.6441

Epoch 578: val_loss improved from 391.65692 to 391.64407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 24s - loss: 385.2012 - MinusLogProbMetric: 385.2012 - val_loss: 391.6441 - val_MinusLogProbMetric: 391.6441 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 579/1000
2023-09-10 13:07:31.707 
Epoch 579/1000 
	 loss: 385.2092, MinusLogProbMetric: 385.2092, val_loss: 391.8671, val_MinusLogProbMetric: 391.8671

Epoch 579: val_loss did not improve from 391.64407
196/196 - 24s - loss: 385.2092 - MinusLogProbMetric: 385.2092 - val_loss: 391.8671 - val_MinusLogProbMetric: 391.8671 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 580/1000
2023-09-10 13:07:56.043 
Epoch 580/1000 
	 loss: 385.2560, MinusLogProbMetric: 385.2560, val_loss: 392.0046, val_MinusLogProbMetric: 392.0046

Epoch 580: val_loss did not improve from 391.64407
196/196 - 24s - loss: 385.2560 - MinusLogProbMetric: 385.2560 - val_loss: 392.0046 - val_MinusLogProbMetric: 392.0046 - lr: 2.0833e-05 - 24s/epoch - 124ms/step
Epoch 581/1000
2023-09-10 13:08:19.749 
Epoch 581/1000 
	 loss: 385.2478, MinusLogProbMetric: 385.2478, val_loss: 391.8601, val_MinusLogProbMetric: 391.8601

Epoch 581: val_loss did not improve from 391.64407
196/196 - 24s - loss: 385.2478 - MinusLogProbMetric: 385.2478 - val_loss: 391.8601 - val_MinusLogProbMetric: 391.8601 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 582/1000
2023-09-10 13:08:43.288 
Epoch 582/1000 
	 loss: 385.2435, MinusLogProbMetric: 385.2435, val_loss: 391.7759, val_MinusLogProbMetric: 391.7759

Epoch 582: val_loss did not improve from 391.64407
196/196 - 24s - loss: 385.2435 - MinusLogProbMetric: 385.2435 - val_loss: 391.7759 - val_MinusLogProbMetric: 391.7759 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 583/1000
2023-09-10 13:09:05.819 
Epoch 583/1000 
	 loss: 385.2457, MinusLogProbMetric: 385.2457, val_loss: 391.6796, val_MinusLogProbMetric: 391.6796

Epoch 583: val_loss did not improve from 391.64407
196/196 - 23s - loss: 385.2457 - MinusLogProbMetric: 385.2457 - val_loss: 391.6796 - val_MinusLogProbMetric: 391.6796 - lr: 2.0833e-05 - 23s/epoch - 115ms/step
Epoch 584/1000
2023-09-10 13:09:28.389 
Epoch 584/1000 
	 loss: 385.2726, MinusLogProbMetric: 385.2726, val_loss: 391.7297, val_MinusLogProbMetric: 391.7297

Epoch 584: val_loss did not improve from 391.64407
196/196 - 23s - loss: 385.2726 - MinusLogProbMetric: 385.2726 - val_loss: 391.7297 - val_MinusLogProbMetric: 391.7297 - lr: 2.0833e-05 - 23s/epoch - 115ms/step
Epoch 585/1000
2023-09-10 13:09:50.311 
Epoch 585/1000 
	 loss: 385.2415, MinusLogProbMetric: 385.2415, val_loss: 391.8880, val_MinusLogProbMetric: 391.8880

Epoch 585: val_loss did not improve from 391.64407
196/196 - 22s - loss: 385.2415 - MinusLogProbMetric: 385.2415 - val_loss: 391.8880 - val_MinusLogProbMetric: 391.8880 - lr: 2.0833e-05 - 22s/epoch - 112ms/step
Epoch 586/1000
2023-09-10 13:10:12.614 
Epoch 586/1000 
	 loss: 385.2626, MinusLogProbMetric: 385.2626, val_loss: 391.5630, val_MinusLogProbMetric: 391.5630

Epoch 586: val_loss improved from 391.64407 to 391.56299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 385.2626 - MinusLogProbMetric: 385.2626 - val_loss: 391.5630 - val_MinusLogProbMetric: 391.5630 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 587/1000
2023-09-10 13:10:37.433 
Epoch 587/1000 
	 loss: 385.2456, MinusLogProbMetric: 385.2456, val_loss: 391.6658, val_MinusLogProbMetric: 391.6658

Epoch 587: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2456 - MinusLogProbMetric: 385.2456 - val_loss: 391.6658 - val_MinusLogProbMetric: 391.6658 - lr: 2.0833e-05 - 24s/epoch - 122ms/step
Epoch 588/1000
2023-09-10 13:11:01.060 
Epoch 588/1000 
	 loss: 385.2406, MinusLogProbMetric: 385.2406, val_loss: 391.7120, val_MinusLogProbMetric: 391.7120

Epoch 588: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2406 - MinusLogProbMetric: 385.2406 - val_loss: 391.7120 - val_MinusLogProbMetric: 391.7120 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 589/1000
2023-09-10 13:11:23.734 
Epoch 589/1000 
	 loss: 385.1915, MinusLogProbMetric: 385.1915, val_loss: 391.7033, val_MinusLogProbMetric: 391.7033

Epoch 589: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.1915 - MinusLogProbMetric: 385.1915 - val_loss: 391.7033 - val_MinusLogProbMetric: 391.7033 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 590/1000
2023-09-10 13:11:47.215 
Epoch 590/1000 
	 loss: 385.1823, MinusLogProbMetric: 385.1823, val_loss: 391.8566, val_MinusLogProbMetric: 391.8566

Epoch 590: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.1823 - MinusLogProbMetric: 385.1823 - val_loss: 391.8566 - val_MinusLogProbMetric: 391.8566 - lr: 2.0833e-05 - 23s/epoch - 120ms/step
Epoch 591/1000
2023-09-10 13:12:10.689 
Epoch 591/1000 
	 loss: 385.2028, MinusLogProbMetric: 385.2028, val_loss: 391.6020, val_MinusLogProbMetric: 391.6020

Epoch 591: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2028 - MinusLogProbMetric: 385.2028 - val_loss: 391.6020 - val_MinusLogProbMetric: 391.6020 - lr: 2.0833e-05 - 23s/epoch - 120ms/step
Epoch 592/1000
2023-09-10 13:12:34.579 
Epoch 592/1000 
	 loss: 385.2296, MinusLogProbMetric: 385.2296, val_loss: 391.7426, val_MinusLogProbMetric: 391.7426

Epoch 592: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2296 - MinusLogProbMetric: 385.2296 - val_loss: 391.7426 - val_MinusLogProbMetric: 391.7426 - lr: 2.0833e-05 - 24s/epoch - 122ms/step
Epoch 593/1000
2023-09-10 13:12:56.736 
Epoch 593/1000 
	 loss: 385.1826, MinusLogProbMetric: 385.1826, val_loss: 391.8388, val_MinusLogProbMetric: 391.8388

Epoch 593: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.1826 - MinusLogProbMetric: 385.1826 - val_loss: 391.8388 - val_MinusLogProbMetric: 391.8388 - lr: 2.0833e-05 - 22s/epoch - 113ms/step
Epoch 594/1000
2023-09-10 13:13:19.494 
Epoch 594/1000 
	 loss: 385.2363, MinusLogProbMetric: 385.2363, val_loss: 391.8813, val_MinusLogProbMetric: 391.8813

Epoch 594: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2363 - MinusLogProbMetric: 385.2363 - val_loss: 391.8813 - val_MinusLogProbMetric: 391.8813 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 595/1000
2023-09-10 13:13:42.246 
Epoch 595/1000 
	 loss: 385.2990, MinusLogProbMetric: 385.2990, val_loss: 391.8855, val_MinusLogProbMetric: 391.8855

Epoch 595: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2990 - MinusLogProbMetric: 385.2990 - val_loss: 391.8855 - val_MinusLogProbMetric: 391.8855 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 596/1000
2023-09-10 13:14:04.092 
Epoch 596/1000 
	 loss: 385.2184, MinusLogProbMetric: 385.2184, val_loss: 391.6959, val_MinusLogProbMetric: 391.6959

Epoch 596: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.2184 - MinusLogProbMetric: 385.2184 - val_loss: 391.6959 - val_MinusLogProbMetric: 391.6959 - lr: 2.0833e-05 - 22s/epoch - 111ms/step
Epoch 597/1000
2023-09-10 13:14:27.174 
Epoch 597/1000 
	 loss: 385.2219, MinusLogProbMetric: 385.2219, val_loss: 391.9410, val_MinusLogProbMetric: 391.9410

Epoch 597: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2219 - MinusLogProbMetric: 385.2219 - val_loss: 391.9410 - val_MinusLogProbMetric: 391.9410 - lr: 2.0833e-05 - 23s/epoch - 118ms/step
Epoch 598/1000
2023-09-10 13:14:50.317 
Epoch 598/1000 
	 loss: 385.2319, MinusLogProbMetric: 385.2319, val_loss: 391.9368, val_MinusLogProbMetric: 391.9368

Epoch 598: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2319 - MinusLogProbMetric: 385.2319 - val_loss: 391.9368 - val_MinusLogProbMetric: 391.9368 - lr: 2.0833e-05 - 23s/epoch - 118ms/step
Epoch 599/1000
2023-09-10 13:15:13.299 
Epoch 599/1000 
	 loss: 385.1714, MinusLogProbMetric: 385.1714, val_loss: 391.6372, val_MinusLogProbMetric: 391.6372

Epoch 599: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.1714 - MinusLogProbMetric: 385.1714 - val_loss: 391.6372 - val_MinusLogProbMetric: 391.6372 - lr: 2.0833e-05 - 23s/epoch - 117ms/step
Epoch 600/1000
2023-09-10 13:15:37.643 
Epoch 600/1000 
	 loss: 385.2193, MinusLogProbMetric: 385.2193, val_loss: 391.9191, val_MinusLogProbMetric: 391.9191

Epoch 600: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2193 - MinusLogProbMetric: 385.2193 - val_loss: 391.9191 - val_MinusLogProbMetric: 391.9191 - lr: 2.0833e-05 - 24s/epoch - 124ms/step
Epoch 601/1000
2023-09-10 13:16:00.859 
Epoch 601/1000 
	 loss: 385.2710, MinusLogProbMetric: 385.2710, val_loss: 391.9767, val_MinusLogProbMetric: 391.9767

Epoch 601: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2710 - MinusLogProbMetric: 385.2710 - val_loss: 391.9767 - val_MinusLogProbMetric: 391.9767 - lr: 2.0833e-05 - 23s/epoch - 118ms/step
Epoch 602/1000
2023-09-10 13:16:24.919 
Epoch 602/1000 
	 loss: 385.3683, MinusLogProbMetric: 385.3683, val_loss: 391.9903, val_MinusLogProbMetric: 391.9903

Epoch 602: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.3683 - MinusLogProbMetric: 385.3683 - val_loss: 391.9903 - val_MinusLogProbMetric: 391.9903 - lr: 2.0833e-05 - 24s/epoch - 123ms/step
Epoch 603/1000
2023-09-10 13:16:49.088 
Epoch 603/1000 
	 loss: 385.3564, MinusLogProbMetric: 385.3564, val_loss: 391.6221, val_MinusLogProbMetric: 391.6221

Epoch 603: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.3564 - MinusLogProbMetric: 385.3564 - val_loss: 391.6221 - val_MinusLogProbMetric: 391.6221 - lr: 2.0833e-05 - 24s/epoch - 123ms/step
Epoch 604/1000
2023-09-10 13:17:12.896 
Epoch 604/1000 
	 loss: 385.2508, MinusLogProbMetric: 385.2508, val_loss: 391.7499, val_MinusLogProbMetric: 391.7499

Epoch 604: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2508 - MinusLogProbMetric: 385.2508 - val_loss: 391.7499 - val_MinusLogProbMetric: 391.7499 - lr: 2.0833e-05 - 24s/epoch - 122ms/step
Epoch 605/1000
2023-09-10 13:17:36.589 
Epoch 605/1000 
	 loss: 385.1913, MinusLogProbMetric: 385.1913, val_loss: 391.6558, val_MinusLogProbMetric: 391.6558

Epoch 605: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.1913 - MinusLogProbMetric: 385.1913 - val_loss: 391.6558 - val_MinusLogProbMetric: 391.6558 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 606/1000
2023-09-10 13:17:59.509 
Epoch 606/1000 
	 loss: 385.2274, MinusLogProbMetric: 385.2274, val_loss: 391.7622, val_MinusLogProbMetric: 391.7622

Epoch 606: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.2274 - MinusLogProbMetric: 385.2274 - val_loss: 391.7622 - val_MinusLogProbMetric: 391.7622 - lr: 2.0833e-05 - 23s/epoch - 117ms/step
Epoch 607/1000
2023-09-10 13:18:23.068 
Epoch 607/1000 
	 loss: 385.2178, MinusLogProbMetric: 385.2178, val_loss: 391.6645, val_MinusLogProbMetric: 391.6645

Epoch 607: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2178 - MinusLogProbMetric: 385.2178 - val_loss: 391.6645 - val_MinusLogProbMetric: 391.6645 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 608/1000
2023-09-10 13:18:46.982 
Epoch 608/1000 
	 loss: 385.2298, MinusLogProbMetric: 385.2298, val_loss: 391.7652, val_MinusLogProbMetric: 391.7652

Epoch 608: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.2298 - MinusLogProbMetric: 385.2298 - val_loss: 391.7652 - val_MinusLogProbMetric: 391.7652 - lr: 2.0833e-05 - 24s/epoch - 122ms/step
Epoch 609/1000
2023-09-10 13:19:09.447 
Epoch 609/1000 
	 loss: 385.1957, MinusLogProbMetric: 385.1957, val_loss: 392.0560, val_MinusLogProbMetric: 392.0560

Epoch 609: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.1957 - MinusLogProbMetric: 385.1957 - val_loss: 392.0560 - val_MinusLogProbMetric: 392.0560 - lr: 2.0833e-05 - 22s/epoch - 115ms/step
Epoch 610/1000
2023-09-10 13:19:34.447 
Epoch 610/1000 
	 loss: 385.2072, MinusLogProbMetric: 385.2072, val_loss: 391.6143, val_MinusLogProbMetric: 391.6143

Epoch 610: val_loss did not improve from 391.56299
196/196 - 25s - loss: 385.2072 - MinusLogProbMetric: 385.2072 - val_loss: 391.6143 - val_MinusLogProbMetric: 391.6143 - lr: 2.0833e-05 - 25s/epoch - 127ms/step
Epoch 611/1000
2023-09-10 13:19:58.023 
Epoch 611/1000 
	 loss: 385.1613, MinusLogProbMetric: 385.1613, val_loss: 391.8110, val_MinusLogProbMetric: 391.8110

Epoch 611: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.1613 - MinusLogProbMetric: 385.1613 - val_loss: 391.8110 - val_MinusLogProbMetric: 391.8110 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 612/1000
2023-09-10 13:20:20.652 
Epoch 612/1000 
	 loss: 385.1854, MinusLogProbMetric: 385.1854, val_loss: 391.7828, val_MinusLogProbMetric: 391.7828

Epoch 612: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.1854 - MinusLogProbMetric: 385.1854 - val_loss: 391.7828 - val_MinusLogProbMetric: 391.7828 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 613/1000
2023-09-10 13:20:44.228 
Epoch 613/1000 
	 loss: 385.1779, MinusLogProbMetric: 385.1779, val_loss: 391.5918, val_MinusLogProbMetric: 391.5918

Epoch 613: val_loss did not improve from 391.56299
196/196 - 24s - loss: 385.1779 - MinusLogProbMetric: 385.1779 - val_loss: 391.5918 - val_MinusLogProbMetric: 391.5918 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 614/1000
2023-09-10 13:21:06.380 
Epoch 614/1000 
	 loss: 385.1892, MinusLogProbMetric: 385.1892, val_loss: 392.1218, val_MinusLogProbMetric: 392.1218

Epoch 614: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.1892 - MinusLogProbMetric: 385.1892 - val_loss: 392.1218 - val_MinusLogProbMetric: 392.1218 - lr: 2.0833e-05 - 22s/epoch - 113ms/step
Epoch 615/1000
2023-09-10 13:21:28.579 
Epoch 615/1000 
	 loss: 385.1877, MinusLogProbMetric: 385.1877, val_loss: 391.8963, val_MinusLogProbMetric: 391.8963

Epoch 615: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.1877 - MinusLogProbMetric: 385.1877 - val_loss: 391.8963 - val_MinusLogProbMetric: 391.8963 - lr: 2.0833e-05 - 22s/epoch - 113ms/step
Epoch 616/1000
2023-09-10 13:21:50.864 
Epoch 616/1000 
	 loss: 385.1920, MinusLogProbMetric: 385.1920, val_loss: 391.8884, val_MinusLogProbMetric: 391.8884

Epoch 616: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.1920 - MinusLogProbMetric: 385.1920 - val_loss: 391.8884 - val_MinusLogProbMetric: 391.8884 - lr: 2.0833e-05 - 22s/epoch - 114ms/step
Epoch 617/1000
2023-09-10 13:22:14.224 
Epoch 617/1000 
	 loss: 385.1628, MinusLogProbMetric: 385.1628, val_loss: 391.7808, val_MinusLogProbMetric: 391.7808

Epoch 617: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.1628 - MinusLogProbMetric: 385.1628 - val_loss: 391.7808 - val_MinusLogProbMetric: 391.7808 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 618/1000
2023-09-10 13:22:35.820 
Epoch 618/1000 
	 loss: 385.2697, MinusLogProbMetric: 385.2697, val_loss: 391.6757, val_MinusLogProbMetric: 391.6757

Epoch 618: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.2697 - MinusLogProbMetric: 385.2697 - val_loss: 391.6757 - val_MinusLogProbMetric: 391.6757 - lr: 2.0833e-05 - 22s/epoch - 110ms/step
Epoch 619/1000
2023-09-10 13:22:57.919 
Epoch 619/1000 
	 loss: 385.2079, MinusLogProbMetric: 385.2079, val_loss: 391.8106, val_MinusLogProbMetric: 391.8106

Epoch 619: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.2079 - MinusLogProbMetric: 385.2079 - val_loss: 391.8106 - val_MinusLogProbMetric: 391.8106 - lr: 2.0833e-05 - 22s/epoch - 112ms/step
Epoch 620/1000
2023-09-10 13:23:20.294 
Epoch 620/1000 
	 loss: 385.1835, MinusLogProbMetric: 385.1835, val_loss: 391.8483, val_MinusLogProbMetric: 391.8483

Epoch 620: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.1835 - MinusLogProbMetric: 385.1835 - val_loss: 391.8483 - val_MinusLogProbMetric: 391.8483 - lr: 2.0833e-05 - 22s/epoch - 114ms/step
Epoch 621/1000
2023-09-10 13:23:42.629 
Epoch 621/1000 
	 loss: 385.2073, MinusLogProbMetric: 385.2073, val_loss: 391.8248, val_MinusLogProbMetric: 391.8248

Epoch 621: val_loss did not improve from 391.56299
196/196 - 22s - loss: 385.2073 - MinusLogProbMetric: 385.2073 - val_loss: 391.8248 - val_MinusLogProbMetric: 391.8248 - lr: 2.0833e-05 - 22s/epoch - 114ms/step
Epoch 622/1000
2023-09-10 13:24:05.249 
Epoch 622/1000 
	 loss: 385.3030, MinusLogProbMetric: 385.3030, val_loss: 392.1611, val_MinusLogProbMetric: 392.1611

Epoch 622: val_loss did not improve from 391.56299
196/196 - 23s - loss: 385.3030 - MinusLogProbMetric: 385.3030 - val_loss: 392.1611 - val_MinusLogProbMetric: 392.1611 - lr: 2.0833e-05 - 23s/epoch - 115ms/step
Epoch 623/1000
2023-09-10 13:24:27.809 
Epoch 623/1000 
	 loss: 385.4028, MinusLogProbMetric: 385.4028, val_loss: 391.4799, val_MinusLogProbMetric: 391.4799

Epoch 623: val_loss improved from 391.56299 to 391.47992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 23s - loss: 385.4028 - MinusLogProbMetric: 385.4028 - val_loss: 391.4799 - val_MinusLogProbMetric: 391.4799 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 624/1000
2023-09-10 13:24:51.928 
Epoch 624/1000 
	 loss: 385.4886, MinusLogProbMetric: 385.4886, val_loss: 391.7607, val_MinusLogProbMetric: 391.7607

Epoch 624: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.4886 - MinusLogProbMetric: 385.4886 - val_loss: 391.7607 - val_MinusLogProbMetric: 391.7607 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 625/1000
2023-09-10 13:25:13.912 
Epoch 625/1000 
	 loss: 385.3487, MinusLogProbMetric: 385.3487, val_loss: 392.0978, val_MinusLogProbMetric: 392.0978

Epoch 625: val_loss did not improve from 391.47992
196/196 - 22s - loss: 385.3487 - MinusLogProbMetric: 385.3487 - val_loss: 392.0978 - val_MinusLogProbMetric: 392.0978 - lr: 2.0833e-05 - 22s/epoch - 112ms/step
Epoch 626/1000
2023-09-10 13:25:35.649 
Epoch 626/1000 
	 loss: 385.3362, MinusLogProbMetric: 385.3362, val_loss: 392.0317, val_MinusLogProbMetric: 392.0317

Epoch 626: val_loss did not improve from 391.47992
196/196 - 22s - loss: 385.3362 - MinusLogProbMetric: 385.3362 - val_loss: 392.0317 - val_MinusLogProbMetric: 392.0317 - lr: 2.0833e-05 - 22s/epoch - 111ms/step
Epoch 627/1000
2023-09-10 13:25:57.728 
Epoch 627/1000 
	 loss: 385.3303, MinusLogProbMetric: 385.3303, val_loss: 391.7932, val_MinusLogProbMetric: 391.7932

Epoch 627: val_loss did not improve from 391.47992
196/196 - 22s - loss: 385.3303 - MinusLogProbMetric: 385.3303 - val_loss: 391.7932 - val_MinusLogProbMetric: 391.7932 - lr: 2.0833e-05 - 22s/epoch - 113ms/step
Epoch 628/1000
2023-09-10 13:26:20.166 
Epoch 628/1000 
	 loss: 385.2792, MinusLogProbMetric: 385.2792, val_loss: 391.5774, val_MinusLogProbMetric: 391.5774

Epoch 628: val_loss did not improve from 391.47992
196/196 - 22s - loss: 385.2792 - MinusLogProbMetric: 385.2792 - val_loss: 391.5774 - val_MinusLogProbMetric: 391.5774 - lr: 2.0833e-05 - 22s/epoch - 114ms/step
Epoch 629/1000
2023-09-10 13:26:42.877 
Epoch 629/1000 
	 loss: 385.2076, MinusLogProbMetric: 385.2076, val_loss: 391.6288, val_MinusLogProbMetric: 391.6288

Epoch 629: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.2076 - MinusLogProbMetric: 385.2076 - val_loss: 391.6288 - val_MinusLogProbMetric: 391.6288 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 630/1000
2023-09-10 13:27:05.152 
Epoch 630/1000 
	 loss: 385.1887, MinusLogProbMetric: 385.1887, val_loss: 391.6705, val_MinusLogProbMetric: 391.6705

Epoch 630: val_loss did not improve from 391.47992
196/196 - 22s - loss: 385.1887 - MinusLogProbMetric: 385.1887 - val_loss: 391.6705 - val_MinusLogProbMetric: 391.6705 - lr: 2.0833e-05 - 22s/epoch - 114ms/step
Epoch 631/1000
2023-09-10 13:27:26.794 
Epoch 631/1000 
	 loss: 385.1790, MinusLogProbMetric: 385.1790, val_loss: 391.6666, val_MinusLogProbMetric: 391.6666

Epoch 631: val_loss did not improve from 391.47992
196/196 - 22s - loss: 385.1790 - MinusLogProbMetric: 385.1790 - val_loss: 391.6666 - val_MinusLogProbMetric: 391.6666 - lr: 2.0833e-05 - 22s/epoch - 110ms/step
Epoch 632/1000
2023-09-10 13:27:50.644 
Epoch 632/1000 
	 loss: 385.2798, MinusLogProbMetric: 385.2798, val_loss: 391.4981, val_MinusLogProbMetric: 391.4981

Epoch 632: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.2798 - MinusLogProbMetric: 385.2798 - val_loss: 391.4981 - val_MinusLogProbMetric: 391.4981 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 633/1000
2023-09-10 13:28:15.331 
Epoch 633/1000 
	 loss: 385.2205, MinusLogProbMetric: 385.2205, val_loss: 391.9123, val_MinusLogProbMetric: 391.9123

Epoch 633: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.2205 - MinusLogProbMetric: 385.2205 - val_loss: 391.9123 - val_MinusLogProbMetric: 391.9123 - lr: 2.0833e-05 - 25s/epoch - 126ms/step
Epoch 634/1000
2023-09-10 13:28:38.569 
Epoch 634/1000 
	 loss: 385.3233, MinusLogProbMetric: 385.3233, val_loss: 391.6251, val_MinusLogProbMetric: 391.6251

Epoch 634: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3233 - MinusLogProbMetric: 385.3233 - val_loss: 391.6251 - val_MinusLogProbMetric: 391.6251 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 635/1000
2023-09-10 13:29:02.067 
Epoch 635/1000 
	 loss: 385.2132, MinusLogProbMetric: 385.2132, val_loss: 392.0145, val_MinusLogProbMetric: 392.0145

Epoch 635: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.2132 - MinusLogProbMetric: 385.2132 - val_loss: 392.0145 - val_MinusLogProbMetric: 392.0145 - lr: 2.0833e-05 - 23s/epoch - 120ms/step
Epoch 636/1000
2023-09-10 13:29:25.626 
Epoch 636/1000 
	 loss: 385.3632, MinusLogProbMetric: 385.3632, val_loss: 391.9916, val_MinusLogProbMetric: 391.9916

Epoch 636: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.3632 - MinusLogProbMetric: 385.3632 - val_loss: 391.9916 - val_MinusLogProbMetric: 391.9916 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 637/1000
2023-09-10 13:29:48.949 
Epoch 637/1000 
	 loss: 385.3492, MinusLogProbMetric: 385.3492, val_loss: 391.6620, val_MinusLogProbMetric: 391.6620

Epoch 637: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3492 - MinusLogProbMetric: 385.3492 - val_loss: 391.6620 - val_MinusLogProbMetric: 391.6620 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 638/1000
2023-09-10 13:30:11.812 
Epoch 638/1000 
	 loss: 385.3526, MinusLogProbMetric: 385.3526, val_loss: 391.7228, val_MinusLogProbMetric: 391.7228

Epoch 638: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3526 - MinusLogProbMetric: 385.3526 - val_loss: 391.7228 - val_MinusLogProbMetric: 391.7228 - lr: 2.0833e-05 - 23s/epoch - 117ms/step
Epoch 639/1000
2023-09-10 13:30:35.293 
Epoch 639/1000 
	 loss: 385.3115, MinusLogProbMetric: 385.3115, val_loss: 391.8714, val_MinusLogProbMetric: 391.8714

Epoch 639: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3115 - MinusLogProbMetric: 385.3115 - val_loss: 391.8714 - val_MinusLogProbMetric: 391.8714 - lr: 2.0833e-05 - 23s/epoch - 120ms/step
Epoch 640/1000
2023-09-10 13:31:00.001 
Epoch 640/1000 
	 loss: 385.3590, MinusLogProbMetric: 385.3590, val_loss: 391.8166, val_MinusLogProbMetric: 391.8166

Epoch 640: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.3590 - MinusLogProbMetric: 385.3590 - val_loss: 391.8166 - val_MinusLogProbMetric: 391.8166 - lr: 2.0833e-05 - 25s/epoch - 126ms/step
Epoch 641/1000
2023-09-10 13:31:23.265 
Epoch 641/1000 
	 loss: 385.2994, MinusLogProbMetric: 385.2994, val_loss: 392.3182, val_MinusLogProbMetric: 392.3182

Epoch 641: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.2994 - MinusLogProbMetric: 385.2994 - val_loss: 392.3182 - val_MinusLogProbMetric: 392.3182 - lr: 2.0833e-05 - 23s/epoch - 119ms/step
Epoch 642/1000
2023-09-10 13:31:45.856 
Epoch 642/1000 
	 loss: 385.3308, MinusLogProbMetric: 385.3308, val_loss: 391.6552, val_MinusLogProbMetric: 391.6552

Epoch 642: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3308 - MinusLogProbMetric: 385.3308 - val_loss: 391.6552 - val_MinusLogProbMetric: 391.6552 - lr: 2.0833e-05 - 23s/epoch - 115ms/step
Epoch 643/1000
2023-09-10 13:32:08.845 
Epoch 643/1000 
	 loss: 385.3344, MinusLogProbMetric: 385.3344, val_loss: 391.7147, val_MinusLogProbMetric: 391.7147

Epoch 643: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3344 - MinusLogProbMetric: 385.3344 - val_loss: 391.7147 - val_MinusLogProbMetric: 391.7147 - lr: 2.0833e-05 - 23s/epoch - 117ms/step
Epoch 644/1000
2023-09-10 13:32:31.604 
Epoch 644/1000 
	 loss: 385.3477, MinusLogProbMetric: 385.3477, val_loss: 392.0662, val_MinusLogProbMetric: 392.0662

Epoch 644: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.3477 - MinusLogProbMetric: 385.3477 - val_loss: 392.0662 - val_MinusLogProbMetric: 392.0662 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 645/1000
2023-09-10 13:32:54.572 
Epoch 645/1000 
	 loss: 385.2141, MinusLogProbMetric: 385.2141, val_loss: 392.2669, val_MinusLogProbMetric: 392.2669

Epoch 645: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.2141 - MinusLogProbMetric: 385.2141 - val_loss: 392.2669 - val_MinusLogProbMetric: 392.2669 - lr: 2.0833e-05 - 23s/epoch - 117ms/step
Epoch 646/1000
2023-09-10 13:33:16.108 
Epoch 646/1000 
	 loss: 385.2399, MinusLogProbMetric: 385.2399, val_loss: 391.7954, val_MinusLogProbMetric: 391.7954

Epoch 646: val_loss did not improve from 391.47992
196/196 - 21s - loss: 385.2399 - MinusLogProbMetric: 385.2399 - val_loss: 391.7954 - val_MinusLogProbMetric: 391.7954 - lr: 2.0833e-05 - 21s/epoch - 110ms/step
Epoch 647/1000
2023-09-10 13:33:40.981 
Epoch 647/1000 
	 loss: 385.2490, MinusLogProbMetric: 385.2490, val_loss: 392.0687, val_MinusLogProbMetric: 392.0687

Epoch 647: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.2490 - MinusLogProbMetric: 385.2490 - val_loss: 392.0687 - val_MinusLogProbMetric: 392.0687 - lr: 2.0833e-05 - 25s/epoch - 127ms/step
Epoch 648/1000
2023-09-10 13:34:05.415 
Epoch 648/1000 
	 loss: 385.2223, MinusLogProbMetric: 385.2223, val_loss: 391.7467, val_MinusLogProbMetric: 391.7467

Epoch 648: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.2223 - MinusLogProbMetric: 385.2223 - val_loss: 391.7467 - val_MinusLogProbMetric: 391.7467 - lr: 2.0833e-05 - 24s/epoch - 125ms/step
Epoch 649/1000
2023-09-10 13:34:28.193 
Epoch 649/1000 
	 loss: 385.1542, MinusLogProbMetric: 385.1542, val_loss: 392.5137, val_MinusLogProbMetric: 392.5137

Epoch 649: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.1542 - MinusLogProbMetric: 385.1542 - val_loss: 392.5137 - val_MinusLogProbMetric: 392.5137 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 650/1000
2023-09-10 13:34:51.729 
Epoch 650/1000 
	 loss: 385.3064, MinusLogProbMetric: 385.3064, val_loss: 391.5876, val_MinusLogProbMetric: 391.5876

Epoch 650: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.3064 - MinusLogProbMetric: 385.3064 - val_loss: 391.5876 - val_MinusLogProbMetric: 391.5876 - lr: 2.0833e-05 - 24s/epoch - 120ms/step
Epoch 651/1000
2023-09-10 13:35:14.372 
Epoch 651/1000 
	 loss: 385.1997, MinusLogProbMetric: 385.1997, val_loss: 392.5551, val_MinusLogProbMetric: 392.5551

Epoch 651: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.1997 - MinusLogProbMetric: 385.1997 - val_loss: 392.5551 - val_MinusLogProbMetric: 392.5551 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 652/1000
2023-09-10 13:35:38.363 
Epoch 652/1000 
	 loss: 385.1608, MinusLogProbMetric: 385.1608, val_loss: 391.8650, val_MinusLogProbMetric: 391.8650

Epoch 652: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.1608 - MinusLogProbMetric: 385.1608 - val_loss: 391.8650 - val_MinusLogProbMetric: 391.8650 - lr: 2.0833e-05 - 24s/epoch - 123ms/step
Epoch 653/1000
2023-09-10 13:36:01.130 
Epoch 653/1000 
	 loss: 385.1973, MinusLogProbMetric: 385.1973, val_loss: 391.6272, val_MinusLogProbMetric: 391.6272

Epoch 653: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.1973 - MinusLogProbMetric: 385.1973 - val_loss: 391.6272 - val_MinusLogProbMetric: 391.6272 - lr: 2.0833e-05 - 23s/epoch - 116ms/step
Epoch 654/1000
2023-09-10 13:36:25.429 
Epoch 654/1000 
	 loss: 385.1761, MinusLogProbMetric: 385.1761, val_loss: 391.7387, val_MinusLogProbMetric: 391.7387

Epoch 654: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.1761 - MinusLogProbMetric: 385.1761 - val_loss: 391.7387 - val_MinusLogProbMetric: 391.7387 - lr: 2.0833e-05 - 24s/epoch - 124ms/step
Epoch 655/1000
2023-09-10 13:36:50.654 
Epoch 655/1000 
	 loss: 385.1794, MinusLogProbMetric: 385.1794, val_loss: 391.8858, val_MinusLogProbMetric: 391.8858

Epoch 655: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1794 - MinusLogProbMetric: 385.1794 - val_loss: 391.8858 - val_MinusLogProbMetric: 391.8858 - lr: 2.0833e-05 - 25s/epoch - 129ms/step
Epoch 656/1000
2023-09-10 13:37:15.637 
Epoch 656/1000 
	 loss: 385.1782, MinusLogProbMetric: 385.1782, val_loss: 391.5448, val_MinusLogProbMetric: 391.5448

Epoch 656: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1782 - MinusLogProbMetric: 385.1782 - val_loss: 391.5448 - val_MinusLogProbMetric: 391.5448 - lr: 2.0833e-05 - 25s/epoch - 127ms/step
Epoch 657/1000
2023-09-10 13:37:40.269 
Epoch 657/1000 
	 loss: 385.1477, MinusLogProbMetric: 385.1477, val_loss: 391.7819, val_MinusLogProbMetric: 391.7819

Epoch 657: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1477 - MinusLogProbMetric: 385.1477 - val_loss: 391.7819 - val_MinusLogProbMetric: 391.7819 - lr: 2.0833e-05 - 25s/epoch - 126ms/step
Epoch 658/1000
2023-09-10 13:38:04.770 
Epoch 658/1000 
	 loss: 385.2158, MinusLogProbMetric: 385.2158, val_loss: 391.6606, val_MinusLogProbMetric: 391.6606

Epoch 658: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.2158 - MinusLogProbMetric: 385.2158 - val_loss: 391.6606 - val_MinusLogProbMetric: 391.6606 - lr: 2.0833e-05 - 25s/epoch - 125ms/step
Epoch 659/1000
2023-09-10 13:38:29.165 
Epoch 659/1000 
	 loss: 385.2347, MinusLogProbMetric: 385.2347, val_loss: 391.8192, val_MinusLogProbMetric: 391.8192

Epoch 659: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.2347 - MinusLogProbMetric: 385.2347 - val_loss: 391.8192 - val_MinusLogProbMetric: 391.8192 - lr: 2.0833e-05 - 24s/epoch - 124ms/step
Epoch 660/1000
2023-09-10 13:38:53.803 
Epoch 660/1000 
	 loss: 385.1546, MinusLogProbMetric: 385.1546, val_loss: 392.1532, val_MinusLogProbMetric: 392.1532

Epoch 660: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1546 - MinusLogProbMetric: 385.1546 - val_loss: 392.1532 - val_MinusLogProbMetric: 392.1532 - lr: 2.0833e-05 - 25s/epoch - 126ms/step
Epoch 661/1000
2023-09-10 13:39:18.608 
Epoch 661/1000 
	 loss: 385.1574, MinusLogProbMetric: 385.1574, val_loss: 392.1024, val_MinusLogProbMetric: 392.1024

Epoch 661: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1574 - MinusLogProbMetric: 385.1574 - val_loss: 392.1024 - val_MinusLogProbMetric: 392.1024 - lr: 2.0833e-05 - 25s/epoch - 127ms/step
Epoch 662/1000
2023-09-10 13:39:42.504 
Epoch 662/1000 
	 loss: 385.2448, MinusLogProbMetric: 385.2448, val_loss: 391.5591, val_MinusLogProbMetric: 391.5591

Epoch 662: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.2448 - MinusLogProbMetric: 385.2448 - val_loss: 391.5591 - val_MinusLogProbMetric: 391.5591 - lr: 2.0833e-05 - 24s/epoch - 122ms/step
Epoch 663/1000
2023-09-10 13:40:07.623 
Epoch 663/1000 
	 loss: 385.2978, MinusLogProbMetric: 385.2978, val_loss: 391.7277, val_MinusLogProbMetric: 391.7277

Epoch 663: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.2978 - MinusLogProbMetric: 385.2978 - val_loss: 391.7277 - val_MinusLogProbMetric: 391.7277 - lr: 2.0833e-05 - 25s/epoch - 128ms/step
Epoch 664/1000
2023-09-10 13:40:32.625 
Epoch 664/1000 
	 loss: 385.0996, MinusLogProbMetric: 385.0996, val_loss: 391.9882, val_MinusLogProbMetric: 391.9882

Epoch 664: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.0996 - MinusLogProbMetric: 385.0996 - val_loss: 391.9882 - val_MinusLogProbMetric: 391.9882 - lr: 2.0833e-05 - 25s/epoch - 127ms/step
Epoch 665/1000
2023-09-10 13:40:56.119 
Epoch 665/1000 
	 loss: 385.1528, MinusLogProbMetric: 385.1528, val_loss: 391.5839, val_MinusLogProbMetric: 391.5839

Epoch 665: val_loss did not improve from 391.47992
196/196 - 23s - loss: 385.1528 - MinusLogProbMetric: 385.1528 - val_loss: 391.5839 - val_MinusLogProbMetric: 391.5839 - lr: 2.0833e-05 - 23s/epoch - 120ms/step
Epoch 666/1000
2023-09-10 13:41:19.912 
Epoch 666/1000 
	 loss: 385.1430, MinusLogProbMetric: 385.1430, val_loss: 391.7886, val_MinusLogProbMetric: 391.7886

Epoch 666: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.1430 - MinusLogProbMetric: 385.1430 - val_loss: 391.7886 - val_MinusLogProbMetric: 391.7886 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 667/1000
2023-09-10 13:41:44.184 
Epoch 667/1000 
	 loss: 385.1366, MinusLogProbMetric: 385.1366, val_loss: 392.0913, val_MinusLogProbMetric: 392.0913

Epoch 667: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.1366 - MinusLogProbMetric: 385.1366 - val_loss: 392.0913 - val_MinusLogProbMetric: 392.0913 - lr: 2.0833e-05 - 24s/epoch - 124ms/step
Epoch 668/1000
2023-09-10 13:42:07.852 
Epoch 668/1000 
	 loss: 385.2198, MinusLogProbMetric: 385.2198, val_loss: 391.8498, val_MinusLogProbMetric: 391.8498

Epoch 668: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.2198 - MinusLogProbMetric: 385.2198 - val_loss: 391.8498 - val_MinusLogProbMetric: 391.8498 - lr: 2.0833e-05 - 24s/epoch - 121ms/step
Epoch 669/1000
2023-09-10 13:42:32.505 
Epoch 669/1000 
	 loss: 385.1925, MinusLogProbMetric: 385.1925, val_loss: 391.6423, val_MinusLogProbMetric: 391.6423

Epoch 669: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1925 - MinusLogProbMetric: 385.1925 - val_loss: 391.6423 - val_MinusLogProbMetric: 391.6423 - lr: 2.0833e-05 - 25s/epoch - 126ms/step
Epoch 670/1000
2023-09-10 13:42:56.571 
Epoch 670/1000 
	 loss: 385.1844, MinusLogProbMetric: 385.1844, val_loss: 391.5824, val_MinusLogProbMetric: 391.5824

Epoch 670: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.1844 - MinusLogProbMetric: 385.1844 - val_loss: 391.5824 - val_MinusLogProbMetric: 391.5824 - lr: 2.0833e-05 - 24s/epoch - 123ms/step
Epoch 671/1000
2023-09-10 13:43:21.042 
Epoch 671/1000 
	 loss: 385.1898, MinusLogProbMetric: 385.1898, val_loss: 391.7001, val_MinusLogProbMetric: 391.7001

Epoch 671: val_loss did not improve from 391.47992
196/196 - 24s - loss: 385.1898 - MinusLogProbMetric: 385.1898 - val_loss: 391.7001 - val_MinusLogProbMetric: 391.7001 - lr: 2.0833e-05 - 24s/epoch - 125ms/step
Epoch 672/1000
2023-09-10 13:43:46.480 
Epoch 672/1000 
	 loss: 385.2212, MinusLogProbMetric: 385.2212, val_loss: 391.9899, val_MinusLogProbMetric: 391.9899

Epoch 672: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.2212 - MinusLogProbMetric: 385.2212 - val_loss: 391.9899 - val_MinusLogProbMetric: 391.9899 - lr: 2.0833e-05 - 25s/epoch - 130ms/step
Epoch 673/1000
2023-09-10 13:44:11.137 
Epoch 673/1000 
	 loss: 385.1890, MinusLogProbMetric: 385.1890, val_loss: 391.7573, val_MinusLogProbMetric: 391.7573

Epoch 673: val_loss did not improve from 391.47992
196/196 - 25s - loss: 385.1890 - MinusLogProbMetric: 385.1890 - val_loss: 391.7573 - val_MinusLogProbMetric: 391.7573 - lr: 2.0833e-05 - 25s/epoch - 126ms/step
Epoch 674/1000
2023-09-10 13:44:35.677 
Epoch 674/1000 
	 loss: 384.8864, MinusLogProbMetric: 384.8864, val_loss: 391.5118, val_MinusLogProbMetric: 391.5118

Epoch 674: val_loss did not improve from 391.47992
196/196 - 25s - loss: 384.8864 - MinusLogProbMetric: 384.8864 - val_loss: 391.5118 - val_MinusLogProbMetric: 391.5118 - lr: 1.0417e-05 - 25s/epoch - 125ms/step
Epoch 675/1000
2023-09-10 13:44:59.765 
Epoch 675/1000 
	 loss: 384.8642, MinusLogProbMetric: 384.8642, val_loss: 391.4295, val_MinusLogProbMetric: 391.4295

Epoch 675: val_loss improved from 391.47992 to 391.42947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 25s - loss: 384.8642 - MinusLogProbMetric: 384.8642 - val_loss: 391.4295 - val_MinusLogProbMetric: 391.4295 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 676/1000
2023-09-10 13:45:25.517 
Epoch 676/1000 
	 loss: 384.8520, MinusLogProbMetric: 384.8520, val_loss: 391.5157, val_MinusLogProbMetric: 391.5157

Epoch 676: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8520 - MinusLogProbMetric: 384.8520 - val_loss: 391.5157 - val_MinusLogProbMetric: 391.5157 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 677/1000
2023-09-10 13:45:49.646 
Epoch 677/1000 
	 loss: 384.8692, MinusLogProbMetric: 384.8692, val_loss: 391.5076, val_MinusLogProbMetric: 391.5076

Epoch 677: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8692 - MinusLogProbMetric: 384.8692 - val_loss: 391.5076 - val_MinusLogProbMetric: 391.5076 - lr: 1.0417e-05 - 24s/epoch - 123ms/step
Epoch 678/1000
2023-09-10 13:46:13.838 
Epoch 678/1000 
	 loss: 384.8661, MinusLogProbMetric: 384.8661, val_loss: 391.6478, val_MinusLogProbMetric: 391.6478

Epoch 678: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8661 - MinusLogProbMetric: 384.8661 - val_loss: 391.6478 - val_MinusLogProbMetric: 391.6478 - lr: 1.0417e-05 - 24s/epoch - 123ms/step
Epoch 679/1000
2023-09-10 13:46:38.438 
Epoch 679/1000 
	 loss: 384.8919, MinusLogProbMetric: 384.8919, val_loss: 391.5488, val_MinusLogProbMetric: 391.5488

Epoch 679: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8919 - MinusLogProbMetric: 384.8919 - val_loss: 391.5488 - val_MinusLogProbMetric: 391.5488 - lr: 1.0417e-05 - 25s/epoch - 125ms/step
Epoch 680/1000
2023-09-10 13:47:01.866 
Epoch 680/1000 
	 loss: 384.8815, MinusLogProbMetric: 384.8815, val_loss: 391.6786, val_MinusLogProbMetric: 391.6786

Epoch 680: val_loss did not improve from 391.42947
196/196 - 23s - loss: 384.8815 - MinusLogProbMetric: 384.8815 - val_loss: 391.6786 - val_MinusLogProbMetric: 391.6786 - lr: 1.0417e-05 - 23s/epoch - 120ms/step
Epoch 681/1000
2023-09-10 13:47:26.190 
Epoch 681/1000 
	 loss: 384.9081, MinusLogProbMetric: 384.9081, val_loss: 391.7267, val_MinusLogProbMetric: 391.7267

Epoch 681: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.9081 - MinusLogProbMetric: 384.9081 - val_loss: 391.7267 - val_MinusLogProbMetric: 391.7267 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 682/1000
2023-09-10 13:47:50.583 
Epoch 682/1000 
	 loss: 384.8941, MinusLogProbMetric: 384.8941, val_loss: 391.6902, val_MinusLogProbMetric: 391.6902

Epoch 682: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8941 - MinusLogProbMetric: 384.8941 - val_loss: 391.6902 - val_MinusLogProbMetric: 391.6902 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 683/1000
2023-09-10 13:48:14.585 
Epoch 683/1000 
	 loss: 384.9002, MinusLogProbMetric: 384.9002, val_loss: 391.4874, val_MinusLogProbMetric: 391.4874

Epoch 683: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.9002 - MinusLogProbMetric: 384.9002 - val_loss: 391.4874 - val_MinusLogProbMetric: 391.4874 - lr: 1.0417e-05 - 24s/epoch - 122ms/step
Epoch 684/1000
2023-09-10 13:48:38.341 
Epoch 684/1000 
	 loss: 384.8448, MinusLogProbMetric: 384.8448, val_loss: 391.6951, val_MinusLogProbMetric: 391.6951

Epoch 684: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8448 - MinusLogProbMetric: 384.8448 - val_loss: 391.6951 - val_MinusLogProbMetric: 391.6951 - lr: 1.0417e-05 - 24s/epoch - 121ms/step
Epoch 685/1000
2023-09-10 13:49:01.108 
Epoch 685/1000 
	 loss: 384.8857, MinusLogProbMetric: 384.8857, val_loss: 391.4569, val_MinusLogProbMetric: 391.4569

Epoch 685: val_loss did not improve from 391.42947
196/196 - 23s - loss: 384.8857 - MinusLogProbMetric: 384.8857 - val_loss: 391.4569 - val_MinusLogProbMetric: 391.4569 - lr: 1.0417e-05 - 23s/epoch - 116ms/step
Epoch 686/1000
2023-09-10 13:49:25.156 
Epoch 686/1000 
	 loss: 384.8624, MinusLogProbMetric: 384.8624, val_loss: 391.6145, val_MinusLogProbMetric: 391.6145

Epoch 686: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8624 - MinusLogProbMetric: 384.8624 - val_loss: 391.6145 - val_MinusLogProbMetric: 391.6145 - lr: 1.0417e-05 - 24s/epoch - 122ms/step
Epoch 687/1000
2023-09-10 13:49:48.841 
Epoch 687/1000 
	 loss: 384.8622, MinusLogProbMetric: 384.8622, val_loss: 391.5805, val_MinusLogProbMetric: 391.5805

Epoch 687: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8622 - MinusLogProbMetric: 384.8622 - val_loss: 391.5805 - val_MinusLogProbMetric: 391.5805 - lr: 1.0417e-05 - 24s/epoch - 121ms/step
Epoch 688/1000
2023-09-10 13:50:12.689 
Epoch 688/1000 
	 loss: 384.8926, MinusLogProbMetric: 384.8926, val_loss: 391.5226, val_MinusLogProbMetric: 391.5226

Epoch 688: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8926 - MinusLogProbMetric: 384.8926 - val_loss: 391.5226 - val_MinusLogProbMetric: 391.5226 - lr: 1.0417e-05 - 24s/epoch - 122ms/step
Epoch 689/1000
2023-09-10 13:50:37.365 
Epoch 689/1000 
	 loss: 384.8566, MinusLogProbMetric: 384.8566, val_loss: 391.5813, val_MinusLogProbMetric: 391.5813

Epoch 689: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8566 - MinusLogProbMetric: 384.8566 - val_loss: 391.5813 - val_MinusLogProbMetric: 391.5813 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 690/1000
2023-09-10 13:51:02.159 
Epoch 690/1000 
	 loss: 384.8629, MinusLogProbMetric: 384.8629, val_loss: 391.6767, val_MinusLogProbMetric: 391.6767

Epoch 690: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8629 - MinusLogProbMetric: 384.8629 - val_loss: 391.6767 - val_MinusLogProbMetric: 391.6767 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 691/1000
2023-09-10 13:51:26.850 
Epoch 691/1000 
	 loss: 384.8605, MinusLogProbMetric: 384.8605, val_loss: 391.5769, val_MinusLogProbMetric: 391.5769

Epoch 691: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8605 - MinusLogProbMetric: 384.8605 - val_loss: 391.5769 - val_MinusLogProbMetric: 391.5769 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 692/1000
2023-09-10 13:51:50.472 
Epoch 692/1000 
	 loss: 384.8678, MinusLogProbMetric: 384.8678, val_loss: 391.8564, val_MinusLogProbMetric: 391.8564

Epoch 692: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8678 - MinusLogProbMetric: 384.8678 - val_loss: 391.8564 - val_MinusLogProbMetric: 391.8564 - lr: 1.0417e-05 - 24s/epoch - 120ms/step
Epoch 693/1000
2023-09-10 13:52:14.630 
Epoch 693/1000 
	 loss: 384.8761, MinusLogProbMetric: 384.8761, val_loss: 391.5122, val_MinusLogProbMetric: 391.5122

Epoch 693: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8761 - MinusLogProbMetric: 384.8761 - val_loss: 391.5122 - val_MinusLogProbMetric: 391.5122 - lr: 1.0417e-05 - 24s/epoch - 123ms/step
Epoch 694/1000
2023-09-10 13:52:38.932 
Epoch 694/1000 
	 loss: 384.8771, MinusLogProbMetric: 384.8771, val_loss: 391.6810, val_MinusLogProbMetric: 391.6810

Epoch 694: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8771 - MinusLogProbMetric: 384.8771 - val_loss: 391.6810 - val_MinusLogProbMetric: 391.6810 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 695/1000
2023-09-10 13:53:02.381 
Epoch 695/1000 
	 loss: 384.8460, MinusLogProbMetric: 384.8460, val_loss: 391.8708, val_MinusLogProbMetric: 391.8708

Epoch 695: val_loss did not improve from 391.42947
196/196 - 23s - loss: 384.8460 - MinusLogProbMetric: 384.8460 - val_loss: 391.8708 - val_MinusLogProbMetric: 391.8708 - lr: 1.0417e-05 - 23s/epoch - 120ms/step
Epoch 696/1000
2023-09-10 13:53:25.866 
Epoch 696/1000 
	 loss: 384.8763, MinusLogProbMetric: 384.8763, val_loss: 391.5322, val_MinusLogProbMetric: 391.5322

Epoch 696: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8763 - MinusLogProbMetric: 384.8763 - val_loss: 391.5322 - val_MinusLogProbMetric: 391.5322 - lr: 1.0417e-05 - 24s/epoch - 120ms/step
Epoch 697/1000
2023-09-10 13:53:49.789 
Epoch 697/1000 
	 loss: 384.8653, MinusLogProbMetric: 384.8653, val_loss: 391.6537, val_MinusLogProbMetric: 391.6537

Epoch 697: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8653 - MinusLogProbMetric: 384.8653 - val_loss: 391.6537 - val_MinusLogProbMetric: 391.6537 - lr: 1.0417e-05 - 24s/epoch - 122ms/step
Epoch 698/1000
2023-09-10 13:54:13.369 
Epoch 698/1000 
	 loss: 384.8657, MinusLogProbMetric: 384.8657, val_loss: 391.4775, val_MinusLogProbMetric: 391.4775

Epoch 698: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8657 - MinusLogProbMetric: 384.8657 - val_loss: 391.4775 - val_MinusLogProbMetric: 391.4775 - lr: 1.0417e-05 - 24s/epoch - 120ms/step
Epoch 699/1000
2023-09-10 13:54:37.616 
Epoch 699/1000 
	 loss: 384.8613, MinusLogProbMetric: 384.8613, val_loss: 391.5755, val_MinusLogProbMetric: 391.5755

Epoch 699: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8613 - MinusLogProbMetric: 384.8613 - val_loss: 391.5755 - val_MinusLogProbMetric: 391.5755 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 700/1000
2023-09-10 13:55:02.036 
Epoch 700/1000 
	 loss: 384.8603, MinusLogProbMetric: 384.8603, val_loss: 391.5307, val_MinusLogProbMetric: 391.5307

Epoch 700: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8603 - MinusLogProbMetric: 384.8603 - val_loss: 391.5307 - val_MinusLogProbMetric: 391.5307 - lr: 1.0417e-05 - 24s/epoch - 125ms/step
Epoch 701/1000
2023-09-10 13:55:25.490 
Epoch 701/1000 
	 loss: 384.8753, MinusLogProbMetric: 384.8753, val_loss: 391.4469, val_MinusLogProbMetric: 391.4469

Epoch 701: val_loss did not improve from 391.42947
196/196 - 23s - loss: 384.8753 - MinusLogProbMetric: 384.8753 - val_loss: 391.4469 - val_MinusLogProbMetric: 391.4469 - lr: 1.0417e-05 - 23s/epoch - 120ms/step
Epoch 702/1000
2023-09-10 13:55:49.002 
Epoch 702/1000 
	 loss: 384.8516, MinusLogProbMetric: 384.8516, val_loss: 391.5681, val_MinusLogProbMetric: 391.5681

Epoch 702: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8516 - MinusLogProbMetric: 384.8516 - val_loss: 391.5681 - val_MinusLogProbMetric: 391.5681 - lr: 1.0417e-05 - 24s/epoch - 120ms/step
Epoch 703/1000
2023-09-10 13:56:14.203 
Epoch 703/1000 
	 loss: 384.8783, MinusLogProbMetric: 384.8783, val_loss: 391.5555, val_MinusLogProbMetric: 391.5555

Epoch 703: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8783 - MinusLogProbMetric: 384.8783 - val_loss: 391.5555 - val_MinusLogProbMetric: 391.5555 - lr: 1.0417e-05 - 25s/epoch - 128ms/step
Epoch 704/1000
2023-09-10 13:56:39.645 
Epoch 704/1000 
	 loss: 384.8633, MinusLogProbMetric: 384.8633, val_loss: 391.6664, val_MinusLogProbMetric: 391.6664

Epoch 704: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8633 - MinusLogProbMetric: 384.8633 - val_loss: 391.6664 - val_MinusLogProbMetric: 391.6664 - lr: 1.0417e-05 - 25s/epoch - 130ms/step
Epoch 705/1000
2023-09-10 13:57:02.849 
Epoch 705/1000 
	 loss: 384.8692, MinusLogProbMetric: 384.8692, val_loss: 391.7127, val_MinusLogProbMetric: 391.7127

Epoch 705: val_loss did not improve from 391.42947
196/196 - 23s - loss: 384.8692 - MinusLogProbMetric: 384.8692 - val_loss: 391.7127 - val_MinusLogProbMetric: 391.7127 - lr: 1.0417e-05 - 23s/epoch - 118ms/step
Epoch 706/1000
2023-09-10 13:57:28.533 
Epoch 706/1000 
	 loss: 384.8475, MinusLogProbMetric: 384.8475, val_loss: 391.6352, val_MinusLogProbMetric: 391.6352

Epoch 706: val_loss did not improve from 391.42947
196/196 - 26s - loss: 384.8475 - MinusLogProbMetric: 384.8475 - val_loss: 391.6352 - val_MinusLogProbMetric: 391.6352 - lr: 1.0417e-05 - 26s/epoch - 131ms/step
Epoch 707/1000
2023-09-10 13:57:53.487 
Epoch 707/1000 
	 loss: 384.8376, MinusLogProbMetric: 384.8376, val_loss: 391.5951, val_MinusLogProbMetric: 391.5951

Epoch 707: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8376 - MinusLogProbMetric: 384.8376 - val_loss: 391.5951 - val_MinusLogProbMetric: 391.5951 - lr: 1.0417e-05 - 25s/epoch - 127ms/step
Epoch 708/1000
2023-09-10 13:58:17.685 
Epoch 708/1000 
	 loss: 384.8764, MinusLogProbMetric: 384.8764, val_loss: 391.6073, val_MinusLogProbMetric: 391.6073

Epoch 708: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8764 - MinusLogProbMetric: 384.8764 - val_loss: 391.6073 - val_MinusLogProbMetric: 391.6073 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 709/1000
2023-09-10 13:58:41.996 
Epoch 709/1000 
	 loss: 384.8528, MinusLogProbMetric: 384.8528, val_loss: 391.5266, val_MinusLogProbMetric: 391.5266

Epoch 709: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8528 - MinusLogProbMetric: 384.8528 - val_loss: 391.5266 - val_MinusLogProbMetric: 391.5266 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 710/1000
2023-09-10 13:59:07.314 
Epoch 710/1000 
	 loss: 384.8844, MinusLogProbMetric: 384.8844, val_loss: 391.5730, val_MinusLogProbMetric: 391.5730

Epoch 710: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8844 - MinusLogProbMetric: 384.8844 - val_loss: 391.5730 - val_MinusLogProbMetric: 391.5730 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 711/1000
2023-09-10 13:59:32.674 
Epoch 711/1000 
	 loss: 384.9075, MinusLogProbMetric: 384.9075, val_loss: 391.7554, val_MinusLogProbMetric: 391.7554

Epoch 711: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.9075 - MinusLogProbMetric: 384.9075 - val_loss: 391.7554 - val_MinusLogProbMetric: 391.7554 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 712/1000
2023-09-10 13:59:57.440 
Epoch 712/1000 
	 loss: 384.8982, MinusLogProbMetric: 384.8982, val_loss: 391.7035, val_MinusLogProbMetric: 391.7035

Epoch 712: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8982 - MinusLogProbMetric: 384.8982 - val_loss: 391.7035 - val_MinusLogProbMetric: 391.7035 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 713/1000
2023-09-10 14:00:21.742 
Epoch 713/1000 
	 loss: 384.8783, MinusLogProbMetric: 384.8783, val_loss: 391.5065, val_MinusLogProbMetric: 391.5065

Epoch 713: val_loss did not improve from 391.42947
196/196 - 24s - loss: 384.8783 - MinusLogProbMetric: 384.8783 - val_loss: 391.5065 - val_MinusLogProbMetric: 391.5065 - lr: 1.0417e-05 - 24s/epoch - 124ms/step
Epoch 714/1000
2023-09-10 14:00:46.433 
Epoch 714/1000 
	 loss: 384.8944, MinusLogProbMetric: 384.8944, val_loss: 391.7939, val_MinusLogProbMetric: 391.7939

Epoch 714: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8944 - MinusLogProbMetric: 384.8944 - val_loss: 391.7939 - val_MinusLogProbMetric: 391.7939 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 715/1000
2023-09-10 14:01:11.717 
Epoch 715/1000 
	 loss: 384.9156, MinusLogProbMetric: 384.9156, val_loss: 391.6551, val_MinusLogProbMetric: 391.6551

Epoch 715: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.9156 - MinusLogProbMetric: 384.9156 - val_loss: 391.6551 - val_MinusLogProbMetric: 391.6551 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 716/1000
2023-09-10 14:01:37.593 
Epoch 716/1000 
	 loss: 384.9177, MinusLogProbMetric: 384.9177, val_loss: 391.6192, val_MinusLogProbMetric: 391.6192

Epoch 716: val_loss did not improve from 391.42947
196/196 - 26s - loss: 384.9177 - MinusLogProbMetric: 384.9177 - val_loss: 391.6192 - val_MinusLogProbMetric: 391.6192 - lr: 1.0417e-05 - 26s/epoch - 132ms/step
Epoch 717/1000
2023-09-10 14:02:02.251 
Epoch 717/1000 
	 loss: 384.9346, MinusLogProbMetric: 384.9346, val_loss: 391.5992, val_MinusLogProbMetric: 391.5992

Epoch 717: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.9346 - MinusLogProbMetric: 384.9346 - val_loss: 391.5992 - val_MinusLogProbMetric: 391.5992 - lr: 1.0417e-05 - 25s/epoch - 126ms/step
Epoch 718/1000
2023-09-10 14:02:27.587 
Epoch 718/1000 
	 loss: 384.9198, MinusLogProbMetric: 384.9198, val_loss: 391.6963, val_MinusLogProbMetric: 391.6963

Epoch 718: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.9198 - MinusLogProbMetric: 384.9198 - val_loss: 391.6963 - val_MinusLogProbMetric: 391.6963 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 719/1000
2023-09-10 14:02:52.889 
Epoch 719/1000 
	 loss: 384.9018, MinusLogProbMetric: 384.9018, val_loss: 391.4952, val_MinusLogProbMetric: 391.4952

Epoch 719: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.9018 - MinusLogProbMetric: 384.9018 - val_loss: 391.4952 - val_MinusLogProbMetric: 391.4952 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 720/1000
2023-09-10 14:03:18.984 
Epoch 720/1000 
	 loss: 384.8599, MinusLogProbMetric: 384.8599, val_loss: 391.7225, val_MinusLogProbMetric: 391.7225

Epoch 720: val_loss did not improve from 391.42947
196/196 - 26s - loss: 384.8599 - MinusLogProbMetric: 384.8599 - val_loss: 391.7225 - val_MinusLogProbMetric: 391.7225 - lr: 1.0417e-05 - 26s/epoch - 133ms/step
Epoch 721/1000
2023-09-10 14:03:44.353 
Epoch 721/1000 
	 loss: 384.8800, MinusLogProbMetric: 384.8800, val_loss: 391.7461, val_MinusLogProbMetric: 391.7461

Epoch 721: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8800 - MinusLogProbMetric: 384.8800 - val_loss: 391.7461 - val_MinusLogProbMetric: 391.7461 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 722/1000
2023-09-10 14:04:09.695 
Epoch 722/1000 
	 loss: 384.8706, MinusLogProbMetric: 384.8706, val_loss: 391.5600, val_MinusLogProbMetric: 391.5600

Epoch 722: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8706 - MinusLogProbMetric: 384.8706 - val_loss: 391.5600 - val_MinusLogProbMetric: 391.5600 - lr: 1.0417e-05 - 25s/epoch - 129ms/step
Epoch 723/1000
2023-09-10 14:04:35.532 
Epoch 723/1000 
	 loss: 384.8635, MinusLogProbMetric: 384.8635, val_loss: 391.6498, val_MinusLogProbMetric: 391.6498

Epoch 723: val_loss did not improve from 391.42947
196/196 - 26s - loss: 384.8635 - MinusLogProbMetric: 384.8635 - val_loss: 391.6498 - val_MinusLogProbMetric: 391.6498 - lr: 1.0417e-05 - 26s/epoch - 132ms/step
Epoch 724/1000
2023-09-10 14:05:00.427 
Epoch 724/1000 
	 loss: 384.8993, MinusLogProbMetric: 384.8993, val_loss: 391.4330, val_MinusLogProbMetric: 391.4330

Epoch 724: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.8993 - MinusLogProbMetric: 384.8993 - val_loss: 391.4330 - val_MinusLogProbMetric: 391.4330 - lr: 1.0417e-05 - 25s/epoch - 127ms/step
Epoch 725/1000
2023-09-10 14:05:22.851 
Epoch 725/1000 
	 loss: 384.8484, MinusLogProbMetric: 384.8484, val_loss: 391.5586, val_MinusLogProbMetric: 391.5586

Epoch 725: val_loss did not improve from 391.42947
196/196 - 22s - loss: 384.8484 - MinusLogProbMetric: 384.8484 - val_loss: 391.5586 - val_MinusLogProbMetric: 391.5586 - lr: 1.0417e-05 - 22s/epoch - 115ms/step
Epoch 726/1000
2023-09-10 14:05:47.827 
Epoch 726/1000 
	 loss: 384.7290, MinusLogProbMetric: 384.7290, val_loss: 391.5445, val_MinusLogProbMetric: 391.5445

Epoch 726: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.7290 - MinusLogProbMetric: 384.7290 - val_loss: 391.5445 - val_MinusLogProbMetric: 391.5445 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 727/1000
2023-09-10 14:06:12.746 
Epoch 727/1000 
	 loss: 384.7192, MinusLogProbMetric: 384.7192, val_loss: 391.6358, val_MinusLogProbMetric: 391.6358

Epoch 727: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.7192 - MinusLogProbMetric: 384.7192 - val_loss: 391.6358 - val_MinusLogProbMetric: 391.6358 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 728/1000
2023-09-10 14:06:38.282 
Epoch 728/1000 
	 loss: 384.7225, MinusLogProbMetric: 384.7225, val_loss: 391.5434, val_MinusLogProbMetric: 391.5434

Epoch 728: val_loss did not improve from 391.42947
196/196 - 25s - loss: 384.7225 - MinusLogProbMetric: 384.7225 - val_loss: 391.5434 - val_MinusLogProbMetric: 391.5434 - lr: 5.2083e-06 - 25s/epoch - 130ms/step
Epoch 729/1000
2023-09-10 14:07:02.074 
Epoch 729/1000 
	 loss: 384.7222, MinusLogProbMetric: 384.7222, val_loss: 391.3742, val_MinusLogProbMetric: 391.3742

Epoch 729: val_loss improved from 391.42947 to 391.37424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_335/weights/best_weights.h5
196/196 - 26s - loss: 384.7222 - MinusLogProbMetric: 384.7222 - val_loss: 391.3742 - val_MinusLogProbMetric: 391.3742 - lr: 5.2083e-06 - 26s/epoch - 132ms/step
Epoch 730/1000
2023-09-10 14:07:28.199 
Epoch 730/1000 
	 loss: 384.7194, MinusLogProbMetric: 384.7194, val_loss: 391.4744, val_MinusLogProbMetric: 391.4744

Epoch 730: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7194 - MinusLogProbMetric: 384.7194 - val_loss: 391.4744 - val_MinusLogProbMetric: 391.4744 - lr: 5.2083e-06 - 24s/epoch - 122ms/step
Epoch 731/1000
2023-09-10 14:07:53.649 
Epoch 731/1000 
	 loss: 384.7174, MinusLogProbMetric: 384.7174, val_loss: 391.5129, val_MinusLogProbMetric: 391.5129

Epoch 731: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7174 - MinusLogProbMetric: 384.7174 - val_loss: 391.5129 - val_MinusLogProbMetric: 391.5129 - lr: 5.2083e-06 - 25s/epoch - 130ms/step
Epoch 732/1000
2023-09-10 14:08:18.515 
Epoch 732/1000 
	 loss: 384.7167, MinusLogProbMetric: 384.7167, val_loss: 391.5633, val_MinusLogProbMetric: 391.5633

Epoch 732: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7167 - MinusLogProbMetric: 384.7167 - val_loss: 391.5633 - val_MinusLogProbMetric: 391.5633 - lr: 5.2083e-06 - 25s/epoch - 126ms/step
Epoch 733/1000
2023-09-10 14:08:42.169 
Epoch 733/1000 
	 loss: 384.7143, MinusLogProbMetric: 384.7143, val_loss: 391.5254, val_MinusLogProbMetric: 391.5254

Epoch 733: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7143 - MinusLogProbMetric: 384.7143 - val_loss: 391.5254 - val_MinusLogProbMetric: 391.5254 - lr: 5.2083e-06 - 24s/epoch - 121ms/step
Epoch 734/1000
2023-09-10 14:09:08.442 
Epoch 734/1000 
	 loss: 384.7135, MinusLogProbMetric: 384.7135, val_loss: 391.4561, val_MinusLogProbMetric: 391.4561

Epoch 734: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7135 - MinusLogProbMetric: 384.7135 - val_loss: 391.4561 - val_MinusLogProbMetric: 391.4561 - lr: 5.2083e-06 - 26s/epoch - 134ms/step
Epoch 735/1000
2023-09-10 14:09:33.328 
Epoch 735/1000 
	 loss: 384.7124, MinusLogProbMetric: 384.7124, val_loss: 391.5177, val_MinusLogProbMetric: 391.5177

Epoch 735: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7124 - MinusLogProbMetric: 384.7124 - val_loss: 391.5177 - val_MinusLogProbMetric: 391.5177 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 736/1000
2023-09-10 14:09:58.095 
Epoch 736/1000 
	 loss: 384.7062, MinusLogProbMetric: 384.7062, val_loss: 391.5092, val_MinusLogProbMetric: 391.5092

Epoch 736: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7062 - MinusLogProbMetric: 384.7062 - val_loss: 391.5092 - val_MinusLogProbMetric: 391.5092 - lr: 5.2083e-06 - 25s/epoch - 126ms/step
Epoch 737/1000
2023-09-10 14:10:22.501 
Epoch 737/1000 
	 loss: 384.7010, MinusLogProbMetric: 384.7010, val_loss: 391.5607, val_MinusLogProbMetric: 391.5607

Epoch 737: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7010 - MinusLogProbMetric: 384.7010 - val_loss: 391.5607 - val_MinusLogProbMetric: 391.5607 - lr: 5.2083e-06 - 24s/epoch - 125ms/step
Epoch 738/1000
2023-09-10 14:10:48.114 
Epoch 738/1000 
	 loss: 384.7151, MinusLogProbMetric: 384.7151, val_loss: 391.5493, val_MinusLogProbMetric: 391.5493

Epoch 738: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7151 - MinusLogProbMetric: 384.7151 - val_loss: 391.5493 - val_MinusLogProbMetric: 391.5493 - lr: 5.2083e-06 - 26s/epoch - 130ms/step
Epoch 739/1000
2023-09-10 14:11:13.558 
Epoch 739/1000 
	 loss: 384.7068, MinusLogProbMetric: 384.7068, val_loss: 391.4582, val_MinusLogProbMetric: 391.4582

Epoch 739: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7068 - MinusLogProbMetric: 384.7068 - val_loss: 391.4582 - val_MinusLogProbMetric: 391.4582 - lr: 5.2083e-06 - 25s/epoch - 130ms/step
Epoch 740/1000
2023-09-10 14:11:37.933 
Epoch 740/1000 
	 loss: 384.7090, MinusLogProbMetric: 384.7090, val_loss: 391.5545, val_MinusLogProbMetric: 391.5545

Epoch 740: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7090 - MinusLogProbMetric: 384.7090 - val_loss: 391.5545 - val_MinusLogProbMetric: 391.5545 - lr: 5.2083e-06 - 24s/epoch - 124ms/step
Epoch 741/1000
2023-09-10 14:12:03.897 
Epoch 741/1000 
	 loss: 384.7015, MinusLogProbMetric: 384.7015, val_loss: 391.5315, val_MinusLogProbMetric: 391.5315

Epoch 741: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7015 - MinusLogProbMetric: 384.7015 - val_loss: 391.5315 - val_MinusLogProbMetric: 391.5315 - lr: 5.2083e-06 - 26s/epoch - 132ms/step
Epoch 742/1000
2023-09-10 14:12:29.487 
Epoch 742/1000 
	 loss: 384.7053, MinusLogProbMetric: 384.7053, val_loss: 391.5419, val_MinusLogProbMetric: 391.5419

Epoch 742: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7053 - MinusLogProbMetric: 384.7053 - val_loss: 391.5419 - val_MinusLogProbMetric: 391.5419 - lr: 5.2083e-06 - 26s/epoch - 130ms/step
Epoch 743/1000
2023-09-10 14:12:53.780 
Epoch 743/1000 
	 loss: 384.7045, MinusLogProbMetric: 384.7045, val_loss: 391.5266, val_MinusLogProbMetric: 391.5266

Epoch 743: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7045 - MinusLogProbMetric: 384.7045 - val_loss: 391.5266 - val_MinusLogProbMetric: 391.5266 - lr: 5.2083e-06 - 24s/epoch - 124ms/step
Epoch 744/1000
2023-09-10 14:13:19.739 
Epoch 744/1000 
	 loss: 384.7059, MinusLogProbMetric: 384.7059, val_loss: 391.6485, val_MinusLogProbMetric: 391.6485

Epoch 744: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7059 - MinusLogProbMetric: 384.7059 - val_loss: 391.6485 - val_MinusLogProbMetric: 391.6485 - lr: 5.2083e-06 - 26s/epoch - 133ms/step
Epoch 745/1000
2023-09-10 14:13:44.230 
Epoch 745/1000 
	 loss: 384.6993, MinusLogProbMetric: 384.6993, val_loss: 391.3957, val_MinusLogProbMetric: 391.3957

Epoch 745: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6993 - MinusLogProbMetric: 384.6993 - val_loss: 391.3957 - val_MinusLogProbMetric: 391.3957 - lr: 5.2083e-06 - 24s/epoch - 125ms/step
Epoch 746/1000
2023-09-10 14:14:10.148 
Epoch 746/1000 
	 loss: 384.7167, MinusLogProbMetric: 384.7167, val_loss: 391.4781, val_MinusLogProbMetric: 391.4781

Epoch 746: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7167 - MinusLogProbMetric: 384.7167 - val_loss: 391.4781 - val_MinusLogProbMetric: 391.4781 - lr: 5.2083e-06 - 26s/epoch - 132ms/step
Epoch 747/1000
2023-09-10 14:14:35.050 
Epoch 747/1000 
	 loss: 384.7084, MinusLogProbMetric: 384.7084, val_loss: 391.4790, val_MinusLogProbMetric: 391.4790

Epoch 747: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7084 - MinusLogProbMetric: 384.7084 - val_loss: 391.4790 - val_MinusLogProbMetric: 391.4790 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 748/1000
2023-09-10 14:15:00.728 
Epoch 748/1000 
	 loss: 384.7148, MinusLogProbMetric: 384.7148, val_loss: 391.5284, val_MinusLogProbMetric: 391.5284

Epoch 748: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7148 - MinusLogProbMetric: 384.7148 - val_loss: 391.5284 - val_MinusLogProbMetric: 391.5284 - lr: 5.2083e-06 - 26s/epoch - 131ms/step
Epoch 749/1000
2023-09-10 14:15:25.438 
Epoch 749/1000 
	 loss: 384.7097, MinusLogProbMetric: 384.7097, val_loss: 391.4778, val_MinusLogProbMetric: 391.4778

Epoch 749: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7097 - MinusLogProbMetric: 384.7097 - val_loss: 391.4778 - val_MinusLogProbMetric: 391.4778 - lr: 5.2083e-06 - 25s/epoch - 126ms/step
Epoch 750/1000
2023-09-10 14:15:48.978 
Epoch 750/1000 
	 loss: 384.7236, MinusLogProbMetric: 384.7236, val_loss: 391.4651, val_MinusLogProbMetric: 391.4651

Epoch 750: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7236 - MinusLogProbMetric: 384.7236 - val_loss: 391.4651 - val_MinusLogProbMetric: 391.4651 - lr: 5.2083e-06 - 24s/epoch - 120ms/step
Epoch 751/1000
2023-09-10 14:16:13.466 
Epoch 751/1000 
	 loss: 384.7007, MinusLogProbMetric: 384.7007, val_loss: 391.4304, val_MinusLogProbMetric: 391.4304

Epoch 751: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7007 - MinusLogProbMetric: 384.7007 - val_loss: 391.4304 - val_MinusLogProbMetric: 391.4304 - lr: 5.2083e-06 - 25s/epoch - 125ms/step
Epoch 752/1000
2023-09-10 14:16:40.531 
Epoch 752/1000 
	 loss: 384.7002, MinusLogProbMetric: 384.7002, val_loss: 391.5179, val_MinusLogProbMetric: 391.5179

Epoch 752: val_loss did not improve from 391.37424
196/196 - 27s - loss: 384.7002 - MinusLogProbMetric: 384.7002 - val_loss: 391.5179 - val_MinusLogProbMetric: 391.5179 - lr: 5.2083e-06 - 27s/epoch - 138ms/step
Epoch 753/1000
2023-09-10 14:17:05.466 
Epoch 753/1000 
	 loss: 384.7104, MinusLogProbMetric: 384.7104, val_loss: 391.5716, val_MinusLogProbMetric: 391.5716

Epoch 753: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7104 - MinusLogProbMetric: 384.7104 - val_loss: 391.5716 - val_MinusLogProbMetric: 391.5716 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 754/1000
2023-09-10 14:17:28.987 
Epoch 754/1000 
	 loss: 384.7085, MinusLogProbMetric: 384.7085, val_loss: 391.4590, val_MinusLogProbMetric: 391.4590

Epoch 754: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7085 - MinusLogProbMetric: 384.7085 - val_loss: 391.4590 - val_MinusLogProbMetric: 391.4590 - lr: 5.2083e-06 - 24s/epoch - 120ms/step
Epoch 755/1000
2023-09-10 14:17:53.768 
Epoch 755/1000 
	 loss: 384.7108, MinusLogProbMetric: 384.7108, val_loss: 391.3875, val_MinusLogProbMetric: 391.3875

Epoch 755: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7108 - MinusLogProbMetric: 384.7108 - val_loss: 391.3875 - val_MinusLogProbMetric: 391.3875 - lr: 5.2083e-06 - 25s/epoch - 126ms/step
Epoch 756/1000
2023-09-10 14:18:18.573 
Epoch 756/1000 
	 loss: 384.7048, MinusLogProbMetric: 384.7048, val_loss: 391.3807, val_MinusLogProbMetric: 391.3807

Epoch 756: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7048 - MinusLogProbMetric: 384.7048 - val_loss: 391.3807 - val_MinusLogProbMetric: 391.3807 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 757/1000
2023-09-10 14:18:43.198 
Epoch 757/1000 
	 loss: 384.7055, MinusLogProbMetric: 384.7055, val_loss: 391.4390, val_MinusLogProbMetric: 391.4390

Epoch 757: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7055 - MinusLogProbMetric: 384.7055 - val_loss: 391.4390 - val_MinusLogProbMetric: 391.4390 - lr: 5.2083e-06 - 25s/epoch - 126ms/step
Epoch 758/1000
2023-09-10 14:19:09.223 
Epoch 758/1000 
	 loss: 384.7078, MinusLogProbMetric: 384.7078, val_loss: 391.6249, val_MinusLogProbMetric: 391.6249

Epoch 758: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7078 - MinusLogProbMetric: 384.7078 - val_loss: 391.6249 - val_MinusLogProbMetric: 391.6249 - lr: 5.2083e-06 - 26s/epoch - 133ms/step
Epoch 759/1000
2023-09-10 14:19:35.684 
Epoch 759/1000 
	 loss: 384.7033, MinusLogProbMetric: 384.7033, val_loss: 391.3944, val_MinusLogProbMetric: 391.3944

Epoch 759: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7033 - MinusLogProbMetric: 384.7033 - val_loss: 391.3944 - val_MinusLogProbMetric: 391.3944 - lr: 5.2083e-06 - 26s/epoch - 135ms/step
Epoch 760/1000
2023-09-10 14:20:00.342 
Epoch 760/1000 
	 loss: 384.7150, MinusLogProbMetric: 384.7150, val_loss: 391.5719, val_MinusLogProbMetric: 391.5719

Epoch 760: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7150 - MinusLogProbMetric: 384.7150 - val_loss: 391.5719 - val_MinusLogProbMetric: 391.5719 - lr: 5.2083e-06 - 25s/epoch - 126ms/step
Epoch 761/1000
2023-09-10 14:20:25.178 
Epoch 761/1000 
	 loss: 384.7079, MinusLogProbMetric: 384.7079, val_loss: 391.4948, val_MinusLogProbMetric: 391.4948

Epoch 761: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7079 - MinusLogProbMetric: 384.7079 - val_loss: 391.4948 - val_MinusLogProbMetric: 391.4948 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 762/1000
2023-09-10 14:20:50.549 
Epoch 762/1000 
	 loss: 384.7050, MinusLogProbMetric: 384.7050, val_loss: 391.4512, val_MinusLogProbMetric: 391.4512

Epoch 762: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7050 - MinusLogProbMetric: 384.7050 - val_loss: 391.4512 - val_MinusLogProbMetric: 391.4512 - lr: 5.2083e-06 - 25s/epoch - 130ms/step
Epoch 763/1000
2023-09-10 14:21:16.487 
Epoch 763/1000 
	 loss: 384.7006, MinusLogProbMetric: 384.7006, val_loss: 391.4877, val_MinusLogProbMetric: 391.4877

Epoch 763: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7006 - MinusLogProbMetric: 384.7006 - val_loss: 391.4877 - val_MinusLogProbMetric: 391.4877 - lr: 5.2083e-06 - 26s/epoch - 132ms/step
Epoch 764/1000
2023-09-10 14:21:42.102 
Epoch 764/1000 
	 loss: 384.7009, MinusLogProbMetric: 384.7009, val_loss: 391.4986, val_MinusLogProbMetric: 391.4986

Epoch 764: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7009 - MinusLogProbMetric: 384.7009 - val_loss: 391.4986 - val_MinusLogProbMetric: 391.4986 - lr: 5.2083e-06 - 26s/epoch - 130ms/step
Epoch 765/1000
2023-09-10 14:22:07.042 
Epoch 765/1000 
	 loss: 384.7118, MinusLogProbMetric: 384.7118, val_loss: 391.5287, val_MinusLogProbMetric: 391.5287

Epoch 765: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.7118 - MinusLogProbMetric: 384.7118 - val_loss: 391.5287 - val_MinusLogProbMetric: 391.5287 - lr: 5.2083e-06 - 25s/epoch - 127ms/step
Epoch 766/1000
2023-09-10 14:22:27.775 
Epoch 766/1000 
	 loss: 384.7089, MinusLogProbMetric: 384.7089, val_loss: 391.4448, val_MinusLogProbMetric: 391.4448

Epoch 766: val_loss did not improve from 391.37424
196/196 - 21s - loss: 384.7089 - MinusLogProbMetric: 384.7089 - val_loss: 391.4448 - val_MinusLogProbMetric: 391.4448 - lr: 5.2083e-06 - 21s/epoch - 106ms/step
Epoch 767/1000
2023-09-10 14:22:53.841 
Epoch 767/1000 
	 loss: 384.7264, MinusLogProbMetric: 384.7264, val_loss: 391.4283, val_MinusLogProbMetric: 391.4283

Epoch 767: val_loss did not improve from 391.37424
196/196 - 26s - loss: 384.7264 - MinusLogProbMetric: 384.7264 - val_loss: 391.4283 - val_MinusLogProbMetric: 391.4283 - lr: 5.2083e-06 - 26s/epoch - 133ms/step
Epoch 768/1000
2023-09-10 14:23:18.092 
Epoch 768/1000 
	 loss: 384.7165, MinusLogProbMetric: 384.7165, val_loss: 391.4466, val_MinusLogProbMetric: 391.4466

Epoch 768: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7165 - MinusLogProbMetric: 384.7165 - val_loss: 391.4466 - val_MinusLogProbMetric: 391.4466 - lr: 5.2083e-06 - 24s/epoch - 124ms/step
Epoch 769/1000
2023-09-10 14:23:42.292 
Epoch 769/1000 
	 loss: 384.7181, MinusLogProbMetric: 384.7181, val_loss: 391.4270, val_MinusLogProbMetric: 391.4270

Epoch 769: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7181 - MinusLogProbMetric: 384.7181 - val_loss: 391.4270 - val_MinusLogProbMetric: 391.4270 - lr: 5.2083e-06 - 24s/epoch - 123ms/step
Epoch 770/1000
2023-09-10 14:24:06.189 
Epoch 770/1000 
	 loss: 384.7186, MinusLogProbMetric: 384.7186, val_loss: 391.4968, val_MinusLogProbMetric: 391.4968

Epoch 770: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.7186 - MinusLogProbMetric: 384.7186 - val_loss: 391.4968 - val_MinusLogProbMetric: 391.4968 - lr: 5.2083e-06 - 24s/epoch - 122ms/step
Epoch 771/1000
2023-09-10 14:24:27.829 
Epoch 771/1000 
	 loss: 384.7358, MinusLogProbMetric: 384.7358, val_loss: 391.5445, val_MinusLogProbMetric: 391.5445

Epoch 771: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.7358 - MinusLogProbMetric: 384.7358 - val_loss: 391.5445 - val_MinusLogProbMetric: 391.5445 - lr: 5.2083e-06 - 22s/epoch - 110ms/step
Epoch 772/1000
2023-09-10 14:24:50.587 
Epoch 772/1000 
	 loss: 384.7227, MinusLogProbMetric: 384.7227, val_loss: 391.4568, val_MinusLogProbMetric: 391.4568

Epoch 772: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7227 - MinusLogProbMetric: 384.7227 - val_loss: 391.4568 - val_MinusLogProbMetric: 391.4568 - lr: 5.2083e-06 - 23s/epoch - 116ms/step
Epoch 773/1000
2023-09-10 14:25:13.200 
Epoch 773/1000 
	 loss: 384.7183, MinusLogProbMetric: 384.7183, val_loss: 391.4779, val_MinusLogProbMetric: 391.4779

Epoch 773: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7183 - MinusLogProbMetric: 384.7183 - val_loss: 391.4779 - val_MinusLogProbMetric: 391.4779 - lr: 5.2083e-06 - 23s/epoch - 115ms/step
Epoch 774/1000
2023-09-10 14:25:36.298 
Epoch 774/1000 
	 loss: 384.7205, MinusLogProbMetric: 384.7205, val_loss: 391.4933, val_MinusLogProbMetric: 391.4933

Epoch 774: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7205 - MinusLogProbMetric: 384.7205 - val_loss: 391.4933 - val_MinusLogProbMetric: 391.4933 - lr: 5.2083e-06 - 23s/epoch - 118ms/step
Epoch 775/1000
2023-09-10 14:25:59.484 
Epoch 775/1000 
	 loss: 384.7069, MinusLogProbMetric: 384.7069, val_loss: 391.5094, val_MinusLogProbMetric: 391.5094

Epoch 775: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7069 - MinusLogProbMetric: 384.7069 - val_loss: 391.5094 - val_MinusLogProbMetric: 391.5094 - lr: 5.2083e-06 - 23s/epoch - 118ms/step
Epoch 776/1000
2023-09-10 14:26:21.735 
Epoch 776/1000 
	 loss: 384.7144, MinusLogProbMetric: 384.7144, val_loss: 391.5880, val_MinusLogProbMetric: 391.5880

Epoch 776: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.7144 - MinusLogProbMetric: 384.7144 - val_loss: 391.5880 - val_MinusLogProbMetric: 391.5880 - lr: 5.2083e-06 - 22s/epoch - 114ms/step
Epoch 777/1000
2023-09-10 14:26:44.602 
Epoch 777/1000 
	 loss: 384.7346, MinusLogProbMetric: 384.7346, val_loss: 391.6524, val_MinusLogProbMetric: 391.6524

Epoch 777: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7346 - MinusLogProbMetric: 384.7346 - val_loss: 391.6524 - val_MinusLogProbMetric: 391.6524 - lr: 5.2083e-06 - 23s/epoch - 117ms/step
Epoch 778/1000
2023-09-10 14:27:07.103 
Epoch 778/1000 
	 loss: 384.7332, MinusLogProbMetric: 384.7332, val_loss: 391.4419, val_MinusLogProbMetric: 391.4419

Epoch 778: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7332 - MinusLogProbMetric: 384.7332 - val_loss: 391.4419 - val_MinusLogProbMetric: 391.4419 - lr: 5.2083e-06 - 23s/epoch - 115ms/step
Epoch 779/1000
2023-09-10 14:27:30.109 
Epoch 779/1000 
	 loss: 384.7440, MinusLogProbMetric: 384.7440, val_loss: 391.5419, val_MinusLogProbMetric: 391.5419

Epoch 779: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.7440 - MinusLogProbMetric: 384.7440 - val_loss: 391.5419 - val_MinusLogProbMetric: 391.5419 - lr: 5.2083e-06 - 23s/epoch - 117ms/step
Epoch 780/1000
2023-09-10 14:27:52.285 
Epoch 780/1000 
	 loss: 384.6822, MinusLogProbMetric: 384.6822, val_loss: 391.4738, val_MinusLogProbMetric: 391.4738

Epoch 780: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6822 - MinusLogProbMetric: 384.6822 - val_loss: 391.4738 - val_MinusLogProbMetric: 391.4738 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 781/1000
2023-09-10 14:28:15.938 
Epoch 781/1000 
	 loss: 384.6630, MinusLogProbMetric: 384.6630, val_loss: 391.5326, val_MinusLogProbMetric: 391.5326

Epoch 781: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6630 - MinusLogProbMetric: 384.6630 - val_loss: 391.5326 - val_MinusLogProbMetric: 391.5326 - lr: 2.6042e-06 - 24s/epoch - 121ms/step
Epoch 782/1000
2023-09-10 14:28:39.427 
Epoch 782/1000 
	 loss: 384.6694, MinusLogProbMetric: 384.6694, val_loss: 391.4231, val_MinusLogProbMetric: 391.4231

Epoch 782: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6694 - MinusLogProbMetric: 384.6694 - val_loss: 391.4231 - val_MinusLogProbMetric: 391.4231 - lr: 2.6042e-06 - 23s/epoch - 120ms/step
Epoch 783/1000
2023-09-10 14:29:01.574 
Epoch 783/1000 
	 loss: 384.6674, MinusLogProbMetric: 384.6674, val_loss: 391.4399, val_MinusLogProbMetric: 391.4399

Epoch 783: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6674 - MinusLogProbMetric: 384.6674 - val_loss: 391.4399 - val_MinusLogProbMetric: 391.4399 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 784/1000
2023-09-10 14:29:26.070 
Epoch 784/1000 
	 loss: 384.6628, MinusLogProbMetric: 384.6628, val_loss: 391.4880, val_MinusLogProbMetric: 391.4880

Epoch 784: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6628 - MinusLogProbMetric: 384.6628 - val_loss: 391.4880 - val_MinusLogProbMetric: 391.4880 - lr: 2.6042e-06 - 24s/epoch - 125ms/step
Epoch 785/1000
2023-09-10 14:29:48.644 
Epoch 785/1000 
	 loss: 384.6664, MinusLogProbMetric: 384.6664, val_loss: 391.5008, val_MinusLogProbMetric: 391.5008

Epoch 785: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6664 - MinusLogProbMetric: 384.6664 - val_loss: 391.5008 - val_MinusLogProbMetric: 391.5008 - lr: 2.6042e-06 - 23s/epoch - 115ms/step
Epoch 786/1000
2023-09-10 14:30:10.806 
Epoch 786/1000 
	 loss: 384.6573, MinusLogProbMetric: 384.6573, val_loss: 391.5028, val_MinusLogProbMetric: 391.5028

Epoch 786: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6573 - MinusLogProbMetric: 384.6573 - val_loss: 391.5028 - val_MinusLogProbMetric: 391.5028 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 787/1000
2023-09-10 14:30:32.606 
Epoch 787/1000 
	 loss: 384.6635, MinusLogProbMetric: 384.6635, val_loss: 391.4297, val_MinusLogProbMetric: 391.4297

Epoch 787: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6635 - MinusLogProbMetric: 384.6635 - val_loss: 391.4297 - val_MinusLogProbMetric: 391.4297 - lr: 2.6042e-06 - 22s/epoch - 111ms/step
Epoch 788/1000
2023-09-10 14:30:56.649 
Epoch 788/1000 
	 loss: 384.6681, MinusLogProbMetric: 384.6681, val_loss: 391.3937, val_MinusLogProbMetric: 391.3937

Epoch 788: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6681 - MinusLogProbMetric: 384.6681 - val_loss: 391.3937 - val_MinusLogProbMetric: 391.3937 - lr: 2.6042e-06 - 24s/epoch - 123ms/step
Epoch 789/1000
2023-09-10 14:31:21.349 
Epoch 789/1000 
	 loss: 384.6770, MinusLogProbMetric: 384.6770, val_loss: 391.4598, val_MinusLogProbMetric: 391.4598

Epoch 789: val_loss did not improve from 391.37424
196/196 - 25s - loss: 384.6770 - MinusLogProbMetric: 384.6770 - val_loss: 391.4598 - val_MinusLogProbMetric: 391.4598 - lr: 2.6042e-06 - 25s/epoch - 126ms/step
Epoch 790/1000
2023-09-10 14:31:42.977 
Epoch 790/1000 
	 loss: 384.6759, MinusLogProbMetric: 384.6759, val_loss: 391.4980, val_MinusLogProbMetric: 391.4980

Epoch 790: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6759 - MinusLogProbMetric: 384.6759 - val_loss: 391.4980 - val_MinusLogProbMetric: 391.4980 - lr: 2.6042e-06 - 22s/epoch - 110ms/step
Epoch 791/1000
2023-09-10 14:32:05.730 
Epoch 791/1000 
	 loss: 384.6738, MinusLogProbMetric: 384.6738, val_loss: 391.5711, val_MinusLogProbMetric: 391.5711

Epoch 791: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6738 - MinusLogProbMetric: 384.6738 - val_loss: 391.5711 - val_MinusLogProbMetric: 391.5711 - lr: 2.6042e-06 - 23s/epoch - 116ms/step
Epoch 792/1000
2023-09-10 14:32:28.005 
Epoch 792/1000 
	 loss: 384.6678, MinusLogProbMetric: 384.6678, val_loss: 391.4372, val_MinusLogProbMetric: 391.4372

Epoch 792: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6678 - MinusLogProbMetric: 384.6678 - val_loss: 391.4372 - val_MinusLogProbMetric: 391.4372 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 793/1000
2023-09-10 14:32:49.967 
Epoch 793/1000 
	 loss: 384.6749, MinusLogProbMetric: 384.6749, val_loss: 391.3870, val_MinusLogProbMetric: 391.3870

Epoch 793: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6749 - MinusLogProbMetric: 384.6749 - val_loss: 391.3870 - val_MinusLogProbMetric: 391.3870 - lr: 2.6042e-06 - 22s/epoch - 112ms/step
Epoch 794/1000
2023-09-10 14:33:12.239 
Epoch 794/1000 
	 loss: 384.6706, MinusLogProbMetric: 384.6706, val_loss: 391.4715, val_MinusLogProbMetric: 391.4715

Epoch 794: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6706 - MinusLogProbMetric: 384.6706 - val_loss: 391.4715 - val_MinusLogProbMetric: 391.4715 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 795/1000
2023-09-10 14:33:35.093 
Epoch 795/1000 
	 loss: 384.6756, MinusLogProbMetric: 384.6756, val_loss: 391.5388, val_MinusLogProbMetric: 391.5388

Epoch 795: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6756 - MinusLogProbMetric: 384.6756 - val_loss: 391.5388 - val_MinusLogProbMetric: 391.5388 - lr: 2.6042e-06 - 23s/epoch - 117ms/step
Epoch 796/1000
2023-09-10 14:33:58.636 
Epoch 796/1000 
	 loss: 384.6682, MinusLogProbMetric: 384.6682, val_loss: 391.4819, val_MinusLogProbMetric: 391.4819

Epoch 796: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6682 - MinusLogProbMetric: 384.6682 - val_loss: 391.4819 - val_MinusLogProbMetric: 391.4819 - lr: 2.6042e-06 - 24s/epoch - 120ms/step
Epoch 797/1000
2023-09-10 14:34:21.845 
Epoch 797/1000 
	 loss: 384.6891, MinusLogProbMetric: 384.6891, val_loss: 391.5159, val_MinusLogProbMetric: 391.5159

Epoch 797: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6891 - MinusLogProbMetric: 384.6891 - val_loss: 391.5159 - val_MinusLogProbMetric: 391.5159 - lr: 2.6042e-06 - 23s/epoch - 118ms/step
Epoch 798/1000
2023-09-10 14:34:43.921 
Epoch 798/1000 
	 loss: 384.6700, MinusLogProbMetric: 384.6700, val_loss: 391.4425, val_MinusLogProbMetric: 391.4425

Epoch 798: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6700 - MinusLogProbMetric: 384.6700 - val_loss: 391.4425 - val_MinusLogProbMetric: 391.4425 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 799/1000
2023-09-10 14:35:06.589 
Epoch 799/1000 
	 loss: 384.6771, MinusLogProbMetric: 384.6771, val_loss: 391.4853, val_MinusLogProbMetric: 391.4853

Epoch 799: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6771 - MinusLogProbMetric: 384.6771 - val_loss: 391.4853 - val_MinusLogProbMetric: 391.4853 - lr: 2.6042e-06 - 23s/epoch - 116ms/step
Epoch 800/1000
2023-09-10 14:35:28.682 
Epoch 800/1000 
	 loss: 384.6790, MinusLogProbMetric: 384.6790, val_loss: 391.5213, val_MinusLogProbMetric: 391.5213

Epoch 800: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6790 - MinusLogProbMetric: 384.6790 - val_loss: 391.5213 - val_MinusLogProbMetric: 391.5213 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 801/1000
2023-09-10 14:35:50.869 
Epoch 801/1000 
	 loss: 384.6811, MinusLogProbMetric: 384.6811, val_loss: 391.3876, val_MinusLogProbMetric: 391.3876

Epoch 801: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6811 - MinusLogProbMetric: 384.6811 - val_loss: 391.3876 - val_MinusLogProbMetric: 391.3876 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 802/1000
2023-09-10 14:36:14.628 
Epoch 802/1000 
	 loss: 384.6752, MinusLogProbMetric: 384.6752, val_loss: 391.5155, val_MinusLogProbMetric: 391.5155

Epoch 802: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6752 - MinusLogProbMetric: 384.6752 - val_loss: 391.5155 - val_MinusLogProbMetric: 391.5155 - lr: 2.6042e-06 - 24s/epoch - 121ms/step
Epoch 803/1000
2023-09-10 14:36:37.614 
Epoch 803/1000 
	 loss: 384.6875, MinusLogProbMetric: 384.6875, val_loss: 391.4356, val_MinusLogProbMetric: 391.4356

Epoch 803: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6875 - MinusLogProbMetric: 384.6875 - val_loss: 391.4356 - val_MinusLogProbMetric: 391.4356 - lr: 2.6042e-06 - 23s/epoch - 117ms/step
Epoch 804/1000
2023-09-10 14:36:59.729 
Epoch 804/1000 
	 loss: 384.6677, MinusLogProbMetric: 384.6677, val_loss: 391.4703, val_MinusLogProbMetric: 391.4703

Epoch 804: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6677 - MinusLogProbMetric: 384.6677 - val_loss: 391.4703 - val_MinusLogProbMetric: 391.4703 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 805/1000
2023-09-10 14:37:21.079 
Epoch 805/1000 
	 loss: 384.6600, MinusLogProbMetric: 384.6600, val_loss: 391.4740, val_MinusLogProbMetric: 391.4740

Epoch 805: val_loss did not improve from 391.37424
196/196 - 21s - loss: 384.6600 - MinusLogProbMetric: 384.6600 - val_loss: 391.4740 - val_MinusLogProbMetric: 391.4740 - lr: 2.6042e-06 - 21s/epoch - 109ms/step
Epoch 806/1000
2023-09-10 14:37:43.111 
Epoch 806/1000 
	 loss: 384.6593, MinusLogProbMetric: 384.6593, val_loss: 391.4056, val_MinusLogProbMetric: 391.4056

Epoch 806: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6593 - MinusLogProbMetric: 384.6593 - val_loss: 391.4056 - val_MinusLogProbMetric: 391.4056 - lr: 2.6042e-06 - 22s/epoch - 112ms/step
Epoch 807/1000
2023-09-10 14:38:06.010 
Epoch 807/1000 
	 loss: 384.6600, MinusLogProbMetric: 384.6600, val_loss: 391.5084, val_MinusLogProbMetric: 391.5084

Epoch 807: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6600 - MinusLogProbMetric: 384.6600 - val_loss: 391.5084 - val_MinusLogProbMetric: 391.5084 - lr: 2.6042e-06 - 23s/epoch - 117ms/step
Epoch 808/1000
2023-09-10 14:38:27.885 
Epoch 808/1000 
	 loss: 384.6604, MinusLogProbMetric: 384.6604, val_loss: 391.4378, val_MinusLogProbMetric: 391.4378

Epoch 808: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6604 - MinusLogProbMetric: 384.6604 - val_loss: 391.4378 - val_MinusLogProbMetric: 391.4378 - lr: 2.6042e-06 - 22s/epoch - 112ms/step
Epoch 809/1000
2023-09-10 14:38:51.227 
Epoch 809/1000 
	 loss: 384.6559, MinusLogProbMetric: 384.6559, val_loss: 391.4272, val_MinusLogProbMetric: 391.4272

Epoch 809: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6559 - MinusLogProbMetric: 384.6559 - val_loss: 391.4272 - val_MinusLogProbMetric: 391.4272 - lr: 2.6042e-06 - 23s/epoch - 119ms/step
Epoch 810/1000
2023-09-10 14:39:13.546 
Epoch 810/1000 
	 loss: 384.6710, MinusLogProbMetric: 384.6710, val_loss: 391.4067, val_MinusLogProbMetric: 391.4067

Epoch 810: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6710 - MinusLogProbMetric: 384.6710 - val_loss: 391.4067 - val_MinusLogProbMetric: 391.4067 - lr: 2.6042e-06 - 22s/epoch - 114ms/step
Epoch 811/1000
2023-09-10 14:39:36.123 
Epoch 811/1000 
	 loss: 384.6562, MinusLogProbMetric: 384.6562, val_loss: 391.4727, val_MinusLogProbMetric: 391.4727

Epoch 811: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6562 - MinusLogProbMetric: 384.6562 - val_loss: 391.4727 - val_MinusLogProbMetric: 391.4727 - lr: 2.6042e-06 - 23s/epoch - 115ms/step
Epoch 812/1000
2023-09-10 14:39:58.329 
Epoch 812/1000 
	 loss: 384.6575, MinusLogProbMetric: 384.6575, val_loss: 391.5694, val_MinusLogProbMetric: 391.5694

Epoch 812: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6575 - MinusLogProbMetric: 384.6575 - val_loss: 391.5694 - val_MinusLogProbMetric: 391.5694 - lr: 2.6042e-06 - 22s/epoch - 113ms/step
Epoch 813/1000
2023-09-10 14:40:21.250 
Epoch 813/1000 
	 loss: 384.6673, MinusLogProbMetric: 384.6673, val_loss: 391.4417, val_MinusLogProbMetric: 391.4417

Epoch 813: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6673 - MinusLogProbMetric: 384.6673 - val_loss: 391.4417 - val_MinusLogProbMetric: 391.4417 - lr: 2.6042e-06 - 23s/epoch - 117ms/step
Epoch 814/1000
2023-09-10 14:40:44.449 
Epoch 814/1000 
	 loss: 384.6752, MinusLogProbMetric: 384.6752, val_loss: 391.4866, val_MinusLogProbMetric: 391.4866

Epoch 814: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6752 - MinusLogProbMetric: 384.6752 - val_loss: 391.4866 - val_MinusLogProbMetric: 391.4866 - lr: 2.6042e-06 - 23s/epoch - 118ms/step
Epoch 815/1000
2023-09-10 14:41:08.347 
Epoch 815/1000 
	 loss: 384.6420, MinusLogProbMetric: 384.6420, val_loss: 391.4683, val_MinusLogProbMetric: 391.4683

Epoch 815: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6420 - MinusLogProbMetric: 384.6420 - val_loss: 391.4683 - val_MinusLogProbMetric: 391.4683 - lr: 2.6042e-06 - 24s/epoch - 122ms/step
Epoch 816/1000
2023-09-10 14:41:31.739 
Epoch 816/1000 
	 loss: 384.6445, MinusLogProbMetric: 384.6445, val_loss: 391.5926, val_MinusLogProbMetric: 391.5926

Epoch 816: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6445 - MinusLogProbMetric: 384.6445 - val_loss: 391.5926 - val_MinusLogProbMetric: 391.5926 - lr: 2.6042e-06 - 23s/epoch - 119ms/step
Epoch 817/1000
2023-09-10 14:41:55.092 
Epoch 817/1000 
	 loss: 384.6536, MinusLogProbMetric: 384.6536, val_loss: 391.4125, val_MinusLogProbMetric: 391.4125

Epoch 817: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6536 - MinusLogProbMetric: 384.6536 - val_loss: 391.4125 - val_MinusLogProbMetric: 391.4125 - lr: 2.6042e-06 - 23s/epoch - 119ms/step
Epoch 818/1000
2023-09-10 14:42:18.391 
Epoch 818/1000 
	 loss: 384.6424, MinusLogProbMetric: 384.6424, val_loss: 391.4272, val_MinusLogProbMetric: 391.4272

Epoch 818: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6424 - MinusLogProbMetric: 384.6424 - val_loss: 391.4272 - val_MinusLogProbMetric: 391.4272 - lr: 2.6042e-06 - 23s/epoch - 119ms/step
Epoch 819/1000
2023-09-10 14:42:41.793 
Epoch 819/1000 
	 loss: 384.6389, MinusLogProbMetric: 384.6389, val_loss: 391.4310, val_MinusLogProbMetric: 391.4310

Epoch 819: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6389 - MinusLogProbMetric: 384.6389 - val_loss: 391.4310 - val_MinusLogProbMetric: 391.4310 - lr: 2.6042e-06 - 23s/epoch - 119ms/step
Epoch 820/1000
2023-09-10 14:43:03.823 
Epoch 820/1000 
	 loss: 384.6426, MinusLogProbMetric: 384.6426, val_loss: 391.4293, val_MinusLogProbMetric: 391.4293

Epoch 820: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6426 - MinusLogProbMetric: 384.6426 - val_loss: 391.4293 - val_MinusLogProbMetric: 391.4293 - lr: 2.6042e-06 - 22s/epoch - 112ms/step
Epoch 821/1000
2023-09-10 14:43:25.465 
Epoch 821/1000 
	 loss: 384.6466, MinusLogProbMetric: 384.6466, val_loss: 391.5542, val_MinusLogProbMetric: 391.5542

Epoch 821: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6466 - MinusLogProbMetric: 384.6466 - val_loss: 391.5542 - val_MinusLogProbMetric: 391.5542 - lr: 2.6042e-06 - 22s/epoch - 110ms/step
Epoch 822/1000
2023-09-10 14:43:48.298 
Epoch 822/1000 
	 loss: 384.6415, MinusLogProbMetric: 384.6415, val_loss: 391.4884, val_MinusLogProbMetric: 391.4884

Epoch 822: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6415 - MinusLogProbMetric: 384.6415 - val_loss: 391.4884 - val_MinusLogProbMetric: 391.4884 - lr: 2.6042e-06 - 23s/epoch - 116ms/step
Epoch 823/1000
2023-09-10 14:44:11.013 
Epoch 823/1000 
	 loss: 384.6414, MinusLogProbMetric: 384.6414, val_loss: 391.4191, val_MinusLogProbMetric: 391.4191

Epoch 823: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6414 - MinusLogProbMetric: 384.6414 - val_loss: 391.4191 - val_MinusLogProbMetric: 391.4191 - lr: 2.6042e-06 - 23s/epoch - 116ms/step
Epoch 824/1000
2023-09-10 14:44:35.141 
Epoch 824/1000 
	 loss: 384.6381, MinusLogProbMetric: 384.6381, val_loss: 391.5070, val_MinusLogProbMetric: 391.5070

Epoch 824: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6381 - MinusLogProbMetric: 384.6381 - val_loss: 391.5070 - val_MinusLogProbMetric: 391.5070 - lr: 2.6042e-06 - 24s/epoch - 123ms/step
Epoch 825/1000
2023-09-10 14:44:58.228 
Epoch 825/1000 
	 loss: 384.6386, MinusLogProbMetric: 384.6386, val_loss: 391.4915, val_MinusLogProbMetric: 391.4915

Epoch 825: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6386 - MinusLogProbMetric: 384.6386 - val_loss: 391.4915 - val_MinusLogProbMetric: 391.4915 - lr: 2.6042e-06 - 23s/epoch - 118ms/step
Epoch 826/1000
2023-09-10 14:45:20.649 
Epoch 826/1000 
	 loss: 384.6419, MinusLogProbMetric: 384.6419, val_loss: 391.4680, val_MinusLogProbMetric: 391.4680

Epoch 826: val_loss did not improve from 391.37424
196/196 - 22s - loss: 384.6419 - MinusLogProbMetric: 384.6419 - val_loss: 391.4680 - val_MinusLogProbMetric: 391.4680 - lr: 2.6042e-06 - 22s/epoch - 114ms/step
Epoch 827/1000
2023-09-10 14:45:44.848 
Epoch 827/1000 
	 loss: 384.6413, MinusLogProbMetric: 384.6413, val_loss: 391.4813, val_MinusLogProbMetric: 391.4813

Epoch 827: val_loss did not improve from 391.37424
196/196 - 24s - loss: 384.6413 - MinusLogProbMetric: 384.6413 - val_loss: 391.4813 - val_MinusLogProbMetric: 391.4813 - lr: 2.6042e-06 - 24s/epoch - 123ms/step
Epoch 828/1000
2023-09-10 14:46:07.364 
Epoch 828/1000 
	 loss: 384.6405, MinusLogProbMetric: 384.6405, val_loss: 391.4411, val_MinusLogProbMetric: 391.4411

Epoch 828: val_loss did not improve from 391.37424
196/196 - 23s - loss: 384.6405 - MinusLogProbMetric: 384.6405 - val_loss: 391.4411 - val_MinusLogProbMetric: 391.4411 - lr: 2.6042e-06 - 23s/epoch - 115ms/step
Epoch 829/1000
2023-09-10 14:46:31.455 
Epoch 829/1000 
	 loss: 384.6412, MinusLogProbMetric: 384.6412, val_loss: 391.3924, val_MinusLogProbMetric: 391.3924

Epoch 829: val_loss did not improve from 391.37424
Restoring model weights from the end of the best epoch: 729.
196/196 - 25s - loss: 384.6412 - MinusLogProbMetric: 384.6412 - val_loss: 391.3924 - val_MinusLogProbMetric: 391.3924 - lr: 2.6042e-06 - 25s/epoch - 125ms/step
Epoch 829: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 2566.9712061460596 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 28355.437571933027 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
